\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amir et~al.(2017)Amir, Taba, Berg, Melano, McKinstry, di~Nolfo, Nayak, Andreopoulos, Garreau, Mendoza, Kusnitz, DeBole, Esser, Delbr{\"u}ck, Flickner, and Modha]{Amir2017ALP}
Amir, A., Taba, B., Berg, D.~J., Melano, T., McKinstry, J.~L., di~Nolfo, C., Nayak, T.~K., Andreopoulos, A., Garreau, G.~J., Mendoza, M., Kusnitz, J.~A., DeBole, M.~V., Esser, S.~K., Delbr{\"u}ck, T., Flickner, M., and Modha, D.~S.
\newblock A low power, fully event-based gesture recognition system.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  7388--7397, 2017.

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018empirical}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}, 2018.

\bibitem[Box et~al.(2015)Box, Jenkins, Reinsel, and Ljung]{box2015time}
Box, G.~E., Jenkins, G.~M., Reinsel, G.~C., and Ljung, G.~M.
\newblock \emph{Time series analysis: forecasting and control}.
\newblock John Wiley \& Sons, 2015.

\bibitem[Chen et~al.(2023)Chen, Wu, Tang, Ren, and Tan]{chen2023unleashing}
Chen, X., Wu, J., Tang, H., Ren, Q., and Tan, K.~C.
\newblock Unleashing the potential of spiking neural networks for sequential modeling with contextual embedding.
\newblock \emph{arXiv preprint arXiv:2308.15150}, 2023.

\bibitem[Cho et~al.(2014)Cho, van Merrienboer, G{\"{u}}l{\c{c}}ehre, Bahdanau, Bougares, Schwenk, and Bengio]{cho2014learning}
Cho, K., van Merrienboer, B., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using {RNN} encoder-decoder for statistical machine translation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2014}, pp.\  1724--1734, 2014.

\bibitem[Ding et~al.(2021)Ding, Yu, Tian, and Huang]{Ding2021OptimalAC}
Ding, J., Yu, Z., Tian, Y., and Huang, T.
\newblock Optimal {ANN-SNN} conversion for fast and accurate inference in deep spiking neural networks.
\newblock In Zhou, Z. (ed.), \emph{Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI} 2021}, pp.\  2328--2336, 2021.

\bibitem[Dominguez-Morales et~al.(2018)Dominguez-Morales, Liu, James, Gutierrez-Galan, Jimenez-Fernandez, Davidson, and Furber]{dominguez2018deep}
Dominguez-Morales, J.~P., Liu, Q., James, R., Gutierrez-Galan, D., Jimenez-Fernandez, A., Davidson, S., and Furber, S.
\newblock Deep spiking neural network model for time-variant signals classification: a real-time speech recognition approach.
\newblock In \emph{2018 International Joint Conference on Neural Networks (IJCNN)}, pp.\  1--8. IEEE, 2018.

\bibitem[Eshraghian et~al.(2021)Eshraghian, Ward, Neftci, Wang, Lenz, Dwivedi, Bennamoun, Jeong, and Lu]{Eshraghian2021TrainingSN}
Eshraghian, J.~K., Ward, M., Neftci, E.~O., Wang, X., Lenz, G., Dwivedi, G., Bennamoun, Jeong, D.~S., and Lu, W.~D.
\newblock Training spiking neural networks using lessons from deep learning.
\newblock \emph{ArXiv}, abs/2109.12894, 2021.

\bibitem[Fang et~al.(2020{\natexlab{a}})Fang, Shrestha, and Qiu]{fang2020multivariate}
Fang, H., Shrestha, A., and Qiu, Q.
\newblock Multivariate time series classification using spiking neural networks.
\newblock In \emph{2020 International Joint Conference on Neural Networks (IJCNN)}, pp.\  1--7. IEEE, 2020{\natexlab{a}}.

\bibitem[Fang et~al.(2020{\natexlab{b}})Fang, Chen, Ding, Chen, Yu, Zhou, Tian, and other contributors]{SpikingJelly}
Fang, W., Chen, Y., Ding, J., Chen, D., Yu, Z., Zhou, H., Tian, Y., and other contributors.
\newblock Spikingjelly, 2020{\natexlab{b}}.

\bibitem[Fang et~al.(2020{\natexlab{c}})Fang, Yu, Chen, Masquelier, Huang, and Tian]{Fang2020IncorporatingLM}
Fang, W., Yu, Z., Chen, Y., Masquelier, T., Huang, T., and Tian, Y.
\newblock Incorporating learnable membrane time constant to enhance learning of spiking neural networks.
\newblock \emph{2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, pp.\  2641--2651, 2020{\natexlab{c}}.

\bibitem[Fang et~al.(2021)Fang, Yu, Chen, Huang, Masquelier, and Tian]{Fang2021DeepRL}
Fang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., and Tian, Y.
\newblock Deep residual learning in spiking neural networks.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Fang et~al.(2023)Fang, Ren, Shan, Shen, Li, Zhang, Yu, and Li]{fang2023learning}
Fang, Y., Ren, K., Shan, C., Shen, Y., Li, Y., Zhang, W., Yu, Y., and Li, D.
\newblock Learning decomposed spatial relations for multi-variate time-series modeling.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pp.\  7530--7538, 2023.

\bibitem[Gonz{\'a}lez~Sope{\~n}a et~al.(2022)Gonz{\'a}lez~Sope{\~n}a, Pakrashi, and Ghosh]{gonzalez2022spiking}
Gonz{\'a}lez~Sope{\~n}a, J.~M., Pakrashi, V., and Ghosh, B.
\newblock A spiking neural network based wind power forecasting model for neuromorphic devices.
\newblock \emph{Energies}, 15\penalty0 (19):\penalty0 7256, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and Pattern Recognition}, pp.\  770--778, 2016.

\bibitem[Horowitz(2014)]{horowitz20141}
Horowitz, M.
\newblock 1.1 computing's energy problem (and what we can do about it).
\newblock In \emph{2014 {IEEE} International Conference on Solid-State Circuits Conference, {ISSCC} 2014, Digest of Technical Papers}, pp.\  10--14. {IEEE}, 2014.

\bibitem[Hu et~al.(2018)Hu, Tang, and Pan]{hu2018spiking}
Hu, Y., Tang, H., and Pan, G.
\newblock Spiking deep residual networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2018.

\bibitem[Izhikevich(2003)]{Izhikevich2003SimpleMO}
Izhikevich, E.~M.
\newblock Simple model of spiking neurons.
\newblock \emph{IEEE Transactions on Neural Networks}, 14 6:\penalty0 1569--72, 2003.

\bibitem[Jeffares et~al.(2022)Jeffares, Guo, Stenetorp, and Moraitis]{jeffares2022spikeinspired}
Jeffares, A., Guo, Q., Stenetorp, P., and Moraitis, T.
\newblock Spike-inspired rank coding for fast and accurate recurrent neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kulkarni et~al.(2013)Kulkarni, Simon, and Sundareswaran]{kulkarni2013spiking}
Kulkarni, S., Simon, S.~P., and Sundareswaran, K.
\newblock A spiking neural network (snn) forecast engine for short-term electrical load forecasting.
\newblock \emph{Applied Soft Computing}, 13\penalty0 (8):\penalty0 3628--3635, 2013.

\bibitem[Lai et~al.(2018)Lai, Chang, Yang, and Liu]{lai2018modeling}
Lai, G., Chang, W.-C., Yang, Y., and Liu, H.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In \emph{The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval}, pp.\  95--104, 2018.

\bibitem[Le{\~n}ero-Bardallo et~al.(2011)Le{\~n}ero-Bardallo, Serrano-Gotarredona, and Linares-Barranco]{lenero20113}
Le{\~n}ero-Bardallo, J.~A., Serrano-Gotarredona, T., and Linares-Barranco, B.
\newblock A 3.6 $\mu$s latency asynchronous frame-free event-driven dynamic-vision-sensor.
\newblock \emph{IEEE Journal of Solid-State Circuits}, 46\penalty0 (6):\penalty0 1443--1455, 2011.

\bibitem[Li et~al.(2023)Li, Deng, Tang, Pan, Tian, Roy, and Maass]{li2023brain}
Li, G., Deng, L., Tang, H., Pan, G., Tian, Y., Roy, K., and Maass, W.
\newblock Brain inspired computing: A systematic survey and future trends.
\newblock \emph{Authorea Preprints}, 2023.

\bibitem[Li et~al.(2017{\natexlab{a}})Li, Liu, Ji, Li, and Shi]{Li2017CIFAR10DVSAE}
Li, H., Liu, H., Ji, X., Li, G., and Shi, L.
\newblock Cifar10-dvs: An event-stream dataset for object classification.
\newblock \emph{Frontiers in Neuroscience}, 11, 2017{\natexlab{a}}.

\bibitem[Li et~al.(2017{\natexlab{b}})Li, Yu, Shahabi, and Liu]{li2017diffusion}
Li, Y., Yu, R., Shahabi, C., and Liu, Y.
\newblock Diffusion convolutional recurrent neural network: Data-driven traffic forecasting.
\newblock \emph{arXiv preprint arXiv:1707.01926}, 2017{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Lei, and Yang]{li2022spikeformer}
Li, Y., Lei, Y., and Yang, X.
\newblock Spikeformer: A novel architecture for training high-performance low-latency spiking neural network.
\newblock \emph{arXiv preprint arXiv:2211.10686}, 2022.

\bibitem[Liu et~al.(2022)Liu, Zeng, Chen, Xu, Lai, Ma, and Xu]{liu2022scinet}
Liu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., and Xu, Q.
\newblock Scinet: Time series modeling and forecasting with sample convolution and interaction.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5816--5828, 2022.

\bibitem[Liu et~al.(2021)Liu, Long, Peng, Wang, Yang, Song, Riscos-N{\'u}{\~n}ez, and P{\'e}rez-Jim{\'e}nez]{liu2021gated}
Liu, Q., Long, L., Peng, H., Wang, J., Yang, Q., Song, X., Riscos-N{\'u}{\~n}ez, A., and P{\'e}rez-Jim{\'e}nez, M.~J.
\newblock Gated spiking neural p systems for time series forecasting.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2021.

\bibitem[Liu et~al.(2024)Liu, Hu, Zhang, Wu, Wang, Ma, and Long]{liu2023itransformer}
Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Long et~al.(2022)Long, Liu, Peng, Wang, and Yang]{long2022multivariate}
Long, L., Liu, Q., Peng, H., Wang, J., and Yang, Q.
\newblock Multivariate time series forecasting method based on nonlinear spiking neural p systems and non-subsampled shearlet transform.
\newblock \emph{Neural Networks}, 152:\penalty0 300--310, 2022.

\bibitem[Lv et~al.(2023{\natexlab{a}})Lv, Li, Xu, Gu, Ling, Zhang, Zheng, and Huang]{Lv2023SpikeBERTAL}
Lv, C., Li, T., Xu, J., Gu, C., Ling, Z., Zhang, C., Zheng, X., and Huang, X.
\newblock Spikebert: A language spikformer learned from bert with knowledge distillation.
\newblock 2023{\natexlab{a}}.

\bibitem[Lv et~al.(2023{\natexlab{b}})Lv, Xu, and Zheng]{lv2023spiking}
Lv, C., Xu, J., and Zheng, X.
\newblock Spiking convolutional neural networks for text classification.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Maass(1997)]{Maas1997NetworksOS}
Maass, W.
\newblock Networks of spiking neurons: the third generation of neural network models.
\newblock \emph{Neural Networks}, 14:\penalty0 1659--1671, 1997.

\bibitem[Qu et~al.(2024)Qu, Wang, Luo, He, and Li]{qu2024cnn}
Qu, E., Wang, Y., Luo, X., He, W., and Li, D.
\newblock {CNN} kernels can be the best shapelets.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Reid et~al.(2014)Reid, Hussain, and Tawfik]{reid2014financial}
Reid, D., Hussain, A.~J., and Tawfik, H.
\newblock Financial time series prediction using spiking neural networks.
\newblock \emph{PloS one}, 9\penalty0 (8):\penalty0 e103656, 2014.

\bibitem[Roberts et~al.(2013)Roberts, Osborne, Ebden, Reece, Gibson, and Aigrain]{roberts2013gaussian}
Roberts, S., Osborne, M., Ebden, M., Reece, S., Gibson, N., and Aigrain, S.
\newblock Gaussian processes for time-series modelling.
\newblock \emph{Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}, 371\penalty0 (1984):\penalty0 20110550, 2013.

\bibitem[Rueckauer et~al.(2017)Rueckauer, Lungu, Hu, Pfeiffer, and Liu]{Rueckauer2017ConversionOC}
Rueckauer, B., Lungu, I.-A., Hu, Y., Pfeiffer, M., and Liu, S.-C.
\newblock Conversion of continuous-valued deep networks to efficient event-driven networks for image classification.
\newblock \emph{Frontiers in Neuroscience}, 11, 2017.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and Williams]{rumelhart1986learning}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[Siami-Namini et~al.(2019)Siami-Namini, Tavakoli, and Namin]{siami2019performance}
Siami-Namini, S., Tavakoli, N., and Namin, A.~S.
\newblock The performance of lstm and bilstm in forecasting time series.
\newblock In \emph{2019 IEEE International Conference on Big Data}, pp.\  3285--3292. IEEE, 2019.

\bibitem[Werbos(1990)]{Werbos1990BackpropagationTT}
Werbos, P.~J.
\newblock Backpropagation through time: What it does and how to do it.
\newblock \emph{Proc. IEEE}, 78:\penalty0 1550--1560, 1990.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
Wu, H., Xu, J., Wang, J., and Long, M.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 22419--22430, 2021.

\bibitem[Wu et~al.(2023)Wu, Hu, Liu, Zhou, Wang, and Long]{wu2022timesnet}
Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}, 2023.

\bibitem[Wu et~al.(2019)Wu, Deng, Li, Zhu, Xie, and Shi]{wu2019direct}
Wu, Y., Deng, L., Li, G., Zhu, J., Xie, Y., and Shi, L.
\newblock Direct training for spiking neural networks: Faster, larger, better.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~33, pp.\  1311--1318, 2019.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Hu, Zhou, Yuan, Tian, Bo, and Li]{yao2023spike}
Yao, M., Hu, J., Zhou, Z., Yuan, L., Tian, Y., Bo, X., and Li, G.
\newblock Spike-driven transformer.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{a}}.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Zhao, Zhang, Hu, Deng, Tian, Xu, and Li]{yao2022attention}
Yao, M., Zhao, G., Zhang, H., Hu, Y., Deng, L., Tian, Y., Xu, B., and Li, G.
\newblock Attention spiking neural networks.
\newblock \emph{{IEEE} Trans. Pattern Anal. Mach. Intell.}, 45\penalty0 (8):\penalty0 9393--9410, 2023{\natexlab{b}}.

\bibitem[Ye \& Keogh(2009)Ye and Keogh]{ye2009time}
Ye, L. and Keogh, E.
\newblock Time series shapelets: a new primitive for data mining.
\newblock In \emph{Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, pp.\  947--956, 2009.

\bibitem[Yu et~al.(2018)Yu, Yin, and Zhu]{yu2017spatio}
Yu, B., Yin, H., and Zhu, Z.
\newblock Spatio-temporal graph convolutional networks: {A} deep learning framework for traffic forecasting.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI} 2018}, pp.\  3634--3640, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Shen, Zhao, and Yang]{zhang2017time}
Zhang, X., Shen, F., Zhao, J., and Yang, G.
\newblock Time series forecasting using gru neural network with multi-lag after decomposition.
\newblock In \emph{Neural Information Processing: 24th International Conference, ICONIP 2017, Guangzhou, China, November 14--18, 2017, Proceedings, Part V 24}, pp.\  523--532. Springer, 2017.

\bibitem[Zhang \& Yan(2022)Zhang and Yan]{zhang2022crossformer}
Zhang, Y. and Yan, J.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Yu, Zhou, Zhang, Ma, Zhou, and Tian]{zhou2023spikingformer}
Zhou, C., Yu, L., Zhou, Z., Zhang, H., Ma, Z., Zhou, H., and Tian, Y.
\newblock Spikingformer: Spike-driven residual learning for transformer-based spiking neural network.
\newblock \emph{arXiv preprint arXiv:2304.11954}, 2023{\natexlab{a}}.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Zhu, He, Wang, Yan, Tian, and Yuan]{Zhou2022SpikformerWS}
Zhou, Z., Zhu, Y., He, C., Wang, Y., Yan, S., Tian, Y., and Yuan, L.
\newblock Spikformer: When spiking neural network meets transformer.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}, 2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2024)Zhou, Che, Fang, Tian, Zhu, Yan, Tian, and Yuan]{zhou2024spikformer}
Zhou, Z., Che, K., Fang, W., Tian, K., Zhu, Y., Yan, S., Tian, Y., and Yuan, L.
\newblock Spikformer v2: Join the high accuracy club on imagenet with an snn ticket.
\newblock \emph{arXiv preprint arXiv:2401.02020}, 2024.

\end{thebibliography}
