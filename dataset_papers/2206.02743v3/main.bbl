\begin{thebibliography}{10}

\bibitem{docT5query_url}
\url{https://github.com/castorini/docTTTTTquery}.

\bibitem{bm25_url}
\url{https://github.com/castorini/anserini}.

\bibitem{bentley1975multidimensional}
Jon~Louis Bentley.
\newblock Multidimensional binary search trees used for associative searching.
\newblock {\em Communications of the ACM}, 18(9):509--517, 1975.

\bibitem{bevilacqua2022autoregressive}
Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian
  Riedel, and Fabio Petroni.
\newblock Autoregressive search engines: Generating substrings as document
  identifiers.
\newblock {\em arXiv preprint arXiv:2204.10628}, 2022.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{chang2019pre}
Wei-Cheng Chang, X~Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.
\newblock Pre-training tasks for embedding-based large-scale retrieval.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{chen2021spann}
Qi~Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li, Mao
  Yang, and Jingdong Wang.
\newblock Spann: Highly-efficient billion-scale approximate nearest neighbor
  search.
\newblock {\em arXiv preprint arXiv:2111.08566}, 2021.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem{dai2019context}
Zhuyun Dai and Jamie Callan.
\newblock Context-aware sentence/passage term importance estimation for first
  stage retrieval.
\newblock {\em arXiv preprint arXiv:1910.10687}, 2019.

\bibitem{datar2004locality}
Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab~S Mirrokni.
\newblock Locality-sensitive hashing scheme based on p-stable distributions.
\newblock In {\em Proceedings of the twentieth annual symposium on
  Computational geometry}, pages 253--262, 2004.

\bibitem{de2021highly}
Nicola De~Cao, Wilker Aziz, and Ivan Titov.
\newblock Highly parallel autoregressive entity linking with discriminative
  correction.
\newblock {\em arXiv preprint arXiv:2109.03792}, 2021.

\bibitem{de2020autoregressive}
Nicola De~Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni.
\newblock Autoregressive entity retrieval.
\newblock {\em arXiv preprint arXiv:2010.00904}, 2020.

\bibitem{de2021multilingual}
Nicola De~Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal, Mikhail
  Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, and Fabio
  Petroni.
\newblock Multilingual autoregressive entity linking.
\newblock {\em arXiv preprint arXiv:2103.12528}, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, 2019.

\bibitem{gao2021condenser}
Luyu Gao and Jamie Callan.
\newblock Condenser: a pre-training architecture for dense retrieval.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 981--993, 2021.

\bibitem{gao2021your}
Luyu Gao and Jamie Callan.
\newblock Is your language model ready for dense representation fine-tuning.
\newblock {\em arXiv preprint arXiv:2104.08253}, 2021.

\bibitem{gao2021unsupervised}
Luyu Gao and Jamie Callan.
\newblock Unsupervised corpus aware language model pre-training for dense
  passage retrieval.
\newblock {\em arXiv preprint arXiv:2108.05540}, 2021.

\bibitem{gao2021coil}
Luyu Gao, Zhuyun Dai, and Jamie Callan.
\newblock Coil: Revisit exact lexical match in information retrieval with
  contextualized inverted list.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 3030--3042, 2021.

\bibitem{gao2020deep}
Weihao Gao, Xiangjun Fan, Chong Wang, Jiankai Sun, Kai Jia, Wenzhi Xiao, Ruofan
  Ding, Xingyan Bin, Hui Yang, and Xiaobing Liu.
\newblock Deep retrieval: Learning a retrievable structure for large-scale
  recommendations.
\newblock {\em arXiv preprint arXiv:2007.07203}, 2020.

\bibitem{guo2016deep}
Jiafeng Guo, Yixing Fan, Qingyao Ai, and W~Bruce Croft.
\newblock A deep relevance matching model for ad-hoc retrieval.
\newblock In {\em Proceedings of the 25th ACM international on conference on
  information and knowledge management}, pages 55--64, 2016.

\bibitem{guo2018comparison}
Tonglei Guo, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, and Xueqi Cheng.
\newblock A comparison between term-based and embedding-based methods for
  initial retrieval.
\newblock In {\em China Conference on Information Retrieval}, pages 28--40.
  Springer, 2018.

\bibitem{hartigan1979algorithm}
John~A Hartigan and Manchek~A Wong.
\newblock Algorithm as 136: A k-means clustering algorithm.
\newblock {\em Journal of the royal statistical society. series c (applied
  statistics)}, 28(1):100--108, 1979.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{huang2013learning}
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li~Deng, Alex Acero, and Larry Heck.
\newblock Learning deep structured semantic models for web search using
  clickthrough data.
\newblock In {\em Proceedings of the 22nd ACM international conference on
  Information \& Knowledge Management}, pages 2333--2338, 2013.

\bibitem{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2704--2713, 2018.

\bibitem{jayaram2019diskann}
Suhas Jayaram~Subramanya, Fnu Devvrit, Harsha~Vardhan Simhadri, Ravishankar
  Krishnawamy, and Rohan Kadekodi.
\newblock Diskann: Fast accurate billion-point nearest neighbor search on a
  single node.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{johnson2019billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with {GPUs}.
\newblock {\em IEEE Transactions on Big Data}, 7(3):535--547, 2019.

\bibitem{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock {\em arXiv preprint arXiv:1705.03551}, 2017.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin, Barlas O{\u{g}}uz, Sewon Min, Patrick Lewis, Ledell Wu,
  Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock {\em arXiv preprint arXiv:2004.04906}, 2020.

\bibitem{khattab2020colbert}
Omar Khattab and Matei Zaharia.
\newblock Colbert: Efficient and effective passage search via contextualized
  late interaction over bert.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR conference on
  research and development in Information Retrieval}, pages 39--48, 2020.

\bibitem{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:453--466, 2019.

\bibitem{lafferty2001document}
John Lafferty and Chengxiang Zhai.
\newblock Document language models, query models, and risk minimization for
  information retrieval.
\newblock In {\em Proceedings of the 24th annual international ACM SIGIR
  conference on Research and development in information retrieval}, pages
  111--119, 2001.

\bibitem{li2020parade}
Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun.
\newblock Parade: Passage representation aggregation for document reranking.
\newblock {\em arXiv preprint arXiv:2008.09093}, 2020.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock {\em CoRR}, abs/1907.11692, 2019.

\bibitem{lu2020twinbert}
Wenhao Lu, Jian Jiao, and Ruofei Zhang.
\newblock Twinbert: Distilling knowledge to twin-structured compressed bert
  models for large-scale retrieval.
\newblock In {\em Proceedings of the 29th ACM International Conference on
  Information \& Knowledge Management}, pages 2645--2652, 2020.

\bibitem{luan2021sparse}
Yi~Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins.
\newblock Sparse, dense, and attentional representations for text retrieval.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:329--345, 2021.

\bibitem{malkov2018efficient}
Yu~A Malkov and Dmitry~A Yashunin.
\newblock Efficient and robust approximate nearest neighbor search using
  hierarchical navigable small world graphs.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  42(4):824--836, 2018.

\bibitem{mao2020generation}
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han,
  and Weizhu Chen.
\newblock Generation-augmented retrieval for open-domain question answering.
\newblock {\em arXiv preprint arXiv:2009.08553}, 2020.

\bibitem{nogueira2019doc2query}
Rodrigo Nogueira, Jimmy Lin, and AI~Epistemic.
\newblock From doc2query to doctttttquery.
\newblock {\em Online preprint}, 2019.

\bibitem{nogueira2019document}
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho.
\newblock Document expansion by query prediction.
\newblock {\em arXiv preprint arXiv:1904.08375}, 2019.

\bibitem{pedregosa2011scikit}
Fabian Pedregosa, Ga{\"e}l Varoquaux, Alexandre Gramfort, Vincent Michel,
  Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
  Weiss, Vincent Dubourg, et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock {\em the Journal of machine Learning research}, 12:2825--2830, 2011.

\bibitem{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv preprint arXiv:1910.10683}, 2019.

\bibitem{robertson2009probabilistic}
Stephen Robertson and Hugo Zaragoza.
\newblock {\em The probabilistic relevance framework: BM25 and beyond}.
\newblock Now Publishers Inc, 2009.

\bibitem{robertson1997relevance}
Stephen~E Robertson and Steve Walker.
\newblock On relevance weights with little relevance information.
\newblock In {\em Proceedings of the 20th annual international ACM SIGIR
  conference on Research and development in information retrieval}, pages
  16--24, 1997.

\bibitem{sachan2021end}
Devendra~Singh Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping,
  William~L Hamilton, and Bryan Catanzaro.
\newblock End-to-end training of neural retrievers for open-domain question
  answering.
\newblock {\em arXiv preprint arXiv:2101.00408}, 2021.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{shen2014learning}
Yelong Shen, Xiaodong He, Jianfeng Gao, Li~Deng, and Gr{\'e}goire Mesnil.
\newblock Learning semantic representations using convolutional neural networks
  for web search.
\newblock In {\em Proceedings of the 23rd international conference on world
  wide web}, pages 373--374, 2014.

\bibitem{tay2020synthesizer}
Yi~Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng.
\newblock Synthesizer: Rethinking self-attention in transformer models.
\newblock {\em arXiv preprint arXiv:2005.00743}, 2020.

\bibitem{tay2022transformer}
Yi~Tay, Vinh~Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen
  Qin, Kai Hui, Zhe Zhao, Jai Gupta, et~al.
\newblock Transformer memory as a differentiable search index.
\newblock {\em arXiv preprint arXiv:2202.06991}, 2022.

\bibitem{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of machine learning research}, 9(11), 2008.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 5998--6008, 2017.

\bibitem{wolf2019huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock {\em arXiv preprint arXiv:1910.03771}, 2019.

\bibitem{xiong2020approximate}
Lee Xiong, Chenyan Xiong, Ye~Li, Kwok-Fung Tang, Jialin Liu, Paul~N Bennett,
  Junaid Ahmed, and Arnold Overwijk.
\newblock Approximate nearest neighbor negative contrastive learning for dense
  text retrieval.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{yang2020retriever}
Sohee Yang and Minjoon Seo.
\newblock Is retriever merely an approximator of reader?
\newblock {\em arXiv preprint arXiv:2010.10999}, 2020.

\bibitem{yang2019simple}
Wei Yang, Haotian Zhang, and Jimmy Lin.
\newblock Simple applications of bert for ad hoc document retrieval.
\newblock {\em arXiv preprint arXiv:1903.10972}, 2019.

\bibitem{zamani2018neural}
Hamed Zamani, Mostafa Dehghani, W~Bruce Croft, Erik Learned-Miller, and Jaap
  Kamps.
\newblock From neural re-ranking to neural ranking: Learning a sparse
  representation for inverted indexing.
\newblock In {\em Proceedings of the 27th ACM international conference on
  information and knowledge management}, pages 497--506, 2018.

\bibitem{zhang2021adversarial}
Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen.
\newblock Adversarial retriever-ranker for dense text retrieval.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Zhuang2021DeepQL}
Shengyao Zhuang, Hang Li, and G.~Zuccon.
\newblock Deep query likelihood model for information retrieval.
\newblock In {\em ECIR}, 2021.

\end{thebibliography}
