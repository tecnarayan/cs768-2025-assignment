\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{RMQAJ13}

\bibitem[AOA{\etalchar{+}}20]{ainslie2020etc}
J.~Ainslie, Santiago Ontañón, Chris Alberti, V.~Cvicek, Zachary~Kenneth
  Fisher, Philip Pham, Anirudh Ravula, Sumit~K. Sanghai, Qifan Wang, and
  Li~Yang.
\newblock Etc: Encoding long and structured inputs in transformers.
\newblock {\em Conference On Empirical Methods In Natural Language Processing},
  2020.

\bibitem[AS68]{Atkinson1968HumanMA}
Richard~C. Atkinson and Richard~M. Shiffrin.
\newblock Human memory: A proposed system and its control processes.
\newblock In {\em The psychology of learning and motivation}, 1968.

\bibitem[BCB14]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em International Conference On Learning Representations}, 2014.

\bibitem[BD20]{barrett2020implicit}
D.~Barrett and B.~Dherin.
\newblock Implicit gradient regularization.
\newblock {\em International Conference On Learning Representations}, 2020.

\bibitem[BFH{\etalchar{+}}18]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem[BZMA20]{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock {\em Advances in neural information processing systems},
  33:12449--12460, 2020.

\bibitem[CLD{\etalchar{+}}20]{choromanski2020rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock {\em arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[DFE{\etalchar{+}}22]{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with
  {IO}-awareness.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem[DMRB22]{gc}
Benoit Dherin, Michael Munn, Mihaela Rosca, and David Barrett.
\newblock Why neural networks find simple solutions: The many regularizers of
  geometric complexity.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, {\em Advances in Neural Information Processing Systems}, volume~35,
  pages 2333--2349. Curran Associates, Inc., 2022.

\bibitem[DYY{\etalchar{+}}19]{dai2019transformerxl}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V. Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock {\em ACL}, 2019.

\bibitem[EUD17]{silu}
Stefan Elfwing, E.~Uchibe, and K.~Doya.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock {\em Neural Networks}, 2017.

\bibitem[FDS{\etalchar{+}}23]{h3}
Daniel~Y Fu, Tri Dao, Khaled~Kamal Saab, Armin~W Thomas, Atri Rudra, and
  Christopher Re.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[FZS22]{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em The Journal of Machine Learning Research}, 23(1):5232--5270,
  2022.

\bibitem[GB22]{gupta2022diagonal}
Ankit Gupta and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock {\em ARXIV.ORG}, 2022.

\bibitem[GDE{\etalchar{+}}20]{NEURIPS2020_102f0bb6}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\'{e}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1474--1487. Curran Associates, Inc., 2020.

\bibitem[GGGR22]{s4d}
Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock {\em ARXIV.ORG}, 2022.

\bibitem[GGR21]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher R'e.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock {\em International Conference On Learning Representations}, 2021.

\bibitem[GLH{\etalchar{+}}19]{goyal2019recurrent}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, S.~Levine, Yoshua
  Bengio, and B.~Scholkopf.
\newblock Recurrent independent mechanisms.
\newblock {\em International Conference on Learning Representations}, 2019.

\bibitem[GLL{\etalchar{+}}22]{guan-etal-2022-transkimmer}
Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, and Minyi Guo.
\newblock Transkimmer: Transformer learns to layer-wise skim.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 7275--7286, Dublin,
  Ireland, may 2022. Association for Computational Linguistics.

\bibitem[Gra16]{act}
A.~Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock {\em ARXIV.ORG}, 2016.

\bibitem[GZYE20]{gale2020sparse}
Trevor Gale, M.~Zaharia, C.~Young, and Erich Elsen.
\newblock Sparse gpu kernels for deep learning.
\newblock {\em International Conference For High Performance Computing,
  Networking, Storage And Analysis}, 2020.

\bibitem[HDLL22]{hua2022transformer}
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc~V. Le.
\newblock Transformer quality in linear time.
\newblock {\em International Conference On Machine Learning}, 2022.

\bibitem[HLW{\etalchar{+}}22]{hasani2022liquid}
Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander
  Amini, and Daniela Rus.
\newblock Liquid structural state-space models.
\newblock {\em arXiv preprint arXiv:2209.12951}, 2022.

\bibitem[Hut06]{enwiki8}
Marcus Hutter.
\newblock The human knowledge compression contest.
\newblock http://prize.hutter1.net/, 2006.

\bibitem[JGB{\etalchar{+}}21]{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and
  João Carreira.
\newblock Perceiver: General perception with iterative attention.
\newblock {\em International Conference On Machine Learning}, 2021.

\bibitem[JGP17]{gsoft}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem[JJNH91]{moe}
Robert~A. Jacobs, Michael~I. Jordan, Steven~J. Nowlan, and Geoffrey~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock {\em Neural Computation}, 3:79--87, 1991.

\bibitem[KH{\etalchar{+}}09]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[KKL20]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[LH18]{adw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[LJH{\etalchar{+}}19]{radam}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock {\em International Conference on Learning Representations}, 2019.

\bibitem[LKV{\etalchar{+}}18]{linsley2018learning}
Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas
  Serre.
\newblock Learning long-range spatial dependencies with horizontal gated
  recurrent units.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem[LLX{\etalchar{+}}20]{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, M.~Krikun, Noam~M. Shazeer, and Z.~Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock {\em International Conference On Learning Representations}, 2020.

\bibitem[LQC{\etalchar{+}}22]{dynamic}
Liu Liu, Zheng Qu, Zhaodong Chen, Fengbin Tu, Yufei Ding, and Yuan Xie.
\newblock Dynamic sparse attention for scalable transformer acceleration.
\newblock {\em IEEE Transactions on Computers}, 71:3165--3178, 2022.

\bibitem[MDP{\etalchar{+}}11]{maas2011learning}
Andrew Maas, Raymond~E Daly, Peter~T Pham, Dan Huang, Andrew~Y Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th annual meeting of the association for
  computational linguistics: Human language technologies}, pages 142--150,
  2011.

\bibitem[MKW{\etalchar{+}}21]{ma2021luna}
Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and
  Luke Zettlemoyer.
\newblock Luna: Linear unified nested attention.
\newblock {\em Advances in Neural Information Processing Systems},
  34:2441--2453, 2021.

\bibitem[MZK{\etalchar{+}}23]{mega}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig,
  Jonathan May, and Luke Zettlemoyer.
\newblock Mega: Moving average equipped gated attention.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[NB18]{nangia2018listops}
Nikita Nangia and Samuel~R Bowman.
\newblock Listops: A diagnostic dataset for latent tree learning.
\newblock {\em arXiv preprint arXiv:1804.06028}, 2018.

\bibitem[PGM{\etalchar{+}}19]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem[RMQAJ13]{radev2013acl}
Dragomir~R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara.
\newblock The acl anthology network corpus.
\newblock {\em Language Resources and Evaluation}, 47:919--944, 2013.

\bibitem[RSVG20a]{routing}
Aurko Roy, M.~Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock {\em International Conference On Topology, Algebra And Categories In
  Logic}, 2020.

\bibitem[RSVG20b]{roy2020efficient}
Aurko Roy, M.~Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock {\em International Conference On Topology, Algebra And Categories In
  Logic}, 2020.

\bibitem[RZW{\etalchar{+}}22]{ren-etal-2022-language}
Liliang Ren, Zixuan Zhang, Han Wang, Clare Voss, ChengXiang Zhai, and Heng Ji.
\newblock Language model pre-training with sparse latent typing.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 1480--1494, Abu Dhabi, United Arab
  Emirates, dec 2022. Association for Computational Linguistics.

\bibitem[SGBJ19]{sukhbaatar2019adaptive}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock {\em arXiv preprint arXiv:1905.07799}, 2019.

\bibitem[SJP{\etalchar{+}}21]{expire}
Sainbayar Sukhbaatar, Da~Ju, Spencer Poff, Stephen Roller, Arthur~D. Szlam,
  J.~Weston, and Angela Fan.
\newblock Not all memories are created equal: Learning to forget by expiring.
\newblock {\em International Conference On Machine Learning}, 2021.

\bibitem[SLK{\etalchar{+}}18]{Seong2018TowardsFL}
Sihyeon Seong, Yegang Lee, Youngwook Kee, Dongyoon Han, and Junmo Kim.
\newblock Towards flatter loss surface via nonmonotonic learning rate
  scheduling.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}, 2018.

\bibitem[SLP{\etalchar{+}}21]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock {\em arXiv preprint arXiv: 2104.09864}, 2021.

\bibitem[SML{\etalchar{+}}21]{so2021primer}
David~R. So, Wojciech Ma'nke, Hanxiao Liu, Zihang Dai, Noam~M. Shazeer, and
  Quoc~V. Le.
\newblock Primer: Searching for efficient transformers for language modeling.
\newblock {\em ARXIV.ORG}, 2021.

\bibitem[SUV18]{shaw2018selfattention}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock {\em NAACL}, 2018.

\bibitem[SWL23]{s5}
Jimmy~T.H. Smith, Andrew Warrington, and Scott Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[TDA{\etalchar{+}}20]{tay2020long}
Yi~Tay, M.~Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  J.~Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock {\em International Conference On Learning Representations}, 2020.

\bibitem[VPSP23]{vardasbi2023state}
Ali Vardasbi, Telmo Pires, Robin~M. Schmidt, and Stephan Peitz.
\newblock State spaces aren't enough: Machine translation needs attention.
\newblock {\em ARXIV.ORG}, 2023.

\bibitem[VSP{\etalchar{+}}17]{vaswani2017attention}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em NIPS}, 2017.

\bibitem[War18]{warden2018speech}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock {\em arXiv preprint arXiv: Arxiv-1804.03209}, 2018.

\bibitem[WLK{\etalchar{+}}20]{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[XM22]{xu2022survey}
Canwen Xu and Julian McAuley.
\newblock A survey on dynamic neural networks for natural language processing.
\newblock {\em EMNLP Findings}, 2022.

\bibitem[ZLL{\etalchar{+}}22]{zhou2022mixture}
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew~M
  Dai, Quoc~V Le, James Laudon, et~al.
\newblock Mixture-of-experts with expert choice routing.
\newblock {\em Advances in Neural Information Processing Systems},
  35:7103--7114, 2022.

\end{thebibliography}
