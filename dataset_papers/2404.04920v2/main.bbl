\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{TT}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 1273--1286, 2021.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ddpm}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Janner et~al.(2022)Janner, Du, Tenenbaum, and Levine]{diffuser}
Michael Janner, Yilun Du, Joshua~B. Tenenbaum, and Sergey Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock In \emph{{ICML}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 9902--9915. {PMLR}, 2022.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and Levine]{IQL}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=68n2s9ZJWF8}.

\bibitem[Ajay et~al.(2023)Ajay, Du, Gupta, Tenenbaum, Jaakkola, and Agrawal]{decisiondiffuser}
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua~B. Tenenbaum, Tommi~S. Jaakkola, and Pulkit Agrawal.
\newblock Is conditional generative modeling all you need for decision making?
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=sP1fo2K9DFG}.

\bibitem[Dong et~al.(2023)Dong, Yuan, Hao, Ni, Mu, Zheng, Hu, Lv, Fan, and Hu]{aligndiff}
Zibin Dong, Yifu Yuan, Jianye Hao, Fei Ni, Yao Mu, Yan Zheng, Yujing Hu, Tangjie Lv, Changjie Fan, and Zhipeng Hu.
\newblock Aligndiff: Aligning diverse human preferences via behavior-customisable diffusion model.
\newblock \emph{CoRR}, abs/2310.02054, 2023.

\bibitem[Ni et~al.(2023)Ni, Hao, Mu, Yuan, Zheng, Wang, and Liang]{metadiffuser}
Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, and Zhixuan Liang.
\newblock Metadiffuser: Diffusion model as conditional planner for offline meta-rl.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 26087--26105. {PMLR}, 2023.

\bibitem[He et~al.(2023)He, Bai, Xu, Yang, Zhang, Wang, Zhao, and Li]{MTDiff}
Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li.
\newblock Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=fAdMly4ki5}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{PbRL2017}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hejna~III and Sadigh(2023)]{hejna2023few}
Donald~Joseph Hejna~III and Dorsa Sadigh.
\newblock Few-shot preference learning for human-in-the-loop rl.
\newblock In \emph{Conference on Robot Learning}, pages 2014--2025. PMLR, 2023.

\bibitem[Ho and Salimans(2021)]{classifierfree}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock In \emph{NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qw8AKxfYbI}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{instructGPT}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Go et~al.(2023)Go, Korbak, Kruszewski, Rozen, Ryu, and Dymetman]{f-DPG}
Dongyoung Go, Tomasz Korbak, Germ{\'{a}}n Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman.
\newblock Aligning language models with preferences through f-divergence minimization.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 11546--11583. {PMLR}, 2023.

\bibitem[Yuan et~al.(2024)Yuan, Huang, Ni, Chen, and Wang]{yuan2024reward}
Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Minshuo Chen, and Mengdi Wang.
\newblock Reward-directed conditional diffusion: Provable distribution estimation and reward improvement.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Kim et~al.(2023)Kim, Park, Shin, Lee, Abbeel, and Lee]{PT}
Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee.
\newblock Preference transformer: Modeling human preferences using transformers for {RL}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Peot1SFDX0}.

\bibitem[Shin et~al.(2023)Shin, Dragan, and Brown]{bench_off_prefer}
Daniel Shin, Anca~D. Dragan, and Daniel~S. Brown.
\newblock Benchmarks and algorithms for offline preference-based reward learning.
\newblock \emph{Trans. Mach. Learn. Res.}, 2023, 2023.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Yu et~al.(2020)Yu, Quillen, He, Julian, Hausman, Finn, and Levine]{metaworld}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
\newblock In \emph{Conference on robot learning}, pages 1094--1100. PMLR, 2020.

\bibitem[Luo(2022)]{understand_dm}
Calvin Luo.
\newblock Understanding diffusion models: A unified perspective.
\newblock \emph{arXiv preprint arXiv:2208.11970}, 2022.

\bibitem[Oko et~al.(2023)Oko, Akiyama, and Suzuki]{distributionestimator}
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki.
\newblock Diffusion models are minimax optimal distribution estimators.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 26517--26582. {PMLR}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Schiff, Gokaslan, Pan, Wang, De~Sa, and Kuleshov]{infodiffusion}
Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De~Sa, and Volodymyr Kuleshov.
\newblock {I}nfo{D}iffusion: Representation learning using information maximizing diffusion models.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 36336--36354. PMLR, 23--29 Jul 2023{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v202/wang23ah.html}.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz, et~al.]{PbRLsurvey}
Christian Wirth, Riad Akrour, Gerhard Neumann, Johannes F{\"u}rnkranz, et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0 (136):\penalty0 1--46, 2017.

\bibitem[Bradley and Terry(1952)]{BTmodel}
Ralph~Allan Bradley and Milton~E Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Lee et~al.(2021)Lee, Smith, and Abbeel]{PEBBLE}
Kimin Lee, Laura~M. Smith, and Pieter Abbeel.
\newblock {PEBBLE:} feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training.
\newblock In \emph{{ICML}}, volume 139 of \emph{Proceedings of Machine Learning Research}, pages 6152--6163. {PMLR}, 2021.

\bibitem[Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos]{IPO}
Mohammad~Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R{\'e}mi Munos.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock \emph{arXiv preprint arXiv:2310.12036}, 2023.

\bibitem[Coste et~al.(2023)Coste, Anwar, Kirk, and Krueger]{reward_ensembles}
Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger.
\newblock Reward model ensembles help mitigate overoptimization.
\newblock \emph{arXiv preprint arXiv:2310.02743}, 2023.

\bibitem[Hejna et~al.(2023)Hejna, Rafailov, Sikchi, Finn, Niekum, Knox, and Sadigh]{contrastivepref}
Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W~Bradley Knox, and Dorsa Sadigh.
\newblock Contrastive prefence learning: Learning from human feedback without rl.
\newblock \emph{arXiv preprint arXiv:2310.13639}, 2023.

\bibitem[Zhao et~al.(2017)Zhao, Song, and Ermon]{infovae}
Shengjia Zhao, Jiaming Song, and Stefano Ermon.
\newblock Infovae: Information maximizing variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1706.02262}, 2017.

\bibitem[Chen et~al.(2016)Chen, Duan, Houthooft, Schulman, Sutskever, and Abbeel]{infogan}
Xi~Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.
\newblock Infogan: Interpretable representation learning by information maximizing generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Beaudry and Renner(2012)]{dataprocessinginequality}
Normand~J Beaudry and Renato Renner.
\newblock An intuitive proof of the data processing inequality.
\newblock \emph{Quantum Information \& Computation}, 12\penalty0 (5-6):\penalty0 432--441, 2012.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Hunt, and Zhou]{DQL}
Zhendong Wang, Jonathan~J Hunt, and Mingyuan Zhou.
\newblock Diffusion policies as an expressive policy class for offline reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=AHvFDPi-FA}.

\bibitem[Chi et~al.(2023)Chi, Feng, Du, Xu, Cousineau, Burchfiel, and Song]{diffusionpolicy}
Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.
\newblock Diffusion policy: Visuomotor policy learning via action diffusion.
\newblock In \emph{Robotics: Science and Systems}, 2023.

\bibitem[Hansen-Estruch et~al.(2023)Hansen-Estruch, Kostrikov, Janner, Kuba, and Levine]{IDQL}
Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub~Grudzien Kuba, and Sergey Levine.
\newblock Idql: Implicit q-learning as an actor-critic method with diffusion policies.
\newblock \emph{arXiv preprint arXiv:2304.10573}, 2023.

\bibitem[Chen et~al.(2023)Chen, Lu, Ying, Su, and Zhu]{SfBC}
Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu.
\newblock Offline reinforcement learning via high-fidelity generative behavior modeling.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=42zs3qa2kpy}.

\bibitem[Pearce et~al.(2023)Pearce, Rashid, Kanervisto, Bignell, Sun, Georgescu, Macua, Tan, Momennejad, Hofmann, and Devlin]{humanbehavior}
Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio~Valcarcel Macua, Shan~Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin.
\newblock Imitating human behaviour with diffusion models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Pv1GPQzRrC8}.

\bibitem[He et~al.(2024)He, Bai, Pan, Zhang, Zhao, and Li]{diffusion-3}
Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, and Xuelong Li.
\newblock Large-scale actionless video pre-training via discrete diffusion for efficient policy learning.
\newblock In \emph{Neural Information Processing Systems}, 2024.

\bibitem[Liang et~al.(2023)Liang, Mu, Ding, Ni, Tomizuka, and Luo]{adaptdiffuser}
Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo.
\newblock Adaptdiffuser: Diffusion models as adaptive self-evolving planners.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 20725--20745. {PMLR}, 2023.

\bibitem[Fan et~al.(2024)Fan, Bai, Shan, He, Zhang, and Wang]{diffusion-1}
Chenyou Fan, Chenjia Bai, Zhao Shan, Haoran He, Yang Zhang, and Zhen Wang.
\newblock Task-agnostic pre-training and task-guided fine-tuning for versatile diffusion planner.
\newblock \emph{arXiv preprint arXiv:2409.19949}, 2024.

\bibitem[Shan et~al.(2024)Shan, Fan, Qiu, Shi, and Bai]{diffusion-2}
Zhao Shan, Chenyou Fan, Shuang Qiu, Jiyuan Shi, and Chenjia Bai.
\newblock Forward kl regularized preference optimization for aligning diffusion policies.
\newblock \emph{arXiv preprint arXiv:2409.05622}, 2024.

\bibitem[Anonymous(2024)]{flow_to_better}
Anonymous.
\newblock Flow to better: Offline preference-based reinforcement learning via preferred trajectory generation.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=EG68RSznLT}.

\bibitem[Park et~al.(2022)Park, Seo, Shin, Lee, Abbeel, and Lee]{SURF}
Jongjin Park, Younggyo Seo, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee.
\newblock {SURF:} semi-supervised reward learning with data augmentation for feedback-efficient preference-based reinforcement learning.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Liu et~al.(2022)Liu, Bai, Du, and Yang]{MRN}
Runze Liu, Fengshuo Bai, Yali Du, and Yaodong Yang.
\newblock Meta-reward-net: Implicitly differentiable reward learning for preference-based reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 22270--22284, 2022.

\bibitem[Kang et~al.(2023)Kang, Shi, Liu, He, and Wang]{OPPO}
Yachen Kang, Diyuan Shi, Jinxin Liu, Li~He, and Donglin Wang.
\newblock Beyond reward: Offline preference-guided policy optimization.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 15753--15768. {PMLR}, 2023.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{DPO}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.18290}.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Yang, Shao, Liu, Yue, Ouyang, and Qiao]{MADPO}
Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu~Liu, Xiangyu Yue, Wanli Ouyang, and Yu~Qiao.
\newblock Beyond one-preference-for-all: Multi-objective direct preference optimization.
\newblock \emph{arXiv preprint arXiv:2310.03708}, 2023.

\bibitem[Song et~al.(2023)Song, Yu, Li, Yu, Huang, Li, and Wang]{pro}
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.
\newblock Preference ranking optimization for human alignment.
\newblock \emph{arXiv preprint arXiv:2306.17492}, 2023.

\bibitem[Mahapatra and Rajan(2020)]{multi-task-pref}
Debabrata Mahapatra and Vaibhav Rajan.
\newblock Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 6597--6607. PMLR, 2020.

\bibitem[Lin et~al.(2019)Lin, Zhen, Li, Zhang, and Kwong]{lin2019pareto}
Xi~Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong.
\newblock Pareto multi-task learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Birlutiu et~al.(2009)Birlutiu, Groot, and Heskes]{Birlutiu2009MultitaskPL}
Adriana Birlutiu, Perry Groot, and Tom~M. Heskes.
\newblock Multi-task preference learning with gaussian processes.
\newblock In \emph{The European Symposium on Artificial Neural Networks}, 2009.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:3892318}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{SAC}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages 1861--1870. PMLR, 2018.

\bibitem[Van~der Maaten and Hinton(2008)]{tsne}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Hu et~al.(2023)Hu, Shen, Zhang, and Tao]{prompt-dt}
Shengchao Hu, Li~Shen, Ya~Zhang, and Dacheng Tao.
\newblock Prompt-tuning decision transformer with preference ranking.
\newblock \emph{arXiv preprint arXiv:2305.09648}, 2023.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{DT}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 15084--15097, 2021.

\bibitem[Emmons et~al.(2022)Emmons, Eysenbach, Kostrikov, and Levine]{rvs}
Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine.
\newblock Rvs: What is essential for offline {RL} via supervised learning?
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=S874XAIpkR-}.

\bibitem[Vamplew et~al.(2022)Vamplew, Smith, K{\"a}llstr{\"o}m, Ramos, R{\u{a}}dulescu, Roijers, Hayes, Heintz, Mannion, Libin, et~al.]{scalar-reward-not-enough}
Peter Vamplew, Benjamin~J Smith, Johan K{\"a}llstr{\"o}m, Gabriel Ramos, Roxana R{\u{a}}dulescu, Diederik~M Roijers, Conor~F Hayes, Fredrik Heintz, Patrick Mannion, Pieter~JK Libin, et~al.
\newblock Scalar reward is not enough: A response to silver, singh, precup and sutton (2021).
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 36\penalty0 (2):\penalty0 41, 2022.

\bibitem[Liu et~al.(2023)Liu, Du, Bai, Lyu, and Li]{zero_shot_pref}
Runze Liu, Yali Du, Fengshuo Bai, Jiafei Lyu, and Xiu Li.
\newblock Zero-shot preference learning for offline rl via optimal transport.
\newblock \emph{arXiv preprint arXiv:2306.03615}, 2023.

\bibitem[Song et~al.(2020)Song, Meng, and Ermon]{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lu et~al.(2022)Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5775--5787, 2022.

\bibitem[Kang et~al.(2024)Kang, Ma, Du, Pang, and Yan]{kang2024efficient}
Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan.
\newblock Efficient diffusion policies for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Tai et~al.(2023)Tai, Zhou, Trajcevski, and Zhong]{tai2023revisiting}
Wenxin Tai, Fan Zhou, Goce Trajcevski, and Ting Zhong.
\newblock Revisiting denoising diffusion probabilistic models for speech enhancement: Condition collapse, efficiency and refinement.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 13627--13635, 2023.

\bibitem[Zhong et~al.(2024)Zhong, Ma, Zhang, Yang, Zhang, Qi, and Yang]{zhong2024panacea}
Yifan Zhong, Chengdong Ma, Xiaoyuan Zhang, Ziran Yang, Qingfu Zhang, Siyuan Qi, and Yaodong Yang.
\newblock Panacea: Pareto alignment via preference adaptation for llms.
\newblock \emph{arXiv preprint arXiv:2402.02030}, 2024.

\bibitem[Chakraborty et~al.(2024)Chakraborty, Qiu, Yuan, Koppel, Huang, Manocha, Bedi, and Wang]{chakraborty2024maxmin}
Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit~Singh Bedi, and Mengdi Wang.
\newblock Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences.
\newblock \emph{arXiv preprint arXiv:2402.08925}, 2024.

\bibitem[Zeng et~al.(2024)Zeng, Dai, Cheng, Wang, Hu, Chen, Du, and Xu]{zeng2024diversified}
Dun Zeng, Yong Dai, Pengyu Cheng, Longyue Wang, Tianhao Hu, Wanshun Chen, Nan Du, and Zenglin Xu.
\newblock On diversified preferences of large language model alignment, 2024.

\bibitem[Yang et~al.(2024)Yang, Pan, Luo, Qiu, Zhong, Yu, and Chen]{yang2024rewards}
Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen.
\newblock Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment.
\newblock \emph{arXiv preprint arXiv:2402.10207}, 2024.

\bibitem[Guo et~al.(2024)Guo, Cui, Yuan, Ding, Wang, Chen, Sun, Xie, Zhou, Lin, et~al.]{guo2024controllable}
Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, et~al.
\newblock Controllable preference optimization: Toward controllable multi-objective alignment.
\newblock \emph{arXiv preprint arXiv:2402.19085}, 2024.

\bibitem[Yang et~al.(2023)Yang, Tao, Lyu, Ge, Chen, Li, Shen, Zhu, and Li]{yang2023using}
Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li.
\newblock Using human feedback to fine-tune diffusion models without any reward model.
\newblock \emph{arXiv preprint arXiv:2311.13231}, 2023.

\end{thebibliography}
