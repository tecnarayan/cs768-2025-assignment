\begin{thebibliography}{10}

\bibitem{allen2020towards}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Towards understanding ensemble, knowledge distillation and self-distillation in deep learning.
\newblock {\em arXiv preprint arXiv:2012.09816}, 2020.

\bibitem{asai-etal-2023-retrieval}
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen.
\newblock Retrieval-based language models and applications.
\newblock In Yun-Nung~(Vivian) Chen, Margot Margot, and Siva Reddy, editors, {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)}, pages 41--46, Toronto, Canada, July 2023. Association for Computational Linguistics.

\bibitem{asai2023selfrag}
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.
\newblock Self-rag: Learning to retrieve, generate, and critique through self-reflection, 2023.

\bibitem{asai2024reliable}
Akari Asai, Zexuan Zhong, Danqi Chen, Pang~Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen tau Yih.
\newblock Reliable, adaptable, and attributable language models with retrieval, 2024.

\bibitem{bajaj2018ms}
Payal Bajaj, Daniel Campos, Nick Craswell, Li~Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang.
\newblock Ms marco: A human generated machine reading comprehension dataset, 2018.

\bibitem{beauchemin2024quebec}
David Beauchemin, Zachary Gagnon, and Ricahrd Khoury.
\newblock Quebec automobile insurance question-answering with retrieval-augmented generation.
\newblock {\em arXiv preprint arXiv:2410.09623}, 2024.

\bibitem{berant2013semantic}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in natural language processing}, pages 1533--1544, 2013.

\bibitem{borgeaud2022improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van~den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack~W. Rae, Erich Elsen, and Laurent Sifre.
\newblock Improving language models by retrieving from trillions of tokens, 2022.

\bibitem{chen2017reading}
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
\newblock Reading wikipedia to answer open-domain questions, 2017.

\bibitem{chen2021dialogsum}
Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang.
\newblock Dialogsum: A real-life scenario dialogue summarization dataset, 2021.

\bibitem{cheng2022neural}
Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan.
\newblock Neural machine translation with contrastive translation memories, 2022.

\bibitem{cheng2023decouple}
Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan.
\newblock Decouple knowledge from parameters for plug-and-play language modeling, 2023.

\bibitem{cheng2023lift}
Xin Cheng, Di~Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan.
\newblock Lift yourself up: Retrieval-augmented text generation with self memory, 2023.

\bibitem{chevalier2023adapting}
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.
\newblock Adapting language models to compress contexts, 2023.

\bibitem{dai2023instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.

\bibitem{dua2019drop}
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
\newblock Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019.

\bibitem{feng2023synergistic}
Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, and Daxin Jiang.
\newblock Synergistic interplay between search and large language models for information retrieval, 2023.

\bibitem{gao2024retrievalaugmented}
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi~Dai, Jiawei Sun, Meng Wang, and Haofen Wang.
\newblock Retrieval-augmented generation for large language models: A survey, 2024.

\bibitem{ge2023incontext}
Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei.
\newblock In-context autoencoder for context compression in a large language model, 2023.

\bibitem{Gliwa_2019}
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.
\newblock Samsum corpus: A human-annotated dialogue dataset for abstractive summarization.
\newblock In {\em Proceedings of the 2nd Workshop on New Frontiers in Summarization}. Association for Computational Linguistics, 2019.

\bibitem{guu2020realm}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training, 2020.

\bibitem{hussein2017imitation}
Ahmed Hussein, Mohamed~Medhat Gaber, Eyad Elyan, and Chrisina Jayne.
\newblock Imitation learning: A survey of learning methods.
\newblock {\em ACM Computing Surveys (CSUR)}, 50(2):1--35, 2017.

\bibitem{ivison2023camels}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A. Smith, Iz~Beltagy, and Hannaneh Hajishirzi.
\newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.

\bibitem{izacard2021leveraging}
Gautier Izacard and Edouard Grave.
\newblock Leveraging passage retrieval with generative models for open domain question answering, 2021.

\bibitem{izacard2022atlas}
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.
\newblock Atlas: Few-shot learning with retrieval augmented language models, 2022.

\bibitem{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem{jiang2024mixtral}
Albert~Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio~Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven~Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mixtral of experts, 2024.

\bibitem{jiang2023llmlingua}
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock Llmlingua: Compressing prompts for accelerated inference of large language models, 2023.

\bibitem{jiang2023longllmlingua}
Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression, 2023.

\bibitem{jiang-etal-2019-freebaseqa}
Kelvin Jiang, Dekun Wu, and Hui Jiang.
\newblock {F}reebase{QA}: A new factoid {QA} data set matching trivia-style question-answer pairs with {F}reebase.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 318--323, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{jin-etal-2019-pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.
\newblock {P}ub{M}ed{QA}: A dataset for biomedical research question answering.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2567--2577, Hong Kong, China, November 2019. Association for Computational Linguistics.

\bibitem{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S. Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension, 2017.

\bibitem{karamcheti2024prismatic}
Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh.
\newblock Prismatic vlms: Investigating the design space of visually-conditioned language models, 2024.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih.
\newblock Dense passage retrieval for open-domain question answering, 2020.

\bibitem{khandelwal2021nearest}
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock Nearest neighbor machine translation, 2021.

\bibitem{khandelwal2020generalization}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock Generalization through memorization: Nearest neighbor language models, 2020.

\bibitem{khattab2023demonstratesearchpredict}
Omar Khattab, Keshav Santhanam, Xiang~Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia.
\newblock Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp, 2023.

\bibitem{kim2023factkg}
Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, and Edward Choi.
\newblock Factkg: Fact verification via reasoning on knowledge graphs.
\newblock {\em arXiv preprint arXiv:2305.06590}, 2023.

\bibitem{kočiský2017narrativeqa}
Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl~Moritz Hermann, Gábor Melis, and Edward Grefenstette.
\newblock The narrativeqa reading comprehension challenge, 2017.

\bibitem{kwiatkowski-etal-2019-natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: A benchmark for question answering research.
\newblock {\em Transactions of the Association for Computational Linguistics}, 7, 2019.

\bibitem{lewis2021retrievalaugmented}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

\bibitem{li2024synthetic}
Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li~Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu Wei.
\newblock Synthetic data (almost) from scratch: Generalized instruction tuning for language models, 2024.

\bibitem{li2023sentence}
Haoran Li, Mingshi Xu, and Yangqiu Song.
\newblock Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence, 2023.

\bibitem{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.

\bibitem{li-etal-2023-compressing}
Yucheng Li, Bo~Dong, Frank Guerin, and Chenghua Lin.
\newblock Compressing context to enhance inference efficiency of large language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 6342--6353, Singapore, December 2023. Association for Computational Linguistics.

\bibitem{li2024promptcompressionlargelanguage}
Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier.
\newblock Prompt compression for large language models: A survey, 2024.

\bibitem{li2024500xcompressorgeneralizedpromptcompression}
Zongqian Li, Yixuan Su, and Nigel Collier.
\newblock 500xcompressor: Generalized prompt compression for large language models, 2024.

\bibitem{lin2023train}
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen tau Yih, and Xilun Chen.
\newblock How to train your dragon: Diverse augmentation towards generalizable dense retrieval, 2023.

\bibitem{lin2022truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods, 2022.

\bibitem{lin2023radit}
Xi~Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih.
\newblock Ra-dit: Retrieval-augmented dual instruction tuning, 2023.

\bibitem{liu2024visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{liu2024leveragingpassageembeddingsefficient}
Qi~Liu, Bo~Wang, Nan Wang, and Jiaxin Mao.
\newblock Leveraging passage embeddings for efficient listwise reranking with large language models, 2024.

\bibitem{lu2024deepseekvl}
Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan.
\newblock Deepseek-vl: Towards real-world vision-language understanding, 2024.

\bibitem{luo2023sail}
Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass.
\newblock Sail: Search-augmented instruction learning, 2023.

\bibitem{luo2024empirical}
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.
\newblock An empirical study of catastrophic forgetting in large language models during continual fine-tuning, 2024.

\bibitem{min2023nonparametric}
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Nonparametric masked language modeling, 2023.

\bibitem{morris2023text}
John~X. Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander~M. Rush.
\newblock Text embeddings reveal (almost) as much as text, 2023.

\bibitem{mu2024learning}
Jesse Mu, Xiang~Lisa Li, and Noah Goodman.
\newblock Learning to compress prompts with gist tokens, 2024.

\bibitem{muennighoff2023mteb}
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers.
\newblock Mteb: Massive text embedding benchmark, 2023.

\bibitem{nguyen2024sfr}
Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, and Shafiq Joty.
\newblock Sfr-rag: Towards contextually faithful llms.
\newblock {\em arXiv preprint arXiv:2409.09916}, 2024.

\bibitem{oh2018self}
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.
\newblock Self-imitation learning.
\newblock In {\em International conference on machine learning}, pages 3878--3887. PMLR, 2018.

\bibitem{pan2024llmlingua2datadistillationefficient}
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H.~Vicky Zhao, Lili Qiu, and Dongmei Zhang.
\newblock Llmlingua-2: Data distillation for efficient and faithful task-agnostic prompt compression, 2024.

\bibitem{petroni2021kilt}
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola~De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel.
\newblock Kilt: a benchmark for knowledge intensive language tasks, 2021.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language supervision, 2021.

\bibitem{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for squad, 2018.

\bibitem{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock Coqa: A conversational question answering challenge, 2019.

\bibitem{rogers2020getting}
Anna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky.
\newblock Getting closer to ai complete question answering: A set of prerequisite real tasks.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 8722--8731, 2020.

\bibitem{santhanam2022colbertv2}
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia.
\newblock Colbertv2: Effective and efficient retrieval via lightweight late interaction, 2022.

\bibitem{see2017point}
Abigail See, Peter~J. Liu, and Christopher~D. Manning.
\newblock Get to the point: Summarization with pointer-generator networks, 2017.

\bibitem{shi2023replug}
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih.
\newblock Replug: Retrieval-augmented black-box language models, 2023.

\bibitem{snell2022learning}
Charlie Snell, Dan Klein, and Ruiqi Zhong.
\newblock Learning by distilling context, 2022.

\bibitem{talmor2019commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge, 2019.

\bibitem{wang2024text}
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.
\newblock Text embeddings by weakly-supervised contrastive pre-training, 2024.

\bibitem{wang2022training}
Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and Michael Zeng.
\newblock Training data is more valuable than you think: A simple and effective method by retrieving from training data, 2022.

\bibitem{wang2023far}
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi~Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah~A. Smith, Iz~Beltagy, and Hannaneh Hajishirzi.
\newblock How far can camels go? exploring the state of instruction tuning on open resources, 2023.

\bibitem{wang2023learning}
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md~Rizwan Parvez, and Graham Neubig.
\newblock Learning to filter context for retrieval-augmented generation, 2023.

\bibitem{wei2022finetuned}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners, 2022.

\bibitem{xiao2023cpack}
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
\newblock C-pack: Packaged resources to advance general chinese embedding, 2023.

\bibitem{xu2023recomp}
Fangyuan Xu, Weijia Shi, and Eunsol Choi.
\newblock Recomp: Improving retrieval-augmented lms with compression and selective augmentation, 2023.

\bibitem{yang-etal-2015-wikiqa}
Yi~Yang, Wen-tau Yih, and Christopher Meek.
\newblock {W}iki{QA}: A challenge dataset for open-domain question answering.
\newblock In Llu{\'\i}s M{\`a}rquez, Chris Callison-Burch, and Jian Su, editors, {\em Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, pages 2013--2018, Lisbon, Portugal, September 2015. Association for Computational Linguistics.

\bibitem{yang2018hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W. Cohen, Ruslan Salakhutdinov, and Christopher~D. Manning.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.

\bibitem{yoon2024compactcompressingretrieveddocuments}
Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang.
\newblock Compact: Compressing retrieved documents actively for question answering, 2024.

\bibitem{yoran2023making}
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant.
\newblock Making retrieval-augmented language models robust to irrelevant context, 2023.

\bibitem{yu2023generate}
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang.
\newblock Generate rather than retrieve: Large language models are strong context generators, 2023.

\bibitem{yun-etal-2023-focus}
Jungmin Yun, Mihyeon Kim, and Youngbin Kim.
\newblock Focus on the core: Efficient attention via pruned token compression for document classification.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 13617--13628, Singapore, December 2023. Association for Computational Linguistics.

\bibitem{zhao2024funnelrag}
Xinping Zhao, Yan Zhong, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li, Baotian Hu, and Min Zhang.
\newblock Funnelrag: A coarse-to-fine progressive retrieval paradigm for rag.
\newblock {\em arXiv preprint arXiv:2410.10293}, 2024.

\bibitem{zhong2022training}
Zexuan Zhong, Tao Lei, and Danqi Chen.
\newblock Training language models with memory augmentation, 2022.

\end{thebibliography}
