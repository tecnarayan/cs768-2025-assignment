\begin{thebibliography}{10}

\bibitem{ahmed2011maximizing}
Shabbir Ahmed and Alper Atamt{\"u}rk.
\newblock Maximizing a class of submodular utility functions.
\newblock {\em Mathematical programming}, 128(1-2):149--169, 2011.

\bibitem{anderson2020tightened}
Brendon~G Anderson, Ziye Ma, Jingqi Li, and Somayeh Sojoudi.
\newblock Tightened convex relaxations for neural network robustness
  certification.
\newblock {\em arXiv preprint arXiv:2004.00570}, 2020.

\bibitem{anderson2020strong}
Ross Anderson, Joey Huchette, Will Ma, Christian Tjandraatmadja, and Juan~Pablo
  Vielma.
\newblock Strong mixed-integer programming formulations for trained neural
  networks.
\newblock {\em Mathematical Programming}, pages 1--37, 2020.

\bibitem{Strong-mixed-integer-programming-formulations-for-trained-CONF}
Ross Anderson, Joey Huchette, Christian Tjandraatmadja, and Juan~Pablo Vielma.
\newblock {Strong mixed-integer programming formulations for trained neural
  networks}.
\newblock In A.~Lodi and V.~Nagarajan, editors, {\em Proceedings of the 20th
  Conference on Integer Programming and Combinatorial Optimization ({IPCO
  2019})}, volume 11480 of {\em Lecture Notes in Computer Science}, pages
  27--42, 2019.

\bibitem{anthony2001discrete}
Martin Anthony.
\newblock {\em Discrete mathematics of neural networks: selected topics},
  volume~8.
\newblock SIAM, 2001.

\bibitem{bach2013learning}
Francis Bach.
\newblock Learning with submodular functions: A convex optimization
  perspective.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  6(2-3):145--373, 2013.

\bibitem{Bertsimas:1997}
Dimitris Bertsimas and John Tsitsiklis.
\newblock {\em Introduction to Linear Optimization}.
\newblock Athena Scientific, 1997.

\bibitem{bhojanapalli2020efficient}
Srinadh Bhojanapalli, Rudy Bunel, Krishnamurthy Dvijotham, and Oliver Hinder.
\newblock An efficient nonconvex reformulation of stagewise convex optimization
  problems.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{botoevaefficient}
Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth
  Misener.
\newblock Efficient verification of {ReLU}-based neural networks via dependency
  analysis.
\newblock In {\em Thirty-Fourth AAAI Conference on Artificial Intelligence},
  2020.

\bibitem{bunel2020branch}
Rudy Bunel, Jingyue Lu, Ilker Turkaslan, P~Kohli, P~Torr, and P~Mudigonda.
\newblock Branch and bound for piecewise linear neural network verification.
\newblock {\em Journal of Machine Learning Research}, 21(2020), 2020.

\bibitem{Carlini:2016}
Nicholas {Carlini} and David {Wagner}.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em 2017 IEEE Symposium on Security and Privacy (SP)}, pages
  39--57, 2017.

\bibitem{cheng2017maximum}
Chih-Hong Cheng, Georg N{\"u}hrenberg, and Harald Ruess.
\newblock Maximum resilience of artificial neural networks.
\newblock In {\em International Symposium on Automated Technology for
  Verification and Analysis}, pages 251--268. Springer, 2017.

\bibitem{conforti2014integer}
Michele Conforti, G{\'e}rard Cornu{\'e}jols, and Giacomo Zambelli.
\newblock {\em Integer programming}, volume 271.
\newblock Springer, 2014.

\bibitem{dutta2018output}
Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari.
\newblock Output range analysis for deep feedforward neural networks.
\newblock In {\em NASA Formal Methods Symposium}, pages 121--138. Springer,
  2018.

\bibitem{dvijotham2018dual}
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy~A Mann, and
  Pushmeet Kohli.
\newblock A dual approach to scalable verification of deep networks.
\newblock In {\em UAI}, volume~1, page~2, 2018.

\bibitem{ehlers2017formal}
Ruediger Ehlers.
\newblock Formal verification of piece-wise linear feed-forward neural
  networks.
\newblock In {\em International Symposium on Automated Technology for
  Verification and Analysis}, pages 269--286. Springer, 2017.

\bibitem{fischetti2017deep}
Matteo Fischetti and Jason Jo.
\newblock Deep neural networks as 0-1 mixed integer linear programs: A
  feasibility study.
\newblock {\em Constraints}, 23:296--309, 2018.

\bibitem{grotschel2012geometric}
Martin Gr{\"o}tschel, L{\'a}szl{\'o} Lov{\'a}sz, and Alexander Schrijver.
\newblock {\em Geometric algorithms and combinatorial optimization}, volume~2.
\newblock Springer Science \& Business Media, 2012.

\bibitem{huang2017safety}
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu.
\newblock Safety verification of deep neural networks.
\newblock In {\em International Conference on Computer Aided Verification},
  pages 3--29. Springer, 2017.

\bibitem{Katz:2017}
Guy Katz, Clark Barrett, David~L. Dill, Kyle Julian, and Mykel~J. Kochenderfer.
\newblock Reluplex: {A}n efficient {SMT} solver for verifying deep neural
  networks.
\newblock In {\em International Conference on Computer Aided Verification},
  pages 97--117, 2017.

\bibitem{Korte:2000}
Bernhard Korte and Jens Vygen.
\newblock {\em Combinatorial Optimization: Theory and Algorithms}.
\newblock Springer, 2000.

\bibitem{liu2019algorithms}
Changliu Liu, Tomer Arnon, Christopher Lazarus, Clark Barrett, and Mykel~J
  Kochenderfer.
\newblock Algorithms for verifying deep neural networks.
\newblock {\em arXiv preprint arXiv:1903.06758}, 2019.

\bibitem{liu2019training}
Chen Liu, Mathieu Salzmann, and Sabine S{\"u}sstrunk.
\newblock Training provably robust models by polyhedral envelope
  regularization.
\newblock {\em arXiv}, pages arXiv--1912, 2019.

\bibitem{lomuscio2017approach}
Alessio Lomuscio and Lalit Maganti.
\newblock An approach to reachability analysis for feed-forward {ReLU} neural
  networks.
\newblock {\em arXiv preprint arXiv:1706.07351}, 2017.

\bibitem{lu2020neural}
Jingyue Lu and M.~Pawan~Kumar.
\newblock Neural network branching for neural network verification.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{lyu2019fastened}
Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, and Luca
  Daniel.
\newblock Fastened {CROWN}: Tightened neural network robustness certificates.
\newblock {\em arXiv preprint arXiv:1912.00574}, 2019.

\bibitem{narodytska2018verifying}
Nina Narodytska, Shiva Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv, and Toby
  Walsh.
\newblock Verifying properties of binarized deep neural networks.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{o1971hyperplane}
Patrick~E O'Neil.
\newblock Hyperplane cuts of an n-cube.
\newblock {\em Discrete Mathematics}, 1(2):193--195, 1971.

\bibitem{Papernot:2016}
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z.~Berkay
  Celik, and Ananthram Swami.
\newblock The limitations of deep learning in adversarial settings.
\newblock In {\em IEEE European Symposium on Security and Privacy}, pages
  372--387, March 2016.

\bibitem{raghunathan2018semidefinite}
Aditi Raghunathan, Jacob Steinhardt, and Percy~S Liang.
\newblock Semidefinite relaxations for certifying robustness to adversarial
  examples.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10877--10887, 2018.

\bibitem{Roessig2019}
Ansgar R{\"o}ssig.
\newblock Verification of neural networks.
\newblock Technical Report 19-40, ZIB, Takustr. 7, 14195 Berlin, 2019.

\bibitem{salman2019convex}
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang.
\newblock A convex relaxation barrier to tight robustness verification of
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9832--9842, 2019.

\bibitem{scheibler2015towards}
Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker.
\newblock Towards verification of artificial neural networks.
\newblock In {\em MBMV}, pages 30--40, 2015.

\bibitem{singh2019beyond}
Gagandeep Singh, Rupanshu Ganvir, Markus P{\"u}schel, and Martin Vechev.
\newblock Beyond the single neuron convex barrier for neural network
  certification.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  15072--15083, 2019.

\bibitem{singh2018fast}
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P{\"u}schel, and Martin
  Vechev.
\newblock Fast and effective robustness certification.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10802--10813, 2018.

\bibitem{singh2019abstract}
Gagandeep Singh, Timon Gehr, Markus P{\"u}schel, and Martin Vechev.
\newblock An abstract domain for certifying neural networks.
\newblock {\em Proceedings of the ACM on Programming Languages}, 3(POPL):1--30,
  2019.

\bibitem{singh2019boosting}
Gagandeep Singh, Timon Gehr, Markus P{\"u}schel, and Martin Vechev.
\newblock Boosting robustness certification of neural networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{eran_benchmark}
Gagandeep Singh, Jonathan Maurer, Christoph Müller, Matthew Mirman, Timon
  Gehr, Adrian Hoffmann, Petar Tsankov, Dana Drachsler~Cohen, Markus Püschel,
  and Martin Vechev.
\newblock {ERAN} verification dataset.
\newblock \url{https://github.com/eth-sri/eran}.

\bibitem{Szegedy:2013}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In {\em International Conference on Learning Representations}, 2014.

\bibitem{tawarmalani2013explicit}
Mohit Tawarmalani, Jean-Philippe~P Richard, and Chuanhui Xiong.
\newblock Explicit convex and concave envelopes through polyhedral
  subdivisions.
\newblock {\em Mathematical Programming}, 138(1-2):531--577, 2013.

\bibitem{Tjeng:2017}
Vincent Tjeng, Kai Xiao, and Russ Tedrake.
\newblock Verifying neural networks with mixed integer programming.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{todd1976computation}
M.J. Todd.
\newblock {\em The Computation of Fixed Points and Applications}.
\newblock Lecture Notes in Mathematics; 513. Springer-Verlag, 1976.

\bibitem{wang2018efficient}
Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana.
\newblock Efficient formal safety analysis of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6367--6377, 2018.

\bibitem{weng2018towards}
Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel,
  Duane Boning, and Inderjit Dhillon.
\newblock Towards fast computation of certified robustness for {R}e{LU}
  networks.
\newblock In Jennifer Dy and Andreas Krause, editors, {\em Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of {\em
  Proceedings of Machine Learning Research}, pages 5276--5285,
  Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem{wong2018provable}
Eric Wong and J.~Zico Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In {\em International Conference on Machine Learning}, pages
  5286--5295, 2018.

\bibitem{Wong:2018}
Eric Wong, Frank Schmidt, Jan~Hendrik Metzen, and J.~Zico Kolter.
\newblock Scaling provable adversarial defenses.
\newblock In {\em 32nd Conference on Neural Information Processing Systems},
  2018.

\bibitem{xiang2018output}
Weiming Xiang, Hoang-Dung Tran, and Taylor~T Johnson.
\newblock Output reachable set estimation and verification for multilayer
  neural networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  29(11):5777--5783, 2018.

\bibitem{Xiao:2018}
Kai~Y. Xiao, Vincent Tjeng, Nur~Muhammad Shafiullah, and Aleksander Madry.
\newblock Training for faster adversarial robustness verification via inducing
  {ReLU} stability.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{zhang2018efficient}
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In {\em Advances in neural information processing systems}, pages
  4939--4948, 2018.

\bibitem{zhu2020improving}
Chen Zhu, Renkun Ni, Ping-yeh Chiang, Hengduo Li, Furong Huang, and Tom
  Goldstein.
\newblock Improving the tightness of convex relaxation bounds for training
  certifiably robust classifiers.
\newblock {\em arXiv preprint arXiv:2002.09766}, 2020.

\end{thebibliography}
