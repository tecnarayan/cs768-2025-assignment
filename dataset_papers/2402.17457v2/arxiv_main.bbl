\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwala et~al.(2022)Agarwala, Pedregosa, and Pennington]{agarwala2022second}
A.~Agarwala, F.~Pedregosa, and J.~Pennington.
\newblock Second-order regression models exhibit progressive sharpening to the edge of stability.
\newblock \emph{arXiv preprint arXiv:2210.04860}, 2022.

\bibitem[Ahn et~al.(2022)Ahn, Zhang, and Sra]{ahn2022understanding}
K.~Ahn, J.~Zhang, and S.~Sra.
\newblock Understanding the unstable convergence of gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 247--257. PMLR, 2022.

\bibitem[Amari(1997)]{amari1997information}
S.~Amari.
\newblock Information geometry.
\newblock \emph{Contemporary Mathematics}, 203:\penalty0 81--96, 1997.

\bibitem[Amari(1998)]{amari1998natural}
S.-I. Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and Wang]{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Arora et~al.(2022)Arora, Li, and Panigrahi]{arora2022understanding}
S.~Arora, Z.~Li, and A.~Panigrahi.
\newblock Understanding gradient descent on the edge of stability in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages 948--1024. PMLR, 2022.

\bibitem[Bergstra and Bengio(2012)]{bergstra2012random}
J.~Bergstra and Y.~Bengio.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of machine learning research}, 13\penalty0 (2), 2012.

\bibitem[Bordelon and Pehlevan(2022)]{bordelon2022self}
B.~Bordelon and C.~Pehlevan.
\newblock Self-consistent dynamical field theory of kernel evolution in wide neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 32240--32256, 2022.

\bibitem[Bordelon and Pehlevan(2023)]{bordelon2023dynamics}
B.~Bordelon and C.~Pehlevan.
\newblock Dynamics of finite width kernel and prediction fluctuations in mean field neural networks.
\newblock \emph{arXiv preprint arXiv:2304.03408}, 2023.

\bibitem[Bordelon et~al.(2023)Bordelon, Noci, Li, Hanin, and Pehlevan]{bordelon2023depthwise}
B.~Bordelon, L.~Noci, M.~B. Li, B.~Hanin, and C.~Pehlevan.
\newblock Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit.
\newblock \emph{arXiv preprint arXiv:2309.16620}, 2023.

\bibitem[Bordelon et~al.(2024)Bordelon, Chaudhry, and Pehlevan]{bordelon2024infinite}
B.~Bordelon, H.~T. Chaudhry, and C.~Pehlevan.
\newblock Infinite limits of multi-head transformer dynamics.
\newblock \emph{arXiv preprint arXiv:2405.15712}, 2024.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{botev2017practical}
A.~Botev, H.~Ritter, and D.~Barber.
\newblock Practical gauss-newton optimisation for deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages 557--565. PMLR, 2017.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
L.~Chizat and F.~Bach.
\newblock On the global convergence of gradient descent for over-parameterized models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
L.~Chizat and F.~Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR, 2020.

\bibitem[Chizat and Netrapalli(2023)]{chizat2023steering}
L.~Chizat and P.~Netrapalli.
\newblock Steering deep feature learning with backward aligned feature updates.
\newblock \emph{arXiv preprint arXiv:2311.18718}, 2023.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
L.~Chizat, E.~Oyallon, and F.~Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Claesen and Moor(2015)]{claesen2015hyperparameter}
M.~Claesen and B.~D. Moor.
\newblock Hyperparameter search in machine learning, 2015.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{cohen2021gradient}
J.~M. Cohen, S.~Kaur, Y.~Li, J.~Z. Kolter, and A.~Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2103.00065}, 2021.

\bibitem[Cohen et~al.(2022)Cohen, Ghorbani, Krishnan, Agarwal, Medapati, Badura, Suo, Cardoze, Nado, Dahl, et~al.]{cohen2022adaptive}
J.~M. Cohen, B.~Ghorbani, S.~Krishnan, N.~Agarwal, S.~Medapati, M.~Badura, D.~Suo, D.~Cardoze, Z.~Nado, G.~E. Dahl, et~al.
\newblock Adaptive gradient methods at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2207.14484}, 2022.

\bibitem[Courant(1920)]{courant1920eigenwerte}
R.~Courant.
\newblock {\"U}ber die eigenwerte bei den differentialgleichungen der mathematischen physik.
\newblock \emph{Mathematische Zeitschrift}, 7\penalty0 (1):\penalty0 1--57, 1920.

\bibitem[Damian et~al.(2022)Damian, Nichani, and Lee]{damian2022self}
A.~Damian, E.~Nichani, and J.~D. Lee.
\newblock Self-stabilization: The implicit bias of gradient descent at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2209.15594}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and Pontil]{franceschi2017forward}
L.~Franceschi, M.~Donini, P.~Frasconi, and M.~Pontil.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 1165--1173. PMLR, 2017.

\bibitem[Garriga-Alonso et~al.(2018)Garriga-Alonso, Rasmussen, and Aitchison]{garriga2018deep}
A.~Garriga-Alonso, C.~E. Rasmussen, and L.~Aitchison.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock \emph{arXiv preprint arXiv:1808.05587}, 2018.

\bibitem[Garrigos and Gower(2023)]{garrigos2023handbook}
G.~Garrigos and R.~M. Gower.
\newblock Handbook of convergence theorems for (stochastic) gradient methods.
\newblock \emph{arXiv preprint arXiv:2301.11235}, 2023.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and Xiao]{ghorbani2019investigation}
B.~Ghorbani, S.~Krishnan, and Y.~Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue density.
\newblock In \emph{International Conference on Machine Learning}, pages 2232--2241. PMLR, 2019.

\bibitem[Gilmer et~al.(2021)Gilmer, Ghorbani, Garg, Kudugunta, Neyshabur, Cardoze, Dahl, Nado, and Firat]{gilmer2021loss}
J.~Gilmer, B.~Ghorbani, A.~Garg, S.~Kudugunta, B.~Neyshabur, D.~Cardoze, G.~E. Dahl, Z.~Nado, and O.~Firat.
\newblock A loss curvature perspective on training instabilities of deep learning models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{Goodfellow-et-al-2016}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and Richt{\'a}rik]{gower2019sgd}
R.~M. Gower, N.~Loizou, X.~Qian, A.~Sailanbayev, E.~Shulgin, and P.~Richt{\'a}rik.
\newblock Sgd: General analysis and improved rates.
\newblock In \emph{International conference on machine learning}, pages 5200--5209. PMLR, 2019.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur2018gradient}
G.~Gur-Ari, D.~A. Roberts, and E.~Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[Hanin and Nica(2019)]{hanin2019finite}
B.~Hanin and M.~Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock \emph{arXiv preprint arXiv:1909.05989}, 2019.

\bibitem[Hanin and Nica(2020)]{hanin2020products}
B.~Hanin and M.~Nica.
\newblock Products of many large random matrices and gradients in deep neural networks.
\newblock \emph{Communications in Mathematical Physics}, 376\penalty0 (1):\penalty0 287--322, 2020.

\bibitem[Hayou(2022)]{hayou2022infinite}
S.~Hayou.
\newblock On the infinite-depth limit of finite-width neural networks.
\newblock \emph{arXiv preprint arXiv:2210.00688}, 2022.

\bibitem[Hayou and Yang(2023)]{hayou2023width}
S.~Hayou and G.~Yang.
\newblock Width and depth limits commute in residual networks.
\newblock \emph{arXiv preprint arXiv:2302.00453}, 2023.

\bibitem[Hayou et~al.(2021)Hayou, Clerico, He, Deligiannidis, Doucet, and Rousseau]{hayou2021stable}
S.~Hayou, E.~Clerico, B.~He, G.~Deligiannidis, A.~Doucet, and J.~Rousseau.
\newblock Stable resnet.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1324--1332. PMLR, 2021.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 1026--1034, 2015.

\bibitem[Horv{\'a}th et~al.(2021)Horv{\'a}th, Klein, Richt{\'a}rik, and Archambeau]{horvath2021hyperparameter}
S.~Horv{\'a}th, A.~Klein, P.~Richt{\'a}rik, and C.~Archambeau.
\newblock Hyperparameter transfer learning with adaptive complexity.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1378--1386. PMLR, 2021.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl-Dickstein, and Novak]{hron2020infinite}
J.~Hron, Y.~Bahri, J.~Sohl-Dickstein, and R.~Novak.
\newblock Infinite attention: Nngp and ntk for deep attention networks.
\newblock In \emph{International Conference on Machine Learning}, pages 4376--4386. PMLR, 2020.

\bibitem[Iyer et~al.(2023)Iyer, Hanin, and Rolnick]{iyer2023maximal}
G.~Iyer, B.~Hanin, and D.~Rolnick.
\newblock Maximal initial learning rates in deep relu networks.
\newblock In \emph{International Conference on Machine Learning}, pages 14500--14530. PMLR, 2023.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jamieson and Talwalkar(2016)]{jamieson2016non}
K.~Jamieson and A.~Talwalkar.
\newblock Non-stochastic best arm identification and hyperparameter optimization.
\newblock In \emph{Artificial intelligence and statistics}, pages 240--248. PMLR, 2016.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor, Cho, and Geras]{jastrzebski2020break}
S.~Jastrzebski, M.~Szymczak, S.~Fort, D.~Arpit, J.~Tabor, K.~Cho, and K.~Geras.
\newblock The break-even point on optimization trajectories of deep neural networks.
\newblock \emph{arXiv preprint arXiv:2002.09572}, 2020.

\bibitem[Jastrz{k{e}}bski et~al.(2018)Jastrz{k{e}}bski, Kenton, Ballas, Fischer, Bengio, and Storkey]{jastrzkebski2018relation}
S.~Jastrz{k{e}}bski, Z.~Kenton, N.~Ballas, A.~Fischer, Y.~Bengio, and A.~Storkey.
\newblock On the relation between the sharpest directions of dnn loss and the sgd step length.
\newblock \emph{arXiv preprint arXiv:1807.05031}, 2018.

\bibitem[Jelassi et~al.(2023)Jelassi, Hanin, Ji, Reddi, Bhojanapalli, and Kumar]{jelassi2023depth}
S.~Jelassi, B.~Hanin, Z.~Ji, S.~J. Reddi, S.~Bhojanapalli, and S.~Kumar.
\newblock Depth dependence of mup learning rates in relu mlps.
\newblock \emph{arXiv preprint arXiv:2305.07810}, 2023.

\bibitem[Kalra and Barkeshli(2023)]{kalra2023phase}
D.~S. Kalra and M.~Barkeshli.
\newblock Phase diagram of early training dynamics in deep neural networks: effect of the learning rate, depth, and width.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Kalra et~al.(2023)Kalra, He, and Barkeshli]{kalra2023universal}
D.~S. Kalra, T.~He, and M.~Barkeshli.
\newblock Universal sharpness dynamics in neural network training: Fixed point analysis, edge of stability, and route to chaos.
\newblock \emph{arXiv preprint arXiv:2311.02076}, 2023.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[LeCun et~al.(2002)LeCun, Bottou, Orr, and M{\"u}ller]{lecun2002efficient}
Y.~LeCun, L.~Bottou, G.~B. Orr, and K.-R. M{\"u}ller.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 9--50. Springer, 2002.

\bibitem[Lee et~al.(2017)Lee, Bahri, Novak, Schoenholz, Pennington, and Sohl-Dickstein]{lee2017deep}
J.~Lee, Y.~Bahri, R.~Novak, S.~S. Schoenholz, J.~Pennington, and J.~Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{arXiv preprint arXiv:1711.00165}, 2017.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein, and Pennington]{lee2019wide}
J.~Lee, L.~Xiao, S.~Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and J.~Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and Gur-Ari]{lewkowycz2020large}
A.~Lewkowycz, Y.~Bahri, E.~Dyer, J.~Sohl-Dickstein, and G.~Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult mechanism.
\newblock \emph{arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[Li et~al.(2018)Li, Jamieson, DeSalvo, Rostamizadeh, and Talwalkar]{li2018hyperband}
L.~Li, K.~Jamieson, G.~DeSalvo, A.~Rostamizadeh, and A.~Talwalkar.
\newblock Hyperband: A novel bandit-based approach to hyperparameter optimization.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0 (185):\penalty0 1--52, 2018.

\bibitem[Li et~al.(2021)Li, Nica, and Roy]{li2021future}
M.~Li, M.~Nica, and D.~Roy.
\newblock The future is log-gaussian: Resnets and their infinite-depth-and-width limit at initialization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 7852--7864, 2021.

\bibitem[Li et~al.(2022)Li, Nica, and Roy]{li2022neural}
M.~Li, M.~Nica, and D.~Roy.
\newblock The neural covariance sde: Shaped infinite depth-and-width networks at initialization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 10795--10808, 2022.

\bibitem[Li and Nica(2023)]{li2023differential}
M.~B. Li and M.~Nica.
\newblock Differential equation scaling limits of shaped and unshaped neural networks.
\newblock \emph{arXiv preprint arXiv:2310.12079}, 2023.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019towards}
Y.~Li, C.~Wei, and T.~Ma.
\newblock Towards explaining the regularization effect of initial large learning rate in training neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Loshchilov(2017)]{loshchilov2017decoupled}
I.~Loshchilov.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and Adams]{maclaurin2015gradient}
D.~Maclaurin, D.~Duvenaud, and R.~Adams.
\newblock Gradient-based hyperparameter optimization through reversible learning.
\newblock In \emph{International conference on machine learning}, pages 2113--2122. PMLR, 2015.

\bibitem[Martens(2016)]{martens2016second}
J.~Martens.
\newblock \emph{Second-order optimization for neural networks}.
\newblock University of Toronto (Canada), 2016.

\bibitem[Martens(2020)]{martens2020new}
J.~Martens.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5776--5851, 2020.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate curvature.
\newblock In \emph{International conference on machine learning}, pages 2408--2417. PMLR, 2015.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pages 2388--2464. PMLR, 2019.

\bibitem[Neal(1995)]{neal1995bayesian}
R.~M. Neal.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course}, volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Noci et~al.(2021)Noci, Bachmann, Roth, Nowozin, and Hofmann]{noci2021precise}
L.~Noci, G.~Bachmann, K.~Roth, S.~Nowozin, and T.~Hofmann.
\newblock Precise characterization of the prior predictive distribution of deep relu networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20851--20862, 2021.

\bibitem[Noci et~al.(2022)Noci, Anagnostidis, Biggio, Orvieto, Singh, and Lucchi]{noci2022signal}
L.~Noci, S.~Anagnostidis, L.~Biggio, A.~Orvieto, S.~P. Singh, and A.~Lucchi.
\newblock Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27198--27211, 2022.

\bibitem[Noci et~al.(2023)Noci, Li, Li, He, Hofmann, Maddison, and Roy]{noci2023shaped}
L.~Noci, C.~Li, M.~B. Li, B.~He, T.~Hofmann, C.~Maddison, and D.~M. Roy.
\newblock The shaped transformer: Attention models in the infinite depth-and-width limit.
\newblock \emph{arXiv preprint arXiv:2306.17759}, 2023.

\bibitem[Orvieto et~al.(2021)Orvieto, Kohler, Pavllo, Hofmann, and Lucchi]{orvieto2021vanishing}
A.~Orvieto, J.~Kohler, D.~Pavllo, T.~Hofmann, and A.~Lucchi.
\newblock Vanishing curvature and the power of adaptive methods in randomly initialized deep networks.
\newblock \emph{arXiv preprint arXiv:2106.03763}, 2021.

\bibitem[Osawa et~al.(2023)Osawa, Ishikawa, Yokota, Li, and Hoefler]{osawa2023asdl}
K.~Osawa, S.~Ishikawa, R.~Yokota, S.~Li, and T.~Hoefler.
\newblock Asdl: A unified interface for gradient preconditioning in pytorch.
\newblock \emph{arXiv preprint arXiv:2305.04684}, 2023.

\bibitem[Ott(2002)]{ott2002chaos}
E.~Ott.
\newblock \emph{Chaos in dynamical systems}.
\newblock Cambridge university press, 2002.

\bibitem[Parlett(1998)]{parlett1998symmetric}
B.~N. Parlett.
\newblock \emph{The symmetric eigenvalue problem}.
\newblock SIAM, 1998.

\bibitem[Perrone et~al.(2018)Perrone, Jenatton, Seeger, and Archambeau]{perrone2018scalable}
V.~Perrone, R.~Jenatton, M.~W. Seeger, and C.~Archambeau.
\newblock Scalable hyperparameter transfer learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Roulet et~al.(2023)Roulet, Agarwala, and Pedregosa]{roulet2023interplay}
V.~Roulet, A.~Agarwala, and F.~Pedregosa.
\newblock On the interplay between stepsize tuning and progressive sharpening.
\newblock In \emph{OPT 2023: Optimization for Machine Learning}, 2023.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016eigenvalues}
L.~Sagun, L.~Bottou, and Y.~LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock \emph{arXiv preprint arXiv:1611.07476}, 2016.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and Bottou]{sagun2017empirical}
L.~Sagun, U.~Evci, V.~U. Guney, Y.~Dauphin, and L.~Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and Boots]{shaban2019truncated}
A.~Shaban, C.-A. Cheng, N.~Hatch, and B.~Boots.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pages 1723--1732. PMLR, 2019.

\bibitem[Singh et~al.(2021)Singh, Bachmann, and Hofmann]{singh2021analytic}
S.~P. Singh, G.~Bachmann, and T.~Hofmann.
\newblock Analytic insights into structure and rank of neural network hessian maps.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23914--23927, 2021.

\bibitem[Smith and Topin(2019)]{smith2019super}
L.~N. Smith and N.~Topin.
\newblock Super-convergence: Very fast training of neural networks using large learning rates.
\newblock In \emph{Artificial intelligence and machine learning for multi-domain operations applications}, volume 11006, pages 369--386. SPIE, 2019.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
J.~Snoek, H.~Larochelle, and R.~P. Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram, Patwary, Prabhat, and Adams]{snoek2015scalable}
J.~Snoek, O.~Rippel, K.~Swersky, R.~Kiros, N.~Satish, N.~Sundaram, M.~Patwary, M.~Prabhat, and R.~Adams.
\newblock Scalable bayesian optimization using deep neural networks.
\newblock In \emph{International conference on machine learning}, pages 2171--2180. PMLR, 2015.

\bibitem[Song and Yun(2023)]{song2023trajectory}
M.~Song and C.~Yun.
\newblock Trajectory alignment: understanding the edge of stability phenomenon via bifurcation theory.
\newblock \emph{arXiv preprint arXiv:2307.04204}, 2023.

\bibitem[Stoll et~al.(2020)Stoll, Franke, Wagner, Selg, and Hutter]{stoll2020hyperparameter}
D.~Stoll, J.~K. Franke, D.~Wagner, S.~Selg, and F.~Hutter.
\newblock Hyperparameter transfer across developer adjustments.
\newblock \emph{arXiv preprint arXiv:2010.13117}, 2020.

\bibitem[Vyas et~al.(2023)Vyas, Atanasov, Bordelon, Morwani, Sainathan, and Pehlevan]{vyas2023feature}
N.~Vyas, A.~Atanasov, B.~Bordelon, D.~Morwani, S.~Sainathan, and C.~Pehlevan.
\newblock Feature-learning networks are consistent across widths at realistic scales.
\newblock \emph{arXiv preprint arXiv:2305.18411}, 2023.

\bibitem[Yaida(2020)]{yaida2020non}
S.~Yaida.
\newblock Non-gaussian processes and neural networks at finite widths.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pages 165--192. PMLR, 2020.

\bibitem[Yaida(2022)]{yaida2022meta}
S.~Yaida.
\newblock Meta-principled family of hyperparameter scaling strategies.
\newblock \emph{arXiv preprint arXiv:2210.04909}, 2022.

\bibitem[Yang(2020)]{yang2020tensor}
G.~Yang.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock \emph{arXiv preprint arXiv:2006.14548}, 2020.

\bibitem[Yang and Hu(2021)]{yang2021tensor}
G.~Yang and E.~J. Hu.
\newblock Tensor programs iv: Feature learning in infinite-width neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 11727--11737. PMLR, 2021.

\bibitem[Yang and Littwin(2023)]{yang2023tensor2}
G.~Yang and E.~Littwin.
\newblock Tensor programs ivb: Adaptive optimization in the infinite-width limit.
\newblock \emph{arXiv preprint arXiv:2308.01814}, 2023.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
G.~Yang, E.~J. Hu, I.~Babuschkin, S.~Sidor, X.~Liu, D.~Farhi, N.~Ryder, J.~Pachocki, W.~Chen, and J.~Gao.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Yang et~al.(2023)Yang, Yu, Zhu, and Hayou]{yang2023tensor}
G.~Yang, D.~Yu, C.~Zhu, and S.~Hayou.
\newblock Tensor programs vi: Feature learning in infinite-depth neural networks.
\newblock \emph{arXiv preprint arXiv:2310.02244}, 2023.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Z.~Yao, A.~Gholami, K.~Keutzer, and M.~W. Mahoney.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE international conference on big data (Big data)}, pages 581--590. IEEE, 2020.

\bibitem[Yogatama and Mann(2014)]{yogatama2014efficient}
D.~Yogatama and G.~Mann.
\newblock Efficient transfer learning method for automatic hyperparameter tuning.
\newblock In \emph{Artificial intelligence and statistics}, pages 1077--1085. PMLR, 2014.

\bibitem[Zavatone-Veth et~al.(2021)Zavatone-Veth, Canatar, Ruben, and Pehlevan]{zavatone2021asymptotics}
J.~Zavatone-Veth, A.~Canatar, B.~Ruben, and C.~Pehlevan.
\newblock Asymptotics of representation learning in finite bayesian neural networks.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 24765--24777, 2021.

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, Du, Yang, Chen, Chen, Jiang, Ren, Li, Tang, Liu, Liu, Nie, and Wen]{zhao2023llmsurvey}
W.~X. Zhao, K.~Zhou, J.~Li, T.~Tang, X.~Wang, Y.~Hou, Y.~Min, B.~Zhang, J.~Zhang, Z.~Dong, Y.~Du, C.~Yang, Y.~Chen, Z.~Chen, J.~Jiang, R.~Ren, Y.~Li, X.~Tang, Z.~Liu, P.~Liu, J.-Y. Nie, and J.-R. Wen.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.
\newblock URL \url{http://arxiv.org/abs/2303.18223}.

\bibitem[Zhu et~al.(2022)Zhu, Wang, Wang, Zhou, and Ge]{zhu2022understanding}
X.~Zhu, Z.~Wang, X.~Wang, M.~Zhou, and R.~Ge.
\newblock Understanding edge-of-stability training dynamics with a minimalist example.
\newblock \emph{arXiv preprint arXiv:2210.03294}, 2022.

\end{thebibliography}
