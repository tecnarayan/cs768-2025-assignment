This paper tackles the problem of selecting among several linear estimators
in non-parametric regression; this includes model selection for linear
regression, the choice of a regularization parameter in kernel ridge
regression, spline smoothing or locally weighted regression, and the choice of
a kernel in multiple kernel learning. We propose a new algorithm which first
estimates consistently the variance of the noise, based upon the concept of
minimal penalty, which was previously introduced in the context of model
selection. Then, plugging our variance estimate in Mallows' $C_L$ penalty is
proved to lead to an algorithm satisfying an oracle inequality. Simulation
experiments with kernel ridge regression and multiple kernel learning show that
the proposed algorithm often improves significantly existing calibration
procedures such as generalized cross-validation.