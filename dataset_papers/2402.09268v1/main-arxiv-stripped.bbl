\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2014)Agarwal, Chapelle, Dud{\'\i}k, and
  Langford]{agarwal2014reliable}
Alekh Agarwal, Olivier Chapelle, Miroslav Dud{\'\i}k, and John Langford.
\newblock A reliable effective terascale linear learning system.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1111--1133, 2014.

\bibitem[Andoni et~al.(2014)Andoni, Nikolov, Onak, and
  Yaroslavtsev]{andoni2014parallel}
Alexandr Andoni, Aleksandar Nikolov, Krzysztof Onak, and Grigory Yaroslavtsev.
\newblock Parallel algorithms for geometric graph problems.
\newblock In \emph{Proceedings of the forty-sixth annual ACM symposium on
  Theory of computing}, pages 574--583, 2014.

\bibitem[Andoni et~al.(2018)Andoni, Song, Stein, Wang, and Zhong]{asswz18}
Alexandr Andoni, Zhao Song, Clifford Stein, Zhengyu Wang, and Peilin Zhong.
\newblock Parallel graph connectivity in log diameter rounds.
\newblock In \emph{2018 IEEE 59th Annual Symposium on Foundations of Computer
  Science (FOCS)}. IEEE, October 2018.
\newblock \doi{10.1109/focs.2018.00070}.
\newblock URL \url{http://dx.doi.org/10.1109/FOCS.2018.00070}.

\bibitem[Angluin et~al.(2023)Angluin, Chiang, and Yang]{acy23}
Dana Angluin, David Chiang, and Andy Yang.
\newblock Masked hard-attention transformers and boolean rasp recognize exactly
  the star-free languages, 2023.

\bibitem[Assadi and N(2021)]{an21}
Sepehr Assadi and Vishvajeet N.
\newblock Graph streaming lower bounds for parameter estimation and property
  testing via a streaming xor lemma.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on
  Theory of Computing}, STOC ’21. ACM, June 2021.
\newblock \doi{10.1145/3406325.3451110}.
\newblock URL \url{http://dx.doi.org/10.1145/3406325.3451110}.

\bibitem[Beame et~al.(2017)Beame, Koutris, and Suciu]{beame2017communication}
Paul Beame, Paraschos Koutris, and Dan Suciu.
\newblock Communication steps for parallel query processing.
\newblock \emph{Journal of the ACM (JACM)}, 64\penalty0 (6):\penalty0 1--58,
  2017.

\bibitem[Behnezhad et~al.(2019)Behnezhad, Brandt, Derakhshan, Fischer,
  Hajiaghayi, Karp, and Uitto]{behnezhad2019massively}
Soheil Behnezhad, Sebastian Brandt, Mahsa Derakhshan, Manuela Fischer,
  MohammadTaghi Hajiaghayi, Richard~M Karp, and Jara Uitto.
\newblock Massively parallel computation of matching and mis in sparse graphs.
\newblock In \emph{Proceedings of the 2019 ACM Symposium on Principles of
  Distributed Computing}, pages 481--490, 2019.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{bpc20}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer, 2020.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{279181}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE Transactions on Neural Networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.
\newblock \doi{10.1109/72.279181}.

\bibitem[Bhattamishra et~al.(2020)Bhattamishra, Ahuja, and Goyal]{bag20}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the ability and limitations of transformers to recognize formal
  languages.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing}, 2020.

\bibitem[Bietti et~al.(2023)Bietti, Cabannes, Bouchacourt, Jegou, and
  Bottou]{bcbjg23}
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon
  Bottou.
\newblock Birth of a transformer: A memory viewpoint, 2023.

\bibitem[Charikar et~al.(2020)Charikar, Ma, and Tan]{cmt20}
Moses Charikar, Weiyun Ma, and Li-Yang Tan.
\newblock New lower bounds for massively parallel computation from query
  complexity, 2020.

\bibitem[Choromanski et~al.(2022)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{cld+22}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller.
\newblock Rethinking attention with performers, 2022.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and
  Bengio]{chung2014empirical}
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Clark et~al.(2019)Clark, Khandelwal, Levy, and Manning]{clark2019does}
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher~D Manning.
\newblock What does bert look at? an analysis of bert's attention.
\newblock \emph{arXiv preprint arXiv:1906.04341}, 2019.

\bibitem[Coy and Czumaj(2022)]{cc22}
Sam Coy and Artur Czumaj.
\newblock Deterministic massively parallel connectivity.
\newblock In \emph{Proceedings of the 54th Annual ACM SIGACT Symposium on
  Theory of Computing}, STOC 2022, page 162–175, New York, NY, USA, 2022.
  Association for Computing Machinery.
\newblock ISBN 9781450392648.
\newblock \doi{10.1145/3519935.3520055}.
\newblock URL \url{https://doi.org/10.1145/3519935.3520055}.

\bibitem[Daniely(2017)]{daniely17}
Amit Daniely.
\newblock Depth separation for neural networks.
\newblock In Satyen Kale and Ohad Shamir, editors, \emph{Proceedings of the
  2017 Conference on Learning Theory}, volume~65 of \emph{Proceedings of
  Machine Learning Research}, pages 690--696. PMLR, 07--10 Jul 2017.
\newblock URL \url{https://proceedings.mlr.press/v65/daniely17a.html}.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and Ré]{flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Dean and Ghemawat(2004)]{dean2004mapreduce}
Jeffrey Dean and Sanjay Ghemawat.
\newblock Mapreduce: Simplified data processing on large clusters.
\newblock In \emph{OSDI}, pages 137--150, 2004.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, 2022.

\bibitem[Duris et~al.(1984)Duris, Galil, and Schnitger]{duris1984lower}
Pavol Duris, Zvi Galil, and Georg Schnitger.
\newblock Lower bounds on communication complexity.
\newblock In \emph{Proceedings of the Sixteenth Annual ACM Symposium on Theory
  of Computing}, page 81–91, 1984.

\bibitem[Eldan and Shamir(2016)]{es16}
Ronen Eldan and Ohad Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors,
  \emph{29th Annual Conference on Learning Theory}, volume~49 of
  \emph{Proceedings of Machine Learning Research}, pages 907--940, Columbia
  University, New York, New York, USA, 23--26 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v49/eldan16.html}.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds,
  Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{eno21}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Ghaffari et~al.(2019)Ghaffari, Kuhn, and Uitto]{gku19}
Mohsen Ghaffari, Fabian Kuhn, and Jara Uitto.
\newblock Conditional hardness results for massively parallel computation from
  distributed lower bounds.
\newblock In \emph{IEEE 60th Annual Symposium on Foundations of Computer
  Science}, pages 1650--1663, 11 2019.
\newblock \doi{10.1109/FOCS.2019.00097}.

\bibitem[Goodrich et~al.(2011)Goodrich, Sitchinava, and
  Zhang]{goodrich2011sorting}
Michael~T Goodrich, Nodari Sitchinava, and Qin Zhang.
\newblock Sorting, searching, and simulation in the mapreduce framework.
\newblock In \emph{International Symposium on Algorithms and Computation},
  pages 374--383. Springer, 2011.

\bibitem[Gu and Dao(2023)]{gd23}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces,
  2023.

\bibitem[Guha and McGregor(2009)]{gm09}
Sudipto Guha and Andrew McGregor.
\newblock Stream order and order statistics: Quantile estimation in
  random-order streams.
\newblock \emph{SIAM Journal on Computing}, 38\penalty0 (5):\penalty0
  2044--2059, 2009.
\newblock \doi{10.1137/07069328X}.
\newblock URL \url{https://doi.org/10.1137/07069328X}.

\bibitem[Hahn(2020)]{hahn20}
Michael Hahn.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 8:\penalty0 156--171, 2020.
\newblock \doi{10.1162/tacl\_{a}{\_{0}{0}{3}}{0}6}.
\newblock URL \url{https://doi.org/10.1162/tacl_a_00306}.

\bibitem[Hao et~al.(2022)Hao, Angluin, and Frank]{haf22}
Yiding Hao, Dana Angluin, and Robert Frank.
\newblock Formal language recognition by hard attention transformers:
  Perspectives from circuit complexity.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 10:\penalty0 800--810,
  2022.
\newblock URL \url{https://transacl.org/ojs/index.php/tacl/article/view/3765}.

\bibitem[Im et~al.(2023)Im, Kumar, Lattanzi, Moseley, Vassilvitskii,
  et~al.]{im2023massively}
Sungjin Im, Ravi Kumar, Silvio Lattanzi, Benjamin Moseley, Sergei
  Vassilvitskii, et~al.
\newblock Massively parallel computation: Algorithms and applications.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  5\penalty0 (4):\penalty0 340--417, 2023.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko,
  et~al.]{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Kacham et~al.(2023)Kacham, Mirrokni, and Zhong]{kmz23}
Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong.
\newblock Polysketchformer: Fast transformers via sketches for polynomial
  kernels, 2023.

\bibitem[Karloff et~al.(2010)Karloff, Suri, and Vassilvitskii]{ksv10}
Howard Karloff, Siddharth Suri, and Sergei Vassilvitskii.
\newblock A model of computation for mapreduce.
\newblock In \emph{Twenty-first Annual ACM-SIAM Symposium on Discrete
  Algorithms}, pages 938--948, 12 2010.
\newblock \doi{10.1137/1.9781611973075.76}.

\bibitem[Kim et~al.(2022)Kim, Nguyen, Min, Cho, Lee, Lee, and Hong]{jts22}
Jinwoo Kim, Tien~Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak
  Lee, and Seunghoon Hong.
\newblock Pure transformers are powerful graph learners, 2022.

\bibitem[Kingma and Ba(2014)]{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2014.

\bibitem[Li and McClelland(2022)]{lm22}
Yuxuan Li and James~L. McClelland.
\newblock Systematic generalization and emergent structures in transformers
  trained on structured tasks, 2022.

\bibitem[Likhosherstov et~al.(2021)Likhosherstov, Choromanski, and
  Weller]{lcw21}
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller.
\newblock On the expressive power of self-attention matrices.
\newblock \emph{arXiv preprint arXiv:2106.03764}, 2021.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{lagkz22}
Bingbin Liu, Jordan~T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata, 2022.

\bibitem[Loukas(2019)]{loukas19}
Andreas Loukas.
\newblock What graph neural networks cannot learn: depth vs width.
\newblock \emph{arXiv preprint arXiv:1907.03199}, 2019.

\bibitem[Malach(2023)]{malach23}
Eran Malach.
\newblock Auto-regressive next-token predictors are universal learners, 2023.

\bibitem[Merrill and Sabharwal(2022)]{ms22-log-prec}
William Merrill and Ashish Sabharwal.
\newblock A logic for expressing log-precision transformers, 2022.

\bibitem[Merrill and Sabharwal(2023{\natexlab{a}})]{ms22-parallelism}
William Merrill and Ashish Sabharwal.
\newblock The parallelism tradeoff: Limitations of log-precision transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  11:\penalty0 531–545, 2023{\natexlab{a}}.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00562}.
\newblock URL \url{http://dx.doi.org/10.1162/tacl_a_00562}.

\bibitem[Merrill and Sabharwal(2023{\natexlab{b}})]{ms23-cot}
William Merrill and Ashish Sabharwal.
\newblock The expressive power of transformers with chain of thought,
  2023{\natexlab{b}}.

\bibitem[Merrill et~al.(2022)Merrill, Sabharwal, and Smith]{mss22}
William Merrill, Ashish Sabharwal, and Noah~A. Smith.
\newblock Saturated transformers are constant-depth threshold circuits.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 843–856, 2022.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00493}.
\newblock URL \url{http://dx.doi.org/10.1162/tacl_a_00493}.

\bibitem[MPICH(2023)]{mpi-doc}
MPICH.
\newblock Mpi allreduce, 2023.
\newblock URL
  \url{https://www.mpich.org/static/docs/latest/www3/MPI_Allreduce.html}.

\bibitem[Nisan and Wigderson(1993)]{nw93}
Noam Nisan and Avi Wigderson.
\newblock Rounds in communication complexity revisited.
\newblock \emph{SIAM Journal on Computing}, 22\penalty0 (1):\penalty0 211--219,
  1993.
\newblock \doi{10.1137/0222016}.
\newblock URL \url{https://doi.org/10.1137/0222016}.

\bibitem[Oren et~al.(2024)Oren, Hassid, Adi, and Schwartz]{ohas24}
Matanel Oren, Michael Hassid, Yossi Adi, and Roy Schwartz.
\newblock Transformers are multi-state rnns, 2024.

\bibitem[Papadimitriou and Sipser(1982)]{papadimitriou1982communication}
Christos~H. Papadimitriou and Michael Sipser.
\newblock Communication complexity.
\newblock In \emph{Proceedings of the Fourteenth Annual ACM Symposium on Theory
  of Computing}, page 196–200, 1982.

\bibitem[P{\'e}rez et~al.(2021)P{\'e}rez, Barcel{\'o}, and
  Marinkovic]{perez2021attention}
Jorge P{\'e}rez, Pablo Barcel{\'o}, and Javier Marinkovic.
\newblock Attention is turing complete.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 3463--3497, 2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{Radford2019LanguageMA}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rogers et~al.(2021)Rogers, Kovaleva, and Rumshisky]{rogers2021primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer in bertology: What we know about how bert works.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 842--866, 2021.

\bibitem[Roughgarden et~al.(2018)Roughgarden, Vassilvitskii, and Wang]{rvw18}
Tim Roughgarden, Sergei Vassilvitskii, and Joshua Wang.
\newblock Shuffles and circuits (on lower bounds for modern parallel
  computation).
\newblock \emph{Journal of the ACM}, 65:\penalty0 1--24, 11 2018.
\newblock \doi{10.1145/3232536}.

\bibitem[Sanford et~al.(2023)Sanford, Hsu, and Telgarsky]{sht23}
Clayton Sanford, Daniel Hsu, and Matus Telgarsky.
\newblock Representational strengths and limitations of transformers, 2023.

\bibitem[Strobl(2023)]{strobl23}
Lena Strobl.
\newblock Average-hard attention transformers are constant-depth uniform
  threshold circuits, 2023.

\bibitem[Strobl et~al.(2023)Strobl, Merrill, Weiss, Chiang, and
  Angluin]{smwca23}
Lena Strobl, William Merrill, Gail Weiss, David Chiang, and Dana Angluin.
\newblock Transformers as recognizers of formal languages: A survey on
  expressivity, 2023.

\bibitem[Telgarsky(2016)]{telgarsky16}
Matus Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors,
  \emph{29th Annual Conference on Learning Theory}, volume~49 of
  \emph{Proceedings of Machine Learning Research}, pages 1517--1539, Columbia
  University, New York, New York, USA, 23--26 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v49/telgarsky16.html}.

\bibitem[Turkoglu et~al.(2021)Turkoglu, D'Aronco, Wegner, and
  Schindler]{turkoglu2021gating}
Mehmet~Ozgur Turkoglu, Stefano D'Aronco, Jan~Dirk Wegner, and Konrad Schindler.
\newblock Gating revisited: Deep multi-layer rnns that can be trained.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 44\penalty0 (8):\penalty0 4081--4092, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vsp+17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, 2017.

\bibitem[Wang et~al.(2022)Wang, Wang, Xu, Zhou, and Lu]{wang2022quantformer}
Ziwei Wang, Changyuan Wang, Xiuwei Xu, Jie Zhou, and Jiwen Lu.
\newblock Quantformer: Learning extremely low-precision vision transformers.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\bibitem[Wei et~al.(2021)Wei, Chen, and Ma]{wcm21}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers, 2021.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yppn21}
Shunyu Yao, Binghui Peng, Christos~H. Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing}, 2021.

\bibitem[Yun et~al.(2020)Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{ybrrk20}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang et~al.(2023)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and
  Wagner]{zbbegw23}
Yi~Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and
  Tal Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task, 2023.

\end{thebibliography}
