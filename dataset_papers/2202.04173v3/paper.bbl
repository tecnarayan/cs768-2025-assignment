\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{gpt2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{megatron}
Mohammad Shoeybi, Mostofa~Ali Patwary, Raul Puri, Patrick LeGresley, Jared
  Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{ArXiv}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{arXiv preprint arXiv:2101.03961}, 2021.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti, Zhang, Child, Aminabadi,
  Bernauer, Song, Shoeybi, He, Houston, Tiwary, and Catanzaro]{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, Elton Zhang, Rewon Child, Reza~Yazdani Aminabadi, Julie
  Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh
  Tiwary, and Bryan Catanzaro.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock \emph{arXiv}, 2022.

\bibitem[McGuffie and Newhouse(2020)]{mcguffie2020radicalization}
Kris McGuffie and Alex Newhouse.
\newblock The radicalization risks of {GPT}-3 and advanced neural language
  models.
\newblock \emph{arXiv}, 2020.

\bibitem[Wallace et~al.(2019)Wallace, Feng, Kandpal, Gardner, and
  Singh]{wallace2019universal}
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
\newblock Universal adversarial triggers for attacking and analyzing nlp.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Dathathri et~al.(2019)Dathathri, Madotto, Lan, Hung, Frank, Molino,
  Yosinski, and Liu]{dathathri2019plug}
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
  Molino, Jason Yosinski, and Rosanne Liu.
\newblock Plug and play language models: A simple approach to controlled text
  generation.
\newblock In \emph{ICLR}, 2019.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Real{T}oxicity{P}rompts: Evaluating neural toxic degeneration in
  language models.
\newblock In \emph{Findings in EMNLP}, 2020.

\bibitem[Schick et~al.(2021)Schick, Udupa, and Sch{\"u}tze]{selfdebiasing}
Timo Schick, Sahana Udupa, and Hinrich Sch{\"u}tze.
\newblock Self-diagnosis and self-debiasing: A proposal for reducing
  corpus-based bias in nlp.
\newblock \emph{TACL}, 2021.

\bibitem[Krause et~al.(2020)Krause, Gotmare, McCann, Keskar, Joty, Socher, and
  Rajani]{krause2020gedi}
Ben Krause, Akhilesh~Deepak Gotmare, Bryan McCann, Nitish~Shirish Keskar,
  Shafiq Joty, Richard Socher, and Nazneen~Fatema Rajani.
\newblock Ge{D}i: Generative discriminator guided sequence generation.
\newblock \emph{arXiv}, 2020.

\bibitem[Xu et~al.(2021)Xu, Pathak, Wallace, Gururangan, Sap, and
  Klein]{xu2021detoxifying}
Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan
  Klein.
\newblock Detoxifying language models risks marginalizing minority voices.
\newblock In \emph{NAACL}, 2021.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Sap, Lu, Swayamdipta, Bhagavatula,
  Smith, and Choi]{liu2021dexperts}
Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula,
  Noah~A Smith, and Yejin Choi.
\newblock D{E}xperts: Decoding-time controlled text generation with experts and
  anti-experts.
\newblock In \emph{ACL}, 2021{\natexlab{a}}.

\bibitem[Welbl et~al.(2021)Welbl, Glaese, Uesato, Dathathri, Mellor, Hendricks,
  Anderson, Kohli, Coppin, and Huang]{welbl2021challenges}
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
  Lisa~Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
  Huang.
\newblock Challenges in detoxifying language models.
\newblock \emph{Findings of EMNLP}, 2021.

\bibitem[Solaiman and Dennison(2021)]{solaiman2021process}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society ({PALMS}) with
  values-targeted datasets.
\newblock \emph{arXiv preprint arXiv:2106.10328}, 2021.

\bibitem[Gokaslan and Cohen(2019)]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  de~Laroussilhe, Gesmundo, Attariyan, and Gelly]{adapter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{ICML}, 2019.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-{T}uning: Optimizing continuous prompts for generation.
\newblock In \emph{ACL}, 2021.

\bibitem[Bengio et~al.(2015)Bengio, Vinyals, Jaitly, and
  Shazeer]{bengio2015scheduled}
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer.
\newblock Scheduled sampling for sequence prediction with recurrent neural
  networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Kim and Rush(2016)]{kim2016sequence}
Yoon Kim and Alexander~M Rush.
\newblock Sequence-level knowledge distillation.
\newblock In \emph{EMNLP}, 2016.

\bibitem[Wang et~al.(2021)Wang, Xu, Wang, Gan, Cheng, Gao, Awadallah, and
  Li]{wang2021adversarial}
Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu~Cheng, Jianfeng Gao,
  Ahmed~Hassan Awadallah, and Bo~Li.
\newblock Adversarial glue: A multi-task benchmark for robustness evaluation of
  language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Wang et~al.(2022)Wang, Xu, Liu, Cheng, and Li]{wang2022semattack}
Boxin Wang, Chejian Xu, Xiangyu Liu, Yu~Cheng, and Bo~Li.
\newblock {S}em{A}ttack: Natural textual attacks via different semantic spaces.
\newblock In \emph{Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, 2022.

\bibitem[Wang et~al.(2020)Wang, Pei, Pan, Chen, Wang, and Li]{wang2020t3}
Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, and Bo~Li.
\newblock T3: Tree-autoencoder regularized adversarial text generation for
  targeted attack.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 6134--6150, 2020.

\bibitem[Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss,
  Lee, Roberts, Brown, Song, Erlingsson, et~al.]{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
  Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pages
  2633--2650, 2021.

\bibitem[Lee et~al.(2022)Lee, Ping, Xu, Patwary, Shoeybi, and
  Catanzaro]{lee2022factuality}
Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Mohammad Shoeybi, and Bryan
  Catanzaro.
\newblock Factuality enhanced language models for open-ended text generation.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Zhao et~al.(2019)Zhao, Wang, Yatskar, Cotterell, Ordonez, and
  Chang]{zhao2019gender}
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and
  Kai-Wei Chang.
\newblock Gender bias in contextualized word embeddings.
\newblock In \emph{NAACL}, 2019.

\bibitem[May et~al.(2019)May, Wang, Bordia, Bowman, and
  Rudinger]{may2019measuring}
Chandler May, Alex Wang, Shikha Bordia, Samuel~R Bowman, and Rachel Rudinger.
\newblock On measuring social biases in sentence encoders.
\newblock In \emph{NAACL}, 2019.

\bibitem[Basta et~al.(2019)Basta, Costa-Juss{\`a}, and
  Casas]{basta2019evaluating}
Christine Basta, Marta~R Costa-Juss{\`a}, and Noe Casas.
\newblock Evaluating the underlying gender bias in contextualized word
  embeddings.
\newblock \emph{arXiv preprint arXiv:1904.08783}, 2019.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 2002.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo,
  Beltagy, Downey, and Smith]{gururangan2020don}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A Smith.
\newblock Don't stop pretraining: adapt language models to domains and tasks.
\newblock In \emph{ACL}, 2020.

\bibitem[Baheti et~al.(2021)Baheti, Sap, Ritter, and
  Riedl]{baheti-etal-2021-just}
Ashutosh Baheti, Maarten Sap, Alan Ritter, and Mark Riedl.
\newblock Just say no: Analyzing the stance of neural dialogue generation in
  offensive contexts.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 4846--4862, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.397}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.397}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese,
  McAleese, and Irving]{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John
  Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock \emph{arXiv preprint arXiv:2202.03286}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformers}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NIPS}, 2017.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding,
  Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and
  Zou]{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou.
\newblock A framework for few-shot language model evaluation, 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{ICLR}, 2019.

\bibitem[Dhamala et~al.(2021)Dhamala, Sun, Kumar, Krishna, Pruksachatkun,
  Chang, and Gupta]{dhamala2021bold}
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun,
  Kai-Wei Chang, and Rahul Gupta.
\newblock Bold: Dataset and metrics for measuring biases in open-ended language
  generation.
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 862--872, 2021.

\bibitem[Nie et~al.(2020)Nie, Williams, Dinan, Bansal, Weston, and Kiela]{anli}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
  Kiela.
\newblock Adversarial nli: A new benchmark for natural language understanding.
\newblock In \emph{ACL}, 2020.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova]{booq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock In \emph{NAACL}, 2019.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In \emph{ACL}, 2019.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock In \emph{NAACL}, 2016.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{AAAI}, 2020.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock In \emph{EMNLP}, 2017.

\bibitem[Pilehvar and Camacho-Collados(2019)]{wic}
Mohammad~Taher Pilehvar and Jose Camacho-Collados.
\newblock Wic: the word-in-context dataset for evaluating context-sensitive
  meaning representations.
\newblock In \emph{NAACL}, 2019.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Le~Bras, Bhagavatula, and
  Choi]{winogrande}
Keisuke Sakaguchi, Ronan Le~Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In \emph{AAAI}, 2020.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig]{survey}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{arXiv}, 2021{\natexlab{b}}.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and Singh]{prompt1}
Taylor Shin, Yasaman Razeghi, Robert~L Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock Autoprompt: Eliciting knowledge from language models with
  automatically generated prompts.
\newblock \emph{arXiv}, 2020.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{prompt3}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv}, 2021.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and Singh]{prompt4}
Tony~Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock \emph{arXiv}, 2021.

\bibitem[Schick and Schütze(2020{\natexlab{a}})]{prompt5}
Timo Schick and Hinrich Schütze.
\newblock Exploiting cloze questions for few-shot text classification and
  natural language inference.
\newblock \emph{arXiv}, 2020{\natexlab{a}}.

\bibitem[Schick and Schütze(2020{\natexlab{b}})]{prompt6}
Timo Schick and Hinrich Schütze.
\newblock It's not just size that matters: Small language models are also
  few-shot learners.
\newblock \emph{arXiv}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2016)Li, Galley, Brockett, Gao, and
  Dolan]{li-etal-2016-diversity}
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan.
\newblock A diversity-promoting objective function for neural conversation
  models.
\newblock In \emph{Proceedings of the 2016 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 110--119, San Diego, California, June 2016. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/N16-1014}.
\newblock URL \url{https://aclanthology.org/N16-1014}.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock \emph{arXiv preprint arXiv:2102.12092}, 2021.

\end{thebibliography}
