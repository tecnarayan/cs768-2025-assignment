

@article{rosset2003margin,
  title={Margin maximizing loss functions},
  author={Rosset, Saharon and Zhu, Ji and Hastie, Trevor},
  journal={Advances in neural information processing systems},
  volume={16},
  year={2003}
}
@article{suggala2018connecting,
  title={Connecting optimization and regularization paths},
  author={Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{ji2020gradient,
  title={Gradient descent follows the regularization path for general losses},
  author={Ji, Ziwei and Dud{\'\i}k, Miroslav and Schapire, Robert E and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={2109--2136},
  year={2020},
  organization={PMLR}
}

@article{tarzanagh2023transformers,
  title={Transformers as support vector machines},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv preprint arXiv:2308.16898},
  year={2023}
}

@article{krizhevsky2014cifar,
  title={The CIFAR-10 dataset},
  author={Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  journal={online: http://www. cs. toronto. edu/kriz/cifar. html},
  volume={55},
  number={5},
  year={2014}
}

@book{mangasarian1994nonlinear,
  title={Nonlinear programming},
  author={Mangasarian, Olvi L},
  year={1994},
  publisher={SIAM}
}

@inproceedings{vardi2021implicit,
  title={Implicit regularization in relu networks with the square loss},
  author={Vardi, Gal and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={4224--4258},
  year={2021},
  organization={PMLR}
}

@article{vardi2022margin,
  title={On margin maximization in linear and relu networks},
  author={Vardi, Gal and Shamir, Ohad and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37024--37036},
  year={2022}
}

@article{frei2023benign,
  title={Benign Overfitting in Linear Classifiers and Leaky ReLU Networks from KKT Conditions for Margin Maximization},
  author={Frei, Spencer and Vardi, Gal and Bartlett, Peter L and Srebro, Nathan},
  journal={arXiv e-prints},
  pages={arXiv--2303},
  year={2023}
}

@inproceedings{yin2022a,
  title={A-vit: Adaptive tokens for efficient vision transformer},
  author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10809--10818},
  year={2022}
}

@article{tachet2020mathematical,
  title={A mathematical theory of attention},
  author={Tachet des Combes, Remi and Vuckovic, James and Aristide, Baratin},
  journal={arXiv preprint arXiv:2007.02876},
  year={2020}
}

@inproceedings{kim2021lipschitz,
  title={The lipschitz constant of self-attention},
  author={Kim, Hyunjik and Papamakarios, George and Mnih, Andriy},
  booktitle={International Conference on Machine Learning},
  pages={5562--5571},
  year={2021},
  organization={PMLR}
}

@inproceedings{hron2020infinite,
  title={Infinite attention: Nngp and ntk for deep attention networks},
  author={Hron, Jiri and Bahri, Yasaman and Sohl-Dickstein, Jascha and Novak, Roman},
  booktitle={International Conference on Machine Learning},
  pages={4376--4386},
  year={2020},
  organization={PMLR}
}

@article{yang2020tensor,
  title={Tensor programs ii: Neural tangent kernel for any architecture},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:2006.14548},
  year={2020}
}

@inproceedings{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{bhattamishra2020ability,
  title={On the ability and limitations of transformers to recognize formal languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  journal={arXiv preprint arXiv:2009.11264},
  year={2020}
}

@article{bhattamishra2020computational,
  title={On the computational power of transformers and its implications in sequence modeling},
  author={Bhattamishra, Satwik and Patel, Arkil and Goyal, Navin},
  journal={arXiv preprint arXiv:2006.09286},
  year={2020}
}

@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}

@inproceedings{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021},
  organization={PMLR}
}

@article{likhosherstov2021expressive,
  title={On the expressive power of self-attention matrices},
  author={Likhosherstov, Valerii and Choromanski, Krzysztof and Weller, Adrian},
  journal={arXiv preprint arXiv:2106.03764},
  year={2021}
}

@inproceedings{cordonnier2019relationship,
  title={On the relationship between self-attention and convolutional layers},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{levine2020limits,
  title={Limits to depth efficiencies of self-attention},
  author={Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22640--22651},
  year={2020}
}

@article{snell2021approximating,
  title={Approximating how single head attention learns},
  author={Snell, Charlie and Zhong, Ruiqi and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.07601},
  year={2021}
}

@article{wei2021statistically,
  title={Statistically meaningful approximation: a case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={arXiv preprint arXiv:2107.13163},
  year={2021}
}

@inproceedings{nguyen2023primal,
  title={A primal-dual framework for transformers and neural networks},
  author={Nguyen, Tan Minh and Nguyen, Tam Minh and Ho, Nhat and Bertozzi, Andrea L and Baraniuk, Richard and Osher, Stanley},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}


@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{jiang2021all,
  title={All tokens matter: Token labeling for training better vision transformers},
  author={Jiang, Zi-Hang and Hou, Qibin and Yuan, Li and Zhou, Daquan and Shi, Yujun and Jin, Xiaojie and Wang, Anran and Feng, Jiashi},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18590--18602},
  year={2021}
}

@inproceedings{wang2021pyramid,
  title={Pyramid vision transformer: A versatile backbone for dense prediction without convolutions},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={568--578},
  year={2021}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{pan2021scalable,
  title={Scalable vision transformers with hierarchical pooling},
  author={Pan, Zizheng and Zhuang, Bohan and Liu, Jing and He, Haoyu and Cai, Jianfei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={377--386},
  year={2021}
}

@inproceedings{rao2021dynamicvit,
  title={Dynamicvit: Efficient vision transformers with dynamic token sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13937--13949},
  year={2021}
}

@inproceedings{liang2022not,
  title={Not all patches are what you need: Expediting vision transformers via token reorganizations},
  author={Liang, Youwei and Chongjian, GE and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{tang2022patch,
  title={Patch slimming for efficient vision transformers},
  author={Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12165--12174},
  year={2022}
}

@inproceedings{zhou2018deep,
  title={Deep interest network for click-through rate prediction},
  author={Zhou, Guorui and Zhu, Xiaoqiang and Song, Chenru and Fan, Ying and Zhu, Han and Ma, Xiao and Yan, Yanghui and Jin, Junqi and Li, Han and Gai, Kun},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1059--1068},
  year={2018}
}

@inproceedings{chen2019behavior,
  title={Behavior sequence transformer for e-commerce recommendation in alibaba},
  author={Chen, Qiwei and Zhao, Huan and Li, Wei and Huang, Pipei and Ou, Wenwu},
  booktitle={Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data},
  pages={1--4},
  year={2019}
}

@inproceedings{sun2019bert4rec,
  title={Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer},
  author={Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={1441--1450},
  year={2019}
}

@inproceedings{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@inproceedings{janner2021reinforcement,
  title={Reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  booktitle={ICML 2021 Workshop on Unsupervised Reinforcement Learning},
  year={2021}
}

@inproceedings{zheng2022online,
  title={Online decision transformer},
  author={Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  booktitle={Proceedings of the 39th International Conference on Machine Learning},
  year={2022}
}

@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and et al.},
  journal={OpenAI Blog},
  year={2018}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and et al.},
  booktitle={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
%%%

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}
@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}

@article{ji2018risk,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1803.07300},
  year={2018}
}

@inproceedings{telgarsky2013margins,
  title={Margins, shrinkage, and boosting},
  author={Telgarsky, Matus},
  booktitle={International Conference on Machine Learning},
  pages={307--315},
  year={2013},
  organization={PMLR}
}

@article{zhang2005boosting,
  title={Boosting with early stopping: Convergence and consistency},
  author={Zhang, Tong and Yu, Bin},
  journal={Annals of Statistics},
  pages={1538},
  year={2005}
}

@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2017},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge},
  edition={2nd}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}

@inproceedings{merrill2019sequential,
    title = "Sequential Neural Networks as Automata",
    author = "Merrill, William",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    pages = "1--13",
    abstract = "This work attempts to explain the types of computation that neural networks can perform by relating them to automata. We first define what it means for a real-time network with bounded precision to accept a language. A measure of network memory follows from this definition. We then characterize the classes of languages acceptable by various recurrent networks, attention, and convolutional networks. We find that LSTMs function like counter machines and relate convolutional networks to the subregular hierarchy. Overall, this work attempts to increase our understanding and ability to interpret neural networks through the lens of theory. These theoretical insights help explain neural computation, as well as the relationship between neural networks and natural language grammar.",
}

@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}

@inproceedings{
perez2019turing,
title={On the {Turing} Completeness of Modern Neural Network Architectures},
author={Jorge Pérez and Javier Marinković and Pablo Barceló},
booktitle={International Conference on Learning Representations},
year={2019}
}


@nproceedings{chen2017recurrent,
    title = "Recurrent Neural Networks as Weighted Language Recognizers",
    author = "Chen, Yining  and
      Gilroy, Sorcha  and
      Maletti, Andreas  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1205",
    doi = "10.18653/v1/N18-1205",
    pages = "2261--2271",
    abstract = "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.",
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
  pages={132--150}
}




@inproceedings{tran2018importance,
    title = "The Importance of Being Recurrent for Modeling Hierarchical Structure",
    author = "Tran, Ke  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "4731--4736",
    abstract = "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures{---}recurrent versus non-recurrent{---}with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at \url{https://github.com/ ketranm/fan_vs_rnn}",
}

@inproceedings{yang2019assessing,
  author    = {Baosong Yang and
               Longyue Wang and
               Derek F. Wong and
               Lidia S. Chao and
               Zhaopeng Tu},
  title     = {Assessing the Ability of Self-Attention Networks to Learn Word Order},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {3635--3644},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/YangWWCT19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (SFCS 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10}}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in {AC}0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on {SRN} behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@inproceedings{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  booktitle={IJCAI'18 Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  year={2018},
  pages={4345--4352}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for {RNN}/{CNN}-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@inproceedings{hao2019modeling,
    title = "Modeling Recurrence for Transformer",
    author = "Hao, Jie  and
      Wang, Xing  and
      Yang, Baosong  and
      Wang, Longyue  and
      Zhang, Jinfeng  and
      Tu, Zhaopeng",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1198--1207",
    abstract = "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence modeling hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.",
}



@inproceedings{voita2019analyzing,
  author    = {Elena Voita and
               David Talbot and
               Fedor Moiseev and
               Rico Sennrich and
               Ivan Titov},
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
               Lifting, the Rest Can Be Pruned},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {5797--5808},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/VoitaTMST19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? {A}n Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@inproceedings{dai2019transformer,
  author    = {Zihang Dai and
               Zhilin Yang and
               Yiming Yang and
               Jaime G. Carbonell and
               Quoc Viet Le and
               Ruslan Salakhutdinov},
  title     = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {2978--2988},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/DaiYYCLS19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{gulordava2018colorless,
    title = "Colorless Green Recurrent Networks Dream Hierarchically",
    author = "Gulordava, Kristina  and
      Bojanowski, Piotr  and
      Grave, Edouard  and
      Linzen, Tal  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    pages = "1195--1205"
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary {E}nglish},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive science},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}



@inproceedings{lin2019open,
    title = "Open {S}esame: Getting inside {BERT}{'}s Linguistic Knowledge",
    author = "Lin, Yongjie  and
      Tan, Yi Chern  and
      Frank, Robert",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "241--253",
    abstract = "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT{'}s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT{'}s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT{'}s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.",
}


@inproceedings{tenney2019bert,
  author    = {Ian Tenney and
               Dipanjan Das and
               Ellie Pavlick},
  title     = {{BERT} Rediscovers the Classical {NLP} Pipeline},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {4593--4601},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/TenneyDP19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
	editor={R. Duncan Luce and Robert R. Bush and Eugene Galanter},
	pages={419--492},
	publisher={John Wiley},
        year = {1963}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={Computational Complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{
miller2018recurrent,
title={Stable Recurrent Models},
author={John Miller and Moritz Hardt},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}


@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@article{parker2017cue,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  journal={Language processing and disorders},
  pages={121--144},
  year={2017},
  publisher={Cambridge Scholars Publishing Newcastle}
}




@article{barrington1992regular,
  title={Regular languages in {NC}1},
  author={Barrington, David A Mix and Compton, Kevin and Straubing, Howard and Th{\'e}rien, Denis},
  journal={Journal of Computer and System Sciences},
  volume={44},
  number={3},
  pages={478--499},
  year={1992},
  publisher={Elsevier}
}

@article{korsky2019computational,
  title={On the Computational Power of {RNN}s},
  author={Samuel A. Korsky and Robert C. Berwick},
  journal={arXiv preprint arXiv:1906.06349},
  year={2019}
}

@inproceedings{hsieh2019robustness,
    title = "On the Robustness of Self-Attentive Models",
    author = "Hsieh, Yu-Lun and
      Cheng, Minhao  and
      Juan, Da-Cheng  and
      Wei, Wei  and
      Hsu, Wen-Lian  and
      Hsieh, Cho-Jui",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "1520--1529"
}

@inproceedings{levy2018long,
  title={Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum},
  author={Levy, Omer and Lee, Kenton and FitzGerald, Nicholas and Zettlemoyer, Luke},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={732--739},
  year={2018}
}

@article{ostmeyer2019machine,
  title={Machine learning on sequential data using a recurrent weighted average},
  author={Ostmeyer, Jared and Cowell, Lindsay},
  journal={Neurocomputing},
  volume={331},
  pages={281--288},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{lei2017deriving,
  title={Deriving neural architectures from sequence and graph kernels},
  author={Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2024--2033},
  year={2017},
  organization={JMLR. org}
}
@article{yin2017comparative,
  title={Comparative Study of CNN and RNN for Natural Language Processing},
  author={Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1702.01923},
  year={2017}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={933--941},
  year={2017},
  organization={JMLR. org}
}
@article{bradbury2016quasi,
  title={Quasi-recurrent neural networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}
@article{lei2015semi,
  title={Semi-supervised question retrieval with gated convolutions},
  author={Lei, Tao and Joshi, Hrishikesh and Barzilay, Regina and Jaakkola, Tommi and Tymoshenko, Katerina and Moschitti, Alessandro and Marquez, Lluis},
  journal={arXiv preprint arXiv:1512.05726},
  year={2015}
}
@inproceedings{sundermeyer2013comparison,
  title={Comparison of feedforward and recurrent neural network language models},
  author={Sundermeyer, Martin and Oparin, Ilya and Gauvain, J-L and Freiberg, Ben and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8430--8434},
  year={2013},
  organization={IEEE}
}
@inproceedings{balduzzi2016strongly,
  title={Strongly-Typed Recurrent Neural Networks},
  author={Balduzzi, David and Ghifary, Muhammad},
  booktitle={International Conference on Machine Learning},
  pages={1292--1300},
  year={2016}
}


@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@book{chomsky1957syntactic,
  title={Syntactic structures.},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton},
  address={The Hague}
}


@article{keener1992limit,
  title={Limit theorems for random walks conditioned to stay positive},
  author={Keener, Robert W and others},
  journal={The Annals of Probability},
  volume={20},
  number={2},
  pages={801--824},
  year={1992},
  publisher={Institute of Mathematical Statistics}
}


@article{chi1999statistical,
  title={Statistical properties of probabilistic context-free grammars},
  author={Chi, Zhiyi},
  journal={Computational Linguistics},
  volume={25},
  number={1},
  pages={131--160},
  year={1999},
  publisher={MIT Press}
}

@incollection{shieber1985evidence,
  title={Evidence against the context-freeness of natural language},
  author={Shieber, Stuart M},
  booktitle={Philosophy, Language, and Artificial Intelligence},
  pages={79--89},
  year={1985},
  publisher={Springer}
}


@inproceedings{kuncoro2018lstms,
  title={Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better},
  author={Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1426--1436},
  year={2018}
}



@article{mccoy2019berts,
  title={BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  author={McCoy, R Thomas and Min, Junghyun and Linzen, Tal},
  journal={arXiv preprint arXiv:1911.02969},
  year={2019}
}



@article{li2023theoretical,
  title={A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity},
  author={Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2302.06015},
  year={2023}
}

@inproceedings{sahiner2022unraveling,
  title={Unraveling attention via convex duality: Analysis and interpretations of vision transformers},
  author={Sahiner, Arda and Ergen, Tolga and Ozturkler, Batu and Pauly, John and Mardani, Morteza and Pilanci, Mert},
  booktitle={International Conference on Machine Learning},
  pages={19050--19088},
  year={2022},
  organization={PMLR}
}

@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2017},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge},
  edition={2nd}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}

@inproceedings{merrill2019sequential,
    title = "Sequential Neural Networks as Automata",
    author = "Merrill, William",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    pages = "1--13",
    abstract = "This work attempts to explain the types of computation that neural networks can perform by relating them to automata. We first define what it means for a real-time network with bounded precision to accept a language. A measure of network memory follows from this definition. We then characterize the classes of languages acceptable by various recurrent networks, attention, and convolutional networks. We find that LSTMs function like counter machines and relate convolutional networks to the subregular hierarchy. Overall, this work attempts to increase our understanding and ability to interpret neural networks through the lens of theory. These theoretical insights help explain neural computation, as well as the relationship between neural networks and natural language grammar.",
}

@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}

@inproceedings{
perez2019turing,
title={On the {Turing} Completeness of Modern Neural Network Architectures},
author={Jorge Pérez and Javier Marinković and Pablo Barceló},
booktitle={International Conference on Learning Representations},
year={2019}
}


@nproceedings{chen2017recurrent,
    title = "Recurrent Neural Networks as Weighted Language Recognizers",
    author = "Chen, Yining  and
      Gilroy, Sorcha  and
      Maletti, Andreas  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1205",
    doi = "10.18653/v1/N18-1205",
    pages = "2261--2271",
    abstract = "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.",
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
  pages={132--150}
}




@inproceedings{tran2018importance,
    title = "The Importance of Being Recurrent for Modeling Hierarchical Structure",
    author = "Tran, Ke  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "4731--4736",
    abstract = "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures{---}recurrent versus non-recurrent{---}with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at \url{https://github.com/ ketranm/fan_vs_rnn}",
}

@inproceedings{yang2019assessing,
  author    = {Baosong Yang and
               Longyue Wang and
               Derek F. Wong and
               Lidia S. Chao and
               Zhaopeng Tu},
  title     = {Assessing the Ability of Self-Attention Networks to Learn Word Order},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {3635--3644},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/YangWWCT19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (SFCS 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10}}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in {AC}0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on {SRN} behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@inproceedings{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  booktitle={IJCAI'18 Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  year={2018},
  pages={4345--4352}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for {RNN}/{CNN}-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}


@inproceedings{chen2018best,
    title = "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation",
    author = "Chen, Mia Xu  and
      Firat, Orhan  and
      Bapna, Ankur  and
      Johnson, Melvin  and
      Macherey, Wolfgang  and
      Foster, George  and
      Jones, Llion  and
      Schuster, Mike  and
      Shazeer, Noam  and
      Parmar, Niki  and
      Vaswani, Ashish  and
      Uszkoreit, Jakob  and
      Kaiser, Lukasz  and
      Chen, Zhifeng  and
      Wu, Yonghui  and
      Hughes, Macduff",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    pages = "76--86",
    abstract = "The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT{'}14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",
}



@inproceedings{hao2019modeling,
    title = "Modeling Recurrence for Transformer",
    author = "Hao, Jie  and
      Wang, Xing  and
      Yang, Baosong  and
      Wang, Longyue  and
      Zhang, Jinfeng  and
      Tu, Zhaopeng",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1198--1207",
    abstract = "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence modeling hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.",
}

@inproceedings{
paulus2017deep,
title={A Deep Reinforced Model for Abstractive Summarization},
author={Romain Paulus and Caiming Xiong and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018}
}




@inproceedings{lin2017structured,
  title={A structured self-attentive sentence embedding},
  author={Lin, Zhouhan and Feng, Minwei and Santos, Cicero Nogueira dos and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2017}
}



@inproceedings{voita2019analyzing,
  author    = {Elena Voita and
               David Talbot and
               Fedor Moiseev and
               Rico Sennrich and
               Ivan Titov},
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
               Lifting, the Rest Can Be Pruned},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {5797--5808},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/VoitaTMST19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? {A}n Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@inproceedings{dai2019transformer,
  author    = {Zihang Dai and
               Zhilin Yang and
               Yiming Yang and
               Jaime G. Carbonell and
               Quoc Viet Le and
               Ruslan Salakhutdinov},
  title     = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {2978--2988},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/DaiYYCLS19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{gulordava2018colorless,
    title = "Colorless Green Recurrent Networks Dream Hierarchically",
    author = "Gulordava, Kristina  and
      Bojanowski, Piotr  and
      Grave, Edouard  and
      Linzen, Tal  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    pages = "1195--1205"
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary {E}nglish},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive science},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}



@inproceedings{lin2019open,
    title = "Open {S}esame: Getting inside {BERT}{'}s Linguistic Knowledge",
    author = "Lin, Yongjie  and
      Tan, Yi Chern  and
      Frank, Robert",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "241--253",
    abstract = "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT{'}s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT{'}s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT{'}s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.",
}


@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{tenney2019bert,
  author    = {Ian Tenney and
               Dipanjan Das and
               Ellie Pavlick},
  title     = {{BERT} Rediscovers the Classical {NLP} Pipeline},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {4593--4601},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/TenneyDP19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
	editor={R. Duncan Luce and Robert R. Bush and Eugene Galanter},
	pages={419--492},
	publisher={John Wiley},
        year = {1963}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={Computational Complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{
miller2018recurrent,
title={Stable Recurrent Models},
author={John Miller and Moritz Hardt},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}


@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@article{parker2017cue,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  journal={Language processing and disorders},
  pages={121--144},
  year={2017},
  publisher={Cambridge Scholars Publishing Newcastle}
}




@article{barrington1992regular,
  title={Regular languages in {NC}1},
  author={Barrington, David A Mix and Compton, Kevin and Straubing, Howard and Th{\'e}rien, Denis},
  journal={Journal of Computer and System Sciences},
  volume={44},
  number={3},
  pages={478--499},
  year={1992},
  publisher={Elsevier}
}

@article{korsky2019computational,
  title={On the Computational Power of {RNN}s},
  author={Samuel A. Korsky and Robert C. Berwick},
  journal={arXiv preprint arXiv:1906.06349},
  year={2019}
}

@inproceedings{hsieh2019robustness,
    title = "On the Robustness of Self-Attentive Models",
    author = "Hsieh, Yu-Lun and
      Cheng, Minhao  and
      Juan, Da-Cheng  and
      Wei, Wei  and
      Hsu, Wen-Lian  and
      Hsieh, Cho-Jui",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "1520--1529"
}

@inproceedings{levy2018long,
  title={Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum},
  author={Levy, Omer and Lee, Kenton and FitzGerald, Nicholas and Zettlemoyer, Luke},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={732--739},
  year={2018}
}

@article{ostmeyer2019machine,
  title={Machine learning on sequential data using a recurrent weighted average},
  author={Ostmeyer, Jared and Cowell, Lindsay},
  journal={Neurocomputing},
  volume={331},
  pages={281--288},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{lei2017deriving,
  title={Deriving neural architectures from sequence and graph kernels},
  author={Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2024--2033},
  year={2017},
  organization={JMLR. org}
}
@article{yin2017comparative,
  title={Comparative Study of CNN and RNN for Natural Language Processing},
  author={Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1702.01923},
  year={2017}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={933--941},
  year={2017},
  organization={JMLR. org}
}
@article{bradbury2016quasi,
  title={Quasi-recurrent neural networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}
@article{lei2015semi,
  title={Semi-supervised question retrieval with gated convolutions},
  author={Lei, Tao and Joshi, Hrishikesh and Barzilay, Regina and Jaakkola, Tommi and Tymoshenko, Katerina and Moschitti, Alessandro and Marquez, Lluis},
  journal={arXiv preprint arXiv:1512.05726},
  year={2015}
}
@inproceedings{sundermeyer2013comparison,
  title={Comparison of feedforward and recurrent neural network language models},
  author={Sundermeyer, Martin and Oparin, Ilya and Gauvain, J-L and Freiberg, Ben and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8430--8434},
  year={2013},
  organization={IEEE}
}
@inproceedings{balduzzi2016strongly,
  title={Strongly-Typed Recurrent Neural Networks},
  author={Balduzzi, David and Ghifary, Muhammad},
  booktitle={International Conference on Machine Learning},
  pages={1292--1300},
  year={2016}
}


@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@book{chomsky1957syntactic,
  title={Syntactic structures.},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton},
  address={The Hague}
}


@article{keener1992limit,
  title={Limit theorems for random walks conditioned to stay positive},
  author={Keener, Robert W and others},
  journal={The Annals of Probability},
  volume={20},
  number={2},
  pages={801--824},
  year={1992},
  publisher={Institute of Mathematical Statistics}
}


@article{chi1999statistical,
  title={Statistical properties of probabilistic context-free grammars},
  author={Chi, Zhiyi},
  journal={Computational Linguistics},
  volume={25},
  number={1},
  pages={131--160},
  year={1999},
  publisher={MIT Press}
}

@incollection{shieber1985evidence,
  title={Evidence against the context-freeness of natural language},
  author={Shieber, Stuart M},
  booktitle={Philosophy, Language, and Artificial Intelligence},
  pages={79--89},
  year={1985},
  publisher={Springer}
}


@inproceedings{kuncoro2018lstms,
  title={Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better},
  author={Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1426--1436},
  year={2018}
}



@article{mccoy2019berts,
  title={BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  author={McCoy, R Thomas and Min, Junghyun and Linzen, Tal},
  journal={arXiv preprint arXiv:1911.02969},
  year={2019}
}
@article{furst1984parity,
  title={Parity, circuits, and the polynomial-time hierarchy},
  author={Furst, Merrick and Saxe, James B and Sipser, Michael},
  journal={Mathematical systems theory},
  volume={17},
  number={1},
  pages={13--27},
  year={1984},
  publisher={Springer}
}

@book{mitzenmacherprobability,
  title={Probability and Computing},
  year={2017},
  author={Mitzenmacher, Michael and Upfal, Eli},
  publisher={Cambridge University Press},
  address={Cambridge},
  edition={2nd}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={740--745},
  year={2018}
}


@inproceedings{sennhauser2018evaluating,
  title={Evaluating the Ability of {LSTM}s to Learn Context-Free Grammars},
  author={Sennhauser, Luzi and Berwick, Robert},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={115--124},
  year={2018}
}


@article{suzgun2019evaluating,
  title={On Evaluating the Generalization of {LSTM} Models in Formal Languages},
  author={Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
  journal={Proceedings of the Society for Computation in Linguistics (SCiL)},
  pages={277--286},
  year={2019}
}

@inproceedings{merrill2019sequential,
    title = "Sequential Neural Networks as Automata",
    author = "Merrill, William",
    booktitle = "Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges",
    month = aug,
    year = "2019",
    address = "Florence",
    publisher = "Association for Computational Linguistics",
    pages = "1--13",
    abstract = "This work attempts to explain the types of computation that neural networks can perform by relating them to automata. We first define what it means for a real-time network with bounded precision to accept a language. A measure of network memory follows from this definition. We then characterize the classes of languages acceptable by various recurrent networks, attention, and convolutional networks. We find that LSTMs function like counter machines and relate convolutional networks to the subregular hierarchy. Overall, this work attempts to increase our understanding and ability to interpret neural networks through the lens of theory. These theoretical insights help explain neural computation, as well as the relationship between neural networks and natural language grammar.",
}

@article{bernardy2018can,
  title={Can Recurrent Neural Networks Learn Nested Recursion?},
  author={Bernardy, Jean-Philippe},
  journal={LiLT (Linguistic Issues in Language Technology)},
  volume={16},
  number={1},
  year={2018}
}

@inproceedings{
perez2019turing,
title={On the {Turing} Completeness of Modern Neural Network Architectures},
author={Jorge Pérez and Javier Marinković and Pablo Barceló},
booktitle={International Conference on Learning Representations},
year={2019}
}


@nproceedings{chen2017recurrent,
    title = "Recurrent Neural Networks as Weighted Language Recognizers",
    author = "Chen, Yining  and
      Gilroy, Sorcha  and
      Maletti, Andreas  and
      May, Jonathan  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1205",
    doi = "10.18653/v1/N18-1205",
    pages = "2261--2271",
    abstract = "We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.",
}



@article{siegelman1991neural,
  title={On    the    computational    power of   neural   nets},
  author={Siegelman, Hava and Sontag, Eduardo D},
  journal={Journal of Computer and System Sciences},
  year={1995},
  volume={50},
  issue={1},
  pages={132--150}
}




@inproceedings{tran2018importance,
    title = "The Importance of Being Recurrent for Modeling Hierarchical Structure",
    author = "Tran, Ke  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "4731--4736",
    abstract = "Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures{---}recurrent versus non-recurrent{---}with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at \url{https://github.com/ ketranm/fan_vs_rnn}",
}

@inproceedings{yang2019assessing,
  author    = {Baosong Yang and
               Longyue Wang and
               Derek F. Wong and
               Lidia S. Chao and
               Zhaopeng Tu},
  title     = {Assessing the Ability of Self-Attention Networks to Learn Word Order},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {3635--3644},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/YangWWCT19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yao1986separating,
author={A. C. {Yao}},
booktitle={26th Annual Symposium on Foundations of Computer Science (SFCS 1985)},
title={Separating the polynomial-time hierarchy by oracles},
year={1985},
volume={},
number={},
pages={1-10}}


@article{hastad1994optimal,
  title={Optimal Depth, Very Small Size Circuits for Symmetrical Functions in {AC}0},
  author={Hastad, Johan and Wegener, Ingo and Wurm, Norbert and Yi, Sang-Zin},
  journal={Information and Computation},
  volume={108},
  number={2},
  pages={200--211},
  year={1994},
  publisher={Elsevier}
}

@book{arora2009computational,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@article{kirov2012processing,
  title={Processing of nested and cross-serial dependencies: an automaton perspective on {SRN} behaviour},
  author={Kirov, Christo and Frank, Robert},
  journal={Connection Science},
  volume={24},
  number={1},
  pages={1--24},
  year={2012},
  publisher={Taylor \& Francis}
}


@inproceedings{kirov2013bayesian,
  title={Bayesian speech production: Evidence from latency and hyperarticulation},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the annual meeting of the cognitive science society},
  volume={35},
  number={35},
  year={2013}
}


@inproceedings{kirov2012specificity,
  title={The specificity of online variation in speech production},
  author={Kirov, Christo and Wilson, Colin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={34},
  number={34},
  year={2012}
}


@inproceedings{shen2018reinforced,
  title={Reinforced self-attention network: a hybrid of hard and soft attention for sequence modeling},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Wang, Sen and Zhang, Chengqi},
  booktitle={IJCAI'18 Proceedings of the 27th International Joint Conference on Artificial Intelligence},
  year={2018},
  pages={4345--4352}
}


@inproceedings{shen2018disan,
  title={Disan: Directional self-attention network for {RNN}/{CNN}-free language understanding},
  author={Shen, Tao and Zhou, Tianyi and Long, Guodong and Jiang, Jing and Pan, Shirui and Zhang, Chengqi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}



@inproceedings{hao2019modeling,
    title = "Modeling Recurrence for Transformer",
    author = "Hao, Jie  and
      Wang, Xing  and
      Yang, Baosong  and
      Wang, Longyue  and
      Zhang, Jinfeng  and
      Tu, Zhaopeng",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    pages = "1198--1207",
    abstract = "Recently, the Transformer model that is based solely on attention mechanisms, has advanced the state-of-the-art on various machine translation tasks. However, recent studies reveal that the lack of recurrence modeling hinders its further improvement of translation capacity. In response to this problem, we propose to directly model recurrence for Transformer with an additional recurrence encoder. In addition to the standard recurrent neural network, we introduce a novel attentive recurrent network to leverage the strengths of both attention models and recurrent networks. Experimental results on the widely-used WMT14 English⇒German and WMT17 Chinese⇒English translation tasks demonstrate the effectiveness of the proposed approach. Our studies also reveal that the proposed model benefits from a short-cut that bridges the source and target sequences with a single recurrent layer, which outperforms its deep counterpart.",
}


@inproceedings{cheng2016long,
    title = "Long Short-Term Memory-Networks for Machine Reading",
    author = "Cheng, Jianpeng  and
      Dong, Li  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    pages = "551--561",
}




@inproceedings{parikh2016decomposable,
	    title = "A Decomposable Attention Model for Natural Language Inference",
	    author = {Parikh, Ankur  and
	      T{\"a}ckstr{\"o}m, Oscar  and
	      Das, Dipanjan  and
	      Uszkoreit, Jakob},
	    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
	    month = nov,
	    year = "2016",
	    address = "Austin, Texas",
	    publisher = "Association for Computational Linguistics",
	    pages = "2249--2255",
	}

@inproceedings{voita2019analyzing,
  author    = {Elena Voita and
               David Talbot and
               Fedor Moiseev and
               Rico Sennrich and
               Ivan Titov},
  title     = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy
               Lifting, the Rest Can Be Pruned},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {5797--5808},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/VoitaTMST19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{clark2019bert,
  title={What Does {BERT} Look At? {A}n Analysis of {BERT}'s Attention},
  author={Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  booktitle={Proceedings of BlackboxNLP 2019},
  year={2019}
}



@inproceedings{dai2019transformer,
  author    = {Zihang Dai and
               Zhilin Yang and
               Yiming Yang and
               Jaime G. Carbonell and
               Quoc Viet Le and
               Ruslan Salakhutdinov},
  title     = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {2978--2988},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/DaiYYCLS19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{gulordava2018colorless,
    title = "Colorless Green Recurrent Networks Dream Hierarchically",
    author = "Gulordava, Kristina  and
      Bojanowski, Piotr  and
      Grave, Edouard  and
      Linzen, Tal  and
      Baroni, Marco",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    pages = "1195--1205"
}

@article{linzen2016assessing,
	Author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	Journal = {Transactions of the Association for Computational Linguistics},
	Title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
    Volume = {4},
    Pages = {521--535},
	Year = {2016}
}

@inproceedings{skachkova2018closing,
  title={Closing Brackets with Recurrent Neural Networks},
  author={Skachkova, Natalia and Trost, Thomas and Klakow, Dietrich},
  booktitle={Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages={232--239},
  year={2018}
}


@article{tabor2000fractal,
  title={Fractal encoding of context-free grammars in connectionist networks},
  author={Tabor, Whitney},
  journal={Expert Systems},
  volume={17},
  number={1},
  pages={41--56},
  year={2000},
  publisher={Wiley Online Library}
}


@article{gruning2006stack,
  title={Stack-like and queue-like dynamics in recurrent neural networks},
  author={Gr{\"u}ning, Andr{\'e}},
  journal={Connection Science},
  volume={18},
  number={1},
  pages={23--42},
  year={2006},
  publisher={Taylor \& Francis}
}


@incollection{chomsky1963algebraic,
  title={The algebraic theory of context-free languages},
  author={Chomsky, Noam and Sch{\"u}tzenberger, Marcel P},
  booktitle={Studies in Logic and the Foundations of Mathematics},
  volume={35},
  pages={118--161},
  year={1963},
  publisher={Elsevier}
}

@article{gers2001lstm,
  title={{LSTM} recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen},
  journal={IEEE Transactions on Neural Networks},
  volume={12},
  number={6},
  pages={1333--1340},
  year={2001},
  publisher={IEEE}
}
@inproceedings{kalinke1998computation,
  title={Computation in recurrent neural networks: From counters to iterated function systems},
  author={Kalinke, Yvonne and Lehmann, Helko},
  booktitle={Australian Joint Conference on Artificial Intelligence},
  pages={179--190},
  year={1998},
  organization={Springer}
}



@article{everaert2015structures,
  title={Structures, not strings: linguistics as part of the cognitive sciences},
  author={Everaert, Martin BH and Huybregts, Marinus AC and Chomsky, Noam and Berwick, Robert C and Bolhuis, Johan J},
  journal={Trends in cognitive sciences},
  volume={19},
  number={12},
  pages={729--743},
  year={2015},
  publisher={Elsevier}
}





@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}


@article{cartling2008implicit,
  title={On the implicit acquisition of a context-free grammar by a simple recurrent neural network},
  author={Cartling, Bo},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1527--1537},
  year={2008},
  publisher={Elsevier}
}

@incollection{montague1973proper,
  title={The proper treatment of quantification in ordinary {E}nglish},
  author={Montague, Richard},
  booktitle={Approaches to natural language},
  pages={221--242},
  year={1973},
  publisher={Springer}
}

@inproceedings{marvin2018targeted,
    title = "Targeted Syntactic Evaluation of Language Models",
    author = "Marvin, Rebecca  and
      Linzen, Tal",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    pages = "1192--1202",
    abstract = "We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.",
}

@article{lewis2005activation,
  title={An activation-based model of sentence processing as skilled memory retrieval},
  author={Lewis, Richard L and Vasishth, Shravan},
  journal={Cognitive science},
  volume={29},
  number={3},
  pages={375--419},
  year={2005},
  publisher={Wiley Online Library}
}



@inproceedings{lin2019open,
    title = "Open {S}esame: Getting inside {BERT}{'}s Linguistic Knowledge",
    author = "Lin, Yongjie  and
      Tan, Yi Chern  and
      Frank, Robert",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "241--253",
    abstract = "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT{'}s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT{'}s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT{'}s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.",
}



@inproceedings{tenney2019bert,
  author    = {Ian Tenney and
               Dipanjan Das and
               Ellie Pavlick},
  title     = {{BERT} Rediscovers the Classical {NLP} Pipeline},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
               Volume 1: Long Papers},
  pages     = {4593--4601},
  year      = {2019},
  timestamp = {Fri, 13 Sep 2019 13:00:42 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/TenneyDP19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{miller-finitary-1963,
        title = {Finitary models of language users},
        booktitle = {Handbook of {Mathematical} {Psychology}},
        author = {Miller, George A and Chomsky, Noam},
	editor={R. Duncan Luce and Robert R. Bush and Eugene Galanter},
	pages={419--492},
	publisher={John Wiley},
        year = {1963}
}

@article{gibson1999memory,
  title={Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical},
  author={Gibson, Edward and Thomas, James},
  journal={Language and Cognitive Processes},
  volume={14},
  number={3},
  pages={225--248},
  year={1999},
  publisher={Taylor \& Francis}
}


@inproceedings{gopalan2016smooth,
  title={Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions},
  author={Gopalan, Parikshit and Nisan, Noam and Servedio, Rocco A and Talwar, Kunal and Wigderson, Avi},
  booktitle={Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science},
  pages={59--70},
  year={2016},
  organization={ACM}
}


@article{de2018deep,
  title={Deep neural networks are biased towards simple functions},
  author={De Palma, Giacomo and Kiani, Bobak Toussi and Lloyd, Seth},
  journal={arXiv preprint arXiv:1812.10156},
  year={2018}
}

@inproceedings{gilmer2015new,
  title={A new approach to the sensitivity conjecture},
  author={Gilmer, Justin and Kouck{\`y}, Michal and Saks, Michael E},
  booktitle={Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science},
  pages={247--254},
  year={2015},
  organization={ACM}
}


@article{rossman2018average,
  title={The average sensitivity of bounded-depth formulas},
  author={Rossman, Benjamin},
  journal={Computational Complexity},
  volume={27},
  number={2},
  pages={209--223},
  year={2018},
  publisher={Springer}
}



@article{boppana1997average,
  title={The average sensitivity of bounded-depth circuits},
  author={Boppana, Ravi B},
  journal={Information processing letters},
  volume={63},
  number={5},
  pages={257--261},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{
miller2018recurrent,
title={Stable Recurrent Models},
author={John Miller and Moritz Hardt},
booktitle={International Conference on Learning Representations},
year={2019}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo and others},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994}
}


@book{mcnaughton1971counter,
  title={Counter-Free Automata (MIT research monograph no. 65)},
  author={McNaughton, Robert and Papert, Seymour A},
  year={1971},
  publisher={The MIT Press}
}

@article{parker2017cue,
  title={The cue-based retrieval theory of sentence comprehension: New findings and new challenges},
  author={Parker, Dan and Shvartsman, Michael and Van Dyke, Julie A},
  journal={Language processing and disorders},
  pages={121--144},
  year={2017},
  publisher={Cambridge Scholars Publishing Newcastle}
}




@article{barrington1992regular,
  title={Regular languages in {NC}1},
  author={Barrington, David A Mix and Compton, Kevin and Straubing, Howard and Th{\'e}rien, Denis},
  journal={Journal of Computer and System Sciences},
  volume={44},
  number={3},
  pages={478--499},
  year={1992},
  publisher={Elsevier}
}

@article{korsky2019computational,
  title={On the Computational Power of {RNN}s},
  author={Samuel A. Korsky and Robert C. Berwick},
  journal={arXiv preprint arXiv:1906.06349},
  year={2019}
}

@inproceedings{hsieh2019robustness,
    title = "On the Robustness of Self-Attentive Models",
    author = "Hsieh, Yu-Lun and
      Cheng, Minhao  and
      Juan, Da-Cheng  and
      Wei, Wei  and
      Hsu, Wen-Lian  and
      Hsieh, Cho-Jui",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    pages = "1520--1529"
}

@inproceedings{levy2018long,
  title={Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum},
  author={Levy, Omer and Lee, Kenton and FitzGerald, Nicholas and Zettlemoyer, Luke},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={732--739},
  year={2018}
}

@article{ostmeyer2019machine,
  title={Machine learning on sequential data using a recurrent weighted average},
  author={Ostmeyer, Jared and Cowell, Lindsay},
  journal={Neurocomputing},
  volume={331},
  pages={281--288},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{lei2017deriving,
  title={Deriving neural architectures from sequence and graph kernels},
  author={Lei, Tao and Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2024--2033},
  year={2017},
  organization={JMLR. org}
}
@article{yin2017comparative,
  title={Comparative Study of CNN and RNN for Natural Language Processing},
  author={Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1702.01923},
  year={2017}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={933--941},
  year={2017},
  organization={JMLR. org}
}
@article{bradbury2016quasi,
  title={Quasi-recurrent neural networks},
  author={Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01576},
  year={2016}
}
@article{lei2015semi,
  title={Semi-supervised question retrieval with gated convolutions},
  author={Lei, Tao and Joshi, Hrishikesh and Barzilay, Regina and Jaakkola, Tommi and Tymoshenko, Katerina and Moschitti, Alessandro and Marquez, Lluis},
  journal={arXiv preprint arXiv:1512.05726},
  year={2015}
}
@inproceedings{sundermeyer2013comparison,
  title={Comparison of feedforward and recurrent neural network language models},
  author={Sundermeyer, Martin and Oparin, Ilya and Gauvain, J-L and Freiberg, Ben and Schl{\"u}ter, Ralf and Ney, Hermann},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={8430--8434},
  year={2013},
  organization={IEEE}
}
@inproceedings{balduzzi2016strongly,
  title={Strongly-Typed Recurrent Neural Networks},
  author={Balduzzi, David and Ghifary, Muhammad},
  booktitle={International Conference on Machine Learning},
  pages={1292--1300},
  year={2016}
}


@article{michel2019sixteen,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@book{chomsky1957syntactic,
  title={Syntactic structures.},
  author={Chomsky, Noam},
  year={1957},
  publisher={Mouton},
  address={The Hague}
}


@article{keener1992limit,
  title={Limit theorems for random walks conditioned to stay positive},
  author={Keener, Robert W and others},
  journal={The Annals of Probability},
  volume={20},
  number={2},
  pages={801--824},
  year={1992},
  publisher={Institute of Mathematical Statistics}
}


@article{chi1999statistical,
  title={Statistical properties of probabilistic context-free grammars},
  author={Chi, Zhiyi},
  journal={Computational Linguistics},
  volume={25},
  number={1},
  pages={131--160},
  year={1999},
  publisher={MIT Press}
}

@incollection{shieber1985evidence,
  title={Evidence against the context-freeness of natural language},
  author={Shieber, Stuart M},
  booktitle={Philosophy, Language, and Artificial Intelligence},
  pages={79--89},
  year={1985},
  publisher={Springer}
}


@inproceedings{kuncoro2018lstms,
  title={Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better},
  author={Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1426--1436},
  year={2018}
}



@article{mccoy2019berts,
  title={BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  author={McCoy, R Thomas and Min, Junghyun and Linzen, Tal},
  journal={arXiv preprint arXiv:1911.02969},
  year={2019}
}


@article{bartlett1998boosting,
  title={Boosting the margin: A new explanation for the effectiveness of voting methods},
  author={Bartlett, Peter and Freund, Yoav and Lee, Wee Sun and Schapire, Robert E},
  journal={The annals of statistics},
  volume={26},
  number={5},
  pages={1651--1686},
  year={1998},
  publisher={Institute of Mathematical Statistics}
}

@book{lang2006introduction,
  title={Introduction to differentiable manifolds},
  author={Lang, Serge},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{sussmann1973orbits,
  title={Orbits of families of vector fields and integrability of distributions},
  author={Sussmann, H{\'e}ctor J},
  journal={Transactions of the American Mathematical Society},
  volume={180},
  pages={171--188},
  year={1973}
}

@article{kiwiel1997free,
  title={Free-steering relaxation methods for problems with strictly convex costs and linear constraints},
  author={Kiwiel, Krzysztof C},
  journal={Mathematics of Operations Research},
  volume={22},
  number={2},
  pages={326--349},
  year={1997},
  publisher={INFORMS}
}

@article{censor1981iterative,
  title={An iterative row-action method for interval convex programming},
  author={Censor, Yair and Lent, Arnold},
  journal={Journal of Optimization theory and Applications},
  volume={34},
  number={3},
  pages={321--353},
  year={1981},
  publisher={Springer}
}

@article{bregman1967relaxation,
  title={The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
  author={Bregman, Lev M},
  journal={USSR computational mathematics and mathematical physics},
  volume={7},
  number={3},
  pages={200--217},
  year={1967},
  publisher={Elsevier}
}

@article{nash1954c1,
  title={C1 isometric imbeddings},
  author={Nash, John},
  journal={Annals of mathematics},
  pages={383--396},
  year={1954},
  publisher={JSTOR}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@inproceedings{gunther1991isometric,
  title={Isometric embeddings of Riemannian manifolds, Kyoto, 1990},
  author={Gunther, Matthias},
  booktitle={Proc. Intern. Congr. Math.},
  pages={1137--1143},
  year={1991},
  organization={Math. Soc. Japan}
}

@article{crouzeix1977relationship,
  title={A relationship between the second derivatives of a convex function and of its conjugate},
  author={Crouzeix, Jean-Pierre},
  journal={Mathematical Programming},
  volume={13},
  number={1},
  pages={364--365},
  year={1977},
  publisher={Springer}
}

@article{bauschke1997legendre,
  title={Legendre functions and the method of random Bregman projections},
  author={Bauschke, Heinz H and Borwein, Jonathan M and others},
  journal={Journal of convex analysis},
  volume={4},
  number={1},
  pages={27--67},
  year={1997},
  publisher={Citeseer}
}

@incollection{rockafellar2015convex,
  title={Convex analysis},
  author={Rockafellar, Ralph Tyrell},
  booktitle={Convex analysis},
  year={2015},
  publisher={Princeton university press}
}


@article{zou2021benefits,
  title={The benefits of implicit regularization from sgd in least squares problems},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Foster, Dean P and Kakade, Sham},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5456--5468},
  year={2021}
}

@inproceedings{wang2021implicit,
  title={The implicit bias for adaptive optimization algorithms on homogeneous neural networks},
  author={Wang, Bohan and Meng, Qi and Chen, Wei and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={10849--10858},
  year={2021},
  organization={PMLR}
}


@article{wang2021momentum,
  title={Momentum Doesn't Change the Implicit Bias},
  author={Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming},
  journal={arXiv preprint arXiv:2110.03891},
  year={2021}
}

@article{gunasekar2018implicitconv,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{razin2020implicit,
  title={Implicit regularization in deep learning may not be explainable by norms},
  author={Razin, Noam and Cohen, Nadav},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21174--21187},
  year={2020}
}


@inproceedings{amid2020winnowing,
  title={Winnowing with gradient descent},
  author={Amid, Ehsan and Warmuth, Manfred K},
  booktitle={Conference on Learning Theory},
  pages={163--182},
  year={2020},
  organization={PMLR}
}


@inproceedings{ji2021characterizing,
  title={Characterizing the implicit bias via a primal-dual analysis},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Algorithmic Learning Theory},
  pages={772--804},
  year={2021},
  organization={PMLR}
}


@inproceedings{ji2019implicit,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1772--1798},
  year={2019},
  organization={PMLR}
}


@article{moroshko2020implicit,
  title={Implicit bias in deep linear classification: Initialization scale vs training accuracy},
  author={Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22182--22193},
  year={2020}
}


@article{nash1956imbedding,
  title={The imbedding problem for Riemannian manifolds},
  author={Nash, John},
  journal={Annals of mathematics},
  pages={20--63},
  year={1956},
  publisher={JSTOR}
}

@book{lee2018introduction,
  title={Introduction to Riemannian manifolds},
  author={Lee, John M},
  year={2018},
  publisher={Springer}
}

@book{lee2013smooth,
  title={Introduction to Smooth Manifolds},
  author={Lee, John M},
  year={2013},
  publisher={Springer}
}

@article{alvarez2004hessian,
  title={Hessian Riemannian gradient flows in convex programming},
  author={Alvarez, Felipe and Bolte, J{\'e}r{\^o}me and Brahic, Olivier},
  journal={SIAM journal on control and optimization},
  volume={43},
  number={2},
  pages={477--501},
  year={2004},
  publisher={SIAM}
}

@article{amid2020reparameterizing,
  title={Reparameterizing mirror descent as gradient descent},
  author={Amid, Ehsan and Warmuth, Manfred KK},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8430--8439},
  year={2020}
}

@inproceedings{gunasekar2021mirrorless,
  title={Mirrorless mirror descent: A natural derivation of mirror descent},
  author={Gunasekar, Suriya and Woodworth, Blake and Srebro, Nathan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2305--2313},
  year={2021},
  organization={PMLR}
}

@inproceedings{azulay2021implicit,
  title={On the implicit bias of initialization shape: Beyond infinitesimal mirror descent},
  author={Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake E and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={468--477},
  year={2021},
  organization={PMLR}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@incollection{tropp2015convex,
  title={Convex recovery of a structured signal from independent random linear measurements},
  author={Tropp, Joel A},
  booktitle={Sampling Theory, a Renaissance},
  pages={67--101},
  year={2015},
  publisher={Springer}
}

@book{karatzas2014brownian,
  title={Brownian motion and stochastic calculus},
  author={Karatzas, Ioannis and Shreve, Steven},
  volume={113},
  year={2014},
  publisher={springer}
}

@book{billingsley2013convergence,
  title={Convergence of probability measures},
  author={Billingsley, Patrick},
  year={2013},
  publisher={John Wiley \& Sons}
}

@book{pollard2012convergence,
  title={Convergence of stochastic processes},
  author={Pollard, David},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{katzenberger1991solutions,
  title={Solutions of a stochastic differential equation forced onto a manifold by a large drift},
  author={Katzenberger, Gary Shon},
  journal={The Annals of Probability},
  pages={1587--1628},
  year={1991},
  publisher={JSTOR}
}



@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as sgd},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}

@article{kahn1995probability,
  title={On the probability that a random$\pm$1-matrix is singular},
  author={Kahn, Jeff and Koml{\'o}s, J{\'a}nos and Szemer{\'e}di, Endre},
  journal={Journal of the American Mathematical Society},
  volume={8},
  number={1},
  pages={223--240},
  year={1995}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@article{wu2017towards,
  title={Towards understanding generalization of deep learning: Perspective of loss landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}

@article{he2019asymmetric,
  title={Asymmetric valleys: Beyond sharp and flat local minima},
  author={He, Haowei and Huang, Gao and Yuan, Yang},
  journal={arXiv preprint arXiv:1902.00744},
  year={2019}
}

@article{cooper2020critical,
  title={The critical locus of overparameterized neural networks},
  author={Cooper, Y},
  journal={arXiv preprint arXiv:2005.04210},
  year={2020}
}

@article{li2018over,
  title={Over-parameterized deep neural networks have no strict local minima for any continuous activations},
  author={Li, Dawei and Ding, Tian and Sun, Ruoyu},
  journal={arXiv preprint arXiv:1812.11039},
  year={2018}
}

@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}

@inproceedings{geyer2020low,
  title={Low-rank regularization and solution uniqueness in over-parameterized matrix sensing},
  author={Geyer, Kelly and Kyrillidis, Anastasios and Kalev, Amir},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={930--940},
  year={2020},
  organization={PMLR}
}

@inproceedings{nguyen2019connected,
  title={On connected sublevel sets in deep learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019},
  organization={PMLR}
}

@article{venturi2018spurious,
  title={Spurious valleys in two-layer neural network optimization landscapes},
  author={Venturi, Luca and Bandeira, Afonso S and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  year={2018}
}

@article{kuditipudi2019explaining,
  title={Explaining landscape connectivity of low-cost solutions for multilayer nets},
  author={Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Arora, Sanjeev and Ge, Rong},
  journal={arXiv preprint arXiv:1906.06247},
  year={2019}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1802.10026},
  year={2018}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{nguyen2018loss,
  title={On the loss landscape of a class of deep neural networks with no bad local valleys},
  author={Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  journal={arXiv preprint arXiv:1809.10749},
  year={2018}
}

@article{freeman2016topology,
  title={Topology and geometry of half-rectified network optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={arXiv preprint arXiv:1611.01540},
  year={2016}
}

@inproceedings{liang2018understanding,
  title={Understanding the loss surface of neural networks for binary classification},
  author={Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={International Conference on Machine Learning},
  pages={2835--2843},
  year={2018},
  organization={PMLR}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018},
  organization={PMLR}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@article{li2019generalization,
  title={On generalization error bounds of noisy gradient methods for non-convex learning},
  author={Li, Jian and Luo, Xuanyuan and Qiao, Mingda},
  journal={arXiv preprint arXiv:1902.00621},
  year={2019}
}


@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1705.08741},
  year={2017}
}

@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrzebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}


@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

@article{wen2019interplay,
  title={Interplay between optimization and generalization of stochastic gradient descent with covariance noise},
  author={Wen, Yeming and Luk, Kevin and Gazeau, Maxime and Zhang, Guodong and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:1902.08234},
  year={2019}
}

@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}

@inproceedings{cheng2020stochastic,
  title={Stochastic gradient and langevin processes},
  author={Cheng, Xiang and Yin, Dong and Bartlett, Peter and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={1810--1819},
  year={2020},
  organization={PMLR}
}

@article{li2020reconciling,
  title={Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate},
  author={Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{li2021validity,
  title={On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)},
  author={Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2102.12470},
  year={2021}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent escapes from sharp minima exponentially fast},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{fehrman2020convergence,
  title={Convergence rates for the stochastic gradient descent method for non-convex objective functions},
  author={Fehrman, Benjamin and Gess, Benjamin and Jentzen, Arnulf},
  journal={Journal of Machine Learning Research},
  volume={21},
  year={2020},
  publisher={MICROTOME PUBL}
}

@article{shi2020learning,
  title={On learning rates and schr$\backslash$" odinger operators},
  author={Shi, Bin and Su, Weijie J and Jordan, Michael I},
  journal={arXiv preprint arXiv:2004.06977},
  year={2020}
}

@book{whitt2002stochastic,
  title={Stochastic-process limits: an introduction to stochastic-process limits and their application to queues},
  author={Whitt, Ward},
  year={2002},
  publisher={Springer Science \& Business Media}
}

@article{Lee_Panageas_Piliouras_Simchowitz_Jordan_Recht_2019, title={First-order methods almost always avoid strict saddle points}, volume={176}, ISSN={0025-5610}, DOI={10.1007/s10107-019-01374-3}, abstractNote={We establish that first-order methods avoid strict saddle points for almost all initializations. Our results apply to a wide variety of first-order methods, including (manifold) gradient descent, block coordinate descent, mirror descent and variants thereof. The connecting thread is that such algorithms can be studied from a dynamical systems perspective in which appropriate instantiations of the Stable Manifold Theorem allow for a global stability analysis. Thus, neither access to second-order derivative information nor randomness beyond initialization is necessary to provably avoid strict saddle points.}, number={1–2}, journal={Mathematical Programming}, author={Lee, Jason D. and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin}, year={2019}, pages={311–337} }


@inproceedings{wei2019regularization,
 author = {Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
 url = {https://proceedings.neurips.cc/paper/2019/file/8744cf92c88433f8cb04a02e6db69a0d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{jacod2013limit,
  title={Limit theorems for stochastic processes},
  author={Jacod, Jean and Shiryaev, Albert},
  volume={288},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{Falconer_1983, 
title={Differentiation of the Limit Mapping in a Dynamical System}, volume={s2-27}, ISSN={0024-6107}, DOI={10.1112/jlms/s2-27.2.356}, number={2}, journal={Journal of the London Mathematical Society}, author={Falconer, K. J.}, year={1983}, pages={356–372} }

@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}


@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1474--1520},
  year={2019},
  publisher={JMLR. org}
}

@book{do2013riemannian,
  title={Riemannian geometry},
  author={Do Carmo, Manfredo P},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{holbrook2018differentiating,
  title={Differentiating the pseudo determinant},
  author={Holbrook, Andrew},
  journal={Linear Algebra and its Applications},
  volume={548},
  pages={293--304},
  year={2018},
  publisher={Elsevier}
}

@book{banyaga2013lectures,
  title={Lectures on Morse homology},
  author={Banyaga, Augustin and Hurtubise, David},
  volume={29},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{li2020convolutional,
  title={Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?},
  author={Li, Zhiyuan and Zhang, Yi and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2010.08515},
  year={2020}
}

@inproceedings{ng2004feature,
  title={Feature selection, L 1 vs. L 2 regularization, and rotational invariance},
  author={Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={78},
  year={2004}
}

@article{lee2017first,
  title={First-order methods almost always avoid saddle points},
  author={Lee, Jason D and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={arXiv preprint arXiv:1710.07406},
  year={2017}
}

@inproceedings{lee2016gradient,
  title={Gradient descent only converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  booktitle={Conference on learning theory},
  pages={1246--1257},
  year={2016},
  organization={PMLR}
}

@article{polyak1964gradient,
  title={Gradient methods for solving equations and inequalities},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={6},
  pages={17--32},
  year={1964},
  publisher={Elsevier}
}

@article{lojasiewicz1963topological,
  title={A topological property of real analytic subsets},
  author={Lojasiewicz, Stanislaw},
  journal={Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
  volume={117},
  number={87-89},
  pages={2},
  year={1963}
}

@article{raskutti2012minimax,
  title={Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming.},
  author={Raskutti, Garvesh and J Wainwright, Martin and Yu, Bin},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}
@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@inproceedings{li2020towards,
  title={Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning},
  author={Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{zhao2019implicit,
  title={Implicit regularization via Hadamard product over-parametrization in high-dimensional linear regression},
  author={Zhao, Peng and Yang, Yun and He, Qiao-Chu},
  journal={arXiv preprint arXiv:1903.09367},
  year={2019}
}

@article{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  journal={arXiv preprint arXiv:1808.01204},
  year={2018}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  journal={arXiv preprint arXiv:1702.08503},
  year={2017}
}

@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}

@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@article{chizat2018lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@inproceedings{yang2021tensor,
  title={Tensor programs iv: Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  booktitle={International Conference on Machine Learning},
  pages={11727--11737},
  year={2021},
  organization={PMLR}
}

@article{lyu2021gradient,
  title={Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{razin2022implicit,
  title={Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks},
  author={Razin, Noam and Maman, Asaf and Cohen, Nadav},
  journal={arXiv preprint arXiv:2201.11729},
  year={2022}
}

@article{jacot2021deep,
  title={Deep linear networks dynamics: Low-rank biases induced by initialization scale and l2 regularization},
  author={Jacot, Arthur and Ged, Fran{\c{c}}ois and Gabriel, Franck and {\c{S}}im{\c{s}}ek, Berfin and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:2106.15933},
  year={2021}
}

@article{stoger2021small,
  title={Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction},
  author={St{\"o}ger, Dominik and Soltanolkotabi, Mahdi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{ge2021understanding,
  title={Understanding Deflation Process in Over-parametrized Tensor Decomposition},
  author={Ge, Rong and Ren, Yunwei and Wang, Xiang and Zhou, Mo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{ji2020directional,
	author = {Ji, Ziwei and Telgarsky, Matus},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
	pages = {17176--17186},
	publisher = {Curran Associates, Inc.},
	title = {Directional convergence and alignment in deep learning},
	volume = {33},
	year = {2020}
}

@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{foote1984regularity,
  title={Regularity of the distance function},
  author={Foote, Robert L},
  journal={Proceedings of the American Mathematical Society},
  volume={92},
  number={1},
  pages={153--155},
  year={1984}
}

@article{ghai2022non,
  title={Non-convex online learning via algorithmic equivalence},
  author={Ghai, Udaya and Lu, Zhou and Hazan, Elad},
  journal={arXiv preprint arXiv:2205.15235},
  year={2022}
}@book{lang2006introduction,
  title={Introduction to differentiable manifolds},
  author={Lang, Serge},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{sussmann1973orbits,
  title={Orbits of families of vector fields and integrability of distributions},
  author={Sussmann, H{\'e}ctor J},
  journal={Transactions of the American Mathematical Society},
  volume={180},
  pages={171--188},
  year={1973}
}

@article{kiwiel1997free,
  title={Free-steering relaxation methods for problems with strictly convex costs and linear constraints},
  author={Kiwiel, Krzysztof C},
  journal={Mathematics of Operations Research},
  volume={22},
  number={2},
  pages={326--349},
  year={1997},
  publisher={INFORMS}
}

@article{censor1981iterative,
  title={An iterative row-action method for interval convex programming},
  author={Censor, Yair and Lent, Arnold},
  journal={Journal of Optimization theory and Applications},
  volume={34},
  number={3},
  pages={321--353},
  year={1981},
  publisher={Springer}
}

@article{bregman1967relaxation,
  title={The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
  author={Bregman, Lev M},
  journal={USSR computational mathematics and mathematical physics},
  volume={7},
  number={3},
  pages={200--217},
  year={1967},
  publisher={Elsevier}
}

@article{nash1954c1,
  title={C1 isometric imbeddings},
  author={Nash, John},
  journal={Annals of mathematics},
  pages={383--396},
  year={1954},
  publisher={JSTOR}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@inproceedings{gunther1991isometric,
  title={Isometric embeddings of Riemannian manifolds, Kyoto, 1990},
  author={Gunther, Matthias},
  booktitle={Proc. Intern. Congr. Math.},
  pages={1137--1143},
  year={1991},
  organization={Math. Soc. Japan}
}

@article{crouzeix1977relationship,
  title={A relationship between the second derivatives of a convex function and of its conjugate},
  author={Crouzeix, Jean-Pierre},
  journal={Mathematical Programming},
  volume={13},
  number={1},
  pages={364--365},
  year={1977},
  publisher={Springer}
}

@article{bauschke1997legendre,
  title={Legendre functions and the method of random Bregman projections},
  author={Bauschke, Heinz H and Borwein, Jonathan M and others},
  journal={Journal of convex analysis},
  volume={4},
  number={1},
  pages={27--67},
  year={1997},
  publisher={Citeseer}
}

@incollection{rockafellar2015convex,
  title={Convex analysis},
  author={Rockafellar, Ralph Tyrell},
  booktitle={Convex analysis},
  year={2015},
  publisher={Princeton university press}
}

@inproceedings{ji2021fast,
  title={Fast margin maximization via dual acceleration},
  author={Ji, Ziwei and Srebro, Nathan and Telgarsky, Matus},
  booktitle={International Conference on Machine Learning},
  pages={4860--4869},
  year={2021},
  organization={PMLR}
}

@article{qian2019implicit,
  title={The implicit bias of adagrad on separable data},
  author={Qian, Qian and Qian, Xiaoyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{razin2020implicit,
  title={Implicit regularization in deep learning may not be explainable by norms},
  author={Razin, Noam and Cohen, Nadav},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21174--21187},
  year={2020}
}


@inproceedings{nacson2019convergence,
  title={Convergence of gradient descent on separable data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019},
  organization={PMLR}
}


@article{yun2020unifying,
  title={A unifying view on implicit bias in training linear neural networks},
  author={Yun, Chulhee and Krishnan, Shankar and Mobahi, Hossein},
  journal={arXiv preprint arXiv:2010.02501},
  year={2020}
}

@inproceedings{li2022what,
title={What Happens after {SGD} Reaches Zero Loss? --A Mathematical Framework},
author={Zhiyuan Li and Tianhao Wang and Sanjeev Arora},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=siCt4xZn5Ve}
}

@article{nash1956imbedding,
  title={The imbedding problem for Riemannian manifolds},
  author={Nash, John},
  journal={Annals of mathematics},
  pages={20--63},
  year={1956},
  publisher={JSTOR}
}

@book{lee2018introduction,
  title={Introduction to Riemannian manifolds},
  author={Lee, John M},
  year={2018},
  publisher={Springer}
}

@book{lee2013smooth,
  title={Introduction to Smooth Manifolds},
  author={Lee, John M},
  year={2013},
  publisher={Springer}
}

@article{alvarez2004hessian,
  title={Hessian Riemannian gradient flows in convex programming},
  author={Alvarez, Felipe and Bolte, J{\'e}r{\^o}me and Brahic, Olivier},
  journal={SIAM journal on control and optimization},
  volume={43},
  number={2},
  pages={477--501},
  year={2004},
  publisher={SIAM}
}


@inproceedings{gunasekar2021mirrorless,
  title={Mirrorless mirror descent: A natural derivation of mirror descent},
  author={Gunasekar, Suriya and Woodworth, Blake and Srebro, Nathan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2305--2313},
  year={2021},
  organization={PMLR}
}

@inproceedings{azulay2021implicit,
  title={On the implicit bias of initialization shape: Beyond infinitesimal mirror descent},
  author={Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake E and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={468--477},
  year={2021},
  organization={PMLR}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@incollection{tropp2015convex,
  title={Convex recovery of a structured signal from independent random linear measurements},
  author={Tropp, Joel A},
  booktitle={Sampling Theory, a Renaissance},
  pages={67--101},
  year={2015},
  publisher={Springer}
}

@book{karatzas2014brownian,
  title={Brownian motion and stochastic calculus},
  author={Karatzas, Ioannis and Shreve, Steven},
  volume={113},
  year={2014},
  publisher={springer}
}

@book{billingsley2013convergence,
  title={Convergence of probability measures},
  author={Billingsley, Patrick},
  year={2013},
  publisher={John Wiley \& Sons}
}

@book{pollard2012convergence,
  title={Convergence of stochastic processes},
  author={Pollard, David},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{katzenberger1991solutions,
  title={Solutions of a stochastic differential equation forced onto a manifold by a large drift},
  author={Katzenberger, Gary Shon},
  journal={The Annals of Probability},
  pages={1587--1628},
  year={1991},
  publisher={JSTOR}
}

@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{haochen2020shape,
  title={Shape matters: Understanding the implicit bias of the noise covariance},
  author={HaoChen, Jeff Z and Wei, Colin and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:2006.08680},
  year={2020}
}

@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as sgd},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}

@article{kahn1995probability,
  title={On the probability that a random$\pm$1-matrix is singular},
  author={Kahn, Jeff and Koml{\'o}s, J{\'a}nos and Szemer{\'e}di, Endre},
  journal={Journal of the American Mathematical Society},
  volume={8},
  number={1},
  pages={223--240},
  year={1995}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@article{wu2017towards,
  title={Towards understanding generalization of deep learning: Perspective of loss landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}

@article{he2019asymmetric,
  title={Asymmetric valleys: Beyond sharp and flat local minima},
  author={He, Haowei and Huang, Gao and Yuan, Yang},
  journal={arXiv preprint arXiv:1902.00744},
  year={2019}
}

@article{cooper2020critical,
  title={The critical locus of overparameterized neural networks},
  author={Cooper, Y},
  journal={arXiv preprint arXiv:2005.04210},
  year={2020}
}

@article{li2018over,
  title={Over-parameterized deep neural networks have no strict local minima for any continuous activations},
  author={Li, Dawei and Ding, Tian and Sun, Ruoyu},
  journal={arXiv preprint arXiv:1812.11039},
  year={2018}
}

@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}

@inproceedings{geyer2020low,
  title={Low-rank regularization and solution uniqueness in over-parameterized matrix sensing},
  author={Geyer, Kelly and Kyrillidis, Anastasios and Kalev, Amir},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={930--940},
  year={2020},
  organization={PMLR}
}

@inproceedings{nguyen2019connected,
  title={On connected sublevel sets in deep learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019},
  organization={PMLR}
}

@article{venturi2018spurious,
  title={Spurious valleys in two-layer neural network optimization landscapes},
  author={Venturi, Luca and Bandeira, Afonso S and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  year={2018}
}

@article{kuditipudi2019explaining,
  title={Explaining landscape connectivity of low-cost solutions for multilayer nets},
  author={Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Arora, Sanjeev and Ge, Rong},
  journal={arXiv preprint arXiv:1906.06247},
  year={2019}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1802.10026},
  year={2018}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{nguyen2018loss,
  title={On the loss landscape of a class of deep neural networks with no bad local valleys},
  author={Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  journal={arXiv preprint arXiv:1809.10749},
  year={2018}
}

@article{freeman2016topology,
  title={Topology and geometry of half-rectified network optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={arXiv preprint arXiv:1611.01540},
  year={2016}
}

@inproceedings{liang2018understanding,
  title={Understanding the loss surface of neural networks for binary classification},
  author={Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={International Conference on Machine Learning},
  pages={2835--2843},
  year={2018},
  organization={PMLR}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@article{li2019generalization,
  title={On generalization error bounds of noisy gradient methods for non-convex learning},
  author={Li, Jian and Luo, Xuanyuan and Qiao, Mingda},
  journal={arXiv preprint arXiv:1902.00621},
  year={2019}
}

@article{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.04595},
  year={2019}
}

@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1705.08741},
  year={2017}
}

@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrzebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}


@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

@article{wen2019interplay,
  title={Interplay between optimization and generalization of stochastic gradient descent with covariance noise},
  author={Wen, Yeming and Luk, Kevin and Gazeau, Maxime and Zhang, Guodong and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:1902.08234},
  year={2019}
}

@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}

@inproceedings{cheng2020stochastic,
  title={Stochastic gradient and langevin processes},
  author={Cheng, Xiang and Yin, Dong and Bartlett, Peter and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={1810--1819},
  year={2020},
  organization={PMLR}
}

@article{li2020reconciling,
  title={Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate},
  author={Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{li2021validity,
  title={On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)},
  author={Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2102.12470},
  year={2021}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent escapes from sharp minima exponentially fast},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{fehrman2020convergence,
  title={Convergence rates for the stochastic gradient descent method for non-convex objective functions},
  author={Fehrman, Benjamin and Gess, Benjamin and Jentzen, Arnulf},
  journal={Journal of Machine Learning Research},
  volume={21},
  year={2020},
  publisher={MICROTOME PUBL}
}

@article{shi2020learning,
  title={On learning rates and schr$\backslash$" odinger operators},
  author={Shi, Bin and Su, Weijie J and Jordan, Michael I},
  journal={arXiv preprint arXiv:2004.06977},
  year={2020}
}

@book{whitt2002stochastic,
  title={Stochastic-process limits: an introduction to stochastic-process limits and their application to queues},
  author={Whitt, Ward},
  year={2002},
  publisher={Springer Science \& Business Media}
}

@article{Lee_Panageas_Piliouras_Simchowitz_Jordan_Recht_2019, title={First-order methods almost always avoid strict saddle points}, volume={176}, ISSN={0025-5610}, DOI={10.1007/s10107-019-01374-3}, abstractNote={We establish that first-order methods avoid strict saddle points for almost all initializations. Our results apply to a wide variety of first-order methods, including (manifold) gradient descent, block coordinate descent, mirror descent and variants thereof. The connecting thread is that such algorithms can be studied from a dynamical systems perspective in which appropriate instantiations of the Stable Manifold Theorem allow for a global stability analysis. Thus, neither access to second-order derivative information nor randomness beyond initialization is necessary to provably avoid strict saddle points.}, number={1–2}, journal={Mathematical Programming}, author={Lee, Jason D. and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin}, year={2019}, pages={311–337} }


@inproceedings{wei2019regularization,
 author = {Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
 url = {https://proceedings.neurips.cc/paper/2019/file/8744cf92c88433f8cb04a02e6db69a0d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{jacod2013limit,
  title={Limit theorems for stochastic processes},
  author={Jacod, Jean and Shiryaev, Albert},
  volume={288},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{Falconer_1983, 
title={Differentiation of the Limit Mapping in a Dynamical System}, volume={s2-27}, ISSN={0024-6107}, DOI={10.1112/jlms/s2-27.2.356}, number={2}, journal={Journal of the London Mathematical Society}, author={Falconer, K. J.}, year={1983}, pages={356–372} }


@article{damian2021label,
  title={Label Noise SGD Provably Prefers Flat Global Minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason},
  journal={arXiv preprint arXiv:2106.06530},
  year={2021}
}

@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1474--1520},
  year={2019},
  publisher={JMLR. org}
}

@book{do2013riemannian,
  title={Riemannian geometry},
  author={Do Carmo, Manfredo P},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{holbrook2018differentiating,
  title={Differentiating the pseudo determinant},
  author={Holbrook, Andrew},
  journal={Linear Algebra and its Applications},
  volume={548},
  pages={293--304},
  year={2018},
  publisher={Elsevier}
}

@book{banyaga2013lectures,
  title={Lectures on Morse homology},
  author={Banyaga, Augustin and Hurtubise, David},
  volume={29},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@article{li2020convolutional,
  title={Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?},
  author={Li, Zhiyuan and Zhang, Yi and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2010.08515},
  year={2020}
}

@inproceedings{ng2004feature,
  title={Feature selection, L 1 vs. L 2 regularization, and rotational invariance},
  author={Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={78},
  year={2004}
}

@article{lee2017first,
  title={First-order methods almost always avoid saddle points},
  author={Lee, Jason D and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={arXiv preprint arXiv:1710.07406},
  year={2017}
}

@inproceedings{lee2016gradient,
  title={Gradient descent only converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  booktitle={Conference on learning theory},
  pages={1246--1257},
  year={2016},
  organization={PMLR}
}

@article{polyak1964gradient,
  title={Gradient methods for solving equations and inequalities},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={6},
  pages={17--32},
  year={1964},
  publisher={Elsevier}
}

@article{lojasiewicz1963topological,
  title={A topological property of real analytic subsets},
  author={Lojasiewicz, Stanislaw},
  journal={Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
  volume={117},
  number={87-89},
  pages={2},
  year={1963}
}

@article{raskutti2012minimax,
  title={Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming.},
  author={Raskutti, Garvesh and J Wainwright, Martin and Yu, Bin},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}
@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@inproceedings{li2020towards,
  title={Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning},
  author={Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{vaskevicius2019implicit,
  title={Implicit regularization for optimal sparse recovery},
  author={Vaskevicius, Tomas and Kanade, Varun and Rebeschini, Patrick},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={2972--2983},
  year={2019}
}

@article{zhao2019implicit,
  title={Implicit regularization via Hadamard product over-parametrization in high-dimensional linear regression},
  author={Zhao, Peng and Yang, Yun and He, Qiao-Chu},
  journal={arXiv preprint arXiv:1903.09367},
  year={2019}
}

@article{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  journal={arXiv preprint arXiv:1808.01204},
  year={2018}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  journal={arXiv preprint arXiv:1702.08503},
  year={2017}
}

@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}
@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@article{chizat2018lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}
@article{oymak2020toward,
  title={Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={84--105},
  year={2020},
  publisher={IEEE}
}

@inproceedings{yang2021tensor,
  title={Tensor programs iv: Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  booktitle={International Conference on Machine Learning},
  pages={11727--11737},
  year={2021},
  organization={PMLR}
}

@article{lyu2021gradient,
  title={Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{razin2022implicit,
  title={Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks},
  author={Razin, Noam and Maman, Asaf and Cohen, Nadav},
  journal={arXiv preprint arXiv:2201.11729},
  year={2022}
}

@article{jacot2021deep,
  title={Deep linear networks dynamics: Low-rank biases induced by initialization scale and l2 regularization},
  author={Jacot, Arthur and Ged, Fran{\c{c}}ois and Gabriel, Franck and {\c{S}}im{\c{s}}ek, Berfin and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:2106.15933},
  year={2021}
}

@article{stoger2021small,
  title={Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction},
  author={St{\"o}ger, Dominik and Soltanolkotabi, Mahdi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{ge2021understanding,
  title={Understanding Deflation Process in Over-parametrized Tensor Decomposition},
  author={Ge, Rong and Ren, Yunwei and Wang, Xiang and Zhou, Mo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{foote1984regularity,
  title={Regularity of the distance function},
  author={Foote, Robert L},
  journal={Proceedings of the American Mathematical Society},
  volume={92},
  number={1},
  pages={153--155},
  year={1984}
}

@article{ghai2022non,
  title={Non-convex online learning via algorithmic equivalence},
  author={Ghai, Udaya and Lu, Zhou and Hazan, Elad},
  journal={arXiv preprint arXiv:2205.15235},
  year={2022}
}

@techreport{novikoff1963convergence,
  title={On convergence proofs for perceptrons},
  author={Novikoff, Albert B},
  year={1963},
  institution={STANFORD RESEARCH INST MENLO PARK CA}
}


@article{bartlett1996valid,
  title={For valid generalization the size of the weights is more important than the size of the network},
  author={Bartlett, Peter},
  journal={Advances in neural information processing systems},
  volume={9},
  year={1996}
}
@inproceedings{oymak2023role,
  title={On the Role of Attention in Prompt-tuning},
  author={Oymak, Samet and Rawat, Ankit Singh and Soltanolkotabi, Mahdi and Thrampoulidis, Christos},
  booktitle={International Conference on Machine Learning},
  year={2023}
}
@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}
@book{vapnik2006estimation,
  title={Estimation of dependences based on empirical data},
  author={Vapnik, Vladimir},
  year={2006},
  publisher={Springer Science \& Business Media}
}


@article{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}
@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}
@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020},
  publisher={MIT Press}
}
@article{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={The International Conference on Learning Representations},
  year={2015}
}
@article{mohammadi2019convergence,
  title={Convergence and sample complexity of gradient methods for the model-free linear quadratic regulator problem},
  author={Mohammadi, Hesameddin and Zare, Armin and Soltanolkotabi, Mahdi and Jovanovi{\'c}, Mihailo R},
  journal={arXiv preprint arXiv:1912.11899},
  year={2019}
}

@article{wei2021pretrained,
  title={Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning},
  author={Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16158--16170},
  year={2021}
}
@inproceedings{oymak2019stochastic,
  title={Stochastic gradient descent learns state equations with nonlinear activations},
  author={Oymak, Samet},
  booktitle={conference on Learning Theory},
  pages={2551--2579},
  year={2019},
  organization={PMLR}
}
@article{frei2022random,
  title={Random feature amplification: Feature learning and generalization in neural networks},
  author={Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2202.07626},
  year={2022}
}

@article{cao2022benign,
  title={Benign overfitting in two-layer convolutional neural networks},
  author={Cao, Yuan and Chen, Zixiang and Belkin, Mikhail and Gu, Quanquan},
  journal={arXiv preprint arXiv:2202.06526},
  year={2022}
}

@article{saharia2022photorealistic,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}

@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@inproceedings{Yun2020Are,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr}
}

@online{chatgpt,
  title={https://openai.com/blog/chatgpt/}
}


@article{edelman2021inductive,
  title={Inductive Biases and Variable Creation in Self-Attention Mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  journal={arXiv preprint arXiv:2110.10090},
  year={2021}
}


@article{baldi2022quarks,
  title={The Quarks of Attention},
  author={Baldi, Pierre and Vershynin, Roman},
  journal={arXiv:2202.08371},
  year={2022}
}

@book{ledoux1991probability,
  title={Probability in Banach Spaces: isoperimetry and processes},
  author={Ledoux, Michel and Talagrand, Michel},
  volume={23},
  year={1991},
  publisher={Springer Science \& Business Media}
}

@inproceedings{pollard1990empirical,
  title={Empirical processes: theory and applications},
  author={Pollard, David},
  year={1990},
  organization={Ims}
}

@inproceedings{
jelassi2022vision,
title={Vision Transformers provably learn spatial structure},
author={Samy Jelassi and Michael Eli Sander and Yuanzhi Li},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=eMW9AkXaREI}
}
@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@inproceedings{karp2021neurips,
 author = {Karp, Stefani and Winston, Ezra and Li, Yuanzhi and Singh, Aarti},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {24883--24897},
 publisher = {Curran Associates, Inc.},
 title = {Local Signal Adaptivity: Provable Feature Learning in Neural Networks Beyond Kernels},
 url = {https://proceedings.neurips.cc/paper/2021/file/d064bf1ad039ff366564f352226e7640-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{dosovitskiy2021vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@InProceedings{dehghani2021scenic,
    author    = {Dehghani, Mostafa and Gritsenko, Alexey and Arnab, Anurag and Minderer, Matthias and Tay, Yi},
    title     = {Scenic: A JAX Library for Computer Vision Research and Beyond},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2022},
    pages     = {21393-21398}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}


%%%
