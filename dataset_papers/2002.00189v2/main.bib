@inproceedings{azizan2018stochastic,
	title={Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization},
	author={Navid Azizan and Babak Hassibi},
	booktitle={International Conference on Learning Representations},
	year={2019},
}

@inproceedings{liang2015learning,
  title={Learning with square loss: Localization through offset rademacher
         complexity},
  author={Liang, Tengyuan and Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Conference on Learning Theory},
  pages={1260--1285},
  year={2015}
}

@article{shamir2015sample,
  title={The sample complexity of learning linear predictors with the squared
         loss},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={3475--3486},
  year={2015},
  publisher={JMLR. org}
}


@inproceedings{mendelson2014learning,
  title={Learning without concentration},
  author={Mendelson, Shahar},
  booktitle={Conference on Learning Theory},
  pages={25--39},
  year={2014}
}

@article{raskutti2014early,
  title={Early stopping and non-parametric regression: an optimal
         data-dependent stopping rule},
  author={Raskutti, Garvesh and Wainwright, Martin J and Yu, Bin},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={335--366},
  year={2014},
  publisher={JMLR. org}
}

@article{wei2019early,
  title={Early stopping for kernel boosting algorithms: A general analysis
         with localized complexities},
  author={Wei, Yuting and Yang, Fanny and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={10},
  pages={6685--6703},
  year={2019},
  publisher={IEEE}
}

@article{yao2007early,
  title={On early stopping in gradient descent learning},
  author={Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
  journal={Constructive Approximation},
  volume={26},
  number={2},
  pages={289--315},
  year={2007},
  publisher={Springer}
}


@article{bauer2007regularization,
  title={On regularization algorithms in learning theory},
  author={Bauer, Frank and Pereverzev, Sergei and Rosasco, Lorenzo},
  journal={Journal of complexity},
  volume={23},
  number={1},
  pages={52--72},
  year={2007},
  publisher={Academic Press}
}

@article{buhlmann2003boosting,
  title={Boosting with the {$\ell_2$} loss: regression and classification},
  author={B{\"u}hlmann, Peter and Yu, Bin},
  journal={Journal of the American Statistical Association},
  volume={98},
  number={462},
  pages={324--339},
  year={2003},
  publisher={Taylor \& Francis}
}

@article{matet2017don,
	title={Don't relax: early stopping for convex regularization},
	author={Matet, Simon and Rosasco, Lorenzo and Villa, Silvia and Vu, Bang Long},
	journal={arXiv preprint arXiv:1707.05422},
	year={2017}
}

@InProceedings{hardt16train,
	title = 	 {Train faster, generalize better: Stability of stochastic gradient descent},
	author = 	 {Moritz Hardt and Ben Recht and Yoram Singer},
	booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
	pages = 	 {1225--1234},
	year = 	 {2016},
	editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
	volume = 	 {48},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {New York, New York, USA},
	month = 	 {20--22 Jun},
	publisher = 	 {PMLR},
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for
         convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}


@book{nemirovsky1983problem,
  title={Problem complexity and method efficiency in optimization.},
  author={Nemirovsky, Arkadi{\u\i} and Yudin, David},
  publisher = {Wiley},
  address = {New York},
  year={1983}
}

@inproceedings{rosasco2015learning,
  title={Learning with incremental iterative regularization},
  author={Rosasco, Lorenzo and Villa, Silvia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1630--1638},
  year={2015}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@book{koltchinskii2011oracle,
  title={Oracle Inequalities in Empirical Risk Minimization and Sparse
         Recovery Problems: Ecole d’Et{\'e} de Probabilit{\'e}s de
         Saint-Flour XXXVIII-2008},
  author={Koltchinskii, Vladimir},
  volume={2033},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{bartlett2005local,
  title={Local rademacher complexities},
  author={Bartlett, Peter L and Bousquet, Olivier and Mendelson, Shahar},
  journal={The Annals of Statistics},
  volume={33},
  number={4},
  pages={1497--1537},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@article{ghai2019exponentiated,
  title={Exponentiated gradient meets gradient descent},
  author={Ghai, Udaya and Hazan, Elad and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01903},
  year={2019}
}


@inproceedings{ali2019continuous,
  title={A Continuous-Time View of Early Stopping for Least Squares
         Regression},
  author={Ali, Alnur and Kolter, J Zico and Tibshirani, Ryan J},
  booktitle={International Conference on Artificial Intelligence
             and Statistics},
  pages={1370--1378},
  year={2019}
}

@inproceedings{suggala2018connecting,
  title={Connecting optimization and regularization paths},
  author={Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10608--10619},
  year={2018}
}

@InProceedings{gunasekar2018characterizing,
	title = 	{Characterizing Implicit Bias in Terms of Optimization Geometry},
	author = 	{Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {1832--1841},
	year = 	 {2018},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
}

@article{woodworth2019kernel,
  title={Kernel and deep regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry,
          Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1906.05827},
  year={2019}
}


@inproceedings{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7411--7422},
  year={2019}
}


@inproceedings{vaskevicius2019implicit,
  title={Implicit Regularization for Optimal Sparse Recovery},
  author={Va\v{s}kevi\v{c}ius, Tomas and Kanade, Varun and Rebeschini, Patrick},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2968--2979},
  year={2019}
}

@inproceedings{gunasekar2017implicit,
    title={Implicit regularization in matrix factorization},
      author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli,
              Srinadh and Neyshabur, Behnam and Srebro, Nati},
    booktitle={Advances in Neural Information Processing Systems},
      pages={6151--6159},
        year={2017}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic Regularization in Over-parameterized Matrix Sensing and
         Neural Networks with Quadratic Activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018}
}

@article{zhao2019implicit,
  title={Implicit Regularization via {H}adamard Product Over-Parametrization in
         High-Dimensional Linear Regression},
  author={Zhao, Peng and Yang, Yun and He, Qiao-Chu},
  journal={arXiv preprint arXiv:1903.09367},
  year={2019}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{raskutti2011minimax,
  title={Minimax rates of estimation for high-dimensional linear regression
         over $l_q$ balls},
  author={Raskutti, Garvesh and Wainwright, Martin J and Yu, Bin},
  journal={IEEE transactions on information theory},
  volume={57},
  number={10},
  pages={6976--6994},
  year={2011},
  publisher={IEEE}
}


@book{scholkopf2001learning,
  title={Learning with kernels: support vector machines, regularization,
         optimization, and beyond},
  author={Scholkopf, Bernhard and Smola, Alexander J},
  year={2001},
  publisher={MIT press}
}

@article{kivinen1997exponentiated,
  title={Exponentiated gradient versus gradient descent for linear
         predictors},
  author={Kivinen, Jyrki and Warmuth, Manfred K},
  journal={information and computation},
  volume={132},
  number={1},
  pages={1--63},
  year={1997},
  publisher={Elsevier}
}

@article{gidel2019implicit,
  title={Implicit regularization of discrete gradient dynamics in deep linear
         neural networks},
  author={Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:1904.13262},
  year={2019}
}

@incollection{koltchinskii2000rademacher,
  title={Rademacher processes and bounding the risk of function learning},
  author={Koltchinskii, Vladimir and Panchenko, Dmitriy},
  booktitle={High dimensional probability II},
  pages={443--457},
  year={2000},
  publisher={Springer}
}

@article{koltchinskii2001rademacher,
  title={Rademacher penalties and structural risk minimization},
  author={Koltchinskii, Vladimir},
  journal={IEEE Transactions on Information Theory},
  volume={47},
  number={5},
  pages={1902--1914},
  year={2001},
  publisher={IEEE}
}


@inproceedings{massart2000some,
  title={Some applications of concentration inequalities to statistics},
  author={Massart, Pascal},
  booktitle={Annales de la Facult{\'e} des sciences de Toulouse:
             Math{\'e}matiques},
  volume={9},
  number={2},
  pages={245--303},
  year={2000}
}

@article{lugosi2004complexity,
  title={Complexity regularization via localized random penalties},
  author={Lugosi, G{\'a}bor and Wegkamp, Marten},
  journal={The Annals of Statistics},
  volume={32},
  number={4},
  pages={1679--1697},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{mendelson2002geometric,
  title={Geometric parameters of kernel machines},
  author={Mendelson, Shahar},
  booktitle={International Conference on Computational Learning Theory},
  pages={29--43},
  year={2002},
  organization={Springer}
}

@article{bartlett2006empirical,
  title={Empirical minimization},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Probability theory and related fields},
  volume={135},
  number={3},
  pages={311--334},
  year={2006},
  publisher={Springer}
}

@article{koltchinskii2006local,
  title={Local Rademacher complexities and oracle inequalities in risk minimization},
  author={Koltchinskii, Vladimir},
  journal={The Annals of Statistics},
  volume={34},
  number={6},
  pages={2593--2656},
  year={2006},
  publisher={Institute of Mathematical Statistics}
}

@article{mendelson2017aggregation,
  title={On aggregation for heavy-tailed classes},
  author={Mendelson, Shahar},
  journal={Probability Theory and Related Fields},
  volume={168},
  number={3-4},
  pages={641--674},
  year={2017},
  publisher={Springer}
}

% Topics in Learning Theory - Societe Mathematique de France, (S. Boucheron and N. Vayatis Eds.), to appear
@article{lecue2013learning,
  title={Learning subgaussian classes: Upper and minimax bounds},
  author={Lecu{\'e}, Guillaume and Mendelson, Shahar},
  journal={arXiv preprint arXiv:1305.4825},
  year={2013}
}

% To appear in "Studia Mathematica".
@article{mendelson2017extending,
  title={Extending the small-ball method},
  author={Mendelson, Shahar},
  journal={arXiv preprint arXiv:1709.00843},
  year={2017}
}

@article{mendelson2017local,
  title={“Local” vs.“global” parameters—breaking the Gaussian complexity barrier},
  author={Mendelson, Shahar},
  journal={The Annals of Statistics},
  volume={45},
  number={5},
  pages={1835--1862},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@article{zhang2005boosting,
  title={Boosting with early stopping: Convergence and consistency},
  author={Zhang, Tong and Yu, Bin},
  journal={The Annals of Statistics},
  volume={33},
  number={4},
  pages={1538--1579},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@article{bartlett2007adaboost,
  title={Adaboost is consistent},
  author={Bartlett, Peter L and Traskin, Mikhail},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={Oct},
  pages={2347--2368},
  year={2007}
}

@article{jiang2004process,
  title={Process consistency for adaboost},
  author={Jiang, Wenxin},
  journal={The Annals of Statistics},
  volume={32},
  number={1},
  pages={13--29},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}

@article{blanchard2018early,
  title={Early stopping for statistical inverse problems via truncated SVD estimation},
  author={Blanchard, Gilles and Hoffmann, Marc and Rei{\ss}, Markus and others},
  journal={Electronic Journal of Statistics},
  volume={12},
  number={2},
  pages={3204--3231},
  year={2018},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{blanchard2012discrepancy,
  title={Discrepancy principle for statistical inverse problems with application to conjugate gradient iteration},
  author={Blanchard, Gilles and Math{\'e}, Peter},
  journal={Inverse problems},
  volume={28},
  number={11},
  pages={115011},
  year={2012},
  publisher={IOP Publishing}
}

@article{bansal2019potential,
  title={Potential-Function Proofs for Gradient Methods},
  author={Bansal, Nikhil and Gupta, Anupam},
  journal={Theory of Computing},
  volume={15},
  number={1},
  pages={1--32},
  year={2019},
  publisher={Theory of Computing Exchange}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural
         results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@article{lugosi2016risk,
  title={Risk minimization by median-of-means tournaments},
  author={Lugosi, Gabor and Mendelson, Shahar},
  journal={arXiv preprint arXiv:1608.00757},
  year={2016}
}

@inproceedings{rakhlin2014online,
  title={Online non-parametric regression},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={Conference on Learning Theory},
  pages={1232--1264},
  year={2014}
}

@inproceedings{audibert2008progressive,
  title={Progressive mixture rules are deviation suboptimal},
  author={Audibert, Jean-Yves},
  booktitle={Advances in Neural Information Processing Systems},
  pages={41--48},
  year={2008}
}

@inproceedings{abernethy2009competing,
  title={Competing in the Dark: An Efficient Algorithm for Bandit Linear
         Optimization},
  author={Jacob D. Abernethy and Elad Hazan and Alexander Rakhlin},
  booktitle={COLT},
  year={2008}
}

@inproceedings{bubeck2018k,
  title={K-server via multiscale entropic regularization},
  author={Bubeck, S{\'e}bastien and Cohen, Michael B and Lee, Yin Tat and
          Lee, James R and M{\k{a}}dry, Aleksander},
  booktitle={Proceedings of the 50th annual ACM SIGACT symposium on theory of
             computing},
  pages={3--16},
  year={2018}
}

@article{hazan2016introduction,
  title={Introduction to Online Convex Optimization},
  author={Hazan, Elad},
  journal={Foundations and Trends in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@inproceedings{bubeck2019metrical,
  title={Metrical task systems on trees via mirror descent and unfair
         gluing},
  author={Bubeck, S{\'e}bastien and Cohen, Michael B and Lee, James R and
          Lee, Yin Tat},
  booktitle={Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
             Discrete Algorithms},
  pages={89--97},
  year={2019},
  organization={SIAM}
}

@inproceedings{pagliana2019implicit,
  title={Implicit Regularization of Accelerated Methods in Hilbert Spaces},
  author={Pagliana, Nicol{\`o} and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14454--14464},
  year={2019}
}

@article{ali2020implicit,
  title={The Implicit Regularization of Stochastic Gradient Flow for Least
         Squares},
  author={Ali, Alnur and Dobriban, Edgar and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:2003.07802},
  year={2020}
}

@inproceedings{lin2016generalization,
  title={Generalization properties and implicit regularization for multiple
         passes SGM},
  author={Lin, Junhong and Camoriano, Raffaello and Rosasco, Lorenzo},
  booktitle={International Conference on Machine Learning},
  pages={2340--2348},
  year={2016}
}

@article{neu2018iterate,
  title={Iterate averaging as regularization for stochastic gradient
         descent},
  author={Neu, Gergely and Rosasco, Lorenzo},
  journal={arXiv preprint arXiv:1802.08009},
  year={2018}
}

@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization
         algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{richards2020graph,
  title={Graph-Dependent Implicit Regularisation for Distributed Stochastic
         Subgradient Descent},
  author={Richards, Dominic and Rebeschini, Patrick},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={34},
  pages={1--44},
  year={2020},
  publisher={Journal of Machine Learning Research}
}

@inproceedings{richards2019optimal,
  title={Optimal Statistical Rates for Decentralised Non-Parametric
         Regression with Linear Speed-Up},
  author={Richards, Dominic and Rebeschini, Patrick},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1214--1225},
  year={2019}
}

@inproceedings{lin2018optimal,
    title={Optimal distributed learning with multi-pass stochastic gradient
           methods},
    author={Lin, Junhong and Cevher, Volkan},
      booktitle={Proceedings of the 35th International Conference on Machine
                 Learning},
    number={CONF},
      year={2018}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}

@article{wegkamp2003model,
  title={Model selection in nonparametric regression},
  author={Wegkamp, Marten and others},
  journal={The Annals of Statistics},
  volume={31},
  number={1},
  pages={252--273},
  year={2003},
  publisher={Institute of Mathematical Statistics}
}

@article{lecue2012oracle,
  title={Oracle inequalities for cross-validation type procedures},
  author={Lecu{\'e}, Guillaume and Mitchell, Charles and others},
  journal={Electronic Journal of Statistics},
  volume={6},
  pages={1803--1837},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the
             Bernoulli Society}
}

@article{zhivotovskiy2018localization,
  title={Localization of VC classes: Beyond local Rademacher complexities},
  author={Zhivotovskiy, Nikita and Hanneke, Steve},
  journal={Theoretical Computer Science},
  volume={742},
  pages={27--49},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{yang2019fast,
  title={Fast-rate PAC-Bayes Generalization Bounds via Shifted Rademacher
         Processes},
  author={Yang, Jun and Sun, Shengyang and Roy, Daniel M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10803--10813},
  year={2019}
}
