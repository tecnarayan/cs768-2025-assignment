\begin{thebibliography}{10}

\bibitem{bottou2009curiously}
L.~Bottou.
\newblock Curiously fast convergence of some stochastic gradient descent
  algorithms.
\newblock In {\em Proceedings of the symposium on learning and data science,
  Paris}, volume~8, pages 2624--2633, 2009.

\bibitem{bottou2018optimization}
L.~Bottou, F.~E. Curtis, and J.~Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em Siam Review}, 60(2):223--311, 2018.

\bibitem{boyd2011distributed}
S.~Boyd, N.~Parikh, and E.~Chu.
\newblock {\em Distributed optimization and statistical learning via the
  alternating direction method of multipliers}.
\newblock Now Publishers Inc, 2011.

\bibitem{chen2001atomic}
S.~S. Chen, D.~L. Donoho, and M.~A. Saunders.
\newblock Atomic decomposition by basis pursuit.
\newblock {\em SIAM review}, 43(1):129--159, 2001.

\bibitem{chow2017cyclic}
Y.~T. Chow, T.~Wu, and W.~Yin.
\newblock Cyclic coordinate-update algorithms for fixed-point problems:
  Analysis and applications.
\newblock {\em SIAM Journal on Scientific Computing}, 39(4):A1280--A1300, 2017.

\bibitem{defazio2014saga}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock {\em arXiv preprint arXiv:1407.0202}, 2014.

\bibitem{defazio2014finito}
A.~Defazio, J.~Domke, et~al.
\newblock Finito: A faster, permutable incremental gradient method for big data
  problems.
\newblock In {\em International Conference on Machine Learning}, pages
  1125--1133. PMLR, 2014.

\bibitem{deng2012mnist}
L.~Deng.
\newblock The mnist database of handwritten digit images for machine learning
  research.
\newblock {\em IEEE Signal Processing Magazine}, 29(6):141--142, 2012.

\bibitem{donoho2006compressed}
D.~L. Donoho.
\newblock Compressed sensing.
\newblock {\em IEEE Transactions on information theory}, 52(4):1289--1306,
  2006.

\bibitem{gurbuzbalaban2017convergence}
M.~Gurbuzbalaban, A.~Ozdaglar, and P.~A. Parrilo.
\newblock On the convergence rate of incremental aggregated gradient
  algorithms.
\newblock {\em SIAM Journal on Optimization}, 27(2):1035--1048, 2017.

\bibitem{gurbuzbalaban2019random}
M.~Gurbuzbalaban, A.~Ozdaglar, and P.~A. Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock {\em Mathematical Programming}, pages 1--36, 2019.

\bibitem{haochen2019random}
J.~Haochen and S.~Sra.
\newblock Random shuffling beats sgd after finite epochs.
\newblock In {\em International Conference on Machine Learning}, pages
  2624--2633. PMLR, 2019.

\bibitem{HUANG2020124211}
X.~Huang, E.~K. Ryu, and W.~Yin.
\newblock Tight coefficients of averaged operators via scaled relative graph.
\newblock {\em Journal of Mathematical Analysis and Applications},
  490(1):124211, 2020.

\bibitem{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock {\em Advances in neural information processing systems}, 26:315--323,
  2013.

\bibitem{Krizhevsky2009LearningML}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{ma2020understanding}
S.~Ma and Y.~Zhou.
\newblock Understanding the impact of model incoherence on convergence of
  incremental sgd with random reshuffle.
\newblock In {\em International Conference on Machine Learning}, pages
  6565--6574. PMLR, 2020.

\bibitem{mairal2015incremental}
J.~Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock {\em SIAM Journal on Optimization}, 25(2):829--855, 2015.

\bibitem{malinovsky2021random}
G.~Malinovsky, A.~Sailanbayev, and P.~Richt{\'a}rik.
\newblock Random reshuffling with variance reduction: New analysis and better
  rates.
\newblock {\em arXiv preprint arXiv:2104.09342}, 2021.

\bibitem{mao2018walk}
X.~Mao, Y.~Gu, and W.~Yin.
\newblock Walk proximal gradient: An energy-efficient algorithm for consensus
  optimization.
\newblock {\em IEEE Internet of Things Journal}, 6(2):2048--2060, 2018.

\bibitem{mateos2010distributed}
G.~Mateos, J.~A. Bazerque, and G.~B. Giannakis.
\newblock Distributed sparse linear regression.
\newblock {\em IEEE Transactions on Signal Processing}, 58(10):5262--5276,
  2010.

\bibitem{mishchenko2021proximal}
K.~Mishchenko, A.~Khaled, and P.~Richt{\'a}rik.
\newblock Proximal and federated random reshuffling.
\newblock {\em arXiv preprint arXiv:2102.06704}, 2021.

\bibitem{mishchenko2020random}
K.~Mishchenko, A.~Khaled Ragab~Bayoumi, and P.~Richt{\'a}rik.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{mokhtari2018surpassing}
A.~Mokhtari, M.~Gurbuzbalaban, and A.~Ribeiro.
\newblock Surpassing gradient descent provably: A cyclic incremental method
  with linear convergence rate.
\newblock {\em SIAM Journal on Optimization}, 28(2):1420--1447, 2018.

\bibitem{nagaraj2019sgd}
D.~Nagaraj, P.~Jain, and P.~Netrapalli.
\newblock Sgd without replacement: Sharper rates for general smooth convex
  functions.
\newblock In {\em International Conference on Machine Learning}, pages
  4703--4711. PMLR, 2019.

\bibitem{park2020linear}
Y.~Park and E.~K. Ryu.
\newblock Linear convergence of cyclic saga.
\newblock {\em Optimization Letters}, 14(6):1583--1598, 2020.

\bibitem{qian2019miso}
X.~Qian, A.~Sailanbayev, K.~Mishchenko, and P.~Richt{\'a}rik.
\newblock Miso is making a comeback with better proofs and rates.
\newblock {\em arXiv preprint arXiv:1906.01474}, 2019.

\bibitem{rajput2020closing}
S.~Rajput, A.~Gupta, and D.~Papailiopoulos.
\newblock Closing the convergence gap of sgd without replacement.
\newblock In {\em International Conference on Machine Learning}, pages
  7964--7973. PMLR, 2020.

\bibitem{robbins1951stochastic}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{nr}
R.~A. Rossi and N.~K. Ahmed.
\newblock The network data repository with interactive graph analytics and
  visualization.
\newblock In {\em AAAI}, 2015.

\bibitem{Ryu2019ScaledRG}
E.~K. Ryu, R.~Hannah, and W.~Yin.
\newblock Scaled relative graph: Nonexpansive operators via 2d euclidean
  geometry.
\newblock {\em arXiv: Optimization and Control}, 2019.

\bibitem{safran2020good}
I.~Safran and O.~Shamir.
\newblock How good is sgd with random shuffling?
\newblock In {\em Conference on Learning Theory}, pages 3250--3284. PMLR, 2020.

\bibitem{schmidt2017minimizing}
M.~Schmidt, N.~Le~Roux, and F.~Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, 162(1-2):83--112, 2017.

\bibitem{sun2019general}
T.~Sun, Y.~Sun, D.~Li, and Q.~Liao.
\newblock General proximal incremental aggregated gradient algorithms: Better
  and novel results under general scheme.
\newblock {\em Advances in Neural Information Processing Systems},
  32:996--1006, 2019.

\bibitem{tibshirani1996regression}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58(1):267--288, 1996.

\bibitem{Vanli2016ASC}
N.~D. Vanli, M.~G{\"u}rb{\"u}zbalaban, and A.~Ozdaglar.
\newblock A stronger convergence result on the proximal incremental aggregated
  gradient method.
\newblock {\em arXiv: Optimization and Control}, 2016.

\bibitem{vanli2018global}
N.~D. Vanli, M.~Gurbuzbalaban, and A.~Ozdaglar.
\newblock Global convergence rate of proximal incremental aggregated gradient
  methods.
\newblock {\em SIAM Journal on Optimization}, 28(2):1282--1300, 2018.

\bibitem{ying2020variance}
B.~Ying, K.~Yuan, and A.~H. Sayed.
\newblock Variance-reduced stochastic learning under random reshuffling.
\newblock {\em IEEE Transactions on Signal Processing}, 68:1390--1408, 2020.

\bibitem{ying2018stochastic}
B.~Ying, K.~Yuan, S.~Vlaski, and A.~H. Sayed.
\newblock Stochastic learning under random reshuffling with constant
  step-sizes.
\newblock {\em IEEE Transactions on Signal Processing}, 67(2):474--489, 2018.

\bibitem{yu2011dual}
H.-F. Yu, F.-L. Huang, and C.-J. Lin.
\newblock Dual coordinate descent methods for logistic regression and maximum
  entropy models.
\newblock {\em Machine Learning}, 85(1-2):41--75, 2011.

\bibitem{Yuan2016StochasticGD}
K.~Yuan, B.~Ying, S.~Vlaski, and A.~Sayed.
\newblock Stochastic gradient descent with finite samples sizes.
\newblock {\em 2016 IEEE 26th International Workshop on Machine Learning for
  Signal Processing (MLSP)}, pages 1--6, 2016.

\bibitem{zhao2015stochastic}
P.~Zhao and T.~Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em international conference on machine learning}, pages 1--9.
  PMLR, 2015.

\end{thebibliography}
