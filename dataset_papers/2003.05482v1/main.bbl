\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@rmstyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{Robbins1951}
H.~Robbins and S.~Monro, ``{A stochastic approximation method},'' \emph{The
  Annals of Statistics}, vol.~22, no.~3, pp. 400--407, 1951.

\bibitem{Pasupathy2011}
R.~Pasupathy, V.~Tech, and S.~Kim, ``{The Stochastic Root Finding Problem:
  Overview, Solutions, and Open Questions},'' \emph{ACM Transactions on
  Modeling and Computational Simulations}, vol.~21, no.~3, p.~19, 2011.

\bibitem{Ruder2016}
S.~Ruder, ``An overview of gradient descent optimization algorithms,''
  \emph{ArXiv}, vol. abs/1609.04747, 2016.

\bibitem{Bottou2018}
L.~Bottou, F.~E. Curtis, and J.~Nocedal, ``{Optimization methods for
  large-scale machine learning},'' \emph{SIAM Review}, vol.~60, no.~2, pp.
  223--311, 2018.

\bibitem{Wright2015}
S.~J. Wright, ``{Coordinate descent algorithms},'' \emph{Mathematical
  Programming}, vol. 151, no.~1, pp. 3--34, 2015.

\bibitem{Beck2013}
A.~Beck and L.~Tetruashvili, ``{On the convergence of block coordinate descent
  type methods},'' \emph{SIAM Journal on Optimization}, vol.~23, no.~4, pp.
  2037--2060, 2013.

\bibitem{Tibshirani2013}
\BIBentryALTinterwordspacing
R.~Tibshirani, ``{Coordinate descent},'' pp. 1--28, 2013. [Online]. Available:
  \url{https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/coord-desc.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Vakili2019a}
S.~{Vakili} and Q.~{Zhao}, ``A random walk approach to first-order stochastic
  convex optimization,'' in \emph{2019 IEEE International Symposium on
  Information Theory (ISIT)}, July 2019, pp. 395--399.

\bibitem{Vakili2019b}
S.~Vakili, S.~Salgia, and Q.~Zhao, ``{Stochastic Gradient Descent on a Tree: An
  Adaptive and Robust Approach to Stochastic Convex Optimization},'' in
  \emph{2019 57th Annual Allerton Conference on Communication, Control, and
  Computing, Allerton 2019}, 2019, pp. 432--438.

\bibitem{Hsieh2008a}
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.~S. Keerthi, and S.~Sundararajan,
  ``{Coordinate Descent Method for Large-scale L2-loss Linear Support Vector
  Machines},'' \emph{Journal of Machine Learning Research}, vol.~9, pp.
  1369--1398, 2008.

\bibitem{Hsieh2008b}
C.~J. Hsieh, K.~W. Chang, C.~J. Lin, S.~S. Keerthi, and S.~Sundararajan, ``{A
  dual coordinate descent method for large-scale linear SVM},'' in
  \emph{Proceedings of the 25th International Conference on Machine Learning},
  2008, pp. 408--415.

\bibitem{Nesterov2012}
Y.~Nesterov, ``{Efficiency of coordinate descent methods on huge-scale
  optimization problems},'' \emph{SIAM Journal on Optimization}, vol.~22,
  no.~2, pp. 341--362, 2012.

\bibitem{Nesterov2014}
Y.~Nesterov, ``{Subgradient methods for huge-scale optimization problems},''
  \emph{Mathematical Programming}, vol. 146, no. 1-2, pp. 275--297, 2014.

\bibitem{Richtarik2016a}
P.~Richt{\'{a}}rik and M.~Tak{\'{a}}{\v{c}}, ``{Parallel coordinate descent
  methods for big data optimization},'' \emph{Mathematical Programming}, vol.
  156, no. 1-2, pp. 433--484, 2016.

\bibitem{Richtarik2016b}
P.~Richt{\'{a}}rik and M.~Tak{\'{a}}{\v{c}}, ``{Distributed coordinate descent
  method for learning with big data},'' \emph{Journal of Machine Learning
  Research}, vol.~17, pp. 1--25, 2016.

\bibitem{Fercoq2019}
O.~Fercoq and P.~Richt{\'{a}}rik, ``{Smooth Minimization of Nonsmooth Functions
  with Parallel Coordinate Descent Methods},'' in \emph{Springer Proceedings in
  Mathematics and Statistics}, vol. 279, 2019, pp. 57--96.

\bibitem{Luo1992}
Z.~Q. Luo and P.~Tseng, ``{On the Convergence of the Coordinate Descent Method
  for Convex Differentiable Minimization},'' \emph{Journal of Optimization
  Theory and Applications}, vol.~72, no.~1, pp. 7--35, 1992.

\bibitem{Tseng2001}
P.~Tseng, ``{Convergence of a block coordinate descent method for
  nondifferentiable minimization},'' \emph{Journal of Optimization Theory and
  Applications}, vol. 109, no.~3, pp. 475--494, 2001.

\bibitem{Tseng2008}
P.~Tseng and S.~Yun, ``Block-coordinate gradient descent method for linearly
  constrained nonsmooth separable optimization,'' \emph{Journal of Optimization
  Theory and Applications}, vol. 140, no.~3, p. 513, Sep 2008.

\bibitem{Tseng2009}
P.~Tseng and S.~Yun, ``{A coordinate gradient descent method for nonsmooth
  separable minimization},'' \emph{Mathematical Programming}, vol. 117, no.
  1-2, pp. 387--423, 2009.

\bibitem{Saha2013}
A.~Saha and A.~Tewari, ``{On the finite time convergence of cyclic coordinate
  descent methods},'' \emph{SIAM Journal on Optimization}, vol.~23, no.~1, pp.
  576--601, 2013.

\bibitem{Leventhal2010}
D.~Leventhal and A.~S. Lewis, ``{Randomized methods for linear constraints:
  Convergence rates and conditioning},'' \emph{Mathematics of Operations
  Research}, vol.~35, no.~3, pp. 641--654, 2010.

\bibitem{Tewari2011}
A.~Tewari and S.~Shalev-Shwartz, ``{Stochastic Methods for l1-regularized Loss
  Minimization},'' \emph{Journal of Machine Learning Research}, vol.~12, pp.
  1865--1892, 2011.

\bibitem{Tao2012}
Q.~Tao, K.~Kong, D.~Chu, and G.~Wu, ``{Stochastic coordinate descent methods
  for regularized smooth and nonsmooth losses},'' in \emph{Lecture Notes in
  Computer Science (including subseries Lecture Notes in Artificial
  Intelligence and Lecture Notes in Bioinformatics)}, vol. 7523, 2012, pp.
  537--552.

\bibitem{Deng2013}
Q.~Deng, J.~Ho, and A.~Rangarajan, ``Stochastic coordinate descent for
  nonsmooth convex optimization,'' 12 2013.

\bibitem{ShalevShwartz14}
S.~Shalev-Shwartz and T.~Zhang, ``Accelerated proximal stochastic dual
  coordinate ascent for regularized loss minimization,'' in \emph{Proceedings
  of the 31st International Conference on Machine Learning}, ser. Proceedings
  of Machine Learning Research, E.~P. Xing and T.~Jebara, Eds., vol.~32.\hskip
  1em plus 0.5em minus 0.4em\relax Beijing, China: PMLR, 22--24 Jun 2014, pp.
  64--72.

\bibitem{Csiba2015}
D.~Csiba, Z.~Qu, and P.~Richtarik, ``{Stochastic dual coordinate ascent with
  adaptive probabilities},'' in \emph{32nd International Conference on Machine
  Learning, ICML 2015}, vol.~1, 2015, pp. 674--683.

\bibitem{Karimi2016}
H.~Karimi, J.~Nutini, and M.~Schmidt, ``{Linear convergence of gradient and
  proximal-gradient methods under the Polyak-{\L}ojasiewicz condition},'' in
  \emph{Lecture Notes in Computer Science (including subseries Lecture Notes in
  Artificial Intelligence and Lecture Notes in Bioinformatics)}, vol. 9851
  LNAI, 2016, pp. 795--811.

\bibitem{Salehi2018}
F.~Salehi, P.~Thiran, and L.~{Elisa Celis}, ``{Coordinate descent with bandit
  sampling},'' in \emph{Advances in Neural Information Processing
  Systems}.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates, Inc.,
  2018, pp. 9247--9257.

\bibitem{Richtarik2014}
P.~Richt{\'{a}}rik and M.~Tak{\'{a}}{\v{c}}, ``{Iteration complexity of
  randomized block-coordinate descent methods for minimizing a composite
  function},'' \emph{Mathematical Programming}, vol. 144, no. 1-2, pp. 1--38,
  2014.

\bibitem{Lu2015}
Z.~Lu and L.~Xiao, ``{On the complexity analysis of randomized block-coordinate
  descent methods},'' \emph{Mathematical Programming}, vol. 152, no. 1-2, pp.
  615--642, 2015.

\bibitem{Razaviyayn2013}
M.~Razaviyayn, M.~Hong, and Z.~Q. Luo, ``{A unified convergence analysis of
  block successive minimization methods for nonsmooth optimization},''
  \emph{SIAM Journal on Optimization}, vol.~23, no.~2, pp. 1126--1153, 2013.

\bibitem{Wang2014}
H.~Wang and A.~Banerjee, ``{Randomized Block Coordinate Descent for Online and
  Stochastic Optimization},'' 2014.

\bibitem{Zhao2014}
T.~Zhao, M.~Yu, Y.~Wang, R.~Arora, and H.~Liu, ``{Accelerated mini-batch
  randomized block coordinate descent method},'' in \emph{Advances in Neural
  Information Processing Systems}, vol.~4, 2014, pp. 3329--3337.

\bibitem{Dang2015}
C.~D. Dang and G.~Lan, ``{Stochastic block mirror descent methods for nonsmooth
  and stochastic optimization},'' \emph{SIAM Journal on Optimization}, vol.~25,
  no.~2, pp. 856--881, 2015.

\bibitem{Reddi2015}
S.~J. Reddi, A.~Hefny, C.~Downey, A.~Dubey, and S.~Sra, ``{Large-scale
  randomized-coordinate descent methods with non-separable linear
  constraints},'' in \emph{Uncertainty in Artificial Intelligence - Proceedings
  of the 31st Conference, UAI 2015}, 2015, pp. 762--771.

\bibitem{Xu2015}
Y.~Xu and W.~Yin, ``{Block stochastic gradient iteration for convex and
  nonconvex optimization},'' \emph{SIAM Journal on Optimization}, vol.~25,
  no.~3, pp. 1686--1716, 2015.

\bibitem{Zhang2016}
A.~Zhang and Q.~Gu, ``{Accelerated stochastic block coordinate descent with
  optimal sampling},'' in \emph{Proceedings of the ACM SIGKDD International
  Conference on Knowledge Discovery and Data Mining}, vol. 13-17-Augu, 2016,
  pp. 2035--2044.

\bibitem{Konecny2016}
J.~{Konečný}, J.~{Liu}, P.~{Richtárik}, and M.~{Takáč}, ``Mini-batch
  semi-stochastic gradient descent in the proximal setting,'' \emph{IEEE
  Journal of Selected Topics in Signal Processing}, vol.~10, no.~2, pp.
  242--255, March 2016.

\bibitem{Grippo1999}
L.~Grippo and M.~Sciandrone, ``{Globally convergent block-coordinate techniques
  for unconstrained optimization},'' \emph{Optimization Methods and Software},
  vol.~10, no.~4, pp. 587--637, 1999.

\bibitem{Tappenden2016}
R.~Tappenden, P.~Richt{\'{a}}rik, and J.~Gondzio, ``{Inexact Coordinate
  Descent: Complexity and Preconditioning},'' \emph{Journal of Optimization
  Theory and Applications}, vol. 170, no.~1, pp. 144--176, 2016.

\bibitem{Hannan1957}
J.~Hannan, ``Approximation to rayes risk in repeated play,''
  \emph{Contributions to the Theory of Games}, vol.~3, pp. 97--139, 1957.

\bibitem{Bradley2011}
J.~K. Bradley, A.~Kyrola, D.~Bickson, and C.~Guestrin, ``{Parallel coordinate
  descent for L1-regularized loss minimization},'' in \emph{Proceedings of the
  28th International Conference on Machine Learning, ICML 2011}, 2011, pp.
  321--328.

\bibitem{Peng2013}
Z.~Peng, M.~Yan, and W.~Yin, ``{Parallel and distributed sparse
  optimization},'' in \emph{Asilomar Conference on Signals, Systems and
  Computers}, 2013, pp. 659--664.

\bibitem{Liu2015}
J.~Liu, S.~J. Wright, C.~R{\'{e}}, and S.~Sridhar, ``{An Asynchronous Parallel
  Stochastic Coordinate Descent Algorithm},'' \emph{Journal of Machine Learning
  Research}, vol.~16, pp. 285--322, 2015.

\bibitem{Marecek2015}
J.~Mare{\v{c}}ek, P.~Richt{\'{a}}rik, and M.~Tak{\'{a}}{\v{c}}, ``{Distributed
  block coordinate descent for minimizing partially separable functions},'' in
  \emph{Springer Proceedings in Mathematics and Statistics}, vol. 134, 2015,
  pp. 261--288.

\bibitem{Richtarik2016c}
P.~Richt{\'a}rik and M.~Tak{\'a}{\v{c}}, ``On optimal probabilities in
  stochastic coordinate descent methods,'' \emph{Optimization Letters},
  vol.~10, no.~6, pp. 1233--1243, Aug 2016.

\bibitem{Ferris1994}
M.~C. Ferris and O.~L. Mangasarian, ``{Parallel Variable Distribution},''
  \emph{SIAM Journal on Optimization}, vol.~4, no.~4, pp. 815--832, 1994.

\bibitem{Zhang2019}
J.~Zhang, S.~P. Karimireddy, A.~Veit, S.~Kim, S.~J. Reddi, S.~Kumar, and
  S.~Sra, ``{Why ADAM Beats SGD for Attention Models},'' 2019.

\bibitem{LeCunn1998}
Y.~{Lecun}, L.~{Bottou}, Y.~{Bengio}, and P.~{Haffner}, ``Gradient-based
  learning applied to document recognition,'' \emph{Proceedings of the IEEE},
  vol.~86, no.~11, pp. 2278--2324, Nov 1998.

\bibitem{Wang2018}
C.~Wang, K.~Cohen, and Q.~Zhao, ``{Information-Directed Random Walk for Rare
  Event Detection in Hierarchical Processes},'' 2018.

\end{thebibliography}
