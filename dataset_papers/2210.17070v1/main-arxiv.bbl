\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WXDX20}

\bibitem[ACCD20]{AsiChChDu20}
Hilal Asi, Karan Chadha, Gary Cheng, and John~C. Duchi.
\newblock Minibatch stochastic approximate proximal point methods.
\newblock In {\em Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[ACG{\etalchar{+}}16]{AbadiChGoMcMiTaZh16}
Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In {\em 23rd ACM Conference on Computer and Communications Security
  (ACM CCS)}, pages 308--318, 2016.

\bibitem[AD19]{AsiDu19siopt}
Hilal Asi and John~C. Duchi.
\newblock Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity.
\newblock {\em SIAM Journal on Optimization}, 29(3):2257--2290, 2019.

\bibitem[AD20]{AsiDu20}
Hilal Asi and John Duchi.
\newblock Near instance-optimality in differential privacy.
\newblock {\em arXiv:2005.10630 [cs.CR]}, 2020.

\bibitem[ADF{\etalchar{+}}21]{AsiDuFaJaTa21}
Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar.
\newblock Private adaptive gradient methods for convex optimization.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, pages 383--392, 2021.

\bibitem[AFKT21]{AsiFeKoTa21}
Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar.
\newblock Private stochastic convex optimization: Optimal rates in {$\ell_1$}
  geometry.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, 2021.

\bibitem[ALD21]{AsiLeDu21}
Hilal Asi, Daniel Levy, and John~C. Duchi.
\newblock Adapting to function difficulty and growth conditions in private
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems 34}, 2021.

\bibitem[BFGT20]{BassilyFeGuTa20}
Raef Bassily, Vitaly Feldman, Crist\'obal Guzm\'an, and Kunal Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock In {\em Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[BFTT19]{BassilyFeTaTh19}
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta.
\newblock Private stochastic convex optimization with optimal rates.
\newblock In {\em Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[BGN21]{BassilyGuNa21}
Raef Bassily, Cristóbal Guzmán, and Anupama Nandi.
\newblock Non-euclidean differentially private stochastic convex optimization.
\newblock In {\em Proceedings of the Thirty Fourth Annual Conference on
  Computational Learning Theory}, pages 474--499, 2021.

\bibitem[BHM18]{BelkinHsMi18}
Mikhail Belkin, Daniel Hsu, and Partha Mitra.
\newblock Overfitting or perfect fitting? {R}isk bounds for classification and
  regression rules that interpolate.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  2300--2311. Curran Associates, Inc., 2018.

\bibitem[BRT19]{BelkinRaTs19}
Mikhail Belkin, Alexander Rakhlin, and Alexandre~B. Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock In {\em Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics}, pages 1611--1619, 2019.

\bibitem[BST14]{BassilySmTh14}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: {E}fficient algorithms and tight
  error bounds.
\newblock In {\em 55th Annual Symposium on Foundations of Computer Science},
  pages 464--473, 2014.

\bibitem[CCD22]{ChadhaChDu22}
Karan Chadha, Gary Cheng, and John Duchi.
\newblock Accelerated, optimal and parallel: Some results on model-based
  stochastic optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  2811--2827. PMLR, 2022.

\bibitem[CMS11]{ChaudhuriMoSa11}
Kamalika Chaudhuri, Claire Monteleoni, and Anand~D. Sarwate.
\newblock Differentially private empirical risk minimization.
\newblock {\em Journal of Machine Learning Research}, 12:1069--1109, 2011.

\bibitem[CSSS11]{CotterShSrSr11}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems 24}, 2011.

\bibitem[DJW13]{DuchiJoWa13_focs}
John~C.\ Duchi, Michael~I.\ Jordan, and Martin~J.\ Wainwright.
\newblock Local privacy and statistical minimax rates.
\newblock In {\em 54th Annual Symposium on Foundations of Computer Science},
  pages 429--438, 2013.

\bibitem[DR14]{DworkRo14}
Cynthia Dwork and Aaron Roth.
\newblock The algorithmic foundations of differential privacy.
\newblock {\em Foundations and Trends in Theoretical Computer Science}, 9(3 \&
  4):211--407, 2014.

\bibitem[Duc18]{Duchi18}
John~C. Duchi.
\newblock Introductory lectures on stochastic convex optimization.
\newblock In {\em The Mathematics of Data}, IAS/Park City Mathematics Series.
  American Mathematical Society, 2018.

\bibitem[FKT20]{FeldmanKoTa20}
Vitaly Feldman, Tomer Koren, and Kunal Talwar.
\newblock Private stochastic convex optimization: Optimal rates in linear time.
\newblock In {\em Proceedings of the Fifty-Second Annual ACM Symposium on the
  Theory of Computing}, 2020.

\bibitem[HUL93]{HiriartUrrutyLe93ab}
J.~Hiriart-Urruty and C.~Lemar\'echal.
\newblock {\em Convex {A}nalysis and {M}inimization {A}lgorithms {I} \& {II}}.
\newblock Springer, New York, 1993.

\bibitem[LB20]{LiuBe20}
Chaoyue Liu and Mikhail Belkin.
\newblock Accelerating sgd with momentum for over-parameterized learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[MBB18]{MaBaBe18}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of {SGD}
  in modern over-parametrized learning.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[NWS14]{NeedellWaSr14}
Deanna Needell, Rachel Ward, and Nati Srebro.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  {K}aczmarz algorithm.
\newblock In {\em Advances in Neural Information Processing Systems 27}, pages
  1017--1025, 2014.

\bibitem[SSSSS09]{ShalevShSrSr09}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Stochastic convex optimization.
\newblock In {\em Proceedings of the Twenty Second Annual Conference on
  Computational Learning Theory}, 2009.

\bibitem[SST10]{SrebroSrTe10}
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock Smoothness, low noise and fast rates.
\newblock In {\em nips2010}, pages 2199--2207, 2010.

\bibitem[ST13]{SmithTh13lasso}
Adam Smith and Abhradeep Thakurta.
\newblock Differentially private feature selection via stability arguments, and
  the robustness of the {L}asso.
\newblock In {\em Proceedings of the Twenty Sixth Annual Conference on
  Computational Learning Theory}, pages 819--850, 2013.

\bibitem[SV09]{StrohmerVe09}
T.~Strohmer and Roman Vershynin.
\newblock A randomized {K}aczmarz algorithm with exponential convergence.
\newblock {\em Journal of Fourier Analysis and Applications}, 15(2):262--278,
  2009.

\bibitem[VBS19]{VaswaniBaSc19}
Sharan Vaswani, Francis Bach, and Mark Schmidt.
\newblock Fast and faster convergence of {SGD} for over-parameterized models
  and an accelerated perceptron.
\newblock In {\em Proceedings of the 22nd International Conference on
  Artificial Intelligence and Statistics}, 2019.

\bibitem[Wai19]{Wainwright19}
Martin~J. Wainwright.
\newblock {\em High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge University Press, 2019.

\bibitem[WS21]{WoodworthSr21}
Blake~E. Woodworth and Nathan Srebro.
\newblock An even more optimal stochastic optimization algorithm: Minibatching
  and interpolation learning.
\newblock In {\em Advances in Neural Information Processing Systems 34}, 2021.

\bibitem[WXDX20]{WangXiDeXu20}
Di~Wang, Hanshen Xiao, Srini Devadas, and Jinhui Xu.
\newblock Private stochastion differentially private stochastic convex
  optimization with heavy-tailed data.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\end{thebibliography}
