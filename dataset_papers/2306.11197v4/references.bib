@article{amer2019review,
  title={A review of modularization techniques in artificial neural networks},
  author={Amer, Mohammed and Maul, Tom{\'a}s},
  journal={Artificial Intelligence Review},
  volume={52},
  pages={527--561},
  year={2019},
  publisher={Springer}
}
@article{jaegle2021perceiver,
  title     = {Perceiver: General Perception with Iterative Attention},
  author    = {Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and João Carreira},
  journal   = {International Conference On Machine Learning},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/63a9daf15ae2d4c1a7859d3105c9e6710903e072},
  url       = {https://arxiv.org/abs/2103.03206v2},
  pdf       = {https://arxiv.org/pdf/2103.03206.pdf}
}
@article{goyal2019recurrent,
  title     = {Recurrent Independent Mechanisms},
  author    = {Anirudh Goyal and Alex Lamb and Jordan Hoffmann and Shagun Sodhani and S. Levine and Yoshua Bengio and B. Scholkopf},
  journal   = {International Conference on Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/67a9dde04f367efc903b6d06097df9bdd9887ae7},
  url       = {https://arxiv.org/abs/1909.10893v6},
  pdf       = {https://arxiv.org/pdf/1909.10893.pdf}
}
@article{radam,
  title     = {On the Variance of the Adaptive Learning Rate and Beyond},
  author    = {Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
  journal   = {International Conference on Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/2bf7c350a8280e7c593d46a60127f99b21517121},
  url       = {https://arxiv.org/abs/1908.03265v4},
  pdf       = {https://arxiv.org/pdf/1908.03265.pdf}
}
@inproceedings{s5,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@article{su2021roformer,
  title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2104.09864}
}
@article{baevski2020wav2vec,
  title   = {wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author  = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {12449-12460},
  year    = {2020}
}

@inproceedings{Seong2018TowardsFL,
  title={Towards Flatter Loss Surface via Nonmonotonic Learning Rate Scheduling},
  author={Sihyeon Seong and Yegang Lee and Youngwook Kee and Dongyoon Han and Junmo Kim},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  year={2018}
}

@article{shaw2018selfattention,
  title   = {Self-Attention with Relative Position Representations},
  author  = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year    = {2018},
  journal = {NAACL}
}

@inproceedings{gsoft,
  author    = {Eric Jang and Shixiang Gu and Ben Poole},
  title     = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=rkE3y85ee},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{moe,
  journal = {Neural Computation},
  volume  = {3},
  pages   = {79-87},
  doi     = {10.1162/neco.1991.3.1.79},
  title   = {Adaptive Mixtures of Local Experts},
  year    = {1991},
  author  = {Robert A. Jacobs and Michael I. Jordan and Steven J. Nowlan and Geoffrey E. Hinton}
}
@article{dai2019transformerxl,
  title   = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
  author  = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  year    = {2019},
  journal = {ACL}
}
@article{lepikhin2020gshard,
  title     = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
  author    = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and M. Krikun and Noam M. Shazeer and Z. Chen},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/1882f194cb43828852cc052887671e55a80f945a}
}

@article{fedus2022switch,
  title     = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author    = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal   = {The Journal of Machine Learning Research},
  volume    = {23},
  number    = {1},
  pages     = {5232-5270},
  year      = {2022},
  publisher = {JMLRORG}
}

@article{zhou2022mixture,
  title   = {Mixture-of-experts with expert choice routing},
  author  = {Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {7103-7114},
  year    = {2022}
}

@article{roy2020efficient,
  title     = {Efficient Content-Based Sparse Attention with Routing Transformers},
  author    = {Aurko Roy and M. Saffar and Ashish Vaswani and David Grangier},
  journal   = {International Conference On Topology, Algebra And Categories In Logic},
  year      = {2020},
  doi       = {10.1162/tacl_a_00353},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2}
}

@article{hua2022transformer,
  title     = {Transformer Quality in Linear Time},
  author    = {Weizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le},
  journal   = {International Conference On Machine Learning},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/dc0102a51a9d33e104a4a3808a18cf17f057228c}
}

@article{act,
  title     = {Adaptive Computation Time for Recurrent Neural Networks},
  author    = {A. Graves},
  journal   = {ARXIV.ORG},
  year      = {2016},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/04cca8e341a5da42b29b0bc831cb25a0f784fa01}
}

@article{expire,
  title     = {Not All Memories are Created Equal: Learning to Forget by Expiring},
  author    = {Sainbayar Sukhbaatar and Da Ju and Spencer Poff and Stephen Roller and Arthur D. Szlam and J. Weston and Angela Fan},
  journal   = {International Conference On Machine Learning},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/64a29bee2e1ad29547d590a3cc26274f4c537145}
}

@inproceedings{ren-etal-2022-language,
  title     = {Language Model Pre-Training with Sparse Latent Typing},
  author    = {Ren, Liliang and Zhang, Zixuan and Wang, Han and Voss, Clare and Zhai, ChengXiang and Ji, Heng},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = {dec},
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.96},
  pages     = {1480-1494},
  abstract  = {Modern large-scale Pre-trained Language Models (PLMs) have achieved tremendous success on a wide range of downstream tasks. However, most of the LM pre-training objectives only focus on text reconstruction, but have not sought to learn latent-level interpretable representations of sentences. In this paper, we manage to push the language models to obtain a deeper understanding of sentences by proposing a new pre-training objective, Sparse Latent Typing, which enables the model to sparsely extract sentence-level keywords with diverse latent types. Experimental results show that our model is able to learn interpretable latent type categories in a self-supervised manner without using any external knowledge. Besides, the language model pre-trained with such an objective also significantly improves Information Extraction related downstream tasks in both supervised and few-shot settings. Our code is publicly available at https://github.com/renll/SparseLT.}
}
@article{barrett2020implicit,
  title     = {Implicit Gradient Regularization},
  author    = {D. Barrett and B. Dherin},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/060eb1ad5da6a059320c244532ad5c9c0ab52485}
}

@inproceedings{guan-etal-2022-transkimmer,
  title     = {Transkimmer: Transformer Learns to Layer-wise Skim},
  author    = {Guan, Yue and Li, Zhengyi and Leng, Jingwen and Lin, Zhouhan and Guo, Minyi},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {may},
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-long.502},
  doi       = {10.18653/v1/2022.acl-long.502},
  pages     = {7275-7286},
  abstract  = {Transformer architecture has become the de-facto model for many machine learning tasks from natural language processing and computer vision. As such, improving its computational efficiency becomes paramount. One of the major computational inefficiency of Transformer based models is that they spend the identical amount of computation throughout all layers. Prior works have proposed to augment the Transformer model with the capability of skimming tokens to improve its computational efficiency. However, they suffer from not having effectual and end-to-end optimization of the discrete skimming predictor. To address the above limitations, we propose the Transkimmer architecture, which learns to identify hidden state tokens that are not required by each layer. The skimmed tokens are then forwarded directly to the final output, thus reducing the computation of the successive layers. The key idea in Transkimmer is to add a parameterized predictor before each layer that learns to make the skimming decision. We also propose to adopt reparameterization trick and add skim loss for the end-to-end training of Transkimmer. Transkimmer achieves 10.97x average speedup on GLUE benchmark compared with vanilla BERT-base baseline with less than 1{\%} accuracy degradation.}
}
@article{routing,
  title     = {Efficient Content-Based Sparse Attention with Routing Transformers},
  author    = {Aurko Roy and M. Saffar and Ashish Vaswani and David Grangier},
  journal   = {International Conference On Topology, Algebra And Categories In Logic},
  year      = {2020},
  doi       = {10.1162/tacl_a_00353},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2}
}
@inproceedings{Atkinson1968HumanMA,
  title={Human Memory: A Proposed System and its Control Processes},
  author={Richard C. Atkinson and Richard M. Shiffrin},
  booktitle={The psychology of learning and motivation},
  year={1968}
}
@article{ainslie2020etc,
  title     = {ETC: Encoding Long and Structured Inputs in Transformers},
  author    = {J. Ainslie and Santiago Ontañón and Chris Alberti and V. Cvicek and Zachary Kenneth Fisher and Philip Pham and Anirudh Ravula and Sumit K. Sanghai and Qifan Wang and Li Yang},
  journal   = {Conference On Empirical Methods In Natural Language Processing},
  year      = {2020},
  doi       = {10.18653/v1/2020.emnlp-main.19},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/56676aef356ebb13cba77fc9e4d70760fbc151f5}
}
@article{so2021primer,
  title     = {Primer: Searching for Efficient Transformers for Language Modeling},
  author    = {David R. So and Wojciech Ma'nke and Hanxiao Liu and Zihang Dai and Noam M. Shazeer and Quoc V. Le},
  journal   = {ARXIV.ORG},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/4a8964ea0de47010fb458021b68fa3ef5c4b77b2}
}
@article{silu,
  title     = {Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning},
  author    = {Stefan Elfwing and E. Uchibe and K. Doya},
  journal   = {Neural Networks},
  year      = {2017},
  doi       = {10.1016/j.neunet.2017.12.012},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b587ee7c802a5bd222a69090f59285e0dfdb29f1}
}
@article{paszke2019pytorch,
  title   = {Pytorch: An imperative style, high-performance deep learning library},
  author  = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}
@article{gu2021efficiently,
  title     = {Efficiently Modeling Long Sequences with Structured State Spaces},
  author    = {Albert Gu and Karan Goel and Christopher R'e},
  journal   = {International Conference On Learning Representations},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ac2618b2ce5cdcf86f9371bcca98bc5e37e46f51}
}
@article{hasani2022liquid,
  title={Liquid structural state-space models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}
@article{s4d,
  title     = {On the Parameterization and Initialization of Diagonal State Space Models},
  author    = {Albert Gu and Ankit Gupta and Karan Goel and Christopher Ré},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2206.11893},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ca444821352a4bd91884413d8070446e2960715a}
}

@article{gupta2022diagonal,
  title     = {Diagonal State Spaces are as Effective as Structured State Spaces},
  author    = {Ankit Gupta and Jonathan Berant},
  journal   = {ARXIV.ORG},
  year      = {2022},
  doi       = {10.48550/arXiv.2203.14343},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/71e15a9a52dcafca57bff5f310b95e2c7d0cfc87}
}

@inproceedings{NEURIPS2020_102f0bb6,
  author    = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1474-1487},
  publisher = {Curran Associates, Inc.},
  title     = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@article{tay2020long,
  title     = {Long Range Arena: A Benchmark for Efficient Transformers},
  author    = {Yi Tay and M. Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and J. Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
  journal   = {International Conference On Learning Representations},
  year      = {2020},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/7e9ff94476f41041c75e253e84f487db00e9c861}
}

@article{bahdanau2014neural,
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author    = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal   = {International Conference On Learning Representations},
  year      = {2014},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5}
}
@article{xu2022survey,
  title     = {A Survey on Dynamic Neural Networks for Natural Language Processing},
  author    = {Canwen Xu and Julian McAuley},
  journal   = {EMNLP Findings},
  year      = {2022},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/802a5d24c78f713e282b003d99b4afd924bd7568}
}

@article{gale2020sparse,
  title     = {Sparse GPU Kernels for Deep Learning},
  author    = {Trevor Gale and M. Zaharia and C. Young and Erich Elsen},
  journal   = {International Conference For High Performance Computing, Networking, Storage And Analysis},
  year      = {2020},
  doi       = {10.1109/SC41405.2020.00021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/70e9a09de05aa7ed8a74d56cf2d13ea9e38a6328}
}

@article{dynamic,
  journal = {IEEE Transactions on Computers},
  volume  = {71},
  pages   = {3165-3178},
  doi     = {10.1109/TC.2022.3208206},
  title   = {Dynamic Sparse Attention for Scalable Transformer Acceleration},
  year    = {2022},
  author  = {Liu Liu and Zheng Qu and Zhaodong Chen and Fengbin Tu and Yufei Ding and Yuan Xie}
}

@article{vardasbi2023state,
  title     = {State Spaces Aren't Enough: Machine Translation Needs Attention},
  author    = {Ali Vardasbi and Telmo Pires and Robin M. Schmidt and Stephan Peitz},
  journal   = {ARXIV.ORG},
  year      = {2023},
  doi       = {10.48550/arXiv.2304.12776},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/01e721df7fcf8b664deb2cdc97ff58d65553af6b}
}

@article{warden2018speech,
  title   = {Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition},
  author  = {Pete Warden},
  year    = {2018},
  journal = {arXiv preprint arXiv: Arxiv-1804.03209}
}

@article{vaswani2017attention,
  title     = {Attention Is All You Need},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  journal   = {NIPS},
  year      = {2017},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776}
}

@inproceedings{
mega,
title={Mega: Moving Average Equipped Gated Attention},
author={Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=qNLe3iq2El}
}

@article{lru,
  title={Resurrecting Recurrent Neural Networks for Long Sequences},
  author={Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  journal={arXiv preprint arXiv:2303.06349},
  year={2023}
}


@inproceedings{
h3,
title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}

@inproceedings{gc,
 author = {Dherin, Benoit and Munn, Michael and Rosca, Mihaela and Barrett, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {2333--2349},
 publisher = {Curran Associates, Inc.},
 title = {Why neural networks find simple solutions:  The many regularizers of geometric complexity},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/0ff3502bb29570b219967278db150a50-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{
unitrans,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr}
}

%DA
@article{ben2010theoryDA,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  number={1},
  pages={151--175},
  year={2010},
  publisher={Springer}
}
@inproceedings{adw,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{umnn,
  title={Unconstrained Monotonic Neural Networks},
  author={Wehenkel, Antoine and Louppe, Gilles},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1545--1555},
  year={2019}
}
@InProceedings{Ganin2015DA,
  title = 	 {Unsupervised Domain Adaptation by Backpropagation},
  author = 	 {Ganin, Yaroslav and Lempitsky, Victor},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1180--1189},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ganin15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ganin15.html},
  abstract = 	 {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}
}

%DNN
@article{dann,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{ae,
author = {Ballard, Dana H.},
title = {Modular Learning in Neural Networks},
year = {1987},
isbn = {0934613427},
publisher = {AAAI Press},
abstract = {In the development of large-scale knowledge networks much recent progress has been
inspired by connections to neurobiology. An important component of any "neural" network
is an accompanying learning algorithm. Such an algorithm, to be biologically plausible,
must work for very large numbers of units. Studies of large-scale systems have so
far been restricted to systems Without internal units (units With no direct connections
to the input or output). Internal units are crucial to such systems as they are the
means by which a system can encode high-order regularities (or invariants) that are
Implicit in its inputs and outputs. Computer simulations of learning using internal
units have been restricted to small-scale systems. This paper describes away of coupling
autoassociative learning modules Into hierarchies that should greatly improve the
performance of learning algorithms in large-scale systems. The Idea has been tested
experimentally with positive results.},
booktitle = {Proceedings of the Sixth National Conference on Artificial Intelligence - Volume 1},
pages = {279–284},
numpages = {6},
location = {Seattle, Washington},
series = {AAAI'87}
}

@inproceedings{vae,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%TL
@misc{zhang2021quantifying,
      title={Quantifying and Improving Transferability in Domain Generalization}, 
      author={Guojun Zhang and Han Zhao and Yaoliang Yu and Pascal Poupart},
      year={2021},
      eprint={2106.03632},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@ARTICLE{pan2010survey,  author={Pan, Sinno Jialin and Yang, Qiang},  journal={IEEE Transactions on Knowledge and Data Engineering},   title={A Survey on Transfer Learning},   year={2010},  volume={22},  number={10},  pages={1345-1359},  doi={10.1109/TKDE.2009.191}}

@InProceedings{Blitzer2008DA,
  author    = {Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Learning Bounds for Domain Adaptation},
  year      = {2008},
  editor    = {J. Platt and D. Koller and Y. Singer and S. Roweis},
  publisher = {Curran Associates, Inc.},
  volume    = {20},
  url       = {https://proceedings.neurips.cc/paper/2007/file/42e77b63637ab381e8be5f8318cc28a2-Paper.pdf},
}

@InProceedings{BenDavid2007RepDA,
  author    = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Analysis of Representations for Domain Adaptation},
  year      = {2007},
  editor    = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
  publisher = {MIT Press},
  volume    = {19},
  url       = {https://proceedings.neurips.cc/paper/2006/file/b1b0432ceafb0ce714426e9114852ac7-Paper.pdf},
}

@Article{Bastani2021ProxiesTransferLearning,
  author   = {Bastani, Hamsa},
  journal  = {Management Science},
  title    = {Predicting with Proxies: Transfer Learning in High Dimension},
  year     = {2021},
  number   = {5},
  pages    = {2964-2984},
  volume   = {67},
  abstract = {Predictive analytics is increasingly used to guide decision making in many applications. However, in practice, we often have limited data on the true predictive task of interest and must instead rely on more abundant data on a closely related proxy predictive task. For example, e-commerce platforms use abundant customer click data (proxy) to make product recommendations rather than the relatively sparse customer purchase data (true outcome of interest); alternatively, hospitals often rely on medical risk scores trained on a different patient population (proxy) rather than their own patient population (true cohort of interest) to assign interventions. Yet, not accounting for the bias in the proxy can lead to suboptimal decisions. Using real data sets, we find that this bias can often be captured by a sparse function of the features. Thus, we propose a novel two-step estimator that uses techniques from high-dimensional statistics to efficiently combine a large amount of proxy data and a small amount of true data. We prove upper bounds on the error of our proposed estimator and lower bounds on several heuristics used by data scientists; in particular, our proposed estimator can achieve the same accuracy with exponentially less true data (in the number of features d). Finally, we demonstrate the effectiveness of our approach on e-commerce and healthcare data sets; in both cases, we achieve significantly better predictive accuracy as well as managerial insights into the nature of the bias in the proxy data.This paper was accepted by George Shanthikumar, big data and analytics.},
  doi      = {10.1287/mnsc.2020.3729},
  eprint   = {https://doi.org/10.1287/mnsc.2020.3729},
  url      = {https://doi.org/10.1287/mnsc.2020.3729},
}

@article{Bastani2019MetaDP,
  author    = {Hamsa Bastani and
               David Simchi{-}Levi and
               Ruihao Zhu},
  title     = {Meta Dynamic Pricing: Learning Across Experiments},
  journal   = {CoRR},
  volume    = {abs/1902.10918},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.10918},
  eprinttype = {arXiv},
  eprint    = {1902.10918},
  timestamp = {Tue, 21 May 2019 18:03:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-10918.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%missing data
@article{muthen1987structural,
  title={On structural equation modeling with data that are not missing completely at random},
  author={Muth{\'e}n, Bengt and Kaplan, David and Hollis, Michael},
  journal={Psychometrika},
  volume={52},
  number={3},
  pages={431--462},
  year={1987},
  publisher={Springer}
}

@article{takahashi2017misingdataMCMC,
author={Takahashi, M.}, 
year={2017},
title={Statistical Inference in Missing Data by MCMC and Non-MCMC Multiple Imputation Algorithms: Assessing the Effects of Between-Imputation Iterations},
 journal={Data Science Journal}, volume={16}, 
 page={37}}
 
 @misc{yoon2018gain,
      title={GAIN: Missing Data Imputation using Generative Adversarial Nets}, 
      author={Jinsung Yoon and James Jordon and Mihaela van der Schaar},
      year={2018},
      eprint={1806.02920},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

%predictive
@InProceedings{Kao2009Regression,
  author    = {Kao, Yi-hao and Roy, Benjamin and Yan, Xiang},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Directed Regression},
  year      = {2009},
  editor    = {Y. Bengio and D. Schuurmans and J. Lafferty and C. Williams and A. Culotta},
  publisher = {Curran Associates, Inc.},
  volume    = {22},
  url       = {https://proceedings.neurips.cc/paper/2009/file/0c74b7f78409a4022a2c4c5a5ca3ee19-Paper.pdf},
}

@InProceedings{Taskar2005Prediction,
  author    = {Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
  title     = {Learning Structured Prediction Models: A Large Margin Approach},
  year      = {2005},
  address   = {New York, NY, USA},
  pages     = {896–903},
  publisher = {Association for Computing Machinery},
  series    = {ICML '05},
  abstract  = {We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods.},
  doi       = {10.1145/1102351.1102464},
  isbn      = {1595931805},
  location  = {Bonn, Germany},
  numpages  = {8},
  url       = {https://doi.org/10.1145/1102351.1102464},
}

@InProceedings{Osokin2017Prediction,
  author    = {Osokin, Anton and Bach, Francis and Lacoste-Julien, Simon},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {On Structured Prediction Theory with Calibrated Convex Surrogate Losses},
  year      = {2017},
  editor    = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper/2017/file/38db3aed920cf82ab059bfccbd02be6a-Paper.pdf},
}

%prescriptive analysis/optimization

@Article{Bertsimas2020Prescriptive,
  author   = {Bertsimas, Dimitris and Kallus, Nathan},
  journal  = {Management Science},
  title    = {From Predictive to Prescriptive Analytics},
  year     = {2020},
  number   = {3},
  pages    = {1025-1044},
  volume   = {66},
  abstract = {We combine ideas from machine learning (ML) and operations research and management science (OR/MS) in developing a framework, along with specific methods, for using data to prescribe optimal decisions in OR/MS problems. In a departure from other work on data-driven optimization, we consider data consisting, not only of observations of quantities with direct effect on costs/revenues, such as demand or returns, but also predominantly of observations of associated auxiliary quantities. The main problem of interest is a conditional stochastic optimization problem, given imperfect observations, where the joint probability distributions that specify the problem are unknown. We demonstrate how our proposed methods are generally applicable to a wide range of decision problems and prove that they are computationally tractable and asymptotically optimal under mild conditions, even when data are not independent and identically distributed and for censored observations. We extend these to the case in which some decision variables, such as price, may affect uncertainty and their causal effects are unknown. We develop the coefficient of prescriptiveness P to measure the prescriptive content of data and the efficacy of a policy from an operations perspective. We demonstrate our approach in an inventory management problem faced by the distribution arm of a large media company, shipping 1 billion units yearly. We leverage both internal data and public data harvested from IMDb, Rotten Tomatoes, and Google to prescribe operational decisions that outperform baseline measures. Specifically, the data we collect, leveraged by our methods, account for an 88\% improvement as measured by our coefficient of prescriptiveness.This paper was accepted by Noah Gans, optimization.},
  doi      = {10.1287/mnsc.2018.3253},
  eprint   = {https://doi.org/10.1287/mnsc.2018.3253},
  url      = {https://doi.org/10.1287/mnsc.2018.3253},
}

@misc{bertsimas2017power,
      title={The Power and Limits of Predictive Approaches to Observational-Data-Driven Optimization}, 
      author={Dimitris Bertsimas and Nathan Kallus},
      year={2017},
      eprint={1605.02347},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@Article{Elmachtoub2021SPO,
  author   = {Elmachtoub, Adam N. and Grigas, Paul},
  journal  = {Management Science},
  title    = {Smart “Predict, then Optimize”},
  year     = {2021},
  number   = {0},
  pages    = {null},
  volume   = {0},
  abstract = {Many real-world analytics problems involve two significant challenges: prediction and optimization. Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart “Predict, then Optimize” (SPO), which directly leverages the optimization problem structure—that is, its objective and constraints—for designing better prediction models. A key component of our framework is the SPO loss function, which measures the decision error induced by a prediction. Training a prediction model with respect to the SPO loss is computationally challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path and portfolio-optimization problems show that the SPO framework can lead to significant improvement under the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the ground truth is highly nonlinear.This paper was accepted by Yinyu Ye, optimization.},
  doi      = {10.1287/mnsc.2020.3922},
  eprint   = {https://doi.org/10.1287/mnsc.2020.3922},
  url      = {https://doi.org/10.1287/mnsc.2020.3922},
}

@Article{Ferreira2016PO,
  author   = {Ferreira, Kris Johnson and Lee, Bin Hong Alex and Simchi-Levi, David},
  journal  = {Manufacturing \& Service Operations Management},
  title    = {Analytics for an Online Retailer: Demand Forecasting and Price Optimization},
  year     = {2016},
  number   = {1},
  pages    = {69-88},
  volume   = {18},
  abstract = {We present our work with an online retailer, Rue La La, as an example of how a retailer can use its wealth of data to optimize pricing decisions on a daily basis. Rue La La is in the online fashion sample sales industry, where they offer extremely limited-time discounts on designer apparel and accessories. One of the retailer’s main challenges is pricing and predicting demand for products that it has never sold before, which account for the majority of sales and revenue. To tackle this challenge, we use machine learning techniques to estimate historical lost sales and predict future demand of new products. The nonparametric structure of our demand prediction model, along with the dependence of a product’s demand on the price of competing products, pose new challenges on translating the demand forecasts into a pricing policy. We develop an algorithm to efficiently solve the subsequent multiproduct price optimization that incorporates reference price effects, and we create and implement this algorithm into a pricing decision support tool for Rue La La’s daily use. We conduct a field experiment and find that sales does not decrease because of implementing tool recommended price increases for medium and high price point products. Finally, we estimate an increase in revenue of the test group by approximately 9.7\% with an associated 90\% confidence interval of [2.3\%, 17.8\%].},
  doi      = {10.1287/msom.2015.0561},
  eprint   = {https://doi.org/10.1287/msom.2015.0561},
  url      = {https://doi.org/10.1287/msom.2015.0561},
}

@Book{Sen2017LearningOptimization,
  author    = {Sen, Suvrajeet and Deng, Yunxiao},
  publisher = {Humboldt-Universität zu Berlin},
  title     = {Learning Enabled Optimization: Towards a Fusion of Statistical Learning and Stochastic Optimization},
  year      = {2017},
  doi       = {http://dx.doi.org/10.18452/18087},
}


%SGD
@misc{bertsekas2017cvx,
      title={Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey}, 
      author={Dimitri P. Bertsekas},
      year={2017},
      eprint={1507.01030},
      archivePrefix={arXiv},
      primaryClass={cs.SY}
}
@inproceedings{dao2022flashattention,
  title={Flash{A}ttention: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{Bottou2018optML,
title = "Optimization methods for large-scale machine learning",
abstract = "This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",
keywords = "Algorithm complexity analysis, Machine learning, Noise reduction methods, Numerical optimization, Second-order methods, Stochastic gradient methods",
author = "L{\'e}on Bottou and Curtis, {Frank E.} and Jorge Nocedal",
note = "Funding Information: ∗Received by the editors June 16, 2016; accepted for publication (in revised form) April 19, 2017; published electronically May 8, 2018. http://www.siam.org/journals/sirev/60-2/M108017.html Funding: The work of the second author was supported by U.S. Department of Energy grant DE-SC0010615 and U.S. National Science Foundation grant DMS-1016291. The work of the third author was supported by Office of Naval Research grant N00014-14-1-0313 P00003 and Department of Energy grant DE-FG02-87ER25047s. †Facebook AI Research, New York, NY 10003 (leon@bottou.org). ‡Department of Industrial and Systems Engineering, Lehigh University, Bethlehem, PA 18015 (frank.e.curtis@gmail.com). §Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL 60201 (j-nocedal@northwestern.edu). Publisher Copyright: {\textcopyright} 2018 Society for Industrial and Applied Mathematics.",
year = "2018",
doi = "10.1137/16M1080173",
language = "English (US)",
volume = "60",
pages = "223--311",
journal = "SIAM Review",
issn = "0036-1445",
publisher = "Society for Industrial and Applied Mathematics Publications",
number = "2",
}

@article{nangia2018listops,
  title={Listops: A diagnostic dataset for latent tree learning},
  author={Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.06028},
  year={2018}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@article{radev2013acl,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  volume={47},
  pages={919--944},
  year={2013},
  publisher={Springer}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@article{linsley2018learning,
  title={Learning long-range spatial dependencies with horizontal gated recurrent units},
  author={Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charles and Serre, Thomas},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@misc{enwiki8,
  title = {The human knowledge compression contest},
  author = {Hutter, Marcus},
  year = {2006},
  howpublished = {http://prize.hutter1.net/}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}
@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{ma2021luna,
  title={Luna: Linear unified nested attention},
  author={Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2441--2453},
  year={2021}
}

@article{sukhbaatar2019adaptive,
  title={Adaptive attention span in transformers},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:1905.07799},
  year={2019}
}

@article{post2018call,
  title={A call for clarity in reporting BLEU scores},
  author={Post, Matt},
  journal={arXiv preprint arXiv:1804.08771},
  year={2018}
}
@inproceedings{ott-etal-2018-scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
}