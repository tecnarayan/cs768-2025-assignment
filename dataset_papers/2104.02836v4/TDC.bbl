\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Archibald et~al.(1995)Archibald, McKinnon, and
  Thomas]{archibald1995generation}
TW~Archibald, KIM McKinnon, and LC~Thomas.
\newblock On the generation of markov decision processes.
\newblock \emph{Journal of the Operational Research Society}, 46\penalty0
  (3):\penalty0 354--361, 1995.

\bibitem[Baird(1995)]{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings}, pages 30--37. Elsevier, 1995.

\bibitem[Bertsekas(2011)]{bertsekas2011dynamic}
Dimitri~P Bertsekas.
\newblock {Dynamic Programming and Optimal Control 3rd edition, volume II}.
\newblock \emph{Belmont, MA: Athena Scientific}, 2011.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pages
  1691--1692. PMLR, 2018.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Precup, Silver, Sutton, Maei, and
  Szepesv{\'a}ri]{bhatnagar2009convergent}
Shalabh Bhatnagar, Doina Precup, David Silver, Richard~S Sutton, Hamid Maei,
  and Csaba Szepesv{\'a}ri.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, volume~22, pages 1204--1212, 2009.

\bibitem[Borkar(2009)]{borkar2009stochastic}
Vivek~S Borkar.
\newblock \emph{Stochastic approximation: a dynamical systems viewpoint},
  volume~48.
\newblock Springer, 2009.

\bibitem[Cai et~al.(2019)Cai, Yang, Lee, and Wang]{cai2019neural}
Qi~Cai, Zhuoran Yang, Jason~D Lee, and Zhaoran Wang.
\newblock Neural temporal-difference learning converges to global optima.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 11312--11322, 2019.

\bibitem[Dalal et~al.(2018)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and
  Mannor]{dalal2018finite}
Gal Dalal, Bal{\'a}zs Sz{\"o}r{\'e}nyi, Gugan Thoppe, and Shie Mannor.
\newblock Finite sample analysis of two-timescale stochastic approximation with
  applications to reinforcement learning.
\newblock \emph{Proceedings of Machine Learning Research}, 75:\penalty0 1--35,
  2018.

\bibitem[Dalal et~al.(2020)Dalal, Szorenyi, and Thoppe]{dalal2020tale}
Gal Dalal, Balazs Szorenyi, and Gugan Thoppe.
\newblock A tale of two-timescale reinforcement learning with the tightest
  finite-time bound.
\newblock In \emph{Proc. AAAI Conference on Artificial Intelligence (AAAI)},
  pages 3701--3708, 2020.

\bibitem[Doan(2021)]{doan2021nonlinear}
Thinh~T Doan.
\newblock Nonlinear two-time-scale stochastic approximation: Convergence and
  finite-time performance.
\newblock In \emph{Learning for Dynamics and Control}, pages 47--47. PMLR,
  2021.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 1675--1685. PMLR, 2019.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gupta et~al.(2019)Gupta, Srikant, and Ying]{gupta2019finite}
Harsh Gupta, R~Srikant, and Lei Ying.
\newblock Finite-time performance bounds and adaptive learning rate selection
  for two time-scale reinforcement learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 4706--4715, 2019.

\bibitem[Hong et~al.(2020)Hong, Wai, Wang, and Yang]{hong2020two}
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang.
\newblock A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic.
\newblock \emph{arXiv preprint arXiv:2007.05170}, 2020.

\bibitem[Kaledin et~al.(2020)Kaledin, Moulines, Naumov, Tadic, and
  Wai]{kaledin2020finite}
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai.
\newblock Finite time analysis of linear two-timescale stochastic approximation
  with markovian noise.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pages
  2144--2203. PMLR, 2020.

\bibitem[Karmakar and Bhatnagar(2018)]{karmakar2018two}
Prasenjit Karmakar and Shalabh Bhatnagar.
\newblock Two time-scale stochastic approximation with controlled {Markov}
  noise and off-policy temporal-difference learning.
\newblock \emph{Mathematics of Operations Research}, 43\penalty0 (1):\penalty0
  130--151, 2018.

\bibitem[Konda et~al.(2004)Konda, Tsitsiklis, et~al.]{konda2004convergence}
Vijay~R Konda, John~N Tsitsiklis, et~al.
\newblock Convergence rate of linear two-time-scale stochastic approximation.
\newblock \emph{The Annals of Applied Probability}, 14\penalty0 (2):\penalty0
  796--819, 2004.

\bibitem[Lakshminarayanan and Szepesvari(2018)]{lakshminarayanan2018linear}
Chandrashekar Lakshminarayanan and Csaba Szepesvari.
\newblock Linear stochastic approximation: {H}ow far does constant step-size
  and iterate averaging go?
\newblock In \emph{Proc. International Conference on Artificial Intelligence
  and Statistics}, pages 1347--1355, 2018.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{liu2015finite}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock In \emph{Proc. International Conference on Uncertainty in Artificial
  Intelligence (UAI)}, pages 504--513. Citeseer, 2015.

\bibitem[Ma et~al.(2020)Ma, Zhou, and Zou]{ma2020variance}
Shaocong Ma, Yi~Zhou, and Shaofeng Zou.
\newblock Variance-reduced off-policy {TDC} learning: Non-asymptotic
  convergence analysis.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~33, pages 14796--14806, 2020.

\bibitem[Ma et~al.(2021)Ma, Zhou, and Zou]{ma2021greedygq}
Shaocong Ma, Yi~Zhou, and Shaofeng Zou.
\newblock Greedy-{GQ} with variance reduction: Finite-time analysis and
  improved complexity.
\newblock In \emph{Proc. International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Maei(2011)]{maei2011gradient}
Hamid~Reza Maei.
\newblock Gradient temporal-difference learning algorithms.
\newblock \emph{Thesis, University of Alberta}, 2011.

\bibitem[Maei et~al.(2010)Maei, Szepesv{\'a}ri, Bhatnagar, and
  Sutton]{maei2010toward}
Hamid~Reza Maei, Csaba Szepesv{\'a}ri, Shalabh Bhatnagar, and Richard~S Sutton.
\newblock Toward off-policy learning control with function approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 719--726, 2010.

\bibitem[Melo et~al.(2008)Melo, Meyn, and Ribeiro]{melo2008analysis}
Francisco~S Melo, Sean~P Meyn, and M~Isabel Ribeiro.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 664--671. ACM, 2008.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{Proc. International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Mokkadem et~al.(2006)Mokkadem, Pelletier,
  et~al.]{mokkadem2006convergence}
Abdelkader Mokkadem, Mariane Pelletier, et~al.
\newblock Convergence rate and averaging of nonlinear two-time-scale stochastic
  approximation algorithms.
\newblock \emph{The Annals of Applied Probability}, 16\penalty0 (3):\penalty0
  1671--1702, 2006.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Behnam Neyshabur.
\newblock Implicit regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1709.01953}, 2017.

\bibitem[Perkins and Precup(2003)]{perkins2003convergent}
Theodore~J Perkins and Doina Precup.
\newblock A convergent form of approximate policy iteration.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 1627--1634, 2003.

\bibitem[Srikant and Ying(2019)]{srikant2019finite}
Rayadurgam Srikant and Lei Ying.
\newblock Finite-time error bounds for linear stochastic approximation andtd
  learning.
\newblock In \emph{Proc. Annual Conference on Learning Theory (CoLT)}, pages
  2803--2830. PMLR, 2019.

\bibitem[Sun et~al.(2020)Sun, Wang, Giannakis, Yang, and Yang]{sun2020finite}
Jun Sun, Gang Wang, Georgios~B Giannakis, Qinmin Yang, and Zaiyue Yang.
\newblock Finite-sample analysis of decentralized temporal-difference learning
  with linear function approximation.
\newblock In \emph{Proeedings of the International Workshop on Artificial
  Intelligence and Statistics}, 2020.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction, Second Edition}.
\newblock The MIT Press, Cambridge, Massachusetts, 2018.

\bibitem[Sutton et~al.(2009{\natexlab{a}})Sutton, Maei, and
  Szepesv{\'a}ri]{sutton2009acov}
Richard~S Sutton, Hamid~R Maei, and Csaba Szepesv{\'a}ri.
\newblock A convergent $ {O} (n) $ temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 1609--1616, 2009{\natexlab{a}}.

\bibitem[Sutton et~al.(2009{\natexlab{b}})Sutton, Maei, Precup, Bhatnagar,
  Silver, Szepesv{\'a}ri, and Wiewiora]{sutton2009fast}
Richard~S Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv{\'a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 993--1000, 2009{\natexlab{b}}.

\bibitem[Tsitsiklis and Van~Roy(1997)]{tsitsiklis1997analysis}
John~N Tsitsiklis and Benjamin Van~Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE transactions on automatic control}, 42\penalty0
  (5):\penalty0 674--690, 1997.

\bibitem[Wang and Zou(2020)]{wang2020finite}
Yue Wang and Shaofeng Zou.
\newblock Finite-sample analysis of greedy-{GQ} with linear function
  approximation under markovian noise.
\newblock In \emph{Proc. International Conference on Uncertainty in Artificial
  Intelligence (UAI)}, pages 11--20. PMLR, 2020.

\bibitem[Wang et~al.(2017)Wang, Chen, Liu, Ma, and Liu]{wang2017finite}
Yue Wang, Wei Chen, Yuting Liu, Zhi-Ming Ma, and Tie-Yan Liu.
\newblock Finite sample analysis of the gtd policy evaluation algorithms in
  markov setting.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 5504--5513, 2017.

\bibitem[Wu et~al.(2020)Wu, Zhang, Xu, and Gu]{wu2020finite}
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu.
\newblock A finite time analysis of two time-scale actor critic methods.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Xu and Gu(2020)]{xu2020finite}
Pan Xu and Quanquan Gu.
\newblock A finite-time analysis of q-learning with neural network function
  approximation.
\newblock In \emph{Proc. International Conference on Machine Learning (ICML)},
  pages 10555--10565. PMLR, 2020.

\bibitem[Xu and Liang(2021)]{xu2021sample}
Tengyu Xu and Yingbin Liang.
\newblock Sample complexity bounds for two timescale value-based reinforcement
  learning algorithms.
\newblock In \emph{Proc. International Conference on Artifical Intelligence and
  Statistics (AISTATS)}, pages 811--819. PMLR, 2021.

\bibitem[Xu et~al.(2019)Xu, Zou, and Liang]{xu2019two}
Tengyu Xu, Shaofeng Zou, and Yingbin Liang.
\newblock Two time-scale off-policy {TD} learning: Non-asymptotic analysis over
  {Markovian} samples.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 10633--10643, 2019.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Shaofeng Zou, Tengyu Xu, and Yingbin Liang.
\newblock Finite-sample analysis for {SARSA} with linear function
  approximation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 8665--8675, 2019.

\end{thebibliography}
