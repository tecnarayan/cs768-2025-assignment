\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baird(1995)]{Baird1995ResidualAR}
Baird, L.~C.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{ICML}, 1995.

\bibitem[Barnard(1993)]{barnard93temporaldifference}
Barnard, E.
\newblock Temporal-difference methods and markov models.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics}, 1993.

\bibitem[Behzadian et~al.(2019)Behzadian, Gharatappeh, and
  Petrik]{Behzadian2019FastFS}
Behzadian, B., Gharatappeh, S., and Petrik, M.
\newblock Fast feature selection for linear value function approximation.
\newblock In \emph{ICAPS}, 2019.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{Bellemare2017ADP}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Bellemare et~al.(2019)Bellemare, Dabney, Dadashi, Ta{\"i}ga, Castro,
  Roux, Schuurmans, Lattimore, and Lyle]{Bellemare2019AGP}
Bellemare, M.~G., Dabney, W., Dadashi, R., Ta{\"i}ga, A.~A., Castro, P.~S.,
  Roux, N.~L., Schuurmans, D., Lattimore, T., and Lyle, C.
\newblock A geometric perspective on optimal representations for reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Benveniste et~al.(1990)Benveniste, Priouret, and
  M\'{e}tivier]{stochasticapproximation}
Benveniste, A., Priouret, P., and M\'{e}tivier, M.
\newblock \emph{Adaptive Algorithms and Stochastic Approximations}.
\newblock Springer-Verlag, Berlin, Heidelberg, 1990.
\newblock ISBN 0387528946.

\bibitem[Bertsekas(2011)]{Bertsekas2011ApproximatePI}
Bertsekas, D.~P.
\newblock Approximate policy iteration: a survey and some new methods.
\newblock \emph{Journal of Control Theory and Applications}, 9:\penalty0
  310--335, 2011.

\bibitem[Bertsekas(2018)]{Bertsekas2018FeaturebasedAA}
Bertsekas, D.~P.
\newblock Feature-based aggregation and deep reinforcement learning: a survey
  and some new implementations.
\newblock \emph{IEEE/CAA Journal of Automatica Sinica}, 6:\penalty0 1--31,
  2018.

\bibitem[Bodnar et~al.(2019)Bodnar, Li, Hausman, Pastor, and
  Kalakrishnan]{bodnar19quantile}
Bodnar, C., Li, A., Hausman, K., Pastor, P., and Kalakrishnan, M.
\newblock Quantile {QT}-opt for risk-aware vision-based robotic grasping.
\newblock \emph{arXiv}, 2019.

\bibitem[Borkar \& Meyn(2000)Borkar and Meyn]{Borkar2000TheOM}
Borkar, V.~S. and Meyn, S.~P.
\newblock The o.d.e. method for convergence of stochastic approximation and
  reinforcement learning.
\newblock \emph{SIAM J. Control and Optimization}, 38:\penalty0 447--469, 2000.

\bibitem[Bradtke \& Barto(1996)Bradtke and Barto]{Bradtke1996LinearLA}
Bradtke, S.~J. and Barto, A.~G.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine Learning}, 22:\penalty0 33--57, 1996.

\bibitem[Cabi et~al.(2019)Cabi, Colmenarejo, Novikov, Konyushkova, Reed, Jeong,
  Zolna, Aytar, Budden, Vecerik, Sushkov, Barker, Scholz, Denil, de~Freitas,
  and Wang]{cabi19framework}
Cabi, S., Colmenarejo, S.~G., Novikov, A., Konyushkova, K., Reed, S., Jeong,
  R., Zolna, K., Aytar, Y., Budden, D., Vecerik, M., Sushkov, O., Barker, D.,
  Scholz, J., Denil, M., de~Freitas, N., and Wang, Z.
\newblock A framework for data-driven robotics.
\newblock \emph{arXiv}, 2019.

\bibitem[Chung et~al.(2019)Chung, Nath, Joseph, and White]{chung19twotimescale}
Chung, W., Nath, S., Joseph, A.~G., and White, M.
\newblock Two-timescale networks for nonlinear value function approximation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Dalal et~al.(2017)Dalal, Sz{\"o}r{\'e}nyi, Thoppe, and
  Mannor]{Dalal2017FiniteSA}
Dalal, G., Sz{\"o}r{\'e}nyi, B., Thoppe, G., and Mannor, S.
\newblock Finite sample analyses for td(0) with function approximation.
\newblock In \emph{AAAI}, 2017.

\bibitem[Dann et~al.(2014)Dann, Neumann, and Peters]{Dann2014PolicyEW}
Dann, C., Neumann, G., and Peters, J.
\newblock Policy evaluation with temporal differences: a survey and comparison.
\newblock \emph{J. Mach. Learn. Res.}, 15:\penalty0 809--883, 2014.

\bibitem[Dayan(1993)]{Dayan1993ImprovingGF}
Dayan, P.
\newblock Improving generalization for temporal difference learning: The
  successor representation.
\newblock \emph{Neural Computation}, 5:\penalty0 613--624, 1993.

\bibitem[Fran{\c c}ois-Lavet et~al.(2018)Fran{\c c}ois-Lavet, Bengio, Precup,
  and Pineau]{francoislavet18combined}
Fran{\c c}ois-Lavet, V., Bengio, Y., Precup, D., and Pineau, J.
\newblock Combined reinforcement learning via abstract representations.
\newblock \emph{arXiv}, 2018.

\bibitem[Gelada et~al.(2019)Gelada, Kumar, Buckman, Nachum, and
  Bellemare]{Gelada2019DeepMDPLC}
Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare, M.~G.
\newblock Deep{MDP}: {L}earning continuous latent space models for
  representation learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Golub \& van Loan(2013)Golub and van Loan]{golub13}
Golub, G.~H. and van Loan, C.~F.
\newblock \emph{Matrix Computations}.
\newblock JHU Press, fourth edition, 2013.
\newblock ISBN 1421407949 9781421407944.
\newblock URL \url{http://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm}.

\bibitem[Gordon(1995)]{Gordon1995StableFA}
Gordon, G.~J.
\newblock Stable function approximation in dynamic programming.
\newblock In \emph{ICML}, 1995.

\bibitem[Jaderberg et~al.(2016)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,
  Silver, and Kavukcuoglu]{Jaderberg2016ReinforcementLW}
Jaderberg, M., Mnih, V., Czarnecki, W., Schaul, T., Leibo, J.~Z., Silver, D.,
  and Kavukcuoglu, K.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock \emph{ArXiv}, abs/1611.05397, 2016.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and
  Parr]{Lagoudakis2003LeastSquaresPI}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{J. Mach. Learn. Res.}, 4:\penalty0 1107--1149, 2003.

\bibitem[Levine et~al.(2017)Levine, Zahavy, Mankowitz, Tamar, and
  Mannor]{levine17shallow}
Levine, N., Zahavy, T., Mankowitz, D., Tamar, A., and Mannor, S.
\newblock Shallow updates for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Machado et~al.(2018)Machado, Rosenbaum, Guo, Liu, Tesauro, and
  Campbell]{machado2018eigenoption}
Machado, M.~C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G., and Campbell, M.
\newblock Eigenoption discovery through the deep successor representation.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=Bk8ZcAxR-}.

\bibitem[Maei et~al.(2009)Maei, Szepesvari, Bhatnagar, Precup, Silver, and
  Sutton]{Maei2009ConvergentTL}
Maei, H.~R., Szepesvari, C., Bhatnagar, S., Precup, D., Silver, D., and Sutton,
  R.~S.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In \emph{NIPS}, 2009.

\bibitem[Mahadevan \& Maggioni(2007)Mahadevan and
  Maggioni]{Mahadevan2007ProtovalueFA}
Mahadevan, S. and Maggioni, M.
\newblock Proto-value functions: A laplacian framework for learning
  representation and control in markov decision processes.
\newblock \emph{J. Mach. Learn. Res.}, 8:\penalty0 2169--2231, 2007.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih15human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Ollivier(2018)]{Ollivier2018ApproximateTD}
Ollivier, Y.
\newblock Approximate temporal difference learning is a gradient descent for
  reversible policies.
\newblock \emph{ArXiv}, abs/1805.00869, 2018.

\bibitem[Parr et~al.(2007)Parr, Painter-Wakefield, Li, and
  Littman]{Parr2007AnalyzingFG}
Parr, R., Painter-Wakefield, C., Li, L., and Littman, M.~L.
\newblock Analyzing feature generation for value-function approximation.
\newblock In \emph{ICML '07}, 2007.

\bibitem[Parr et~al.(2008)Parr, Li, Taylor, Painter-Wakefield, and
  Littman]{Parr2008AnAO}
Parr, R., Li, L., Taylor, G., Painter-Wakefield, C., and Littman, M.~L.
\newblock An analysis of linear models, linear value-function approximation,
  and feature selection for reinforcement learning.
\newblock In \emph{ICML '08}, 2008.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{Pathak2017CuriosityDrivenEB}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW)}, pp.\  488--489, 2017.

\bibitem[Petrik(2007)]{PetrikKrylov}
Petrik, M.
\newblock An analysis of laplacian methods for value function approximation in
  mdps.
\newblock In \emph{Proceedings of the 20th International Joint Conference on
  Artifical Intelligence}, IJCAI’07, pp.\  2574–2579, San Francisco, CA,
  USA, 2007. Morgan Kaufmann Publishers Inc.

\bibitem[Puterman(1994)]{PutermanBook}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., USA, 1st edition, 1994.
\newblock ISBN 0471619779.

\bibitem[Shi \& Wei(2012)Shi and Wei]{sharpbauerfike}
Shi, X. and Wei, Y.
\newblock A sharp version of bauer–fike’s theorem.
\newblock \emph{Journal of Computational and Applied Mathematics}, 236\penalty0
  (13):\penalty0 3218 -- 3227, 2012.
\newblock ISSN 0377-0427.
\newblock \doi{https://doi.org/10.1016/j.cam.2012.02.021}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S0377042712000787}.

\bibitem[Stachenfeld et~al.(2014)Stachenfeld, Botvinick, and
  Gershman]{stachenfeld14design}
Stachenfeld, K.~L., Botvinick, M., and Gershman, S.~J.
\newblock Design principles of the hippocampal cognitive map.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton18reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT Press, 2nd edition, 2018.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{Sutton1999BetweenMA}
Sutton, R.~S., Precup, D., and Singh, S.~P.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artif. Intell.}, 112:\penalty0 181--211, 1999.

\bibitem[Trefethen \& Embree(2005)Trefethen and Embree]{Trefethen2005SpectraAP}
Trefethen, L.~N. and Embree, M.
\newblock Spectra and pseudospectra : the behavior of nonnormal matrices and
  operators.
\newblock 2005.

\bibitem[Tsitsiklis \& Roy(1996)Tsitsiklis and Roy]{Tsitsiklis1996AnalysisOT}
Tsitsiklis, J.~N. and Roy, B.~V.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In \emph{NIPS}, 1996.

\bibitem[van Hasselt et~al.(2018)van Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{Hasselt2018DeepRL}
van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil,
  J.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{ArXiv}, abs/1812.02648, 2018.

\bibitem[Vecerik et~al.(2019)Vecerik, Sushkov, Barker, Rothörl, Hester, and
  Scholz]{vecerik19practical}
Vecerik, M., Sushkov, O., Barker, D., Rothörl, T., Hester, T., and Scholz, J.
\newblock A practical approach to insertion with variable socket position using
  deep reinforcement learning.
\newblock 2019.

\bibitem[Wu et~al.(2018)Wu, Tucker, and Nachum]{Wu2018TheLI}
Wu, Y., Tucker, G., and Nachum, O.
\newblock The laplacian in rl: Learning representations with efficient
  approximations.
\newblock \emph{ArXiv}, abs/1810.04586, 2018.

\bibitem[Yu \& Bertsekas(2009)Yu and Bertsekas]{yu09basis}
Yu, H. and Bertsekas, D.~P.
\newblock Basis function adaptation methods for cost approximation in mdp.
\newblock In \emph{Proceedings of the {IEEE} Symposium on Adaptive Dynamic
  Programming and Reinforcement Learning}, 2009.

\bibitem[Zadeh \& Desoer(2008)Zadeh and Desoer]{zadeh2008linear}
Zadeh, L. and Desoer, C.
\newblock \emph{Linear system theory: the state space approach}.
\newblock Courier Dover Publications, 2008.

\end{thebibliography}
