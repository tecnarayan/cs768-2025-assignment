\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22--31,
  2017.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
Argall, B.~D., Chernova, S., Veloso, M., and Browning, B.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and Autonomous Systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Azizzadenesheli et~al.(2018)Azizzadenesheli, Brunskill, and
  Anandkumar]{azizzadenesheli2018bayesian}
Azizzadenesheli, K., Brunskill, E., and Anandkumar, A.
\newblock Efficient exploration through bayesian deep q-networks.
\newblock \emph{arXiv preprint arXiv:1802.04412}, 2018.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{bertsekas1995dynamic}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena scientific Belmont, MA, 1996.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{OpenAIGym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Buckman et~al.(2018)Buckman, Hafner, Tucker, Brevdo, and
  Lee]{buckman2018sample}
Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H.
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8234--8244, 2018.

\bibitem[Chemali \& Lazaric(2015)Chemali and Lazaric]{chemali2015direct}
Chemali, J. and Lazaric, A.
\newblock Direct policy iteration with demonstrations.
\newblock In \emph{Proceedings of the Twenty-Fourth International Joint
  Conference on Artificial Intelligence}, 2015.

\bibitem[Cheng et~al.(2018)Cheng, Yan, Wagener, and Boots]{cheng2018fast}
Cheng, C.-A., Yan, X., Wagener, N., and Boots, B.
\newblock Fast policy learning through imitation and reinforcement.
\newblock \emph{arXiv preprint arXiv:1805.10413}, 2018.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018deep}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  4759--4770, 2018.

\bibitem[Dayan \& Watkins(1992)Dayan and Watkins]{dayan1992q}
Dayan, P. and Watkins, C. J. C.~H.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 279--292, 1992.

\bibitem[de~Bruin et~al.(2015)de~Bruin, Kober, Tuyls, and
  Babu{\v{s}}ka]{de2015expreplay}
de~Bruin, T., Kober, J., Tuyls, K., and Babu{\v{s}}ka, R.
\newblock The importance of experience replay database composition in deep
  reinforcement learning.
\newblock In \emph{Deep Reinforcement Learning Workshop, NIPS}, 2015.

\bibitem[de~Bruin et~al.(2016)de~Bruin, Kober, Tuyls, and
  Babu{\v{s}}ka]{de2016improved}
de~Bruin, T., Kober, J., Tuyls, K., and Babu{\v{s}}ka, R.
\newblock Improved deep reinforcement learning for robotics through
  distribution-based experience retention.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, pp.\  3947--3952. IEEE, 2016.

\bibitem[Dearden et~al.(1998)Dearden, Friedman, and
  Russell]{dearden1998bayesian}
Dearden, R., Friedman, N., and Russell, S.
\newblock Bayesian q-learning.
\newblock In \emph{AAAI/IAAI}, pp.\  761--768, 1998.

\bibitem[Deisenroth \& Rasmussen(2011)Deisenroth and
  Rasmussen]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  465--472, 2011.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmark}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1329--1338, 2016.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 503--556, 2005.

\bibitem[Evans(2016)]{evans2016learning}
Evans, O.
\newblock Learning the preferences of ignorant, inconsistent agents.
\newblock In \emph{AAAI}, pp.\  323--329, 2016.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pp.\  1587--1596. PMLR, 2018.

\bibitem[Gal et~al.(2016)Gal, McAllister, and Rasmussen]{gal2016improving}
Gal, Y., McAllister, R., and Rasmussen, C.~E.
\newblock Improving pilco with bayesian neural network dynamics models.
\newblock In \emph{Data-Efficient Machine Learning workshop, International
  Conference on Machine Learning}, 2016.

\bibitem[Gao et~al.(2018)Gao, Lin, Yu, Levine, and Darrell]{gao2018imperfect}
Gao, Y., Lin, J., Yu, F., Levine, S., and Darrell, T.
\newblock Reinforcement learning from imperfect demonstrations.
\newblock \emph{arXiv preprint arXiv:1802.05313}, 2018.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{goodfellow2013empirical}
Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6211}, 2013.

\bibitem[Gordon(1995)]{gordon1995stable}
Gordon, G.~J.
\newblock Stable function approximation in dynamic programming.
\newblock In \emph{Machine Learning Proceedings 1995}, pp.\  261--268.
  Elsevier, 1995.

\bibitem[{Henderson} et~al.(2017){Henderson}, {Islam}, {Bachman}, {Pineau},
  {Precup}, and {Meger}]{hendersonRL2017}
{Henderson}, P., {Islam}, R., {Bachman}, P., {Pineau}, J., {Precup}, D., and
  {Meger}, D.
\newblock {Deep Reinforcement Learning that Matters}.
\newblock \emph{arXiv preprint arXiv:1709.06560}, 2017.

\bibitem[Hester et~al.(2017)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Dulac-Arnold, et~al.]{hester2017deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Dulac-Arnold, G., et~al.
\newblock Deep q-learning from demonstrations.
\newblock \emph{arXiv preprint arXiv:1704.03732}, 2017.

\bibitem[Higuera et~al.(2018)Higuera, Meger, and
  Dudek]{higuera2018synthesizing}
Higuera, J. C.~G., Meger, D., and Dudek, G.
\newblock Synthesizing neural network controllers with probabilistic model
  based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.02291}, 2018.

\bibitem[Ho et~al.(2016)Ho, Gupta, and Ermon]{ho2016model}
Ho, J., Gupta, J., and Ermon, S.
\newblock Model-free imitation learning with policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2760--2769, 2016.

\bibitem[Hussein et~al.(2017)Hussein, Gaber, Elyan, and
  Jayne]{hussein2017imitation}
Hussein, A., Gaber, M.~M., Elyan, E., and Jayne, C.
\newblock Imitation learning: A survey of learning methods.
\newblock \emph{ACM Computing Surveys (CSUR)}, 50\penalty0 (2):\penalty0 21,
  2017.

\bibitem[Isele \& Cosgun(2018)Isele and Cosgun]{isele2018selective}
Isele, D. and Cosgun, A.
\newblock Selective experience replay for lifelong learning.
\newblock \emph{arXiv preprint arXiv:1802.10269}, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2016doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  652--661, 2016.

\bibitem[Johannink et~al.(2018)Johannink, Bahl, Nair, Luo, Kumar, Loskyll,
  Ojea, Solowjow, and Levine]{johannink2018residual}
Johannink, T., Bahl, S., Nair, A., Luo, J., Kumar, A., Loskyll, M., Ojea,
  J.~A., Solowjow, E., and Levine, S.
\newblock Residual reinforcement learning for robot control.
\newblock \emph{arXiv preprint arXiv:1812.03201}, 2018.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, volume~2,
  pp.\  267--274, 2002.

\bibitem[Kim et~al.(2013)Kim, Farahmand, Pineau, and Precup]{kim2013learning}
Kim, B., Farahmand, A.-m., Pineau, J., and Precup, D.
\newblock Learning from limited demonstrations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2859--2867, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Konda \& Tsitsiklis(2003)Konda and Tsitsiklis]{konda2003onactor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock On actor-critic algorithms.
\newblock \emph{SIAM journal on Control and Optimization}, 42\penalty0
  (4):\penalty0 1143--1166, 2003.

\bibitem[Lai \& Robbins(1985)Lai and Robbins]{lai1985asymptotically}
Lai, T.~L. and Robbins, H.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in Applied Mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pp.\  45--73. Springer, 2012.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{DDPG}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lin(1992)]{expreplay1992}
Lin, L.-J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Liu et~al.(2018)Liu, Gottesman, Raghu, Komorowski, Faisal,
  Doshi-Velez, and Brunskill]{liu2018representation}
Liu, Y., Gottesman, O., Raghu, A., Komorowski, M., Faisal, A.~A., Doshi-Velez,
  F., and Brunskill, E.
\newblock Representation balancing mdps for off-policy policy evaluation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2644--2653, 2018.

\bibitem[Lu et~al.(2018)Lu, Schuurmans, and Boutilier]{lu2018delusional}
Lu, T., Schuurmans, D., and Boutilier, C.
\newblock Non-delusional q-learning and value-iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9971--9981, 2018.

\bibitem[McCloskey \& Cohen(1989)McCloskey and
  Cohen]{mccloskey1989catastrophic}
McCloskey, M. and Cohen, N.~J.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of Learning and Motivation}, volume~24, pp.\
  109--165. Elsevier, 1989.

\bibitem[Melo(2001)]{melo2001convergence}
Melo, F.~S.
\newblock Convergence of q-learning: A simple proof.
\newblock \emph{Institute Of Systems and Robotics, Tech. Rep}, pp.\  1--4,
  2001.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{DQN}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1054--1062, 2016.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{nair2018overcoming}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  6292--6299. IEEE, 2018.

\bibitem[O'Donoghue et~al.(2018)O'Donoghue, Osband, Munos, and
  Mnih]{UncertaintyBellman}
O'Donoghue, B., Osband, I., Munos, R., and Mnih, V.
\newblock The uncertainty {B}ellman equation and exploration.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pp.\  3839--3848. PMLR, 2018.

\bibitem[Ormoneit \& Sen(2002)Ormoneit and Sen]{ormoneit2002kernel}
Ormoneit, D. and Sen, {\'S}.
\newblock Kernel-based reinforcement learning.
\newblock \emph{Machine learning}, 49\penalty0 (2-3):\penalty0 161--178, 2002.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4026--4034, 2016.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband2018randomized}
Osband, I., Aslanides, J., and Cassirer, A.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  8626--8638, 2018.

\bibitem[Peshkin \& Shelton(2002)Peshkin and Shelton]{peshkin2002learning}
Peshkin, L. and Shelton, C.~R.
\newblock Learning from scarce experience.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  498--505, 2002.

\bibitem[Peters \& M{\"u}lling(2010)Peters and M{\"u}lling]{peters2010relative}
Peters, J. and M{\"u}lling, K.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI}, pp.\  1607--1612, 2010.

\bibitem[Pham et~al.(2018)Pham, De~Magistris, Agravante, Chaudhury, Munawar,
  and Tachibana]{pham2018constrained}
Pham, T.-H., De~Magistris, G., Agravante, D.~J., Chaudhury, S., Munawar, A.,
  and Tachibana, R.
\newblock Constrained exploration and recovery from experience shaping.
\newblock \emph{arXiv preprint arXiv:1809.08925}, 2018.

\bibitem[Piot et~al.(2014)Piot, Geist, and Pietquin]{piot2014boosted}
Piot, B., Geist, M., and Pietquin, O.
\newblock Boosted bellman residual minimization handling expert demonstrations.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  549--564. Springer, 2014.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
Precup, D., Sutton, R.~S., and Dasgupta, S.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  417--424, 2001.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:1401.4082}, 2014.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Riedmiller, M.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European Conference on Machine Learning}, pp.\  317--328.
  Springer, 2005.

\bibitem[Schaal(1999)]{schaal1999imitation}
Schaal, S.
\newblock Is imitation learning the route to humanoid robots?
\newblock \emph{Trends in Cognitive Sciences}, 3\penalty0 (6):\penalty0
  233--242, 1999.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{DPG}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  387--395, 2014.

\bibitem[Silver et~al.(2018)Silver, Allen, Tenenbaum, and
  Kaelbling]{silver2018residual}
Silver, T., Allen, K., Tenenbaum, J., and Kaelbling, L.
\newblock Residual policy learning.
\newblock \emph{arXiv preprint arXiv:1812.06298}, 2018.

\bibitem[Singh et~al.(2000)Singh, Jaakkola, Littman, and
  Szepesv{\'a}ri]{singh2000convergence}
Singh, S., Jaakkola, T., Littman, M.~L., and Szepesv{\'a}ri, C.
\newblock Convergence results for single-step on-policy reinforcement-learning
  algorithms.
\newblock \emph{Machine learning}, 38\penalty0 (3):\penalty0 287--308, 2000.

\bibitem[Sohn et~al.(2015)Sohn, Lee, and Yan]{sohn2015learning}
Sohn, K., Lee, H., and Yan, X.
\newblock Learning structured output representation using deep conditional
  generative models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3483--3491, 2015.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Sun et~al.(2017)Sun, Venkatraman, Gordon, Boots, and
  Bagnell]{sun2017deeply}
Sun, W., Venkatraman, A., Gordon, G.~J., Boots, B., and Bagnell, J.~A.
\newblock Deeply aggrevated: Differentiable imitation learning for sequential
  prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3309--3318, 2017.

\bibitem[Sun et~al.(2018)Sun, Bagnell, and Boots]{sun2018truncated}
Sun, W., Bagnell, J.~A., and Boots, B.
\newblock Truncated horizon policy search: Combining reinforcement learning \&
  imitation learning.
\newblock \emph{arXiv preprint arXiv:1805.11240}, 2018.

\bibitem[Sutton(1988)]{sutton1988tdlearning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Thomas et~al.(2015)Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015high}
Thomas, P., Theocharous, G., and Ghavamzadeh, M.
\newblock High confidence policy improvement.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2380--2388, 2015.

\bibitem[Thrun \& Schwartz(1993)Thrun and Schwartz]{thrun1993bias}
Thrun, S. and Schwartz, A.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In \emph{Proceedings of the 1993 Connectionist Models Summer School
  Hillsdale, NJ. Lawrence Erlbaum}, 1993.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Touati et~al.(2018)Touati, Satija, Romoff, Pineau, and
  Vincent]{touati2018randomized}
Touati, A., Satija, H., Romoff, J., Pineau, J., and Vincent, P.
\newblock Randomized value functions via multiplicative normalizing flows.
\newblock \emph{arXiv preprint arXiv:1806.02315}, 2018.

\bibitem[Van~Hasselt(2010)]{hasselt2010double}
Van~Hasselt, H.
\newblock Double q-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2613--2621, 2010.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{DoubleDQN}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, pp.\  2094--2100, 2016.

\bibitem[Van~Hoof et~al.(2017)Van~Hoof, Neumann, and Peters]{van2017non}
Van~Hoof, H., Neumann, G., and Peters, J.
\newblock Non-parametric policy search with limited information loss.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 2472--2517, 2017.

\bibitem[Ve{\v{c}}er{\'\i}k et~al.(2017)Ve{\v{c}}er{\'\i}k, Hester, Scholz,
  Wang, Pietquin, Piot, Heess, Roth{\"o}rl, Lampe, and
  Riedmiller]{vevcerik2017leveraging}
Ve{\v{c}}er{\'\i}k, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot,
  B., Heess, N., Roth{\"o}rl, T., Lampe, T., and Riedmiller, M.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Watkins(1989)]{watkins1989qlearning}
Watkins, C. J. C.~H.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, King's College, Cambridge, 1989.

\bibitem[Xu et~al.(2018)Xu, Li, Tian, Darrell, and Ma]{xu2018algorithmic}
Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T.
\newblock Algorithmic framework for model-based reinforcement learning with
  theoretical guarantees.
\newblock \emph{arXiv preprint arXiv:1807.03858}, 2018.

\bibitem[Zhang \& Sutton(2017)Zhang and Sutton]{zhang2017expreplay}
Zhang, S. and Sutton, R.~S.
\newblock A deeper look at experience replay.
\newblock \emph{arXiv preprint arXiv:1712.01275}, 2017.

\end{thebibliography}
