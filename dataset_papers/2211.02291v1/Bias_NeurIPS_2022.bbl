\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahng et~al.(2020)Bahng, Chun, Yun, Choo, and Oh]{bahng2019rebias}
Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong~Joon Oh.
\newblock Learning de-biased representations with biased representations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Bang et~al.(2022)Bang, Baek, Kim, Jeon, Kim, Kim, Lee, and
  Shim]{logitmix}
Duhyeon Bang, Kyungjune Baek, Jiwoo Kim, Yunho Jeon, Jin-Hwa Kim, Jiwon Kim,
  Jongwuk Lee, and Hyunjung Shim.
\newblock Logit mixing training for more reliable and accurate prediction.
\newblock In \emph{Proceedings of the Thirty-First International Joint
  Conference on Artificial Intelligence, {IJCAI-22}}, pages 2812--2819, 2022.

\bibitem[Bao et~al.(2022)Bao, Chang, and Barzilay]{bao2022learning}
Yujia Bao, Shiyu Chang, and Regina Barzilay.
\newblock Learning stable classifiers by transferring unstable features.
\newblock In \emph{International Conference on Machine Learning}, pages
  1483--1507. PMLR, 2022.

\bibitem[Brendel and Bethge(2018)]{brendel2018approximating}
Wieland Brendel and Matthias Bethge.
\newblock Approximating cnns with bag-of-local-features models works
  surprisingly well on imagenet.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Cadene et~al.(2019)Cadene, Dancette, Cord, Parikh,
  et~al.]{cadene2019rubi}
Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et~al.
\newblock Rubi: Reducing unimodal biases for visual question answering.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Dagaev et~al.(2021)Dagaev, Roads, Luo, Barry, Patil, and
  Love]{dagaev2021too}
Nikolay Dagaev, Brett~D Roads, Xiaoliang Luo, Daniel~N Barry, Kaustubh~R Patil,
  and Bradley~C Love.
\newblock A too-good-to-be-true prior to reduce shortcut reliance.
\newblock \emph{arXiv preprint arXiv:2102.06406}, 2021.

\bibitem[Giannone et~al.(2021)Giannone, Havrylov, Massiah, Yilmaz, and
  Jiao]{giannone2021just}
Giorgio Giannone, Serhii Havrylov, Jordan Massiah, Emine Yilmaz, and Yunlong
  Jiao.
\newblock Just mix once: Mixing samples with implicit group distribution.
\newblock In \emph{NeurIPS 2021 Workshop on Distribution Shifts: Connecting
  Methods and Applications}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 9729--9738, 2020.

\bibitem[Hong and Yang(2021)]{hong2021unbiased}
Youngkyu Hong and Eunho Yang.
\newblock Unbiased classification through bias-contrastive and bias-balanced
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 4401--4410, 2019.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18661--18673, 2020.

\bibitem[Kim et~al.(2019)Kim, Kim, Kim, Kim, and Kim]{kim2019learning}
Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim.
\newblock Learning not to learn: Training deep neural networks with biased
  data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9012--9020, 2019.

\bibitem[Kim et~al.(2021)Kim, Lee, and Choo]{kim2021biaswap}
Eungyeup Kim, Jihyeon Lee, and Jaegul Choo.
\newblock Biaswap: Removing dataset bias with bias-tailored swapping
  augmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 14992--15001, 2021.

\bibitem[Kim et~al.(2020)Kim, Choo, and Song]{kim2020puzzle}
Jang-Hyun Kim, Wonho Choo, and Hyun~Oh Song.
\newblock Puzzle mix: Exploiting saliency and local statistics for optimal
  mixup.
\newblock In \emph{International Conference on Machine Learning}, pages
  5275--5285. PMLR, 2020.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun and Cortes(2010)]{lecun-mnisthandwrittendigit-2010}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Lee et~al.(2021)Lee, Kim, Lee, Lee, and Choo]{lee2021learning}
Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo.
\newblock Learning debiased representation via disentangled feature
  augmentation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Li and Vasconcelos(2019)]{li2019repair}
Yi~Li and Nuno Vasconcelos.
\newblock Repair: Removing representation bias by dataset resampling.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9572--9581, 2019.

\bibitem[Liu et~al.(2021)Liu, Haghgoo, Chen, Raghunathan, Koh, Sagawa, Liang,
  and Finn]{liu2021just}
Evan~Z Liu, Behzad Haghgoo, Annie~S Chen, Aditi Raghunathan, Pang~Wei Koh,
  Shiori Sagawa, Percy Liang, and Chelsea Finn.
\newblock Just train twice: Improving group robustness without training group
  information.
\newblock In \emph{International Conference on Machine Learning}, pages
  6781--6792. PMLR, 2021.

\bibitem[Locatello et~al.(2019)Locatello, Bauer, Lucic, Raetsch, Gelly,
  Sch{\"o}lkopf, and Bachem]{locatello2019challenging}
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly,
  Bernhard Sch{\"o}lkopf, and Olivier Bachem.
\newblock Challenging common assumptions in the unsupervised learning of
  disentangled representations.
\newblock In \emph{international conference on machine learning}, pages
  4114--4124. PMLR, 2019.

\bibitem[Nam et~al.(2020)Nam, Cha, Ahn, Lee, and Shin]{nam2020learning}
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin.
\newblock Learning from failure: De-biasing classifier from biased classifier.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20673--20684, 2020.

\bibitem[Park et~al.(2022)Park, Yun, and Chun]{park2022unified}
Chanwoo Park, Sangdoo Yun, and Sanghyuk Chun.
\newblock A unified analysis of mixed sample data augmentation: A loss function
  perspective.
\newblock \emph{arXiv preprint arXiv:2208.09913}, 2022.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, Belinkov, and Rush]{sanh2020learning}
Victor Sanh, Thomas Wolf, Yonatan Belinkov, and Alexander~M Rush.
\newblock Learning from others' mistakes: Avoiding dataset biases without
  modeling them.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Sch{\"o}lkopf et~al.(2021)Sch{\"o}lkopf, Locatello, Bauer, Ke,
  Kalchbrenner, Goyal, and Bengio]{scholkopf2021toward}
Bernhard Sch{\"o}lkopf, Francesco Locatello, Stefan Bauer, Nan~Rosemary Ke, Nal
  Kalchbrenner, Anirudh Goyal, and Yoshua Bengio.
\newblock Toward causal representation learning.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 612--634,
  2021.

\bibitem[Scimeca et~al.(2022)Scimeca, Oh, Chun, Poli, and
  Yun]{scimeca2022which}
Luca Scimeca, Seong~Joon Oh, Sanghyuk Chun, Michael Poli, and Sangdoo Yun.
\newblock Which shortcut cues will {DNN}s choose? a study from the
  parameter-space perspective.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfalls}
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth
  Netrapalli.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9573--9585, 2020.

\bibitem[Tartaglione et~al.(2021)Tartaglione, Barbano, and
  Grangetto]{tartaglione2021end}
Enzo Tartaglione, Carlo~Alberto Barbano, and Marco Grangetto.
\newblock End: Entangling and disentangling deep representations for bias
  correction.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13508--13517, 2021.

\bibitem[Teney et~al.(2022)Teney, Abbasnejad, Lucey, and
  Hengel]{teney2021evading}
Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van~den Hengel.
\newblock Evading the simplicity bias: Training a diverse set of models
  discovers solutions with superior ood generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2022.

\bibitem[Van~der Maaten and Hinton(2008)]{van2008visualizing}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Wang et~al.(2018)Wang, He, Lipton, and Xing]{wang2018learning}
Haohan Wang, Zexue He, Zachary~C Lipton, and Eric~P Xing.
\newblock Learning robust representations by projecting superficial statistics
  out.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Yao et~al.(2022)Yao, Wang, Li, Zhang, Liang, Zou, and
  Finn]{yao2022improving}
Huaxiu Yao, Yu~Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea
  Finn.
\newblock Improving out-of-distribution robustness via selective augmentation.
\newblock \emph{arXiv preprint arXiv:2201.00299}, 2022.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zhang et~al.(2022)Zhang, Lopez-Paz, and Bottou]{zhang2022rich}
Jianyu Zhang, David Lopez-Paz, and L{\'e}on Bottou.
\newblock Rich feature construction for the optimization-generalization
  dilemma.
\newblock \emph{arXiv preprint arXiv:2203.15516}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Deng, Kawaguchi, Ghorbani, and
  Zou]{zhang2020does}
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou.
\newblock How does mixup help with robustness and generalization?
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang and Sabuncu(2018)]{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\end{thebibliography}
