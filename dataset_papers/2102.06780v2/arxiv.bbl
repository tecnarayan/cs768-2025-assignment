\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agafonov et~al.(2021)Agafonov, Dvurechensky, Scutari, Gasnikov,
  Kamzolov, Lukashevich, and Daneshmand]{agafonov2021accelerated}
Agafonov, A., Dvurechensky, P., Scutari, G., Gasnikov, A., Kamzolov, D.,
  Lukashevich, A., and Daneshmand, A.
\newblock An accelerated second-order method for distributed stochastic
  optimization.
\newblock \emph{arXiv:2103.14392}, 2021.

\bibitem[Arjevani \& Shamir(2015)Arjevani and Shamir]{Arjevani-ShamirNIPS15}
Arjevani, Y. and Shamir, O.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems (NIPS)}, volume~1, pp.\  1756--1764, December
  2015.

\bibitem[Bach(2010)]{Bach10}
Bach, F.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4:\penalty0 384--414., 2010.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and McAulffe]{Bartlett06}
Bartlett, P.~L., Jordan, M.~I., and McAulffe, J.~D.
\newblock Convexity, classiÔ¨Åcation, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Bekkerman et~al.(2011)Bekkerman, Bilenko, and
  Langford]{Bekkerman_book11}
Bekkerman, R., Bilenko, M., and Langford, J.
\newblock \emph{Scaling up Machine Learning: Parallel and Distributed
  Approaches}.
\newblock Cambridge University Press, 2011.

\bibitem[Berthier et~al.(2020)Berthier, Bach, and Gaillard]{Berthier2020}
Berthier, R., Bach, F., and Gaillard, P.
\newblock Accelerated gossip in networks of given dimension using jacobi
  polynomial iterations.
\newblock \emph{SIAM J. on Mathematics of Data Science}, 1:\penalty0 24--47,
  2020.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou18}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Bousquet(2002)]{Bousquet02}
Bousquet, O.
\newblock \emph{Concentration inequalities and empirical processes theory
  applied to the analysis of learning algorithms}.
\newblock PhD thesis, Ecole Polytechnique: Department of Applied Mathematics
  Paris, France, 2002.

\bibitem[{Di Lorenzo} \& Scutari(2016){Di Lorenzo} and Scutari]{NEXT16}
{Di Lorenzo}, P. and Scutari, G.
\newblock {NEXT: I}n-network nonconvex optimization.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 2\penalty0 (2):\penalty0 120--136, June 2016.

\bibitem[Dvurechensky et~al.(2021)Dvurechensky, Kamzolov, Lukashevich, Lee,
  Ordentlich, Uribe, and Gasnikov]{dvurechensky2021hyperfast}
Dvurechensky, P., Kamzolov, D., Lukashevich, A., Lee, S., Ordentlich, E.,
  Uribe, C.~A., and Gasnikov, A.
\newblock Hyperfast second-order local solvers for efficient statistically
  preconditioned distributed optimization.
\newblock \emph{arXiv:2102.08246}, 2021.

\bibitem[Eisen et~al.(2019)Eisen, Mokhtari, and Ribeiro]{MokhtariNewton3}
Eisen, M., Mokhtari, A., and Ribeiro, A.
\newblock A primal-dual quasi-newton method for exact consensus optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (23):\penalty0 5983--5997, 2019.

\bibitem[Fan et~al.(2019)Fan, Guo, and Wang]{Fan2020}
Fan, J., Guo, Y., and Wang, K.
\newblock Communication-efficient accurate statistical estimation.
\newblock \emph{arXiv:1906.04870}, 2019.

\bibitem[Flake \& Lawrence(2002)Flake and Lawrence]{flake2002efficient}
Flake, G.~W. and Lawrence, S.
\newblock Efficient {SVM} regression training with {SMO}.
\newblock \emph{Machine Learning}, 46\penalty0 (1):\penalty0 271--290, 2002.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and Sidford]{Frostig15}
Frostig, R., Ge, R., Kakade, S.~M., and Sidford, A.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In \emph{Conference on learning theory (COLT)}, pp.\  728--763, 2015.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Rogozin, Beznosikov, Dvinskikh, and
  Gasnikov]{gorbunov2020recent}
Gorbunov, E., Rogozin, A., Beznosikov, A., Dvinskikh, D., and Gasnikov, A.
\newblock Recent theoretical advances in decentralized distributed convex
  optimization.
\newblock \emph{arXiv:2011.13259}, 2020.

\bibitem[Hendrikx et~al.(2020{\natexlab{a}})Hendrikx, Bach, and
  Massoulie]{hendrikx2020optimal}
Hendrikx, H., Bach, F., and Massoulie, L.
\newblock An optimal algorithm for decentralized finite sum optimization.
\newblock \emph{arXiv:2005.10675}, 2020{\natexlab{a}}.

\bibitem[Hendrikx et~al.(2020{\natexlab{b}})Hendrikx, Xiao, Bubeck, Bach, and
  Massoulie]{pmlr-v119-hendrikx20a}
Hendrikx, H., Xiao, L., Bubeck, S., Bach, F., and Massoulie, L.
\newblock Statistically preconditioned accelerated gradient method for
  distributed optimization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pp.\  4203--4227, 13--18 Jul 2020{\natexlab{b}}.

\bibitem[{Jadbabaie} et~al.(2009){Jadbabaie}, {Ozdaglar}, and
  {Zargham}]{jadbabaie2009distributed}
{Jadbabaie}, A., {Ozdaglar}, A., and {Zargham}, M.
\newblock A distributed newton method for network optimization.
\newblock In \emph{Proceedings of the 48h IEEE Conference on Decision and
  Control (CDC) held jointly with 2009 28th Chinese Control Conference}, pp.\
  2736--2741, 2009.
\newblock \doi{10.1109/CDC.2009.5400289}.

\bibitem[Jahani et~al.(2020)Jahani, He, Ma, Mokhtari, Mudigere, Ribeiro, and
  Takac]{DANCE20}
Jahani, M., He, X., Ma, C., Mokhtari, A., Mudigere, D., Ribeiro, A., and Takac,
  M.
\newblock Efficient distributed hessian free algorithm for large-scale
  empirical risk minimization via accumulating sample strategy.
\newblock In \emph{Twenty Third International Conference on Artificial
  Intelligence and Statistics}, pp.\  2634--2644, 2020.

\bibitem[Jakoveti{\'c} et~al.(2014)Jakoveti{\'c}, Xavier, and
  Moura]{jakovetic2014fast}
Jakoveti{\'c}, D., Xavier, J., and Moura, J.~M.
\newblock Fast distributed gradient methods.
\newblock \emph{IEEE Transactions on Automatic Control}, 59\penalty0
  (5):\penalty0 1131--1146, 2014.

\bibitem[Jiaojiao et~al.(2020)Jiaojiao, Ling, and So]{So2020}
Jiaojiao, Z., Ling, Q., and So, A.
\newblock A newton tracking algorithm with exact linear convergence rate for
  decentralized consensus optimization.
\newblock In \emph{59th IEEE Conference on Decision and Control (CDC)}, 2020.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in neural information processing systems},
  26:\penalty0 315--323, 2013.

\bibitem[Kovalev et~al.(2020)Kovalev, Salim, and
  Richt{\'a}rik]{kovalev2020optimal}
Kovalev, D., Salim, A., and Richt{\'a}rik, P.
\newblock Optimal and practical algorithms for smooth and strongly convex
  decentralized optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Lan et~al.(2017)Lan, Lee, and Zhou]{lan2017communication}
Lan, G., Lee, S., and Zhou, Y.
\newblock Communication-efficient algorithms for decentralized and stochastic
  optimization.
\newblock \emph{Mathematical Programming}, pp.\  1--48, 2017.

\bibitem[Li et~al.(2019)Li, Cen, Chen, and Chi]{NetDane}
Li, B., Cen, S., Chen, Y., and Chi, Y.
\newblock Communication-efficient distributed optimization in networks with
  gradient tracking and variance reduction.
\newblock \emph{arXiv:1909.05844v3}, 2019.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and Liu.]{Lian17}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu., J.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5330--5340, 2017.

\bibitem[Ma \& Takac(2017)Ma and Takac]{Ma17}
Ma, C. and Takac, M.
\newblock Distributed inexact damped newton method: Data partitioning and
  work-balancing.
\newblock In \emph{Workshops at the Thirty-First AAAI Conference on Artificial
  Intelligence}, 2017.

\bibitem[Mokhtari et~al.(2016{\natexlab{a}})Mokhtari, Daneshmand, Lucchi,
  Hofmann, and Ribeiro]{ADA-Newton16}
Mokhtari, A., Daneshmand, H., Lucchi, A., Hofmann, T., and Ribeiro, A.
\newblock Adaptive newton method for empirical risk minimization to statistical
  accuracy.
\newblock In \emph{Proceedings of the Advances in Neural Information Processing
  Systems}, pp.\  4062--4070, 2016{\natexlab{a}}.

\bibitem[Mokhtari et~al.(2016{\natexlab{b}})Mokhtari, Ling, and
  Ribeiro]{mokhtari2016network}
Mokhtari, A., Ling, Q., and Ribeiro, A.
\newblock Network newton distributed optimization methods.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (1):\penalty0 146--161, 2016{\natexlab{b}}.

\bibitem[Mokhtari et~al.(2016{\natexlab{c}})Mokhtari, Shi, Ling, and
  Ribeiro]{MokhtariDQM16}
Mokhtari, A., Shi, W., Ling, Q., and Ribeiro, A.
\newblock {DQM}: Decentralized quadratically approximated alternating direction
  method of multipliers.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (19):\penalty0 5158--5173, 2016{\natexlab{c}}.

\bibitem[Mokhtari et~al.(2016{\natexlab{d}})Mokhtari, Shi, Ling, and
  Ribeiro]{MokhtariNewton2}
Mokhtari, A., Shi, W., Ling, Q., and Ribeiro, A.
\newblock A decentralized second-order method with exact linear convergence
  rate for consensus optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 2\penalty0
  (4):\penalty0 507--522, 2016{\natexlab{d}}.

\bibitem[Mokhtari et~al.(2017)Mokhtari, Ling, and Ribeiro]{Network-Newton17}
Mokhtari, A., Ling, Q., and Ribeiro, A.
\newblock Network newton distributed optimization methods.
\newblock \emph{IEEE Transactions on Signal Processing}, 65:\penalty0 146--161,
  2017.

\bibitem[Nediƒá et~al.(2018)Nediƒá, Olshevsky, and Rabbat]{Nedich_tutorial}
Nediƒá, A., Olshevsky, A., and Rabbat, M.~G.
\newblock Network topology and communication ‚Äì computation tradeoffs in
  decentralized optimization.
\newblock \emph{Proceedings of the IEEE}, 106\penalty0 (5):\penalty0 953--976,
  2018.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Nedic, A., Olshevsky, A., and Shi, W.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Nesterov, Y.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Nesterov \& Polyak(2006)Nesterov and Polyak]{Nesterov--Cubic06}
Nesterov, Y. and Polyak, B.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108:\penalty0 177--205, 2006.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Reddi, S.~J., Kone{\v{c}}n{\`y}, J., Richt{\'a}rik, P., P{\'o}cz{\'o}s, B., and
  Smola, A.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv:1608.06879}, 2016.

\bibitem[Rogozin et~al.(2020)Rogozin, Lukoshkin, Gasnikov, Kovalev, and
  Shulgin]{rogozin2020towards}
Rogozin, A., Lukoshkin, V., Gasnikov, A., Kovalev, D., and Shulgin, E.
\newblock Towards accelerated rates for distributed optimization over
  time-varying networks.
\newblock \emph{arXiv:2009.11069}, 2020.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Scaman, K., Bach, F., Bubeck, S., Lee, Y.~T., and Massouli{\'e}, L.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70, pp.\  3027--3036, 2017.

\bibitem[Shai \& Ben-David(2014)Shai and Ben-David]{SHai-Shalev-book}
Shai, S.-S. and Ben-David, S.
\newblock \emph{Understanding Machine Learning: From Theory to Algorihtms}.
\newblock Cambridge University Press, 2014.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{Shwartz_et_al}
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K.
\newblock Stochastic convex optimization.
\newblock In \emph{Proceedings of the 22nd Annual Conference on Learning Theory
  (COLT)}, Montreal, Canada, June 18-21 2009.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{Shwartz-JMLR2010}
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 2635--2670,
  2010.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{DANE}
Shamir, O., Srebro, N., and Zhang, T.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning (PMLR)}, volume~32, pp.\  1000--1008, 2014.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Shi, W., Ling, Q., Wu, G., and Yin, W.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Soori et~al.(2020)Soori, Mishchenko, Mokhtari, Dehnavi, and
  Gurbuzbalaban]{pmlr-v108-soori20a}
Soori, S., Mishchenko, K., Mokhtari, A., Dehnavi, M.~M., and Gurbuzbalaban, M.
\newblock Dave-qn: A distributed averaged quasi-newton method with local
  superlinear convergence rate.
\newblock In \emph{Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}, volume 108, pp.\  1965--1976, 2020.

\bibitem[Sridharan et~al.(2008)Sridharan, Shalev-Shwartz, and
  Srebro]{sridharan2008fast}
Sridharan, K., Shalev-Shwartz, S., and Srebro, N.
\newblock Fast rates for regularized objectives.
\newblock \emph{Advances in neural information processing systems},
  21:\penalty0 1545--1552, 2008.

\bibitem[Sun \& Tran-Dinh(2019)Sun and Tran-Dinh]{Sun19-Self-concordance}
Sun, T. and Tran-Dinh, Q.
\newblock Generalized self-concordant functions: a recipe for newton-type
  methods.
\newblock \emph{Mathematical Programming}, 178:\penalty0 145--213, 2019.

\bibitem[Sun et~al.(2019)Sun, Daneshmand, and Scutari]{sun2019distributed}
Sun, Y., Daneshmand, A., and Scutari, G.
\newblock Distributed optimization based on gradient-tracking revisited:
  Enhancing convergence rate via surrogation.
\newblock \emph{arXiv:1905.02637}, 2019.

\bibitem[{Tutunov} et~al.(2019){Tutunov}, {Bou-Ammar}, and
  {Jadbabaie}]{tutunov2019distributed}
{Tutunov}, R., {Bou-Ammar}, H., and {Jadbabaie}, A.
\newblock Distributed newton method for large-scale consensus optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 64\penalty0
  (10):\penalty0 3983--3994, 2019.
\newblock \doi{10.1109/TAC.2019.2907711}.

\bibitem[Uribe \& Jadbabaie(2020{\natexlab{a}})Uribe and Jadbabaie]{Uribe20}
Uribe, C.~A. and Jadbabaie, A.
\newblock A distributed cubic-regularized newton method for smooth convex
  optimization over networks.
\newblock \emph{arXiv:2007.03562}, 2020{\natexlab{a}}.

\bibitem[Uribe \& Jadbabaie(2020{\natexlab{b}})Uribe and
  Jadbabaie]{uribe2020distributed}
Uribe, C.~A. and Jadbabaie, A.
\newblock A distributed cubic-regularized newton method for smooth convex
  optimization over networks, 2020{\natexlab{b}}.

\bibitem[Uribe et~al.(2020)Uribe, Lee, Gasnikov, and Nedi{\'c}]{uribe2020dual}
Uribe, C.~A., Lee, S., Gasnikov, A., and Nedi{\'c}, A.
\newblock A dual approach for optimal algorithms in distributed optimization
  over networks.
\newblock \emph{Optimization Methods and Software}, pp.\  1--40, 2020.

\bibitem[Vapnik(2013)]{Vapnik13}
Vapnik, V.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem[Wang et~al.(2018)Wang, Roosta-Khorasani, Xu, and Mahoney]{GIANT}
Wang, S., Roosta-Khorasani, F., Xu, P., and Mahoney, M.~W.
\newblock Giant: Globally improved approximate newton method for distributed
  optimization.
\newblock In \emph{Proceedings of the 32nd 32nd International Conference on
  Neural Information Processing Systems}, volume~37, pp.\  2338--2348, 2018.

\bibitem[{Wei} et~al.(2013){Wei}, {Ozdaglar}, and
  {Jadbabaie}]{wei2013distributed}
{Wei}, E., {Ozdaglar}, A., and {Jadbabaie}, A.
\newblock A distributed newton method for network utility maximization‚Äîpart
  ii: Convergence.
\newblock \emph{IEEE Transactions on Automatic Control}, 58\penalty0
  (9):\penalty0 2176--2188, 2013.
\newblock \doi{10.1109/TAC.2013.2253223}.

\bibitem[Wien(2011)]{auzinger2011iterative}
Wien, A.
\newblock \emph{Iterative solution of large linear systems}.
\newblock Lecture Notes, TU Wien, 2011.

\bibitem[Xiao et~al.(2007)Xiao, Boyd, and Kim]{xiao2007distributed}
Xiao, L., Boyd, S., and Kim, S.-J.
\newblock Distributed average consensus with least-mean-square deviation.
\newblock \emph{Journal of parallel and distributed computing}, 67\penalty0
  (1):\penalty0 33--46, 2007.

\bibitem[Xu et~al.(2018)Xu, Zhu, Soh, and Xie]{Xu-TAC:hs}
Xu, J., Zhu, S., Soh, Y.~C., and Xie, L.
\newblock {Convergence of Asynchronous Distributed Gradient Methods Over
  Stochastic Networks}.
\newblock \emph{IEEE Transactions on Automatic Control}, 63\penalty0
  (2):\penalty0 434--448, 2018.

\bibitem[Yuan \& Li(2019)Yuan and Li]{yuan2019convergence}
Yuan, X.-T. and Li, P.
\newblock On convergence of distributed approximate newton methods:
  Globalization, sharper bounds and beyond.
\newblock \emph{arXiv:1908.02246}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, You, and Basar]{Zhang20}
Zhang, J., You, K., and Basar, T.
\newblock Distributed adaptive newton methods with globally superlinear
  convergence.
\newblock \emph{arXiv:2002.07378}, 2020.

\bibitem[Zhang \& Xiao(2015)Zhang and Xiao]{DISCO}
Zhang, Y. and Xiao, L.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (PMLR)}, volume~37, pp.\  362--370, 2015.

\end{thebibliography}
