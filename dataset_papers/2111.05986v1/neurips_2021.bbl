\begin{thebibliography}{10}

\bibitem{Abascal2005}
J.~L.~F. Abascal and C.~Vega.
\newblock A general purpose model for the condensed phases of water:
  Tip4p/2005.
\newblock {\em J. Chem. Phys.}, 123:234505, 2005.

\bibitem{abraham2008foundations}
R.~Abraham and J.E. Marsden.
\newblock {\em Foundations of Mechanics}.
\newblock AMS Chelsea publishing. AMS Chelsea Pub./American Mathematical
  Society, 2008.

\bibitem{allen2020lagnetvip}
Christine Allen-Blanchette, Sushant Veer, Anirudha Majumdar, and Naomi~Ehrich
  Leonard.
\newblock {LagNetViP}: A {L}agrangian neural network for video prediction.
\newblock In {\em Proceedings of AAAI Conference on Artificial Intelligence},
  2020.

\bibitem{azencot2020forecasting}
Omri Azencot, N~Benjamin Erichson, Vanessa Lin, and Michael~W Mahoney.
\newblock Forecasting sequential data using consistent {K}oopman autoencoders.
\newblock In {\em Proceedings of International Conference on Machine Learning
  (ICML)}, 2020.

\bibitem{bailey2019multi}
James~P Bailey and Georgios Piliouras.
\newblock Multi-agent learning in network zero-sum games is a hamiltonian
  system.
\newblock In {\em International Conference on Autonomous Agents and MultiAgent
  Systems}, pages 233--241, 2019.

\bibitem{balduzzi2018mechanics}
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls,
  and Thore Graepel.
\newblock The mechanics of n-player differentiable games.
\newblock In {\em International Conference on Machine Learning}, pages
  354--363, 2018.

\bibitem{Botev2021Which}
Alexandar Botev, Andrew Jaegle, Peter Wirnsberger, Daniel Hennes, and Irina
  Higgins.
\newblock Which priors matter? {B}enchmarking models for learning latent
  dynamics.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem{chen2019symplectic}
Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and L{\'e}on Bottou.
\newblock Symplectic recurrent neural networks.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{choudhary2020forecasting}
Anshul Choudhary, John~F Lindner, Elliott~G Holliday, Scott~T Miller, Sudeshna
  Sinha, and William~L Ditto.
\newblock Forecasting {H}amiltonian dynamics without canonical coordinates.
\newblock {\em Nonlinear Dynamics}, pages 1--10, 2020.

\bibitem{Cornell1995}
Wendy~D. Cornell, Piotr Cieplak, Christopher~I. Bayly, Ian~R. Gould, Kenneth~M.
  Merz, David~M. Ferguson, David~C. Spellmeyer, Thomas Fox, James~W. Caldwell,
  and Peter~A. Kollman.
\newblock A second generation force field for the simulation of proteins,
  nucleic acids, and organic molecules.
\newblock {\em J. Am. Chem. Soc.}, 117(19):5179--5197, 1995.

\bibitem{cranmer2020lagrangian}
Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel,
  and Shirley Ho.
\newblock {L}agrangian neural networks.
\newblock In {\em ICLR Deep Differential Equations Workshop}, 2020.

\bibitem{desai2020vign}
Shaan~A. Desai, Marios Mattheakis, and Stephen~J. Roberts.
\newblock Variational integrator graph networks for learning energy-conserving
  dynamical systems.
\newblock {\em Physical Review E.}, 104(3), 2020.

\bibitem{devore2008probability}
Jay~L. Devore.
\newblock {\em Probability and Statistics for Engineering and the Sciences}.
\newblock Spinger, 2008.

\bibitem{dipietro2020sparse}
Daniel~M. DiPietro, Shiying Xiong, and Bo~Zhu.
\newblock Sparse symplectically integrated neural networks.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{Dosovitskiy2016Generating}
Alexey Dosovitskiy and Thomas Brox.
\newblock Generating images with perceptual similarity metrics based on deep
  networks.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem{duan2019unsupervised}
Sunny Duan, Loic Matthey, Andre Saraiva, Nicholas Watters, Christopher~P
  Burgess, Alexander Lerchner, and Irina Higgins.
\newblock Unsupervised model selection for variational disentangled
  representation learning.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{finzi2020simplifying}
Marc Finzi, Ke~Alexander Wang, and Andrew~Gordon Wilson.
\newblock Simplifying {H}amiltonian and {L}agrangian neural networks via
  explicit constraints.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{frank2008symplectic}
Jason Frank.
\newblock Symplectic flows and maps and volume preservation.
\newblock https://webspace.science.uu.nl/~frank011/Classes/numwisk/ch16.pdf,
  2008.

\bibitem{Frenkel2002}
Daan Frenkel and Berend Smit.
\newblock {\em Understanding Molecular Simulation}.
\newblock Academic Press, San Diego, second edition, 2002.

\bibitem{greydanus2019hamiltonian}
Samuel Greydanus, Misko Dzamba, and Jason Yosinski.
\newblock {H}amiltonian neural networks.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem{heim2019rodent}
Niklas Heim, V{\'a}clav {\v{S}}m{\'\i}dl, and Tom{\'a}{\v{s}} Pevn{\`y}.
\newblock Rodent: Relevance determination in differential equations.
\newblock {\em arXiv preprint arXiv:1912.00656}, 2019.

\bibitem{higgins2017betavae}
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
  Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock $\beta$-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem{Hoover1985}
William~G. Hoover.
\newblock Canonical dynamics: Equilibrium phase-space distributions.
\newblock {\em Physical Review A}, 31:1695--1697, 1985.

\bibitem{huh2020time}
In~Huh, Eunho Yang, Sung~Ju Hwang, and Jinwoo Shin.
\newblock Time-reversal symmetric {ODE} network.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{Jeffs2020classical}
Maxim Jeffs.
\newblock {Harvard, Lecture Notes: Classical mechanics and symplectic
  geometry}, 2020.
\newblock URL: \url{http://people.math.harvard.edu/~jeffs/SymplecticNotes.pdf}.

\bibitem{jin2020sympnets}
Pengzhan Jin, Zhen Zhang, Aiqing Zhu, Yifa Tang, and George~Em Karniadakis.
\newblock {S}ymp{N}ets: Intrinsic structure-preserving symplectic networks for
  identifying {H}amiltonian systems.
\newblock {\em Neural Networks}, 132:166--179, 2020.

\bibitem{Jones1924}
J.~E. Jones and Sydney Chapman.
\newblock On the determination of molecular fields. --{II}. {F}rom the equation
  of state of a gas.
\newblock {\em Proc. R. Soc. Lond. A}, 106(738):463--477, 1924.

\bibitem{multiobjectdatasets19}
Rishabh Kabra, Chris Burgess, Loic Matthey, Raphael~Lopez Kaufman, Klaus Greff,
  Malcolm Reynolds, and Alexander Lerchner.
\newblock Multi-object datasets.
\newblock https://github.com/deepmind/multi-object-datasets/, 2019.

\bibitem{landau2013course}
Lev~Davidovich Landau and Evgenii~Mikhailovich Lifshitz.
\newblock {\em Course of theoretical physics}.
\newblock Elsevier, 2013.

\bibitem{Larsen2016autoencoding}
Anders Boesen~Lindbo Larsen, Søren~Kaae Sønderby, Hugo Larochelle, and Ole
  Winther.
\newblock Autoencoding beyond pixels using a learned similarity metric.
\newblock In {\em Proceedings of International Conference on Machine Learning
  (ICML)}, 2016.

\bibitem{li2020visual}
Yunzhu Li, Toru Lin, Kexin Yi, Daniel Bear, Daniel~LK Yamins, Jiajun Wu,
  Joshua~B Tenenbaum, and Antonio Torralba.
\newblock Visual grounding of learned physical models.
\newblock In {\em Proceedings of International Conference on Machine Learning
  (ICML)}, 2020.

\bibitem{lutter2019deep}
Michael Lutter, Christian Ritter, and Jan Peters.
\newblock Deep {L}agrangian networks: Using physics as model prior for deep
  learning.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, and Andrew~Y Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em Proceedings of International Conference on Machine Learning
  (ICML)}, 2013.

\bibitem{mescheder2018training}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for {GAN}s do actually converge?
\newblock In {\em Proceedings of International Conference on Machine Learning
  (ICML)}, 2018.

\bibitem{mescheder2017numerics}
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
\newblock The numerics of {GAN}s.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem{metz2016unrolled}
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein.
\newblock Unrolled generative adversarial networks.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem{Nose1984}
Shuichi Nos{\'e}.
\newblock A unified formulation of the constant temperature molecular dynamics
  methods.
\newblock {\em J. Chem. Phys.}, 81(1):511--519, 1984.

\bibitem{qin2020training}
Chongli Qin, Yan Wu, Jost~Tobias Springenberg, Andrew Brock, Jeff Donahue,
  Timothy~P Lillicrap, and Pushmeet Kohli.
\newblock Training generative adversarial networks by solving ordinary
  differential equations.
\newblock In {\em Proceedings of Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem{ramachandran2017swish}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Swish: a self-gated activation function.
\newblock {\em arXiv preprint arXiv:1710.05941}, 2017.

\bibitem{rath2020symplectic}
Katharina Rath, Christopher~G. Albert, Bernd Bischl, and Udo von Toussaint.
\newblock Symplectic {G}aussian process regression of maps in {H}amiltonian
  systems.
\newblock {\em Chaos}, 31, 2021.

\bibitem{rezende2019equivariant}
Danilo~Jimenez Rezende, S{\'e}bastien Racani{\`e}re, Irina Higgins, and Peter
  Toth.
\newblock Equivariant {H}amiltonian flows.
\newblock In {\em NeurIPS workshop: Machine Learning and the Physical
  Sciences}, 2019.

\bibitem{rezende2018taming}
Danilo~Jimenez Rezende and Fabio Viola.
\newblock Taming {VAE}s.
\newblock {\em arXiv preprint arXiv:1810.00597}, 2018.

\bibitem{roehrl2020modeling}
Manuel~A Roehrl, Thomas~A Runkler, Veronika Brandtstetter, Michel Tokic, and
  Stefan Obermayer.
\newblock Modeling system dynamics with physics-informed neural networks based
  on {L}agrangian mechanics.
\newblock In {\em International Federation of Automatic Control (IFAC) World
  Congress}, 2020.

\bibitem{Rybkin2018reasonable}
Oleh Rybkin.
\newblock {The reasonable ineffectiveness of pixel metrics for future
  prediction (and what to do about it)}, 2018.
\newblock URL: \url{https://bit.ly/2YHPRg2}.

\bibitem{saemundsson2020variational}
Steindor Saemundsson, Alexander Terenin, Katja Hofmann, and Marc Deisenroth.
\newblock Variational integrator networks for physically structured embeddings.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem{Stewart2016advanced}
Iain~W. Stewart.
\newblock {MIT, Lecture Notes: Advanced Classical Mechanics}, 2016.
\newblock URL: \url{https://bit.ly/3oP7fuc}.

\bibitem{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em Proceedings of IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS)}, 2012.

\bibitem{tong2020symplectic}
Yunjin Tong, Shiying Xiong, Xingzhe He, Guanghan Pan, and Bo~Zhu.
\newblock Symplectic neural networks in {T}aylor series form for {H}amiltonian
  systems.
\newblock {\em Journal of Computational Physics}, 437, 2020.

\bibitem{toth2020hamiltonian}
Peter Toth, Danilo~Jimenez Rezende, Andrew Jaegle, S{\'e}bastien Racani{\`e}re,
  Aleksandar Botev, and Irina Higgins.
\newblock {H}amiltonian generative networks.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{udrescu2020symbolic}
Silviu-Marian Udrescu and Max Tegmark.
\newblock Symbolic pregression: Discovering physical laws from raw distorted
  video.
\newblock {\em Physical Review E}, 103, 2021.

\bibitem{WardeFarley2017Improving}
David Warde-Farley and Yoshua Bengio.
\newblock Improving generative adversarial networks with denoising feature
  matching.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem{watters2019spatial}
Nicholas Watters, Loic Matthey, Christopher~P Burgess, and Alexander Lerchner.
\newblock Spatial broadcast decoder: A simple architecture for learning
  disentangled representations in {VAE}s.
\newblock {\em arXiv preprint arXiv:1901.07017}, 2019.

\bibitem{wu2005hamiltotian}
Jinshan Wu.
\newblock {H}amiltonian formalism of game theory.
\newblock {\em arXiv preprint quant-ph/0501088}, 2005.

\bibitem{xiong2020nonseparable}
Shiying Xiong, Yunjin Tong, Xingzhe He, Cheng Yang, Shuqi Yang, and Bo~Zhu.
\newblock Nonseparable symplectic neural networks.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{zhong2020dissipative}
Yaofeng~Desmond Zhong, Biswadip Dey, and Amit Chakraborty.
\newblock Dissipative {S}ym{ODEN}et: Encoding {H}amiltonian dynamics with
  dissipation and control into deep learning.
\newblock {\em arXiv preprint arXiv:2002.08860}, 2020.

\bibitem{zhong2019symplectic}
Yaofeng~Desmond Zhong, Biswadip Dey, and Amit Chakraborty.
\newblock Symplectic {ODE}-net: Learning {H}amiltonian dynamics with control.
\newblock In {\em Proceedings of International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem{zhong2020benchmarking}
Yaofeng~Desmond Zhong, Biswadip Dey, and Amit Chakraborty.
\newblock Benchmarking energy-conserving neural networks for learning dynamics
  from data.
\newblock In {\em Conference on Learning for Dynamics and Control (L4DC)},
  2021.

\bibitem{zhong2020unsupervised}
Yaofeng~Desmond Zhong and Naomi Leonard.
\newblock Unsupervised learning of {L}agrangian dynamics from images for
  prediction and control.
\newblock {\em Proceedings of Neural Information Processing Systems (NeurIPS)},
  2020.

\end{thebibliography}
