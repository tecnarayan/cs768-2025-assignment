\begin{thebibliography}{100}

\bibitem{agarwal2005geometric}
Pankaj~K Agarwal, Sariel Har-Peled, Kasturi~R Varadarajan, et~al.
\newblock Geometric approximation via coresets.
\newblock {\em Combinatorial and computational geometry}, 2005.

\bibitem{alfeld2016data}
Scott Alfeld, Xiaojin Zhu, and Paul Barford.
\newblock Data poisoning attacks against autoregressive models.
\newblock In {\em AAAI}, 2016.

\bibitem{alfeld2017explicit}
Scott Alfeld, Xiaojin Zhu, and Paul Barford.
\newblock Explicit defense actions against test-set attacks.
\newblock In {\em AAAI}, 2017.

\bibitem{allgower2012numerical}
Eugene~L Allgower and Kurt Georg.
\newblock {\em Numerical continuation methods: an introduction}, volume~13.
\newblock Springer Science \& Business Media, 2012.

\bibitem{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio~G{\'o}mez Colmenarejo, Matthew~W
  Hoffman, David Pfau, Tom Schaul, Brendan Shillingford, and Nando de~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In {\em NeurIPS}, 2016.

\bibitem{armijo1966minimization}
Larry Armijo.
\newblock Minimization of functions having lipschitz continuous first partial
  derivatives.
\newblock {\em Pacific Journal of mathematics}, 1966.

\bibitem{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In {\em ICML}, 2009.

\bibitem{chen2020angular}
Beidi Chen, Weiyang Liu, Zhiding Yu, Jan Kautz, Anshumali Shrivastava, Animesh
  Garg, and Animashree Anandkumar.
\newblock Angular visual hardness.
\newblock In {\em ICML}, 2020.

\bibitem{chen2018near}
Yuxin Chen, Oisin Mac~Aodha, Shihan Su, Pietro Perona, and Yisong Yue.
\newblock Near-optimal machine teaching via explanatory teaching sets.
\newblock In {\em AISTATS}, 2018.

\bibitem{chen2018understanding}
Yuxin Chen, Adish Singla, Oisin Mac~Aodha, Pietro Perona, and Yisong Yue.
\newblock Understanding the role of adaptivity in machine teaching: The case of
  version space learners.
\newblock In {\em NeurIPS}, 2018.

\bibitem{chorowski2016towards}
Jan Chorowski and Navdeep Jaitly.
\newblock Towards better decoding and language model integration in sequence to
  sequence models.
\newblock {\em arXiv preprint arXiv:1612.02695}, 2016.

\bibitem{dai2018coupled}
Bo~Dai, Hanjun Dai, Niao He, Weiyang Liu, Zhen Liu, Jianshu Chen, Lin Xiao, and
  Le~Song.
\newblock Coupled variational bayes via optimization embedding.
\newblock In {\em NeurIPS}, 2018.

\bibitem{doliwa2014recursive}
Thorsten Doliwa, Gaojian Fan, Hans~Ulrich Simon, and Sandra Zilles.
\newblock Recursive teaching dimension, vc-dimension and sample compression.
\newblock {\em JMLR}, 2014.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2020.

\bibitem{eaves2016infant}
Baxter~S Eaves~Jr, Naomi~H Feldman, Thomas~L Griffiths, and Patrick Shafto.
\newblock Infant-directed speech is consistent with teaching.
\newblock {\em Psychological review}, 2016.

\bibitem{eaves2016toward}
Baxter~S Eaves~Jr and Patrick Shafto.
\newblock Toward a general, scaleable framework for bayesian teaching with
  applications to topic models.
\newblock {\em arXiv preprint arXiv:1605.07999}, 2016.

\bibitem{elman1993learning}
Jeffrey~L Elman.
\newblock Learning and development in neural networks: The importance of
  starting small.
\newblock {\em Cognition}, 1993.

\bibitem{fan2018learning}
Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu.
\newblock Learning to teach.
\newblock In {\em ICLR}, 2018.

\bibitem{fan2020learning}
Yang Fan, Yingce Xia, Lijun Wu, Shufang Xie, Weiqing Liu, Jiang Bian, Tao Qin,
  and Xiang-Yang Li.
\newblock Learning to reweight with deep interactions.
\newblock In {\em AAAI}, 2021.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em ICML}, 2017.

\bibitem{gao2015teaching}
Ziyuan Gao, Hans~Ulrich Simon, and Sandra Zilles.
\newblock On the teaching complexity of linear sets.
\newblock In {\em ALT}, 2015.

\bibitem{Goldman1995}
S.A. Goldman and M.J. Kearns.
\newblock On the complexity of teaching.
\newblock {\em Journal of Computer and System Sciences}, 1995.

\bibitem{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock In {\em ICLR}, 2016.

\bibitem{haug2018teaching}
Luis Haug, Sebastian Tschiatschek, and Adish Singla.
\newblock Teaching inverse reinforcement learners via features and
  demonstrations.
\newblock In {\em NeurIPS}, 2018.

\bibitem{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em arXiv preprint arXiv:1512.03385}, 2015.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, 2017.

\bibitem{huggins2016coresets}
Jonathan~H Huggins, Trevor Campbell, and Tamara Broderick.
\newblock Coresets for scalable bayesian logistic regression.
\newblock In {\em NIPS}, 2016.

\bibitem{hunziker2019teaching}
Anette Hunziker, Yuxin Chen, Oisin Mac~Aodha, Manuel~Gomez Rodriguez, Andreas
  Krause, Pietro Perona, Yisong Yue, and Adish Singla.
\newblock Teaching multiple concepts to a forgetful learner.
\newblock In {\em NeurIPS}, 2019.

\bibitem{jia2016dynamic}
Xu~Jia, Bert De~Brabandere, Tinne Tuytelaars, and Luc Van~Gool.
\newblock Dynamic filter networks.
\newblock In {\em NIPS}, 2016.

\bibitem{johns2015becoming}
Edward Johns, Oisin Mac~Aodha, and Gabriel~J Brostow.
\newblock Becoming the expert-interactive multi-class machine teaching.
\newblock In {\em CVPR}, 2015.

\bibitem{kamalaruban2019interactive}
Parameswaran Kamalaruban, Rati Devidze, Volkan Cevher, and Adish Singla.
\newblock Interactive teaching algorithms for inverse reinforcement learning.
\newblock {\em arXiv preprint arXiv:1905.11867}, 2019.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em NeurIPS}, 2012.

\bibitem{krueger2009flexible}
Kai~A Krueger and Peter Dayan.
\newblock Flexible shaping: How learning in small steps helps.
\newblock {\em Cognition}, 2009.

\bibitem{kumar2020average}
Akash Kumar, Adish Singla, Yisong Yue, and Yuxin Chen.
\newblock Average-case complexity of teaching convex polytopes via halfspace
  queries.
\newblock {\em arXiv preprint arXiv:2006.14677}, 2020.

\bibitem{kumar2020teaching}
Akash Kumar, Hanqi Zhang, Adish Singla, and Yuxin Chen.
\newblock The teaching dimension of kernel perceptron.
\newblock {\em arXiv preprint arXiv:2010.14043}, 2020.

\bibitem{kumar2010self}
M~Pawan Kumar, Benjamin Packer, and Daphne Koller.
\newblock Self-paced learning for latent variable models.
\newblock In {\em NeurIPS}, 2010.

\bibitem{lessard2019optimal}
Laurent Lessard, Xuezhou Zhang, and Xiaojin Zhu.
\newblock An optimal control approach to sequential machine teaching.
\newblock In {\em AISTATS}, 2019.

\bibitem{li2016learning}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock {\em arXiv preprint arXiv:1606.01885}, 2016.

\bibitem{lin2003some}
Gui-Hua Lin and M~Fukushima.
\newblock Some exact penalty results for nonlinear programs and mathematical
  programs with equilibrium constraints.
\newblock {\em Journal of Optimization Theory and Applications}, 2003.

\bibitem{lin2020regularizing}
Rongmei Lin, Weiyang Liu, Zhen Liu, Chen Feng, Zhiding Yu, James~M Rehg,
  Li~Xiong, and Le~Song.
\newblock Regularizing neural networks via minimizing hyperspherical energy.
\newblock In {\em CVPR}, 2020.

\bibitem{liu2016teaching}
Ji~Liu, Xiaojin Zhu, and H~Gorune Ohannessian.
\newblock The teaching dimension of linear learners.
\newblock In {\em ICML}, 2016.

\bibitem{liu2017iterative}
Weiyang Liu, Bo~Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda~B. Smith,
  James~M. Rehg, and Le~Song.
\newblock Iterative machine teaching.
\newblock In {\em ICML}, 2017.

\bibitem{liu2018blackbox}
Weiyang Liu, Bo~Dai, Xingguo Li, Zhen Liu, James Rehg, and Le~Song.
\newblock Towards black-box iterative machine teaching.
\newblock In {\em ICML}, 2018.

\bibitem{liu2018learning}
Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo~Dai, and Le~Song.
\newblock Learning towards minimum hyperspherical energy.
\newblock In {\em NeurIPS}, 2018.

\bibitem{liu2021orthogonal}
Weiyang Liu, Rongmei Lin, Zhen Liu, James~M Rehg, Liam Paull, Li~Xiong,
  Le~Song, and Adrian Weller.
\newblock Orthogonal over-parameterized training.
\newblock In {\em CVPR}, 2021.

\bibitem{liu2021learning}
Weiyang Liu, Rongmei Lin, Zhen Liu, Li~Xiong, Bernhard Sch{\"o}lkopf, and
  Adrian Weller.
\newblock Learning with hyperspherical uniformity.
\newblock In {\em AISTATS}, 2021.

\bibitem{liu2019neural}
Weiyang Liu, Zhen Liu, James~M Rehg, and Le~Song.
\newblock Neural similarity learning.
\newblock {\em NeurIPS}, 2019.

\bibitem{liu2018decoupled}
Weiyang Liu, Zhen Liu, Zhiding Yu, Bo~Dai, Rongmei Lin, Yisen Wang, James~M
  Rehg, and Le~Song.
\newblock Decoupled networks.
\newblock In {\em CVPR}, 2018.

\bibitem{liu2017sphereface}
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le~Song.
\newblock Sphereface: Deep hypersphere embedding for face recognition.
\newblock In {\em CVPR}, 2017.

\bibitem{liu2016large}
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang.
\newblock Large-margin softmax loss for convolutional neural networks.
\newblock In {\em ICML}, 2016.

\bibitem{liu2017deep}
Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhen Liu, Bo~Dai, Tuo Zhao, and
  Le~Song.
\newblock Deep hyperspherical learning.
\newblock In {\em NIPS}, 2017.

\bibitem{lopez2015unifying}
David Lopez-Paz, L{\'e}on Bottou, Bernhard Sch{\"o}lkopf, and Vladimir Vapnik.
\newblock Unifying distillation and privileged information.
\newblock {\em arXiv preprint arXiv:1511.03643}, 2015.

\bibitem{ma2019policy}
Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu.
\newblock Policy poisoning in batch reinforcement learning and control.
\newblock In {\em NeurIPS}, 2019.

\bibitem{mac2018teaching}
Oisin Mac~Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue.
\newblock Teaching categories to human learners with visual explanations.
\newblock In {\em CVPR}, 2018.

\bibitem{mansouri2019preference}
Farnam Mansouri, Yuxin Chen, Ara Vartanian, Jerry Zhu, and Adish Singla.
\newblock Preference-based batch and sequential teaching: Towards a unified
  view of models.
\newblock In {\em NeurIPS}, 2019.

\bibitem{mei2015using}
Shike Mei and Xiaojin Zhu.
\newblock Using machine teaching to identify optimal training-set attacks on
  machine learners.
\newblock In {\em AAAI}, 2015.

\bibitem{monga2021algorithm}
Vishal Monga, Yuelong Li, and Yonina~C Eldar.
\newblock Algorithm unrolling: Interpretable, efficient deep learning for
  signal and image processing.
\newblock {\em IEEE Signal Processing Magazine}, 2021.

\bibitem{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey~E Hinton.
\newblock When does label smoothing help?
\newblock In {\em NeurIPS}, 2019.

\bibitem{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 2009.

\bibitem{peltola2019machine}
Tomi Peltola, Mustafa~Mert {\c{C}}elikok, Pedram Daee, and Samuel Kaski.
\newblock Machine teaching of active sequential learners.
\newblock In {\em NeurIPS}, 2019.

\bibitem{peterson2004day}
Gail~B Peterson.
\newblock A day of great illumination: Bf skinner's discovery of shaping.
\newblock {\em Journal of the experimental analysis of behavior}, 2004.

\bibitem{rakhsha2020policy}
Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla.
\newblock Policy teaching via environment poisoning: Training-time adversarial
  attacks against reinforcement learning.
\newblock In {\em ICML}, 2020.

\bibitem{roosta2016sub}
Farbod Roosta-Khorasani and Michael~W Mahoney.
\newblock Sub-sampled newton methods ii: Local convergence rates.
\newblock {\em arXiv preprint arXiv:1601.04738}, 2016.

\bibitem{samei2014algebraic}
Rahim Samei, Pavel Semukhin, Boting Yang, and Sandra Zilles.
\newblock Algebraic methods proving sauer's bound for teaching complexity.
\newblock {\em Theoretical Computer Science}, 2014.

\bibitem{shafto2008teaching}
Patrick Shafto and Noah Goodman.
\newblock Teaching games: Statistical sampling assumptions for learning in
  pedagogical situations.
\newblock In {\em CogSci}, 2008.

\bibitem{shafto2012learning}
Patrick Shafto, Noah~D Goodman, and Michael~C Frank.
\newblock Learning from others: The consequences of psychological reasoning for
  human learning.
\newblock {\em Perspectives on Psychological Science}, 2012.

\bibitem{shafto2014rational}
Patrick Shafto, Noah~D Goodman, and Thomas~L Griffiths.
\newblock A rational account of pedagogical reasoning: Teaching by, and
  learning from, examples.
\newblock {\em Cognitive psychology}, 2014.

\bibitem{shafto2021cooperative}
Patrick Shafto, Junqi Wang, and Pei Wang.
\newblock Cooperative communication as belief transport.
\newblock {\em Trends in Cognitive Sciences}, 2021.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{singla2013actively}
Adish Singla, Ilija Bogunovic, G~Bart{\'o}k, A~Karbasi, and A~Krause.
\newblock On actively teaching the crowd to classify.
\newblock In {\em NeurIPS Workshop on Data Driven Education}, 2013.

\bibitem{singla2014near}
Adish Singla, Ilija Bogunovic, Gabor Bartok, Amin Karbasi, and Andreas Krause.
\newblock Near-optimally teaching the crowd to classify.
\newblock In {\em ICML}, 2014.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em CVPR}, 2016.

\bibitem{tabibian2019enhancing}
Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard
  Sch{\"o}lkopf, and Manuel Gomez-Rodriguez.
\newblock Enhancing human learning via spaced repetition optimization.
\newblock {\em Proceedings of the National Academy of Sciences}, 2019.

\bibitem{thiel2008classification}
Christian Thiel.
\newblock Classification on soft labels is robust against label noise.
\newblock In {\em International Conference on Knowledge-Based and Intelligent
  Information and Engineering Systems}, pages 65--73. Springer, 2008.

\bibitem{tschiatschek2019learner}
Sebastian Tschiatschek, Ahana Ghosh, Luis Haug, Rati Devidze, and Adish Singla.
\newblock Learner-aware teaching: Inverse reinforcement learning with
  preferences and constraints.
\newblock In {\em NeurIPS}, 2019.

\bibitem{vapnik2015learning}
Vladimir Vapnik and Rauf Izmailov.
\newblock Learning using privileged information: similarity control and
  knowledge transfer.
\newblock {\em JMLR}, 2015.

\bibitem{Vapnik2009privileged}
Vladimir Vapnik and Akshay Vashist.
\newblock A new learning paradigm: Learning using privileged information.
\newblock {\em Neural Networks}, 22(5):544--557, 2009.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em arXiv preprint arXiv:1706.03762}, 2017.

\bibitem{vaswani2019painless}
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
  Simon Lacoste-Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock In {\em NeurIPS}, 2019.

\bibitem{wang2018additive}
Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng.
\newblock Additive margin softmax for face verification.
\newblock {\em arXiv preprint arXiv:1801.05599}, 2018.

\bibitem{wang2020sequential}
Junqi Wang, Pei Wang, and Patrick Shafto.
\newblock Sequential cooperative bayesian inference.
\newblock In {\em ICML}, 2020.

\bibitem{wang2019generalizing}
Pei Wang, Pushpi Paranamana, and Patrick Shafto.
\newblock Generalizing the theory of cooperative inference.
\newblock In {\em AISTATS}, 2019.

\bibitem{wang2020mathematical}
Pei Wang, Junqi Wang, Pushpi Paranamana, and Patrick Shafto.
\newblock A mathematical theory of cooperative communication.
\newblock In {\em NeurIPS}, 2020.

\bibitem{werbos1988generalization}
Paul~J Werbos.
\newblock Generalization of backpropagation with application to a recurrent gas
  market model.
\newblock {\em Neural networks}, 1988.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 1992.

\bibitem{wolsey1999integer}
Laurence~A Wolsey and George~L Nemhauser.
\newblock {\em Integer and combinatorial optimization}.
\newblock John Wiley \& Sons, 1999.

\bibitem{wu2018learning}
Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Jianhuang Lai, and Tie-Yan
  Liu.
\newblock Learning to teach with dynamic loss functions.
\newblock In {\em NeurIPS}, 2010.

\bibitem{xie2016disturblabel}
Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, and Qi~Tian.
\newblock Disturblabel: Regularizing cnn on the loss layer.
\newblock In {\em CVPR}, 2016.

\bibitem{xie2020self}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In {\em CVPR}, 2020.

\bibitem{Xu2021LST}
Zhaozhuo Xu, Beidi Chen, Chaojian Li, Weiyang Liu, Le~Song, Yingyan Lin, and
  Anshumali Shrivastava.
\newblock Locality sensitive teaching.
\newblock In {\em NeurIPS}, 2021.

\bibitem{yang2017explainable}
Scott Cheng-Hsin Yang and Patrick Shafto.
\newblock Explainable artificial intelligence via bayesian teaching.
\newblock In {\em NIPS 2017 workshop on Teaching Machines, Robots, and Humans},
  2017.

\bibitem{yang2018optimal}
Scott Cheng-Hsin Yang, Yue Yu, Pei Wang, Wai~Keen Vong, Patrick Shafto, et~al.
\newblock Optimal cooperative inference.
\newblock In {\em AISTATS}, 2018.

\bibitem{yeo2019iterative}
Teresa Yeo, Parameswaran Kamalaruban, Adish Singla, Arpit Merchant, Thibault
  Asselborn, Louis Faucon, Pierre Dillenbourg, and Volkan Cevher.
\newblock Iterative classroom teaching.
\newblock In {\em AAAI}, 2019.

\bibitem{zhang2020teaching}
Xuezhou Zhang, Shubham~Kumar Bharti, Yuzhe Ma, Adish Singla, and Xiaojin Zhu.
\newblock The teaching dimension of q-learning.
\newblock {\em arXiv preprint arXiv:2006.09324}, 2020.

\bibitem{zhang2020adaptive}
Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu.
\newblock Adaptive reward-poisoning attacks against reinforcement learning.
\newblock In {\em ICML}, 2020.

\bibitem{zhang2020online}
Xuezhou Zhang, Xiaojin Zhu, and Laurent Lessard.
\newblock Online data poisoning attacks.
\newblock In {\em L4DC}, 2020.

\bibitem{zhang2018training}
Xuezhou Zhang, Xiaojin Zhu, and Stephen Wright.
\newblock Training set debugging using trusted items.
\newblock In {\em AAAI}, 2018.

\bibitem{zhao2015stochastic}
Peilin Zhao and Tong Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em ICML}, 2015.

\bibitem{zhou2018unlearn}
Yao Zhou, Arun~Reddy Nelakurthi, and Jingrui He.
\newblock Unlearn what you have learned: Adaptive crowd teaching with
  exponentially decayed memory learners.
\newblock In {\em KDD}, 2018.

\bibitem{zhou2020crowd}
Yao Zhou, Arun~Reddy Nelakurthi, Ross Maciejewski, Wei Fan, and Jingrui He.
\newblock Crowd teaching with imperfect labels.
\newblock In {\em WWW}, 2020.

\bibitem{zhu2013machine}
Xiaojin Zhu.
\newblock Machine teaching for bayesian learners in the exponential family.
\newblock In {\em NeurIPS}, 2013.

\bibitem{zhu2015machine}
Xiaojin Zhu.
\newblock Machine teaching: An inverse problem to machine learning and an
  approach toward optimal education.
\newblock In {\em AAAI}, 2015.

\bibitem{zhu2009introduction}
Xiaojin Zhu and Andrew~B Goldberg.
\newblock Introduction to semi-supervised learning.
\newblock {\em Synthesis lectures on artificial intelligence and machine
  learning}, 2009.

\bibitem{zhu2017no}
Xiaojin Zhu, Ji~Liu, and Manuel Lopes.
\newblock No learner left behind: On the complexity of teaching multiple
  learners simultaneously.
\newblock In {\em IJCAI}, 2017.

\bibitem{zhu2018overview}
Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna~N Rafferty.
\newblock An overview of machine teaching.
\newblock {\em arXiv preprint arXiv:1801.05927}, 2018.

\bibitem{zi2021revisiting}
Bojia Zi, Shihao Zhao, Xingjun Ma, and Yu-Gang Jiang.
\newblock Revisiting adversarial robustness distillation: Robust soft labels
  make student better.
\newblock In {\em ICCV}, 2021.

\bibitem{zilles2008teaching}
Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich.
\newblock Teaching dimensions based on cooperative learning.
\newblock In {\em COLT}, 2008.

\end{thebibliography}
