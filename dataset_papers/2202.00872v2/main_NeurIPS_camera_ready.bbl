\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Absil et~al.(2005)Absil, Mahony, and Andrews]{absil2005}
P.-A. Absil, R.~Mahony, and B.~Andrews.
\newblock Convergence of the iterates of descent methods for analytic cost
  functions.
\newblock \emph{SIAM Journal on Optimization}, 16\penalty0 (2):\penalty0
  531--547, 2005.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and Mahajan]{agarwal2020}
A.~Agarwal, S.~M. Kakade, J.~D. Lee, and G.~Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift, 2020.

\bibitem[Amari(2012)]{amari2012differential}
S.-i. Amari.
\newblock \emph{Differential-geometrical methods in statistics}, volume~28.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Bai and Jin(2020)]{bai2020}
Y.~Bai and C.~Jin.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  551--560. PMLR, 2020.

\bibitem[Bowling and Veloso(2000)]{Bowling00}
M.~Bowling and M.~Veloso.
\newblock An analysis of stochastic game theory for multiagent reinforcement
  learning.
\newblock Technical report, Carnegie-Mellon Univ Pittsburgh Pa School of
  Computer Science, 2000.

\bibitem[Bu{\c{s}}oniu et~al.(2010)Bu{\c{s}}oniu, Babu{\v{s}}ka, and
  De~Schutter]{Bucsoniu10}
L.~Bu{\c{s}}oniu, R.~Babu{\v{s}}ka, and B.~De~Schutter.
\newblock Multi-agent reinforcement learning: An overview.
\newblock \emph{Innovations in multi-agent systems and applications-1}, pages
  183--221, 2010.

\bibitem[Cen et~al.(2021)Cen, Cheng, Chen, Wei, and Chi]{cen2021}
S.~Cen, C.~Cheng, Y.~Chen, Y.~Wei, and Y.~Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock \emph{Operations Research}, 2021.

\bibitem[Claes et~al.(2011)Claes, Holvoet, and Weyns]{claes2011}
R.~Claes, T.~Holvoet, and D.~Weyns.
\newblock A decentralized approach for anticipatory vehicle routing using
  delegate multiagent systems.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems},
  12\penalty0 (2):\penalty0 364--373, 2011.

\bibitem[Daskalakis et~al.(2021)Daskalakis, Foster, and
  Golowich]{daskalakis2021}
C.~Daskalakis, D.~J. Foster, and N.~Golowich.
\newblock Independent policy gradient methods for competitive reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2101.04233}, 2021.

\bibitem[Dechert and O’Donnell(2006)]{Dechert06}
W.~D. Dechert and S.~O’Donnell.
\newblock The stochastic lake game: A numerical solution.
\newblock \emph{Journal of Economic Dynamics and Control}, 30\penalty0
  (9-10):\penalty0 1569--1587, 2006.

\bibitem[Fox et~al.(2021)Fox, McAleer, Overman, and Panageas]{Fox21}
R.~Fox, S.~McAleer, W.~Overman, and I.~Panageas.
\newblock Independent natural policy gradient always converges in markov
  potential games.
\newblock \emph{CoRR}, abs/2110.10614, 2021.

\bibitem[Gonz{\'a}lez-S{\'a}nchez and Hern{\'a}ndez-Lerma(2013)]{Gonzalez13}
D.~Gonz{\'a}lez-S{\'a}nchez and O.~Hern{\'a}ndez-Lerma.
\newblock \emph{Discrete--time stochastic control and dynamic potential games:
  the Euler--Equation approach}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{haarnoja2017}
T.~Haarnoja, H.~Tang, P.~Abbeel, and S.~Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pages
  1352--1361. PMLR, 2017.

\bibitem[I{\~n}igo-Blasco et~al.(2012)I{\~n}igo-Blasco, Diaz-del Rio,
  Romero-Ternero, Cagigas-Mu{\~n}iz, and Vicente-Diaz]{inigo2012}
P.~I{\~n}igo-Blasco, F.~Diaz-del Rio, M.~C. Romero-Ternero,
  D.~Cagigas-Mu{\~n}iz, and S.~Vicente-Diaz.
\newblock Robotics software frameworks for multi-agent robotic systems
  development.
\newblock \emph{Robotics and Autonomous Systems}, 60\penalty0 (6):\penalty0
  803--821, 2012.

\bibitem[Kakade and Langford(2002)]{Kakade02}
S.~M. Kakade and J.~Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In C.~Sammut and A.~G. Hoffmann, editors, \emph{Machine Learning,
  Proceedings of the Nineteenth International Conference {(ICML} 2002),
  University of New South Wales, Sydney, Australia, July 8-12, 2002}, pages
  267--274. Morgan Kaufmann, 2002.

\bibitem[Khodadadian et~al.(2021)Khodadadian, Jhunjhunwala, Varma, and
  Maguluri]{Khodadadian2021}
S.~Khodadadian, P.~R. Jhunjhunwala, S.~M. Varma, and S.~T. Maguluri.
\newblock On the linear convergence of natural policy gradient algorithm.
\newblock \emph{arXiv preprint arXiv:2105.01424}, 2021.

\bibitem[Lanctot et~al.(2017)Lanctot, Zambaldi, Gruslys, Lazaridou, Tuyls,
  P{\'e}rolat, Silver, and Graepel]{Lanctot17}
M.~Lanctot, V.~Zambaldi, A.~Gruslys, A.~Lazaridou, K.~Tuyls, J.~P{\'e}rolat,
  D.~Silver, and T.~Graepel.
\newblock A unified game-theoretic approach to multiagent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1711.00832}, 2017.

\bibitem[Leonardos et~al.(2021)Leonardos, Overman, Panageas, and
  Piliouras]{leonardos21}
S.~Leonardos, W.~Overman, I.~Panageas, and G.~Piliouras.
\newblock Global convergence of multi-agent policy gradient in markov potential
  games.
\newblock \emph{arXiv preprint arXiv:2106.01969}, 2021.

\bibitem[Levhari and Mirman(1980)]{Levhari80}
D.~Levhari and L.~Mirman.
\newblock The great fish war: An example using a dynamic cournot-nash solution.
\newblock \emph{Bell Journal of Economics}, 11\penalty0 (1):\penalty0 322--334,
  1980.

\bibitem[Li et~al.(2021)Li, Wei, Chi, Gu, and Chen]{Li2021softmax}
G.~Li, Y.~Wei, Y.~Chi, Y.~Gu, and Y.~Chen.
\newblock Softmax policy gradient methods can take exponential time to
  converge.
\newblock \emph{CoRR}, abs/2102.11270, 2021.

\bibitem[Littman(1994)]{Littman94}
M.~L. Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings 1994}, pages 157--163.
  Elsevier, 1994.

\bibitem[Liu and Wu(2018)]{liu2018}
J.~Liu and J.~Wu.
\newblock \emph{Multiagent robotic systems}.
\newblock CRC press, 2018.

\bibitem[Macua et~al.(2018)Macua, Zazo, and Zazo]{Macua18}
S.~V. Macua, J.~Zazo, and S.~Zazo.
\newblock Learning parametric closed-loop policies for markov potential games.
\newblock \emph{CoRR}, abs/1802.00899, 2018.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{Mei20}
J.~Mei, C.~Xiao, C.~Szepesvari, and D.~Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 6820--6829. PMLR,
  13--18 Jul 2020.

\bibitem[Mei et~al.(2021)Mei, Dai, Xiao, Szepesvari, and Schuurmans]{Mei2021}
J.~Mei, B.~Dai, C.~Xiao, C.~Szepesvari, and D.~Schuurmans.
\newblock Understanding the effect of stochasticity in policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mguni(2020)]{mguni20}
D.~Mguni.
\newblock Stochastic potential games.
\newblock \emph{arXiv preprint arXiv:2005.13527}, 2020.

\bibitem[Mguni et~al.(2021)Mguni, Wu, Du, Yang, Wang, Li, Wen, Jennings, and
  Wang]{mguni2021learning}
D.~Mguni, Y.~Wu, Y.~Du, Y.~Yang, Z.~Wang, M.~Li, Y.~Wen, J.~Jennings, and
  J.~Wang.
\newblock Learning in nonzero-sum stochastic games with potentials.
\newblock \emph{arXiv preprint arXiv:2103.09284}, 2021.

\bibitem[Monderer and Shapley(1996)]{Monderer96}
D.~Monderer and L.~S. Shapley.
\newblock Potential games.
\newblock \emph{Games and economic behavior}, 14\penalty0 (1):\penalty0
  124--143, 1996.

\bibitem[Rao(1992)]{rao1992information}
C.~R. Rao.
\newblock Information and the accuracy attainable in the estimation of
  statistical parameters.
\newblock In \emph{Breakthroughs in statistics}, pages 235--247. Springer,
  1992.

\bibitem[Roscia et~al.(2013)Roscia, Longo, and Lazaroiu]{roscia2013}
M.~Roscia, M.~Longo, and G.~C. Lazaroiu.
\newblock Smart city by multi-agent systems.
\newblock In \emph{2013 International Conference on Renewable Energy Research
  and Applications (ICRERA)}, pages 371--376. IEEE, 2013.

\bibitem[Shapley(1953)]{shapley53}
L.~S. Shapley.
\newblock Stochastic games.
\newblock \emph{Proceedings of the national academy of sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.

\bibitem[Shoham et~al.(2003)Shoham, Powers, and Grenager]{Shoham03}
Y.~Shoham, R.~Powers, and T.~Grenager.
\newblock Multi-agent reinforcement learning: a critical survey.
\newblock Technical report, Technical report, Stanford University, 2003.

\bibitem[Song et~al.(2021)Song, Mei, and Bai]{song2021}
Z.~Song, S.~Mei, and Y.~Bai.
\newblock When can we learn general-sum markov games with a large number of
  players sample-efficiently?, 2021.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, Mansour,
  et~al.]{Sutton1999}
R.~S. Sutton, D.~A. McAllester, S.~P. Singh, Y.~Mansour, et~al.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{NIPs}, volume~99, pages 1057--1063. Citeseer, 1999.

\bibitem[Tao et~al.(2001)Tao, Baxter, and Weaver]{tao2001}
N.~Tao, J.~Baxter, and L.~Weaver.
\newblock A multi-agent, policy-gradient approach to network routing.
\newblock In \emph{In: Proc. of the 18th Int. Conf. on Machine Learning}.
  Citeseer, 2001.

\bibitem[Ventre et~al.(2013)Ventre, Maturo, Ho{\v{s}}kov{\'a}-Mayerov{\'a}, and
  Kacprzyk]{ventre2013}
A.~G. Ventre, A.~Maturo, {\v{S}}.~Ho{\v{s}}kov{\'a}-Mayerov{\'a}, and
  J.~Kacprzyk.
\newblock \emph{Multicriteria and Multiagent Decision Making with Applications
  to Economics and Social Sciences}, volume 305.
\newblock Springer, 2013.

\bibitem[Zazo et~al.(2016)Zazo, Macua, S{\'a}nchez-Fern{\'a}ndez, and
  Zazo]{zazo16}
S.~Zazo, S.~V. Macua, M.~S{\'a}nchez-Fern{\'a}ndez, and J.~Zazo.
\newblock Dynamic potential games with constraints: Fundamentals and
  applications in communications.
\newblock \emph{IEEE Transactions on Signal Processing}, 64\penalty0
  (14):\penalty0 3806--3821, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Yang, and Ba{\c{s}}ar]{Zhang19}
K.~Zhang, Z.~Yang, and T.~Ba{\c{s}}ar.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \emph{arXiv preprint arXiv:1911.10635}, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Ren, and Li]{zhang21}
R.~Zhang, Z.~Ren, and N.~Li.
\newblock Gradient play in multi-agent markov stochastic games: stationary
  points, convergence, and sample complexity.
\newblock \emph{CoRR}, abs/2106.00198, 2021.

\end{thebibliography}
