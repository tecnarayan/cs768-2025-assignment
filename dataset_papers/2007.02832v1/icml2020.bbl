\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abels et~al.(2018)Abels, Roijers, Lenaerts, Now{\'e}, and
  Steckelmacher]{abels2018dynamic}
Abels, A., Roijers, D.~M., Lenaerts, T., Now{\'e}, A., and Steckelmacher, D.
\newblock Dynamic weights in multi-objective deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1809.07803}, 2018.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, O.~P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5048--5058, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Badia et~al.(2020)Badia, Piot, Kapturowski, Sprechmann, Vitvitskyi,
  Guo, and Blundell]{badia2020agent57}
Badia, A.~P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo,
  D., and Blundell, C.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock \emph{arXiv preprint arXiv:2003.13350}, 2020.

\bibitem[Baranes \& Oudeyer(2010)Baranes and Oudeyer]{baranes2010intrinsically}
Baranes, A. and Oudeyer, P.-Y.
\newblock Intrinsically motivated goal exploration for active motor learning in
  robots: A case study.
\newblock In \emph{2010 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  1766--1773. IEEE, 2010.

\bibitem[Baranes \& Oudeyer(2013)Baranes and Oudeyer]{baranes2013active}
Baranes, A. and Oudeyer, P.-Y.
\newblock Active learning of inverse models with intrinsically motivated goal
  exploration in robots.
\newblock \emph{Robotics and Autonomous Systems}, 61\penalty0 (1):\penalty0
  49--73, 2013.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1471--1479, 2016.

\bibitem[Bharadhwaj et~al.(2020)Bharadhwaj, Garg, and
  Shkurti]{bharadhwaj2020dynamics}
Bharadhwaj, H., Garg, A., and Shkurti, F.
\newblock Leaf: Latent exploration along the frontier.
\newblock \emph{arXiv preprint arXiv:2005.10934}, 2020.

\bibitem[Bishop(2006)]{bishop2006pattern}
Bishop, C.~M.
\newblock \emph{Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym2016}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Burda et~al.(2019)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Colas et~al.(2018)Colas, Sigaud, and Oudeyer]{colas2018curious}
Colas, C., Sigaud, O., and Oudeyer, P.-Y.
\newblock Curious: Intrinsically motivated multi-task, multi-goal reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1810.06284}, 2018.

\bibitem[Cover \& Thomas(2012)Cover and Thomas]{cover2012elements}
Cover, T.~M. and Thomas, J.~A.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 2012.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{baselines}
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S.
\newblock Density estimation using real nvp.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Duan et~al.(2017)Duan, Andrychowicz, Stadie, Ho, Schneider, Sutskever,
  Abbeel, and Zaremba]{duan2017one}
Duan, Y., Andrychowicz, M., Stadie, B., Ho, O.~J., Schneider, J., Sutskever,
  I., Abbeel, P., and Zaremba, W.
\newblock One-shot imitation learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1087--1098, 2017.

\bibitem[Ecoffet et~al.(2019)Ecoffet, Huizinga, Lehman, Stanley, and
  Clune]{ecoffet2019go}
Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K.~O., and Clune, J.
\newblock Go-explore: a new approach for hard-exploration problems.
\newblock \emph{arXiv preprint arXiv:1901.10995}, 2019.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Fang et~al.(2019)Fang, Zhou, Du, Han, and Zhang]{fang2019curriculum}
Fang, M., Zhou, T., Du, Y., Han, L., and Zhang, Z.
\newblock Curriculum-guided hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12602--12613, 2019.

\bibitem[Florensa et~al.(2018)Florensa, Held, Geng, and
  Abbeel]{florensa2018automatic}
Florensa, C., Held, D., Geng, X., and Abbeel, P.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  1515--1528, 2018.

\bibitem[Frank \& Wolfe(1956)Frank and Wolfe]{frank1956algorithm}
Frank, M. and Wolfe, P.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Van~Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Van~Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{arXiv preprint arXiv:1802.09477}, 2018.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and
  Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Gregor et~al.(2016)Gregor, Rezende, and
  Wierstra]{gregor2016variational}
Gregor, K., Rezende, D.~J., and Wierstra, D.
\newblock Variational intrinsic control.
\newblock \emph{arXiv preprint arXiv:1611.07507}, 2016.

\bibitem[Hansen et~al.(2020)Hansen, Dabney, Barreto, Van~de Wiele,
  Warde-Farley, and Mnih]{hansen2019fast}
Hansen, S., Dabney, W., Barreto, A., Van~de Wiele, T., Warde-Farley, D., and
  Mnih, V.
\newblock Fast task inference with variational intrinsic successor features.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hartikainen et~al.(2020)Hartikainen, Geng, Haarnoja, and
  Levine]{hartikainen2020dynamical}
Hartikainen, K., Geng, X., Haarnoja, T., and Levine, S.
\newblock Dynamical distance learning for semi-supervised and unsupervised
  skill discovery.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hazan et~al.(2018)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2018provably}
Hazan, E., Kakade, S.~M., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock \emph{arXiv preprint arXiv:1812.02690}, 2018.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Holz et~al.(2010)Holz, Basilico, Amigoni, and
  Behnke]{holz2010evaluating}
Holz, D., Basilico, N., Amigoni, F., and Behnke, S.
\newblock Evaluating the efficiency of frontier-based exploration strategies.
\newblock In \emph{ISR 2010 (41st International Symposium on Robotics) and
  ROBOTIK 2010 (6th German Conference on Robotics)}, pp.\  1--8. VDE, 2010.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
Kaelbling, L.~P.
\newblock Learning to achieve goals.
\newblock In \emph{IJCAI}, pp.\  1094--1099. Citeseer, 1993.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Klyubin et~al.(2005)Klyubin, Polani, and
  Nehaniv]{klyubin2005empowerment}
Klyubin, A.~S., Polani, D., and Nehaniv, C.~L.
\newblock Empowerment: A universal agent-centric measure of control.
\newblock In \emph{2005 IEEE Congress on Evolutionary Computation}, volume~1,
  pp.\  128--135. IEEE, 2005.

\bibitem[Kolter \& Ng(2009)Kolter and Ng]{kolter2009near}
Kolter, J.~Z. and Ng, A.~Y.
\newblock Near-bayesian exploration in polynomial time.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, 2009.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019efficient}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov,
  R.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Lee et~al.(2020)Lee, Eysenbach, Salakhutdinov, Finn,
  et~al.]{lee2020weakly}
Lee, L., Eysenbach, B., Salakhutdinov, R., Finn, C., et~al.
\newblock Weakly-supervised reinforcement learning for controllable behavior.
\newblock \emph{arXiv preprint arXiv:2004.02860}, 2020.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lopes et~al.(2012)Lopes, Lang, Toussaint, and
  Oudeyer]{lopes2012exploration}
Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y.
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In \emph{Advances in neural information processing systems}, 2012.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mohamed \& Rezende(2015)Mohamed and Rezende]{mohamed2015variational}
Mohamed, S. and Rezende, D.~J.
\newblock Variational information maximisation for intrinsically motivated
  reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2125--2133, 2015.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{nachum2018data}
Nachum, O., Gu, S.~S., Lee, H., and Levine, S.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3303--3313, 2018.

\bibitem[Nair et~al.(2018{\natexlab{a}})Nair, McGrew, Andrychowicz, Zaremba,
  and Abbeel]{nair2018overcoming}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}. IEEE, 2018{\natexlab{a}}.

\bibitem[Nair et~al.(2018{\natexlab{b}})Nair, Pong, Dalal, Bahl, Lin, and
  Levine]{nair2018visual}
Nair, A.~V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S.
\newblock Visual reinforcement learning with imagined goals.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2018{\natexlab{b}}.

\bibitem[Newell(1969)]{newell1969artificial}
Newell, A.
\newblock \emph{Artificial intelligence and the concept of mind}.
\newblock Carnegie-Mellon University, Department of Computer Science, 1969.

\bibitem[Osband et~al.(2014)Osband, Van~Roy, and Wen]{osband2014generalization}
Osband, I., Van~Roy, B., and Wen, Z.
\newblock Generalization and exploration via randomized value functions.
\newblock \emph{arXiv preprint arXiv:1402.0635}, 2014.

\bibitem[Osband et~al.(2017)Osband, Russo, Wen, and Van~Roy]{osband2017deep}
Osband, I., Russo, D., Wen, Z., and Van~Roy, B.
\newblock Deep exploration via randomized value functions.
\newblock \emph{Journal of Machine Learning Research}, 2017.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, van~den Oord, and
  Munos]{ostrovski2017count}
Ostrovski, G., Bellemare, M.~G., van~den Oord, A., and Munos, R.
\newblock Count-based exploration with neural density models.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2721--2730. JMLR. org, 2017.

\bibitem[Papamakarios et~al.(2019)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2019normalizing}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and
  Lakshminarayanan, B.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{arXiv preprint arXiv:1912.02762}, 2019.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{ICML}, 2017.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.]{pedregosa2011scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (Oct):\penalty0 2825--2830, 2011.

\bibitem[Pitis et~al.(2019)Pitis, Chan, and Ba]{pitisprotoge}
Pitis, S., Chan, H., and Ba, J.
\newblock Protoge: Prototype goal encodings for multi-goal reinforcement
  learning.
\newblock \emph{The 4th Multidisciplinary Conference on Reinforcement Learning
  and Decision Making}, 2019.

\bibitem[Pitis et~al.(2020)Pitis, Chan, and Zhao]{mrl}
Pitis, S., Chan, H., and Zhao, S.
\newblock mrl: modular rl.
\newblock \url{https://github.com/spitis/mrl}, 2020.

\bibitem[Plappert et~al.(2017)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  Asfour, Abbeel, and Andrychowicz]{plappert2017parameter}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  Asfour, T., Abbeel, P., and Andrychowicz, M.
\newblock Parameter space noise for exploration.
\newblock \emph{arXiv preprint arXiv:1706.01905}, 2017.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, McGrew, Baker,
  Powell, Schneider, Tobin, Chociej, Welinder, et~al.]{plappert2018multi}
Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G.,
  Schneider, J., Tobin, J., Chociej, M., Welinder, P., et~al.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock \emph{arXiv preprint arXiv:1802.09464}, 2018.

\bibitem[{Pong} et~al.(2019){Pong}, {Dalal}, {Lin}, {Nair}, {Bahl}, and
  {Levine}]{pong2019skew}
{Pong}, V.~H., {Dalal}, M., {Lin}, S., {Nair}, A., {Bahl}, S., and {Levine}, S.
\newblock {Skew-Fit: State-Covering Self-Supervised Reinforcement Learning}.
\newblock \emph{arXiv e-prints}, March 2019.

\bibitem[Ren et~al.(2018)Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,
  Larochelle, and Zemel]{ren2018meta}
Ren, M., Triantafillou, E., Ravi, S., Snell, J., Swersky, K., Tenenbaum, J.~B.,
  Larochelle, H., and Zemel, R.~S.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock \emph{arXiv preprint arXiv:1803.00676}, 2018.

\bibitem[Rosenblatt(1956)]{rosenblatt1956remarks}
Rosenblatt, M.
\newblock Remarks on some nonparametric estimates of a density function.
\newblock \emph{The Annals of Mathematical Statistics}, pp.\  832--837, 1956.

\bibitem[Salge et~al.(2014)Salge, Glackin, and Polani]{salge2014empowerment}
Salge, C., Glackin, C., and Polani, D.
\newblock Empowerment--an introduction.
\newblock In \emph{Guided Self-Organization: Inception}, pp.\  67--114.
  Springer, 2014.

\bibitem[Schaul et~al.(2015{\natexlab{a}})Schaul, Horgan, Gregor, and
  Silver]{schaul2015universal}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1312--1320, 2015{\natexlab{a}}.

\bibitem[Schaul et~al.(2015{\natexlab{b}})Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015{\natexlab{b}}.

\bibitem[Schmidhuber(2010)]{schmidhuber2010formal}
Schmidhuber, J.
\newblock Formal theory of creativity, fun, and intrinsic motivation
  (1990--2010).
\newblock \emph{IEEE Transactions on Autonomous Mental Development}, 2\penalty0
  (3):\penalty0 230--247, 2010.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shyam et~al.(2018)Shyam, Ja{\'s}kowski, and Gomez]{shyam2018model}
Shyam, P., Ja{\'s}kowski, W., and Gomez, F.
\newblock Model-based active exploration.
\newblock \emph{arXiv preprint arXiv:1810.12162}, 2018.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{ICML}, 2014.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
Sutton, R.~S., Modayil, J., Delp, M., Degris, T., Pilarski, P.~M., White, A.,
  and Precup, D.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pp.\  761--768, 2011.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  DeTurck, and Abbeel]{tang2017exploration}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O.~X., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2753--2762, 2017.

\bibitem[Thomas et~al.(2015)Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015high}
Thomas, P.~S., Theocharous, G., and Ghavamzadeh, M.
\newblock High-confidence off-policy evaluation.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Trott et~al.(2019)Trott, Zheng, Xiong, and Socher]{trott2019keeping}
Trott, A., Zheng, S., Xiong, C., and Socher, R.
\newblock Keeping your distance: Solving sparse reward tasks using
  self-balancing shaped rewards.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  10376--10386. 2019.

\bibitem[Van~Seijen et~al.(2019)Van~Seijen, Fatemi, and Tavakoli]{van2019using}
Van~Seijen, H., Fatemi, M., and Tavakoli, A.
\newblock Using a logarithmic mapping to enable lower discount factors in
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  14111--14121. 2019.

\bibitem[Warde-Farley et~al.(2019)Warde-Farley, de~Wiele, Kulkarni, Ionescu,
  Hansen, and Mnih]{warde-farley2018unsupervised}
Warde-Farley, D., de~Wiele, T.~V., Kulkarni, T., Ionescu, C., Hansen, S., and
  Mnih, V.
\newblock Unsupervised control through non-parametric discriminative rewards.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Yamauchi(1997)]{yamauchi1997frontier}
Yamauchi, B.
\newblock A frontier-based approach for autonomous exploration.
\newblock In \emph{Proceedings 1997 IEEE International Symposium on
  Computational Intelligence in Robotics and Automation CIRA'97.'Towards New
  Computational Principles for Robotics and Automation'}. IEEE, 1997.

\bibitem[Zhang et~al.(2020)Zhang, Abbeel, and Pinto]{zhang2020automatic}
Zhang, Y., Abbeel, P., and Pinto, L.
\newblock Automatic curriculum learning through value disagreement.
\newblock \emph{arXiv preprint arXiv:2006.09641}, 2020.

\bibitem[Zhao et~al.(2019)Zhao, Sun, and Tresp]{zhao2019maximum}
Zhao, R., Sun, X., and Tresp, V.
\newblock Maximum entropy-regularized multi-goal reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.08786}, 2019.

\end{thebibliography}
