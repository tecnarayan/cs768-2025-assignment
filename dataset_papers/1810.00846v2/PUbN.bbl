\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{JMLR}, 3\penalty0 (Nov):\penalty0 463--482, 2002.

\bibitem[Belkin et~al.(2006)Belkin, Niyogi, and Sindhwani]{belkin2006manifold}
Belkin, M., Niyogi, P., and Sindhwani, V.
\newblock Manifold regularization: A geometric framework for learning from
  labeled and unlabeled examples.
\newblock \emph{JMLR}, 7\penalty0 (Nov):\penalty0 2399--2434, 2006.

\bibitem[Chapelle et~al.(2010)Chapelle, Schlkopf, and
  Zien]{Chapelle:2010:SL:1841234}
Chapelle, O., Schlkopf, B., and Zien, A.
\newblock \emph{Semi-Supervised Learning}.
\newblock The MIT Press, 1st edition, 2010.

\bibitem[du~Plessis et~al.(2015)du~Plessis, Niu, and Sugiyama]{du2015convex}
du~Plessis, M., Niu, G., and Sugiyama, M.
\newblock Convex formulation for learning from positive and unlabeled data.
\newblock In \emph{ICML}, pp.\  1386--1394, 2015.

\bibitem[du~Plessis et~al.(2014)du~Plessis, Niu, and Sugiyama]{du2014analysis}
du~Plessis, M.~C., Niu, G., and Sugiyama, M.
\newblock Analysis of learning from positive and unlabeled data.
\newblock In \emph{NeurIPS}, pp.\  703--711, 2014.

\bibitem[du~Plessis et~al.(2017)du~Plessis, Niu, and
  Sugiyama]{Plessis:2017:CEL:3085961.3085999}
du~Plessis, M.~C., Niu, G., and Sugiyama, M.
\newblock Class-prior estimation for learning from positive and unlabeled data.
\newblock \emph{Maching Learning}, 106\penalty0 (4):\penalty0 463--492, April
  2017.
\newblock ISSN 0885-6125.

\bibitem[Elkan \& Noto(2008)Elkan and Noto]{Elkan2008LearningCF}
Elkan, C. and Noto, K.
\newblock Learning classifiers from only positive and unlabeled data.
\newblock In \emph{KDD}, 2008.

\bibitem[Fei \& Liu(2015)Fei and Liu]{fei2015social}
Fei, G. and Liu, B.
\newblock Social media text classification under negative covariate shift.
\newblock In \emph{Proc. of EMNLP}, pp.\  2347--2356, 2015.

\bibitem[Fung et~al.(2006)Fung, Yu, Lu, and Yu]{fung2006text}
Fung, G. P.~C., Yu, J.~X., Lu, H., and Yu, P.~S.
\newblock Text classification without negative examples revisit.
\newblock \emph{TKDE}, 18\penalty0 (1):\penalty0 6--20, 2006.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
Golowich, N., Rakhlin, A., and Shamir, O.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{COLT}, pp.\  297--299, 2018.

\bibitem[Grandvalet \& Bengio(2005)Grandvalet and Bengio]{grandvalet2005semi}
Grandvalet, Y. and Bengio, Y.
\newblock Semi-supervised learning by entropy minimization.
\newblock In \emph{NeurIPS}, pp.\  529--536, 2005.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, pp.\  630--645. Springer, 2016.

\bibitem[Heckman(1979)]{heckman1979sample}
Heckman, J.~J.
\newblock Sample selection bias as a specification error.
\newblock \emph{Econometrica}, 47\penalty0 (1):\penalty0 153--161, 1979.

\bibitem[Hido et~al.(2008)Hido, Tsuboi, Kashima, Sugiyama, and
  Kanamori]{hido2008inlier}
Hido, S., Tsuboi, Y., Kashima, H., Sugiyama, M., and Kanamori, T.
\newblock Inlier-based outlier detection via direct density ratio estimation.
\newblock In \emph{ICDM}, pp.\  223--232. IEEE, 2008.

\bibitem[Hu et~al.(2018)Hu, Niu, Sato, and Sugiyama]{hu2018does}
Hu, W., Niu, G., Sato, I., and Sugiyama, M.
\newblock Does distributionally robust supervised learning give robust
  classifiers?
\newblock In \emph{ICML}, pp.\  2034--2042, 2018.

\bibitem[Huang et~al.(2007)Huang, Gretton, Borgwardt, Sch{\"o}lkopf, and
  Smola]{huang2007correcting}
Huang, J., Gretton, A., Borgwardt, K.~M., Sch{\"o}lkopf, B., and Smola, A.~J.
\newblock Correcting sample selection bias by unlabeled data.
\newblock In \emph{NeurIPS}, pp.\  601--608, 2007.

\bibitem[Ishida et~al.(2018)Ishida, Niu, and Sugiyama]{ishida2018binary}
Ishida, T., Niu, G., and Sugiyama, M.
\newblock Binary classification from positive-confidence data.
\newblock In \emph{NeurIPS}, pp.\  5921--5932, 2018.

\bibitem[Jain et~al.(2016)Jain, White, and Radivojac]{jain2016estimating}
Jain, S., White, M., and Radivojac, P.
\newblock Estimating the class prior and posterior from noisy positives and
  unlabeled data.
\newblock In \emph{NeurIPS}, pp.\  2693--2701, 2016.

\bibitem[Kanamori et~al.(2009)Kanamori, Hido, and Sugiyama]{Kanamori2009ALA}
Kanamori, T., Hido, S., and Sugiyama, M.
\newblock A least-squares approach to direct importance estimation.
\newblock \emph{JMLR}, 10:\penalty0 1391--1445, 2009.

\bibitem[Kiryo et~al.(2017)Kiryo, Niu, du~Plessis, and
  Sugiyama]{kiryo2017positive}
Kiryo, R., Niu, G., du~Plessis, M.~C., and Sugiyama, M.
\newblock Positive-unlabeled learning with non-negative risk estimator.
\newblock In \emph{NeurIPS}, pp.\  1675--1685, 2017.

\bibitem[Laine \& Aila(2017)Laine and Aila]{laine2017temporal}
Laine, S. and Aila, T.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In \emph{ICLR}, 2017.

\bibitem[Li et~al.(2011)Li, Guo, and Elkan]{li2011positive}
Li, W., Guo, Q., and Elkan, C.
\newblock A positive and unlabeled learning algorithm for one-class
  classification of remote-sensing data.
\newblock \emph{TGRS}, 49\penalty0 (2):\penalty0 717--725, 2011.

\bibitem[Li et~al.(2010)Li, Liu, and Ng]{li2010neg}
Li, X.-L., Liu, B., and Ng, S.-K.
\newblock Negative training data can be harmful to text classification.
\newblock In \emph{Proc. of EMNLP}, pp.\  218--228, 2010.

\bibitem[Liu et~al.(2002)Liu, Lee, Yu, and Li]{liu2002partially}
Liu, B., Lee, W.~S., Yu, P.~S., and Li, X.
\newblock Partially supervised classification of text documents.
\newblock In \emph{ICML}, volume~2, pp.\  387--394, 2002.

\bibitem[Liu et~al.(2003)Liu, Dai, Li, Lee, and Yu]{liu2003building}
Liu, B., Dai, Y., Li, X., Lee, W.~S., and Yu, P.~S.
\newblock Building text classifiers using positive and unlabeled examples.
\newblock In \emph{ICDM}, pp.\  179--186. IEEE, 2003.

\bibitem[Miyato et~al.(2016)Miyato, Maeda, Koyama, Nakae, and
  Ishii]{miyato2016distributional}
Miyato, T., Maeda, S.-i., Koyama, M., Nakae, K., and Ishii, S.
\newblock Distributional smoothing with virtual adversairal training.
\newblock In \emph{ICLR}, 2016.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2012foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of machine learning}.
\newblock The MIT press, 2012.

\bibitem[Mordelet \& Vert(2014)Mordelet and Vert]{mordelet2014bagging}
Mordelet, F. and Vert, J.-P.
\newblock A bagging svm to learn from positive and unlabeled examples.
\newblock \emph{Pattern Recognition Letters}, 37:\penalty0 201--209, 2014.

\bibitem[Nguyen et~al.(2011)Nguyen, Li, and Ng]{nguyen2011positive}
Nguyen, M.~N., Li, X.-L., and Ng, S.-K.
\newblock Positive unlabeled learning for time series classification.
\newblock In \emph{IJCAI}, volume~11, pp.\  1421--1426, 2011.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{oliver2018realistic}
Oliver, A., Odena, A., Raffel, C.~A., Cubuk, E.~D., and Goodfellow, I.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock In \emph{NeurIPS}, pp.\  3239--3250. 2018.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{Peters:2018}
Peters, M.~E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and
  Zettlemoyer, L.
\newblock Deep contextualized word representations.
\newblock In \emph{Proc. of NAACL}, 2018.

\bibitem[Quionero-Candela et~al.(2009)Quionero-Candela, Sugiyama, Schwaighofer,
  and Lawrence]{quionero2009dataset}
Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N.~D.
\newblock \emph{Dataset Shift in Machine Learning}.
\newblock The MIT Press, 2009.

\bibitem[Ramaswamy et~al.(2016)Ramaswamy, Scott, and
  Tewari]{ramaswamy2016mixture}
Ramaswamy, H., Scott, C., and Tewari, A.
\newblock Mixture proportion estimation via kernel embeddings of distributions.
\newblock In \emph{ICML}, pp.\  2052--2060, 2016.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{j.2018on}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock In \emph{ICLR}, 2018.

\bibitem[R{\"{u}}ckl{\'{e}} et~al.(2018)R{\"{u}}ckl{\'{e}}, Eger, Peyrard, and
  Gurevych]{andreas2018conc}
R{\"{u}}ckl{\'{e}}, A., Eger, S., Peyrard, M., and Gurevych, I.
\newblock Concatenated power mean word embeddings as universal cross-lingual
  sentence representations.
\newblock \emph{arXiv preprint arXiv:1803.01400}, 2018.

\bibitem[Sakai et~al.(2017)Sakai, du~Plessis, Niu, and Sugiyama]{sakai2016semi}
Sakai, T., du~Plessis, M. C.~d., Niu, G., and Sugiyama, M.
\newblock Semi-supervised classification based on classification from positive
  and unlabeled data.
\newblock In \emph{ICML}, volume~70, pp.\  2998--3006, 2017.

\bibitem[Scott \& Blanchard(2009)Scott and Blanchard]{scott2009novelty}
Scott, C. and Blanchard, G.
\newblock Novelty detection: Unlabeled data definitely help.
\newblock In \emph{AISTATS}, pp.\  464--471, 2009.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{Shalev-Shwartz:2014:UML:2621980}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Shi et~al.(2018)Shi, Pan, Yang, and Gong]{ijcai2018-373}
Shi, H., Pan, S., Yang, J., and Gong, C.
\newblock Positive and unlabeled learning via loss decomposition and centroid
  estimation.
\newblock In \emph{IJCAI}, pp.\  2689--2695, 2018.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Shimodaira, H.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{JMLR}, 90\penalty0 (2):\penalty0 227--244, 2000.

\bibitem[Sugiyama \& Kawanabe(2012)Sugiyama and
  Kawanabe]{book:Sugiyama+Kawanabe:2012}
Sugiyama, M. and Kawanabe, M.
\newblock \emph{Machine Learning in Non-Stationary Environments: {I}ntroduction
  to Covariate Shift Adaptation}.
\newblock The MIT Press, Cambridge, Massachusetts, USA, 2012.

\bibitem[Sugiyama \& Storkey(2007)Sugiyama and Storkey]{sugiyama2007mixture}
Sugiyama, M. and Storkey, A.~J.
\newblock Mixture regression for covariate shift.
\newblock In \emph{NeurIPS}, pp.\  1337--1344, 2007.

\bibitem[Sugiyama et~al.(2008)Sugiyama, Nakajima, Kashima, Buenau, and
  Kawanabe]{sugiyama2008direct}
Sugiyama, M., Nakajima, S., Kashima, H., Buenau, P.~V., and Kawanabe, M.
\newblock Direct importance estimation with model selection and its application
  to covariate shift adaptation.
\newblock In \emph{NeurIPS}, pp.\  1433--1440, 2008.

\bibitem[Ward et~al.(2009)Ward, Hastie, Barry, Elith, and
  Leathwick]{Ward2009PresenceonlyDA}
Ward, G.~A., Hastie, T.~J., Barry, S.~T., Elith, J., and Leathwick, J.~R.
\newblock Presence-only data and the em algorithm.
\newblock \emph{Biometrics}, 65 2:\penalty0 554--63, 2009.

\bibitem[Zadrozny(2004)]{zadrozny2004learning}
Zadrozny, B.
\newblock Learning and evaluating classifiers under sample selection bias.
\newblock In \emph{ICML}, pp.\  903--910, 2004.

\bibitem[Zhang(2004)]{zhang2004statistical}
Zhang, T.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock \emph{Annals of Statistics}, pp.\  56--85, 2004.

\bibitem[Zuluaga et~al.(2011)Zuluaga, Hush, J~F Delgado~Leyton,
  Hernandez~Hoyos, and Orkisz]{zu2011learn}
Zuluaga, M., Hush, D., J~F Delgado~Leyton, E., Hernandez~Hoyos, M., and Orkisz,
  M.
\newblock Learning from only positive and unlabeled data to detect lesions in
  vascular ct images.
\newblock In \emph{MICCAI}, volume LNCS 6893, pp.\  9--16, 2011.

\end{thebibliography}
