\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{abadi2016tensorflow}
M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S. Corrado,
  A.~Davis, J.~Dean, M.~Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[Andrews and Mallows(1974)]{andrews1974scale}
D.~F. Andrews and C.~L. Mallows.
\newblock Scale mixtures of normal distributions.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, pages 99--102, 1974.

\bibitem[Armagan et~al.(2011)Armagan, Clyde, and
  Dunson]{armagan2011generalized}
A.~Armagan, M.~Clyde, and D.~B. Dunson.
\newblock Generalized beta mixtures of gaussians.
\newblock In \emph{Advances in neural information processing systems}, pages
  523--531, 2011.

\bibitem[Azarkhish et~al.(2017)Azarkhish, Rossi, Loi, and
  Benini]{azarkhish2017neurostream}
E.~Azarkhish, D.~Rossi, I.~Loi, and L.~Benini.
\newblock Neurostream: Scalable and energy efficient deep learning with smart
  memory cubes.
\newblock \emph{arXiv preprint arXiv:1701.06420}, 2017.

\bibitem[Ba and Caruana(2014)]{ba2014deep}
J.~Ba and R.~Caruana.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Advances in neural information processing systems}, pages
  2654--2662, 2014.

\bibitem[Beale et~al.(1959)Beale, Mallows, et~al.]{beale1959scale}
E.~Beale, C.~Mallows, et~al.
\newblock Scale mixing of symmetric distributions with zero means.
\newblock \emph{The Annals of Mathematical Statistics}, 30\penalty0
  (4):\penalty0 1145--1151, 1959.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock \emph{Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015}, 2015.

\bibitem[Carvalho et~al.(2010)Carvalho, Polson, and
  Scott]{carvalho2010horseshoe}
C.~M. Carvalho, N.~G. Polson, and J.~G. Scott.
\newblock The horseshoe estimator for sparse signals.
\newblock \emph{Biometrika}, 97\penalty0 (2):\penalty0 465--480, 2010.

\bibitem[Chai et~al.(2017)Chai, Raghavan, Zhang, Amer, and
  Shields]{chai2017low}
S.~Chai, A.~Raghavan, D.~Zhang, M.~Amer, and T.~Shields.
\newblock Low precision neural networks using subband decomposition.
\newblock \emph{arXiv preprint arXiv:1703.08595}, 2017.

\bibitem[Chen et~al.(2015)Chen, Wilson, Tyree, Weinberger, and Chen]{Chen2015}
W.~Chen, J.~T. Wilson, S.~Tyree, K.~Q. Weinberger, and Y.~Chen.
\newblock Compressing convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1506.04449}, 2015.

\bibitem[Courbariaux and Bengio(2016)]{courbariaux2016binarynet}
M.~Courbariaux and Y.~Bengio.
\newblock Binarynet: Training deep neural networks with weights and activations
  constrained to $+1$ or $-1$.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Courbariaux et~al.(2014)Courbariaux, David, and
  Bengio]{courbariaux2014training}
M.~Courbariaux, J.-P. David, and Y.~Bengio.
\newblock Training deep neural networks with low precision multiplications.
\newblock \emph{arXiv preprint arXiv:1412.7024}, 2014.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
M.~Courbariaux, Y.~Bengio, and J.-P. David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3105--3113, 2015.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, de~Freitas, et~al.]{Denil2013}
M.~Denil, B.~Shakibi, L.~Dinh, N.~de~Freitas, et~al.
\newblock Predicting parameters in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2148--2156, 2013.

\bibitem[Dong et~al.(2017)Dong, Huang, Yang, and Yan]{dong2017more}
X.~Dong, J.~Huang, Y.~Yang, and S.~Yan.
\newblock More is less: A more complicated network with less inference
  complexity.
\newblock \emph{arXiv preprint arXiv:1703.08651}, 2017.

\bibitem[Figueiredo(2002)]{figueiredo2002adaptive}
M.~A. Figueiredo.
\newblock Adaptive sparseness using jeffreys' prior.
\newblock \emph{Advances in neural information processing systems}, 1:\penalty0
  697--704, 2002.

\bibitem[Gal and Ghahramani(2016)]{gal2015dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock \emph{ICML}, 2016.

\bibitem[Gong et~al.(2015)Gong, Liu, Yang, and Bourdev]{Gong2014}
Y.~Gong, L.~Liu, M.~Yang, and L.~Bourdev.
\newblock Compressing deep convolutional networks using vector quantization.
\newblock \emph{ICLR}, 2015.

\bibitem[Graves(2011)]{graves2011practical}
A.~Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2348--2356, 2011.

\bibitem[Gr{\"u}nwald(2007)]{grunwald2007minimum}
P.~D. Gr{\"u}nwald.
\newblock \emph{The minimum description length principle}.
\newblock MIT press, 2007.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Y.~Guo, A.~Yao, and Y.~Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  1379--1387, 2016.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{Gupta2015}
S.~Gupta, A.~Agrawal, K.~Gopalakrishnan, and P.~Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock \emph{CoRR, abs/1502.02551}, 392, 2015.

\bibitem[Gysel(2016)]{gysel2016ristretto}
P.~Gysel.
\newblock Ristretto: Hardware-oriented approximation of convolutional neural
  networks.
\newblock \emph{Master's thesis, University of California}, 2016.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{Han2015}
S.~Han, J.~Pool, J.~Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1135--1143, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
S.~Han, H.~Mao, and W.~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{ICLR}, 2016.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1026--1034, 2015.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hinton and Van~Camp(1993)]{hinton1993keeping}
G.~E. Hinton and D.~Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the sixth annual conference on Computational
  learning theory}, pages 5--13. ACM, 1993.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012improving}
G.~E. Hinton, N.~Srivastava, A.~Krizhevsky, I.~Sutskever, and R.~R.
  Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Honkela and Valpola(2004)]{Honkela2004}
A.~Honkela and H.~Valpola.
\newblock Variational learning and bits-back coding: an information-theoretic
  view to bayesian learning.
\newblock \emph{IEEE Transactions on Neural Networks}, 15\penalty0
  (4):\penalty0 800--810, 2004.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
A.~G. Howard, M.~Zhu, B.~Chen, D.~Kalenichenko, W.~Wang, T.~Weyand,
  M.~Andreetto, and H.~Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Iandola et~al.(2017)Iandola, Han, Moskewicz, Ashraf, Dally, and
  Keutzer]{iandola2016squeezenet}
F.~N. Iandola, S.~Han, M.~W. Moskewicz, K.~Ashraf, W.~J. Dally, and K.~Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock \emph{ICLR}, 2017.

\bibitem[Ingraham and Marks(2016)]{ingraham2016bayesian}
J.~B. Ingraham and D.~S. Marks.
\newblock Bayesian sparsity for intractable distributions.
\newblock \emph{arXiv preprint arXiv:1602.03807}, 2016.

\bibitem[Karaletsos and R{\"a}tsch(2015)]{karaletsos2015automatic}
T.~Karaletsos and G.~R{\"a}tsch.
\newblock Automatic relevance determination for deep generative models.
\newblock \emph{arXiv preprint arXiv:1505.07765}, 2015.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations (ICLR),
  San Diego}, 2015.

\bibitem[Kingma and Welling(2014)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2014.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
D.~P. Kingma, T.~Salimans, and M.~Welling.
\newblock Variational dropout and the local reparametrization trick.
\newblock \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Lawrence(2002)]{lawrence2002note}
N.~D. Lawrence.
\newblock Note relevance determination.
\newblock In \emph{Neural Nets WIRN Vietri-01}, pages 128--133. Springer, 2002.

\bibitem[LeCun et~al.(1989)LeCun, Denker, Solla, Howard, and
  Jackel]{Lecun1989BrainDamage}
Y.~LeCun, J.~S. Denker, S.~A. Solla, R.~E. Howard, and L.~D. Jackel.
\newblock Optimal brain damage.
\newblock In \emph{NIPs}, volume~2, pages 598--605, 1989.

\bibitem[LeCun et~al.(1998{\natexlab{a}})LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998{\natexlab{a}}.

\bibitem[LeCun et~al.(1998{\natexlab{b}})LeCun, Cortes, and
  Burges]{lecun1998mnist}
Y.~LeCun, C.~Cortes, and C.~J. Burges.
\newblock The mnist database of handwritten digits, 1998{\natexlab{b}}.

\bibitem[Lin and Talathi(2016)]{lin2016overcoming}
D.~D. Lin and S.~S. Talathi.
\newblock Overcoming challenges in fixed point training of deep convolutional
  networks.
\newblock \emph{Workshop ICML}, 2016.

\bibitem[Lin et~al.(2015)Lin, Talathi, and Annapureddy]{Lin2015}
D.~D. Lin, S.~S. Talathi, and V.~S. Annapureddy.
\newblock Fixed point quantization of deep convolutional networks.
\newblock \emph{arXiv preprint arXiv:1511.06393}, 2015.

\bibitem[Louizos(2015)]{louizos2015smart}
C.~Louizos.
\newblock Smart regularization of deep architectures.
\newblock \emph{Master's thesis, University of Amsterdam}, 2015.

\bibitem[{Louizos} and {Welling}(2017)]{louizos2017multiplicative}
C.~{Louizos} and M.~{Welling}.
\newblock {Multiplicative Normalizing Flows for Variational Bayesian Neural
  Networks}.
\newblock \emph{ArXiv e-prints}, Mar. 2017.

\bibitem[MacKay(1995)]{mackay1995probable}
D.~J. MacKay.
\newblock Probable networks and plausible predictions—a review of practical
  bayesian methods for supervised neural networks.
\newblock \emph{Network: Computation in Neural Systems}, 6\penalty0
  (3):\penalty0 469--505, 1995.

\bibitem[Mellempudi et~al.(2017)Mellempudi, Kundu, Mudigere, Das, Kaul, and
  Dubey]{mellempudi2017ternary}
N.~Mellempudi, A.~Kundu, D.~Mudigere, D.~Das, B.~Kaul, and P.~Dubey.
\newblock Ternary neural networks with fine-grained quantization.
\newblock \emph{arXiv preprint arXiv:1705.01462}, 2017.

\bibitem[Merolla et~al.(2016)Merolla, Appuswamy, Arthur, Esser, and
  Modha]{Merolla2016}
P.~Merolla, R.~Appuswamy, J.~Arthur, S.~K. Esser, and D.~Modha.
\newblock Deep neural networks are robust to weight binarization and other
  non-linear distortions.
\newblock \emph{arXiv preprint arXiv:1606.01981}, 2016.

\bibitem[Mitchell and Beauchamp(1988)]{mitchell1988bayesian}
T.~J. Mitchell and J.~J. Beauchamp.
\newblock Bayesian variable selection in linear regression.
\newblock \emph{Journal of the American Statistical Association}, 83\penalty0
  (404):\penalty0 1023--1032, 1988.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
D.~Molchanov, A.~Ashukha, and D.~Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock \emph{arXiv preprint arXiv:1701.05369}, 2017.

\bibitem[Nalisnick et~al.(2015)Nalisnick, Anandkumar, and
  Smyth]{nalisnick2015scale}
E.~Nalisnick, A.~Anandkumar, and P.~Smyth.
\newblock A scale mixture perspective of multiplicative noise in neural
  networks.
\newblock \emph{arXiv preprint arXiv:1506.03208}, 2015.

\bibitem[Neal(1995)]{neal1995bayesian}
R.~M. Neal.
\newblock \emph{Bayesian learning for neural networks}.
\newblock PhD thesis, Citeseer, 1995.

\bibitem[Neville et~al.(2014)Neville, Ormerod, Wand, et~al.]{neville2014mean}
S.~E. Neville, J.~T. Ormerod, M.~Wand, et~al.
\newblock Mean field variational bayes for continuous sparse signal shrinkage:
  pitfalls and remedies.
\newblock \emph{Electronic Journal of Statistics}, 8\penalty0 (1):\penalty0
  1113--1151, 2014.

\bibitem[Papaspiliopoulos et~al.(2007)Papaspiliopoulos, Roberts, and
  Sk{\"o}ld]{papaspiliopoulos2007general}
O.~Papaspiliopoulos, G.~O. Roberts, and M.~Sk{\"o}ld.
\newblock A general framework for the parametrization of hierarchical models.
\newblock \emph{Statistical Science}, pages 59--73, 2007.

\bibitem[Peterson(1987)]{peterson1987mean}
C.~Peterson.
\newblock A mean field theory learning algorithm for neural networks.
\newblock \emph{Complex systems}, 1:\penalty0 995--1019, 1987.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
M.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, pages 525--542.
  Springer, 2016.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the 31th International Conference on Machine
  Learning, {ICML} 2014, Beijing, China, 21-26 June 2014}, pages 1278--1286,
  2014.

\bibitem[Rissanen(1978)]{Rissanen1978}
J.~Rissanen.
\newblock Modeling by shortest data description.
\newblock \emph{Automatica}, 14\penalty0 (5):\penalty0 465--471, 1978.

\bibitem[Rissanen(1986)]{Rissanen1986}
J.~Rissanen.
\newblock Stochastic complexity and modeling.
\newblock \emph{The annals of statistics}, pages 1080--1100, 1986.

\bibitem[Scardapane et~al.(2016)Scardapane, Comminiello, Hussain, and
  Uncini]{scardapane2016group}
S.~Scardapane, D.~Comminiello, A.~Hussain, and A.~Uncini.
\newblock Group sparse regularization for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1607.00485}, 2016.

\bibitem[Shi and Chu(2017)]{shi2017speeding}
S.~Shi and X.~Chu.
\newblock Speeding up convolutional neural networks by exploiting the sparsity
  of rectifier units.
\newblock \emph{arXiv preprint arXiv:1704.07724}, 2017.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{ICLR}, 2015.

\bibitem[Sites(2008)]{sites2008ieee}
M.~Sites.
\newblock Ieee standard for floating-point arithmetic.
\newblock 2008.

\bibitem[S{\o}nderby et~al.(2016)S{\o}nderby, Raiko, Maal{\o}e, S{\o}nderby,
  and Winther]{sonderby2016ladder}
C.~K. S{\o}nderby, T.~Raiko, L.~Maal{\o}e, S.~K. S{\o}nderby, and O.~Winther.
\newblock Ladder variational autoencoders.
\newblock \emph{arXiv preprint arXiv:1602.02282}, 2016.

\bibitem[Srinivas and Babu(2016)]{srinivas2016generalized}
S.~Srinivas and R.~V. Babu.
\newblock Generalized dropout.
\newblock \emph{arXiv preprint arXiv:1611.06791}, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sze et~al.(2017)Sze, Chen, Yang, and Emer]{sze2017efficient}
V.~Sze, Y.-H. Chen, T.-J. Yang, and J.~Emer.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock \emph{arXiv preprint arXiv:1703.09039}, 2017.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, pages 267--288, 1996.

\bibitem[Ullrich et~al.(2017)Ullrich, Meeds, and Welling]{ullrich2017soft}
K.~Ullrich, E.~Meeds, and M.~Welling.
\newblock Soft weight-sharing for neural network compression.
\newblock \emph{ICLR}, 2017.

\bibitem[Venkatesh et~al.(2016)Venkatesh, Nurvitadhi, and Marr]{Venkatesh2016}
G.~Venkatesh, E.~Nurvitadhi, and D.~Marr.
\newblock Accelerating deep convolutional networks using low-precision and
  sparsity.
\newblock \emph{arXiv preprint arXiv:1610.00324}, 2016.

\bibitem[Wallace(1990)]{Wallace1990}
C.~S. Wallace.
\newblock Classification by minimum-message-length inference.
\newblock In \emph{International Conference on Computing and Information},
  pages 72--81. Springer, 1990.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{Wen2016}
W.~Wen, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  2074--2082, 2016.

\bibitem[Yang et~al.(2017)Yang, Chen, and Sze]{yang2016designing}
T.-J. Yang, Y.-H. Chen, and V.~Sze.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock \emph{CVPR}, 2017.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhu et~al.(2017)Zhu, Han, Mao, and Dally]{zhu2016trained}
C.~Zhu, S.~Han, H.~Mao, and W.~J. Dally.
\newblock Trained ternary quantization.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}
