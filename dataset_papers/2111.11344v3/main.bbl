\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Axelsson \& Gustafsson(2014)Axelsson and
  Gustafsson]{axelsson2014discrete}
Axelsson, P. and Gustafsson, F.
\newblock Discrete-time solutions to the continuous-time differential lyapunov
  equation with applications to kalman filtering.
\newblock \emph{IEEE Transactions on Automatic Control}, 60\penalty0
  (3):\penalty0 632--643, 2014.

\bibitem[Bayer \& Osendorfer(2014)Bayer and Osendorfer]{bayer2014learning}
Bayer, J. and Osendorfer, C.
\newblock Learning stochastic recurrent networks.
\newblock \emph{arXiv preprint arXiv:1411.7610}, 2014.

\bibitem[Becker et~al.(2019)Becker, Pandya, Gebhardt, Zhao, Taylor, and
  Neumann]{becker2019recurrent}
Becker, P., Pandya, H., Gebhardt, G., Zhao, C., Taylor, C.~J., and Neumann, G.
\newblock Recurrent kalman networks: Factorized inference in high-dimensional
  deep feature spaces.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  544--552. PMLR, 2019.

\bibitem[Brouwer et~al.(2019)Brouwer, Simm, Arany, and Moreau]{brouwer2019gru}
Brouwer, E.~D., Simm, J., Arany, A., and Moreau, Y.
\newblock Gru-ode-bayes: continuous modeling of sporadically-observed time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7379--7390, 2019.

\bibitem[Cao et~al.(2018)Cao, Wang, Li, Zhou, Li, and Li]{cao2018brits}
Cao, W., Wang, D., Li, J., Zhou, H., Li, L., and Li, Y.
\newblock Brits: Bidirectional recurrent imputation for time series.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6775--6785, 2018.

\bibitem[Che et~al.(2018)Che, Purushotham, Cho, Sontag, and
  Liu]{che2018recurrent}
Che, Z., Purushotham, S., Cho, K., Sontag, D., and Liu, Y.
\newblock Recurrent neural networks for multivariate time series with missing
  values.
\newblock \emph{Scientific reports}, 8\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Chen, R.~T., Rubanova, Y., Bettencourt, J., and Duvenaud, D.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6572--6583, 2018.

\bibitem[Choi et~al.(2018)Choi, Xiao, Stewart, and Sun]{choi2018mime}
Choi, E., Xiao, C., Stewart, W.~F., and Sun, J.
\newblock Mime: Multilevel medical embedding of electronic health records for
  predictive healthcare.
\newblock \emph{arXiv preprint arXiv:1810.09593}, 2018.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and
  Bengio]{chung2014empirical}
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Chung et~al.(2015)Chung, Kastner, Dinh, Goel, Courville, and
  Bengio]{chung2015recurrent}
Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A.~C., and Bengio, Y.
\newblock A recurrent latent variable model for sequential data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2980--2988, 2015.

\bibitem[Deng et~al.(2020)Deng, Chang, Brubaker, Mori, and
  Lehrmann]{DengCBML20}
Deng, R., Chang, B., Brubaker, M.~A., Mori, G., and Lehrmann, A.
\newblock Modeling continuous stochastic processes with dynamic normalizing
  flows.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7805--7815, 2020.

\bibitem[Deng et~al.(2021)Deng, Brubaker, Mori, and Lehrmann]{DengBML21}
Deng, R., Brubaker, M.~A., Mori, G., and Lehrmann, A.~M.
\newblock Continuous latent process flows.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5162--5173, 2021.

\bibitem[Fraccaro et~al.(2016)Fraccaro, S{\o}nderby, Paquet, and
  Winther]{fraccaro2016sequential}
Fraccaro, M., S{\o}nderby, S.~K., Paquet, U., and Winther, O.
\newblock Sequential neural models with stochastic layers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2199--2207, 2016.

\bibitem[Fraccaro et~al.(2017)Fraccaro, Kamronn, Paquet, and
  Winther]{fraccaro2017disentangled}
Fraccaro, M., Kamronn, S.~D., Paquet, U., and Winther, O.
\newblock A disentangled recognition and nonlinear dynamics model for
  unsupervised learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3601--3610, 2017.

\bibitem[Goyal et~al.(2017)Goyal, Sordoni, C{\^{o}}t{\'{e}}, Ke, and
  Bengio]{goyal2017z}
Goyal, A., Sordoni, A., C{\^{o}}t{\'{e}}, M., Ke, N.~R., and Bengio, Y.
\newblock Z-forcing: Training stochastic recurrent networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6713--6723, 2017.

\bibitem[Herrera et~al.(2021)Herrera, Krach, and Teichmann]{herrera2020neural}
Herrera, C., Krach, F., and Teichmann, J.
\newblock Neural jump ordinary differential equations: Consistent
  continuous-time prediction and filtering.
\newblock In \emph{9th International Conference on Learning Representations},
  2021.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Horn et~al.(2020)Horn, Moor, Bock, Rieck, and Borgwardt]{horn2020set}
Horn, M., Moor, M., Bock, C., Rieck, B., and Borgwardt, K.
\newblock Set functions for time series.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4353--4363. PMLR, 2020.

\bibitem[Jazwinski(1970)]{jazwinski1970stochastic}
Jazwinski, A.~H.
\newblock \emph{Stochastic Processes and Filtering Theory}.
\newblock Academic Press, 1970.

\bibitem[Jia \& Benson(2019)Jia and Benson]{jia2019neural}
Jia, J. and Benson, A.~R.
\newblock Neural jump stochastic differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9843--9854, 2019.

\bibitem[Kalman(1960)]{kalman1960new}
Kalman, R.~E.
\newblock A new approach to linear filtering and prediction problems.
\newblock 1960.

\bibitem[Karl et~al.(2017)Karl, Soelch, Bayer, and van~der Smagt]{karl2016deep}
Karl, M., Soelch, M., Bayer, J., and van~der Smagt, P.
\newblock Deep variational bayes filters: Unsupervised learning of state space
  models from raw data.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and
  Lyons]{kidger2020neural}
Kidger, P., Morrill, J., Foster, J., and Lyons, T.~J.
\newblock Neural controlled differential equations for irregular time series.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kidger et~al.(2021)Kidger, Foster, Li, and Lyons]{kidger2021neural}
Kidger, P., Foster, J., Li, X., and Lyons, T.~J.
\newblock Neural sdes as infinite-dimensional gans.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5453--5463. PMLR, 2021.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Krishnan et~al.(2015)Krishnan, Shalit, and Sontag]{krishnan2015deep}
Krishnan, R.~G., Shalit, U., and Sontag, D.
\newblock Deep kalman filters.
\newblock \emph{arXiv preprint arXiv:1511.05121}, 2015.

\bibitem[Lechner \& Hasani(2020)Lechner and Hasani]{lechner2020learning}
Lechner, M. and Hasani, R.
\newblock Learning long-term dependencies in irregularly-sampled time series.
\newblock \emph{arXiv preprint arXiv:2006.04418}, 2020.

\bibitem[LeCun et~al.(1988)LeCun, Touresky, Hinton, and
  Sejnowski]{lecun1988theoretical}
LeCun, Y., Touresky, D., Hinton, G., and Sejnowski, T.
\newblock A theoretical framework for back-propagation.
\newblock In \emph{Proceedings of the 1988 Connectionist Models Summer School},
  pp.\  21--28, 1988.

\bibitem[Lezcano-Casado(2019)]{lezcano2019trivializations}
Lezcano-Casado, M.
\newblock Trivializations for gradient-based optimization on manifolds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9154--9164, 2019.

\bibitem[Li et~al.(2020)Li, Wong, Chen, and Duvenaud]{li2020scalable}
Li, X., Wong, T.-K.~L., Chen, R.~T., and Duvenaud, D.
\newblock Scalable gradients for stochastic differential equations.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3870--3882. PMLR, 2020.

\bibitem[Lipton et~al.(2016)Lipton, Kale, and Wetzel]{lipton2016directly}
Lipton, Z.~C., Kale, D., and Wetzel, R.
\newblock Directly modeling missing data in sequences with rnns: Improved
  classification of clinical time series.
\newblock In \emph{Machine Learning for Healthcare Conference}, pp.\  253--270.
  PMLR, 2016.

\bibitem[Menne et~al.(2015)Menne, Williams~Jr, and Vose]{menne2015long}
Menne, M., Williams~Jr, C., and Vose, R.
\newblock Long-term daily climate records from stations across the contiguous
  united states, 2015.

\bibitem[Morrill et~al.(2021)Morrill, Salvi, Kidger, and
  Foster]{morrill2021neural}
Morrill, J., Salvi, C., Kidger, P., and Foster, J.
\newblock Neural rough differential equations for long time series.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7829--7838. PMLR, 2021.

\bibitem[Mozer et~al.(2017)Mozer, Kazakov, and Lindsey]{mozer2017discrete}
Mozer, M.~C., Kazakov, D., and Lindsey, R.~V.
\newblock Discrete event, continuous time rnns.
\newblock \emph{arXiv preprint arXiv:1710.04110}, 2017.

\bibitem[Pearlmutter(1989)]{pearlmutter1989learning}
Pearlmutter, B.~A.
\newblock Learning state space trajectories in recurrent neural networks.
\newblock \emph{Neural Computation}, 1\penalty0 (2):\penalty0 263--269, 1989.

\bibitem[Pearlmutter(1995)]{pearlmutter1995gradient}
Pearlmutter, B.~A.
\newblock Gradient calculations for dynamic recurrent neural networks: A
  survey.
\newblock \emph{IEEE Transactions on Neural networks}, 6\penalty0 (5):\penalty0
  1212--1228, 1995.

\bibitem[Pineda(1987)]{pineda1987generalization}
Pineda, F.
\newblock Generalization of back propagation to recurrent and higher order
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  602--611, 1987.

\bibitem[Rome(1969)]{rome1969direct}
Rome, H.
\newblock A direct solution to the linear variance equation of a time-invariant
  linear system.
\newblock \emph{IEEE Transactions on Automatic Control}, 14\penalty0
  (5):\penalty0 592--593, 1969.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Rubanova, Y., Chen, R.~T., and Duvenaud, D.
\newblock Latent odes for irregularly-sampled time series.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5320--5330, 2019.

\bibitem[Sato(1990)]{sato1990learning}
Sato, M.-a.
\newblock A learning algorithm to teach spatiotemporal patterns to recurrent
  neural networks.
\newblock \emph{Biological Cybernetics}, 62\penalty0 (3):\penalty0 259--263,
  1990.

\bibitem[Schmidt \& Hofmann(2018)Schmidt and Hofmann]{schmidt2018deep}
Schmidt, F. and Hofmann, T.
\newblock Deep state space models for unconditional word generation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6161--6171, 2018.

\bibitem[Shukla \& Marlin(2020)Shukla and Marlin]{shukla2020survey}
Shukla, S.~N. and Marlin, B.~M.
\newblock A survey on principles, models and methods for learning from
  irregularly sampled time series.
\newblock \emph{arXiv preprint arXiv:2012.00168}, 2020.

\bibitem[Shukla \& Marlin(2021)Shukla and Marlin]{ShuklaM21}
Shukla, S.~N. and Marlin, B.~M.
\newblock Multi-time attention networks for irregularly sampled time series.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Silva et~al.(2012)Silva, Moody, Scott, Celi, and
  Mark]{silva2012predicting}
Silva, I., Moody, G., Scott, D.~J., Celi, L.~A., and Mark, R.~G.
\newblock Predicting in-hospital mortality of icu patients: The
  physionet/computing in cardiology challenge 2012.
\newblock In \emph{Computing in Cardiology}, pp.\  245--248. IEEE, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Yang, Ivy, and Chi]{zhang2019attain}
Zhang, Y., Yang, X., Ivy, J.~S., and Chi, M.
\newblock {ATTAIN:} attention-based time-aware {LSTM} networks for disease
  progression modeling.
\newblock In \emph{Proceedings of the Twenty-Eighth International Joint
  Conference on Artificial Intelligence}, pp.\  4369--4375, 2019.

\end{thebibliography}
