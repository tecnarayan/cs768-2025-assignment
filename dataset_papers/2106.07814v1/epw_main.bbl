\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{HMMA{\etalchar{+}}17}

\bibitem[AHK{\etalchar{+}}14]{agarwal14}
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert
  Schapire.
\newblock Taming the monster: A fast and simple algorithm for contextual
  bandits.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2014.

\bibitem[AKKS20]{agarwal20flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem[AKLM20]{agarwal20}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In {\em Proceedings of the Conference on Learning Theory}, 2020.

\bibitem[AL20]{allen2020feature}
Zeyuan {Allen-Zhu} and Yuanzhi Li.
\newblock Feature purification: How adversarial training performs robust deep
  learning.
\newblock {\em arXiv preprint arXiv:2005.10190}, 2020.

\bibitem[ALS19]{als18dnn}
Zeyuan {Allen-Zhu}, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em ICML}, 2019.
\newblock Full version available at \url{http://arxiv.org/abs/1811.03962}.

\bibitem[AZL19]{allen-zhu19ntk}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem[AZL20]{allen2020backward}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Backward feature correction: How deep learning performs deep
  learning.
\newblock {\em arXiv preprint arXiv:2001.04413}, 2020.

\bibitem[AZLS19]{allen-zhu19}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[B{\etalchar{+}}19]{berner19}
Christopher Berner et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock {\em arXiv preprint arxiv:1912.06680}, 2019.

\bibitem[BNVB13]{bellemare13}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem[BPK{\etalchar{+}}20]{badia20}
Adri{\`a}~Puigdom{\`e}nech Badia, Bilal Piot, Steven Kapturowski, Pablo
  Sprechmann, Alex Vitvitskyi, Zhaohan~Daniel Guo, and Charles Blundell.
\newblock Agent57: Outperforming the {A}tari human benchmark.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[CKH{\etalchar{+}}19]{cobbe19}
Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman.
\newblock Quantifying generalization in reinforcement learning.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[DKJ{\etalchar{+}}19]{du19decoding}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and
  John Langford.
\newblock Provably efficient {RL} with rich observations via latent state
  decoding.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[DKWY20]{du20lowerbound}
Simon~S. Du, Sham~M. Kakade, Ruosong Wang, and Lin~F. Yang.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[DLL{\etalchar{+}}19]{du19gdnn}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[DLMW20]{du20}
Simon~S. Du, Jason~D. Lee, Gaurav Mahajan, and Ruosong Wang.
\newblock Agnostic q-learning with function approximation in deterministic
  systems: Near-optimal bounds on approximation error and sample complexity.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem[DLWZ19]{du19}
Simon~S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang.
\newblock Provably efficient q-learning with function approximation via
  distribution shift error checking oracle.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem[DLY{\etalchar{+}}20]{dong20}
Kefan Dong, Yuping Luo, Tianhe Yu, Chelsea Finn, and Tengyu Ma.
\newblock On the expressivity of neural networks for deep reinforcement
  learning.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[DMM{\etalchar{+}}19]{dean19}
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock {\em Foundations of Computational Mathematics}, pages 1--47, 2019.

\bibitem[DPWZ20]{dong20generalfunc}
Kefan Dong, Jian Peng, Yining Wang, and Yuan Zhou.
\newblock Root-n-regret for learning in markov decision processes with function
  approximation and low bellman rank.
\newblock In {\em Proceedings of the Conference on Learning Theory}, 2020.

\bibitem[DSL{\etalchar{+}}18]{dai18}
Bo~Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and
  Le~Song.
\newblock {SBEED}: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[FGKM18]{fazel18}
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[FRH{\etalchar{+}}19]{fazlyab19}
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George
  Pappas.
\newblock Efficient and accurate estimation of lipschitz constants for deep
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem[HMMA{\etalchar{+}}17]{hadfield-menell17}
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart~J Russell, and Anca
  Dragan.
\newblock Inverse reward design.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[JKA{\etalchar{+}}17]{jiang17contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E.
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[JLM21]{jin21bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms, 2021.

\bibitem[JYWJ20]{jin20}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Proceedings of the Conference on Learning Theory}, 2020.

\bibitem[LFDA16]{levine16}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em The Journal of Machine Learning Research}, 17:1334--1373, 2016.

\bibitem[LL18a]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem[LL18b]{li18}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem[LMZ20]{li20ntk}
Yuanzhi Li, Tengyu Ma, and Hongyang~R. Zhang.
\newblock Learning over-parametrized two-layer neural networks beyond ntk.
\newblock In {\em Proceedings of the Conference on Learning Theory}, 2020.

\bibitem[LSW20]{lattimore20}
Tor Lattimore, Csaba Szepesvari, and Gellert Weisz.
\newblock Learning with good feature representations in bandits and in {RL}
  with a generative model.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[M{\etalchar{+}}15]{mnih15}
Volodymyr Mnih et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518:529--533, 2015.

\bibitem[MHKL20]{misra20}
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[MKS{\etalchar{+}}13]{mnih13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock In {\em NIPS Deep Learning Workshop}. 2013.

\bibitem[MLR21]{malik21}
Dhruv Malik, Yuanzhi Li, and Pradeep Ravikumar.
\newblock When is generalizable reinforcement learning tractable?
\newblock {\em arXiv preprint arxiv:2101.00300}, 2021.

\bibitem[MPB{\etalchar{+}}20]{malik20}
Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter~L Bartlett,
  and Martin~J Wainwright.
\newblock Derivative-free methods for policy optimization: Guarantees for
  linear quadratic systems.
\newblock {\em Journal of Machine Learning Research}, 21:1--51, 2020.

\bibitem[PW21]{pananjady21}
A.~{Pananjady} and M.~J. {Wainwright}.
\newblock Instance-dependent $\ell_{\infty}$-bounds for policy evaluation in
  tabular reinforcement learning.
\newblock {\em IEEE Transactions on Information Theory}, 67:566--585, 2021.

\bibitem[SJ19]{simchowitz19}
Max Simchowitz and Kevin~G Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In {\em Advances in Neural Information Processing Systems}. 2019.

\bibitem[SLA{\etalchar{+}}15]{schulman15}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2015.

\bibitem[Ver18]{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[VRD19]{vanroy19}
Benjamin Van~Roy and Shi Dong.
\newblock Comments on the du-kakade-wang-yang lower bounds.
\newblock {\em arXiv preprint arxiv:1911.07910}, 2019.

\bibitem[WAJ{\etalchar{+}}21]{weisz21}
Gellert Weisz, Philip Amortila, Barnabas Janzer, Yasin Abbasi-Yadkori, Nan
  Jiang, and Csaba Szepesvari.
\newblock On query-efficient planning in mdps under linear realizability of the
  optimal state-value function.
\newblock In {\em Proceedings of the Conference on Learning Theory}, 2021.

\bibitem[WAS21]{weisz21lowerbound}
Gellert Weisz, Philip Amortila, and {Cs}aba {Sz}epesvari.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock In {\em Proceedings of the International Conference on Algorithmic
  Learning Theory}, 2021.

\bibitem[WSY20]{wang20generalfunc}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem[WVR13]{wen13}
Zheng Wen and Benjamin Van~Roy.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock In {\em Advances in Neural Information Processing Systems}, 2013.

\bibitem[WWDK21]{wang21}
Yining Wang, Ruosong Wang, Simon~S. Du, and Akshay Krishnamurthy.
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem[YW19]{yang19}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[YW20]{yang20}
Lin Yang and Mengdi Wang.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2020.

\end{thebibliography}
