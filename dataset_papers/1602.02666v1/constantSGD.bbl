\begin{thebibliography}{}

\bibitem[\protect\astroncite{Ahn et~al.}{2012}]{ahn2012bayesian}
Ahn, S., Korattikara, A., and Welling, M. (2012).
\newblock Bayesian posterior sampling via stochastic gradient fisher scoring.
\newblock {\em arXiv preprint arXiv:1206.6380}.

\bibitem[\protect\astroncite{Bach and Moulines}{2013}]{bach2013non}
Bach, F. and Moulines, E. (2013).
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate o (1/n).
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  773--781.

\bibitem[\protect\astroncite{Bishop}{2006}]{Bishop:2006}
Bishop, C. (2006).
\newblock {\em Pattern Recognition and Machine Learning}.
\newblock Springer New York.

\bibitem[\protect\astroncite{Bottou}{1998}]{bottou1998online}
Bottou, L. (1998).
\newblock Online learning and stochastic approximations.
\newblock {\em On-line learning in neural networks}, 17(9):25.

\bibitem[\protect\astroncite{Chen et~al.}{2015a}]{chen2015bridging}
Chen, C., Carlson, D., Gan, Z., Li, C., and Carin, L. (2015a).
\newblock Bridging the gap between stochastic gradient mcmc and stochastic
  optimization.
\newblock {\em arXiv preprint arXiv:1512.07962}.

\bibitem[\protect\astroncite{Chen et~al.}{2015b}]{chen2015convergence}
Chen, C., Ding, N., and Carin, L. (2015b).
\newblock On the convergence of stochastic gradient mcmc algorithms with
  high-order integrators.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2269--2277.

\bibitem[\protect\astroncite{Chen et~al.}{2014}]{chen2014stochastic}
Chen, T., Fox, E.~B., and Guestrin, C. (2014).
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock {\em arXiv preprint arXiv:1402.4102}.

\bibitem[\protect\astroncite{D{\'e}fossez and
  Bach}{2015}]{defossez2015averaged}
D{\'e}fossez, A. and Bach, F. (2015).
\newblock Averaged least-mean-squares: Bias-variance trade-offs and optimal
  sampling distributions.
\newblock In {\em Proceedings of the Eighteenth International Conference on
  Artificial Intelligence and Statistics}, pages 205--213.

\bibitem[\protect\astroncite{Ding et~al.}{2014}]{ding2014bayesian}
Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R.~D., and Neven, H. (2014).
\newblock Bayesian sampling using stochastic gradient thermostats.
\newblock In {\em Advances in neural information processing systems}, pages
  3203--3211.

\bibitem[\protect\astroncite{Duchi et~al.}{2011}]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y. (2011).
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em The Journal of Machine Learning Research}, 12:2121--2159.

\bibitem[\protect\astroncite{Flammarion and
  Bach}{2015}]{flammarion2015averaging}
Flammarion, N. and Bach, F. (2015).
\newblock From averaging to acceleration, there is only a step-size.
\newblock {\em arXiv preprint arXiv:1504.01577}.

\bibitem[\protect\astroncite{Gardiner et~al.}{1985}]{gardiner1985handbook}
Gardiner, C.~W. et~al. (1985).
\newblock {\em Handbook of stochastic methods}, volume~4.
\newblock Springer Berlin.

\bibitem[\protect\astroncite{Jordan et~al.}{1999a}]{Jordan:1999}
Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. (1999a).
\newblock Introduction to variational methods for graphical models.
\newblock {\em Machine Learning}, 37:183--233.

\bibitem[\protect\astroncite{Jordan et~al.}{1999b}]{jordan1999introduction}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K. (1999b).
\newblock An introduction to variational methods for graphical models.
\newblock {\em Machine learning}, 37(2):183--233.

\bibitem[\protect\astroncite{Kramers}{1940}]{kramers1940brownian}
Kramers, H.~A. (1940).
\newblock Brownian motion in a field of force and the diffusion model of
  chemical reactions.
\newblock {\em Physica}, 7(4):284--304.

\bibitem[\protect\astroncite{Kucukelbir et~al.}{2015}]{kucukelbir2015automatic}
Kucukelbir, A., Ranganath, R., Gelman, A., and Blei, D. (2015).
\newblock Automatic variational inference in stan.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  568--576.

\bibitem[\protect\astroncite{Kushner and Yin}{2003}]{kushner2003stochastic}
Kushner, H.~J. and Yin, G. (2003).
\newblock {\em Stochastic approximation and recursive algorithms and
  applications}, volume~35.
\newblock Springer Science \& Business Media.

\bibitem[\protect\astroncite{Li et~al.}{2015}]{li2015dynamics}
Li, Q., Tai, C., et~al. (2015).
\newblock Dynamics of stochastic gradient algorithms.
\newblock {\em arXiv preprint arXiv:1511.06251}.

\bibitem[\protect\astroncite{Ljung et~al.}{2012}]{ljung2012stochastic}
Ljung, L., Pflug, G.~C., and Walk, H. (2012).
\newblock {\em Stochastic approximation and optimization of random systems},
  volume~17.
\newblock Birkh{\"a}user.

\bibitem[\protect\astroncite{Longford}{1987}]{longford1987fast}
Longford, N.~T. (1987).
\newblock A fast scoring algorithm for maximum likelihood estimation in
  unbalanced mixed models with nested random effects.
\newblock {\em Biometrika}, 74(4):817--827.

\bibitem[\protect\astroncite{Ma et~al.}{2015}]{ma2015complete}
Ma, Y.-A., Chen, T., and Fox, E.~B. (2015).
\newblock A complete recipe for stochastic gradient mcmc.
\newblock {\em arXiv preprint arXiv:1506.04696}.

\bibitem[\protect\astroncite{Maclaurin et~al.}{2015}]{maclaurin2015early}
Maclaurin, D., Duvenaud, D., and Adams, R.~P. (2015).
\newblock Early stopping is nonparametric variational inference.
\newblock {\em arXiv preprint arXiv:1504.01344}.

\bibitem[\protect\astroncite{Robbins and Monro}{1951}]{robbins1951stochastic}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407.

\bibitem[\protect\astroncite{Sakrison}{1965}]{sakrison1965efficient}
Sakrison, D.~J. (1965).
\newblock Efficient recursive estimation; application to estimating the
  parameters of a covariance function.
\newblock {\em International Journal of Engineering Science}, 3(4):461--483.

\bibitem[\protect\astroncite{Sato and Nakagawa}{2014}]{sato2014approximation}
Sato, I. and Nakagawa, H. (2014).
\newblock Approximation analysis of stochastic gradient langevin dynamics by
  using fokker-planck equation and ito process.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pages 982--990.

\bibitem[\protect\astroncite{Tieleman and Hinton}{2012}]{tieleman2012lecture}
Tieleman, T. and Hinton, G. (2012).
\newblock {Lecture 6.5---RmsProp: Divide the Gradient by a Running Average of
  its Recent Magnitude}.
\newblock COURSERA: Neural Networks for Machine Learning.

\bibitem[\protect\astroncite{Toulis et~al.}{2014}]{toulis2014statistical}
Toulis, P., Airoldi, E., and Rennie, J. (2014).
\newblock Statistical analysis of stochastic gradient methods for generalized
  linear models.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pages 667--675.

\bibitem[\protect\astroncite{Toulis et~al.}{}]{toulistowards}
Toulis, P., Tran, D., and Airoldi, E.~M.
\newblock Towards stability and optimality in stochastic gradient descent.

\bibitem[\protect\astroncite{Uhlenbeck and
  Ornstein}{1930}]{uhlenbeck1930theory}
Uhlenbeck, G.~E. and Ornstein, L.~S. (1930).
\newblock On the theory of the brownian motion.
\newblock {\em Physical review}, 36(5):823.

\bibitem[\protect\astroncite{Welling and Teh}{2011}]{welling2011bayesian}
Welling, M. and Teh, Y.~W. (2011).
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 681--688.

\bibitem[\protect\astroncite{Widrow and Stearns}{1985}]{widrow1985adaptive}
Widrow, B. and Stearns, S.~D. (1985).
\newblock Adaptive signal processing.
\newblock {\em Englewood Cliffs, NJ, Prentice-Hall, Inc., 1985, 491 p.}, 1.

\bibitem[\protect\astroncite{Zhang}{2004}]{zhang2004solving}
Zhang, T. (2004).
\newblock Solving large scale linear prediction problems using stochastic
  gradient descent algorithms.
\newblock In {\em Proceedings of the twenty-first international conference on
  Machine learning}, page 116. ACM.

\end{thebibliography}
