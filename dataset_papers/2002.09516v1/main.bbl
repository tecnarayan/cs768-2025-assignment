\begin{thebibliography}{20}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bertsekas et~al.(1995)Bertsekas, Bertsekas, Bertsekas, and
  Bertsekas]{bertsekas1995dynamic}
Bertsekas, D.~P., Bertsekas, D.~P., Bertsekas, D.~P., and Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control}, volume~1.
\newblock Athena scientific Belmont, MA, 1995.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{dani2008stochastic}
Dani, V., Hayes, T.~P., and Kakade, S.~M.
\newblock Stochastic linear optimization under bandit feedback.
\newblock 2008.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2018policy}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Fonteneau et~al.(2013)Fonteneau, Murphy, Wehenkel, and
  Ernst]{fonteneau2013batch}
Fonteneau, R., Murphy, S.~A., Wehenkel, L., and Ernst, D.
\newblock Batch mode reinforcement learning based on the synthesis of
  artificial trajectories.
\newblock \emph{Annals of operations research}, 208\penalty0 (1):\penalty0
  383--416, 2013.

\bibitem[Freedman(1975)]{freedman1975tail}
Freedman, D.~A.
\newblock On tail probabilities for martingales.
\newblock \emph{The Annals of Probability}, 3\penalty0 (1):\penalty0 100--118,
  1975.

\bibitem[Grunewalder et~al.(2012)Grunewalder, Lever, Baldassarre, Pontil, and
  Gretton]{grunewalder2012modelling}
Grunewalder, S., Lever, G., Baldassarre, L., Pontil, M., and Gretton, A.
\newblock Modelling transition dynamics in mdps with rkhs embeddings.
\newblock 2012.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2015doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Jong \& Stone(2007)Jong and Stone]{jong2007model}
Jong, N.~K. and Stone, P.
\newblock Model-based function approximation in reinforcement learning.
\newblock In \emph{Proceedings of the 6th international joint conference on
  Autonomous agents and multiagent systems}, pp.\  1--8, 2007.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{lagoudakis2003least}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Dec):\penalty0 1107--1149, 2003.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5356--5366, 2018.

\bibitem[Liu et~al.(2019)Liu, Swaminathan, Agarwal, and Brunskill]{liu2019off}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.
\newblock Off-policy policy gradient with state distribution correction.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, 2019.

\bibitem[Mannor et~al.(2004)Mannor, Simester, Sun, and
  Tsitsiklis]{mannor2004bias}
Mannor, S., Simester, D., Sun, P., and Tsitsiklis, J.~N.
\newblock Bias and variance in value function estimation.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, pp.\ ~72, 2004.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L.
\newblock {DualDICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems 32}. 2019.

\bibitem[Precup(2000)]{precup2000eligibility}
Precup, D.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series}, pp.\
  ~80, 2000.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{thomas2016data}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Tropp et~al.(2011)]{tropp2011freedman}
Tropp, J. et~al.
\newblock Freedman's inequality for matrix martingales.
\newblock \emph{Electronic Communications in Probability}, 16:\penalty0
  262--270, 2011.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
Xie, T., Ma, Y., and Wang, Y.-X.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9665--9675, 2019.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019reinforcement}
Yang, L.~F. and Wang, M.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \emph{arXiv preprint arXiv:1905.10389}, 2019.

\bibitem[Yin \& Wang(2020)Yin and Wang]{yin2020asymptotically}
Yin, M. and Wang, Y.-X.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2001.10742}, 2020.

\end{thebibliography}
