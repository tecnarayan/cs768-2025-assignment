\newcommand{\etalchar}[1]{$^{#1}$}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
% \MRhref is called by the amsart/book/proc definition of \MR.
\providecommand{\MRhref}[2]{%
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{MHPG{\etalchar{+}}22}

\bibitem[ABM22]{mergedstaircase}
Emmanuel Abbe, Enric Boix{-}Adsera, and Theodor Misiakiewicz, \emph{The
  merged-staircase property: a necessary and nearly sufficient condition for
  {SGD} learning of sparse functions on two-layer neural networks, {COLT}},
  2022.

\bibitem[ABM23]{abbe2023sgd}
\bysame, \emph{Sgd learning on neural networks: leap complexity and
  saddle-to-saddle dynamics}, arXiv preprint arXiv:2302.11055 (2023).

\bibitem[ACH18]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan, \emph{On the optimization of deep
  networks: Implicit acceleration by overparameterization}, International
  Conference on Machine Learning, PMLR, 2018, pp.~244--253.

\bibitem[ACHL19]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo, \emph{Implicit
  regularization in deep matrix factorization}, Advances in Neural Information
  Processing Systems \textbf{32} (2019).

\bibitem[AZG20]{aghajanyan2020intrinsic}
Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta, \emph{Intrinsic
  dimensionality explains the effectiveness of language model fine-tuning},
  arXiv preprint arXiv:2012.13255 (2020).

\bibitem[Bac20]{bach2020effortless}
Francis Bach, \emph{Effortless optimization through gradient flows}, Machine
  Learning Research Blog. https://francisbach. com/gradient-flows (2020).

\bibitem[BBSS22]{bietti2022learning}
Alberto Bietti, Joan Bruna, Clayton Sanford, and Min~Jae Song, \emph{Learning
  single-index models with shallow neural networks}, arXiv preprint
  arXiv:2210.15651 (2022).

\bibitem[Ber22]{berthier2022incremental}
Rapha{\"e}l Berthier, \emph{Incremental learning in diagonal linear networks},
  arXiv preprint arXiv:2208.14673 (2022).

\bibitem[BMR{\etalchar{+}}20]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al., \emph{Language models are few-shot learners}, Advances in neural
  information processing systems \textbf{33} (2020), 1877--1901.

\bibitem[BPVF22]{boursier2022gradient}
Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion, \emph{Gradient
  flow dynamics of shallow relu networks for square loss and orthogonal
  inputs}, arXiv preprint arXiv:2206.00939 (2022).

\bibitem[COB18]{lazy}
L{\'e}na{\"i}c Chizat, Edouard Oyallon, and Francis~R. Bach, \emph{On lazy
  training in differentiable programming}, Neural Information Processing
  Systems, 2018.

\bibitem[DBK{\etalchar{+}}20]{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, \emph{An image is
  worth 16x16 words: Transformers for image recognition at scale}, ArXiv
  \textbf{abs/2010.11929} (2020).

\bibitem[DCLT19]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, \emph{Bert:
  Pre-training of deep bidirectional transformers for language understanding},
  ArXiv \textbf{abs/1810.04805} (2019).

\bibitem[DHL18]{du2018algorithmic}
Simon~S Du, Wei Hu, and Jason~D Lee, \emph{Algorithmic regularization in
  learning deep homogeneous models: Layers are automatically balanced},
  Advances in Neural Information Processing Systems \textbf{31} (2018).

\bibitem[DLS22]{damian2022neural}
Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi, \emph{Neural networks
  can learn representations with gradient descent}, Conference on Learning
  Theory, PMLR, 2022, pp.~5413--5452.

\bibitem[FVB{\etalchar{+}}22]{frei2022implicit}
Spencer Frei, Gal Vardi, Peter~L Bartlett, Nathan Srebro, and Wei Hu,
  \emph{Implicit bias in leaky relu networks trained on high-dimensional data},
  arXiv preprint arXiv:2210.07082 (2022).

\bibitem[GBLJ19]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien, \emph{Implicit
  regularization of discrete gradient dynamics in linear neural networks},
  Advances in Neural Information Processing Systems \textbf{32} (2019).

\bibitem[GMMM19]{lazy_limit}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari,
  \emph{Limitations of lazy training of two-layers neural network}, Advances in
  Neural Information Processing Systems \textbf{32} (2019).

\bibitem[GSJW19]{lazy2}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart,
  \emph{Disentangling feature and lazy learning in deep neural networks: an
  empirical study}, ArXiv \textbf{abs/1906.08034} (2019).

\bibitem[GSSD19]{gissin2019implicit}
Daniel Gissin, Shai Shalev-Shwartz, and Amit Daniely, \emph{The implicit bias
  of depth: How incremental learning drives generalization}, arXiv preprint
  arXiv:1909.12051 (2019).

\bibitem[HSW{\etalchar{+}}21]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen, \emph{Lora: Low-rank adaptation of large
  language models}, arXiv preprint arXiv:2106.09685 (2021).

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler, \emph{Neural tangent
  kernel: Convergence and generalization in neural networks}, Advances in
  neural information processing systems \textbf{31} (2018).

\bibitem[JGS{\etalchar{+}}21]{saddle}
Arthur Jacot, Francois~Gaston Ged, Berfin Simsek, Cl{\'e}ment Hongler, and
  Franck Gabriel, \emph{Saddle-to-saddle dynamics in deep linear networks:
  Small initialization training, symmetry, and sparsity}, 2021.

\bibitem[JLL{\etalchar{+}}23]{jin2023understanding}
Jikai Jin, Zhiyuan Li, Kaifeng Lyu, Simon~S Du, and Jason~D Lee,
  \emph{Understanding incremental learning of gradient descent: A fine-grained
  analysis of matrix sensing}, arXiv preprint arXiv:2301.11500 (2023).

\bibitem[KB14]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba, \emph{Adam: A method for stochastic
  optimization}, arXiv preprint arXiv:1412.6980 (2014).

\bibitem[KC22]{small2}
Daesung Kim and Hye~Won Chung, \emph{Rank-1 matrix completion with gradient
  descent and small random initialization}, ArXiv \textbf{abs/2212.09396}
  (2022).

\bibitem[LFLY18]{li2018measuring}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski, \emph{Measuring
  the intrinsic dimension of objective landscapes}, arXiv preprint
  arXiv:1804.08838 (2018).

\bibitem[LLL20]{lowrank3}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu, \emph{Towards resolving the implicit
  bias of gradient descent for matrix factorization: Greedy low-rank learning},
  ArXiv \textbf{abs/2012.09839} (2020).

\bibitem[LOG{\etalchar{+}}19]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov, \emph{Roberta: A
  robustly optimized bert pretraining approach}, ArXiv \textbf{abs/1907.11692}
  (2019).

\bibitem[MGW{\etalchar{+}}20]{scale}
Edward Moroshko, Suriya Gunasekar, Blake~E. Woodworth, J.~Lee, Nathan Srebro,
  and Daniel Soudry, \emph{Implicit bias in deep linear classification:
  Initialization scale vs training accuracy}, ArXiv \textbf{abs/2007.06738}
  (2020).

\bibitem[MHPG{\etalchar{+}}22]{mousavi2022neural}
Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and
  Murat~A Erdogdu, \emph{Neural networks efficiently learn low-dimensional
  representations with sgd}, arXiv preprint arXiv:2209.14863 (2022).

\bibitem[MKAA21]{lowrank2}
Paolo Milanesi, Hachem Kadri, S.~Ayache, and Thierry Arti{\`e}res,
  \emph{Implicit regularization in deep tensor factorization}, 2021
  International Joint Conference on Neural Networks (IJCNN) (2021), 1--8.

\bibitem[MKAS21]{quantifying}
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro,
  \emph{Quantifying the benefit of using differentiable learning over tangent
  kernels}, Proceedings of the 38th International Conference on Machine
  Learning (Marina Meila and Tong Zhang, eds.), Proceedings of Machine Learning
  Research, vol. 139, PMLR, 18--24 Jul 2021, pp.~7379--7389.

\bibitem[PA23]{Patel_Ahmad_2023}
Dylan Patel and Afzal Ahmad, \emph{Google “we have no moat, and neither does
  openai”}, May 2023.

\bibitem[PF23]{pesme2023saddle}
Scott Pesme and Nicolas Flammarion, \emph{Saddle-to-saddle dynamics in diagonal
  linear networks}, arXiv preprint arXiv:2304.00488 (2023).

\bibitem[RC20]{razin2020implicit}
Noam Razin and Nadav Cohen, \emph{Implicit regularization in deep learning may
  not be explainable by norms}, Advances in neural information processing
  systems \textbf{33} (2020), 21174--21187.

\bibitem[SKZ{\etalchar{+}}23]{simon2023stepwise}
James~B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham~J Fetterman,
  and Joshua Albrecht, \emph{On the stepwise nature of self-supervised
  learning}, arXiv preprint arXiv:2303.15438 (2023).

\bibitem[SMG13]{saxe2013exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli, \emph{Exact solutions to
  the nonlinear dynamics of learning in deep linear neural networks}, arXiv
  preprint arXiv:1312.6120 (2013).

\bibitem[SS21]{small}
Dominik St{\"o}ger and Mahdi Soltanolkotabi, \emph{Small random initialization
  is akin to spectral learning: Optimization and generalization guarantees for
  overparameterized low-rank matrix reconstruction}, Neural Information
  Processing Systems, 2021.

\bibitem[TGZ{\etalchar{+}}23]{taori2023alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B Hashimoto, \emph{Alpaca: A strong,
  replicable instruction-following model}, Stanford Center for Research on
  Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html
  (2023).

\bibitem[TVS23]{timor2023implicit}
Nadav Timor, Gal Vardi, and Ohad Shamir, \emph{Implicit regularization towards
  rank minimization in relu networks}, International Conference on Algorithmic
  Learning Theory, PMLR, 2023, pp.~1429--1459.

\bibitem[VSP{\etalchar{+}}17]{transformer}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin, \emph{Attention is all
  you need}, ArXiv \textbf{abs/1706.03762} (2017).

\bibitem[WGL{\etalchar{+}}19]{diagonal4}
Blake~E. Woodworth, Suriya Gunasekar, J.~Lee, Edward Moroshko, Pedro H.~P.
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro, \emph{Kernel and rich
  regimes in overparametrized models}, ArXiv \textbf{abs/2002.09277} (2019).

\bibitem[YW23]{yu2023compressing}
Hao Yu and Jianxin Wu, \emph{Compressing transformers: Features are low-rank,
  but weights are not!}

\bibitem[ZZC{\etalchar{+}}23]{zhao2023inrank}
Jiawei Zhao, Yifei Zhang, Beidi Chen, Florian Sch{\"a}fer, and Anima
  Anandkumar, \emph{Inrank: Incremental low-rank learning}, arXiv preprint
  arXiv:2306.11250 (2023).

\end{thebibliography}
