\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Y.~Abbasi-Yadkori, D.~P{\'a}l, and C.~Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2312--2320, 2011.

\bibitem[Agrawal(1995)]{Agr95}
R.~Agrawal.
\newblock Sample mean based index policies with ${O}(log n)$ regret for the
  multi-armed bandit problem.
\newblock \emph{Advances in Applied Probability}, 27:\penalty0 1054--1078,
  1995.

\bibitem[Agrawal and Jia(2017)]{agrawal2017optimistic}
S.~Agrawal and R.~Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1184--1194, 2017.

\bibitem[Auer and Ortner(2007)]{auer2007logarithmic}
P.~Auer and R.~Ortner.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  49--56, 2007.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
P.~Auer, N.~Cesa-Bianchi, and P.~Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine Learning Journal}, 47\penalty0 (2-3):\penalty0
  235--256, 2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning-Volume 70},
  pages 263--272. JMLR. org, 2017.

\bibitem[Bartlett and Tewari(2009)]{bartlett2012regal}
P.~L. Bartlett and A.~Tewari.
\newblock {REGAL}: a regularization based algorithm for reinforcement learning
  in weakly communicating {MDPs}.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  35--42, 2009.

\bibitem[Bellman(1957)]{bellman57}
R.~Bellman.
\newblock \emph{Dynamic Programming}.
\newblock Princeton University Press, Princeton, New Jersey, 1957.

\bibitem[Bertsekas(2007)]{Ber07:DPbookVol2}
D.~P. Bertsekas.
\newblock \emph{Dynamic Programming and Optimal Control}, volume~2.
\newblock Athena Scientific, Belmont, MA, 3 edition, 2007.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and Massart]{BLM13}
S.~Boucheron, G.~Lugosi, and P.~Massart.
\newblock \emph{Concentration inequalities: {A} Nonasymptotic Theory of
  Independence}.
\newblock Oxford University Press, 2013.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
S.~Boyd, S.~P. Boyd, and L.~Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Bradtke and Barto(1996)]{BraBa96}
S.~J. Bradtke and A.~G. Barto.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine Learning}, 22:\penalty0 33--57, 1996.

\bibitem[Brafman and Tennenholtz(2002)]{BraTen02}
R.~I. Brafman and M.~Tennenholtz.
\newblock {R-MAX} - a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 213--231,
  2002.

\bibitem[Burnetas and Katehakis(1996)]{BuKa96}
A.~Burnetas and M.~Katehakis.
\newblock Optimal adaptive policies for sequential allocation problems.
\newblock \emph{Advances in Applied Mathematics}, 17:\penalty0 122--142, 1996.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{DHK08}
V.~Dani, T.~P. Hayes, and S.~M. Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In R.~A. Servedio and T.~Zhang, editors, \emph{21st Annual Conference
  on Learning Theory - COLT 2008, Helsinki, Finland, July 9-12, 2008}, pages
  355--366. Omnipress, 2008.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{DLB17}
C.~Dann, T.~Lattimore, and E.~Brunskill.
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  5713--5723. 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
C.~Dann, L.~Li, W.~Wei, and E.~Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1507--1516, 2019.

\bibitem[de~Ghellinck(1960)]{Ghe60}
G.~de~Ghellinck.
\newblock Les probl\`emes de d\'ecisions s\'equentielles.
\newblock \emph{Cahiers du Centre dâ€™\'Etudes de Recherche Op\'erationnelle},
  2:\penalty0 161--179, 1960.

\bibitem[Denardo(1970)]{Den70}
E.~V. Denardo.
\newblock On linear programming in a {Markov} decision problem.
\newblock \emph{Management Science}, 16\penalty0 (5):\penalty0 281--288, 1970.

\bibitem[Filippi et~al.(2010)Filippi, Capp{\'e}, and
  Garivier]{filippi2010optimism}
S.~Filippi, O.~Capp{\'e}, and A.~Garivier.
\newblock Optimism in reinforcement learning and kullback-leibler divergence.
\newblock In \emph{Allerton Conference on Communication, Control, and Computing
  (Allerton)}, pages 115--122. IEEE, 2010.

\bibitem[Fruit et~al.(2019)Fruit, Pirotta, and
  Lazaric]{improved_analysis_UCRL2B}
R.~Fruit, M.~Pirotta, and A.~Lazaric.
\newblock Improved analysis of {UCRL2B}, 2019.
\newblock \url{https://rlgammazero.github.io/docs/ucrl2b_improved.pdf}.

\bibitem[Howard(1960)]{howard60}
R.~A. Howard.
\newblock \emph{Dynamic Programming and {M}arkov Processes}.
\newblock The MIT Press, Cambridge, MA, 1960.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
C.~Jin, Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2019)Jin, Yang, Wang, and Jordan]{jin2020provably}
C.~Jin, Z.~Yang, Z.~Wang, and M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1907.05388}, 2019.

\bibitem[Kakade(2003)]{KakadeThesis:2003}
S.~Kakade.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, Gatsby Computational Neuroscience Unit, University
  College London, 2003.

\bibitem[Lagoudakis and Parr(2003)]{lagoudakis2003least}
M.~G. Lagoudakis and R.~Parr.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Dec):\penalty0 1107--1149, 2003.

\bibitem[Lai and Robbins(1985)]{LaiRo85}
T.~L. Lai and H.~Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in Applied Mathematics}, 6:\penalty0 4--22, 1985.

\bibitem[Lattimore and Szepesv{\'a}ri(2019)]{LSz19book}
T.~Lattimore and {\relax Cs}.~Szepesv{\'a}ri.
\newblock Bandit algorithms.
\newblock \emph{book draft}, 2019.

\bibitem[Liese and Vajda(2006)]{LV06}
F.~Liese and I.~Vajda.
\newblock On divergences and informations in statistics and information theory.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (10):\penalty0 4394--4412, 2006.

\bibitem[Maillard et~al.(2014)Maillard, Mann, and Mannor]{maillard2014hard}
O.-A. Maillard, T.~A. Mann, and S.~Mannor.
\newblock ``{How hard is my MDP}?'' {The} distribution-norm to the rescue".
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1835--1843, 2014.

\bibitem[Manne(1960)]{Man60}
A.~S. Manne.
\newblock Linear programming and sequential decisions.
\newblock \emph{Management Science}, 6\penalty0 (3):\penalty0 259--267, 1960.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
A.~Maurer and M.~Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock In \emph{Conference on Learning Theory}, 2009.

\bibitem[Osband and Van~Roy(2016)]{osband2016lower}
I.~Osband and B.~Van~Roy.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1608.02732}, 2016.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
I.~Osband, D.~Russo, and B.~Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3003--3011, 2013.

\bibitem[Parr et~al.(2008)Parr, Li, Taylor, Painter-Wakefield, and
  Littman]{PalITaPWLi08}
R.~Parr, L.~Li, G.~Taylor, C.~Painter-Wakefield, and M.~L. Littman.
\newblock An analysis of linear models, linear value-function approximation,
  and feature selection for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  752--759, 2008.

\bibitem[Pires and Szepesv{\'a}ri(2016)]{PS16}
B.~{\'A}. Pires and {\text Cs}.~Szepesv{\'a}ri.
\newblock Policy error bounds for model-based reinforcement learning with
  factored linear models.
\newblock In \emph{Conference on Learning Theory}, pages 121--151, 2016.

\bibitem[Puterman(1994)]{Puterman1994}
M.~L. Puterman.
\newblock \emph{{M}arkov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock Wiley-Interscience, April 1994.

\bibitem[Rosenberg and Mansour(2019)]{rosenberg2019online}
A.~Rosenberg and Y.~Mansour.
\newblock Online convex optimization in adversarial {Markov} decision
  processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  5478--5486, 2019.

\bibitem[Russo(2019)]{russo2019worst}
D.~Russo.
\newblock Worst-case regret bounds for exploration via randomized value
  functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14410--14420, 2019.

\bibitem[Simchowitz and Jamieson(2019)]{simchowitz2019non}
M.~Simchowitz and K.~G. Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular {MDPs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1151--1160, 2019.

\bibitem[Strehl and Littman(2008)]{SL08}
A.~L. Strehl and M.~L. Littman.
\newblock An analysis of model-based interval estimation for {M}arkov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Sutton and Barto(2018)]{SB18}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction. 2nd edition.}
\newblock 2018.

\bibitem[{Sz}epesv\'{a}ri(2010)]{Sze10}
{\relax Cs}.~{Sz}epesv\'{a}ri.
\newblock \emph{Algorithms for Reinforcement Learning}.
\newblock Synthesis Lectures on Artificial Intelligence and Machine Learning.
  Morgan \& Claypool Publishers, 2010.

\bibitem[Szita and Szepesv{\'a}ri()]{SziSze10}
I.~Szita and {\relax Cs}.~Szepesv{\'a}ri.
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds.
\newblock pages 1031--1038.

\bibitem[Talebi and Maillard(2018)]{talebi2018variance}
M.~S. Talebi and O.-A. Maillard.
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in {MDPs}.
\newblock In \emph{Algorithmic Learning Theory}, pages 770--805, 2018.

\bibitem[Tewari and Bartlett(2008)]{tewari2008optimistic}
A.~Tewari and P.~L. Bartlett.
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible {MDPs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1505--1512, 2008.

\bibitem[Yang and Wang(2019)]{yang2019reinforcement}
L.~F. Yang and M.~Wang.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \emph{arXiv preprint arXiv:1905.10389}, 2019.

\bibitem[Yao et~al.(2014)Yao, Szepesv\'ari, Pires, and Zhang]{factlin}
H.~Yao, {\text Cs}.~Szepesv\'ari, B.~Pires, and X.~Zhang.
\newblock Pseudo-{MDPs} and factored linear action models.
\newblock 10 2014.
\newblock \doi{10.1109/ADPRL.2014.7010633}.

\bibitem[Zanette and Brunskill(2019)]{ZB19}
A.~Zanette and E.~Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Pirotta,
  and Lazaric]{zanette2019frequentist}
A.~Zanette, D.~Brandfonbrener, M.~Pirotta, and A.~Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{Artificial Intelligence and Statistics}, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
A.~Zanette, A.~Lazaric, M.~Kochenderfer, and E.~Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock \emph{arXiv preprint arXiv:2003.00153}, 2020{\natexlab{b}}.

\bibitem[Zimin and Neu(2013)]{zimin2013online}
A.~Zimin and G.~Neu.
\newblock Online learning in episodic {Markovian} decision processes by
  relative entropy policy search.
\newblock In \emph{Advances in neural information processing systems}, pages
  1583--1591, 2013.

\end{thebibliography}
