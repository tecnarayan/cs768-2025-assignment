\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(2016)]{amari2016information}
Amari, S.
\newblock \emph{Information geometry and its applications}.
\newblock Springer, 2016.

\bibitem[Anderson \& Peterson(1987)Anderson and Peterson]{anderson1987mean}
Anderson, J.~R. and Peterson, C.
\newblock A mean field theory learning algorithm for neural networks.
\newblock \emph{Complex Systems}, 1:\penalty0 995--1019, 1987.

\bibitem[Balan et~al.(2015)Balan, Rathod, Murphy, and
  Welling]{balan2015bayesian}
Balan, A.~K., Rathod, V., Murphy, K.~P., and Welling, M.
\newblock Bayesian dark knowledge.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3438--3446, 2015.

\bibitem[Barber \& Bishop(1998)Barber and Bishop]{barber1998ensemble}
Barber, D. and Bishop, C.~M.
\newblock Ensemble learning in {Bayesian} neural networks.
\newblock \emph{Generalization in Neural Networks and Machine Learning},
  168:\penalty0 215--238, 1998.

\bibitem[Bishop(2006)]{bishop2006pattern}
Bishop, C.~M.
\newblock \emph{Pattern Recognition and Machine Learning}.
\newblock 2006.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1613--1622, 2015.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{bottou2016optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv preprint arXiv:1606.04838}, 2016.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{1606.01540}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock {OpenAI Gym}, 2016.

\bibitem[Chaudhari et~al.(2016)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2016entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J.~T., Sagun, L., and Zecchina, R.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Cochran(1977)]{cochran77}
Cochran, W.~G.
\newblock \emph{Sampling Techniques, 3rd Edition.}
\newblock John Wiley, 1977.
\newblock ISBN 0-471-16240-X.

\bibitem[Denker \& Lecun(1991)Denker and Lecun]{denker1991transforming}
Denker, J.~S. and Lecun, Y.
\newblock Transforming neural-net output levels to probability distributions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  853--859, 1991.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Fortunato et~al.(2018)Fortunato, Azar, Piot, Menick, Osband, Graves,
  Mnih, Munos, Hassabis, Pietquin, et~al.]{fortunato2017noisy}
Fortunato, M., Azar, M.~G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih,
  V., Munos, R., Hassabis, D., Pietquin, O., et~al.
\newblock Noisy networks for exploration.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Gal(2016)]{Gal2016Uncertainty}
Gal, Y.
\newblock \emph{Uncertainty in Deep Learning}.
\newblock PhD thesis, University of Cambridge, 2016.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{yarin16dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a {Bayesian} approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1050--1059, 2016.

\bibitem[{Goodfellow}(2015)]{goodfellow2015efficient}
{Goodfellow}, I.
\newblock {Efficient Per-Example Gradient Computations}.
\newblock \emph{ArXiv e-prints}, October 2015.

\bibitem[Graves(2011)]{graves2011practical}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2348--2356, 2011.

\bibitem[Hasenclever et~al.(2017)Hasenclever, Webb, Lienart, Vollmer,
  Lakshminarayanan, Blundell, and Teh]{hasenclever2017distributed}
Hasenclever, L., Webb, S., Lienart, T., Vollmer, S., Lakshminarayanan, B.,
  Blundell, C., and Teh, Y.~W.
\newblock Distributed {Bayesian} learning with stochastic natural gradient
  expectation propagation and the posterior server.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 1--37,
  2017.

\bibitem[Hazan et~al.(2016)Hazan, Levy, and Shalev-Shwartz]{hazan2016graduated}
Hazan, E., Levy, K.~Y., and Shalev-Shwartz, S.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1833--1841, 2016.

\bibitem[Hensman et~al.(2012)Hensman, Rattray, and Lawrence]{hensman2012fast}
Hensman, J., Rattray, M., and Lawrence, N.~D.
\newblock Fast variational inference in the conjugate exponential family.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2888--2896, 2012.

\bibitem[Hernandez-Lobato \& Adams(2015)Hernandez-Lobato and
  Adams]{hernandez15pbp}
Hernandez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of {B}ayesian
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1869, 2015.

\bibitem[Higgins et~al.(2016)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2016beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Annual Conference on Computational Learning Theory}, pp.\
  5--13, 1993.

\bibitem[Khan \& Lin(2017)Khan and Lin]{khan2017conjugate}
Khan, M.~E. and Lin, W.
\newblock Conjugate-computation variational inference: converting variational
  inference in non-conjugate models to inferences in conjugate models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  878--887, 2017.

\bibitem[Khan et~al.(2016)Khan, Babanezhad, Lin, Schmidt, and
  Sugiyama]{khan2016faster}
Khan, M.~E., Babanezhad, R., Lin, W., Schmidt, M., and Sugiyama, M.
\newblock Faster stochastic variational inference using proximal-gradient
  methods with general divergence functions.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, 2016.

\bibitem[{Khan} et~al.(2017){Khan}, {Lin}, {Tangkaratt}, {Liu}, and
  {Nielsen}]{2017arXiv171105560E}
{Khan}, M.~E., {Lin}, W., {Tangkaratt}, V., {Liu}, Z., and {Nielsen}, D.
\newblock {Variational Adaptive-Newton Method for Explorative Learning}.
\newblock \emph{ArXiv e-prints}, November 2017.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Leordeanu \& Hebert(2008)Leordeanu and Hebert]{leordeanu2008smoothing}
Leordeanu, M. and Hebert, M.
\newblock Smoothing-based optimization.
\newblock In \emph{Computer Vision and Pattern Recognition}, pp.\  1--8, 2008.

\bibitem[Li et~al.(2016)Li, Chen, Carlson, and Carin]{li2016preconditioned}
Li, C., Chen, C., Carlson, D.~E., and Carin, L.
\newblock Preconditioned stochastic gradient langevin dynamics for deep neural
  networks.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, pp.\  4--10,
  2016.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{DBLP:journals/corr/LillicrapHPHETS15}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{CoRR}, abs/1509.02971, 2015.

\bibitem[Louizos \& Welling(2016)Louizos and Welling]{louizos2016structured}
Louizos, C. and Welling, M.
\newblock Structured and efficient variational deep learning with matrix
  gaussian posteriors.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1708--1716, 2016.

\bibitem[MacKay(2003)]{mackay2003information}
MacKay, D.~J.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
Mandt, S., Hoffman, M.~D., and Blei, D.~M.
\newblock Stochastic gradient descent as approximate {B}ayesian inference.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 1--35,
  2017.

\bibitem[Marlin et~al.(2011)Marlin, Khan, and Murphy]{marlin2011piecewise}
Marlin, B., Khan, M., and Murphy, K.
\newblock Piecewise bounds for estimating {B}ernoulli-logistic latent
  {G}aussian models.
\newblock In \emph{International Conference on Machine Learning}, 2011.

\bibitem[Martens(2014)]{martens2014new}
Martens, J.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{arXiv preprint arXiv:1412.1193}, 2014.

\bibitem[Mobahi \& Fisher~III(2015)Mobahi and
  Fisher~III]{mobahi2015theoretical}
Mobahi, H. and Fisher~III, J.~W.
\newblock A theoretical analysis of optimization by {G}aussian continuation.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, pp.\
  1205--1211, 2015.

\bibitem[Murphy(2012)]{Murphy:2012:MLP:2380985}
Murphy, K.~P.
\newblock \emph{Machine Learning: A Probabilistic Perspective}.
\newblock The MIT Press, 2012.
\newblock ISBN 0262018020, 9780262018029.

\bibitem[Neal(1995)]{neal95}
Neal, R.~M.
\newblock \emph{{B}ayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Opper \& Archambeau(2009)Opper and Archambeau]{Opper:09}
Opper, M. and Archambeau, C.
\newblock The variational {G}aussian approximation revisited.
\newblock \emph{Neural Computation}, 21\penalty0 (3):\penalty0 786--792, 2009.

\bibitem[Plappert et~al.(2018)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  Asfour, Abbeel, and Andrychowicz]{plappert2017parameter}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  Asfour, T., Abbeel, P., and Andrychowicz, M.
\newblock Parameter space noise for exploration.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and
  Blei]{ranganath2013black}
Ranganath, R., Gerrish, S., and Blei, D.~M.
\newblock Black box variational inference.
\newblock In \emph{International conference on Artificial Intelligence and
  Statistics}, pp.\  814--822, 2014.

\bibitem[Raskutti \& Mukherjee(2015)Raskutti and
  Mukherjee]{raskutti2015information}
Raskutti, G. and Mukherjee, S.
\newblock The information geometry of mirror descent.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (3):\penalty0 1451--1457, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1278--1286, 2014.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
Ritter, H., Botev, A., and Barber, D.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Robert \& Casella(2005)Robert and Casella]{robert2005}
Robert, C.~P. and Casella, G.
\newblock \emph{Monte Carlo Statistical Methods (Springer Texts in
  Statistics)}.
\newblock Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2005.
\newblock ISBN 0387212396.

\bibitem[R{\"{u}}ckstie{\ss} et~al.(2010)R{\"{u}}ckstie{\ss}, Sehnke, Schaul,
  Wierstra, Sun, and Schmidhuber]{DBLP:journals/paladyn/RuckstiessSSWSS10}
R{\"{u}}ckstie{\ss}, T., Sehnke, F., Schaul, T., Wierstra, D., Sun, Y., and
  Schmidhuber, J.
\newblock Exploring parameter space in reinforcement learning.
\newblock \emph{Paladyn}, 1\penalty0 (1):\penalty0 14--24, 2010.
\newblock \doi{10.2478/s13230-010-0002-4}.

\bibitem[Salimans et~al.(2013)Salimans, Knowles, et~al.]{salimans2013fixed}
Salimans, T., Knowles, D.~A., et~al.
\newblock Fixed-form variational posterior approximation through stochastic
  linear regression.
\newblock \emph{Bayesian Analysis}, 8\penalty0 (4):\penalty0 837--882, 2013.

\bibitem[Saul et~al.(1996)Saul, Jaakkola, and Jordan]{saul1996mean}
Saul, L.~K., Jaakkola, T., and Jordan, M.~I.
\newblock Mean field theory for sigmoid belief networks.
\newblock \emph{Journal of Artificial Intelligence Research}, 4:\penalty0
  61--76, 1996.

\bibitem[Schraudolph(2002)]{schraudolph2002fast}
Schraudolph, N.~N.
\newblock Fast curvature matrix-vector products for second-order gradient
  descent.
\newblock \emph{Neural computation}, 14\penalty0 (7):\penalty0 1723--1738,
  2002.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{DBLP:conf/icml/SilverLHDWR14}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller,
  M.~A.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{Proceedings of the 31th International Conference on Machine
  Learning, {ICML} 2014, Beijing, China, 21-26 June 2014}, pp.\  387--395,
  2014.

\bibitem[Staines \& Barber(2013)Staines and Barber]{2012arXiv12124507S}
Staines, J. and Barber, D.
\newblock Optimization by variational bounding.
\newblock In \emph{European Symposium on Artificial Neural Networks}, 2013.

\bibitem[Sun et~al.(2017)Sun, Chen, and Carin]{sun2017learning}
Sun, S., Chen, C., and Carin, L.
\newblock Learning structured weight uncertainty in {B}ayesian neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1283--1292, 2017.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{hintonTieleman}
Tieleman, T. and Hinton, G.
\newblock {Lecture 6.5-{R}MSprop: Divide the gradient by a running average of
  its recent magnitude.}
\newblock \emph{COURSERA: Neural Networks for Machine Learning 4}, 2012.

\bibitem[Wierstra et~al.(2014)Wierstra, Schaul, Glasmachers, Sun, Peters, and
  Schmidhuber]{wierstra2008natural}
Wierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., and
  Schmidhuber, J.
\newblock Natural evolution strategies.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 949--980, 2014.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4151--4161, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and
  Grosse]{DBLP:journals/corr/abs-1712-02390}
Zhang, G., Sun, S., Duvenaud, D.~K., and Grosse, R.~B.
\newblock Noisy natural gradient as variational inference.
\newblock \emph{arXiv preprint arXiv:1712.02390}, 2018.

\bibitem[Zhou \& Hu(2014)Zhou and Hu]{zhou2014gradient}
Zhou, E. and Hu, J.
\newblock Gradient-based adaptive stochastic search for non-differentiable
  optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 59\penalty0
  (7):\penalty0 1818--1832, 2014.

\end{thebibliography}
