\begin{thebibliography}{10}

\bibitem{blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {BLIP}-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In {\em International conference on machine learning}, pages 19730--19742. PMLR, 2023.

\bibitem{flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:23716--23736, 2022.

\bibitem{llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock {\em arXiv preprint arXiv:2312.14238}, 2023.

\bibitem{shikra}
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.
\newblock Shikra: Unleashing multimodal llm's referential dialogue magic.
\newblock {\em arXiv preprint arXiv:2306.15195}, 2023.

\bibitem{qwenvl}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, AnthonyMeng Huat, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.06500}, 2023.

\bibitem{vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{zhang2024map}
Ge~Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou~Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et~al.
\newblock Map-neo: Highly capable and transparent bilingual large language model series.
\newblock {\em arXiv preprint arXiv:2405.19327}, 2024.

\bibitem{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{vary}
Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.
\newblock Vary: Scaling up the vision vocabulary for large vision-language models.
\newblock {\em arXiv preprint arXiv:2312.06109}, 2023.

\bibitem{sphinx}
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et~al.
\newblock Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.
\newblock {\em arXiv preprint arXiv:2311.07575}, 2023.

\bibitem{mof}
Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi~Ma, Yann LeCun, and Saining Xie.
\newblock Eyes wide shut? exploring the visual shortcomings of multimodal llms.
\newblock {\em arXiv preprint arXiv:2401.06209}, 2024.

\bibitem{dinov2}
Maxime Oquab, Timoth{\'e}e Darcet, Th{\'e}o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et~al.
\newblock Dinov2: Learning robust visual features without supervision.
\newblock {\em arXiv preprint arXiv:2304.07193}, 2023.

\bibitem{mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{docvqa}
Minesh Mathew, Dimosthenis Karatzas, and CV~Jawahar.
\newblock Docvqa: A dataset for vqa on document images.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pages 2200--2209, 2021.

\bibitem{chartqa}
Ahmed Masry, Do~Xuan Long, Jia~Qing Tan, Shafiq Joty, and Enamul Hoque.
\newblock Chartqa: A benchmark for question answering about charts with visual and logical reasoning.
\newblock {\em arXiv preprint arXiv:2203.10244}, 2022.

\bibitem{gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional question answering.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 6700--6709, 2019.

\bibitem{pope}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Evaluating object hallucination in large vision-language models.
\newblock {\em arXiv preprint arXiv:2305.10355}, 2023.

\bibitem{refcoco}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14}, pages 69--85. Springer, 2016.

\bibitem{slake}
Bo~Liu, Li-Ming Zhan, Li~Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.
\newblock Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering.
\newblock In {\em 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)}, pages 1650--1654. IEEE, 2021.

\bibitem{codetr}
Zhuofan Zong, Guanglu Song, and Yu~Liu.
\newblock Detrs with collaborative hybrid assignments training.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 6748--6758, 2023.

\bibitem{sam}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock {\em arXiv preprint arXiv:2304.02643}, 2023.

\bibitem{pix2struct}
Kenton Lee, Mandar Joshi, Iulia~Raluca Turc, Hexiang Hu, Fangyu Liu, Julian~Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.
\newblock Pix2struct: Screenshot parsing as pretraining for visual language understanding.
\newblock In {\em International Conference on Machine Learning}, pages 18893--18912. PMLR, 2023.

\bibitem{deplot}
Fangyu Liu, Julian~Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun.
\newblock Deplot: One-shot visual language reasoning by plot-to-table translation.
\newblock {\em arXiv preprint arXiv:2212.10505}, 2022.

\bibitem{biomedclip}
Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu~Wei, Naveen Valluri, Cliff Wong, et~al.
\newblock Large-scale domain-specific pretraining for biomedical vision-language processing.
\newblock {\em arXiv preprint arXiv:2303.00915}, 2023.

\bibitem{llava15}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2310.03744}, 2023.

\bibitem{toolllm}
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et~al.
\newblock Toolllm: Facilitating large language models to master 16000+ real-world apis.
\newblock {\em arXiv preprint arXiv:2307.16789}, 2023.

\bibitem{lmdrive}
Hao Shao, Yuxuan Hu, Letian Wang, Steven~L Waslander, Yu~Liu, and Hongsheng Li.
\newblock Lmdrive: Closed-loop end-to-end driving with large language models.
\newblock {\em arXiv preprint arXiv:2312.07488}, 2023.

\bibitem{visualcot}
Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu~Liu, and Hongsheng Li.
\newblock Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models.
\newblock {\em arXiv preprint arXiv:2403.16999}, 2024.

\bibitem{jiao2024lumen}
Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang.
\newblock Lumen: Unleashing versatile vision-centric capabilities of large multimodal models.
\newblock {\em arXiv preprint arXiv:2403.07304}, 2024.

\bibitem{ma2024exploringrolelargelanguage}
Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu~Liu.
\newblock Exploring the role of large language models in prompt encoding for diffusion models, 2024.

\bibitem{guo2023owl}
Hongcheng Guo, Jian Yang, Jiaheng Liu, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, et~al.
\newblock Owl: A large language model for it operations.
\newblock {\em arXiv preprint arXiv:2309.09298}, 2023.

\bibitem{zhou2024aligning}
Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao.
\newblock Aligning modalities in vision large language models via preference fine-tuning.
\newblock {\em arXiv preprint arXiv:2402.11411}, 2024.

\bibitem{zhou2024calibrated}
Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao.
\newblock Calibrated self-rewarding vision language models.
\newblock {\em arXiv preprint arXiv:2405.14622}, 2024.

\bibitem{jiang2024comat}
Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu~Liu, and Hongsheng Li.
\newblock Comat: Aligning text-to-image diffusion model with image-to-text concept matching.
\newblock {\em arXiv preprint arXiv:2404.03653}, 2024.

\bibitem{xia2024rule}
Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao.
\newblock Rule: Reliable multimodal rag for factuality in medical vision language models.
\newblock {\em arXiv preprint arXiv:2407.05131}, 2024.

\bibitem{xia2024mmed}
Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao.
\newblock Mmed-rag: Versatile multimodal rag system for medical vision language models.
\newblock {\em arXiv preprint arXiv:2410.13085}, 2024.

\bibitem{yang2024boosting}
Zaiquan Yang, Yuhao Liu, Jiaying Lin, Gerhard Hancke, and Rynson~WH Lau.
\newblock Boosting weakly-supervised referring image segmentation via progressive comprehension.
\newblock {\em arXiv preprint arXiv:2410.01544}, 2024.

\bibitem{yue2024mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9556--9567, 2024.

\bibitem{li2024eyes}
Yian Li, Wentao Tian, Yang Jiao, Jingjing Chen, and Yu-Gang Jiang.
\newblock Eyes can deceive: Benchmarking counterfactual reasoning abilities of multi-modal large language models.
\newblock {\em arXiv preprint arXiv:2404.12966}, 2024.

\bibitem{zhang2024eventhallusion}
Jiacheng Zhang, Yang Jiao, Shaoxiang Chen, Jingjing Chen, and Yu-Gang Jiang.
\newblock Eventhallusion: Diagnosing event hallucinations in video llms.
\newblock {\em arXiv preprint arXiv:2409.16597}, 2024.

\bibitem{mmsearch}
Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Guanglu Song, Peng Gao, et~al.
\newblock Mmsearch: Benchmarking the potential of large models as multi-modal search engines.
\newblock {\em arXiv preprint arXiv:2409.12959}, 2024.

\bibitem{xia2024cares}
Peng Xia, Ze~Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et~al.
\newblock Cares: A comprehensive benchmark of trustworthiness in medical vision language models.
\newblock {\em arXiv preprint arXiv:2406.06007}, 2024.

\bibitem{xia2024mmie}
Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, et~al.
\newblock Mmie: Massive multimodal interleaved comprehension benchmark for large vision-language models.
\newblock {\em arXiv preprint arXiv:2410.10139}, 2024.

\bibitem{wang2023rolellm}
Zekun~Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et~al.
\newblock Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models.
\newblock {\em arXiv preprint arXiv:2310.00746}, 2023.

\bibitem{laion400m}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
\newblock {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{laion5b}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:25278--25294, 2022.

\bibitem{t2ibench}
Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu.
\newblock T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{sharegpt4v}
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
\newblock Sharegpt4v: Improving large multi-modal models with better captions.
\newblock {\em arXiv preprint arXiv:2311.12793}, 2023.

\bibitem{young2024yi}
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al.
\newblock Yi: Open foundation models by 01. ai.
\newblock {\em arXiv preprint arXiv:2403.04652}, 2024.

\bibitem{gpt4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{allava}
Guiming~Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang.
\newblock Allava: Harnessing gpt4v-synthesized data for a lite vision-language model.
\newblock {\em arXiv preprint arXiv:2402.11684}, 2024.

\bibitem{objects365}
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.
\newblock Objects365: A large-scale, high-quality dataset for object detection.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 8430--8439, 2019.

\bibitem{visualgenome}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced dense image annotations.
\newblock {\em International journal of computer vision}, 123:32--73, 2017.

\bibitem{pointqa}
Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russakovsky.
\newblock Point and ask: Incorporating pointing into visual question answering.
\newblock {\em arXiv preprint arXiv:2011.13681}, 2020.

\bibitem{flickr30k}
Bryan~A Plummer, Liwei Wang, Chris~M Cervantes, Juan~C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 2641--2649, 2015.

\bibitem{mmc}
Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu.
\newblock Mmc: Advancing multimodal chart understanding with large-scale instruction tuning.
\newblock {\em arXiv preprint arXiv:2311.10774}, 2023.

\bibitem{chart2text}
Shankar Kantharaj, Rixie~Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty.
\newblock Chart-to-text: A large-scale benchmark for chart summarization.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 4005--4023, 2022.

\bibitem{dvqa}
Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan.
\newblock Dvqa: Understanding data visualizations via question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5648--5656, 2018.

\bibitem{scigraphqa}
Shengzhi Li and Nima Tajbakhsh.
\newblock Scigraphqa: A large-scale synthetic multi-turn question-answering dataset for scientific graphs.
\newblock {\em arXiv preprint arXiv:2308.03349}, 2023.

\bibitem{llavar}
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.
\newblock Llavar: Enhanced visual instruction tuning for text-rich image understanding.
\newblock {\em arXiv preprint arXiv:2306.17107}, 2023.

\bibitem{llava_med}
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.
\newblock Llava-med: Training a large language-and-vision assistant for biomedicine in one day.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{infographicvqa}
Minesh Mathew, Viraj Bagal, Rub{\`e}n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV~Jawahar.
\newblock Infographicvqa.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 1697--1706, 2022.

\bibitem{ai2d}
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock A diagram is worth a dozen images.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14}, pages 235--251. Springer, 2016.

\bibitem{stvqa}
Ali~Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar{\c{c}}al Rusinol, Ernest Valveny, CV~Jawahar, and Dimosthenis Karatzas.
\newblock Scene text visual question answering.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 4291--4301, 2019.

\bibitem{textvqa}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem{donut}
Geewook Kim, Teakgyu Hong, Moonbin Yim, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
\newblock Donut: Document understanding transformer without ocr.
\newblock {\em arXiv preprint arXiv:2111.15664}, 7:15, 2021.

\bibitem{geo3k}
Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu.
\newblock Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning.
\newblock {\em arXiv preprint arXiv:2105.04165}, 2021.

\bibitem{pgps9k}
Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu.
\newblock A multi-modal neural geometric solver with textual clauses parsed from diagram.
\newblock {\em arXiv preprint arXiv:2302.11097}, 2023.

\bibitem{gllava}
Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et~al.
\newblock G-llava: Solving geometric problem with multi-modal large language model.
\newblock {\em arXiv preprint arXiv:2312.11370}, 2023.

\bibitem{vqa_rad}
Jason~J Lau, Soumya Gayen, Asma Ben~Abacha, and Dina Demner-Fushman.
\newblock A dataset of clinically generated visual questions and answers about radiology images.
\newblock {\em Scientific data}, 5(1):1--10, 2018.

\bibitem{laion4v}
LAION.
\newblock Gpt-4v dataset.
\newblock \url{https://huggingface.co/datasets/laion/gpt4v-dataset}, 2023.

\bibitem{textocr4v}
Carter Jimmy.
\newblock Textocr-gpt4v.
\newblock \url{https://huggingface.co/datasets/jimmycarter/textocr-gpt4v}, 2024.

\bibitem{gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{mplugowl2}
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi~Qian, Ji~Zhang, Fei Huang, and Jingren Zhou.
\newblock mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.
\newblock {\em arXiv preprint arXiv:2311.04257}, 2023.

\bibitem{llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.

\bibitem{mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, et~al.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{qbench}
Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et~al.
\newblock Q-bench: A benchmark for general-purpose foundation models on low-level vision.
\newblock {\em arXiv preprint arXiv:2309.14181}, 2023.

\bibitem{mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.
\newblock {\em arXiv preprint arXiv:2310.02255}, 2023.

\bibitem{mathverse}
Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et~al.
\newblock Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?
\newblock {\em arXiv preprint arXiv:2403.14624}, 2024.

\bibitem{yi}
01-AI.
\newblock Yi.
\newblock \url{https://huggingface.co/01-ai}, 2023.

\bibitem{cogagent}
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et~al.
\newblock Cogagent: A visual language model for gui agents.
\newblock {\em arXiv preprint arXiv:2312.08914}, 2023.

\bibitem{palix}
Xi~Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos~Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi~Tay, et~al.
\newblock Pali-x: On scaling up a multilingual vision and language model.
\newblock {\em arXiv preprint arXiv:2305.18565}, 2023.

\bibitem{vqav2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6904--6913, 2017.

\bibitem{scienceqa}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock In {\em The 36th Conference on Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem{ferret}
Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang.
\newblock Ferret: Refer and ground anything anywhere at any granularity.
\newblock {\em arXiv preprint arXiv:2310.07704}, 2023.

\bibitem{groundingdino}
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et~al.
\newblock Grounding dino: Marrying dino with grounded pre-training for open-set object detection.
\newblock {\em arXiv preprint arXiv:2303.05499}, 2023.

\bibitem{uninext}
Bin Yan, Yi~Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu.
\newblock Universal instance perception as object discovery and retrieval.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15325--15336, 2023.

\bibitem{hop}
Zhuofan Zong, Dongzhi Jiang, Guanglu Song, Zeyue Xue, Jingyong Su, Hongsheng Li, and Yu~Liu.
\newblock Temporal enhanced training of multi-view 3d object detector via historical object prediction.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 3781--3790, 2023.

\bibitem{dino_det}
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel~M Ni, and Heung-Yeung Shum.
\newblock Dino: Detr with improved denoising anchor boxes for end-to-end object detection.
\newblock {\em arXiv preprint arXiv:2203.03605}, 2022.

\bibitem{layoutlmv3}
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei.
\newblock Layoutlmv3: Pre-training for document ai with unified text and image masking.
\newblock In {\em Proceedings of the 30th ACM International Conference on Multimedia}, pages 4083--4091, 2022.

\bibitem{sit}
Zhuofan Zong, Kunchang Li, Guanglu Song, Yali Wang, Yu~Qiao, Biao Leng, and Yu~Liu.
\newblock Self-slimmed vision transformer.
\newblock In {\em European Conference on Computer Vision}, pages 432--448. Springer, 2022.

\bibitem{lisa}
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.
\newblock Lisa: Reasoning segmentation via large language model.
\newblock {\em arXiv preprint arXiv:2308.00692}, 2023.

\bibitem{visionllm}
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu~Qiao, et~al.
\newblock Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}
