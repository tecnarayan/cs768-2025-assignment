\begin{thebibliography}{}

\bibitem[Abbasi-Yadkori et~al., 2011]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C. (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock {\em Advances in neural information processing systems}, 24.

\bibitem[Abbasi-Yadkori and Szepesv{\'a}ri, 2011]{abbasi2011regret}
Abbasi-Yadkori, Y. and Szepesv{\'a}ri, C. (2011).
\newblock Regret bounds for the adaptive control of linear quadratic systems.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning Theory}, pages 1--26. JMLR Workshop and Conference Proceedings.

\bibitem[Asadi et~al., 2018]{asadi2018lipschitz}
Asadi, K., Misra, D., and Littman, M. (2018).
\newblock Lipschitz continuity in model-based reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 264--273. PMLR.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 263--272. PMLR.

\bibitem[Bemporad et~al., 2002]{bemporad2002explicit}
Bemporad, A., Morari, M., Dua, V., and Pistikopoulos, E.~N. (2002).
\newblock The explicit linear quadratic regulator for constrained systems.
\newblock {\em Automatica}, 38(1):3--20.

\bibitem[Bertsekas and Shreve, 1996]{bertsekas1996stochastic}
Bertsekas, D. and Shreve, S.~E. (1996).
\newblock {\em Stochastic optimal control: the discrete-time case}, volume~5.
\newblock Athena Scientific.

\bibitem[Brockman et~al., 2016]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016).
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}.

\bibitem[Chowdhury and Gopalan, 2019]{chowdhury2019online}
Chowdhury, S.~R. and Gopalan, A. (2019).
\newblock Online learning in kernelized markov decision processes.
\newblock In {\em The 22nd International Conference on Artificial Intelligence and Statistics}, pages 3197--3205. PMLR.

\bibitem[Cohen et~al., 2019]{cohen2019learning}
Cohen, A., Koren, T., and Mansour, Y. (2019).
\newblock Learning linear-quadratic regulators efficiently with only $\sqrt t$ regret.
\newblock In {\em International Conference on Machine Learning}, pages 1300--1309. PMLR.

\bibitem[Da~Costa et~al., 2023]{da2023sample}
Da~Costa, N., Pf{\"o}rtner, M., Da~Costa, L., and Hennig, P. (2023).
\newblock Sample path regularity of gaussian processes from the covariance kernel.
\newblock {\em arXiv preprint arXiv:2312.14886}.

\bibitem[Damiani et~al., 2022]{damiani2022balancing}
Damiani, A., Manganini, G., Metelli, A.~M., and Restelli, M. (2022).
\newblock Balancing sample efficiency and suboptimality in inverse reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 4618--4629. PMLR.

\bibitem[Dean et~al., 2018]{dean2018regret}
Dean, S., Mania, H., Matni, N., Recht, B., and Tu, S. (2018).
\newblock Regret bounds for robust adaptive control of the linear quadratic regulator.
\newblock {\em Advances in Neural Information Processing Systems}, 31.

\bibitem[Domingues et~al., 2021]{domingues2021kernel}
Domingues, O.~D., M{\'e}nard, P., Pirotta, M., Kaufmann, E., and Valko, M. (2021).
\newblock Kernel-based reinforcement learning: A finite-time analysis.
\newblock In {\em International Conference on Machine Learning}, pages 2783--2792. PMLR.

\bibitem[Folland, 1999]{folland1999real}
Folland, G.~B. (1999).
\newblock {\em Real analysis: modern techniques and their applications}, volume~40.
\newblock John Wiley \& Sons.

\bibitem[Hambly et~al., 2023]{hambly2023recent}
Hambly, B., Xu, R., and Yang, H. (2023).
\newblock Recent advances in reinforcement learning in finance.
\newblock {\em Mathematical Finance}, 33(3):437--503.

\bibitem[Janz et~al., 2020]{janz2020bandit}
Janz, D., Burt, D., and Gonz{\'a}lez, J. (2020).
\newblock Bandit optimisation of functions in the mat{\'e}rn kernel rkhs.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 2486--2495. PMLR.

\bibitem[Jin et~al., 2021]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021).
\newblock Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.
\newblock {\em Advances in neural information processing systems}, 34:13406--13418.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Kiran et~al., 2021]{kiran2021deep}
Kiran, B.~R., Sobh, I., Talpaert, V., Mannion, P., Al~Sallab, A.~A., Yogamani, S., and P{\'e}rez, P. (2021).
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems}, 23(6):4909--4926.

\bibitem[Kober et~al., 2013]{kober2013reinforcement}
Kober, J., Bagnell, J.~A., and Peters, J. (2013).
\newblock Reinforcement learning in robotics: A survey.
\newblock {\em The International Journal of Robotics Research}, 32(11):1238--1274.

\bibitem[Le~Lan et~al., 2021]{le2021metrics}
Le~Lan, C., Bellemare, M.~G., and Castro, P.~S. (2021).
\newblock Metrics and continuity in reinforcement learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 8261--8269.

\bibitem[Liotet et~al., 2022]{liotet2022delayed}
Liotet, P., Maran, D., Bisi, L., and Restelli, M. (2022).
\newblock Delayed reinforcement learning by imitation.
\newblock In {\em International Conference on Machine Learning}, pages 13528--13556. PMLR.

\bibitem[Liu et~al., 2021]{liu2021smooth}
Liu, Y., Wang, Y., and Singh, A. (2021).
\newblock Smooth bandit optimization: generalization to holder space.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 2206--2214. PMLR.

\bibitem[Maran et~al., 2024a]{maran2024projection}
Maran, D., Metelli, A.~M., Papini, M., and Restell, M. (2024a).
\newblock Projection by convolution: Optimal sample complexity for reinforcement learning in continuous-space mdps.
\newblock In {\em Proceedings of the 37th Annual Conference on Learning Theory}, pages 3743--3774. PMLR.

\bibitem[Maran et~al., 2024b]{maran2024no}
Maran, D., Metelli, A.~M., Papini, M., and Restelli, M. (2024b).
\newblock No-regret reinforcement learning in smooth mdps.
\newblock In {\em Forty-first International Conference on Machine Learning}, pages 34760--34789. PMLR.

\bibitem[Maran et~al., 2023]{maran2023tight}
Maran, D., Metelli, A.~M., and Restelli, M. (2023).
\newblock Tight performance guarantees of imitator policies with continuous actions.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 9073--9080.

\bibitem[Metelli, 2022]{metelli2022exploiting}
Metelli, A.~M. (2022).
\newblock {\em Exploiting environment configurability in reinforcement learning}, volume 361.
\newblock IOS Press.

\bibitem[Metelli et~al., 2020]{metelli2020control}
Metelli, A.~M., Mazzolini, F., Bisi, L., Sabbioni, L., and Restelli, M. (2020).
\newblock Control frequency adaptation via action persistence in batch reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 6862--6873. PMLR.

\bibitem[Pirotta et~al., 2015]{pirotta2015policy}
Pirotta, M., Restelli, M., and Bascetta, L. (2015).
\newblock Policy gradient in lipschitz markov decision processes.
\newblock {\em Machine Learning}, 100:255--283.

\bibitem[Puterman, 2014]{puterman2014markov}
Puterman, M.~L. (2014).
\newblock {\em Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons.

\bibitem[Rachelson and Lagoudakis, 2010]{rachelson2010locality}
Rachelson, E. and Lagoudakis, M.~G. (2010).
\newblock On the locality of action domination in sequential decision making.
\newblock {\em International Symposium on Artificial Intelligence and Mathematics}.

\bibitem[Sinclair et~al., 2020]{sinclair2020adaptive}
Sinclair, S., Wang, T., Jain, G., Banerjee, S., and Yu, C. (2020).
\newblock Adaptive discretization for model-based reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:3858--3871.

\bibitem[Sinclair et~al., 2019]{sinclair2019adaptive}
Sinclair, S.~R., Banerjee, S., and Yu, C.~L. (2019).
\newblock Adaptive discretization for episodic reinforcement learning in metric spaces.
\newblock {\em Proceedings of the ACM on Measurement and Analysis of Computing Systems}, 3(3):1--44.

\bibitem[Song and Sun, 2019]{song2019efficient}
Song, Z. and Sun, W. (2019).
\newblock Efficient model-free reinforcement learning in metric spaces.
\newblock {\em arXiv preprint arXiv:1905.00475}.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Todorov et~al., 2012]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y. (2012).
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ international conference on intelligent robots and systems}, pages 5026--5033. IEEE.

\bibitem[Vakili et~al., 2021]{vakili2021information}
Vakili, S., Khezeli, K., and Picheny, V. (2021).
\newblock On information gain and regret bounds in gaussian process bandits.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 82--90. PMLR.

\bibitem[Vakili and Olkhovskaya, 2024a]{vakili2024kernelized}
Vakili, S. and Olkhovskaya, J. (2024a).
\newblock Kernelized reinforcement learning with order optimal regret bounds.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Vakili and Olkhovskaya, 2024b]{vakili2024arxiv}
Vakili, S. and Olkhovskaya, J. (2024b).
\newblock Kernelized reinforcement learning with order optimal regret bounds.
\newblock {\em arXiv preprint arXiv:2306.07745}.

\bibitem[Yang and Wang, 2019]{yang2019sample}
Yang, L. and Wang, M. (2019).
\newblock Sample-optimal parametric q-learning using linearly additive features.
\newblock In {\em {ICML}}, volume~97 of {\em Proceedings of Machine Learning Research}, pages 6995--7004. {PMLR}.

\bibitem[Yang et~al., 2020]{yang2020provably}
Yang, Z., Jin, C., Wang, Z., Wang, M., and Jordan, M. (2020).
\newblock Provably efficient reinforcement learning with kernel and neural function approximations.
\newblock {\em Advances in Neural Information Processing Systems}, 33:13903--13916.

\bibitem[Zanette et~al., 2020]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020).
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In {\em International Conference on Machine Learning}, pages 10978--10989. PMLR.

\bibitem[Zhang et~al., 2021]{zhang2021reinforcement}
Zhang, Z., Ji, X., and Du, S.~S. (2021).
\newblock Is reinforcement learning more difficult than bandits? {A} near-optimal algorithm escaping the curse of horizon.
\newblock In {\em {COLT}}, volume 134 of {\em Proceedings of Machine Learning Research}, pages 4528--4531. {PMLR}.

\end{thebibliography}
