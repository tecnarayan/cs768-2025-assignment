@article{babakniya2023slora,
  title={{SLoRA}: Federated parameter efficient fine-tuning of language models},
  author={Babakniya, Sara and Elkordy, Ahmed Roushdy and Ezzeldin, Yahya H and Liu, Qingfeng and Song, Kee-Bong and El-Khamy, Mostafa and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2308.06522},
  year={2023}
}

@inproceedings{hu2022lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}},
  year         = {2022}
}


@article{malladi2023mezo,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}


@inproceedings{li2021communication,
  author       = {Zan Li and
                  Li Chen},
  title        = {Communication-Efficient Decentralized Zeroth-order Method on Heterogeneous
                  Data},
  booktitle    = {International Conference on Wireless Communications and Signal
                  Processing, {WCSP}},
  pages        = {1--6},
  year         = {2021}
}


@article{fang2022communication,
  title={Communication-efficient stochastic zeroth-order optimization for federated learning},
  author={Fang, Wenzhi and Yu, Ziyi and Jiang, Yuning and Shi, Yuanming and Jones, Colin N and Zhou, Yong},
  journal={IEEE Transactions on Signal Processing},
  volume={70},
  pages={5058--5073},
  year={2022}
}


@inproceedings{naturalinstructions,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle={ACL},
  year={2022}
}

@inproceedings{supernaturalinstructions,
  author       = {Yizhong Wang and
                  Swaroop Mishra and
                  Pegah Alipoormolabashi and
                  Yeganeh Kordi and
                  Amirreza Mirzaei and
                  Atharva Naik and
                  Arjun Ashok and
                  Arut Selvan Dhanasekaran and
                  Anjana Arunkumar and
                  David Stap and
                  Eshaan Pathak and
                  Giannis Karamanolakis and
                  Haizhi Gary Lai and
                  Ishan Purohit and
                  Ishani Mondal and
                  Jacob Anderson and
                  Kirby Kuznia and
                  Krima Doshi and
                  Kuntal Kumar Pal and
                  Maitreya Patel and
                  Mehrad Moradshahi and
                  Mihir Parmar and
                  Mirali Purohit and
                  Neeraj Varshney and
                  Phani Rohitha Kaza and
                  Pulkit Verma and
                  Ravsehaj Singh Puri and
                  Rushang Karia and
                  Savan Doshi and
                  Shailaja Keyur Sampat and
                  Siddhartha Mishra and
                  Sujan Reddy A and
                  Sumanta Patro and
                  Tanay Dixit and
                  Xudong Shen},
  title        = {Super-NaturalInstructions: Generalization via Declarative Instructions
                  on 1600+ {NLP} Tasks},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {5085--5109},
  publisher    = {Association for Computational Linguistics},
  year         = {2022}
}


@inproceedings{wei2022FLAN,
  author       = {Jason Wei and
                  Maarten Bosma and
                  Vincent Y. Zhao and
                  Kelvin Guu and
                  Adams Wei Yu and
                  Brian Lester and
                  Nan Du and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {Finetuned Language Models are Zero-Shot Learners},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}},
  year         = {2022}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{touvron2023llama,
  title={{LLaMA}: Open and efficient foundation language models},
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@inproceedings{chen2023datajuicer,
  title={Data-Juicer: {A} One-Stop Data Processing System for Large Language Models},
  author       = {Daoyuan Chen and
                  Yilun Huang and
                  Zhijian Ma and
                  Hesen Chen and
                  Xuchen Pan and
                  Ce Ge and
                  Dawei Gao and
                  Yuexiang Xie and
                  Zhaoyang Liu and
                  Jinyang Gao and
                  Yaliang Li and
                  Bolin Ding and
                  Jingren Zhou},
  booktitle={International Conference on Management of Data},
  year={2024}
}


@inproceedings{zhang2023fedit,
  title={Towards building the federatedGPT: Federated instruction tuning},
  author={Zhang, Jianyi and Vahidian, Saeed and Kuo, Martin and Li, Chunyuan and Zhang, Ruiyi and Yu, Tong and Wang, Guoyin and Chen, Yiran},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6915--6919},
  year={2024},
  organization={IEEE}
}


@misc{DatabricksBlog2023DollyV2,
  author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
  title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned {LLM}},
  year      = {2023},
  url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
  urldate   = {2023-06-30}
}


@article{zelikman2023onebyte,
  author       = {Eric Zelikman and
                  Qian Huang and
                  Percy Liang and
                  Nick Haber and
                  Noah D. Goodman},
  title        = {Just One Byte (per gradient): {A} Note on Low-Bandwidth Decentralized
                  Language Model Finetuning Using Shared Randomness},
  journal      = {CoRR},
  volume       = {abs/2306.10015},
  year         = {2023}
}


@article{feng2023baffle,
  title={Does Federated Learning Really Need Backpropagation?},
  author={Feng, Haozhe and Pang, Tianyu and Du, Chao and Chen, Wei and Yan, Shuicheng and Lin, Min},
  journal={arXiv preprint arXiv:2301.12195},
  year={2023}
}

@article{shu2023Trajectory,
  title={Federated Zeroth-Order Optimization using Trajectory-Informed Surrogate Gradients},
  author={Shu, Yao and Lin, Xiaoqiang and Dai, Zhongxiang and Low, Bryan Kian Hsiang},
  journal={arXiv preprint arXiv:2308.04077},
  year={2023}
}


@article{liu2018zeroth,
  title={Zeroth-order stochastic variance reduction for nonconvex optimization},
  author={Liu, Sijia and Kailkhura, Bhavya and Chen, Pin-Yu and Ting, Paishun and Chang, Shiyu and Amini, Lisa},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@article{kuang2023federatedscope-LLM,
  title={{FederatedScope}-{LLM}: A comprehensive package for fine-tuning large language models in federated learning},
  author={Kuang, Weirui and Qian, Bingchen and Li, Zitao and Chen, Daoyuan and Gao, Dawei and Pan, Xuchen and Xie, Yuexiang and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  journal={arXiv preprint arXiv:2309.00363},
  year={2023}
}


@article{xu2023billion-sized,
  title={{FwdLLM}: Efficient {FedLLM} using Forward Gradient},
  author={Xu, Mengwei and Wu, Yaozong and Cai, Dongqi and Li, Xiang and Wang, Shangguang},
  journal={arXiv preprint arXiv:2308.13894},
  year={2023}
}



@inproceedings{zhang2023-FedPETuning,
  author       = {Zhuo Zhang and
                  Yuanhang Yang and
                  Yong Dai and
                  Qifan Wang and
                  Yue Yu and
                  Lizhen Qu and
                  Zenglin Xu},
  title        = {{FedPETuning}: When Federated Learning Meets the Parameter-Efficient
                  Tuning Methods of Pre-trained Language Models},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL}},
  pages        = {9963--9977},
  year         = {2023}
}


@article{fan2023-FATE-LLM,
  author       = {Tao Fan and
                  Yan Kang and
                  Guoqiang Ma and
                  Weijing Chen and
                  Wenbin Wei and
                  Lixin Fan and
                  Qiang Yang},
  title        = {{FATE-LLM:} {A} Industrial Grade Federated Learning Framework for
                  Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2310.10049},
  year         = {2023},
}


@article{maritan2023fedzen,
  author       = {Alessio Maritan and
                  Subhrakanti Dey and
                  Luca Schenato},
  title        = {{FedZeN}: Towards superlinear zeroth-order federated learning via incremental
                  Hessian estimation},
  journal      = {CoRR},
  volume       = {abs/2309.17174},
  year         = {2023}
}

@article{rahimi2023evofed,
  title={{EvoFed}: Leveraging Evolutionary Strategies for Communication-Efficient Federated Learning},
  author={Rahimi, Mohammad Mahdi and Bhatti, Hasnain Irshad and Park, Younghyun and Kousar, Humaira and Kim, Do-Yeon and Moon, Jaekyun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{spall1992multivariate,
  title={Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
  author={Spall, James C},
  journal={IEEE transactions on automatic control},
  volume={37},
  number={3},
  pages={332--341},
  year={1992}
}


@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}


@inproceedings{dettmers2023qlora,
  author       = {Tim Dettmers and
                  Artidoro Pagnoni and
                  Ari Holtzman and
                  Luke Zettlemoyer},
  title        = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  booktitle    = {Advances in Neural Information Processing Systems},
  year         = {2023}
}


@inproceedings{chen2023fine,
  title={Fine-Grained Theoretical Analysis of Federated Zeroth-Order Optimization},
  author={Chen, Jun and Chen, Hong and Gu, Bin and Deng, Hao},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@article{liu2023gpt,
  title={{GPT} understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  year={2023},
  publisher={Elsevier}
}



@inproceedings{lester2021power,
  author       = {Brian Lester and
                  Rami Al{-}Rfou and
                  Noah Constant},
  title        = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  booktitle    = {Proceedings of the 2021 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {3045--3059},
  year         = {2021}
}


@inproceedings{adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  title        = {Adam: {A} Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR}},
  year         = {2015}
}


@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford {Alpaca}: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}


@article{paszke2019pytorch,
  title={{PyTorch}: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@Misc{peft,
  title =        {{PEFT}: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author =       {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year =         {2022}
}

@inproceedings{wolf-etal-2020-transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{tan2022towards,
  title={Towards personalized federated learning},
  author={Tan, Alysa Ziying and Yu, Han and Cui, Lizhen and Yang, Qiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{bai2024federated,
      title={Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources}, 
      author={Jiamu Bai and Daoyuan Chen and Bingchen Qian and Liuyi Yao and Yaliang Li},
  journal={arXiv preprint arXiv:2402.11505},
  year={2024}
}

@article{ling2024convergence,
      title={On the Convergence of Zeroth-Order Federated Tuning for Large Language Models}, 
      author={Zhenqing Ling and Daoyuan Chen and Liuyi Yao and Yaliang Li and Ying Shen},
  journal={arXiv preprint arXiv:2402.05926},
  year={2024}
}


@article{woisetschlager2023very-edge,
  title={Federated Fine-Tuning of {LLMs} on the Very Edge: The Good, the Bad, the Ugly},
  author={Woisetschl{\"a}ger, Herbert and Isenko, Alexander and Wang, Shiqiang and Mayer, Ruben and Jacobsen, Hans-Arno},
  journal={arXiv preprint arXiv:2310.03150},
  year={2023}
}


@article{jiang2023low-parameter,
  title={Low-Parameter Federated Learning with Large Language Models},
  author={Jiang, Jingang and Liu, Xiangyang and Fan, Chenyou},
  journal={arXiv preprint arXiv:2307.13896},
  year={2023}
}


@article{chen2023position,
  title={Federated large language model: A position paper},
  author={Chen, Chaochao and Feng, Xiaohua and Zhou, Jun and Yin, Jianwei and Zheng, Xiaolin},
  journal={arXiv preprint arXiv:2307.08925},
  year={2023}
}


@inproceedings{dong2023towards,
  title={Towards Next-Generation Intelligent Assistants Leveraging {LLM} Techniques},
  author={Dong, Xin Luna and Moon, Seungwhan and Xu, Yifan Ethan and Malik, Kshitiz and Yu, Zhou},
  booktitle={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5792--5793},
  year={2023}
}

@inproceedings{qin2024blockdfl,
  title={{BlockDFL}: A Blockchain-based Fully Decentralized Peer-to-Peer Federated Learning Framework},
  author={Qin, Zhen and Yan, Xueqiang and Zhou, Mengchu and Deng, Shuiguang},
  booktitle={Proceedings of the ACM on Web Conference 2024},
  pages={2914--2925},
  year={2024}
}

@inproceedings{dorfman2023docofl,
  title={{DoCoFL}: downlink compression for cross-device federated learning},
  author={Dorfman, Ron and Vargaftik, Shay and Ben-Itzhak, Yaniv and Levy, Kfir Yehuda},
  booktitle={International Conference on Machine Learning},
  pages={8356--8388},
  year={2023},
  organization={PMLR}
}


@article{pu2023empirical,
  title={Empirical analysis of the strengths and weaknesses of peft techniques for llms},
  author={Pu, George and Jain, Anirudh and Yin, Jihan and Kaplan, Russell},
  journal={arXiv preprint arXiv:2304.14999},
  year={2023}
}

@inproceedings{chen2022revisiting,
  author       = {Guanzheng Chen and
                  Fangyu Liu and
                  Zaiqiao Meng and
                  Shangsong Liang},
  title        = {Revisiting Parameter-Efficient Tuning: Are We Really There Yet?},
  booktitle    = {Proceedings of the 2022 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP}},
  pages        = {2612--2626},
  year         = {2022}
}

@inproceedings{chen2023fsreal,
  author = {Chen, Daoyuan and Gao, Dawei and Xie, Yuexiang and Pan, Xuchen and Li, Zitao and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  title = {{FS-REAL}: Towards Real-World Cross-Device Federated Learning},
  year = {2023},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages = {3829–3841},
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}


@article{sun2023comparative,
  title={A Comparative Study between Full-Parameter and {LoRA}-based Fine-Tuning on Chinese Instruction Data for Instruction Following Large Language Model},
  author={Sun, Xianghui and Ji, Yunjie and Ma, Baochang and Li, Xiangang},
  journal={arXiv preprint arXiv:2304.08109},
  year={2023}
}

@inproceedings{che2023FedPepTAO,
  title={Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization},
  author={Che, Tianshi and Liu, Ji and Zhou, Yang and Ren, Jiaxiang and Zhou, Jiwen and Sheng, Victor and Dai, Huaiyu and Dou, Dejing},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={7871--7888},
  year={2023}
}


@inproceedings{chen2023efficient,
  author       = {Daoyuan Chen and
                  Liuyi Yao and
                  Dawei Gao and
                  Bolin Ding and
                  Yaliang Li},
  title        = {Efficient Personalized Federated Learning via Sparse Model-Adaptation},
  booktitle    = {International Conference on Machine Learning, {ICML}},
  volume       = {202},
  pages        = {5234--5256},
  year         = {2023}
}


@inproceedings{li2018measuring-intrinsic,
  author       = {Chunyuan Li and
                  Heerad Farkhoor and
                  Rosanne Liu and
                  Jason Yosinski},
  title        = {Measuring the Intrinsic Dimension of Objective Landscapes},
  booktitle    = {International Conference on Learning Representations, {ICLR}},
  year         = {2018}
}


@inproceedings{aghajanyan2021intrinsic,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={7319--7328},
  year={2021}
}


@article{Kairouz2021federated,
  author       = {Peter Kairouz and
                  H. Brendan McMahan and
                  Brendan Avent and
                  Aur{\'{e}}lien Bellet and
                  Mehdi Bennis and
                  Arjun Nitin Bhagoji and
                  Kallista A. Bonawitz and
                  Zachary Charles and
                  Graham Cormode and
                  Rachel Cummings and
                  Rafael G. L. D'Oliveira and
                  Hubert Eichner and
                  Salim El Rouayheb and
                  David Evans and
                  Josh Gardner and
                  Zachary Garrett and
                  Adri{\`{a}} Gasc{\'{o}}n and
                  Badih Ghazi and
                  Phillip B. Gibbons and
                  Marco Gruteser and
                  Za{\"{\i}}d Harchaoui and
                  Chaoyang He and
                  Lie He and
                  Zhouyuan Huo and
                  Ben Hutchinson and
                  Justin Hsu and
                  Martin Jaggi and
                  Tara Javidi and
                  Gauri Joshi and
                  Mikhail Khodak and
                  Jakub Kone{\v{c}}n{\'y} and
                  Aleksandra Korolova and
                  Farinaz Koushanfar and
                  Sanmi Koyejo and
                  Tancr{\`{e}}de Lepoint and
                  Yang Liu and
                  Prateek Mittal and
                  Mehryar Mohri and
                  Richard Nock and
                  Ayfer {\"{O}}zg{\"{u}}r and
                  Rasmus Pagh and
                  Hang Qi and
                  Daniel Ramage and
                  Ramesh Raskar and
                  Mariana Raykova and
                  Dawn Song and
                  Weikang Song and
                  Sebastian U. Stich and
                  Ziteng Sun and
                  Ananda Theertha Suresh and
                  Florian Tram{\`{e}}r and
                  Praneeth Vepakomma and
                  Jianyu Wang and
                  Li Xiong and
                  Zheng Xu and
                  Qiang Yang and
                  Felix X. Yu and
                  Han Yu and
                  Sen Zhao},
  title        = {Advances and Open Problems in Federated Learning},
  journal      = {Foundations and Trends{\textregistered} in Machine Learning},
  volume       = {14},
  number       = {1-2},
  pages        = {1--210},
  year         = {2021}
}



@article{xi2023-4bit,
  title={Training transformers with 4-bit integers},
  author={Xi, Haocheng and Li, Changhao and Chen, Jianfei and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={49146--49168},
  year={2023}
}


@article{villalobos2022will,
  title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  journal={arXiv preprint arXiv:2211.04325},
  year={2022}
}


@inproceedings{li2022federated,
  title={Federated learning on non-{IID} data silos: An experimental study},
  author={Li, Qinbin and Diao, Yiqun and Chen, Quan and He, Bingsheng},
  booktitle={2022 IEEE 38th International Conference on Data Engineering, {ICDE}},
  pages={965--978},
  year={2022},
  organization={IEEE}
}


@article{melas2021intrinisic,
  title={Intrinisic Gradient Compression for Federated Learning},
  author={Melas-Kyriazi, Luke and Wang, Franklyn},
  journal={arXiv preprint arXiv:2112.02656},
  year={2021}
}


@inproceedings{Rothchild2020FetchSGD,
  title={{FetchSGD}: Communication-efficient federated learning with sketching},
  author={Rothchild, Daniel and Panda, Ashwinee and Ullah, Enayat and Ivkin, Nikita and Stoica, Ion and Braverman, Vladimir and Gonzalez, Joseph and Arora, Raman},
  booktitle={International Conference on Machine Learning},
  pages={8253--8265},
  year={2020},
  organization={PMLR}
}


@inproceedings{borzunov2023distributed,
  title={Distributed Inference and Fine-tuning of Large Language Models Over The Internet},
  author={Borzunov, Alexander and Ryabinin, Max and Chumachenko, Artem and Baranchuk, Dmitry and Dettmers, Tim and Belkada, Younes and Samygin, Pavel and Raffel, Colin},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}



@article{shayan2020biscotti,
  title={Biscotti: A blockchain system for private and secure federated learning},
  author={Shayan, Muhammad and Fung, Clement and Yoon, Chris JM and Beschastnikh, Ivan},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={32},
  number={7},
  pages={1513--1525},
  year={2020},
  publisher={IEEE}
}


@inproceedings{huang2016snapshot,
  title={Snapshot Ensembles: Train 1, Get {M} for Free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  booktitle={International Conference on Learning Representations},
  year={2016}
}


@inproceedings{li2020convergence,
  author       = {Xiang Li and
                  Kaixuan Huang and
                  Wenhao Yang and
                  Shusen Wang and
                  Zhihua Zhang},
  title        = {On the Convergence of FedAvg on Non-IID Data},
  booktitle    = {International Conference on Learning Representations, {ICLR}},
  year         = {2020}
}



@inproceedings{li2020fedprox,
  author       = {Tian Li and
                  Anit Kumar Sahu and
                  Manzil Zaheer and
                  Maziar Sanjabi and
                  Ameet Talwalkar and
                  Virginia Smith},
  title        = {Federated Optimization in Heterogeneous Networks},
  booktitle    = {Proceedings of Machine Learning and Systems 2020, MLSys 2020},
  year         = {2020}
}