\begin{thebibliography}{}

\bibitem[Achiam et~al., 2017]{achiam17a}
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017).
\newblock Constrained policy optimization.
\newblock In {\em ICML}.

\bibitem[Amos and Kolter, 2017]{Amos2017}
Amos, B. and Kolter, J.~Z. (2017).
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In {\em ICML}.

\bibitem[As et~al., 2022]{as2022constrained}
As, Y., Usmanova, I., Curi, S., and Krause, A. (2022).
\newblock Constrained policy optimization via bayesian world models.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Berkenkamp et~al., 2017]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A.~P., and Krause, A. (2017).
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In {\em NeurIPS}.

\bibitem[Berner et~al., 2019]{OpenAI2019Five}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., J{\'{o}}zefowicz, R., Gray,
  S., Olsson, C., Pachocki, J., Petrov, M., de~Oliveira~Pinto, H.~P., Raiman,
  J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I.,
  Tang, J., Wolski, F., and Zhang, S. (2019).
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock {\em arXiv}.

\bibitem[Bertsekas, 1999]{Bertsekas1999}
Bertsekas, D. (1999).
\newblock {\em Nonlinear Programming}.
\newblock Athena Scientific.

\bibitem[Bhatnagar and Lakshmanan, 2012]{Bhatnagar2012}
Bhatnagar, S. and Lakshmanan, K. (2012).
\newblock An online actor–critic algorithm with function approximation for
  constrained markov decision processes.
\newblock {\em Journal of Optimization Theory and Applications}, 153.

\bibitem[Bohez et~al., 2019]{bohez2019value}
Bohez, S., Abdolmaleki, A., Neunert, M., Buchli, J., Heess, N., and Hadsell, R.
  (2019).
\newblock Value constrained model-free continuous control.
\newblock {\em arXiv}.

\bibitem[Brockman et~al., 2016]{Brockman2016}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016).
\newblock Openai gym.

\bibitem[Cheng et~al., 2019]{Cheng2019}
Cheng, R., Orosz, G., Murray, R.~M., and Burdick, J.~W. (2019).
\newblock End-to-end safe reinforcement learning through barrier functions for
  safety-critical continuous control tasks.
\newblock In {\em AAAI}.

\bibitem[Chou et~al., 2017]{chou2017improving}
Chou, P.-W., Maturana, D., and Scherer, S. (2017).
\newblock Improving stochastic policy gradients in continuous control with deep
  reinforcement learning using the beta distribution.
\newblock In {\em ICML}, pages 834--843.

\bibitem[Chow et~al., 2018]{chow2018lyapunovbased}
Chow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M. (2018).
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock In {\em NeurIPS}.

\bibitem[Chow et~al., 2019]{chow2019lyapunovbased}
Chow, Y., Nachum, O., Faust, A., Duenez-Guzman, E., and Ghavamzadeh, M. (2019).
\newblock Lyapunov-based safe policy optimization for continuous control.
\newblock {\em arXiv}.

\bibitem[Dalal et~al., 2018]{Dalal2018}
Dalal, G., Dvijotham, K., Vecer{\'{\i}}k, M., Hester, T., Paduraru, C., and
  Tassa, Y. (2018).
\newblock Safe exploration in continuous action spaces.
\newblock {\em CoRR}.

\bibitem[Flet{-}Berliac and Basu, 2022]{Flet-Berliac2022}
Flet{-}Berliac, Y. and Basu, D. (2022).
\newblock {SAAC:} safe reinforcement learning as an adversarial game of
  actor-critics.
\newblock In {\em Conference on Reinforcement Learning and Decision Making}.

\bibitem[Haarnoja et~al., 2018]{Haarnoja2018}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic algorithms and applications.
\newblock {\em arXiv}, abs/1812.05905.

\bibitem[Kim et~al., 2004]{Kim2003}
Kim, H., Jordan, M., Sastry, S., and Ng, A. (2004).
\newblock Autonomous helicopter flight via reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~16.

\bibitem[Langlois and Everitt, 2021]{langlois2021rl}
Langlois, E.~D. and Everitt, T. (2021).
\newblock How rl agents behave when their actions are modified.
\newblock In {\em AAAI}.

\bibitem[Levine et~al., 2016]{Levine2016}
Levine, S., Pastor, P., Krizhevsky, A., and Quillen, D. (2016).
\newblock Learning hand-eye coordination for robotic grasping with deep
  learning and large-scale data collection.
\newblock {\em arXiv}.

\bibitem[Li et~al., 2021]{Li2021SafeRL}
Li, Y., Li, N., Tseng, H.~E., Girard, A.~R., Filev, D., and Kolmanovsky, I.~V.
  (2021).
\newblock Safe reinforcement learning using robust action governor.
\newblock In {\em L4DC}.

\bibitem[Likhosherstov et~al., 2021]{likhosherstov2021debiasing}
Likhosherstov, V., Song, X., Choromanski, K., Davis, J., and Weller, A. (2021).
\newblock Debiasing a first-order heuristic for approximate bi-level
  optimization.
\newblock In {\em ICML}.

\bibitem[Liu et~al., 2022]{Liu2022}
Liu, Z., Cen, Z., Isenbaev, V., Liu, W., Wu, Z., Li, B., and Zhao, D. (2022).
\newblock Constrained variational policy optimization for safe reinforcement
  learning.
\newblock In {\em ICML}.

\bibitem[Luo and Ma, 2021]{Luo2021}
Luo, Y. and Ma, T. (2021).
\newblock Learning barrier certificates: Towards safe reinforcement learning
  with zero training-time violations.
\newblock In {\em NeurIPS}.

\bibitem[Mguni et~al., 2021]{Mguni2021}
Mguni, D., Jennings, J., Jafferjee, T., Sootla, A., Yang, Y., Yu, C., Islam,
  U., Wang, Z., and Wang, J. (2021).
\newblock {DESTA:} {A} framework for safe reinforcement learning with markov
  games of intervention.
\newblock {\em arXiv}.

\bibitem[Miret et~al., 2020]{miret2020safety}
Miret, S., Majumdar, S., and Wainwright, C. (2020).
\newblock Safety aware reinforcement learning (sarl).
\newblock {\em arXiv}.

\bibitem[Moffaert and Now{{\'e}}, 2014]{moffaert2015}
Moffaert, K.~V. and Now{{\'e}}, A. (2014).
\newblock Multi-objective reinforcement learning using sets of pareto
  dominating policies.
\newblock {\em JAIR}.

\bibitem[OpenAI et~al., 2019]{OpenAI2019}
OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B.,
  Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., Schneider, J.,
  Tezak, N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., and
  Zhang, L. (2019).
\newblock Solving rubik's cube with a robot hand.
\newblock {\em arXiv}.

\bibitem[Pham et~al., 2018]{pham2018optlayer}
Pham, T.-H., Magistris, G.~D., and Tachibana, R. (2018).
\newblock Optlayer - practical constrained optimization for deep reinforcement
  learning in the real world.
\newblock In {\em ICRA}.

\bibitem[Qin et~al., 2021]{qin2021density}
Qin, Z., Chen, Y., and Fan, C. (2021).
\newblock Density constrained reinforcement learning.
\newblock In {\em ICML}.

\bibitem[Ray et~al., 2019]{Ray2019}
Ray, A., Achiam, J., and Amodei, D. (2019).
\newblock {Benchmarking Safe Exploration in Deep Reinforcement Learning}.

\bibitem[Roijers et~al., 2013]{Roijers2013}
Roijers, D.~M., Vamplew, P., Whiteson, S., and Dazeley, R. (2013).
\newblock A survey of multi-objective sequential decision-making.
\newblock {\em JAIR}.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv}.

\bibitem[Silver et~al., 2016]{Silver2016}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
  (2016).
\newblock Mastering the game of {Go} with deep neural networks and tree search.
\newblock {\em Nature}, 529(7587):484--489.

\bibitem[Stooke et~al., 2020]{stooke2020responsive}
Stooke, A., Achiam, J., and Abbeel, P. (2020).
\newblock Responsive safety in reinforcement learning by pid lagrangian
  methods.
\newblock In {\em ICML}.

\bibitem[Tassa et~al., 2020]{tassa2020dmcontrol}
Tassa, Y., Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S.,
  Merel, J., Erez, T., Lillicrap, T., and Heess, N. (2020).
\newblock dm\_control: Software and tasks for continuous control.

\bibitem[Tessler et~al., 2019]{Tessler2019}
Tessler, C., Mankowitz, D.~J., and Mannor, S. (2019).
\newblock Reward constrained policy optimization.
\newblock In {\em ICLR}.

\bibitem[Thananjeyan et~al., 2021]{Thananjeyan2021}
Thananjeyan, B., Balakrishna, A., Nair, S., Luo, M., Srinivasan, K., Hwang, M.,
  Gonzalez, J.~E., Ibarz, J., Finn, C., and Goldberg, K. (2021).
\newblock Recovery {RL:} safe reinforcement learning with learned recovery
  zones.
\newblock In {\em ICRA}.

\bibitem[Thomas et~al., 2021]{thomas2021safe}
Thomas, G., Luo, Y., and Ma, T. (2021).
\newblock Safe reinforcement learning by imagining the near future.
\newblock In {\em NeurIPS}.

\bibitem[Turchetta et~al., 2020]{turchetta2020safe}
Turchetta, M., Kolobov, A., Shah, S., Krause, A., and Agarwal, A. (2020).
\newblock Safe reinforcement learning via curriculum induction.
\newblock In {\em NeurIPS}.

\bibitem[Van~Moffaert et~al., 2013]{Moffaert2013}
Van~Moffaert, K., Drugan, M.~M., and Nowé, A. (2013).
\newblock Scalarized multi-objective reinforcement learning: Novel design
  techniques.
\newblock In {\em 2013 IEEE Symposium on Adaptive Dynamic Programming and
  Reinforcement Learning (ADPRL)}.

\bibitem[Vinyals et~al., 2019]{Vinyals2019GrandmasterLI}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D.,
  Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J.~P.,
  Jaderberg, M., Vezhnevets, A.~S., Leblond, R., Pohlen, T., Dalibard, V.,
  Budden, D., Sulsky, Y., Molloy, J., Paine, T.~L., Gulcehre, C., Wang, Z.,
  Pfaff, T., Wu, Y., Ring, R., Yogatama, D., W{\"u}nsch, D., McKinney, K.,
  Smith, O., Schaul, T., Lillicrap, T.~P., Kavukcuoglu, K., Hassabis, D., Apps,
  C., and Silver, D. (2019).
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, pages 1--5.

\bibitem[Yang et~al., 2020]{Yang2020Projection-Based}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J. (2020).
\newblock Projection-based constrained policy optimization.
\newblock In {\em ICLR}.

\bibitem[Zhang et~al., 2020]{zhang2020order}
Zhang, Y., Vuong, Q., and Ross, K.~W. (2020).
\newblock First order constrained optimization in policy space.
\newblock In {\em NeurIPS}.

\bibitem[Zhao et~al., 2020]{Zhao2020}
Zhao, W., Queralta, J.~P., and Westerlund, T. (2020).
\newblock Sim-to-real transfer in deep reinforcement learning for robotics: a
  survey.
\newblock In {\em 2020 IEEE Symposium Series on Computational Intelligence
  (SSCI)}.

\end{thebibliography}
