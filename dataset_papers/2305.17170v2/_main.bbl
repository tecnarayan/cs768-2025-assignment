\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aronszajn(1950)]{aronszajn1950theory}
N.~Aronszajn.
\newblock Theory of reproducing kernels.
\newblock \emph{Transactions of the American Mathematical Society}, 68\penalty0
  (3):\penalty0 337--404, 1950.

\bibitem[Bach(2017)]{bach2017equivalence}
F.~Bach.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 714--751, 2017.

\bibitem[Bhattacharya et~al.(2021)Bhattacharya, Hosseini, Kovachki, and
  Stuart]{pca}
K.~Bhattacharya, B.~Hosseini, N.~B. Kovachki, and A.~M. Stuart.
\newblock Model {Reduction} {And} {Neural} {Networks} {For} {Parametric}
  {PDEs}.
\newblock \emph{The SMAI journal of computational mathematics}, 7:\penalty0
  121--157, 2021.
\newblock \doi{10.5802/smai-jcm.74}.
\newblock URL
  \url{https://smai-jcm.centre-mersenne.org/articles/10.5802/smai-jcm.74/}.

\bibitem[Boull{\'e} and Townsend(2022)]{boulle2022learning}
N.~Boull{\'e} and A.~Townsend.
\newblock Learning elliptic partial differential equations with randomized
  linear algebra.
\newblock \emph{Foundations of Computational Mathematics}, pages 1--31, 2022.

\bibitem[Brault et~al.(2016)Brault, Heinonen, and Buc]{brault2016random}
R.~Brault, M.~Heinonen, and F.~Buc.
\newblock Random fourier features for operator-valued kernels.
\newblock In \emph{Asian Conference on Machine Learning}, pages 110--125. PMLR,
  2016.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007optimal}
A.~Caponnetto and E.~De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Caponnetto et~al.(2008)Caponnetto, Micchelli, Pontil, and
  Ying]{caponnetto2008universal}
A.~Caponnetto, C.~A. Micchelli, M.~Pontil, and Y.~Ying.
\newblock Universal multi-task kernels.
\newblock \emph{The Journal of Machine Learning Research}, 9:\penalty0
  1615--1646, 2008.

\bibitem[Carmeli et~al.(2006)Carmeli, De~Vito, and Toigo]{carmeli2006vector}
C.~Carmeli, E.~De~Vito, and A.~Toigo.
\newblock {Vector valued reproducing kernel Hilbert spaces of integrable
  functions and Mercer theorem}.
\newblock \emph{Analysis and Applications}, 4\penalty0 (04):\penalty0 377--408,
  2006.

\bibitem[Carmeli et~al.(2010)Carmeli, De~Vito, Toigo, and
  Umanit{\'a}]{carmeli2010vector}
C.~Carmeli, E.~De~Vito, A.~Toigo, and V.~Umanit{\'a}.
\newblock {Vector valued reproducing kernel Hilbert spaces and universality}.
\newblock \emph{Analysis and Applications}, 8\penalty0 (01):\penalty0 19--61,
  2010.

\bibitem[Chen et~al.(2022)Chen, Schaeffer, and Ward]{chen2022concentration}
Z.~Chen, H.~Schaeffer, and R.~Ward.
\newblock {Concentration of Random Feature Matrices in High-Dimensions}.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pages
  287--302. PMLR, 2022.

\bibitem[Cohen et~al.(2011)Cohen, Devore, and Schwab]{CDS2011}
A.~Cohen, R.~Devore, and C.~Schwab.
\newblock Analytic regularity and polynomial approximation of parametric and
  stochastic elliptic {PDE}'s.
\newblock \emph{Analysis and Applications}, 9\penalty0 (01):\penalty0 11--47,
  2011.

\bibitem[de~Hoop et~al.(2023)de~Hoop, Kovachki, Nelsen, and
  Stuart]{de2023convergence}
M.~V. de~Hoop, N.~B. Kovachki, N.~H. Nelsen, and A.~M. Stuart.
\newblock {Convergence Rates for Learning Linear Operators from Noisy Data}.
\newblock \emph{{SIAM/ASA Journal on Uncertainty Quantification}}, 11\penalty0
  (2):\penalty0 480--513, 2023.
\newblock \doi{10.1137/21M1442942}.

\bibitem[Deng et~al.(2022)Deng, Shin, Lu, Zhang, and
  Karniadakis]{deng2021convergence}
B.~Deng, Y.~Shin, L.~Lu, Z.~Zhang, and G.~E. Karniadakis.
\newblock {Approximation rates of DeepONets for learning operators arising from
  advection--diffusion equations}.
\newblock \emph{Neural Networks}, 153:\penalty0 411--426, 2022.

\bibitem[E et~al.(2020)E, Ma, and Wu]{ma2020min}
W.~E, C.~Ma, and L.~Wu.
\newblock The generalization error of the minimum-norm solutions for
  over-parameterized neural networks.
\newblock \emph{Pure and Applied Functional Analysis}, 5\penalty0 (6):\penalty0
  1445--1460, 2020.

\bibitem[Ghorbani et~al.(2021)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2021linearized}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{Ann. Statist.}, 49:\penalty0 1029--1054, 2021.

\bibitem[Gin et~al.(2021)Gin, Shea, Brunton, and Kutz]{gin2020deepgreen}
C.~R. Gin, D.~E. Shea, S.~L. Brunton, and J.~N. Kutz.
\newblock {DeepGreen: deep learning of Greenâ€™s functions for nonlinear
  boundary value problems}.
\newblock \emph{Scientific Reports}, 11\penalty0 (1):\penalty0 21614, 2021.

\bibitem[Gonon et~al.(2023)Gonon, Grigoryeva, and
  Ortega]{gonon2023approximation}
L.~Gonon, L.~Grigoryeva, and J.-P. Ortega.
\newblock Approximation bounds for random neural networks and reservoir
  systems.
\newblock \emph{The Annals of Applied Probability}, 33\penalty0 (1):\penalty0
  28--69, 2023.

\bibitem[Hashemi et~al.(2023)Hashemi, Schaeffer, Shi, Topcu, Tran, and
  Ward]{hashemi2023generalization}
A.~Hashemi, H.~Schaeffer, R.~Shi, U.~Topcu, G.~Tran, and R.~Ward.
\newblock Generalization bounds for sparse random feature expansions.
\newblock \emph{Applied and Computational Harmonic Analysis}, 62:\penalty0
  310--330, 2023.

\bibitem[Herrmann et~al.(2022)Herrmann, Schwab, and Zech]{herrmann2022neural}
L.~Herrmann, C.~Schwab, and J.~Zech.
\newblock Neural and gpc operator surrogates: construction and expression rate
  bounds.
\newblock \emph{arXiv preprint arXiv:2207.04950}, 2022.

\bibitem[Hesthaven and Ubbiali(2018)]{pca2}
J.~Hesthaven and S.~Ubbiali.
\newblock Non-intrusive reduced order modeling of nonlinear problems using
  neural networks.
\newblock \emph{Journal of Computational Physics}, 363:\penalty0 55--78, 2018.
\newblock ISSN 0021-9991.
\newblock \doi{https://doi.org/10.1016/j.jcp.2018.02.037}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0021999118301190}.

\bibitem[Jin et~al.(2023)Jin, Lu, Blanchet, and Ying]{jin2022minimax}
J.~Jin, Y.~Lu, J.~Blanchet, and L.~Ying.
\newblock {Minimax Optimal Kernel Operator Learning via Multilevel Training}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=zEn1BhaNYsC}.

\bibitem[Kovachki et~al.(2021)Kovachki, Lanthaler, and Mishra]{thfno}
N.~B. Kovachki, S.~Lanthaler, and S.~Mishra.
\newblock {On Universal Approximation and Error Bounds for Fourier Neural
  Operators}.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (290):\penalty0 1--76, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/21-0806.html}.

\bibitem[Kovachki et~al.(2023)Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya,
  Stuart, and Anandkumar]{kovachki2023neural}
N.~B. Kovachki, Z.~Li, B.~Liu, K.~Azizzadenesheli, K.~Bhattacharya, A.~M.
  Stuart, and A.~Anandkumar.
\newblock {Neural Operator: Learning Maps Between Function Spaces With
  Applications to PDEs}.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (89):\penalty0 1--97, 2023.

\bibitem[Lanthaler et~al.(2022)Lanthaler, Mishra, and Karniadakis]{thdeeponet}
S.~Lanthaler, S.~Mishra, and G.~E. Karniadakis.
\newblock {Error estimates for DeepONets: a deep learning framework in infinite
  dimensions}.
\newblock \emph{Transactions of Mathematics and Its Applications}, 6\penalty0
  (1), 03 2022.
\newblock ISSN 2398-4945.
\newblock \doi{10.1093/imatrm/tnac001}.
\newblock URL \url{https://doi.org/10.1093/imatrm/tnac001}.
\newblock tnac001.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Kovachki, Azizzadenesheli, Liu,
  Bhattacharya, Stuart, and Anandkumar]{fourierop2020}
Z.~Li, N.~B. Kovachki, K.~Azizzadenesheli, B.~Liu, K.~Bhattacharya, A.~M.
  Stuart, and A.~Anandkumar.
\newblock {Fourier Neural Operator for Parametric Partial Differential
  Equations}.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=c8P9NQVtmnO}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Ton, Oglic, and
  Sejdinovic]{li2021towards}
Z.~Li, J.-F. Ton, D.~Oglic, and D.~Sejdinovic.
\newblock {Towards a unified analysis of random Fourier features}.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (108),
  2021{\natexlab{b}}.

\bibitem[Lu et~al.(2021)Lu, Jin, Pang, Zhang, and Karniadakis]{lu2021learning}
L.~Lu, P.~Jin, G.~Pang, Z.~Zhang, and G.~E. Karniadakis.
\newblock {Learning nonlinear operators via DeepONet based on the universal
  approximation theorem of operators}.
\newblock \emph{Nature Machine Intelligence}, 3\penalty0 (3):\penalty0
  218--229, 2021.

\bibitem[Maurer and Pontil(2021)]{maurer2021concentration}
A.~Maurer and M.~Pontil.
\newblock {Concentration inequalities under sub-Gaussian and sub-exponential
  conditions}.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 7588--7597, 2021.

\bibitem[Mei et~al.(2022)Mei, Misiakiewicz, and
  Montanari]{mei2022generalization}
S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Generalization error of random feature and kernel methods:
  hypercontractivity and kernel matrix concentration.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  3--84, 2022.

\bibitem[Minh(2016)]{minh2016operator}
H.~Q. Minh.
\newblock {Operator-valued Bochner theorem, Fourier feature maps for
  operator-valued kernels, and vector-valued learning}.
\newblock \emph{arXiv preprint arXiv:1608.05639}, 2016.

\bibitem[Mollenhauer et~al.(2022)Mollenhauer, M{\"u}cke, and
  Sullivan]{mollenhauer2022learning}
M.~Mollenhauer, N.~M{\"u}cke, and T.~Sullivan.
\newblock {Learning linear operators: Infinite-dimensional regression as a
  well-behaved non-compact inverse problem}.
\newblock \emph{arXiv preprint arXiv:2211.08875}, 2022.

\bibitem[Nelsen and Stuart(2021)]{rfm}
N.~H. Nelsen and A.~M. Stuart.
\newblock {The random feature model for input-output maps between Banach
  spaces}.
\newblock \emph{SIAM Journal on Scientific Computing}, 43\penalty0
  (5):\penalty0 A3212--A3243, 2021.

\bibitem[Ogawa(1988)]{ogawa1988operator}
H.~Ogawa.
\newblock An operator pseudo-inversion lemma.
\newblock \emph{SIAM Journal on Applied Mathematics}, 48\penalty0 (6):\penalty0
  1527--1531, 1988.

\bibitem[Owhadi and Scovel(2017)]{owhadi2017separability}
H.~Owhadi and C.~Scovel.
\newblock {Separability of reproducing kernel spaces}.
\newblock \emph{Proceedings of the American Mathematical Society}, 145\penalty0
  (5):\penalty0 2131--2138, 2017.

\bibitem[Pinelis and Sakhanenko(1986)]{pinelis1986remarks}
I.~F. Pinelis and A.~I. Sakhanenko.
\newblock Remarks on inequalities for large deviation probabilities.
\newblock \emph{Theory of Probability \& Its Applications}, 30\penalty0
  (1):\penalty0 143--148, 1986.

\bibitem[Rahimi and Recht(2007)]{rahimi2007random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock \emph{Advances in Neural Information Processing Systems}, 20, 2007.

\bibitem[Rahimi and Recht(2008)]{RR2008}
A.~Rahimi and B.~Recht.
\newblock {Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning}.
\newblock \emph{Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem[Rudi and Rosasco(2017)]{rudi2017generalization}
A.~Rudi and L.~Rosasco.
\newblock Generalization properties of learning with random features.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Ryck and Mishra(2022)]{ryck2022generic}
T.~D. Ryck and S.~Mishra.
\newblock Generic bounds on the approximation error for physics-informed (and)
  operator learning.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, editors,
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=bF4eYy3LTR9}.

\bibitem[Sch{\"a}fer and Owhadi(2023)]{schafer2021sparse}
F.~Sch{\"a}fer and H.~Owhadi.
\newblock Sparse recovery of elliptic solvers from matrix-vector products.
\newblock \emph{arXiv preprint arXiv:2110.05351 (to appear in SIAM J. Sci.
  Comput.)}, 2023.

\bibitem[Scholkopf and Smola(2018)]{scholkopf2018learning}
B.~Scholkopf and A.~J. Smola.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT Press, 2018.

\bibitem[Schwab and Zech(2019)]{SchwabZech2019}
C.~Schwab and J.~Zech.
\newblock {Deep learning in high dimension: Neural network expression rates for
  generalized polynomial chaos expansions in UQ}.
\newblock \emph{Analysis and Applications}, 17\penalty0 (01):\penalty0 19--55,
  2019.

\bibitem[Smale and Zhou(2005)]{smale2005shannon}
S.~Smale and D.-X. Zhou.
\newblock {Shannon sampling II: Connections to learning theory}.
\newblock \emph{Applied and Computational Harmonic Analysis}, 19\penalty0
  (3):\penalty0 285--302, 2005.

\bibitem[Stepaniants(2023)]{stepaniants2023learning}
G.~Stepaniants.
\newblock {Learning partial differential equations in reproducing kernel
  Hilbert spaces}.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (86):\penalty0 1--72, 2023.

\bibitem[Sun et~al.(2018)Sun, Gilbert, and Tewari]{sun2018but}
Y.~Sun, A.~Gilbert, and A.~Tewari.
\newblock {But how does it work in theory? Linear SVM with random features}.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Vershynin(2018)]{vershynin2018high}
R.~Vershynin.
\newblock {High-dimensional probability: An introduction with applications in
  data science}, 2018.

\bibitem[Wainwright(2019)]{wainwright2019high}
M.~J. Wainwright.
\newblock \emph{{High-dimensional statistics: A non-asymptotic viewpoint}},
  volume~48.
\newblock Cambridge University Press, 2019.

\end{thebibliography}
