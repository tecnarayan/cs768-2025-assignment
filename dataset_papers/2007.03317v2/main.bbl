\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein gan.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Asuncion and Newman(2007)]{asuncion2007uci}
Arthur Asuncion and David Newman.
\newblock Uci machine learning repository, 2007.

\bibitem[Bao et~al.(2020)Bao, Li, Xu, Su, Zhu, and Zhang]{bao-bi}
Fan Bao, Chongxuan Li, Kun Xu, Hang Su, Jun Zhu, and Bo~Zhang.
\newblock Bi-level score matching for learning energy-based latent variable
  models.
\newblock In \emph{https://arxiv.org/abs/2010.07856}, 2020.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Brunel and Nadal(1998)]{brunel1998mutual}
Nicolas Brunel and Jean-Pierre Nadal.
\newblock Mutual information, fisher information, and population coding.
\newblock \emph{Neural computation}, 10\penalty0 (7):\penalty0 1731--1757,
  1998.

\bibitem[Burda et~al.(2015)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov.
\newblock Importance weighted autoencoders.
\newblock \emph{arXiv preprint arXiv:1509.00519}, 2015.

\bibitem[Choi et~al.(2018)Choi, Jang, and Alemi]{choi2018waic}
Hyunsun Choi, Eric Jang, and Alexander~A Alemi.
\newblock Waic, but why? generative ensembles for robust anomaly detection.
\newblock \emph{arXiv preprint arXiv:1810.01392}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2009.

\bibitem[Dinh et~al.(2014)Dinh, Krueger, and Bengio]{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock \emph{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Dumoulin et~al.(2017)Dumoulin, Shlens, and
  Kudlur]{dumoulin2016learned}
Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
\newblock A learned representation for artistic style.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pages 2672--2680, 2014.

\bibitem[Grathwohl et~al.(2020)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi,
  and Swersky]{Grathwohl2020Your}
Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Griewank(1993)]{griewank1993some}
Andreas Griewank.
\newblock Some bounds on the complexity of gradients, jacobians, and hessians.
\newblock In \emph{Complexity in numerical optimization}, pages 128--162. World
  Scientific, 1993.

\bibitem[Griewank and Walther(2008)]{griewank2008evaluating}
Andreas Griewank and Andrea Walther.
\newblock \emph{Evaluating derivatives: principles and techniques of
  algorithmic differentiation}, volume 105.
\newblock Siam, 2008.

\bibitem[Gutmann and Hyv{\"a}rinen(2010)]{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 297--304, 2010.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[He et~al.(2019)He, Spokoyny, Neubig, and
  Berg-Kirkpatrick]{he2019lagging}
Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick.
\newblock Lagging inference networks and posterior collapse in variational
  autoencoders.
\newblock \emph{arXiv preprint arXiv:1901.05534}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778, 2016.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6626--6637, 2017.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 6\penalty0
  (Apr):\penalty0 695--709, 2005.

\bibitem[Isaacson and Keller(2012)]{isaacson2012analysis}
Eugene Isaacson and Herbert~Bishop Keller.
\newblock \emph{Analysis of numerical methods}.
\newblock Courier Corporation, 2012.

\bibitem[Johnson(2004)]{johnson2004information}
Oliver Johnson.
\newblock \emph{Information theory and the central limit theorem}.
\newblock World Scientific, 2004.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2014)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Kingma and Cun(2010)]{kingma2010regularized}
Durk~P Kingma and Yann~L Cun.
\newblock Regularized estimation of image statistics by score matching.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, pages 1126--1134, 2010.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[K{\"o}nigsberger(2004)]{konigsberger2004analysis}
Konrad K{\"o}nigsberger.
\newblock Analysis 2 springer verlag, 2004.

\bibitem[Krizhevsky and Hinton(2009)]{Krizhevsky2012}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Kuleshov and Ermon(2017)]{kuleshov2017neural}
Volodymyr Kuleshov and Stefano Ermon.
\newblock Neural variational inference and learning in undirected graphical
  models.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Kumar et~al.(2019)Kumar, Ozair, Goyal, Courville, and
  Bengio]{kumar2019maximum}
Rithesh Kumar, Sherjil Ozair, Anirudh Goyal, Aaron Courville, and Yoshua
  Bengio.
\newblock Maximum entropy generators for energy-based models.
\newblock \emph{arXiv preprint arXiv:1901.08508}, 2019.

\bibitem[Kurach et~al.(2018)Kurach, Lucic, Zhai, Michalski, and
  Gelly]{kurach2018large}
Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly.
\newblock A large-scale study on regularization and normalization in gans.
\newblock \emph{arXiv preprint arXiv:1807.04720}, 2018.

\bibitem[LeCun(1993)]{lecun1993efficient}
Yann LeCun.
\newblock Efficient learning and second-order methods.
\newblock \emph{A tutorial at NIPS}, 93:\penalty0 61, 1993.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{Lecun1998}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, and F~Huang.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting structured data}, 1\penalty0 (0), 2006.

\bibitem[Li and Turner(2018)]{li2017gradient}
Yingzhen Li and Richard~E Turner.
\newblock Gradient estimators for implicit models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Li et~al.(2019)Li, Chen, and Sommer]{li2019annealed}
Zengyi Li, Yubei Chen, and Friedrich~T Sommer.
\newblock Annealed denoising score matching: Learning energy-based models in
  high-dimensional spaces.
\newblock \emph{arXiv preprint arXiv:1910.07762}, 2019.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 2737--2745, 2015.

\bibitem[Lin et~al.(2017)Lin, Milan, Shen, and Reid]{lin2017refinenet}
Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid.
\newblock Refinenet: Multi-path refinement networks for high-resolution
  semantic segmentation.
\newblock In \emph{IEEE International Conference on Computer Vision (CVPR)},
  pages 1925--1934, 2017.

\bibitem[Liu et~al.(2016)Liu, Lee, and Jordan]{liu2016kernelized}
Qiang Liu, Jason Lee, and Michael Jordan.
\newblock A kernelized stein discrepancy for goodness-of-fit tests.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem[Luo et~al.(2020)Luo, Beatson, Norouzi, Zhu, Duvenaud, Adams, and
  Chen]{Luo2020SUMO}
Yucen Luo, Alex Beatson, Mohammad Norouzi, Jun Zhu, David Duvenaud, Ryan~P.
  Adams, and Ricky T.~Q. Chen.
\newblock Sumo: Unbiased estimation of log marginal probability for latent
  variable models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Martens et~al.(2012)Martens, Sutskever, and
  Swersky]{martens2012estimating}
James Martens, Ilya Sutskever, and Kevin Swersky.
\newblock Estimating the hessian by back-propagating curvature.
\newblock \emph{arXiv preprint arXiv:1206.6464}, 2012.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{mescheder2018training}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for gans do actually converge?
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Mnih and Hinton(2005)]{mnih2005learning}
Andriy Mnih and Geoffrey Hinton.
\newblock Learning nonlinear constraints with contrastive backpropagation.
\newblock In \emph{International Joint Conference on Neural Networks (IJCNN)},
  volume~2, pages 1302--1307. IEEE, 2005.

\bibitem[M{\o}ller(1990)]{moller1990scaled}
Martin~F M{\o}ller.
\newblock \emph{A scaled conjugate gradient algorithm for fast supervised
  learning}.
\newblock Aarhus University, Computer Science Department, 1990.

\bibitem[Nalisnick et~al.(2019)Nalisnick, Matsukawa, Teh, and
  Lakshminarayanan]{nalisnick2019detecting}
Eric Nalisnick, Akihiro Matsukawa, Yee~Whye Teh, and Balaji Lakshminarayanan.
\newblock Detecting out-of-distribution inputs to deep generative models using
  a test for typicality.
\newblock \emph{arXiv preprint arXiv:1906.02994}, 2019.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning}, 2011.

\bibitem[Nijkamp et~al.(2019)Nijkamp, Hill, Han, Zhu, and
  Wu]{nijkamp2019anatomy}
Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying~Nian Wu.
\newblock On the anatomy of mcmc-based maximum likelihood learning of
  energy-based models.
\newblock \emph{arXiv preprint arXiv:1903.12370}, 2019.

\bibitem[Oord et~al.(2016)Oord, Kalchbrenner, and Kavukcuoglu]{oord2016pixel}
Aaron van~den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 8024--8035, 2019.

\bibitem[Poon and Domingos(2011)]{poon2011sum}
Hoifung Poon and Pedro Domingos.
\newblock Sum-product networks: A new deep architecture.
\newblock In \emph{2011 IEEE International Conference on Computer Vision
  Workshops (ICCV Workshops)}, pages 689--690. IEEE, 2011.

\bibitem[Rabin et~al.(2011)Rabin, Peyr{\'e}, Delon, and
  Bernot]{rabin2011wasserstein}
Julien Rabin, Gabriel Peyr{\'e}, Julie Delon, and Marc Bernot.
\newblock Wasserstein barycenter and its application to texture mixing.
\newblock In \emph{International Conference on Scale Space and Variational
  Methods in Computer Vision}, pages 435--446. Springer, 2011.

\bibitem[Rifai et~al.(2011)Rifai, Mesnil, Vincent, Muller, Bengio, Dauphin, and
  Glorot]{rifai2011higher}
Salah Rifai, Gr{\'e}goire Mesnil, Pascal Vincent, Xavier Muller, Yoshua Bengio,
  Yann Dauphin, and Xavier Glorot.
\newblock Higher order contractive auto-encoder.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 645--660. Springer, 2011.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Saremi and Hyvarinen(2019)]{saremi2019neural}
Saeed Saremi and Aapo Hyvarinen.
\newblock Neural empirical bayes.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 20:\penalty0
  1--23, 2019.

\bibitem[Saremi et~al.(2018)Saremi, Mehrjou, Sch{\"o}lkopf, and
  Hyv{\"a}rinen]{saremi2018deep}
Saeed Saremi, Arash Mehrjou, Bernhard Sch{\"o}lkopf, and Aapo Hyv{\"a}rinen.
\newblock Deep energy estimator networks.
\newblock \emph{arXiv preprint arXiv:1805.08306}, 2018.

\bibitem[Sasaki et~al.(2014)Sasaki, Hyv{\"a}rinen, and
  Sugiyama]{sasaki2014clustering}
Hiroaki Sasaki, Aapo Hyv{\"a}rinen, and Masashi Sugiyama.
\newblock Clustering via mode seeking by direct estimation of the gradient of a
  log-density.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 19--34. Springer, 2014.

\bibitem[Shi et~al.(2018)Shi, Sun, and Zhu]{shi2018spectral}
Jiaxin Shi, Shengyang Sun, and Jun Zhu.
\newblock A spectral approach to gradient estimation for implicit
  distributions.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Sohl-Dickstein et~al.(2011)Sohl-Dickstein, Battaglino, and
  DeWeese]{sohl2011new}
Jascha Sohl-Dickstein, Peter~B Battaglino, and Michael~R DeWeese.
\newblock New method for parameter estimation in probabilistic models: minimum
  probability flow.
\newblock \emph{Physical review letters}, 107\penalty0 (22):\penalty0 220601,
  2011.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 11895--11907, 2019.

\bibitem[Song et~al.(2019)Song, Garg, Shi, and Ermon]{song2019sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  2019.

\bibitem[Sriperumbudur et~al.(2017)Sriperumbudur, Fukumizu, Gretton,
  Hyv{\"a}rinen, and Kumar]{sriperumbudur2017density}
Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Aapo Hyv{\"a}rinen, and
  Revant Kumar.
\newblock Density estimation in infinite dimensional exponential families.
\newblock \emph{The Journal of Machine Learning Research (JMLR)}, 18\penalty0
  (1):\penalty0 1830--1888, 2017.

\bibitem[Stoer and Bulirsch(2013)]{stoer2013introduction}
Josef Stoer and Roland Bulirsch.
\newblock \emph{Introduction to numerical analysis}, volume~12.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Strathmann et~al.(2015)Strathmann, Sejdinovic, Livingstone, Szabo, and
  Gretton]{strathmann2015gradient}
Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo, and Arthur
  Gretton.
\newblock Gradient-free hamiltonian monte carlo with efficient kernel
  exponential families.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 955--963, 2015.

\bibitem[Sutherland et~al.(2018)Sutherland, Strathmann, Arbel, and
  Gretton]{sutherland2018efficient}
Dougal Sutherland, Heiko Strathmann, Michael Arbel, and Arthur Gretton.
\newblock Efficient and principled score estimation with nystr{\"o}m kernel
  exponential families.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 652--660, 2018.

\bibitem[Teh et~al.(2003)Teh, Welling, Osindero, and Hinton]{teh2003energy}
Yee~Whye Teh, Max Welling, Simon Osindero, and Geoffrey~E Hinton.
\newblock Energy-based models for sparse overcomplete representations.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 4\penalty0
  (Dec):\penalty0 1235--1260, 2003.

\bibitem[Tolstikhin et~al.(2017)Tolstikhin, Bousquet, Gelly, and
  Schoelkopf]{tolstikhin2017wasserstein}
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf.
\newblock Wasserstein auto-encoders.
\newblock \emph{arXiv preprint arXiv:1711.01558}, 2017.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Wang et~al.(2020)Wang, Cheng, Li, Zhu, and Zhang]{wang2020wasserstein}
Ziyu Wang, Shuyu Cheng, Yueru Li, Jun Zhu, and Bo~Zhang.
\newblock A wasserstein minimum velocity approach to learning unnormalized
  models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2011.

\bibitem[Wenliang et~al.(2019)Wenliang, Sutherland, Strathmann, and
  Gretton]{wenliang2019learning}
Li~Wenliang, Dougal Sutherland, Heiko Strathmann, and Arthur Gretton.
\newblock Learning deep kernels for exponential family densities.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xie et~al.(2016)Xie, Lu, Zhu, and Wu]{xie2016theory}
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu.
\newblock A theory of generative convnet.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Xu et~al.(2019)Xu, Li, Wei, Zhu, and Zhang]{xu2019understanding}
Kun Xu, Chongxuan Li, Huanshu Wei, Jun Zhu, and Bo~Zhang.
\newblock Understanding and stabilizing gans' training dynamics with control
  theory.
\newblock \emph{arXiv preprint arXiv:1909.13188}, 2019.

\bibitem[Yu et~al.(2017)Yu, Koltun, and Funkhouser]{yu2017dilated}
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser.
\newblock Dilated residual networks.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 472--480, 2017.

\bibitem[Yuan(2014)]{yuan2014rotation}
Feiniu Yuan.
\newblock Rotation and scale invariant local binary pattern based on high order
  directional derivatives for texture classification.
\newblock \emph{Digital Signal Processing}, 26:\penalty0 142--152, 2014.

\bibitem[Zheng et~al.(2015)Zheng, Yang, Liu, Liang, and Li]{zheng2015improving}
Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li.
\newblock Improving deep neural networks using softplus units.
\newblock In \emph{International Joint Conference on Neural Networks (IJCNN)},
  pages 1--4. IEEE, 2015.

\bibitem[Zhou et~al.(2020)Zhou, Shi, and Zhu]{zhou2020nonparametric}
Yuhao Zhou, Jiaxin Shi, and Jun Zhu.
\newblock Nonparametric score estimators.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Zhu et~al.(2017)Zhu, Park, Isola, and Efros]{zhu2017unpaired}
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei~A Efros.
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In \emph{IEEE International Conference on Computer Vision (CVPR)},
  pages 2223--2232, 2017.

\end{thebibliography}
