\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe et~al.(2022)Abbe, Cornacchia, Hazla, and
  Marquis]{abbe2022initial}
Emmanuel Abbe, Elisabetta Cornacchia, Jan Hazla, and Christopher Marquis.
\newblock An initial alignment between neural network and target is needed for
  gradient descent to learn.
\newblock In \emph{International Conference on Machine Learning}, pages 33--52.
  PMLR, 2022.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Liang]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[Bach(2017)]{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Bertoin et~al.(2021)Bertoin, Bolte, Gerchinovitz, and
  Pauwels]{bertoin2021numerical}
David Bertoin, J{\'e}r{\^o}me Bolte, S{\'e}bastien Gerchinovitz, and Edouard
  Pauwels.
\newblock Numerical influence of {ReLu}â€™(0) on backpropagation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Bietti and Mairal(2019)]{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Bolte et~al.(2007)Bolte, Daniilidis, and Lewis]{bolte2007lojasiewicz}
J{\'e}r{\^o}me Bolte, Aris Daniilidis, and Adrian Lewis.
\newblock The {{\L}}ojasiewicz inequality for nonsmooth subanalytic functions
  with applications to subgradient dynamical systems.
\newblock \emph{SIAM Journal on Optimization}, 17\penalty0 (4):\penalty0
  1205--1223, 2007.

\bibitem[Bolte et~al.(2010)Bolte, Daniilidis, Ley, and
  Mazet]{bolte2010characterizations}
J{\'e}r{\^o}me Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet.
\newblock Characterizations of {{\L}}ojasiewicz inequalities: subgradient
  flows, talweg, convexity.
\newblock \emph{Transactions of the American Mathematical Society},
  362\penalty0 (6):\penalty0 3319--3363, 2010.

\bibitem[Chatterjee(2022)]{chatterjee2022convergence}
Sourav Chatterjee.
\newblock Convergence of gradient descent for deep neural networks.
\newblock \emph{arXiv preprint arXiv:2203.16462}, 2022.

\bibitem[Chen et~al.(2022)Chen, Vanden-Eijnden, and Bruna]{chen2022feature}
Zhengdao Chen, Eric Vanden-Eijnden, and Joan Bruna.
\newblock On feature learning in neural networks with global convergence
  guarantees.
\newblock \emph{arXiv preprint arXiv:2204.10782}, 2022.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR,
  2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{dauphin2014identifying}
Yann~N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
  and Yoshua Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Debarre et~al.(2022)Debarre, Denoyelle, Unser, and
  Fageot]{debarre2022sparsest}
Thomas Debarre, Quentin Denoyelle, Michael Unser, and Julien Fageot.
\newblock Sparsest piecewise-linear regression of one-dimensional data.
\newblock \emph{Journal of Computational and Applied Mathematics},
  406:\penalty0 114044, 2022.

\bibitem[Eberle et~al.(2021)Eberle, Jentzen, Riekert, and
  Weiss]{eberle2021existence}
Simon Eberle, Arnulf Jentzen, Adrian Riekert, and Georg~S Weiss.
\newblock Existence, uniqueness, and convergence rates for gradient flows in
  the training of artificial neural networks with {ReLU} activation.
\newblock \emph{arXiv preprint arXiv:2108.08106}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jacot et~al.(2021)Jacot, Ged, Gabriel, {\c{S}}im{\c{s}}ek, and
  Hongler]{jacot2021deep}
Arthur Jacot, Fran{\c{c}}ois Ged, Franck Gabriel, Berfin {\c{S}}im{\c{s}}ek,
  and Cl{\'e}ment Hongler.
\newblock Saddle-to-saddle dynamics in deep linear networks: {S}mall
  initialization training, symmetry, and sparsity.
\newblock \emph{arXiv preprint arXiv:2106.15933}, 2021.

\bibitem[Ji and Telgarsky(2019{\natexlab{a}})]{ji2018gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Ji and Telgarsky(2019{\natexlab{b}})]{ji2019implicit}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pages 1772--1798. PMLR,
  2019{\natexlab{b}}.

\bibitem[Ji and Telgarsky(2020)]{ji2020directional}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17176--17186, 2020.

\bibitem[Kalimeris et~al.(2019)Kalimeris, Kaplun, Nakkiran, Edelman, Yang,
  Barak, and Zhang]{kalimeris2019sgd}
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan
  Yang, Boaz Barak, and Haofeng Zhang.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Kurkov{\'a} and Sanguineti(2001)]{kurkova2001variation}
Vera Kurkov{\'a} and Marcello Sanguineti.
\newblock Bounds on rates of variable-basis and neural-network approximation.
\newblock \emph{IEEE Transactions on Information Theory}, 47\penalty0
  (6):\penalty0 2659--2665, 2001.

\bibitem[Li et~al.(2019)Li, Tai, and Weinan]{li2019stochastic}
Qianxiao Li, Cheng Tai, and E~Weinan.
\newblock Stochastic modified equations and dynamics of stochastic gradient
  algorithms i: Mathematical foundations.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 1474--1520, 2019.

\bibitem[Li et~al.(2020)Li, Luo, and Lyu]{li2020towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 2022.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
\newblock Gradient descent on two-layer nets: Margin maximization and
  simplicity bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Maennel et~al.(2018)Maennel, Bousquet, and Gelly]{maennel2018gradient}
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly.
\newblock Gradient descent quantizes {ReLu} network features.
\newblock \emph{arXiv preprint arXiv:1803.08367}, 2018.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Min et~al.(2021)Min, Tarmoun, Vidal, and Mallada]{min2021on}
Hancheng Min, Salma Tarmoun, Rene Vidal, and Enrique Mallada.
\newblock On the explicit role of initialization on the convergence and
  implicit bias of overparametrized linear networks.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pages 7760--7768. PMLR, 18--24 Jul 2021.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Ongie et~al.(2019)Ongie, Willett, Soudry, and
  Srebro]{ongie2019function}
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro.
\newblock A function space view of bounded norm infinite width {ReLu} nets: The
  multivariate case.
\newblock \emph{arXiv preprint arXiv:1910.01635}, 2019.

\bibitem[Parhi and Nowak(2022)]{parhi2022kinds}
Rahul Parhi and Robert~D Nowak.
\newblock What kinds of functions do deep neural networks learn? {I}nsights
  from variational spline theory.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 4\penalty0
  (2):\penalty0 464--489, 2022.

\bibitem[Pesme et~al.(2021)Pesme, Pillaud-Vivien, and
  Flammarion]{pesme2021implicit}
Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Implicit bias of sgd for diagonal linear networks: {A} provable
  benefit of stochasticity.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Petrini et~al.(2022)Petrini, Cagnetta, Vanden-Eijnden, and
  Wyart]{petrini2022learning}
Leonardo Petrini, Francesco Cagnetta, Eric Vanden-Eijnden, and Matthieu Wyart.
\newblock Learning sparse features can lead to overfitting in neural networks.
\newblock \emph{arXiv preprint arXiv:2206.12314}, 2022.

\bibitem[Phuong and Lampert(2020)]{phuong2020inductive}
Mary Phuong and Christoph~H Lampert.
\newblock The inductive bias of {ReLU} networks on orthogonally separable data.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Rotskoff and Vanden-Eijnden(2022)]{rotskoff2022}
Grant Rotskoff and Eric Vanden-Eijnden.
\newblock Trainability and accuracy of artificial neural networks: An
  interacting particle system approach.
\newblock \emph{Communications on Pure and Applied Mathematics}, 75\penalty0
  (9):\penalty0 1889--1935, 2022.
\newblock \doi{https://doi.org/10.1002/cpa.22074}.

\bibitem[Safran et~al.(2021)Safran, Yehudai, and Shamir]{safran2021effects}
Itay~M Safran, Gilad Yehudai, and Ohad Shamir.
\newblock The effects of mild over-parameterization on the optimization
  landscape of shallow {ReLU} neural networks.
\newblock In \emph{Proceedings of Thirty Fourth Conference on Learning Theory},
  volume 134 of \emph{Proceedings of Machine Learning Research}, pages
  3889--3934. PMLR, 15--19 Aug 2021.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and
  Srebro]{savarese2019infinite}
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro.
\newblock How do infinite width bounded norm networks look in function space?
\newblock In \emph{Conference on Learning Theory}, pages 2667--2690. PMLR,
  2019.

\bibitem[Shevchenko et~al.(2021)Shevchenko, Kungurtsev, and
  Mondelli]{shevchenko2021mean}
Alexander Shevchenko, Vyacheslav Kungurtsev, and Marco Mondelli.
\newblock Mean-field analysis of piecewise linear solutions for wide {ReLu}
  networks.
\newblock \emph{arXiv preprint arXiv:2111.02278}, 2021.

\bibitem[Sirignano and Spiliopoulos(2020)]{sirignano2020mean}
Justin Sirignano and Konstantinos Spiliopoulos.
\newblock Mean field analysis of neural networks: A law of large numbers.
\newblock \emph{SIAM Journal on Applied Mathematics}, 80\penalty0 (2):\penalty0
  725--752, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Vardi and Shamir(2021)]{vardi2021implicit}
Gal Vardi and Ohad Shamir.
\newblock Implicit regularization in {ReLu} networks with the square loss.
\newblock In \emph{Conference on Learning Theory}, pages 4224--4258. PMLR,
  2021.

\bibitem[Wang and Pilanci(2021)]{wang2021convex}
Yifei Wang and Mert Pilanci.
\newblock The convex geometry of backpropagation: Neural network gradient flows
  converge to extreme points of the dual convex program.
\newblock \emph{arXiv preprint arXiv:2110.06488}, 2021.

\bibitem[Wojtowytsch(2020)]{wojtowytsch2020convergence}
Stephan Wojtowytsch.
\newblock On the convergence of gradient descent training for two-layer
  {ReLu}-networks in the mean field regime.
\newblock \emph{arXiv preprint arXiv:2005.13530}, 2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, pages 3635--3673. PMLR,
  2020.

\bibitem[Yang and Hu(2021)]{yang2021tensor}
Greg Yang and Edward~J Hu.
\newblock Tensor programs iv: Feature learning in infinite-width neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  11727--11737. PMLR, 2021.

\bibitem[Yun et~al.(2021)Yun, Krishnan, and Mobahi]{yun2021unifying}
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhou et~al.(2021)Zhou, Ge, and Jin]{zhou2021local}
Mo~Zhou, Rong Ge, and Chi Jin.
\newblock A local convergence theory for mildly over-parameterized two-layer
  neural network.
\newblock In \emph{Conference on Learning Theory}, pages 4577--4632. PMLR,
  2021.

\end{thebibliography}
