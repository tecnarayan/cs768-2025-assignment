\begin{thebibliography}{110}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Andreassen et~al.(2021)Andreassen, Bahri, Neyshabur, and Roelofs]{andreassen2021evolution}
Andreassen, A., Bahri, Y., Neyshabur, B., and Roelofs, R.
\newblock The evolution of out-of-distribution robustness throughout fine-tuning.
\newblock \emph{arXiv preprint arXiv:2106.15831}, 2021.

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{bain2021frozen}
Bain, M., Nagrani, A., Varol, G., and Zisserman, A.
\newblock Frozen in time: A joint video and image encoder for end-to-end retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  1728--1738, 2021.

\bibitem[Ben-David et~al.(2006)Ben-David, Blitzer, Crammer, and Pereira]{ben2006analysis}
Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F.
\newblock Analysis of representations for domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 19:\penalty0 137--144, 2006.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira, and Vaughan]{ben2010theory}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J.~W.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79:\penalty0 151--175, 2010.

\bibitem[Bousmalis et~al.(2016)Bousmalis, Trigeorgis, Silberman, Krishnan, and Erhan]{bousmalis2016domain}
Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D.
\newblock Domain separation networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\  343--351, 2016.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and Soricut]{changpinyo2021conceptual}
Changpinyo, S., Sharma, P., Ding, N., and Soricut, R.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  3558--3568, 2021.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Xie, Huang, Rong, Ding, Huang, Xu, and Huang]{chen2019progressive}
Chen, C., Xie, W., Huang, W., Rong, Y., Ding, X., Huang, Y., Xu, T., and Huang, J.
\newblock Progressive feature alignment for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  627--636, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2022)Chen, Chen, Wei, Jin, Tan, Jin, and Chen]{Chen_2022_CVPR}
Chen, L., Chen, H., Wei, Z., Jin, X., Tan, X., Jin, Y., and Chen, E.
\newblock Reusing the task-specific classifier as a discriminator: Discriminator-free adversarial domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  7181--7190, June 2022.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Kira, AlRegib, Yoo, Chen, and Zheng]{chen2019temporal}
Chen, M.-H., Kira, Z., AlRegib, G., Yoo, J., Chen, R., and Zheng, J.
\newblock Temporal attentive alignment for large-scale video domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  6321--6330, 2019{\natexlab{b}}.

\bibitem[Chen et~al.(2024)Chen, Zhang, Jiang, Lu, and Zhang]{chen2024large}
Chen, S., Zhang, Y., Jiang, W., Lu, J., and Zhang, Y.
\newblock Large language models as visual cross-domain learners.
\newblock \emph{arXiv preprint arXiv:2401.03253}, 2024.

\bibitem[Choi et~al.(2020)Choi, Sharma, Chandraker, and Huang]{choi2020unsupervised}
Choi, J., Sharma, G., Chandraker, M., and Huang, J.-B.
\newblock Unsupervised and semi-supervised domain adaptation for action recognition from drones.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.\  1717--1726, 2020.

\bibitem[Dasgupta et~al.(2022)Dasgupta, Jawahar, and Alahari]{dasgupta2022overcoming}
Dasgupta, A., Jawahar, C., and Alahari, K.
\newblock Overcoming label noise for source-free unsupervised video domain adaptation.
\newblock In \emph{Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing}, pp.\  1--9, 2022.

\bibitem[Deng et~al.(2019)Deng, Luo, and Zhu]{deng2019cluster}
Deng, Z., Luo, Y., and Zhu, J.
\newblock Cluster alignment with a teacher for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  9944--9953, 2019.

\bibitem[Desai \& Johnson(2021)Desai and Johnson]{desai2021virtex}
Desai, K. and Johnson, J.
\newblock Virtex: Learning visual representations from textual annotations.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  11162--11173, 2021.

\bibitem[Desai et~al.(2021)Desai, Kaul, Aysola, and Johnson]{desai2021redcaps}
Desai, K., Kaul, G., Aysola, Z., and Johnson, J.
\newblock Redcaps: Web-curated image-text data created by the people, for the people.
\newblock \emph{arXiv preprint arXiv:2111.11431}, 2021.

\bibitem[Devillers et~al.(2021)Devillers, Choksi, Bielawski, and VanRullen]{devillers2021does}
Devillers, B., Choksi, B., Bielawski, R., and VanRullen, R.
\newblock Does language help generalization in vision models?
\newblock \emph{arXiv preprint arXiv:2104.08313}, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{North American Chapter of the Association for Computational Linguistics}, 2019.
\newblock \doi{10.18653/v1/N19-1423}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Du et~al.(2021)Du, Li, Su, Zhu, and Lu]{du2021cross}
Du, Z., Li, J., Su, H., Zhu, L., and Lu, K.
\newblock Cross-domain gradient discrepancy minimization for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  3937--3946, 2021.

\bibitem[Dunlap et~al.(2023)Dunlap, Mohri, Guillory, Zhang, Darrell, Gonzalez, Raghunathan, and Rohrbach]{dunlap2023using}
Dunlap, L., Mohri, C., Guillory, D., Zhang, H., Darrell, T., Gonzalez, J.~E., Raghunathan, A., and Rohrbach, A.
\newblock Using language to extend to unseen domains.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=eR2dG8yjnQ}.

\bibitem[El~Banani et~al.(2023)El~Banani, Desai, and Johnson]{el2023learning}
El~Banani, M., Desai, K., and Johnson, J.
\newblock Learning visual representations via language-guided sampling.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  19208--19220, 2023.

\bibitem[French et~al.(2017)French, Mackiewicz, and Fisher]{french2017self}
French, G., Mackiewicz, M., and Fisher, M.
\newblock Self-ensembling for visual domain adaptation.
\newblock \emph{arXiv preprint arXiv:1706.05208}, 2017.

\bibitem[Ganin \& Lempitsky(2015)Ganin and Lempitsky]{DANN}
Ganin, Y. and Lempitsky, V.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In \emph{International conference on machine learning}, pp.\  1180--1189. PMLR, 2015.

\bibitem[Girdhar et~al.(2022)Girdhar, Singh, Ravi, van~der Maaten, Joulin, and Misra]{girdhar2022omnivore}
Girdhar, R., Singh, M., Ravi, N., van~der Maaten, L., Joulin, A., and Misra, I.
\newblock Omnivore: A single model for many visual modalities.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  16102--16112, 2022.

\bibitem[Gokhale et~al.(2021)Gokhale, Anirudh, Kailkhura, Thiagarajan, Baral, and Yang]{gokhale2021attribute}
Gokhale, T., Anirudh, R., Kailkhura, B., Thiagarajan, J.~J., Baral, C., and Yang, Y.
\newblock Attribute-guided adversarial training for robustness to natural perturbations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  7574--7582, 2021.

\bibitem[Goyal et~al.(2022)Goyal, Duval, Seessel, Caron, Misra, Sagun, Joulin, and Bojanowski]{goyal2022vision}
Goyal, P., Duval, Q., Seessel, I., Caron, M., Misra, I., Sagun, L., Joulin, A., and Bojanowski, P.
\newblock Vision models are more robust and fair when pretrained on uncurated images without supervision.
\newblock \emph{arXiv preprint arXiv:2202.08360}, 2022.

\bibitem[Goyal et~al.(2023)Goyal, Kumar, Garg, Kolter, and Raghunathan]{goyal2023finetune}
Goyal, S., Kumar, A., Garg, S., Kolter, Z., and Raghunathan, A.
\newblock Finetune like you pretrain: Improved finetuning of zero-shot vision models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  19338--19347, 2023.

\bibitem[Grauman et~al.(2022)Grauman, Westbury, Byrne, Chavis, Furnari, Girdhar, Hamburger, Jiang, Liu, Liu, et~al.]{grauman2022ego4d}
Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., et~al.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  18995--19012, 2022.

\bibitem[Grauman et~al.(2023)Grauman, Westbury, Torresani, Kitani, Malik, Afouras, Ashutosh, Baiyya, Bansal, Boote, et~al.]{grauman2023ego}
Grauman, K., Westbury, A., Torresani, L., Kitani, K., Malik, J., Afouras, T., Ashutosh, K., Baiyya, V., Bansal, S., Boote, B., et~al.
\newblock Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives.
\newblock \emph{arXiv preprint arXiv:2311.18259}, 2023.

\bibitem[Gu et~al.(2020)Gu, Sun, and Xu]{gu2020spherical}
Gu, X., Sun, J., and Xu, Z.
\newblock Spherical space domain adaptation with robust pseudo-label loss.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  9101--9110, 2020.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016baseline}
Hendrycks, D. and Gimpel, K.
\newblock A baseline for detecting misclassified and out-of-distribution examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Huang et~al.(2024)Huang, Chen, Xu, Zhang, Yang, Pei, Zhang, Dong, Wang, Wang, et~al.]{huang2024egoexolearn}
Huang, Y., Chen, G., Xu, J., Zhang, M., Yang, L., Pei, B., Zhang, H., Dong, L., Wang, Y., Wang, L., et~al.
\newblock Egoexolearn: A dataset for bridging asynchronous ego-and exo-centric view of procedural activities in real world.
\newblock \emph{arXiv preprint arXiv:2403.16182}, 2024.

\bibitem[Huang et~al.(2023)Huang, Zhou, Ling, Cai, Wang, and Lee]{huang2023sentence}
Huang, Z., Zhou, A., Ling, Z., Cai, M., Wang, H., and Lee, Y.~J.
\newblock A sentence speaks a thousand images: Domain generalization through distilling clip with language guidance.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  11685--11695, 2023.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{jia2021scaling}
Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International conference on machine learning}, pp.\  4904--4916. PMLR, 2021.

\bibitem[Kalluri \& Chandraker(2022)Kalluri and Chandraker]{kalluri2022cluster}
Kalluri, T. and Chandraker, M.
\newblock Cluster-to-adapt: Few shot domain adaptation for semantic segmentation across disjoint labels.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  4121--4131, 2022.

\bibitem[Kalluri et~al.(2022)Kalluri, Sharma, and Chandraker]{kalluri2022memsac}
Kalluri, T., Sharma, A., and Chandraker, M.
\newblock Memsac: Memory augmented sample consistency for large scale domain adaptation.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXX}, pp.\  550--568. Springer, 2022.

\bibitem[Kalluri et~al.(2023)Kalluri, Xu, and Chandraker]{kalluri2023geonet}
Kalluri, T., Xu, W., and Chandraker, M.
\newblock Geonet: Benchmarking unsupervised adaptation across geographies.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  15368--15379, June 2023.

\bibitem[Kang et~al.(2019)Kang, Jiang, Yang, and Hauptmann]{kang2019contrastive}
Kang, G., Jiang, L., Yang, Y., and Hauptmann, A.~G.
\newblock Contrastive adaptation network for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  4893--4902, 2019.

\bibitem[Kumar et~al.(2018)Kumar, Sattigeri, Wadhawan, Karlinsky, Feris, Freeman, and Wornell]{kumar2018co}
Kumar, A., Sattigeri, P., Wadhawan, K., Karlinsky, L., Feris, R., Freeman, B., and Wornell, G.
\newblock Co-regularized alignment for unsupervised domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  9345--9356, 2018.

\bibitem[Kumar et~al.(2020)Kumar, Ma, and Liang]{kumar2020understanding}
Kumar, A., Ma, T., and Liang, P.
\newblock Understanding self-training for gradual domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5468--5479. PMLR, 2020.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and Liang]{kumar2022fine}
Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P.
\newblock Fine-tuning can distort pretrained features and underperform out-of-distribution.
\newblock \emph{arXiv preprint arXiv:2202.10054}, 2022.

\bibitem[Lai et~al.(2023)Lai, Vesdapunt, Zhou, Wu, Huynh, Li, Fu, and Chuah]{lai2023padclip}
Lai, Z., Vesdapunt, N., Zhou, N., Wu, J., Huynh, C.~P., Li, X., Fu, K.~K., and Chuah, C.-N.
\newblock Padclip: Pseudo-labeling with adaptive debiasing in clip for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  16155--16165, 2023.

\bibitem[Lai et~al.(2024)Lai, Bai, Zhang, Du, Shan, Yang, Chuah, and Cao]{lai2024empowering}
Lai, Z., Bai, H., Zhang, H., Du, X., Shan, J., Yang, Y., Chuah, C.-N., and Cao, M.
\newblock Empowering unsupervised domain adaptation with large-scale pre-trained vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pp.\  2691--2701, 2024.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip2}
Li, J., Li, D., Savarese, S., and Hoi, S. C.~H.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock \emph{International Conference on Machine Learning}, 2023.
\newblock \doi{10.48550/arXiv.2301.12597}.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Xie, Lv, Liu, Liang, Qin, and Li]{li2021semantic}
Li, S., Xie, M., Lv, F., Liu, C.~H., Liang, J., Qin, C., and Li, W.
\newblock Semantic concentration for domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  9102--9111, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Nagarajan, Xiong, and Grauman]{li2021ego}
Li, Y., Nagarajan, T., Xiong, B., and Grauman, K.
\newblock Ego-exo: Transferring visual representations from third-person to first-person videos.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  6943--6953, 2021{\natexlab{b}}.

\bibitem[Lin et~al.(2022)Lin, Wang, Soldan, Wray, Yan, XU, Gao, Tu, Zhao, Kong, et~al.]{lin2022egocentric}
Lin, K.~Q., Wang, J., Soldan, M., Wray, M., Yan, R., XU, E.~Z., Gao, D., Tu, R.-C., Zhao, W., Kong, W., et~al.
\newblock Egocentric video-language pretraining.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 7575--7586, 2022.

\bibitem[Liu \& Wang(2023)Liu and Wang]{liu2023tdg}
Liu, G. and Wang, Y.
\newblock Tdg: Text-guided domain generalization.
\newblock \emph{arXiv preprint arXiv:2308.09931}, 2023.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Wang, and Long]{liu2021cycle}
Liu, H., Wang, J., and Long, M.
\newblock Cycle self-training for domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 22968--22981, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2023)Liu, Li, Wu, and Lee]{liu2023visual}
Liu, H., Li, C., Wu, Q., and Lee, Y.~J.
\newblock Visual instruction tuning.
\newblock \emph{NEURIPS}, 2023.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  10012--10022, 2021{\natexlab{b}}.

\bibitem[Long et~al.(2017)Long, Zhu, Wang, and Jordan]{long2017deep}
Long, M., Zhu, H., Wang, J., and Jordan, M.~I.
\newblock Deep transfer learning with joint adaptation networks.
\newblock In \emph{International conference on machine learning}, pp.\  2208--2217. PMLR, 2017.

\bibitem[Long et~al.(2018)Long, Cao, Wang, and Jordan]{CDAN}
Long, M., Cao, Z., Wang, J., and Jordan, M.~I.
\newblock Conditional adversarial domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  1640--1650, 2018.

\bibitem[Luo et~al.(2019)Luo, Zheng, Guan, Yu, and Yang]{luo2019taking}
Luo, Y., Zheng, L., Guan, T., Yu, J., and Yang, Y.
\newblock Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  2507--2516, 2019.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li, Bharambe, and Van Der~Maaten]{mahajan2018exploring}
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., and Van Der~Maaten, L.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pp.\  181--196, 2018.

\bibitem[Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and Sivic]{miech2019howto100m}
Miech, A., Zhukov, D., Alayrac, J.-B., Tapaswi, M., Laptev, I., and Sivic, J.
\newblock Howto100m: Learning a text-video embedding by watching hundred million narrated video clips.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  2630--2640, 2019.

\bibitem[Min et~al.(2022)Min, Park, Kim, Park, and Kim]{min2022grounding}
Min, S., Park, N., Kim, S., Park, S., and Kim, J.
\newblock Grounding visual representations with texts for domain generalization.
\newblock In \emph{European Conference on Computer Vision}, pp.\  37--53. Springer, 2022.

\bibitem[Munro \& Damen(2020)Munro and Damen]{munro2020multi}
Munro, J. and Damen, D.
\newblock Multi-modal domain adaptation for fine-grained action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  122--132, 2020.

\bibitem[Ohkawa et~al.(2023)Ohkawa, Yagi, Nishimura, Furuta, Hashimoto, Ushiku, and Sato]{ohkawa2023exo2egodvc}
Ohkawa, T., Yagi, T., Nishimura, T., Furuta, R., Hashimoto, A., Ushiku, Y., and Sato, Y.
\newblock Exo2egodvc: Dense video captioning of egocentric procedural activities using web instructional videos.
\newblock \emph{arXiv preprint arXiv:2311.16444}, 2023.

\bibitem[Park et~al.(2020)Park, Lee, Yoo, Hur, and Yoon]{park2020joint}
Park, C., Lee, J., Yoo, J., Hur, M., and Yoon, S.
\newblock Joint contrastive learning for unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:2006.10297}, 2020.

\bibitem[Pei et~al.(2018)Pei, Cao, Long, and Wang]{pei2018multi}
Pei, Z., Cao, Z., Long, M., and Wang, J.
\newblock Multi-adversarial domain adaptation.
\newblock \emph{arXiv preprint arXiv:1809.02176}, 2018.

\bibitem[Peng et~al.(2017)Peng, Usman, Kaushik, Hoffman, Wang, and Saenko]{peng2017visda}
Peng, X., Usman, B., Kaushik, N., Hoffman, J., Wang, D., and Saenko, K.
\newblock Visda: The visual domain adaptation challenge.
\newblock \emph{arXiv preprint arXiv:1710.06924}, 2017.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and Wang]{peng2019moment}
Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision}, pp.\  1406--1415, 2019.

\bibitem[Pham et~al.(2023)Pham, Dai, Ghiasi, Kawaguchi, Liu, Yu, Yu, Chen, Luong, Wu, et~al.]{pham2023combined}
Pham, H., Dai, Z., Ghiasi, G., Kawaguchi, K., Liu, H., Yu, A.~W., Yu, J., Chen, Y.-T., Luong, M.-T., Wu, Y., et~al.
\newblock Combined scaling for zero-shot transfer learning.
\newblock \emph{Neurocomputing}, 555:\penalty0 126658, 2023.

\bibitem[Plizzari et~al.(2023)Plizzari, Perrett, Caputo, and Damen]{plizzari2023cook}
Plizzari, C., Perrett, T., Caputo, B., and Damen, D.
\newblock What can a cook in italy teach a mechanic in india? action recognition generalisation over scenarios and locations.
\newblock \emph{arXiv preprint arXiv: 2306.08713}, 2023.

\bibitem[Prabhu et~al.(2022)Prabhu, Selvaraju, Hoffman, and Naik]{prabhu2022can}
Prabhu, V., Selvaraju, R.~R., Hoffman, J., and Naik, N.
\newblock Can domain adaptation make object recognition work for everyone?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  3981--3988, 2022.

\bibitem[Quattrocchi et~al.(2023)Quattrocchi, Furnari, Di~Mauro, Giuffrida, and Farinella]{quattrocchi2023synchronization}
Quattrocchi, C., Furnari, A., Di~Mauro, D., Giuffrida, M.~V., and Farinella, G.~M.
\newblock Synchronization is all you need: Exocentric-to-egocentric transfer for temporal action segmentation with unlabeled synchronized video pairs.
\newblock \emph{arXiv preprint arXiv:2312.02638}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pp.\  8748--8763. PMLR, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and Sutskever]{ramesh2021zero}
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8821--8831. PMLR, 2021.

\bibitem[Reimers \& Gurevych(2019)Reimers and Gurevych]{reimers-2019-sentence-bert}
Reimers, N. and Gurevych, I.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing}. Association for Computational Linguistics, 11 2019.
\newblock URL \url{http://arxiv.org/abs/1908.10084}.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Saenko et~al.(2010)Saenko, Kulis, Fritz, and Darrell]{saenko2010adapting}
Saenko, K., Kulis, B., Fritz, M., and Darrell, T.
\newblock Adapting visual category models to new domains.
\newblock In \emph{Computer Vision--ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11}, pp.\  213--226. Springer, 2010.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton, Ghasemipour, Gontijo~Lopes, Karagol~Ayan, Salimans, et~al.]{saharia2022photorealistic}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.~L., Ghasemipour, K., Gontijo~Lopes, R., Karagol~Ayan, B., Salimans, T., et~al.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 36479--36494, 2022.

\bibitem[Sahoo et~al.(2021)Sahoo, Shah, Panda, Saenko, and Das]{sahoo2021contrast}
Sahoo, A., Shah, R., Panda, R., Saenko, K., and Das, A.
\newblock Contrast and mix: Temporal contrastive video domain adaptation with background mixing.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Saito \& Saenko(2021)Saito and Saenko]{saito2021ovanet}
Saito, K. and Saenko, K.
\newblock Ovanet: One-vs-all network for universal domain adaptation.
\newblock \emph{IEEE International Conference on Computer Vision}, 2021.
\newblock \doi{10.1109/ICCV48922.2021.00887}.

\bibitem[Saito et~al.(2017)Saito, Ushiku, Harada, and Saenko]{saito2017adversarial}
Saito, K., Ushiku, Y., Harada, T., and Saenko, K.
\newblock Adversarial dropout regularization.
\newblock \emph{arXiv preprint arXiv:1711.01575}, 2017.

\bibitem[Saito et~al.(2018)Saito, Watanabe, Ushiku, and Harada]{saito2018maximum}
Saito, K., Watanabe, K., Ushiku, Y., and Harada, T.
\newblock Maximum classifier discrepancy for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  3723--3732, 2018.

\bibitem[Saito et~al.(2020)Saito, Kim, Sclaroff, and Saenko]{saito2020universal}
Saito, K., Kim, D., Sclaroff, S., and Saenko, K.
\newblock Universal domain adaptation through self supervision.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 16282--16292, 2020.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
\newblock \emph{NEURIPS}, 2019.

\bibitem[Sariyildiz et~al.(2020)Sariyildiz, Perez, and Larlus]{sariyildiz2020learning}
Sariyildiz, M.~B., Perez, J., and Larlus, D.
\newblock Learning visual representations with caption annotations.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part VIII 16}, pp.\  153--170. Springer, 2020.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25278--25294, 2022.

\bibitem[Sharma et~al.(2021)Sharma, Kalluri, and Chandraker]{sharma2021instance}
Sharma, A., Kalluri, T., and Chandraker, M.
\newblock Instance level affinity-based transfer for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  5361--5371, 2021.

\bibitem[Sigurdsson et~al.(2018)Sigurdsson, Gupta, Schmid, Farhadi, and Alahari]{sigurdsson2018actor}
Sigurdsson, G.~A., Gupta, A., Schmid, C., Farhadi, A., and Alahari, K.
\newblock Actor and observer: Joint modeling of first and third-person videos.
\newblock In \emph{proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  7396--7404, 2018.

\bibitem[Singh et~al.(2022)Singh, Gustafson, Adcock, de~Freitas~Reis, Gedik, Kosaraju, Mahajan, Girshick, Doll{\'a}r, and Van Der~Maaten]{singh2022revisiting}
Singh, M., Gustafson, L., Adcock, A., de~Freitas~Reis, V., Gedik, B., Kosaraju, R.~P., Mahajan, D., Girshick, R., Doll{\'a}r, P., and Van Der~Maaten, L.
\newblock Revisiting weakly supervised pre-training of visual perception models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  804--814, 2022.

\bibitem[Sun \& Saenko(2016)Sun and Saenko]{sun2016deep}
Sun, B. and Saenko, K.
\newblock Deep coral: Correlation alignment for deep domain adaptation.
\newblock In \emph{Computer Vision--ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14}, pp.\  443--450. Springer, 2016.

\bibitem[Sun et~al.(2022)Sun, Lu, Zhang, and Ling]{sun2022safe}
Sun, T., Lu, C., Zhang, T., and Ling, H.
\newblock Safe self-refinement for transformer-based domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  7191--7200, 2022.

\bibitem[Tan et~al.(2020)Tan, Peng, and Saenko]{tan2020class}
Tan, S., Peng, X., and Saenko, K.
\newblock Class-imbalanced domain adaptation: an empirical odyssey.
\newblock In \emph{Computer Vision--ECCV 2020 Workshops: Glasgow, UK, August 23--28, 2020, Proceedings, Part I 16}, pp.\  585--602. Springer, 2020.

\bibitem[Thomee et~al.(2016)Thomee, Shamma, Friedland, Elizalde, Ni, Poland, Borth, and Li]{thomee2016yfcc100m}
Thomee, B., Shamma, D.~A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J.
\newblock Yfcc100m: The new data in multimedia research.
\newblock \emph{Communications of the ACM}, 59\penalty0 (2):\penalty0 64--73, 2016.

\bibitem[Tzeng et~al.(2015)Tzeng, Hoffman, Darrell, and Saenko]{tzeng2015simultaneous}
Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K.
\newblock Simultaneous deep transfer across domains and tasks.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  4068--4076, 2015.

\bibitem[Tzeng et~al.(2017)Tzeng, Hoffman, Saenko, and Darrell]{tzeng2017adversarial}
Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T.
\newblock Adversarial discriminative domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  7167--7176, 2017.

\bibitem[Udandarao et~al.(2023)Udandarao, Gupta, and Albanie]{udandarao2023sus}
Udandarao, V., Gupta, A., and Albanie, S.
\newblock Sus-x: Training-free name-only transfer of vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  2725--2736, 2023.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and Panchanathan]{venkateswara2017deep}
Venkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  5018--5027, 2017.

\bibitem[Wang et~al.(2022)Wang, Wu, Weng, Chen, Qi, and Jiang]{wang2022cross}
Wang, R., Wu, Z., Weng, Z., Chen, J., Qi, G.-J., and Jiang, Y.-G.
\newblock Cross-domain contrastive learning for unsupervised domain adaptation.
\newblock \emph{IEEE Transactions on Multimedia}, 2022.

\bibitem[Wang et~al.(2024)Wang, Zhang, Wang, and Zhu]{wang2024landa}
Wang, Z., Zhang, L., Wang, L., and Zhu, M.
\newblock Landa: Language-guided multi-source domain adaptation.
\newblock \emph{arXiv preprint arXiv:2401.14148}, 2024.

\bibitem[Wei et~al.(2021)Wei, Lan, Zeng, Zhang, and Chen]{wei2021toalign}
Wei, G., Lan, C., Zeng, W., Zhang, Z., and Chen, Z.
\newblock Toalign: Task-oriented alignment for unsupervised domain adaptation.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Wei et~al.(2023)Wei, Kong, Qu, Ren, Xu, Jiang, and Yin]{wei2022unsupervised}
Wei, P., Kong, L., Qu, X., Ren, Y., Xu, Z., Jiang, J., and Yin, X.
\newblock Unsupervised video domain adaptation for action recognition: A disentanglement perspective.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Kim, Li, Kornblith, Roelofs, Lopes, Hajishirzi, Farhadi, Namkoong, et~al.]{wortsman2022robust}
Wortsman, M., Ilharco, G., Kim, J.~W., Li, M., Kornblith, S., Roelofs, R., Lopes, R.~G., Hajishirzi, H., Farhadi, A., Namkoong, H., et~al.
\newblock Robust fine-tuning of zero-shot models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  7959--7971, 2022.

\bibitem[Xie et~al.(2018)Xie, Zheng, Chen, and Chen]{xie2018learning}
Xie, S., Zheng, Z., Chen, L., and Chen, C.
\newblock Learning semantic representations for unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5423--5432, 2018.

\bibitem[Xu et~al.(2024)Xu, Huang, Hou, Chen, Zhang, Feng, and Xie]{xu2024retrieval}
Xu, J., Huang, Y., Hou, J., Chen, G., Zhang, Y., Feng, R., and Xie, W.
\newblock Retrieval-augmented egocentric video captioning.
\newblock \emph{arXiv preprint arXiv:2401.00789}, 2024.

\bibitem[Xu et~al.(2019)Xu, Li, Yang, and Lin]{xu2019larger}
Xu, R., Li, G., Yang, J., and Lin, L.
\newblock Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision}, pp.\  1426--1435, 2019.

\bibitem[Xu et~al.(2021)Xu, Chen, Wang, Wang, Li, and Jin]{xu2021cdtrans}
Xu, T., Chen, W., Wang, P., Wang, F., Li, H., and Jin, R.
\newblock Cdtrans: Cross-domain transformer for unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:2109.06165}, 2021.

\bibitem[Xue \& Grauman(2023)Xue and Grauman]{xue2024learning}
Xue, Z.~S. and Grauman, K.
\newblock Learning fine-grained view-invariant representations from unpaired ego-exo videos via temporal alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[You et~al.(2019)You, Long, Cao, Wang, and Jordan]{you2019universal}
You, K., Long, M., Cao, Z., Wang, J., and Jordan, M.~I.
\newblock Universal domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  2720--2729, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Gu, Matsuo, and Iwasawa]{zhang2023domain}
Zhang, X., Gu, S.~S., Matsuo, Y., and Iwasawa, Y.
\newblock Domain prompt learning for efficiently adapting clip to unseen domains.
\newblock \emph{Transactions of the Japanese Society for Artificial Intelligence}, 38\penalty0 (6):\penalty0 B--MC2\_1, 2023.

\bibitem[Zhang et~al.(2019)Zhang, Liu, Long, and Jordan]{zhang2019bridging}
Zhang, Y., Liu, T., Long, M., and Jordan, M.~I.
\newblock Bridging theory and algorithm for domain adaptation.
\newblock \emph{arXiv preprint arXiv:1904.05801}, 2019.

\bibitem[Zhao et~al.(2023)Zhao, Misra, Kr{\"a}henb{\"u}hl, and Girdhar]{zhao2023learning}
Zhao, Y., Misra, I., Kr{\"a}henb{\"u}hl, P., and Girdhar, R.
\newblock Learning video representations from large language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  6586--6597, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Bai, and Wang]{zhu2023patch}
Zhu, J., Bai, H., and Wang, L.
\newblock Patch-mix transformer for unsupervised domain adaptation: A game perspective.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  3561--3571, 2023.

\end{thebibliography}
