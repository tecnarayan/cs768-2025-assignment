\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akhtar \& Rajawat(2021)Akhtar and Rajawat]{9483167}
Akhtar, Z. and Rajawat, K.
\newblock Momentum based projection free stochastic optimization under affine
  constraints.
\newblock In \emph{2021 American Control Conference (ACC)}, pp.\  2619--2624,
  2021.
\newblock \doi{10.23919/ACC50511.2021.9483167}.

\bibitem[Allen-Zhu(2018)]{allen2018katyusha}
Allen-Zhu, Z.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (221):\penalty0 1--51, 2018.

\bibitem[Allen-Zhu \& Yuan(2016)Allen-Zhu and Yuan]{allen2016improved}
Allen-Zhu, Z. and Yuan, Y.
\newblock Improved svrg for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In \emph{International conference on machine learning}, pp.\
  1080--1089. PMLR, 2016.

\bibitem[Bach(2011)]{bachSub2011}
Bach, F.
\newblock Learning with submodular functions: A convex optimization
  perspective, 2011.
\newblock URL \url{https://arxiv.org/abs/1111.6453}.

\bibitem[Bellet et~al.(2015)Bellet, Liang, Garakani, Balcan, and
  Sha]{bellet2015distributed}
Bellet, A., Liang, Y., Garakani, A.~B., Balcan, M.-F., and Sha, F.
\newblock A distributed frank-wolfe algorithm for communication-efficient
  sparse learning.
\newblock In \emph{Proceedings of the 2015 SIAM international conference on
  data mining}, pp.\  478--486. SIAM, 2015.

\bibitem[Bojanowski et~al.(2014)Bojanowski, Lajugie, Bach, Laptev, Ponce,
  Schmid, and Sivic]{bojanowski2014weakly}
Bojanowski, P., Lajugie, R., Bach, F., Laptev, I., Ponce, J., Schmid, C., and
  Sivic, J.
\newblock Weakly supervised action labeling in videos under ordering
  constraints.
\newblock In \emph{European Conference on Computer Vision}, pp.\  628--643.
  Springer, 2014.

\bibitem[Braun et~al.(2022)Braun, Carderera, Combettes, Hassani, Karbasi,
  Mokhtari, and Pokutta]{braun2022conditional}
Braun, G., Carderera, A., Combettes, C.~W., Hassani, H., Karbasi, A., Mokhtari,
  A., and Pokutta, S.
\newblock Conditional gradient methods.
\newblock \emph{arXiv preprint arXiv:2211.14103}, 2022.

\bibitem[Chang \& Lin(2011)Chang and Lin]{chang2011libsvm}
Chang, C.-C. and Lin, C.-J.
\newblock Libsvm: a library for support vector machines.
\newblock \emph{ACM transactions on intelligent systems and technology (TIST)},
  2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky2019momentum}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{arXiv preprint arXiv:1905.10018}, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Demianov \& Rubinov(1970)Demianov and
  Rubinov]{demianov1970approximate}
Demianov, V.~F. and Rubinov, A.~M.
\newblock \emph{Approximate methods in optimization problems}.
\newblock Number~32. Elsevier Publishing Company, 1970.

\bibitem[Dunn \& Harshbarger(1978)Dunn and Harshbarger]{dunn1978conditional}
Dunn, J.~C. and Harshbarger, S.
\newblock Conditional gradient algorithms with open loop step size rules.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 62\penalty0
  (2):\penalty0 432--444, 1978.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock \emph{arXiv preprint arXiv:1807.01695}, 2018.

\bibitem[Frank \& Wolfe(1956)Frank and Wolfe]{frank1956algorithm}
Frank, M. and Wolfe, P.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Gao \& Huang(2020)Gao and Huang]{pmlr-v119-gao20b}
Gao, H. and Huang, H.
\newblock Can stochastic zeroth-order frank-{W}olfe method converge faster for
  non-convex problems?
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3377--3386. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/gao20b.html}.

\bibitem[Garber \& Hazan(2015)Garber and Hazan]{garber2015fast}
Garber, D. and Hazan, E.
\newblock Fast and simple pca via convex optimization.
\newblock \emph{arXiv preprint arXiv:1509.05647}, 2015.

\bibitem[Gorbunov et~al.(2021)Gorbunov, Burlachenko, Li, and
  Richtarik]{pmlr-v139-gorbunov21a}
Gorbunov, E., Burlachenko, K.~P., Li, Z., and Richtarik, P.
\newblock Marina: Faster non-convex distributed learning with compression.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3788--3798. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/gorbunov21a.html}.

\bibitem[Han et~al.(2024)Han, Xie, and Zhang]{han2024lower}
Han, Y., Xie, G., and Zhang, Z.
\newblock Lower complexity bounds of finite-sum optimization problems: The
  results and construction.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0
  (2):\penalty0 1--86, 2024.

\bibitem[Hazan \& Kale(2012)Hazan and Kale]{hazan2012projection}
Hazan, E. and Kale, S.
\newblock Projection-free online learning.
\newblock \emph{arXiv preprint arXiv:1206.4657}, 2012.

\bibitem[Hazan \& Luo(2016)Hazan and Luo]{hazan2016variance}
Hazan, E. and Luo, H.
\newblock Variance-reduced and projection-free stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1263--1271. PMLR, 2016.

\bibitem[Hou et~al.(2022)Hou, Zeng, Wang, Sun, and Chen]{hou2022distributed}
Hou, J., Zeng, X., Wang, G., Sun, J., and Chen, J.
\newblock Distributed momentum-based frank-wolfe algorithm for stochastic
  optimization.
\newblock \emph{IEEE/CAA Journal of Automatica Sinica}, 2022.

\bibitem[Hu et~al.(2019)Hu, Li, Lian, Liu, and Yuan]{hu2019efficient}
Hu, W., Li, C.~J., Lian, X., Liu, J., and Yuan, H.
\newblock Efficient smooth non-convex stochastic compositional optimization via
  stochastic recursive gradient descent.
\newblock 2019.

\bibitem[Jaggi(2013)]{pmlr-v28-jaggi13}
Jaggi, M.
\newblock Revisiting {Frank-Wolfe}: Projection-free sparse convex optimization.
\newblock In Dasgupta, S. and McAllester, D. (eds.), \emph{Proceedings of the
  30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  427--435, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/jaggi13.html}.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{NIPS2013_ac1dd209}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and
  Weinberger, K. (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc., 2013.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2013/file/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Paper.pdf}.

\bibitem[Kovalev et~al.(2020)Kovalev, Horv{\'a}th, and
  Richt{\'a}rik]{kovalev2020don}
Kovalev, D., Horv{\'a}th, S., and Richt{\'a}rik, P.
\newblock Don’t jump through hoops and remove those loops: Svrg and katyusha
  are better without the outer loop.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  451--467. PMLR, 2020.

\bibitem[Krishnan et~al.(2015)Krishnan, Lacoste-Julien, and
  Sontag]{krishnan2015barrier}
Krishnan, R.~G., Lacoste-Julien, S., and Sontag, D.
\newblock Barrier frank-wolfe for marginal inference.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Lacoste-Julien(2016)]{lacoste2016convergence}
Lacoste-Julien, S.
\newblock Convergence rate of frank-wolfe for non-convex objectives.
\newblock \emph{arXiv preprint arXiv:1607.00345}, 2016.

\bibitem[Lacoste-Julien \& Jaggi(2015)Lacoste-Julien and
  Jaggi]{lacoste2015global}
Lacoste-Julien, S. and Jaggi, M.
\newblock On the global linear convergence of frank-wolfe optimization
  variants.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Lan \& Zhou(2016)Lan and Zhou]{doi:10.1137/140992382}
Lan, G. and Zhou, Y.
\newblock Conditional gradient sliding for convex optimization.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (2):\penalty0
  1379--1409, 2016.
\newblock \doi{10.1137/140992382}.
\newblock URL \url{https://doi.org/10.1137/140992382}.

\bibitem[Levitin \& Polyak(1966)Levitin and Polyak]{levitin1966constrained}
Levitin, E.~S. and Polyak, B.~T.
\newblock Constrained minimization methods.
\newblock \emph{USSR Computational mathematics and mathematical physics},
  6\penalty0 (5):\penalty0 1--50, 1966.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Bao, Zhang, and
  Richtarik]{pmlr-v139-li21a}
Li, Z., Bao, H., Zhang, X., and Richtarik, P.
\newblock Page: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  6286--6295. PMLR,
  18--24 Jul 2021{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v139/li21a.html}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Hanzely, and
  Richt{\'a}rik]{li2021zerosarah}
Li, Z., Hanzely, S., and Richt{\'a}rik, P.
\newblock Zerosarah: Efficient nonconvex finite-sum optimization with zero full
  gradient computation.
\newblock \emph{arXiv preprint arXiv:2103.01447}, 2021{\natexlab{b}}.

\bibitem[Lu \& Freund(2021)Lu and Freund]{lu2021generalized}
Lu, H. and Freund, R.~M.
\newblock Generalized stochastic frank--wolfe algorithm with stochastic
  “substitute” gradient for structured convex optimization.
\newblock \emph{Mathematical Programming}, 187\penalty0 (1):\penalty0 317--349,
  2021.

\bibitem[Mairal(2015)]{mairal2015incremental}
Mairal, J.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  829--855, 2015.

\bibitem[Miech et~al.(2017)Miech, Alayrac, Bojanowski, Laptev, and
  Sivic]{miech2017learning}
Miech, A., Alayrac, J.-B., Bojanowski, P., Laptev, I., and Sivic, J.
\newblock Learning from video and text via large-scale discriminative
  clustering.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  5257--5266, 2017.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Hassani, and
  Karbasi]{mokhtari2020stochastic}
Mokhtari, A., Hassani, H., and Karbasi, A.
\newblock Stochastic conditional gradient methods: From convex minimization to
  submodular maximization.
\newblock \emph{Journal of machine learning research}, 2020.

\bibitem[N{\'e}giar et~al.(2020)N{\'e}giar, Dresdner, Tsai, El~Ghaoui,
  Locatello, Freund, and Pedregosa]{negiar2020stochastic}
N{\'e}giar, G., Dresdner, G., Tsai, A., El~Ghaoui, L., Locatello, F., Freund,
  R., and Pedregosa, F.
\newblock Stochastic frank-wolfe for constrained finite-sum minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7253--7262. PMLR, 2020.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Nguyen et~al.(2017{\natexlab{a}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock {SARAH:} a novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2613--2621. PMLR, 2017{\natexlab{a}}.

\bibitem[Nguyen et~al.(2017{\natexlab{b}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017stochastic}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Stochastic recursive gradient algorithm for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1705.07261}, 2017{\natexlab{b}}.

\bibitem[Nguyen et~al.(2021)Nguyen, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2021inexact}
Nguyen, L.~M., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Inexact {SARAH} algorithm for stochastic optimization.
\newblock \emph{Optimization Methods and Software}, 36\penalty0 (1):\penalty0
  237--258, 2021.

\bibitem[Patriksson(1993)]{patriksson1993partial}
Patriksson, M.
\newblock Partial linearization methods in nonlinear programming.
\newblock \emph{Journal of Optimization Theory and Applications}, 78\penalty0
  (2):\penalty0 227--246, 1993.

\bibitem[Qian et~al.(2019)Qian, Qu, and Richt{\'a}rik]{qian2019saga}
Qian, X., Qu, Z., and Richt{\'a}rik, P.
\newblock Saga with arbitrary sampling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5190--5199. PMLR, 2019.

\bibitem[Qu et~al.(2018)Qu, Li, and Xu]{pmlr-v80-qu18a}
Qu, C., Li, Y., and Xu, H.
\newblock Non-convex conditional gradient sliding.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  4208--4217. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/qu18a.html}.

\bibitem[Reddi et~al.(2016)Reddi, Sra, P{\'o}czos, and
  Smola]{reddi2016stochastic}
Reddi, S.~J., Sra, S., P{\'o}czos, B., and Smola, A.
\newblock Stochastic frank-wolfe methods for nonconvex optimization.
\newblock In \emph{2016 54th annual Allerton conference on communication,
  control, and computing (Allerton)}, pp.\  1244--1251. IEEE, 2016.

\bibitem[Richtarik et~al.(2021)Richtarik, Sokolov, and
  Fatkhullin]{NEURIPS2021_231141b3}
Richtarik, P., Sokolov, I., and Fatkhullin, I.
\newblock Ef21: A new, simpler, theoretically better, and practically faster
  error feedback.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  4384--4396. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/231141b34c82aa95e48810a9d1b33a79-Paper.pdf}.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Schmidt, M., Le~Roux, N., and Bach, F.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0
  83--112, 2017.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shamir(2015)]{shamir2015stochastic}
Shamir, O.
\newblock A stochastic pca and svd algorithm with an exponential convergence
  rate.
\newblock In \emph{International conference on machine learning}, pp.\
  144--152. PMLR, 2015.

\bibitem[Shen et~al.(2019)Shen, Fang, Zhao, Huang, and Qian]{pmlr-v89-shen19b}
Shen, Z., Fang, C., Zhao, P., Huang, J., and Qian, H.
\newblock Complexities in projection-free stochastic non-convex minimization.
\newblock In Chaudhuri, K. and Sugiyama, M. (eds.), \emph{Proceedings of the
  Twenty-Second International Conference on Artificial Intelligence and
  Statistics}, volume~89 of \emph{Proceedings of Machine Learning Research},
  pp.\  2868--2876. PMLR, 16--18 Apr 2019.
\newblock URL \url{https://proceedings.mlr.press/v89/shen19b.html}.

\bibitem[Stich(2019)]{stich2019unified}
Stich, S.~U.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock \emph{arXiv preprint arXiv:1907.04232}, 2019.

\bibitem[Weber \& Sra(2022)Weber and Sra]{weber2022projection}
Weber, M. and Sra, S.
\newblock Projection-free nonconvex stochastic optimization on riemannian
  manifolds.
\newblock \emph{IMA Journal of Numerical Analysis}, 42\penalty0 (4):\penalty0
  3241--3271, 2022.

\bibitem[Yang et~al.(2021)Yang, Chen, and Wang]{yang2021accelerating}
Yang, Z., Chen, Z., and Wang, C.
\newblock Accelerating mini-batch {SARAH} by step size rules.
\newblock \emph{Information Sciences}, 558:\penalty0 157--173, 2021.

\bibitem[Yurtsever et~al.(2019)Yurtsever, Sra, and
  Cevher]{pmlr-v97-yurtsever19b}
Yurtsever, A., Sra, S., and Cevher, V.
\newblock Conditional gradient methods via stochastic path-integrated
  differential estimator.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  7282--7291. PMLR,
  09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/yurtsever19b.html}.

\bibitem[Zhang et~al.(2020)Zhang, Shen, Mokhtari, Hassani, and
  Karbasi]{zhang2020one}
Zhang, M., Shen, Z., Mokhtari, A., Hassani, H., and Karbasi, A.
\newblock One sample stochastic frank-wolfe.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4012--4023. PMLR, 2020.

\end{thebibliography}
