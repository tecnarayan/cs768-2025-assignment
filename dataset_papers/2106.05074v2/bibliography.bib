% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Edo Airoldi at 2007-02-02 15:02:49 -0500 


%% Saved with string encoding Western (ASCII) 

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={J. Royal Stat. Soc. Ser. B Methodol.},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}


@article{eberhardt2007interventions,
  title={Interventions and causal inference},
  author={Eberhardt, Frederick and Scheines, Richard},
  journal={Philos. Sci.},
  volume={74},
  number={5},
  pages={981--995},
  year={2007},
  publisher={The University of Chicago Press}
}


@article{huang2020causal,
  title={Causal discovery from heterogeneous/nonstationary data},
  author={Huang, Biwei and Zhang, Kun and Zhang, Jiji and Ramsey, Joseph and Sanchez-Romero, Ruben and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
  journal={JMLR},
  volume={21},
  number={89},
  pages={1--53},
  year={2020}
}


@inproceedings{kocaoglu2019characterization,
  title={Characterization and learning of causal graphs with latent variables from soft interventions},
  author={Kocaoglu, Murat and Jaber, Amin and Shanmugam, Karthikeyan and Bareinboim, Elias},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14369--14379},
  year={2019}
}

@unpublished{ke2019learning,
  title={Learning neural causal models from unknown interventions},
  author={Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Sch{\"o}lkopf, Bernhard and Mozer, Michael C and Pal, Chris and Bengio, Yoshua},
  note={\emph{arXiv} preprint, 1910.01075},
  year={2019}
}
@unpublished{kaddour2021,
  title={Graph Intervention Networks for Causal Effect Estimation},
  author={Jean Kaddour and Qi Liu and Yuchen Zhu and Matt J. Kusner and Ricardo Silva},
  note={\emph{arXiv} preprint, 2106.01939},
  year={2021}
}

@inproceedings{rothenhausler2015backshift,
  title={BACKSHIFT: Learning causal cyclic graphs from unknown shift interventions},
  author={Rothenh{\"a}usler, Dominik and Heinze, Christina and Peters, Jonas and Meinshausen, Nicolai},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1513--1521},
  year={2015}
}

@inproceedings{brouillard2020differentiable,
  title={Differentiable Causal Discovery from Interventional Data},
  author={Brouillard, Philippe and Lachapelle, S{\'e}bastien and Lacoste, Alexandre and Lacoste-Julien, Simon and Drouin, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}


@inproceedings{jaber2020causal,
  title={Causal Discovery from Soft Interventions with Unknown Targets: Characterization and Learning},
  author={Jaber, Amin and Kocaoglu, Murat and Shanmugam, Karthikeyan and Bareinboim, Elias},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@InProceedings{eaton2007exact,
  title = 	 {{Exact Bayesian structure learning from uncertain interventions}},
  author = 	 {Daniel Eaton and Kevin Murphy},
  pages = 	 {107--114},
  year = 	 {2007},
  editor = 	 {Marina Meila and Xiaotong Shen},
  volume = 	 {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Juan, Puerto Rico},
  month = 	 {21--24 Mar},
  publisher =    {PMLR},
  abstract = 	 {We show how to apply the dynamic programming algorithm of Koivisto and Sood [KS04, Koi06], which computes the exact posterior marginal edge probabilities p(G_ij = 1|D) of a DAG G given data D, to the case where the data is obtained by interventions (experiments). In particular, we consider the case where the targets of the interventions are a priori unknown. We show that it is possible to learn the targets of intervention at the same time as learning the causal structure. We apply our exact technique to a biological data set that had previously been analyzed using MCMC [SPP+ 05, EW06, WGH06].}
}

@unpublished{arjovsky2020invariant,
      title={Invariant Risk Minimization}, 
      author={Martin Arjovsky and Léon Bottou and Ishaan Gulrajani and David Lopez-Paz},
      year={2020},
      note={\emph{arXiv} preprint, 1907.02893}
}
@book{Pearl2000,
address = {New York},
author = {Pearl, Judea},
publisher = {Cambridge University Press},
title = {{Causality: Models, Reasoning, and Inference}},
year = {2000}
}
@book{Spirtes2000,
address = {New York},
author = {Spirtes, Peter and Glymour, Clark N. and Scheines, Richard},
publisher = {Springer-Verlag},
title = {{Causation, Prediction, and Search}},
year = {1993}
}
@article{Rubin1974,
author = {Rubin, Donald B.},
journal = {Journal of Educational Psychology},
number = {5},
pages = {688--701},
title = {{Estimating causal effects of treatments in randomized and nonrandomized studies}},
volume = {66},
year = {1974}
}
@inproceedings{zheng_dags_tears,
abstract = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.},
address = {Red Hook, NY, USA},
author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {9492--9503},
publisher = {Curran Associates Inc.},
series = {NIPS'18},
title = {{DAGs with NO TEARS: Continuous Optimization for Structure Learning}},
year = {2018}
}
@inproceedings{gultchin20a,
abstract = {Discovering the causal effect of a decision is critical to nearly all forms of decision-making. In particular, it is a key quantity in drug development, in crafting government policy, and when implementing a real-world machine learning system. Given only observational data, confounders often obscure the true causal effect. Luckily, in some cases, it is possible to recover the causal effect by using certain observed variables to adjust for the effects of confounders. However, without access to the true causal model, finding this adjustment requires brute-force search. In this work, we present an algorithm that exploits auxiliary variables, similar to instruments, in order to find an appropriate adjustment by a gradient-based optimization method. We demonstrate that it outperforms practical alternatives in estimating the true causal effect, without knowledge of the full causal graph.},
address = {Online},
author = {Gultchin, Limor and Kusner, Matt and Kanade, Varun and Silva, Ricardo},
editor = {Chiappa, Silvia and Calandra, Roberto},
pages = {3970--3979},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Differentiable Causal Backdoor Discovery}},
volume = {108},
year = {2020}
}
@article{Chernozhukov2018,
annote = {doi: 10.1111/ectj.12097},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
issn = {1368-4221},
journal = {The Econometrics Journal},
month = {feb},
number = {1},
pages = {C1--C68},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Hahn2020,
abstract = {This paper presents a novel nonlinear regression model for estimating heterogeneous treatment effects, geared specifically towards situations with small effect sizes, heterogeneous effects, and strong confounding by observables. Standard nonlinear regression models, which may work quite well for prediction, have two notable weaknesses when used to estimate heterogeneous treatment effects. First, they can yield badly biased estimates of treatment effects when fit to data with strong confounding. The Bayesian causal forest model presented in this paper avoids this problem by directly incorporating an estimate of the propensity function in the specification of the response model, implicitly inducing a covariate-dependent prior on the regression function. Second, standard approaches to response surface modeling do not provide adequate control over the strength of regularization over effect heterogeneity. The Bayesian causal forest model permits treatment effect heterogeneity to be regularized separately from the prognostic effect of control variables, making it possible to informatively "shrink to homogeneity". While we focus on observational data, our methods are equally useful for inferring heterogeneous treatment effects from randomized controlled experiments where careful regularization is somewhat less complicated but no less important. We illustrate these benefits via the reanalysis of an observational study assessing the causal effects of smoking on medical expenditures as well as extensive simulation studies.},
author = {Hahn, P Richard and Murray, Jared S and Carvalho, Carlos M},
doi = {10.1214/19-BA1195},
isbn = {1936-0975},
journal = {Bayesian Anal.},
keywords = {Bayesian,causal inference,heterogeneous treatment effects,machine learning,predictor-dependent priors,regression trees,regularization,shrinkage},
language = {en},
number = {3},
pages = {965--1056},
publisher = {International Society for Bayesian Analysis},
title = {{Bayesian Regression Tree Models for Causal Inference: Regularization, Confounding, and Heterogeneous Effects}},
volume = {15},
year = {2020}
}
@misc{nie2017quasioracle,
    title={Quasi-Oracle Estimation of Heterogeneous Treatment Effects},
    author={Xinkun Nie and Stefan Wager},
    year={2017},
    eprint={1712.04912},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@book{VanderLaan2018,
address = {New York},
editor = {van der Laan, Mark J. and Rose, Sherri},
publisher = {Springer},
title = {{Targeted Learning in Data Science: Causal Inference for Complex Longitudinal Studies}},
year = {2018}
}

@inproceedings{hossain-etal-2019-president,
    title = "{``}President Vows to Cut {\textless}Taxes{\textgreater} Hair{''}: Dataset and Analysis of Creative Text Editing for Humorous Headlines",
    author = "Hossain, Nabil  and
      Krumm, John  and
      Gamon, Michael",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2019",
    pages = "133--142",
    abstract = "We introduce, release, and analyze a new dataset, called Humicroedit, for research in computational humor. Our publicly available data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. We carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline. The simple edits, usually just a single word replacement, mean we can apply straightforward analysis techniques to determine what makes our edited headlines humorous. We show how the data support classic theories of humor, such as incongruity, superiority, and setup/punchline. Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor.",
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543}
}
@book{Woodward2003a,
address = {New York},
author = {Woodward, James},
publisher = {Oxford University Press},
title = {{Making Things Happen: A Theory of Causal Explanation}},
year = {2003}
}
@article{Heinze2018,
address = {Berlin, Boston},
author = {Heinze-Deml, Christina and Peters, Jonas and Meinshausen, Nicolai},
journal = {J. Causal Inference},
number = {2},
publisher = {De Gruyter},
title = {{Invariant Causal Prediction for Nonlinear Models}},
volume = {6},
year = {2018}
}
@article{Lei2018,
author = {Lei, Jing and G'Sell, Max and Rinaldo, Alessandro and Tibshirani, Ryan J and Wasserman, Larry},
journal = {J. Am. Stat. Assoc.},
number = {523},
pages = {1094--1111},
publisher = {Taylor {\&} Francis},
title = {{Distribution-Free Predictive Inference for Regression}},
volume = {113},
year = {2018}
}
@article{Shah2018,
archivePrefix = {arXiv},
arxivId = {1804.07203v1},
author = {Shah, Rajen and Peters, Jonas},
eprint = {1804.07203v1},
journal = {Ann. Statist.},
number = {3},
pages = {1514--1538},
title = {{The Hardness of Conditional Independence Testing and the Generalised Covariance Measure}},
volume = {48},
year = {2020}
}
@article{Holm1979,
abstract = {This paper presents a simple and widely applicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a time until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
author = {Holm, Sture},
journal = {Scand. Stat. Theory Appl.},
number = {2},
pages = {65--70},
publisher = {[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]},
title = {{A Simple Sequentially Rejective Multiple Test Procedure}},
volume = {6},
year = {1979}
}

@article{peters2016,
author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
title = {Causal inference by using invariant prediction: identification and confidence intervals},
journal = {J. Royal Stat. Soc. Ser. B Methodol.},
volume = {78},
number = {5},
pages = {947-1012},
keywords = {Causal discovery, Causal inference, Confidence intervals, Invariant prediction},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12167},
abstract = {Summary What is the difference between a prediction that is made with a causal model and that with a non-causal model? Suppose that we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (e.g. various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
year = {2016}
}
@article{Buhlmann2020,
abstract = {We discuss recent work for causal inference and predictive robustness in a unifying way. The key idea relies on a notion of probabilistic invariance or stability: it opens up new insights for formulating causality as a certain risk minimization problem with a corresponding notion of robustness. The invariance itself can be estimated from general heterogeneous or perturbation data which frequently occur with nowadays data collection. The novel methodology is potentially useful in many applications, offering more robustness and better "causal-oriented" interpretation than machine learning or estimation in standard regression or classification frameworks.},
author = {B{\"{u}}hlmann, Peter},
journal = {Statist. Sci.},
keywords = {Anchor regression,Random Forests,causal regularization,distributional robustness,heterogeneous data,instrumental variables regression,interventional data,variable importance},
language = {en},
number = {3},
pages = {404--426},
publisher = {The Institute of Mathematical Statistics},
title = {{Invariance, Causality and Robustness}},
volume = {35},
year = {2020}
}
@inproceedings{locatello19a,
abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than {\$}12000{\$} models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
address = {Long Beach, California, USA},
author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"{o}}lkopf, Bernhard and Bachem, Olivier},
editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
pages = {4114--4124},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Challenging common assumptions in the unsupervised learning of disentangled representations}},
volume = {97},
year = {2019}
}
@article{Marbach2012,
abstract = {Reconstructing gene regulatory networks from high-throughput data is a long-standing challenge. Through the Dialogue on Reverse Engineering Assessment and Methods (DREAM) project, we performed a comprehensive blind assessment of over 30 network inference methods on Escherichia coli, Staphylococcus aureus, Saccharomyces cerevisiae and in silico microarray data. We characterize the performance, data requirements and inherent biases of different inference approaches, and we provide guidelines for algorithm application and development. We observed that no single inference method performs optimally across all data sets. In contrast, integration of predictions from multiple inference methods shows robust and high performance across diverse data sets. We thereby constructed high-confidence networks for E. coli and S. aureus, each comprising {\~{}}1,700 transcriptional interactions at a precision of {\~{}}50{\%}. We experimentally tested 53 previously unobserved regulatory interactions in E. coli, of which 23 (43{\%}) were supported. Our results establish community-based methods as a powerful and robust tool for the inference of transcriptional gene regulatory networks.},
author = {Marbach, Daniel and Costello, James C and K{\"{u}}ffner, Robert and Vega, Nicole M and Prill, Robert J and Camacho, Diogo M and Allison, Kyle R and Aderhold, Andrej and Allison, Kyle R and Bonneau, Richard ... and Stolovitzky, Gustavo},
journal = {Nat. Methods},
number = {8},
pages = {796--804},
title = {{Wisdom of crowds for robust gene network inference}},
volume = {9},
year = {2012}
}
@article{Huynh-Thu2010,
abstract = {One of the pressing open problems of computational systems biology is the elucidation of the topology of genetic regulatory networks (GRNs) using high throughput genomic data, in particular microarray gene expression data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenge aims to evaluate the success of GRN inference algorithms on benchmarks of simulated data. In this article, we present GENIE3, a new algorithm for the inference of GRNs that was best performer in the DREAM4 In Silico Multifactorial challenge. GENIE3 decomposes the prediction of a regulatory network between p genes into p different regression problems. In each of the regression problems, the expression pattern of one of the genes (target gene) is predicted from the expression patterns of all the other genes (input genes), using tree-based ensemble methods Random Forests or Extra-Trees. The importance of an input gene in the prediction of the target gene expression pattern is taken as an indication of a putative regulatory link. Putative regulatory links are then aggregated over all genes to provide a ranking of interactions from which the whole network is reconstructed. In addition to performing well on the DREAM4 In Silico Multifactorial challenge simulated data, we show that GENIE3 compares favorably with existing algorithms to decipher the genetic regulatory network of Escherichia coli. It doesn't make any assumption about the nature of gene regulation, can deal with combinatorial and non-linear interactions, produces directed GRNs, and is fast and scalable. In conclusion, we propose a new algorithm for GRN inference that performs well on both synthetic and real gene expression data. The algorithm, based on feature selection with tree-based ensemble methods, is simple and generic, making it adaptable to other types of genomic data and interactions.},
author = {Huynh-Thu, V{\^{a}}n Anh and Irrthum, Alexandre and Wehenkel, Louis and Geurts, Pierre},
journal = {PLoS ONE},
number = {9},
pages = {1--10},
pmid = {20927193},
title = {{Inferring regulatory networks from expression data using tree-based methods}},
volume = {5},
year = {2010}
}
@article{Ruyssinck2014,
abstract = {One of the long-standing open challenges in computational systems biology is the topology inference of gene regulatory networks from high-throughput omics data. Recently, two community-wide efforts, DREAM4 and DREAM5, have been established to benchmark network inference techniques using gene expression measurements. In these challenges the overall top performer was the GENIE3 algorithm. This method decomposes the network inference task into separate regression problems for each gene in the network in which the expression values of a particular target gene are predicted using all other genes as possible predictors. Next, using tree-based ensemble methods, an importance measure for each predictor gene is calculated with respect to the target gene and a high feature importance is considered as putative evidence of a regulatory link existing between both genes. The contribution of this work is twofold. First, we generalize the regression decomposition strategy of GENIE3 to other feature importance methods. We compare the performance of support vector regression, the elastic net, random forest regression, symbolic regression and their ensemble variants in this setting to the original GENIE3 algorithm. To create the ensemble variants, we propose a subsampling approach which allows us to cast any feature selection algorithm that produces a feature ranking into an ensemble feature importance algorithm. We demonstrate that the ensemble setting is key to the network inference task, as only ensemble variants achieve top performance. As second contribution, we explore the effect of using rankwise averaged predictions of multiple ensemble algorithms as opposed to only one. We name this approach NIMEFI (Network Inference using Multiple Ensemble Feature Importance algorithms) and show that this approach outperforms all individual methods in general, although on a specific network a single method can perform better. An implementation of NIMEFI has been made publicly available.},
author = {Ruyssinck, Joeri and Huynh-Thu, V{\^{a}}n Anh and Geurts, Pierre and Dhaene, Tom and Demeester, Piet and Saeys, Yvan},
doi = {10.1371/journal.pone.0092709},
issn = {1932-6203},
journal = {PloS one},
keywords = {*Algorithms,*Models, Genetic,Gene Expression Regulation/*physiology,Gene Regulatory Networks/*physiology},
language = {eng},
month = {mar},
number = {3},
pages = {e92709--e92709},
publisher = {Public Library of Science},
title = {{NIMEFI: gene regulatory network inference using multiple ensemble feature importance algorithms}},
volume = {9},
year = {2014}
}
@inproceedings{kpca,
address = {Cambridge, MA},
author = {Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J and M{\"{u}}ller, Klaus-Robert},
booktitle = {Advances in Kernel Methods: Support Vector Learning},
pages = {327--352},
publisher = {MIT Press},
title = {{Kernel Principal Component Analysis}},
year = {1999}
}


@inproceedings{chalupka2016unsupervised,
author = {Chalupka, Krzysztof and Bischoff, Tobias and Perona, Pietro and Eberhardt, Frederick},
booktitle = {Proceedings of the Thirty-Second International Conference on Uncertainty in Artificial Intelligence},
pages = {72--81},
title = {{Unsupervised Discovery of El Ni\~{n}o Using Causal Feature Learning on Microlevel Climate Data}},
year = {2016}
}

@inproceedings{beckers2019abstracting,
  title={Abstracting causal models},
  author={Beckers, Sander and Halpern, Joseph Y},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={2678--2685},
  year={2019}
}

@inproceedings{Beckers2019,
abstract = {Scientific models describe natural phenomena at different levels of abstraction. Abstract descriptions can provide the basis for interventions on the system and explanation of observed phenomena at a level of granularity that is coarser than the most fundamental account of the system. Beckers and Halpern (2019), building on work of Rubenstein et al. (2017), developed an account of abstraction for causal models that is exact. Here we extend this account to the more realistic case where an abstract causal model offers only an approximation of the underlying system. We show how the resulting account handles the discrepancy that can arise between low- and high-level causal models of the same system, and in the process provide an account of how one causal model approximates another, a topic of independent interest. Finally, we extend the account of approximate abstractions to probabilistic causal models, indicating how and where uncertainty can enter into an approximate abstraction.},
author = {Beckers, Sander and Eberhardt, Frederick and Halpern, Joseph Y},
booktitle = {Proceedings of the Conference on Uncertainty in Artificial Intelligence},
issn = {1525-3384},
language = {eng},
month = {jul},
pages = {210},
title = {{Approximate Causal Abstraction}},
year = {2019}
}
@inproceedings{correa2020calculus,
  author = "Correa, J. and Bareinboim, E.",
  title = "A Calculus For Stochastic Interventions: Causal Effect Identification and Surrogate Experiments",
  booktitle = "Proceedings of the 34th AAAI Conference on Artificial Intelligence",
  year = "2020",
}

@misc{de2010dream5,
  title={The DREAM5 Systems Genetics Challenges},
  author={de la Fuente, A and Stolovitzky, G},
  year={2010}
}

@article{vanderweele2013,
  title={Causal inference under multiple versions of treatment},
  author={Tyler J. VanderWeele and Miguel A. Hern\'{a}n},
  journal={J. Causal Inference},
  volume={1},
  pages={1--20},
  year={2013}
}

@unpublished{dawid2020,
  title={Decision-theoretic foundations for statistical causality},
  author={A. P. Dawid},
  note={\emph{arXiv} preprint, 2004.12493},
  year={2020}
}
@unpublished{nabi2017,
  title={Semiparametric causal sufficient dimension reduction of high dimensional treatment},
  author={R. Nabi and I. Shpitser},
  note={\emph{arXiv} preprint, 1710.06727},
  year={2017}
}

@inproceedings{qi2020stanza,
 author = {Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
 title = {Stanza: A {Python} Natural Language Processing Toolkit for Many Human Languages},
 year = {2020}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Breiman, Leo},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Machine Learning},
number = {1},
pages = {1--33},
pmid = {25246403},
title = {{Random Forests}},
volume = {45},
year = {2001}
}
@article{pearl2014,
author = {Pearl, Judea and Bareinboim, Elias},
doi = {10.1214/14-STS486},
journal = {Statist. Sci.},
number = {4},
pages = {579--595},
publisher = {The Institute of Mathematical Statistics},
title = {{External Validity: From Do-Calculus to Transportability Across Populations}},
volume = {29},
year = {2014}
}
@article{Bareinboim2016,
abstract = {We review concepts, principles, and tools that unify current approaches to causal analysis and attend to new challenges presented by big data. In particular, we address the problem of data fusion—piecing together multiple datasets collected under heterogeneous conditions (i.e., different populations, regimes, and sampling methods) to obtain valid answers to queries of interest. The availability of multiple heterogeneous datasets presents new opportunities to big data analysts, because the knowledge that can be acquired from combined data would not be possible from any individual source alone. However, the biases that emerge in heterogeneous environments require new analytical tools. Some of these biases, including confounding, sampling selection, and cross-population biases, have been addressed in isolation, largely in restricted parametric models. We here present a general, nonparametric framework for handling these biases and, ultimately, a theoretical solution to the problem of data fusion in causal inference tasks.},
author = {Bareinboim, Elias and Pearl, Judea},
journal = {Proc. Natl. Acad. Sci.},
number = {27},
pages = {7345--7352},
title = {{Causal inference and the data-fusion problem}},
volume = {113},
year = {2016}
}
@article{shpitser2008,
author = {Shpitser, Ilya and Pearl, Judea},
journal = {J. Mach. Learn. Res.},
pages = {1941--1979},
publisher = {JMLR.org},
title = {{Complete Identification Methods for the Causal Hierarchy}},
volume = {9},
year = {2008}
}
@inproceedings{suter19a,
abstract = {The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.},
address = {Long Beach, California, USA},
author = {Suter, Raphael and Miladinovic, Djordje and Sch{\"{o}}lkopf, Bernhard and Bauer, Stefan},
editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
pages = {6056--6065},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness}},
volume = {97},
year = {2019}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation,  and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
issn = {1939-3539 (Electronic)},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Humans,Neural Networks, Computer,trends},
language = {eng},
month = {aug},
number = {8},
pages = {1798--1828},
pmid = {23787338},
title = {{Representation learning: a review and new perspectives.}},
volume = {35},
year = {2013}
}
@article{hyvarinen_ica,
title = "Independent component analysis: algorithms and applications",
journal = "Neural Networks",
volume = "13",
number = "4",
pages = "411 - 430",
year = "2000",
author = "A. Hyvärinen and E. Oja",
keywords = "Independent component analysis, Projection pursuit, Blind signal separation, Source separation, Factor analysis, Representation",
abstract = "A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject."
}
@book{Jolliffe2002,
address = {New York},
author = {Jolliffe, I.T.},
publisher = {Springer},
title = {{Principal Component Analysis}},
year = {2002}
}

@misc{gelman:2009,
  author = {A. Gelman},
  title = {How to think about instrumental variables when you get confused},
  howpublished = {\url{https://statmodeling.stat.columbia.edu/2009/07/14/how_to_think_ab_2/}},
  year = {2009},
  note = {Accessed: 02-02-2021}
}

@inproceedings{lattimore:2016,
  title={Causal Bandits: Learning Good Interventions via Causal Inference},
  author={Lattimore, Finnian and Lattimore, Tor and Reid, Mark D.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1181--1189},
  year={2016}
}

@inproceedings{sanghack:2018,
  title={Structural Causal Bandits: Where to Intervene?},
  author={Lee, Sanghack and Bareinboim, Elias},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2568--2578},
  year={2018}
}

@unpublished{dekroon:2020,
  title={Causal Discovery for Causal Bandits utilizing Separating Sets},
  author={de Kroon, Arnoud A. W. M. and Belgrave, Danielle and Mooij, Joris M.},
  note={\emph{arXiv}:2009.07916},
  year={2020}
}

@inproceedings{zhang:2013,
  title={Domain Adaptation under Target and Conditional Shift},
  author={Zhang, Kun and Schölkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
  booktitle={Proceedings of the 30th International Conference on Machine Learning},
  pages={819--827},
  year={2013}
}

@article{spirtes:2004,
  title={Causal Inference of Ambiguous Manipulations},
  author={Spirtes, Peter and Scheines, Richard},
  journal={Philos. Sci.},
  volume={71},
  pages={833--845},
  year={2004}
}

@inproceedings{sanghack:2019,
  title={Structural Causal Bandits with Non-manipulable Variables},
  author={Lee, Sanghack and Bareinboim, Elias},
  booktitle={Proceedings of the 33rd AAAI Conference on Artificial Intelligence},
  pages={4164--4172},
  year={2019}
}

@article{arnold:2020,
  title={A causal inference perspective on the analysis of compositional data},
  author={Arnold, Kellyn F. and Berrie, Laurie and Tennant, Peter W. G. and Gilthorpe, Mark S.},
  journal={Int. J. Epidemiol.},
  volume={49},
  pages={1307-1313},
  year={2020}
}


@article{chalupka:2017,
  title={Causal feature learning: an overview},
  author={Chalupka, Krzysztof and Eberhardt, Frederick and Perona, Pietro},
  journal={Behaviormetrika},
  volume={44},
  pages={137--164},
  year={2017}
}

@unpublished{robins:2020,
  title={An Interventionist Approach to Mediation Analysis},
  author={Robins, James M. and Richardson, Thomas S. and Shpitser, Ilya},
  note={\emph{arXiv}:2008.06019},
  year={2020}
}

@book{vanderweele:2015,
  title = {Explanation in Causal Inference: Methods for Mediation and Interaction},
  author = {VanderWeele, Tyler},
  publisher = {Oxford University Press},
  year = {2015}
}
 
@article{singh:2019,
  title={Kernel instrumental variable regression},
  author={Singh, Rahul and Sahani, Maneesh and Gretton, Arthur},
  journal={Advances in Neural Information Processing Systems},
  pages={4593--4605},
  year={2019}
}

@article{muandet:2020,
  title={Dual Instrumental Variable Regression },
  author={Muandet, Krikamol and Mehrjou, Arash and Lee, Si Kai and Raj, Anant},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}
@article{gamella2020,
  title={Active Invariant Causal Prediction: Experiment Selection through Stability},
  author={Gamella, Juan L. and Heinze-Deml, Christina},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@misc{xu2020learning,
      title={Learning Deep Features in Instrumental Variable Regression}, 
      author={Liyuan Xu and Yutian Chen and Siddarth Srinivasan and Nando de Freitas and Arnaud Doucet and Arthur Gretton},
      year={2020},
      eprint={2010.07154},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{blum1998combining,
  title={Combining labeled and unlabeled data with co-training},
  author={Blum, Avrim and Mitchell, Tom},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={92--100},
  year={1998}
}

@inproceedings{chen2011co,
  title={Co-Training for Domain Adaptation.},
  author={Chen, Minmin and Weinberger, Kilian Q and Blitzer, John},
  booktitle={Nips},
  volume={24},
  pages={2456--2464},
  year={2011},
  organization={Citeseer}
}