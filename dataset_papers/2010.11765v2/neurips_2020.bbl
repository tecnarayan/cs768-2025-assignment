\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akrout et~al.(2019)Akrout, Wilson, Humphreys, Lillicrap, and
  Tweed]{akrout2019deep}
M.~Akrout, C.~Wilson, P.~Humphreys, T.~Lillicrap, and D.~B. Tweed.
\newblock Deep learning without weight transport.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  974--982, 2019.

\bibitem[Baldi and Hornik(1989)]{baldi1989neural}
P.~Baldi and K.~Hornik.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Bartunov et~al.(2018)Bartunov, Santoro, Richards, Marris, Hinton, and
  Lillicrap]{bartunov2018assessing}
S.~Bartunov, A.~Santoro, B.~Richards, L.~Marris, G.~E. Hinton, and
  T.~Lillicrap.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9368--9378, 2018.

\bibitem[Bengio(2014)]{bengio_how_2014}
Y.~Bengio.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock \emph{ArXiv}, abs/1407.7906, 2014.

\bibitem[Buchlovsky et~al.(2019)Buchlovsky, Budden, Grewe, Jones, Aslanides,
  Besse, Brock, Clark, Colmenarejo, Pope, et~al.]{buchlovsky2019tf}
P.~Buchlovsky, D.~Budden, D.~Grewe, C.~Jones, J.~Aslanides, F.~Besse, A.~Brock,
  A.~Clark, S.~G. Colmenarejo, A.~Pope, et~al.
\newblock Tf-replicator: Distributed machine learning for researchers.
\newblock \emph{arXiv preprint arXiv:1902.00465}, 2019.

\bibitem[Cadena et~al.(2019)Cadena, Denfield, Walker, Gatys, Tolias, Bethge,
  and Ecker]{cadena2019deep}
S.~A. Cadena, G.~H. Denfield, E.~Y. Walker, L.~A. Gatys, A.~S. Tolias,
  M.~Bethge, and A.~S. Ecker.
\newblock Deep convolutional models improve predictions of macaque v1 responses
  to natural images.
\newblock \emph{PLoS computational biology}, 15\penalty0 (4):\penalty0
  e1006897, 2019.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{The 37th International Conference on Machine Learning (ICML
  2020)}, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.05709}.

\bibitem[Crick(1989)]{crick_recent_1989}
F.~Crick.
\newblock The recent excitement about neural networks.
\newblock \emph{Nature}, 337:\penalty0 129–132, 1989.
\newblock \doi{10.1038/337129a0}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[DiCarlo et~al.(2012)DiCarlo, Zoccolan, and Rust]{dicarlo2012does}
J.~J. DiCarlo, D.~Zoccolan, and N.~C. Rust.
\newblock How does the brain solve visual object recognition?
\newblock \emph{Neuron}, 73\penalty0 (3):\penalty0 415--434, 2012.

\bibitem[Feather et~al.(2019)Feather, Durango, Gonzalez, and
  McDermott]{feather2019metamers}
J.~Feather, A.~Durango, R.~Gonzalez, and J.~McDermott.
\newblock Metamers of neural networks reveal divergence from human perceptual
  systems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10078--10089, 2019.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore,
  Plakal, and Ritter]{gemmeke2017audio}
J.~F. Gemmeke, D.~P. Ellis, D.~Freedman, A.~Jansen, W.~Lawrence, R.~C. Moore,
  M.~Plakal, and M.~Ritter.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In \emph{2017 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 776--780. IEEE, 2017.

\bibitem[Grossberg(1987)]{grossberg_competitive_1987}
S.~Grossberg.
\newblock Competitive learning: From interactive activation to adaptive
  resonance.
\newblock \emph{Cognitive Science}, 11:\penalty0 23--63, 1987.

\bibitem[Guerguiev et~al.(2017)Guerguiev, Lillicrap, and
  Richards]{guerguiev_towards_2017}
J.~Guerguiev, T.~P. Lillicrap, and B.~A. Richards.
\newblock Towards deep learning with segregated dendrites.
\newblock \emph{eLife}, 6, 2017.
\newblock \doi{10.7554/eLife.22901}.

\bibitem[Harris et~al.(2019)Harris, Mihalas, Hirokawa, Whitesell, Choi,
  Bernard, Bohn, Caldejon, Casal, Cho, et~al.]{harris2019hierarchical}
J.~A. Harris, S.~Mihalas, K.~E. Hirokawa, J.~D. Whitesell, H.~Choi, A.~Bernard,
  P.~Bohn, S.~Caldejon, L.~Casal, A.~Cho, et~al.
\newblock Hierarchical organization of cortical and thalamic connectivity.
\newblock \emph{Nature}, 575\penalty0 (7781):\penalty0 195--202, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he_deep_2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep {Residual} {Learning} for {Image} {Recognition}.
\newblock In \emph{2016 {IEEE} {Conference} on {Computer} {Vision} and
  {Pattern} {Recognition} ({CVPR})}, pages 770--778, June 2016.
\newblock \doi{10.1109/CVPR.2016.90}.
\newblock ISSN: 1063-6919.

\bibitem[Hebb(1949)]{hebb1949organization}
D.~O. Hebb.
\newblock \emph{The organization of behavior: a neuropsychological theory}.
\newblock J. Wiley; Chapman \& Hall, 1949.

\bibitem[Heskes and Kappen(1991)]{heskes1991learning}
T.~M. Heskes and B.~Kappen.
\newblock Learning processes in neural networks.
\newblock \emph{Physical Review A}, 44\penalty0 (4):\penalty0 2718, 1991.

\bibitem[Holtmaat et~al.(2005)Holtmaat, Trachtenberg, Wilbrecht, Shepherd,
  Zhang, Knott, and Svoboda]{holtmaat2005transient}
A.~J. Holtmaat, J.~T. Trachtenberg, L.~Wilbrecht, G.~M. Shepherd, X.~Zhang,
  G.~W. Knott, and K.~Svoboda.
\newblock Transient and persistent dendritic spines in the neocortex in vivo.
\newblock \emph{Neuron}, 45\penalty0 (2):\penalty0 279--291, 2005.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Jabri and Flower(1992)]{jabri1992weight}
M.~Jabri and B.~Flower.
\newblock Weight perturbation: An optimal architecture and learning technique
  for analog vlsi feedforward and recurrent multilayer networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 3\penalty0 (1):\penalty0
  154--157, 1992.

\bibitem[Kar et~al.(2019)Kar, Kubilius, Schmidt, Issa, and
  DiCarlo]{kar2019evidence}
K.~Kar, J.~Kubilius, K.~Schmidt, E.~B. Issa, and J.~J. DiCarlo.
\newblock Evidence that recurrent circuits are critical to the ventral
  stream’s execution of core object recognition behavior.
\newblock \emph{Nature neuroscience}, 22\penalty0 (6):\penalty0 974--983, 2019.

\bibitem[Kell et~al.(2018)Kell, Yamins, Shook, Norman-Haignere, and
  McDermott]{kell2018task}
A.~J. Kell, D.~L. Yamins, E.~N. Shook, S.~V. Norman-Haignere, and J.~H.
  McDermott.
\newblock A task-optimized neural network replicates human auditory behavior,
  predicts brain responses, and reveals a cortical processing hierarchy.
\newblock \emph{Neuron}, 98\penalty0 (3):\penalty0 630--644, 2018.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[K{\"o}hn et~al.(2016)K{\"o}hn, Stegen, and Baumann]{kohn2016mining}
A.~K{\"o}hn, F.~Stegen, and T.~Baumann.
\newblock Mining the spoken wikipedia for speech data and beyond.
\newblock In \emph{Proceedings of the Tenth International Conference on
  Language Resources and Evaluation (LREC'16)}, pages 4644--4647, 2016.

\bibitem[Krizhevsky(2010)]{krizhevsky2010cifar}
A.~Krizhevsky.
\newblock Cifar-10 and cifar-100 datasets.
\newblock 2010.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Krizhevsky(2012)]{krizhevsky2012cuda}
A.~Krizhevsky.
\newblock Cuda-convnet, 2012.
\newblock URL
  \url{https://github.com/BVLC/caffe/blob/master/examples/cifar10/cifar10_full.prototxt}.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Kunin et~al.(2020)Kunin, Nayebi, Sagastuy-Brena, Ganguli, Bloom, and
  Yamins]{tworoutes2020}
D.~Kunin, A.~Nayebi, J.~Sagastuy-Brena, S.~Ganguli, J.~Bloom, and D.~L. Yamins.
\newblock Two routes to scalable credit assignment without weight symmetry.
\newblock In \emph{The 37th International Conference on Machine Learning (ICML
  2020)}, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.01513}.

\bibitem[Lansdell and Kording(2019)]{lansdell2019spiking}
B.~J. Lansdell and K.~P. Kording.
\newblock Spiking allows neurons to estimate their causal effect.
\newblock \emph{bioRxiv}, page 253351, 2019.

\bibitem[Lee et~al.(2015)Lee, Zhang, Fischer, and Bengio]{lee_difference_2015}
D.-H. Lee, S.~Zhang, A.~Fischer, and Y.~Bengio.
\newblock Difference target propagation.
\newblock In \emph{Joint european conference on machine learning and knowledge
  discovery in databases}, pages 498--515. Springer, 2015.

\bibitem[Liao et~al.(2016)Liao, Leibo, and Poggio]{liao_how_2016}
Q.~Liao, J.~Z. Leibo, and T.~Poggio.
\newblock How important is weight symmetry in backpropagation?
\newblock In \emph{Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence}, AAAI'16, pages 1837--1844. AAAI Press, 2016.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3016100.3016156}.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap_random_2016}
T.~P. Lillicrap, D.~Cownden, D.~B. Tweed, and C.~J. Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature Communications}, 7:\penalty0 13276, Nov. 2016.
\newblock ISSN 2041-1723.
\newblock \doi{10.1038/ncomms13276}.
\newblock URL \url{https://www.nature.com/articles/ncomms13276}.

\bibitem[Lim et~al.(2015)Lim, McKee, Woloszyn, Amit, Freedman, Sheinberg, and
  Brunel]{lim2015inferring}
S.~Lim, J.~L. McKee, L.~Woloszyn, Y.~Amit, D.~J. Freedman, D.~L. Sheinberg, and
  N.~Brunel.
\newblock Inferring learning rules from distributions of firing rates in
  cortical neurons.
\newblock \emph{Nature neuroscience}, 18\penalty0 (12):\penalty0 1804, 2015.

\bibitem[Majaj et~al.(2015)Majaj, Hong, Solomon, and
  {DiCarlo}]{majaj2015simple}
N.~J. Majaj, H.~Hong, E.~A. Solomon, and J.~J. {DiCarlo}.
\newblock Simple learned weighted sums of inferior temporal neuronal firing
  rates accurately predict human core object recognition performance.
\newblock \emph{The Journal of neuroscience : the official journal of the
  Society for Neuroscience}, 35:\penalty0 13402--18, 2015 Sep 30 2015.
\newblock ISSN 1529-2401.
\newblock \doi{10.1523/JNEUROSCI.5181-14.2015}.

\bibitem[Moskovitz et~al.(2018)Moskovitz, Litwin{-}Kumar, and
  Abbott]{moskovitz_feedback_2018}
T.~H. Moskovitz, A.~Litwin{-}Kumar, and L.~F. Abbott.
\newblock Feedback alignment in deep convolutional networks.
\newblock \emph{CoRR}, abs/1812.06488, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.06488}.

\bibitem[Nayebi et~al.(2018)Nayebi, Bear, Kubilius, Kar, Ganguli, Sussillo,
  DiCarlo, and Yamins]{nayebi2018task}
A.~Nayebi, D.~Bear, J.~Kubilius, K.~Kar, S.~Ganguli, D.~Sussillo, J.~J.
  DiCarlo, and D.~L. Yamins.
\newblock Task-driven convolutional recurrent models of the visual system.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5290--5301, 2018.

\bibitem[Nøkland(2016)]{nokland_direct_2016}
A.~Nøkland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock 09 2016.

\bibitem[Paul and Baker(1992)]{paul1992design}
D.~B. Paul and J.~Baker.
\newblock The design for the wall street journal-based csr corpus.
\newblock In \emph{Speech and Natural Language: Proceedings of a Workshop Held
  at Harriman, New York, February 23-26, 1992}, 1992.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart_learning_1986}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323:\penalty0 533–536, October 1986.
\newblock \doi{10.1038/323533a0}.

\bibitem[Schrimpf et~al.(2018)Schrimpf, Kubilius, Hong, Majaj, Rajalingham,
  Issa, Kar, Bashivan, Prescott-Roy, Schmidt, et~al.]{schrimpf2018brain}
M.~Schrimpf, J.~Kubilius, H.~Hong, N.~J. Majaj, R.~Rajalingham, E.~B. Issa,
  K.~Kar, P.~Bashivan, J.~Prescott-Roy, K.~Schmidt, et~al.
\newblock Brain-score: Which artificial neural network for object recognition
  is most brain-like?
\newblock \emph{BioRxiv}, page 407007, 2018.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
I.~Sutskever, J.~Martens, G.~Dahl, and G.~Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem[Werfel et~al.(2004)Werfel, Xie, and Seung]{werfel2004learning}
J.~Werfel, X.~Xie, and H.~S. Seung.
\newblock Learning curves for stochastic gradient descent in linear feedforward
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1197--1204, 2004.

\bibitem[Widrow and Lehr(1990)]{widrow199030}
B.~Widrow and M.~A. Lehr.
\newblock 30 years of adaptive neural networks: perceptron, madaline, and
  backpropagation.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (9):\penalty0 1415--1442,
  1990.

\bibitem[Wojtowicz and Atwood(1985)]{wojtowicz1985correlation}
J.~Wojtowicz and H.~Atwood.
\newblock Correlation of presynaptic and postsynaptic events during
  establishment of long-term facilitation at crayfish neuromuscular junction.
\newblock \emph{Journal of neurophysiology}, 54\penalty0 (2):\penalty0
  220--230, 1985.

\bibitem[Xiao et~al.(2019)Xiao, Chen, Liao, and
  Poggio]{xiao_biologically-plausible_2019}
W.~Xiao, H.~Chen, Q.~Liao, and T.~Poggio.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SygvZ209F7}.

\bibitem[Yamins et~al.(2014)Yamins, Hong, Cadieu, Solomon, Seibert, and
  DiCarlo]{yamins2014performance}
D.~L. Yamins, H.~Hong, C.~F. Cadieu, E.~A. Solomon, D.~Seibert, and J.~J.
  DiCarlo.
\newblock Performance-optimized hierarchical models predict neural responses in
  higher visual cortex.
\newblock \emph{Proceedings of the National Academy of Sciences}, 111\penalty0
  (23):\penalty0 8619--8624, 2014.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Y.~You, I.~Gitman, and B.~Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[Zhuang et~al.(2020)Zhuang, Yan, Nayebi, Schrimpf, Frank, DiCarlo, and
  Yamins]{zhuang2020unsupervised}
C.~Zhuang, S.~Yan, A.~Nayebi, M.~Schrimpf, M.~Frank, J.~DiCarlo, and D.~Yamins.
\newblock Unsupervised neural network models of the ventral visual stream.
\newblock \emph{bioRxiv}, 2020.

\end{thebibliography}
