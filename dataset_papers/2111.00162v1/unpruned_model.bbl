\begin{thebibliography}{10}

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock {\em arXiv preprint arXiv:1810.02340}, 2018.

\bibitem{wang2019picking}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{You:2019tz}
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,
  Richard~G. Baraniuk, Zhangyang Wang, and Yingyan Lin.
\newblock Drawing early-bird tickets: Toward more efficient training of deep
  networks.
\newblock In {\em 8th International Conference on Learning Representations},
  2020.

\bibitem{frankle2020pruning}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock {\em arXiv preprint arXiv:2009.08576}, 2020.

\bibitem{uchida2017embedding}
Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin'ichi Satoh.
\newblock Embedding watermarks into deep neural networks.
\newblock In {\em Proceedings of the 2017 ACM on International Conference on
  Multimedia Retrieval}, pages 269--277, 2017.

\bibitem{adi2018turning}
Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.
\newblock Turning your weakness into a strength: Watermarking deep neural
  networks by backdooring.
\newblock In {\em 27th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 18)}, pages 1615--1631, 2018.

\bibitem{zhang2018protecting}
Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc~Ph Stoecklin, Heqing
  Huang, and Ian Molloy.
\newblock Protecting intellectual property of deep neural networks with
  watermarking.
\newblock In {\em Proceedings of the 2018 on Asia Conference on Computer and
  Communications Security}, pages 159--172, 2018.

\bibitem{ma2021undistillable}
Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang
  Wang.
\newblock Undistillable: Making a nasty teacher that cannot teach students.
\newblock {\em arXiv preprint arXiv:2105.07381}, 2021.

\bibitem{darvish2019deepsigns}
Bita Darvish~Rouhani, Huili Chen, and Farinaz Koushanfar.
\newblock Deepsigns: An end-to-end watermarking framework for ownership
  protection of deep neural networks.
\newblock In {\em Proceedings of the Twenty-Fourth International Conference on
  Architectural Support for Programming Languages and Operating Systems}, pages
  485--497, 2019.

\bibitem{zhang2020passport}
Jie Zhang, Dongdong Chen, Jing Liao, Weiming Zhang, Gang Hua, and Nenghai Yu.
\newblock Passport-aware normalization for deep model protection.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{tanaka2020pruning}
Hidenori Tanaka, Daniel Kunin, Daniel~L Yamins, and Surya Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{janowsky1989pruning}
Steven~A Janowsky.
\newblock Pruning versus clipping in neural networks.
\newblock {\em Physical Review A}, 39(12):6600, 1989.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William~J Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em NIPS}, 2015.

\bibitem{mozer1989skeletonization}
Michael~C Mozer and Paul Smolensky.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In {\em Advances in neural information processing systems}, pages
  107--115, 1989.

\bibitem{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem{hassibi1993second}
Babak Hassibi and David~G Stork.
\newblock {\em Second order derivatives for network pruning: Optimal brain
  surgeon}.
\newblock Morgan Kaufmann, 1993.

\bibitem{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock {\em arXiv preprint arXiv:1611.06440}, 2016.

\bibitem{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno~Jialin Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock {\em arXiv preprint arXiv:1705.07565}, 2017.

\bibitem{yu2018nisp}
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad~I Morariu, Xintong Han,
  Mingfei Gao, Ching-Yung Lin, and Larry~S Davis.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9194--9203, 2018.

\bibitem{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In {\em International Conference on Machine Learning}, pages
  3259--3269. PMLR, 2020.

\bibitem{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv}, abs/1902.09574, 2019.

\bibitem{pmlr-v139-zhang21c}
Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang.
\newblock Efficient lottery ticket finding: Less data is more.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 12380--12390. PMLR, 18--24 Jul 2021.

\bibitem{chen2020lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
  Wang, and Michael Carbin.
\newblock The lottery ticket hypothesis for pre-trained bert networks, 2020.

\bibitem{chen2020lottery2}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael
  Carbin, and Zhangyang Wang.
\newblock The lottery tickets hypothesis for supervised and self-supervised
  pre-training in computer vision models.
\newblock {\em arXiv preprint arXiv:2012.06908}, 2020.

\bibitem{yu2019playing}
Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari~S. Morcos.
\newblock Playing the lottery with rewards and multiple languages: lottery
  tickets in rl and nlp.
\newblock In {\em 8th International Conference on Learning Representations},
  2020.

\bibitem{chen2021gans}
Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen.
\newblock {\{}GAN{\}}s can play lottery tickets too.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{ma2021good}
Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang
  Wang.
\newblock Good students play big lottery better.
\newblock {\em arXiv preprint arXiv:2101.03255}, 2021.

\bibitem{gan2021playing}
Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu~Cheng, Shuohang Wang, and
  Jingjing Liu.
\newblock Playing lottery tickets with vision and language.
\newblock {\em arXiv preprint arXiv:2104.11832}, 2021.

\bibitem{chen2021unified}
Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang.
\newblock A unified lottery ticket hypothesis for graph neural networks.
\newblock {\em arXiv preprint arXiv:2102.06790}, 2021.

\bibitem{kalibhat2021winning}
Neha~Mukund Kalibhat, Yogesh Balaji, and Soheil Feizi.
\newblock Winning lottery tickets in deep generative models, 2021.

\bibitem{chen2021ultra}
Tianlong Chen, Yu~Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang.
\newblock Ultra-data-efficient gan training: Drawing a lottery ticket first,
  then training it toughly.
\newblock {\em arXiv preprint arXiv:2103.00397}, 2021.

\bibitem{le2020adversarial}
Erwan Le~Merrer, Patrick Perez, and Gilles Tr{\'e}dan.
\newblock Adversarial frontier stitching for remote neural network
  watermarking.
\newblock {\em Neural Computing and Applications}, 32(13):9233--9244, 2020.

\bibitem{fan2019rethinking}
Lixin Fan, Kam~Woh Ng, and Chee~Seng Chan.
\newblock Rethinking deep neural network ownership verification: Embedding
  passports to defeat ambiguity attacks.
\newblock 2019.

\bibitem{patil2020phew}
Shreyas~Malakarjun Patil and Constantine Dovrolis.
\newblock Phew: Paths with higher edge-weights give "winning tickets" without
  training data, 2020.

\bibitem{soon2008qr}
Tan~Jin Soon.
\newblock Qr code.
\newblock {\em Synthesis Journal}, 2008:59--78, 2008.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\end{thebibliography}
