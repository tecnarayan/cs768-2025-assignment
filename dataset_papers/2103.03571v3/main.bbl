\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Albadawy et~al.(2018)Albadawy, Saha, and Mazurowski]{2018Deep}
Albadawy, E.~A., Saha, A., and Mazurowski, M.~A.
\newblock Deep learning for segmentation of brain tumors: Impact of
  cross-institutional training and testing.
\newblock \emph{Medical Physics}, 45\penalty0 (3), 2018.

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O'Connor, and
  McGuinness]{DBLP:journals/corr/abs-1908-02983}
Arazo, E., Ortego, D., Albert, P., O'Connor, N.~E., and McGuinness, K.
\newblock Pseudo-labeling and confirmation bias in deep semi-supervised
  learning.
\newblock \emph{CoRR}, abs/1908.02983, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{NIPS2019_9025}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{NeurIPS}, pp.\  8141--8150. 2019.

\bibitem[Bachman et~al.(2014)Bachman, Alsharif, and Precup]{NIPS2014_66be31e4}
Bachman, P., Alsharif, O., and Precup, D.
\newblock Learning with pseudo-ensembles.
\newblock In \emph{NeurIPS}, volume~27, pp.\  3365--3373, 2014.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{JMLR}, 3\penalty0 (Nov):\penalty0 463--482, 2002.

\bibitem[Ben-David \& Urner(2012)Ben-David and
  Urner]{10.1007/978-3-642-34106-9_14}
Ben-David, S. and Urner, R.
\newblock On the hardness of domain adaptation and the utility of unlabeled
  target samples.
\newblock In \emph{ALT}, pp.\  139--153, 2012.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{cite:ML10DAT}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan,
  J.~W.
\newblock A theory of learning from different domains.
\newblock \emph{Machine Learning}, 79\penalty0 (1-2):\penalty0 151--175, 2010.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and
  Raffel, C.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Bertinetto et~al.(2019)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto2018metalearning}
Bertinetto, L., Henriques, J.~F., Torr, P., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In \emph{ICLR}, 2019.

\bibitem[Blitzer et~al.(2007)Blitzer, Dredze, and
  Pereira]{blitzer-etal-2007-biographies}
Blitzer, J., Dredze, M., and Pereira, F.
\newblock Biographies, {B}ollywood, boom-boxes and blenders: Domain adaptation
  for sentiment classification.
\newblock In \emph{ACL}, pp.\  440--447, 2007.

\bibitem[Cai et~al.(2021)Cai, Gao, Lee, and Lei]{cai2021theory}
Cai, T., Gao, R., Lee, J.~D., and Lei, Q.
\newblock A theory of label propagation for subpopulation shift, 2021.

\bibitem[Carlini(2021)]{carlini2021poisoning}
Carlini, N.
\newblock Poisoning the unlabeled dataset of semi-supervised learning, 2021.

\bibitem[Chapelle et~al.(2006)Chapelle, Sch{\"o}lkopf, and
  Zien]{cite:Book06SSL}
Chapelle, O., Sch{\"o}lkopf, B., and Zien, A.
\newblock \emph{Semi-supervised learning}.
\newblock MIT press Cambridge, 2006.

\bibitem[{Chen} et~al.(2019){Chen}, {Xie}, {Huang}, {Rong}, {Ding}, {Huang},
  {Xu}, and {Huang}]{8953748}
{Chen}, C., {Xie}, W., {Huang}, W., {Rong}, Y., {Ding}, X., {Huang}, Y., {Xu},
  T., and {Huang}, J.
\newblock Progressive feature alignment for unsupervised domain adaptation.
\newblock In \emph{CVPR}, pp.\  627--636, 2019.

\bibitem[Chen et~al.(2020)Chen, Wei, Kumar, and Ma]{NEURIPS2020_f1298750}
Chen, Y., Wei, C., Kumar, A., and Ma, T.
\newblock Self-training avoids using spurious features under domain shift.
\newblock In \emph{NeurIPS}, pp.\  21061--21071, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, pp.\  4171--4186, 2019.

\bibitem[Du et~al.(2021)Du, Grave, Gunel, Chaudhary, Celebi, Auli, Stoyanov,
  and Conneau]{du-etal-2021-self}
Du, J., Grave, E., Gunel, B., Chaudhary, V., Celebi, O., Auli, M., Stoyanov,
  V., and Conneau, A.
\newblock Self-training improves pre-training for natural language
  understanding.
\newblock In \emph{NAACL}, pp.\  5408--5418, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{pmlr-v70-finn17a}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, pp.\  1126--1135, 2017.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{ICLR}, 2021.

\bibitem[Frei et~al.(2021)Frei, Zou, Chen, and Gu]{frei2021self}
Frei, S., Zou, D., Chen, Z., and Gu, Q.
\newblock Self-training converts weak learners to strong learners in mixture
  models.
\newblock \emph{arXiv preprint arXiv:2106.13805}, 2021.

\bibitem[French et~al.(2018)French, Mackiewicz, and
  Fisher]{french2018selfensembling}
French, G., Mackiewicz, M., and Fisher, M.
\newblock Self-ensembling for visual domain adaptation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Marchand, and Lempitsky]{cite:JMLR17DANN}
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Marchand, M.,
  and Lempitsky, V.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{JMLR}, 17\penalty0 (1):\penalty0 2096--2030, 2016.

\bibitem[Ghiasi et~al.(2021)Ghiasi, Zoph, Cubuk, Le, and Lin]{ghiasi2021multi}
Ghiasi, G., Zoph, B., Cubuk, E.~D., Le, Q.~V., and Lin, T.-Y.
\newblock Multi-task self-training for learning general representations.
\newblock In \emph{ICCV}, pp.\  8856--8865, 2021.

\bibitem[Grandvalet \& Bengio(2004)Grandvalet and Bengio]{cite:NIPS04SSLEM}
Grandvalet, Y. and Bengio, Y.
\newblock Semi-supervised learning by entropy minimization.
\newblock In \emph{NeurIPS}, pp.\  529--536, 2004.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{pmlr-v70-guo17a}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{ICML}, pp.\  1321--1330, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{cite:CVPR16DRL}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, pp.\  770--778, 2016.

\bibitem[Hoffman et~al.(2018)Hoffman, Tzeng, Park, Zhu, Isola, Saenko, Efros,
  and Darrell]{cite:ICML18CYCADA}
Hoffman, J., Tzeng, E., Park, T., Zhu, J., Isola, P., Saenko, K., Efros, A.~A.,
  and Darrell, T.
\newblock Cycada: Cycle-consistent adversarial domain adaptation.
\newblock In \emph{ICML}, pp.\  1994--2003, 2018.

\bibitem[Jiang et~al.(2020)Jiang, Lao, Matwin, and Havaei]{pmlr-v119-jiang20d}
Jiang, X., Lao, Q., Matwin, S., and Havaei, M.
\newblock Implicit class-conditioned domain alignment for unsupervised domain
  adaptation.
\newblock In \emph{ICML}, pp.\  4816--4827, 2020.

\bibitem[Johansson et~al.(2019)Johansson, Sontag, and
  Ranganath]{pmlr-v89-johansson19a}
Johansson, F.~D., Sontag, D., and Ranganath, R.
\newblock Support and invertibility in domain-invariant representations.
\newblock In \emph{AISTATS}, pp.\  527--536, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Ma, and Liang]{pmlr-v119-kumar20c}
Kumar, A., Ma, T., and Liang, P.
\newblock Understanding self-training for gradual domain adaptation.
\newblock In \emph{ICML}, pp.\  5468--5479, 2020.

\bibitem[Lee(2013)]{article-pl}
Lee, D.-H.
\newblock Pseudo-label : The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock \emph{ICML Workshop: Challenges in Representation Learning (WREPL)},
  2013.

\bibitem[Li et~al.(2020)Li, Wang, Che, Zhang, Zhao, Xu, Zhou, Bengio, and
  Keutzer]{Li2020RethinkingDM}
Li, B., Wang, Y., Che, T., Zhang, S., Zhao, S., Xu, P., Zhou, W., Bengio, Y.,
  and Keutzer, K.
\newblock Rethinking distributional matching based domain adaptation.
\newblock \emph{ArXiv}, abs/2006.13352, 2020.

\bibitem[Liu et~al.(2019)Liu, Long, Wang, and Jordan]{pmlr-v97-liu19b}
Liu, H., Long, M., Wang, J., and Jordan, M.
\newblock Transferable adversarial training: A general approach to adapting
  deep classifiers.
\newblock In \emph{ICML}, volume~97, pp.\  4013--4022, 2019.

\bibitem[Long et~al.(2015)Long, Cao, Wang, and Jordan]{cite:ICML15DAN}
Long, M., Cao, Y., Wang, J., and Jordan, M.~I.
\newblock Learning transferable features with deep adaptation networks.
\newblock In \emph{ICML}, pp.\  97--105, 2015.

\bibitem[Long et~al.(2016)Long, Zhu, Wang, and Jordan]{cite:NIPS16RTN}
Long, M., Zhu, H., Wang, J., and Jordan, M.~I.
\newblock Unsupervised domain adaptation with residual transfer networks.
\newblock In \emph{NeurIPS}, pp.\  136--144, 2016.

\bibitem[Long et~al.(2017)Long, Zhu, Wang, and Jordan]{cite:ICML17JAN}
Long, M., Zhu, H., Wang, J., and Jordan, M.~I.
\newblock Deep transfer learning with joint adaptation networks.
\newblock In \emph{ICML}, pp.\  2208--2217, 2017.

\bibitem[Long et~al.(2018)Long, Cao, Wang, and Jordan]{cite:NIPS18CDAN}
Long, M., Cao, Z., Wang, J., and Jordan, M.~I.
\newblock Conditional adversarial domain adaptation.
\newblock In \emph{NeurIPS}, pp.\  1640--1650. 2018.

\bibitem[Lu et~al.(2020)Lu, Yang, Zhu, Liu, Song, and Xiang]{lu2020stochastic}
Lu, Z., Yang, Y., Zhu, X., Liu, C., Song, Y.-Z., and Xiang, T.
\newblock Stochastic classifiers for unsupervised domain adaptation.
\newblock In \emph{CVPR}, pp.\  9111--9120, 2020.

\bibitem[Mey \& Loog(2016)Mey and Loog]{sl2016A}
Mey, A. and Loog, M.
\newblock A soft-labeled self-training approach.
\newblock In \emph{ICPR}, 2016.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Ishii, and Koyama]{cite:TPAMI18VAT}
Miyato, T., Maeda, S., Ishii, S., and Koyama, M.
\newblock Virtual adversarial training: A regularization method for supervised
  and semi-supervised learning.
\newblock \emph{TPAMI}, 2018.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Mukherjee \& Awadallah(2020)Mukherjee and
  Awadallah]{NEURIPS2020_f23d125d}
Mukherjee, S. and Awadallah, A.
\newblock Uncertainty-aware self-training for few-shot text classification.
\newblock In \emph{NeurIPS}, volume~33, pp.\  21199--21212, 2020.

\bibitem[Pan \& Yang(2010)Pan and Yang]{cite:TKDE10TLSurvey}
Pan, S.~J. and Yang, Q.
\newblock A survey on transfer learning.
\newblock \emph{TKDE}, 22\penalty0 (10):\penalty0 1345--1359, 2010.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_bdbca288}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, volume~32, pp.\  8026--8037, 2019.

\bibitem[Peng et~al.(2017)Peng, Usman, Kaushik, Hoffman, Wang, and
  Saenko]{DBLP:journals/corr/abs-1710-06924}
Peng, X., Usman, B., Kaushik, N., Hoffman, J., Wang, D., and Saenko, K.
\newblock Visda: The visual domain adaptation challenge.
\newblock \emph{CoRR}, abs/1710.06924, 2017.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and Wang]{9010750}
Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{ICCV}, pp.\  1406--1415, 2019.

\bibitem[Prabhu et~al.(2020)Prabhu, Khare, Kartik, and
  Hoffman]{prabhu2020sentry}
Prabhu, V., Khare, S., Kartik, D., and Hoffman, J.
\newblock Sentry: Selective entropy optimization via committee consistency for
  unsupervised domain adaptation, 2020.

\bibitem[Prabhu et~al.(2021)Prabhu, Khare, Kartik, and
  Hoffman]{Prabhu_2021_ICCV}
Prabhu, V., Khare, S., Kartik, D., and Hoffman, J.
\newblock Sentry: Selective entropy optimization via committee consistency for
  unsupervised domain adaptation.
\newblock In \emph{ICCV}, pp.\  8558--8567, October 2021.

\bibitem[Qu et~al.(2019)Qu, Zou, Cheng, Yang, and
  Zhou]{qu-etal-2019-adversarial}
Qu, X., Zou, Z., Cheng, Y., Yang, Y., and Zhou, P.
\newblock Adversarial category alignment network for cross-domain sentiment
  classification.
\newblock In \emph{NAACL}, 2019.

\bibitem[Quionero-Candela et~al.(2009)Quionero-Candela, Sugiyama, Schwaighofer,
  and Lawrence]{cite:MIT2009Dataset}
Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N.~D.
\newblock \emph{Dataset Shift in Machine Learning}.
\newblock The MIT Press, 2009.

\bibitem[Rasmus et~al.(2015)Rasmus, Berglund, Honkala, Valpola, and
  Raiko]{NIPS2015_378a063b}
Rasmus, A., Berglund, M., Honkala, M., Valpola, H., and Raiko, T.
\newblock Semi-supervised learning with ladder networks.
\newblock In \emph{NeurIPS}, volume~28, pp.\  3546--3554, 2015.

\bibitem[{Rosenberg} et~al.(2005){Rosenberg}, {Hebert}, and
  {Schneiderman}]{4129456}
{Rosenberg}, C., {Hebert}, M., and {Schneiderman}, H.
\newblock Semi-supervised self-training of object detection models.
\newblock In \emph{WACV}, volume~1, pp.\  29--36, 2005.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{cite:ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{IJCV}, 115\penalty0 (3):\penalty0 211--252, 2015.

\bibitem[Saito et~al.(2018)Saito, Watanabe, Ushiku, and Harada]{cite:CVPR18MCD}
Saito, K., Watanabe, K., Ushiku, Y., and Harada, T.
\newblock Maximum classifier discrepancy for unsupervised domain adaptation.
\newblock In \emph{CVPR}, pp.\  3723--3732, 2018.

\bibitem[Sajjadi et~al.(2016)Sajjadi, Javanmardi, and
  Tasdizen]{NIPS2016_30ef30b6}
Sajjadi, M., Javanmardi, M., and Tasdizen, T.
\newblock Regularization with stochastic transformations and perturbations for
  deep semi-supervised learning.
\newblock In \emph{NeurIPS}, volume~29, pp.\  1163--1171, 2016.

\bibitem[Shu et~al.(2018)Shu, Bui, Narui, and Ermon]{shu2018a}
Shu, R., Bui, H., Narui, H., and Ermon, S.
\newblock A {DIRT}-t approach to unsupervised domain adaptation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel,
  Cubuk, Kurakin, and Li]{fixmatch}
Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.~A.,
  Cubuk, E.~D., Kurakin, A., and Li, C.-L.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, 2014.

\bibitem[Talagrand(2014)]{talagrand2014upper}
Talagrand, M.
\newblock \emph{Upper and lower bounds for stochastic processes: modern methods
  and classical problems}, volume~60.
\newblock Springer Science \& Business Media, 2014.

\bibitem[Tan et~al.(2020)Tan, Peng, and Saenko]{2019Class}
Tan, S., Peng, X., and Saenko, K.
\newblock Class-imbalanced domain adaptation: An empirical odyssey.
\newblock In \emph{ECCV Workshop}, 2020.

\bibitem[Tarvainen \& Valpola(2017)Tarvainen and Valpola]{NIPS2017_68053af2}
Tarvainen, A. and Valpola, H.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In \emph{NeurIPS}, volume~30, pp.\  1195--1204, 2017.

\bibitem[Tsallis(1988)]{1988Possible}
Tsallis, C.
\newblock Possible generalization of boltzmann-gibbs statistics.
\newblock \emph{Journal of Statistical Physics}, 52\penalty0 (1-2):\penalty0
  479--487, 1988.

\bibitem[Tzeng et~al.(2017)Tzeng, Hoffman, Saenko, and
  Darrell]{cite:CVPR17ADDA}
Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T.
\newblock Adversarial discriminative domain adaptation.
\newblock In \emph{CVPR}, pp.\  7167--7176, 2017.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and
  Panchanathan]{cite:CVPR17OfficeHome}
Venkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In \emph{CVPR}, pp.\  5018--5027, 2017.

\bibitem[Vu et~al.(2021)Vu, Luong, Le, Simon, and Iyyer]{vu2021strata}
Vu, T., Luong, M.-T., Le, Q.~V., Simon, G., and Iyyer, M.
\newblock Strata: Self-training with task augmentation for better few-shot
  learning.
\newblock \emph{arXiv preprint arXiv:2109.06270}, 2021.

\bibitem[Wei et~al.(2021)Wei, Shen, Yining, and Ma]{wei_2021_ICLR}
Wei, C., Shen, K., Yining, C., and Ma, T.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data.
\newblock In \emph{ICLR}, 2021.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{2020Self}
Xie, Q., Luong, M.~T., Hovy, E., and Le, Q.~V.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{CVPR}, 2020.

\bibitem[Xie et~al.(2021)Xie, Kumar, Jones, Khani, Ma, and
  Liang]{xie2020innout}
Xie, S.~M., Kumar, A., Jones, R., Khani, F., Ma, T., and Liang, P.
\newblock In-n-out: Pre-training and self-training using auxiliary information
  for out-of-distribution robustness.
\newblock In \emph{ICLR}, 2021.

\bibitem[Xu et~al.(2019)Xu, Li, Yang, and Lin]{2020Larger}
Xu, R., Li, G., Yang, J., and Lin, L.
\newblock Larger norm more transferable: An adaptive feature norm approach for
  unsupervised domain adaptation.
\newblock In \emph{ICCV}, 2019.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{cite:NIPS14CNN}
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.
\newblock How transferable are features in deep neural networks?
\newblock In \emph{NeurIPS}, pp.\  3320--3328. 2014.

\bibitem[Zellinger et~al.(2017)Zellinger, Grubinger, Lughofer,
  Natschl{\"{a}}ger, and Saminger{-}Platz]{cite:ICLR17CMD}
Zellinger, W., Grubinger, T., Lughofer, E., Natschl{\"{a}}ger, T., and
  Saminger{-}Platz, S.
\newblock Central moment discrepancy {(CMD)} for domain-invariant
  representation learning.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Liu, Long, and Jordan]{pmlr-v97-zhang19i}
Zhang, Y., Liu, T., Long, M., and Jordan, M.
\newblock Bridging theory and algorithm for domain adaptation.
\newblock In \emph{ICML}, pp.\  7404--7413, 2019.

\bibitem[Zhao et~al.(2019)Zhao, Combes, Zhang, and Gordon]{pmlr-v97-zhao19a}
Zhao, H., Combes, R. T.~D., Zhang, K., and Gordon, G.
\newblock On learning invariant representations for domain adaptation.
\newblock In \emph{ICML}, volume~97, pp.\  7523--7532, 2019.

\bibitem[Ziser \& Reichart(2018)Ziser and Reichart]{ziser-reichart-2018-pivot}
Ziser, Y. and Reichart, R.
\newblock Pivot based language modeling for improved neural domain adaptation.
\newblock In \emph{NAACL}, pp.\  1241--1251, 2018.

\bibitem[Zoph et~al.(2020)Zoph, Ghiasi, Lin, Cui, Liu, Cubuk, and
  Le]{NEURIPS2020_27e9661e}
Zoph, B., Ghiasi, G., Lin, T.-Y., Cui, Y., Liu, H., Cubuk, E.~D., and Le, Q.
\newblock Rethinking pre-training and self-training.
\newblock In \emph{NeurIPS}, volume~33, pp.\  3833--3845, 2020.

\bibitem[Zou et~al.(2018)Zou, Yu, Vijaya~Kumar, and
  Wang]{10.1007/978-3-030-01219-9_18}
Zou, Y., Yu, Z., Vijaya~Kumar, B. V.~K., and Wang, J.
\newblock Unsupervised domain adaptation for semantic segmentation via
  class-balanced self-training.
\newblock In \emph{ECCV}, pp.\  297--313, 2018.

\bibitem[Zou et~al.(2019)Zou, Yu, Liu, Kumar, and Wang]{Zou_2019_ICCV}
Zou, Y., Yu, Z., Liu, X., Kumar, B.~V., and Wang, J.
\newblock Confidence regularized self-training.
\newblock In \emph{ICCV}, October 2019.

\end{thebibliography}
