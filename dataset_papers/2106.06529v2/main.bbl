\begin{thebibliography}{93}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2020)Agrawal, Papamarkou, and Hinkle]{agrawal2020wide}
D.~Agrawal, T.~Papamarkou, and J.~Hinkle.
\newblock Wide neural networks with bottlenecks are deep {G}aussian processes.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (175):\penalty0 1--66, 2020.

\bibitem[Aitchison(2020)]{aitchison2020bigger}
L.~Aitchison.
\newblock Why bigger is not always better: on finite and infinite neural
  networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Aitchison et~al.(2021)Aitchison, Yang, and Ober]{aitchison2021deep}
L.~Aitchison, A.~Yang, and S.~W. Ober.
\newblock Deep kernel processes.
\newblock In \emph{ICML}, 2021.

\bibitem[Allen-Zhu and Li(2019)]{allen2019can}
Z.~Allen-Zhu and Y.~Li.
\newblock What can {R}es{N}et learn efficiently, going beyond kernels?
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Allen-Zhu and Li(2020)]{allen2020backward}
Z.~Allen-Zhu and Y.~Li.
\newblock Backward feature correction: How deep learning performs deep
  learning.
\newblock \emph{arXiv preprint arXiv:2001.04413}, 2020.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Liang]{allen2019learning}
Z.~Allen-Zhu, Y.~Li, and Y.~Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
S.~Arora, S.~Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{ICML}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{NeurIPS}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{arora2020harnessing}
S.~Arora, S.~S. Du, Z.~Li, R.~Salakhutdinov, R.~Wang, and D.~Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Asuncion and Newman(2007)]{asuncion2007uci}
A.~Asuncion and D.~Newman.
\newblock {UCI} machine learning repository, 2007.
\newblock URL \url{http://archive.ics.uci.edu/ml/index.php}.

\bibitem[Bai and Lee(2020)]{bai2020beyond}
Y.~Bai and J.~D. Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Bengio et~al.(2005)Bengio, Delalleau, and Le~Roux]{bengio2005curse}
Y.~Bengio, O.~Delalleau, and N.~Le~Roux.
\newblock The curse of dimensionality for local kernel machines.
\newblock 2005.

\bibitem[Billingsley(2013)]{billingsley2013convergence}
P.~Billingsley.
\newblock \emph{Convergence of probability measures}.
\newblock John Wiley \& Sons, 2013.

\bibitem[Bingham et~al.(2019)Bingham, Chen, Jankowiak, Obermeyer, Pradhan,
  Karaletsos, Singh, Szerlip, Horsfall, and Goodman]{bingham2019pyro}
E.~Bingham, J.~P. Chen, M.~Jankowiak, F.~Obermeyer, N.~Pradhan, T.~Karaletsos,
  R.~Singh, P.~Szerlip, P.~Horsfall, and N.~D. Goodman.
\newblock Pyro: Deep universal probabilistic programming.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 973--978, 2019.

\bibitem[Bishop(2006)]{bishop2006pattern}
C.~M. Bishop.
\newblock \emph{Pattern recognition and machine learning}.
\newblock Springer, 2006.

\bibitem[Blomqvist et~al.(2019)Blomqvist, Kaski, and
  Heinonen]{blomqvist2019deep}
K.~Blomqvist, S.~Kaski, and M.~Heinonen.
\newblock Deep convolutional {G}aussian processes.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 2019.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{ICML}, 2015.

\bibitem[Bui et~al.(2016)Bui, Hern{\'a}ndez-Lobato, Hernandez-Lobato, Li, and
  Turner]{bui2016deep}
T.~Bui, D.~Hern{\'a}ndez-Lobato, J.~Hernandez-Lobato, Y.~Li, and R.~Turner.
\newblock Deep {G}aussian processes for regression using approximate
  expectation propagation.
\newblock In \emph{ICML}, 2016.

\bibitem[Cao and Gu(2020)]{cao2020generalization}
Y.~Cao and Q.~Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2020)Chen, Bai, Lee, Zhao, Wang, Xiong, and
  Socher]{chen2020towards}
M.~Chen, Y.~Bai, J.~D. Lee, T.~Zhao, H.~Wang, C.~Xiong, and R.~Socher.
\newblock Towards understanding hierarchical learning: Benefits of neural
  representations.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
L.~Chizat and F.~Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Y.~Cho and L.~Saul.
\newblock Kernel methods for deep learning.
\newblock In \emph{NeurIPS}, 2009.

\bibitem[Cutajar et~al.(2017)Cutajar, Bonilla, Michiardi, and
  Filippone]{cutajar2017random}
K.~Cutajar, E.~V. Bonilla, P.~Michiardi, and M.~Filippone.
\newblock Random feature expansions for deep {G}aussian processes.
\newblock In \emph{ICML}, 2017.

\bibitem[Dai et~al.(2016)Dai, Damianou, Gonz{\'a}lez, and
  Lawrence]{dai2016variational}
Z.~Dai, A.~C. Damianou, J.~Gonz{\'a}lez, and N.~D. Lawrence.
\newblock Variational auto-encoded deep {G}aussian processes.
\newblock In \emph{ICLR}, 2016.

\bibitem[Damianou(2015)]{damianou2015deep}
A.~Damianou.
\newblock \emph{Deep {G}aussian processes and variational propagation of
  uncertainty}.
\newblock PhD thesis, University of Sheffield, 2015.

\bibitem[Damianou and Lawrence(2013)]{damianou2013deep}
A.~Damianou and N.~Lawrence.
\newblock Deep {G}aussian processes.
\newblock In \emph{AISTATS}, 2013.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Dunlop et~al.(2018)Dunlop, Girolami, Stuart, and
  Teckentrup]{dunlop2018deep}
M.~M. Dunlop, M.~A. Girolami, A.~M. Stuart, and A.~L. Teckentrup.
\newblock How deep are deep {G}aussian processes?
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (54):\penalty0 1--46, 2018.

\bibitem[Dutordoir et~al.(2020)Dutordoir, Wilk, Artemev, and
  Hensman]{dutordoir2020bayesian}
V.~Dutordoir, M.~Wilk, A.~Artemev, and J.~Hensman.
\newblock {B}ayesian image classification with deep convolutional {G}aussian
  processes.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Dutordoir et~al.(2021{\natexlab{a}})Dutordoir, Hensman, van~der Wilk,
  Ek, Ghahramani, and Durrande]{dutordoir2021deep}
V.~Dutordoir, J.~Hensman, M.~van~der Wilk, C.~H. Ek, Z.~Ghahramani, and
  N.~Durrande.
\newblock Deep neural networks as point estimates for deep {G}aussian
  processes.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Dutordoir et~al.(2021{\natexlab{b}})Dutordoir, Salimbeni, Hambro,
  McLeod, Leibfried, Artemev, van~der Wilk, Hensman, Deisenroth, and
  John]{dutordoir2021gpflux}
V.~Dutordoir, H.~Salimbeni, E.~Hambro, J.~McLeod, F.~Leibfried, A.~Artemev,
  M.~van~der Wilk, J.~Hensman, M.~P. Deisenroth, and S.~John.
\newblock {GP}flux: A library for deep {G}aussian processes.
\newblock \emph{arXiv preprint arXiv:2104.05674}, 2021{\natexlab{b}}.

\bibitem[Duvenaud et~al.(2014)Duvenaud, Rippel, Adams, and
  Ghahramani]{duvenaud2014avoiding}
D.~Duvenaud, O.~Rippel, R.~Adams, and Z.~Ghahramani.
\newblock Avoiding pathologies in very deep networks.
\newblock In \emph{AISTATS}, 2014.

\bibitem[Foong et~al.(2020)Foong, Burt, Li, and
  Turner]{foong2020expressiveness}
A.~Y. Foong, D.~R. Burt, Y.~Li, and R.~E. Turner.
\newblock On the expressiveness of approximate inference in bayesian neural
  networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fort2020deep}
S.~Fort, G.~K. Dziugaite, M.~Paul, S.~Kharaghani, D.~M. Roy, and S.~Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{ICML}, 2016.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Weinberger, Bindel, and
  Wilson]{gardner2018gpytorch}
J.~R. Gardner, G.~Pleiss, K.~Q. Weinberger, D.~Bindel, and A.~G. Wilson.
\newblock {GP}y{T}orch: Blackbox matrix-matrix {G}aussian process inference
  with {GPU} acceleration.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Rasmussen, and
  Aitchison]{garriga2019deep}
A.~Garriga-Alonso, C.~E. Rasmussen, and L.~Aitchison.
\newblock Deep convolutional networks as shallow {G}aussian processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Geiger et~al.(2020)Geiger, Jacot, Spigler, Gabriel, Sagun, dâ€™Ascoli,
  Biroli, Hongler, and Wyart]{geiger2020scaling}
M.~Geiger, A.~Jacot, S.~Spigler, F.~Gabriel, L.~Sagun, S.~dâ€™Ascoli,
  G.~Biroli, C.~Hongler, and M.~Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (2), 2020.

\bibitem[Genton(2001)]{genton2001classes}
M.~G. Genton.
\newblock Classes of kernels for machine learning: a statistics perspective.
\newblock \emph{Journal of Machine Learning Research}, 2\penalty0
  (Dec):\penalty0 299--312, 2001.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019limitations}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Limitations of lazy training of two-layers neural networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2020neural}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Golikov(2020)]{golikov2020towards}
E.~Golikov.
\newblock Towards a general theory of infinite-width limits of neural
  classifiers.
\newblock In \emph{ICML}, 2020.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, A.~Courville, and Y.~Bengio.
\newblock \emph{Deep learning}.
\newblock MIT press Cambridge, 2016.

\bibitem[Halverson et~al.(2021)Halverson, Maiti, and
  Stoner]{halverson2021neural}
J.~Halverson, A.~Maiti, and K.~Stoner.
\newblock Neural networks and quantum field theory.
\newblock \emph{Machine Learning: Science and Technology}, 2\penalty0 (3),
  2021.

\bibitem[Havasi et~al.(2018)Havasi, Hern{\'a}ndez-Lobato, and
  Murillo-Fuentes]{havasi2018inference}
M.~Havasi, J.~M. Hern{\'a}ndez-Lobato, and J.~J. Murillo-Fuentes.
\newblock Inference in deep {G}aussian processes using stochastic gradient
  {H}amiltonian {M}onte {C}arlo.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hoffman and Gelman(2014)]{hoffman2014no}
M.~D. Hoffman and A.~Gelman.
\newblock The no-u-turn sampler: adaptively setting path lengths in
  {H}amiltonian {M}onte {C}arlo.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1593--1623, 2014.

\bibitem[Hron et~al.(2020{\natexlab{a}})Hron, Bahri, Novak, Pennington, and
  Sohl-Dickstein]{hron2020exact}
J.~Hron, Y.~Bahri, R.~Novak, J.~Pennington, and J.~Sohl-Dickstein.
\newblock Exact posterior distributions of wide bayesian neural networks.
\newblock In \emph{ICML Workshop on Uncertainty and Robustness in Deep
  Learning}, 2020{\natexlab{a}}.

\bibitem[Hron et~al.(2020{\natexlab{b}})Hron, Bahri, Sohl-Dickstein, and
  Novak]{hron2020infinite}
J.~Hron, Y.~Bahri, J.~Sohl-Dickstein, and R.~Novak.
\newblock Infinite attention: {NNGP} and {NTK} for deep attention networks.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Izmailov et~al.(2021)Izmailov, Vikram, Hoffman, and
  Wilson]{izmailov2021bayesian}
P.~Izmailov, S.~Vikram, M.~D. Hoffman, and A.~G. Wilson.
\newblock What are bayesian neural network posteriors really like?
\newblock In \emph{ICML}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017deep}
J.~Lee, Y.~Bahri, R.~Novak, S.~S. Schoenholz, J.~Pennington, and
  J.~Sohl-Dickstein.
\newblock Deep neural networks as {G}aussian processes.
\newblock In \emph{ICLR}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
J.~Lee, L.~Xiao, S.~S. Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
J.~Lee, S.~Schoenholz, J.~Pennington, B.~Adlam, L.~Xiao, R.~Novak, and
  J.~Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
H.~Li, Z.~Xu, G.~Taylor, C.~Studer, and T.~Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Li et~al.(2020)Li, Ma, and Zhang]{li2020learning}
Y.~Li, T.~Ma, and H.~R. Zhang.
\newblock Learning over-parametrized two-layer neural networks beyond {NTK}.
\newblock In \emph{COLT}, 2020.

\bibitem[Louizos and Welling(2016)]{louizos2016structured}
C.~Louizos and M.~Welling.
\newblock Structured and efficient variational deep learning with matrix
  {G}aussian posteriors.
\newblock In \emph{ICML}, 2016.

\bibitem[Lu et~al.(2020)Lu, Yang, Hao, and Shafto]{lu2020interpretable}
C.-K. Lu, S.~C.-H. Yang, X.~Hao, and P.~Shafto.
\newblock Interpretable deep {G}aussian processes with moments.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu2017expressive}
Z.~Lu, H.~Pu, F.~Wang, Z.~Hu, and L.~Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
A.~G. d.~G. Matthews, M.~Rowland, J.~Hron, R.~E. Turner, and Z.~Ghahramani.
\newblock {G}aussian process behaviour in wide deep neural networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
S.~Mei, A.~Montanari, and P.-M. Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Micchelli et~al.(2006)Micchelli, Xu, and
  Zhang]{micchelli2006universal}
C.~A. Micchelli, Y.~Xu, and H.~Zhang.
\newblock Universal kernels.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0 (12), 2006.

\bibitem[Mont{\'u}far et~al.(2014)Mont{\'u}far, Pascanu, Cho, and
  Bengio]{montufar2014number}
G.~Mont{\'u}far, R.~Pascanu, K.~Cho, and Y.~Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2020deep}
P.~Nakkiran, G.~Kaplun, Y.~Bansal, T.~Yang, B.~Barak, and I.~Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In \emph{ICLR}, 2020.

\bibitem[Neal(1995)]{neal1995bayesian}
R.~M. Neal.
\newblock \emph{{B}ayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Nguyen and Hein(2017)]{nguyen2017loss}
Q.~Nguyen and M.~Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Novak et~al.(2019)Novak, Xiao, Lee, Bahri, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{novak2019bayesian}
R.~Novak, L.~Xiao, J.~Lee, Y.~Bahri, G.~Yang, J.~Hron, D.~A. Abolafia,
  J.~Pennington, and J.~Sohl-Dickstein.
\newblock {B}ayesian deep convolutional networks with many channels are
  {G}aussian processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Ober and Aitchison(2021)]{ober2020global}
S.~W. Ober and L.~Aitchison.
\newblock Global inducing point variational posteriors for {B}ayesian neural
  networks and deep {G}aussian processes.
\newblock In \emph{ICML}, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock {P}y{T}orch: An imperative style, high-performance deep learning
  library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{poole2016exponential}
B.~Poole, S.~Lahiri, M.~Raghu, J.~Sohl-Dickstein, and S.~Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{raghu2017expressive}
M.~Raghu, B.~Poole, J.~Kleinberg, S.~Ganguli, and J.~Sohl-Dickstein.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Rahimi et~al.(2007)Rahimi, Recht, et~al.]{rahimi2007random}
A.~Rahimi, B.~Recht, et~al.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{NeurIPS}, 2007.

\bibitem[Rasmussen and Williams(2006)]{rasmussen2006gaussian}
C.~E. Rasmussen and C.~Williams.
\newblock \emph{{G}aussian processes for machine learning}, volume~1.
\newblock MIT Press, 2006.

\bibitem[Romero et~al.(2015)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock In \emph{ICLR}, 2015.

\bibitem[Salimbeni and Deisenroth(2017)]{salimbeni2017doubly}
H.~Salimbeni and M.~Deisenroth.
\newblock Doubly stochastic variational inference for deep {G}aussian
  processes.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Schoenberg(1938)]{schoenberg1938metric}
I.~J. Schoenberg.
\newblock Metric spaces and completely monotone functions.
\newblock \emph{Annals of Mathematics}, pages 811--841, 1938.

\bibitem[Shankar et~al.(2020)Shankar, Fang, Guo, Fridovich-Keil, Ragan-Kelley,
  Schmidt, and Recht]{shankar2020neural}
V.~Shankar, A.~Fang, W.~Guo, S.~Fridovich-Keil, J.~Ragan-Kelley, L.~Schmidt,
  and B.~Recht.
\newblock Neural kernels without tangents.
\newblock In \emph{ICML}, 2020.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018theoretical}
M.~Soltanolkotabi, A.~Javanmard, and J.~D. Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (2):\penalty0 742--769, 2018.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
M.~Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In \emph{COLT}, 2016.

\bibitem[Vladimirova et~al.(2019)Vladimirova, Verbeek, Mesejo, and
  Arbel]{vladimirova2019understanding}
M.~Vladimirova, J.~Verbeek, P.~Mesejo, and J.~Arbel.
\newblock Understanding priors in {B}ayesian neural networks at the unit level.
\newblock In \emph{ICML}, 2019.

\bibitem[Wang et~al.(2016)Wang, Brubaker, Chaib-Draa, and
  Urtasun]{wang2016sequential}
Y.~Wang, M.~Brubaker, B.~Chaib-Draa, and R.~Urtasun.
\newblock Sequential inference for deep {G}aussian process.
\newblock In \emph{AISTATS}, 2016.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
C.~Wei, J.~Lee, Q.~Liu, and T.~Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Yaglom(1987)]{yaglom1987correlation}
A.~Yaglom.
\newblock \emph{Correlation theory of stationary and related random functions}.
\newblock Springer Series in Statistics, New York, 1987.

\bibitem[Yang(2019)]{yang2019tensor}
G.~Yang.
\newblock Tensor programs {I}: Wide feedforward or recurrent neural networks of
  any architecture are gaussian processes.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Yang(2020)]{yang2020tensor}
G.~Yang.
\newblock Tensor programs {II}: Neural tangent kernel for any architecture.
\newblock \emph{arXiv preprint arXiv:2006.14548}, 2020.

\bibitem[Yang and Hu(2021)]{yang2020feature}
G.~Yang and E.~J. Hu.
\newblock Tensor programs {IV}: {F}eature learning in infinite-width neural
  networks.
\newblock In \emph{ICML}, 2021.

\bibitem[Yehudai and Shamir(2019)]{yehudai2019power}
G.~Yehudai and O.~Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Zavatone-Veth and Pehlevan(2021)]{zavatoneveth2021exact}
J.~A. Zavatone-Veth and C.~Pehlevan.
\newblock Exact priors of finite neural networks.
\newblock In \emph{NeurIPS}, 2021.

\end{thebibliography}
