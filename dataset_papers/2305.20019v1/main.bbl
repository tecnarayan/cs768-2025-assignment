\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh,
  Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
Anil, C., Wu, Y., Andreassen, A.~J., Lewkowycz, A., Misra, V., Ramasesh, V.~V.,
  Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B.
\newblock Exploring length generalization in large language models.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=zSkYVeX7bC4}.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2015neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1409.0473}.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai2019deq}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock Deep equilibrium models.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf}.

\bibitem[Banino et~al.(2021)Banino, Balaguer, and Blundell]{banio2021ponder}
Banino, A., Balaguer, J., and Blundell, C.
\newblock Pondernet: Learning to ponder.
\newblock \emph{ICML Workshop}, abs/2107.05407, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.05407}.

\bibitem[Bird et~al.(2009)Bird, Klein, and Loper]{nltk}
Bird, S., Klein, E., and Loper, E.
\newblock \emph{Natural Language Processing with Python}.
\newblock O'Reilly Media, Inc., 1st edition, 2009.
\newblock ISBN 0596516495.

\bibitem[Carrasco et~al.(1995)Carrasco, Evert, Chang, and Katz]{Carrasco1995}
Carrasco, M., Evert, D.~L., Chang, I., and Katz, S.~M.
\newblock The eccentricity effect: Target eccentricity affects performance on
  conjunction searches.
\newblock \emph{Perception {\&} Psychophysics}, 57\penalty0 (8):\penalty0
  1241--1261, November 1995.
\newblock \doi{10.3758/bf03208380}.
\newblock URL \url{https://doi.org/10.3758/bf03208380}.

\bibitem[Chang et~al.(2021)Chang, Xu, Xu, and Tu]{chang-etal-2021-convolutions}
Chang, T., Xu, Y., Xu, W., and Tu, Z.
\newblock Convolutions and self-attention: {R}e-interpreting relative positions
  in pre-trained language models.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  4322--4333,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.333}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.333}.

\bibitem[Chung et~al.(2014)Chung, G{\"{u}}l{\c{c}}ehre, Cho, and
  Bengio]{chung2014gru}
Chung, J., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Cho, K., and Bengio, Y.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{NeurIPS Workshop}, abs/1412.3555, 2014.
\newblock URL \url{http://arxiv.org/abs/1412.3555}.

\bibitem[Csord{\'a}s et~al.(2022)Csord{\'a}s, Irie, and
  Schmidhuber]{csordas2022the}
Csord{\'a}s, R., Irie, K., and Schmidhuber, J.
\newblock The neural data router: Adaptive control flow in transformers
  improves systematic generalization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=KBQP4A_J1K}.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai-etal-2019-transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2978--2988, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1285}.
\newblock URL \url{https://aclanthology.org/P19-1285}.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L.
\newblock Universal transformers.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyzdRiR9Y7}.

\bibitem[Dubois et~al.(2020)Dubois, Dagan, Hupkes, and
  Bruni]{dubois-etal-2020-location}
Dubois, Y., Dagan, G., Hupkes, D., and Bruni, E.
\newblock {L}ocation {A}ttention for {E}xtrapolation to {L}onger {S}equences.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  403--413, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.39}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.39}.

\bibitem[Dufter et~al.(2022)Dufter, Schmitt, and Schütze]{duffer2022pi}
Dufter, P., Schmitt, M., and Schütze, H.
\newblock {Position Information in Transformers: An Overview}.
\newblock \emph{Computational Linguistics}, 48\penalty0 (3):\penalty0 733--763,
  09 2022.
\newblock ISSN 0891-2017.
\newblock \doi{10.1162/coli_a_00445}.
\newblock URL \url{https://doi.org/10.1162/coli\_a\_00445}.

\bibitem[Garg et~al.(2022)Garg, Vaidyanath, Kim, Song, and Ermon]{garg2022lisa}
Garg, D., Vaidyanath, S., Kim, K., Song, J., and Ermon, S.
\newblock {LISA}: Learning interpretable skill abstractions from language.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=XZhipvOUBB}.

\bibitem[Graves(2016)]{graves2016adaptive}
Graves, A.
\newblock Adaptive computation time for recurrent neural networks.
\newblock \emph{ArXiv}, abs/1603.08983, 2016.
\newblock URL \url{http://arxiv.org/abs/1603.08983}.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014ntm}
Graves, A., Wayne, G., and Danihelka, I.
\newblock Neural turing machines.
\newblock \emph{ArXiv}, abs/1410.5401, 2014.
\newblock URL \url{http://arxiv.org/abs/1410.5401}.

\bibitem[Kaiser \& Sutskever(2016)Kaiser and Sutskever]{Kaiser2016neuralgpu}
Kaiser, L. and Sutskever, I.
\newblock Neural gpus learn algorithms.
\newblock In \emph{ICLR (Poster)}, 2016.
\newblock URL \url{http://arxiv.org/abs/1511.08228}.

\bibitem[Ke et~al.(2021)Ke, He, and Liu]{ke2021rethinking}
Ke, G., He, D., and Liu, T.-Y.
\newblock Rethinking positional encoding in language pre-training.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=09-528y2Fgf}.

\bibitem[Keysers et~al.(2020)Keysers, Sch{\"a}rli, Scales, Buisman, Furrer,
  Kashubin, Momchev, Sinopalnikov, Stafiniak, Tihon, Tsarkov, Wang, van Zee,
  and Bousquet]{keysers2020measuring}
Keysers, D., Sch{\"a}rli, N., Scales, N., Buisman, H., Furrer, D., Kashubin,
  S., Momchev, N., Sinopalnikov, D., Stafiniak, L., Tihon, T., Tsarkov, D.,
  Wang, X., van Zee, M., and Bousquet, O.
\newblock Measuring compositional generalization: A comprehensive method on
  realistic data.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SygcCnNKwr}.

\bibitem[Kim et~al.(2022)Kim, Linzen, and Smolensky]{kim2022uncontrolled}
Kim, N., Linzen, T., and Smolensky, P.
\newblock Uncontrolled lexical exposure leads to overestimation of
  compositional generalization in pretrained models, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.10769}.

\bibitem[Lake \& Baroni(2018)Lake and Baroni]{lake2018generalization}
Lake, B. and Baroni, M.
\newblock Generalization without systematicity: On the compositional skills of
  sequence-to-sequence recurrent networks.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  2873--2882. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/lake18a.html}.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis-etal-2020-bart}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
  Stoyanov, V., and Zettlemoyer, L.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  7871--7880, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.703}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.703}.

\bibitem[Liang et~al.(2021)Liang, Anil, Wu, and
  Grosse]{Liang2021OutofDistributionGW}
Liang, K.-P., Anil, C., Wu, Y., and Grosse, R.~B.
\newblock Out-of-distribution generalization with deep equilibrium models.
\newblock In \emph{ICML Workshop}, 2021.

\bibitem[Likhomanenko et~al.(2021)Likhomanenko, Xu, Synnaeve, Collobert, and
  Rogozhnikov]{likhomenko2021cape}
Likhomanenko, T., Xu, Q., Synnaeve, G., Collobert, R., and Rogozhnikov, A.
\newblock Cape: Encoding relative positions with continuous augmented
  positional embeddings.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  16079--16092. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/865bf46435bd84fa5d89f64cf3ba7347-Paper.pdf}.

\bibitem[Liska et~al.(2018)Liska, Kruszewski, and Baroni]{liska2018memorize}
Liska, A., Kruszewski, G., and Baroni, M.
\newblock Memorize or generalize? searching for a compositional {RNN} in a
  haystack.
\newblock \emph{ICML Workshop}, abs/1802.06467, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.06467}.

\bibitem[Luo et~al.(2021)Luo, Li, Cai, He, Peng, Zheng, Ke, Wang, and
  Liu]{luo2021stable}
Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S., Ke, G., Wang, L., and
  Liu, T.-Y.
\newblock Stable, fast and accurate: Kernelized attention with relative
  positional encoding.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  22795--22807. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf}.

\bibitem[Luo et~al.(2022)Luo, Li, Zheng, Liu, Wang, and He]{luo2022your}
Luo, S., Li, S., Zheng, S., Liu, T.-Y., Wang, L., and He, D.
\newblock Your transformer may not be as powerful as you expect.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=NQFFNdsOGD}.

\bibitem[Luong et~al.(2015)Luong, Pham, and Manning]{luong-etal-2015-effective}
Luong, T., Pham, H., and Manning, C.~D.
\newblock Effective approaches to attention-based neural machine translation.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1412--1421, Lisbon, Portugal, September
  2015. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D15-1166}.
\newblock URL \url{https://aclanthology.org/D15-1166}.

\bibitem[Newman et~al.(2020)Newman, Hewitt, Liang, and
  Manning]{Newman2020TheED}
Newman, B., Hewitt, J., Liang, P., and Manning, C.~D.
\newblock The eos decision and length extrapolation.
\newblock In \emph{BlackboxNLP Workshop on Analyzing and Interpreting Neural
  Networks for NLP}, 2020.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur{-}Ari, Michalewski, Austin,
  Bieber, Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{maxwell2021show}
Nye, M.~I., Andreassen, A.~J., Gur{-}Ari, G., Michalewski, H., Austin, J.,
  Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and
  Odena, A.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{ArXiv}, abs/2112.00114, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.00114}.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{press2022train}
Press, O., Smith, N., and Lewis, M.
\newblock Train short, test long: Attention with linear biases enables input
  length extrapolation.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=R8sQPpGCv0}.

\bibitem[Qu et~al.(2021)Qu, Niu, and Mo]{qu-etal-2021-explore}
Qu, A., Niu, J., and Mo, S.
\newblock Explore better relative position embeddings from encoding perspective
  for transformer models.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2989--2997, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.237}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.237}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Schmidhuber(2012)]{Schmidhuber2012delimiting}
Schmidhuber, J.
\newblock Self-delimiting neural networks.
\newblock \emph{arXiv}, abs/1210.0118, 2012.
\newblock URL \url{http://arxiv.org/abs/1210.0118}.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw-etal-2018-self}
Shaw, P., Uszkoreit, J., and Vaswani, A.
\newblock Self-attention with relative position representations.
\newblock In \emph{Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, pp.\  464--468, New Orleans,
  Louisiana, June 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-2074}.
\newblock URL \url{https://aclanthology.org/N18-2074}.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{Su2021Ro}
Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{ArXiv}, abs/2104.09864, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.09864}.

\bibitem[Sun et~al.(2022)Sun, Dong, Patra, Ma, Huang, Benhaim, Chaudhary, Song,
  and Wei]{sun2022length}
Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V.,
  Song, X., and Wei, F.
\newblock A length-extrapolatable transformer.
\newblock \emph{arXiv preprint arXiv:2212.10554}, 2022.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014seq2seq}
Sutskever, I., Vinyals, O., and Le, Q.~V.
\newblock Sequence to sequence learning with neural networks.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and
  Weinberger, K. (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Wang et~al.(2020)Wang, Zhao, Lioma, Li, Zhang, and
  Simonsen]{Wang2020Encoding}
Wang, B., Zhao, D., Lioma, C., Li, Q., Zhang, P., and Simonsen, J.~G.
\newblock Encoding word order in complex embeddings.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Hke-WTVtwr}.

\bibitem[Wennberg \& Henter(2021)Wennberg and
  Henter]{wennberg-henter-2021-case}
Wennberg, U. and Henter, G.~E.
\newblock The case for translation-invariant self-attention in
  transformer-based language models.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 2: Short Papers)}, pp.\  130--140,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-short.18}.
\newblock URL \url{https://aclanthology.org/2021.acl-short.18}.

\bibitem[Wu et~al.(2021)Wu, Wu, and Huang]{wu-etal-2021-da}
Wu, C., Wu, F., and Huang, Y.
\newblock {DA}-transformer: Distance-aware transformer.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  2059--2068, Online, June 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.166}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.166}.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2018pay}
Wu, F., Fan, A., Baevski, A., Dauphin, Y., and Auli, M.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SkVhlh09tX}.

\bibitem[Yang et~al.(2018)Yang, Tu, Wong, Meng, Chao, and
  Zhang]{yang-etal-2018-modeling}
Yang, B., Tu, Z., Wong, D.~F., Meng, F., Chao, L.~S., and Zhang, T.
\newblock Modeling localness for self-attention networks.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  4449--4458, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1475}.
\newblock URL \url{https://aclanthology.org/D18-1475}.

\end{thebibliography}
