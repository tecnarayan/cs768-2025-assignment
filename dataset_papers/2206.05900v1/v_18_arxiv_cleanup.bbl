\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2020]{NEURIPS2020_e894d787}
Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. (2020).
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H., editors, {\em Advances in Neural Information Processing Systems},
  volume~33, pages 20095--20107. Curran Associates, Inc.

\bibitem[Agarwal et~al., 2022]{DBLP:journals/corr/abs-2205-14571}
Agarwal, A., Song, Y., Sun, W., Wang, K., Wang, M., and Zhang, X. (2022).
\newblock Provable benefits of representational transfer in reinforcement
  learning.
\newblock {\em CoRR}, abs/2205.14571.

\bibitem[Arora et~al., 2020]{DBLP:conf/icml/AroraDKLS20}
Arora, S., Du, S.~S., Kakade, S.~M., Luo, Y., and Saunshi, N. (2020).
\newblock Provable representation learning for imitation learning via bi-level
  optimization.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 367--376. {PMLR}.

\bibitem[Arulkumaran et~al., 2022]{DBLP:journals/corr/abs-2202-11960}
Arulkumaran, K., Ashley, D.~R., Schmidhuber, J., and Srivastava, R.~K. (2022).
\newblock All you need is supervised learning: From imitation learning to
  meta-rl with upside down {RL}.
\newblock {\em CoRR}, abs/2202.11960.

\bibitem[Azar et~al., 2011]{DBLP:conf/nips/AzarMGK11}
Azar, M.~G., Munos, R., Ghavamzadeh, M., and Kappen, H.~J. (2011).
\newblock Speedy q-learning.
\newblock In Shawe{-}Taylor, J., Zemel, R.~S., Bartlett, P.~L., Pereira, F.
  C.~N., and Weinberger, K.~Q., editors, {\em Advances in Neural Information
  Processing Systems 24: 25th Annual Conference on Neural Information
  Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011,
  Granada, Spain}, pages 2411--2419.

\bibitem[Azar et~al., 2017]{DBLP:conf/icml/AzarOM17}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In Precup, D. and Teh, Y.~W., editors, {\em Proceedings of the 34th
  International Conference on Machine Learning, {ICML} 2017, Sydney, NSW,
  Australia, 6-11 August 2017}, volume~70 of {\em Proceedings of Machine
  Learning Research}, pages 263--272. {PMLR}.

\bibitem[Azizi et~al., 2022]{DBLP:journals/corr/abs-2202-13001}
Azizi, M.~J., Duong, T., Abbasi{-}Yadkori, Y., Gy{\"{o}}rgy, A., Vernade, C.,
  and Ghavamzadeh, M. (2022).
\newblock Non-stationary bandits and meta-learning with a small set of optimal
  arms.
\newblock {\em CoRR}, abs/2202.13001.

\bibitem[Brunskill and Li, 2013]{DBLP:conf/uai/BrunskillL13}
Brunskill, E. and Li, L. (2013).
\newblock Sample complexity of multi-task reinforcement learning.
\newblock In Nicholson, A.~E. and Smyth, P., editors, {\em Proceedings of the
  Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, {UAI}
  2013, Bellevue, WA, USA, August 11-15, 2013}. {AUAI} Press.

\bibitem[Cai et~al., 2020]{DBLP:conf/icml/CaiYJW20}
Cai, Q., Yang, Z., Jin, C., and Wang, Z. (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 1283--1294. {PMLR}.

\bibitem[Calandriello et~al., 2015]{DBLP:journals/ia/CalandrielloLR15}
Calandriello, D., Lazaric, A., and Restelli, M. (2015).
\newblock Sparse multi-task reinforcement learning.
\newblock {\em Intelligenza Artificiale}, 9(1):5--20.

\bibitem[Cella et~al., 2022]{DBLP:journals/corr/abs-2202-10066}
Cella, L., Lounici, K., and Pontil, M. (2022).
\newblock Multi-task representation learning with stochastic linear bandits.
\newblock {\em CoRR}, abs/2202.10066.

\bibitem[Cella and Pontil, 2021]{DBLP:conf/uai/CellaP21}
Cella, L. and Pontil, M. (2021).
\newblock Multi-task and meta-learning with sparse linear bandits.
\newblock In de~Campos, C.~P., Maathuis, M.~H., and Quaeghebeur, E., editors,
  {\em Proceedings of the Thirty-Seventh Conference on Uncertainty in
  Artificial Intelligence, {UAI} 2021, Virtual Event, 27-30 July 2021}, volume
  161 of {\em Proceedings of Machine Learning Research}, pages 1692--1702.
  {AUAI} Press.

\bibitem[Dann et~al., 2017]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E. (2017).
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.07710}.

\bibitem[D'Eramo et~al., 2020]{DBLP:conf/iclr/DEramoTBR020}
D'Eramo, C., Tateo, D., Bonarini, A., Restelli, M., and Peters, J. (2020).
\newblock Sharing knowledge in multi-task deep reinforcement learning.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[Deshmukh et~al., 2017]{DBLP:conf/nips/DeshmukhDS17}
Deshmukh, A.~A., Dogan, {\"{U}}., and Scott, C. (2017).
\newblock Multi-task learning for contextual bandits.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pages
  4848--4856.

\bibitem[Du et~al., 2019]{du2019provably}
Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J.
  (2019).
\newblock Provably efficient rl with rich observations via latent state
  decoding.
\newblock In {\em International Conference on Machine Learning}, pages
  1665--1674. PMLR.

\bibitem[Du et~al., 2020]{du2020few}
Du, S.~S., Hu, W., Kakade, S.~M., Lee, J.~D., and Lei, Q. (2020).
\newblock Few-shot learning via learning the representation, provably.
\newblock {\em arXiv preprint arXiv:2002.09434}.

\bibitem[Du et~al., 2021]{du2021bilinear}
Du, S.~S., Kakade, S.~M., Lee, J.~D., Lovett, S., Mahajan, G., Sun, W., and
  Wang, R. (2021).
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock {\em arXiv preprint arXiv:2103.10897}.

\bibitem[Hu et~al., 2021]{DBLP:conf/icml/HuCJL021}
Hu, J., Chen, X., Jin, C., Li, L., and Wang, L. (2021).
\newblock Near-optimal representation learning for linear bandits and linear
  {RL}.
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 4349--4358. {PMLR}.

\bibitem[Jaksch et~al., 2010]{DBLP:journals/jmlr/JakschOA10}
Jaksch, T., Ortner, R., and Auer, P. (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em J. Mach. Learn. Res.}, 11:1563--1600.

\bibitem[Jiang et~al., 2017]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
  (2017).
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In {\em International Conference on Machine Learning}, pages
  1704--1713. PMLR.

\bibitem[Jin et~al., 2018]{DBLP:conf/nips/JinABJ18}
Jin, C., Allen{-}Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018).
\newblock Is q-learning provably efficient?
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pages 4868--4878.

\bibitem[Jin et~al., 2021a]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021a).
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Jin et~al., 2021b]{Jin2021IsPP}
Jin, Y., Yang, Z., and Wang, Z. (2021b).
\newblock Is pessimism provably efficient for offline rl?
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 5084--5096. {PMLR}.

\bibitem[Koenig and Simmons, 1993]{DBLP:conf/aaai/KoenigS93}
Koenig, S. and Simmons, R.~G. (1993).
\newblock Complexity analysis of real-time reinforcement learning.
\newblock In Fikes, R. and Lehnert, W.~G., editors, {\em Proceedings of the
  11th National Conference on Artificial Intelligence. Washington, DC, USA,
  July 11-15, 1993}, pages 99--107. {AAAI} Press / The {MIT} Press.

\bibitem[Kong et~al., 2020]{DBLP:conf/icml/KongSSKO20}
Kong, W., Somani, R., Song, Z., Kakade, S.~M., and Oh, S. (2020).
\newblock Meta-learning for mixed linear regression.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 5394--5404. {PMLR}.

\bibitem[Lattimore and Hutter, 2012]{DBLP:conf/alt/LattimoreH12}
Lattimore, T. and Hutter, M. (2012).
\newblock {PAC} bounds for discounted mdps.
\newblock In Bshouty, N.~H., Stoltz, G., Vayatis, N., and Zeugmann, T.,
  editors, {\em Algorithmic Learning Theory - 23rd International Conference,
  {ALT} 2012, Lyon, France, October 29-31, 2012. Proceedings}, volume 7568 of
  {\em Lecture Notes in Computer Science}, pages 320--334. Springer.

\bibitem[Lu et~al., 2021]{lu2021power}
Lu, R., Huang, G., and Du, S.~S. (2021).
\newblock On the power of multitask representation learning in linear mdp.
\newblock {\em arXiv preprint arXiv:2106.08053}.

\bibitem[Lu et~al., 2022]{DBLP:journals/corr/abs-2205-15701}
Lu, R., Zhao, A., Du, S.~S., and Huang, G. (2022).
\newblock Provable general function class representation learning in multitask
  bandits and mdps.
\newblock {\em CoRR}, abs/2205.15701.

\bibitem[Maurer et~al., 2016]{DBLP:journals/jmlr/MaurerPR16}
Maurer, A., Pontil, M., and Romera{-}Paredes, B. (2016).
\newblock The benefit of multitask representation learning.
\newblock {\em J. Mach. Learn. Res.}, 17:81:1--81:32.

\bibitem[Misra et~al., 2020]{misra2020kinematic}
Misra, D., Henaff, M., Krishnamurthy, A., and Langford, J. (2020).
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  6961--6971. PMLR.

\bibitem[Modi et~al., 2021]{modi2021model}
Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2021).
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock {\em arXiv preprint arXiv:2102.07035}.

\bibitem[M{\"{u}}ller and Pacchiano, 2022]{DBLP:journals/corr/abs-2201-08732}
M{\"{u}}ller, R. and Pacchiano, A. (2022).
\newblock Meta learning mdps with linear transition models.
\newblock {\em CoRR}, abs/2201.08732.

\bibitem[Neu and Pike{-}Burke, 2020]{DBLP:conf/nips/NeuP20}
Neu, G. and Pike{-}Burke, C. (2020).
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.,
  editors, {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}.

\bibitem[Osband et~al., 2016]{DBLP:conf/icml/OsbandRW16}
Osband, I., Roy, B.~V., and Wen, Z. (2016).
\newblock Generalization and exploration via randomized value functions.
\newblock In Balcan, M. and Weinberger, K.~Q., editors, {\em Proceedings of the
  33nd International Conference on Machine Learning, {ICML} 2016, New York
  City, NY, USA, June 19-24, 2016}, volume~48 of {\em {JMLR} Workshop and
  Conference Proceedings}, pages 2377--2386. JMLR.org.

\bibitem[Qin et~al., 2022]{DBLP:journals/corr/abs-2201-04805}
Qin, Y., Menara, T., Oymak, S., Ching, S., and Pasqualetti, F. (2022).
\newblock Non-stationary representation learning in sequential linear bandits.
\newblock {\em CoRR}, abs/2201.04805.

\bibitem[Ren et~al., 2021]{DBLP:conf/nips/RenLDDS21}
Ren, T., Li, J., Dai, B., Du, S.~S., and Sanghavi, S. (2021).
\newblock Nearly horizon-free offline reinforcement learning.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W., editors, {\em Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pages 15621--15634.

\bibitem[Sidford et~al., 2018]{DBLP:conf/soda/SidfordWWY18}
Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018).
\newblock Variance reduced value iteration and faster algorithms for solving
  markov decision processes.
\newblock In Czumaj, A., editor, {\em Proceedings of the Twenty-Ninth Annual
  {ACM-SIAM} Symposium on Discrete Algorithms, {SODA} 2018, New Orleans, LA,
  USA, January 7-10, 2018}, pages 770--787. {SIAM}.

\bibitem[Sodhani et~al., 2021]{DBLP:conf/icml/Sodhani0P21}
Sodhani, S., Zhang, A., and Pineau, J. (2021).
\newblock Multi-task reinforcement learning with context-based representations.
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 9767--9779. {PMLR}.

\bibitem[Strehl et~al., 2006]{DBLP:conf/icml/StrehlLWLL06}
Strehl, A.~L., Li, L., Wiewiora, E., Langford, J., and Littman, M.~L. (2006).
\newblock {PAC} model-free reinforcement learning.
\newblock In Cohen, W.~W. and Moore, A.~W., editors, {\em Machine Learning,
  Proceedings of the Twenty-Third International Conference {(ICML} 2006),
  Pittsburgh, Pennsylvania, USA, June 25-29, 2006}, volume 148 of {\em {ACM}
  International Conference Proceeding Series}, pages 881--888. {ACM}.

\bibitem[Sun et~al., 2019a]{DBLP:conf/colt/SunJKA019}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. (2019a).
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and
  exponential improvements over model-free approaches.
\newblock In Beygelzimer, A. and Hsu, D., editors, {\em Conference on Learning
  Theory, {COLT} 2019, 25-28 June 2019, Phoenix, AZ, {USA}}, volume~99 of {\em
  Proceedings of Machine Learning Research}, pages 2898--2933. {PMLR}.

\bibitem[Sun et~al., 2019b]{sun2019model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. (2019b).
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In {\em Conference on learning theory}, pages 2898--2933. PMLR.

\bibitem[Teh et~al., 2017]{DBLP:conf/nips/TehBCQKHHP17}
Teh, Y.~W., Bapst, V., Czarnecki, W.~M., Quan, J., Kirkpatrick, J., Hadsell,
  R., Heess, N., and Pascanu, R. (2017).
\newblock Distral: Robust multitask reinforcement learning.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pages
  4496--4506.

\bibitem[Tripuraneni et~al., 2021]{DBLP:conf/icml/TripuraneniJJ21}
Tripuraneni, N., Jin, C., and Jordan, M.~I. (2021).
\newblock Provable meta-learning of linear representations.
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 10434--10443. {PMLR}.

\bibitem[Uehara et~al., 2021]{uehara2021representation}
Uehara, M., Zhang, X., and Sun, W. (2021).
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock {\em arXiv preprint arXiv:2110.04652}.

\bibitem[Wang et~al., 2021a]{DBLP:conf/iclr/WangFK21}
Wang, R., Foster, D.~P., and Kakade, S.~M. (2021a).
\newblock What are the statistical limits of offline {RL} with linear function
  approximation?
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net.

\bibitem[Wang et~al., 2021b]{DBLP:conf/iclr/0001WDK21}
Wang, Y., Wang, R., Du, S.~S., and Krishnamurthy, A. (2021b).
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net.

\bibitem[Xie et~al., 2021a]{DBLP:conf/nips/XieCJMA21}
Xie, T., Cheng, C., Jiang, N., Mineiro, P., and Agarwal, A. (2021a).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W., editors, {\em Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pages 6683--6694.

\bibitem[Xie et~al., 2021b]{xie2021policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021b).
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34.

\bibitem[Yang et~al., 2021]{DBLP:conf/iclr/YangHLD21}
Yang, J., Hu, W., Lee, J.~D., and Du, S.~S. (2021).
\newblock Impact of representation learning in linear bandits.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net.

\bibitem[Yang and Wang, 2020]{DBLP:conf/icml/YangW20}
Yang, L. and Wang, M. (2020).
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 10746--10756. {PMLR}.

\bibitem[Yin et~al., 2022]{DBLP:journals/corr/abs-2203-05804}
Yin, M., Duan, Y., Wang, M., and Wang, Y. (2022).
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.
\newblock {\em CoRR}, abs/2203.05804.

\bibitem[Yin and Wang, 2021a]{DBLP:conf/nips/YinW21a}
Yin, M. and Wang, Y. (2021a).
\newblock Optimal uniform {OPE} and model-based offline reinforcement learning
  in time-homogeneous, reward-free and task-agnostic settings.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W., editors, {\em Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pages 12890--12903.

\bibitem[Yin and Wang, 2021b]{yin2021towards}
Yin, M. and Wang, Y.-X. (2021b).
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock {\em Advances in neural information processing systems}, 34.

\bibitem[Zanette, 2021]{DBLP:conf/icml/Zanette21}
Zanette, A. (2021).
\newblock Exponential lower bounds for batch reinforcement learning: Batch {RL}
  can be exponentially harder than online {RL}.
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 12287--12297. {PMLR}.

\bibitem[Zanette et~al., 2020a]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020a).
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In {\em International Conference on Machine Learning}, pages
  10978--10989. PMLR.

\bibitem[Zanette et~al., 2020b]{zanette2020provably}
Zanette, A., Lazaric, A., Kochenderfer, M.~J., and Brunskill, E. (2020b).
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock {\em arXiv preprint arXiv:2008.07737}.

\bibitem[Zanette et~al., 2021]{DBLP:conf/nips/ZanetteWB21}
Zanette, A., Wainwright, M.~J., and Brunskill, E. (2021).
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W., editors, {\em Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pages 13626--13640.

\bibitem[Zhang et~al., 2021]{zhang2021provably}
Zhang, W., He, J., Zhou, D., Zhang, A., and Gu, Q. (2021).
\newblock Provably efficient representation learning in low-rank markov
  decision processes.

\bibitem[Zhang et~al., 2022]{zhang2022efficient}
Zhang, X., Song, Y., Uehara, M., Wang, M., Sun, W., and Agarwal, A. (2022).
\newblock Efficient reinforcement learning in block mdps: A model-free
  representation learning approach.
\newblock {\em arXiv preprint arXiv:2202.00063}.

\end{thebibliography}
