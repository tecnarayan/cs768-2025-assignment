\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
B.~D. Argall, S.~Chernova, M.~Veloso, and B.~Browning.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and Autonomous Systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Bentivegna et~al.(2004)Bentivegna, Atkeson, and
  Cheng]{Bentivegna2004robot}
D.~C. Bentivegna, C.~G. Atkeson, and G.~Cheng.
\newblock Learning tasks from observation and practice.
\newblock \emph{Robotics and Autonomous Systems}, 47\penalty0 (2):\penalty0
  163--169, 2004.
\newblock Robot Learning from Demonstration.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesvari, and
  Schuurmans]{dai2020coindice}
B.~Dai, O.~Nachum, Y.~Chow, L.~Li, C.~Szepesvari, and D.~Schuurmans.
\newblock {CoinDICE}: Off-policy confidence interval estimation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~33, pages 9398--9411, 2020.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Levine, and
  Salakhutdinov]{eysenbach2021replacing}
B.~Eysenbach, S.~Levine, and R.~R. Salakhutdinov.
\newblock Replacing rewards with examples: Example-based policy search via
  recursive classification.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
J.~Fu, A.~Kumar, O.~Nachum, G.~Tucker, and S.~Levine.
\newblock D4{RL}: Datasets for deep data-driven reinforcement learning, 2020.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019bcq}
S.~Fujimoto, D.~Meger, and D.~Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2052--2062. PMLR, 2019.

\bibitem[Garg et~al.(2021)Garg, Chakraborty, Cundy, Song, and
  Ermon]{garg2021iq}
D.~Garg, S.~Chakraborty, C.~Cundy, J.~Song, and S.~Ermon.
\newblock Iq-learn: Inverse soft-q learning for imitation.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~C. Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2014.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improvedgan}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~Courville.
\newblock Improved training of {W}asserstein {GAN}s.
\newblock \emph{arXiv preprint arXiv:1704.00028}, 2017.

\bibitem[Ho and Ermon(2016)]{ho2016gail}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Y.~Jin, Z.~Yang, and Z.~Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pages 5084--5096, 18--24 Jul 2021.

\bibitem[Ke et~al.(2020)Ke, Choudhury, Barnes, Sun, Lee, and
  Srinivasa]{ke2020generalil}
L.~Ke, S.~Choudhury, M.~Barnes, W.~Sun, G.~Lee, and S.~Srinivasa.
\newblock Imitation learning as f-divergence minimization.
\newblock In \emph{International Workshop on the Algorithmic Foundations of
  Robotics}, pages 313--329. Springer, 2020.

\bibitem[Kidambi et~al.(2021)Kidambi, Chang, and Sun]{kidambi2021mobile}
R.~Kidambi, J.~D. Chang, and W.~Sun.
\newblock Mob{ILE}: Model-based imitation learning from observation alone.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=_Rtm4rYnIIL}.

\bibitem[Kim et~al.(2022)Kim, Seo, Lee, Jeon, Hwang, Yang, and
  Kim]{kim2022demodice}
G.-H. Kim, S.~Seo, J.~Lee, W.~Jeon, H.~Hwang, H.~Yang, and K.-E. Kim.
\newblock Demo{DICE}: Offline imitation learning with supplementary imperfect
  demonstrations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Kostrikov et~al.(2019)Kostrikov, Agrawal, Dwibedi, Levine, and
  Tompson]{kostrikov2019discriminator}
I.~Kostrikov, K.~K. Agrawal, D.~Dwibedi, S.~Levine, and J.~Tompson.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Nachum, and
  Tompson]{kostrikov2020valuedice}
I.~Kostrikov, O.~Nachum, and J.~Tompson.
\newblock Imitation learning via off-policy distribution matching.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and Levine]{kumar2019bear}
A.~Kumar, J.~Fu, M.~Soh, G.~Tucker, and S.~Levine.
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{kumar2020cql}
A.~Kumar, A.~Zhou, G.~Tucker, and S.~Levine.
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019spibb}
R.~Laroche, P.~Trichelair, and R.~T. Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  3652--3661. PMLR, 2019.

\bibitem[Lee et~al.(2020)Lee, Lee, Vrancx, Kim, and Kim]{lee2020bopah}
B.~Lee, J.~Lee, P.~Vrancx, D.~Kim, and K.-E. Kim.
\newblock Batch reinforcement learning with hyperparameter gradients.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  5725--5735. PMLR, 2020.

\bibitem[Lee et~al.(2021)Lee, Jeon, Lee, Pineau, and Kim]{lee2021optidice}
J.~Lee, W.~Jeon, B.-J. Lee, J.~Pineau, and K.-E. Kim.
\newblock {OptiDICE}: Offline policy optimization via stationary distribution
  correction estimation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Lee et~al.(2022)Lee, Paduraru, Mankowitz, Heess, Precup, Kim, and
  Guez]{lee2022coptidice}
J.~Lee, C.~Paduraru, D.~J. Mankowitz, N.~Heess, D.~Precup, K.-E. Kim, and
  A.~Guez.
\newblock {CO}pti{DICE}: Offline constrained reinforcement learning via
  stationary distribution correction estimation.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=FLA55mBee6Q}.

\bibitem[Li et~al.(2022)Li, Xu, Yu, and Luo]{li2022rethinking}
Z.~Li, T.~Xu, Y.~Yu, and Z.-Q. Luo.
\newblock Rethinking valuedice: Does it really improve performance?
\newblock \emph{arXiv preprint arXiv:2202.02468}, 2022.

\bibitem[Liu et~al.(2019)Liu, Ling, Mu, and Su]{liu2019sail}
F.~Liu, Z.~Ling, T.~Mu, and H.~Su.
\newblock State alignment-based imitation learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Liu et~al.(2018)Liu, Gupta, Abbeel, and Levine]{liu2018ifovideo}
Y.~Liu, A.~Gupta, P.~Abbeel, and S.~Levine.
\newblock Imitation from observation: Learning to imitate behaviors from raw
  video via context translation.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 1118--1125. IEEE, 2018.

\bibitem[Ma et~al.(2021)Ma, Jayaraman, and Bastani]{ma2021conservative}
Y.~Ma, D.~Jayaraman, and O.~Bastani.
\newblock Conservative offline distributional reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ma et~al.(2022)Ma, Shen, Jayaraman, and Bastani]{ma2022smodice}
Y.~J. Ma, A.~Shen, D.~Jayaraman, and O.~Bastani.
\newblock Smodice: Versatile offline imitation learning via state occupancy
  matching.
\newblock \emph{arXiv preprint arXiv:2202.02433}, 2022.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{nachum2019dualdice}
O.~Nachum, Y.~Chow, B.~Dai, and L.~Li.
\newblock Dual{DICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019{\natexlab{a}}.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachumE2019algaedice}
O.~Nachum, B.~Dai, I.~Kostrikov, Y.~Chow, L.~Li, and D.~Schuurmans.
\newblock Algae{DICE}: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019{\natexlab{b}}.

\bibitem[Owen(2013)]{owen2013snis}
A.~B. Owen.
\newblock \emph{Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem[Puterman(1994)]{puterman1994markov}
M.~L. Puterman.
\newblock Markov decision processes: Discrete stochastic dynamic programming,
  1994.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
S.~Ross, G.~Gordon, and J.~A. Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, pages 627--635, 2011.

\bibitem[Schaal et~al.(1997)]{schaal1997lfd}
S.~Schaal et~al.
\newblock Learning from demonstration.
\newblock \emph{Advances in neural information processing systems (NeurIPS)},
  pages 1040--1046, 1997.

\bibitem[Sun et~al.(2019)Sun, Vemula, Boots, and Bagnell]{sun2019fail}
W.~Sun, A.~Vemula, B.~Boots, and D.~Bagnell.
\newblock Provably efficient imitation learning from observation alone.
\newblock In \emph{International conference on machine learning}, pages
  6036--6045. PMLR, 2019.

\bibitem[Sutton and Barto(1998)]{sutton1998rlbook}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: {A}n introduction}.
\newblock MIT Press, 1998.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem[Torabi et~al.(2018)Torabi, Warnell, and Stone]{torabi2018bco}
F.~Torabi, G.~Warnell, and P.~Stone.
\newblock Behavioral cloning from observation.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2018.

\bibitem[Torabi et~al.(2019)Torabi, Warnell, and Stone]{torabi2018gaifo}
F.~Torabi, G.~Warnell, and P.~Stone.
\newblock Generative adversarial imitation from observation.
\newblock \emph{ICML Workshop on Imitation, Intent, and Interaction (I3)},
  2019.

\bibitem[Yang et~al.(2019)Yang, Ma, Huang, Sun, Liu, Huang, and
  Gan]{yang2019iddm}
C.~Yang, X.~Ma, W.~Huang, F.~Sun, H.~Liu, J.~Huang, and C.~Gan.
\newblock Imitation learning from observations by minimizing inverse dynamics
  disagreement.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Yang et~al.(2020{\natexlab{a}})Yang, Dai, Nachum, Tucker, and
  Schuurmans]{yang2020offline}
M.~Yang, B.~Dai, O.~Nachum, G.~Tucker, and D.~Schuurmans.
\newblock Offline policy selection under uncertainty, 2020{\natexlab{a}}.

\bibitem[Yang et~al.(2020{\natexlab{b}})Yang, Nachum, Dai, Li, and
  Schuurmans]{yang2020offpolicy}
M.~Yang, O.~Nachum, B.~Dai, L.~Li, and D.~Schuurmans.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2019gendice}
R.~Zhang, B.~Dai, L.~Li, and D.~Schuurmans.
\newblock Gen{DICE}: Generalized offline estimation of stationary values.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
S.~Zhang, B.~Liu, and S.~Whiteson.
\newblock Gradient{DICE}: Rethinking generalized offline estimation of
  stationary values.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, 2020{\natexlab{b}}.

\bibitem[Zhu et~al.(2020)Zhu, Lin, Dai, and Zhou]{ZhuEtal2020opolo}
Z.~Zhu, K.~Lin, B.~Dai, and J.~Zhou.
\newblock Off-policy imitation learning from observations.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\end{thebibliography}
