\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori \& Szepesv{\'a}ri(2011)Abbasi-Yadkori and
  Szepesv{\'a}ri]{abbasi2011regret}
Abbasi-Yadkori, Y. and Szepesv{\'a}ri, C.
\newblock Regret bounds for the adaptive control of linear quadratic systems.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, pp.\  1--26. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Abbeel \& Ng(2005)Abbeel and Ng]{abbeel2005exploration}
Abbeel, P. and Ng, A.~Y.
\newblock Exploration and apprenticeship learning in reinforcement learning.
\newblock In Raedt, L.~D. and Wrobel, S. (eds.), \emph{Machine Learning,
  Proceedings of the Twenty-Second International Conference {(ICML} 2005),
  Bonn, Germany, August 7-11, 2005}, volume 119 of \emph{{ACM} International
  Conference Proceeding Series}, pp.\  1--8. {ACM}, 2005.
\newblock \doi{10.1145/1102351.1102352}.
\newblock URL \url{https://doi.org/10.1145/1102351.1102352}.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Azar, M.~G., Munos, R., and Kappen, H.~J.
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock \emph{Machine learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Bagnell et~al.(2003)Bagnell, Kakade, Ng, and
  Schneider]{bagnell2003psdp}
Bagnell, J.~A., Kakade, S.~M., Ng, A.~Y., and Schneider, J.~G.
\newblock Policy search by dynamic programming.
\newblock In Thrun, S., Saul, L.~K., and Sch{\"{o}}lkopf, B. (eds.),
  \emph{Advances in Neural Information Processing Systems 16 [Neural
  Information Processing Systems, {NIPS} 2003, December 8-13, 2003, Vancouver
  and Whistler, British Columbia, Canada]}, pp.\  831--838. {MIT} Press, 2003.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2003/hash/3837a451cd0abc5ce4069304c5442c87-Abstract.html}.

\bibitem[Bertsekas(2005)]{bertsekas2005dynamic}
Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control, 3rd Edition}.
\newblock Athena Scientific, 2005.
\newblock ISBN 1886529264.
\newblock URL \url{https://www.worldcat.org/oclc/314894080}.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
Cesa-Bianchi, N., Conconi, A., and Gentile, C.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018deep}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4754--4765, 2018.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Khazatsky, Levine, and
  Salakhutdinov]{eysenbach2021mismatched}
Eysenbach, B., Khazatsky, A., Levine, S., and Salakhutdinov, R.
\newblock Mismatched no more: Joint model-policy optimization for model-based
  rl.
\newblock \emph{arXiv preprint arXiv:2110.02758}, 2021.

\bibitem[Farahmand et~al.(2017)Farahmand, Barreto, and
  Nikovski]{farahmand2017value}
Farahmand, A.~M., Barreto, A., and Nikovski, D.
\newblock Value-aware loss function for model-based reinforcement learning.
\newblock In Singh, A. and Zhu, X.~J. (eds.), \emph{Proceedings of the 20th
  International Conference on Artificial Intelligence and Statistics, {AISTATS}
  2017, 20-22 April 2017, Fort Lauderdale, FL, {USA}}, volume~54 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1486--1494. {PMLR},
  2017.
\newblock URL \url{http://proceedings.mlr.press/v54/farahmand17a.html}.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{freund1997hedge}
Freund, Y. and Schapire, R.~E.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{J. Comput. Syst. Sci.}, 55\penalty0 (1):\penalty0 119--139,
  1997.
\newblock \doi{10.1006/jcss.1997.1504}.
\newblock URL \url{https://doi.org/10.1006/jcss.1997.1504}.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Grimm et~al.(2020)Grimm, Barreto, Singh, and Silver]{grimm2020value}
Grimm, C., Barreto, A., Singh, S., and Silver, D.
\newblock The value equivalence principle for model-based reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5541--5552, 2020.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Norouzi, and
  Ba]{hafner2020mastering}
Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J.
\newblock Mastering atari with discrete world models.
\newblock \emph{arXiv preprint arXiv:2010.02193}, 2020.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner2023mastering}
Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Hazan(2019)]{hazan2019oco}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{CoRR}, abs/1909.05207, 2019.
\newblock URL \url{http://arxiv.org/abs/1909.05207}.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Osband, I., et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019trust}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock When to trust your model: Model-based policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Jiang(2018)]{jiang2018imperfect}
Jiang, N.
\newblock {PAC} reinforcement learning with an imperfect model.
\newblock In McIlraith, S.~A. and Weinberger, K.~Q. (eds.), \emph{Proceedings
  of the Thirty-Second {AAAI} Conference on Artificial Intelligence, (AAAI-18),
  the 30th innovative Applications of Artificial Intelligence (IAAI-18), and
  the 8th {AAAI} Symposium on Educational Advances in Artificial Intelligence
  (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018}, pp.\
  3334--3341. {AAAI} Press, 2018.
\newblock URL
  \url{https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16052}.

\bibitem[Joseph et~al.(2013)Joseph, Geramifard, Roberts, How, and
  Roy]{joseph2013rl}
Joseph, J.~M., Geramifard, A., Roberts, J.~W., How, J.~P., and Roy, N.
\newblock Reinforcement learning with misspecified model classes.
\newblock In \emph{2013 {IEEE} International Conference on Robotics and
  Automation, Karlsruhe, Germany, May 6-10, 2013}, pp.\  939--946. {IEEE},
  2013.
\newblock \doi{10.1109/ICRA.2013.6630686}.
\newblock URL \url{https://doi.org/10.1109/ICRA.2013.6630686}.

\bibitem[Kakade et~al.(2020)Kakade, Krishnamurthy, Lowrey, Ohnishi, and
  Sun]{kakade2020information}
Kakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W.
\newblock Information theoretic regret bounds for online nonlinear control.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15312--15325, 2020.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.~M.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock University of London, University College London (United Kingdom),
  2003.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximate}
Kakade, S.~M. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In Sammut, C. and Hoffmann, A.~G. (eds.), \emph{Machine Learning,
  Proceedings of the Nineteenth International Conference {(ICML} 2002),
  University of New South Wales, Sydney, Australia, July 8-12, 2002}, pp.\
  267--274. Morgan Kaufmann, 2002.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[Kearns et~al.(1999)Kearns, Mansour, and Ng]{kearns1999approximate}
Kearns, M.~J., Mansour, Y., and Ng, A.~Y.
\newblock Approximate planning in large pomdps via reusable trajectories.
\newblock In Solla, S.~A., Leen, T.~K., and M{\"{u}}ller, K. (eds.),
  \emph{Advances in Neural Information Processing Systems 12, {[NIPS}
  Conference, Denver, Colorado, USA, November 29 - December 4, 1999]}, pp.\
  1001--1007. The {MIT} Press, 1999.
\newblock URL
  \url{http://papers.nips.cc/paper/1664-approximate-planning-in-large-pomdps-via-reusable-trajectories}.

\bibitem[Lambert et~al.(2020)Lambert, Amos, Yadan, and
  Calandra]{lambert2020mismatch}
Lambert, N.~O., Amos, B., Yadan, O., and Calandra, R.
\newblock Objective mismatch in model-based reinforcement learning.
\newblock In Bayen, A.~M., Jadbabaie, A., Pappas, G.~J., Parrilo, P.~A., Recht,
  B., Tomlin, C.~J., and Zeilinger, M.~N. (eds.), \emph{Proceedings of the 2nd
  Annual Conference on Learning for Dynamics and Control, {L4DC} 2020, Online
  Event, Berkeley, CA, USA, 11-12 June 2020}, volume 120 of \emph{Proceedings
  of Machine Learning Research}, pp.\  761--770. {PMLR}, 2020.
\newblock URL \url{http://proceedings.mlr.press/v120/lambert20a.html}.

\bibitem[Levine \& Abbeel(2014)Levine and Abbeel]{levine2014learning}
Levine, S. and Abbeel, P.
\newblock Learning neural network policies with guided policy search under
  unknown dynamics.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Li \& Todorov(2004)Li and Todorov]{li2004ilqr}
Li, W. and Todorov, E.
\newblock Iterative linear quadratic regulator design for nonlinear biological
  movement systems.
\newblock In Ara{\'{u}}jo, H., Vieira, A., Braz, J., Encarna{\c{c}}{\~{a}}o,
  B., and Carvalho, M. (eds.), \emph{{ICINCO} 2004, Proceedings of the First
  International Conference on Informatics in Control, Automation and Robotics,
  Set{\'{u}}bal, Portugal, August 25-28, 2004}, pp.\  222--229. {INSTICC}
  Press, 2004.

\bibitem[Ljung(1998)]{ljung1998system}
Ljung, L.
\newblock System identification.
\newblock In \emph{Signal analysis and prediction}, pp.\  163--173. Springer,
  1998.

\bibitem[Modhe et~al.(2021)Modhe, Kamath, Batra, and Kalyan]{modhe2021model}
Modhe, N., Kamath, H., Batra, D., and Kalyan, A.
\newblock Model-advantage optimization for model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.14080}, 2021.

\bibitem[Morari \& Lee(1999)Morari and Lee]{morari1999model}
Morari, M. and Lee, J.~H.
\newblock Model predictive control: past, present and future.
\newblock \emph{Computers \& Chemical Engineering}, 23\penalty0 (4-5):\penalty0
  667--682, 1999.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{nair2018overcoming}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  6292--6299. IEEE, 2018.

\bibitem[Pineda et~al.(2021)Pineda, Amos, Zhang, Lambert, and
  Calandra]{pineda2021mbrl}
Pineda, L., Amos, B., Zhang, A., Lambert, N.~O., and Calandra, R.
\newblock Mbrl-lib: A modular library for model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2104.10159}, 2021.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{rajeswaran2017learning}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Ross \& Bagnell(2012)Ross and Bagnell]{ross2012agnostic}
Ross, S. and Bagnell, J.~A.
\newblock Agnostic system identification for model-based reinforcement
  learning.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pp.\  1905--1912, 2012.

\bibitem[Ross \& Bagnell(2014)Ross and Bagnell]{ross2014aggrevate}
Ross, S. and Bagnell, J.~A.
\newblock Reinforcement and imitation learning via interactive no-regret
  learning.
\newblock \emph{CoRR}, abs/1406.5979, 2014.
\newblock URL \url{http://arxiv.org/abs/1406.5979}.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel,
  et~al.]{schrittwieser2020mastering}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,
  Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Song \& Sun(2021)Song and Sun]{song2021pc}
Song, Y. and Sun, W.
\newblock Pc-mlp: Model-based reinforcement learning with policy cover guided
  exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9801--9811. PMLR, 2021.

\bibitem[Song et~al.(2022)Song, Zhou, Sekhari, Bagnell, Krishnamurthy, and
  Sun]{song2022hybrid}
Song, Y., Zhou, Y., Sekhari, A., Bagnell, J.~A., Krishnamurthy, A., and Sun, W.
\newblock Hybrid rl: Using both offline and online data can make rl efficient.
\newblock \emph{arXiv preprint arXiv:2210.06718}, 2022.

\bibitem[Sun et~al.(2019{\natexlab{a}})Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on learning theory}, pp.\  2898--2933. PMLR,
  2019{\natexlab{a}}.

\bibitem[Sun et~al.(2019{\natexlab{b}})Sun, Vemula, Boots, and
  Bagnell]{sun2019ilfo}
Sun, W., Vemula, A., Boots, B., and Bagnell, D.
\newblock Provably efficient imitation learning from observation alone.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pp.\  6036--6045. {PMLR}, 2019{\natexlab{b}}.
\newblock URL \url{http://proceedings.mlr.press/v97/sun19b.html}.

\bibitem[Sutton(1991)]{sutton1991dyna}
Sutton, R.~S.
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock \emph{ACM Sigart Bulletin}, 2\penalty0 (4):\penalty0 160--163, 1991.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Swamy et~al.(2021)Swamy, Choudhury, Bagnell, and Wu]{swamy2021moments}
Swamy, G., Choudhury, S., Bagnell, J.~A., and Wu, S.
\newblock Of moments and matching: A game-theoretic framework for closing the
  imitation gap.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10022--10032. PMLR, 2021.

\bibitem[Tu \& Recht(2019)Tu and Recht]{tu2019gap}
Tu, S. and Recht, B.
\newblock The gap between model-based and model-free methods on the linear
  quadratic regulator: An asymptotic viewpoint.
\newblock In \emph{Conference on Learning Theory}, pp.\  3036--3083. PMLR,
  2019.

\bibitem[Vecerik et~al.(2017)Vecerik, Hester, Scholz, Wang, Pietquin, Piot,
  Heess, Roth{\"o}rl, Lampe, and Riedmiller]{vecerik2017leveraging}
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess,
  N., Roth{\"o}rl, T., Lampe, T., and Riedmiller, M.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Vemula et~al.(2020)Vemula, Oza, Bagnell, and
  Likhachev]{vemula2020planning}
Vemula, A., Oza, Y., Bagnell, J.~A., and Likhachev, M.
\newblock Planning and execution using inaccurate models with provable
  guarantees.
\newblock In Toussaint, M., Bicchi, A., and Hermans, T. (eds.), \emph{Robotics:
  Science and Systems XVI, Virtual Event / Corvalis, Oregon, USA, July 12-16,
  2020}, 2020.
\newblock \doi{10.15607/RSS.2020.XVI.001}.
\newblock URL \url{https://doi.org/10.15607/RSS.2020.XVI.001}.

\bibitem[Voloshin et~al.(2021)Voloshin, Jiang, and Yue]{voloshin2021minimax}
Voloshin, C., Jiang, N., and Yue, Y.
\newblock Minimax model learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1612--1620. PMLR, 2021.

\bibitem[Wu et~al.(2022)Wu, Escontrela, Hafner, Goldberg, and
  Abbeel]{wu2022daydreamer}
Wu, P., Escontrela, A., Hafner, D., Goldberg, K., and Abbeel, P.
\newblock Daydreamer: World models for physical robot learning.
\newblock \emph{arXiv preprint arXiv:2206.14176}, 2022.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 6683--6694, 2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Jiang, Wang, Xiong, and
  Bai]{xie2021policy}
Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y.
\newblock Policy finetuning: Bridging sample-efficient offline and online
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 27395--27407, 2021{\natexlab{b}}.

\end{thebibliography}
