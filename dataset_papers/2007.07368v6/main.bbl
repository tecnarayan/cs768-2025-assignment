\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aleksziev(2019)]{Aleksziev}
Rita Aleksziev.
\newblock {Tangent Space Separability in Feedforward Neural Networks}.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Arora et~al.(2020)Arora, Bartlett, Mianjy, and Srebro]{Arora2020}
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro.
\newblock {Dropout: Explicit Forms and Capacity Control}.
\newblock 2020.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{NEURIPS2019_dbc4d84b}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Bishop(1995)]{Bishop1995}
Chris~M. Bishop.
\newblock {Training with Noise is Equivalent to Tikhonov Regularization}.
\newblock \emph{Neural Computation}, 7\penalty0 (1):\penalty0 108--116, 1995.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{Botev2017}
Aleksandar Botev, Hippolyt Ritter, and David Barber.
\newblock {Practical Gauss-Newton optimisation for deep learning}.
\newblock In \emph{ICML}, 2017.

\bibitem[Burger and Neubauer(2003)]{Burger2003}
Martin Burger and Andreas Neubauer.
\newblock {Analysis of Tikhonov regularization for function approximation by
  neural networks}.
\newblock \emph{Neural Networks}, 16\penalty0 (1):\penalty0 79--90, 2003.

\bibitem[Chen et~al.(2020)Chen, Cao, Gu, and Zhang]{chen2020generalized}
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang.
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks, 2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat:hal-01945578}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock {On Lazy Training in Differentiable Programming}.
\newblock In \emph{NeurIPS}, December 2019.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{Cohen2019}
Jeremy Cohen, Elan Rosenfeld, and J.~Zico Kolter.
\newblock {Certified adversarial robustness via randomized smoothing}.
\newblock In \emph{ICML}, 2019.

\bibitem[Constantine and Savits(1996)]{constantine}
G.~M. Constantine and T.~H. Savits.
\newblock A multivariate faa di bruno formula with applications.
\newblock \emph{Transactions of the American Mathematical Society},
  348\penalty0 (2):\penalty0 503--520, 1996.

\bibitem[Cucker and Smale(2002)]{Cucker2002}
Felipe Cucker and Steve Smale.
\newblock {On the mathematical foundations of learning}.
\newblock \emph{Bulletin of the American Mathematical Society}, 39\penalty0
  (1):\penalty0 1--49, 2002.

\bibitem[Czarnecki et~al.(2017)Czarnecki, Osindero, Jaderberg, Swirszcz, and
  Pascanu]{Czarnecki2017}
Wojciech~Marian Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz,
  and Razvan Pascanu.
\newblock {Sobolev training for neural networks}.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Dawid(1982)]{Dawid1982}
A~P Dawid.
\newblock {The Well-Calibrated Bayesian}.
\newblock \emph{Journal of the American Statistical Association}, 77\penalty0
  (379), 1982.

\bibitem[DeGroot and Fienberg(1983)]{Degroot1982TheForecasters}
Morris~H. DeGroot and Stephen~E. Fienberg.
\newblock The comparison and evaluation of forecasters.
\newblock \emph{Journal of the Royal Statistical Society. Series D (The
  Statistician)}, 32:\penalty0 12--22, 1983.

\bibitem[Dieng et~al.(2018)Dieng, Ranganath, Altosaar, and
  Blei]{dieng2018noisin}
Adji~B Dieng, Rajesh Ranganath, Jaan Altosaar, and David~M Blei.
\newblock Noisin: Unbiased regularization for recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1805.01500}, 2018.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{Dinh2017}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock {Sharp minima can generalize for deep nets}.
\newblock In \emph{ICML}, 2017.

\bibitem[Farquhar et~al.(2020)Farquhar, Smith, and Gal]{Farquhar2020}
Sebastian Farquhar, Lewis Smith, and Yarin Gal.
\newblock {Try Depth Instead of Weight Correlations: Mean-field is a Less
  Restrictive Assumption for Deeper Networks}.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Girosi and Poggio(1990)]{Girosi1990}
F~Girosi and T~Poggio.
\newblock {Biological Cybernetics Networks and the Best Approximation
  Property}.
\newblock \emph{Artificial Intelligence}, 176:\penalty0 169--176, 1990.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{Guo2017}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q. Weinberger.
\newblock {On calibration of modern neural networks}.
\newblock In \emph{ICML}, 2017.

\bibitem[Hauser and Ray(2017)]{Hauser2017}
Michael Hauser and Asok Ray.
\newblock {Principles of Riemannian geometry in neural networks}.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Helmbold and Long(2015)]{Helmbold2015}
David~P. Helmbold and Philip~M. Long.
\newblock {On the inductive bias of dropout}.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 3403--3454,
  2015.

\bibitem[Hornik(1991)]{Hornik1991}
Kurt Hornik.
\newblock {Approximation capabilities of multilayer feedforward networks}.
\newblock \emph{Neural Networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{ntk}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Jakubovitz and Giryes(2018)]{Jakubovitz2018}
Daniel Jakubovitz and Raja Giryes.
\newblock {Improving DNN robustness to adversarial attacks using jacobian
  regularization}.
\newblock \emph{Lecture Notes in Computer Science}, pages 525--541, 2018.

\bibitem[Jastrz{\c{e}}bski et~al.(2017)Jastrz{\c{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{Jastrzebski2017}
Stanis≈Çaw Jastrz{\c{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock {Three Factors Influencing Minima in SGD}.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Keskar et~al.(2019)Keskar, Nocedal, Tang, Mudigere, and
  Smelyanskiy]{Keskar2019OnMinima}
Nitish~Shirish Keskar, Jorge Nocedal, Ping Tak~Peter Tang, Dheevatsa Mudigere,
  and Mikhail Smelyanskiy.
\newblock {On large-batch training for deep learning: Generalization gap and
  sharp minima}.
\newblock In \emph{ICLR}, 2019.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and Welling]{Kingma}
Diederik~P. Kingma, Tim Salimans, and Max Welling.
\newblock {Variational dropout and the local reparameterization trick}.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Kunin et~al.(2019)Kunin, Bloom, Goeva, and Seed]{Kunin1999}
Daniel Kunin, Jonathan~M. Bloom, Aleksandrina Goeva, and Cotton Seed.
\newblock {Loss landscapes of regularized linear autoencoders}.
\newblock In \emph{ICML}, 2019.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Orr, and M{\"u}ller]{LeCun1998}
Yann~A. LeCun, L{\'e}on Bottou, Genevieve~B. Orr, and Klaus-Robert M{\"u}ller.
\newblock \emph{Efficient BackProp}, pages 9--48.
\newblock 1998.

\bibitem[Li et~al.(2018)Li, Chen, Wang, and Carin]{Li2018}
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin.
\newblock Second-order adversarial attack and certifiable robustness.
\newblock \emph{CoRR}, 2018.

\bibitem[Liu et~al.(2019)Liu, Dong, Zhang, Gong, and Shi]{Liu2018}
Yuhang Liu, Wenyong Dong, Lei Zhang, Dong Gong, and Qinfeng Shi.
\newblock {Variational bayesian dropout with a hierarchical prior}.
\newblock In \emph{IEEE CVPR}, 2019.

\bibitem[Mele and Altarelli(1993)]{Mele1993}
Barbara Mele and Guido Altarelli.
\newblock {Lepton spectra as a measure of b quark polarization at LEP}.
\newblock \emph{Physics Letters B}, 299\penalty0 (3-4):\penalty0 345--350,
  1993.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and Hauskrecht]{Naeini2015}
Mahdi~Pakdaman Naeini, Gregory~F. Cooper, and Milos Hauskrecht.
\newblock {Obtaining well calibrated probabilities using Bayesian Binning}.
\newblock \emph{Proceedings of the National Conference on Artificial
  Intelligence}, 4:\penalty0 2901--2907, 2015.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and Srebro]{Neyshabur}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{PMLR}, 2015.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{Neyshabur2017}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro.
\newblock {Exploring generalization in deep learning}.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Niculescu-Mizil and Caruana(2005)]{Niculescu-Mizil2005}
Alexandru Niculescu-Mizil and Rich Caruana.
\newblock {Predicting good probabilities with supervised learning}.
\newblock In \emph{ICML}, 2005.

\bibitem[Poole et~al.(2014)Poole, Sohl-Dickstein, and Ganguli]{Poole2014}
Ben Poole, Jascha Sohl-Dickstein, and Surya Ganguli.
\newblock {Analyzing noise in autoencoders and deep networks}.
\newblock 2014.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{Poole2016}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock {Exponential expressivity in deep neural networks through transient
  chaos}.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{Rahaman2019}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Sagun et~al.(2018)Sagun, Evci, G{\"{u}}ney, Dauphin, and
  Bottou]{Sagun2018}
Levent Sagun, Utku Evci, V.~Ugur G{\"{u}}ney, Yann Dauphin, and L{\'{e}}on
  Bottou.
\newblock {Empirical analysis of the hessian of over-parametrized neural
  networks}.
\newblock 2018.

\bibitem[Sokoli{\'{c}} et~al.(2017)Sokoli{\'{c}}, Giryes, Sapiro, and
  Rodrigues]{Sokolic2017}
Jure Sokoli{\'{c}}, Raja Giryes, Guillermo Sapiro, and Miguel~R.D. Rodrigues.
\newblock {Robust Large Margin Deep Neural Networks}.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (16):\penalty0 4265--4280, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{Srivastava2014}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock {Dropout: A simple way to prevent neural networks from overfitting}.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1929--1958,
  2014.

\bibitem[Tikhonov(1977)]{Tikhonov1977}
A~N (Andrei~Nikolaevich) Tikhonov.
\newblock \emph{{Solutions of ill-posed problems / Andrey N. Tikhonov and
  Vasiliy Y. Arsenin ; translation editor, Fritz John}}.
\newblock 1977.

\bibitem[Wager et~al.(2013)Wager, Wang, and Liang]{wager2013dropout}
Stefan Wager, Sida Wang, and Percy~S Liang.
\newblock Dropout training as adaptive regularization.
\newblock In \emph{Advances in neural information processing systems}, pages
  351--359, 2013.

\bibitem[Webb(1994)]{Webb1994}
Andrew~R. Webb.
\newblock {Functional Approximation by FeedForward Networks: A Least-Squares
  Approach to Generalization}.
\newblock \emph{IEEE Transactions on Neural Networks}, 5\penalty0 (3):\penalty0
  363--371, 1994.

\bibitem[Wei et~al.(2020)Wei, Kakade, and Ma]{Wei2020}
Colin Wei, Sham Kakade, and Tengyu Ma.
\newblock {The Implicit and Explicit Regularization Effects of Dropout}.
\newblock 2020.

\bibitem[Zhang et~al.(2017)Zhang, Recht, Bengio, Hardt, and Vinyals]{Zhang2019}
Chiyuan Zhang, Benjamin Recht, Samy Bengio, Moritz Hardt, and Oriol Vinyals.
\newblock {Understanding deep learning requires rethinking generalization}.
\newblock In \emph{ICLR}, 2017.

\end{thebibliography}
