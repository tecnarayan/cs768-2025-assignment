\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for conditional computation.
\newblock {\em arXiv preprint arXiv:1308.3432}, 2013.

\bibitem{bondarenko2021understanding}
Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
\newblock Understanding and overcoming the challenges of efficient transformer quantization.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 7947--7969, 2021.

\bibitem{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, march 2023.
\newblock {\em URL https://lmsys. org/blog/2023-03-30-vicuna}, 3(5), 2023.

\bibitem{dettmers2022gpt3}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock {\em Advances in Neural Information Processing Systems}, 35:30318--30332, 2022.

\bibitem{dettmers2024qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{dettmers2023spqr}
Tim Dettmers, Ruslan~A Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
\newblock Spqr: A sparse-quantized representation for near-lossless llm weight compression.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{frantar2023gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{hassibi1993optimal}
Babak Hassibi, David~G Stork, and Gregory~J Wolff.
\newblock Optimal brain surgeon and general network pruning.
\newblock In {\em IEEE international conference on neural networks}, pages 293--299. IEEE, 1993.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{kim2023squeezellm}
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael~W Mahoney, and Kurt Keutzer.
\newblock Squeezellm: Dense-and-sparse quantization.
\newblock {\em arXiv preprint arXiv:2306.07629}, 2023.

\bibitem{kovaleva2021bert}
Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky.
\newblock Bert busters: Outlier dimensions that disrupt transformers.
\newblock {\em Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, 2021.

\bibitem{krishnamoorthi2018quantizing}
Raghuraman Krishnamoorthi.
\newblock Quantizing deep convolutional networks for efficient inference: A whitepaper.
\newblock {\em arXiv preprint arXiv:1806.08342}, 2018.

\bibitem{lecun1989optimal}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock {\em Advances in neural information processing systems}, 2, 1989.

\bibitem{li2020brecq}
Yuhang Li, Ruihao Gong, Xu~Tan, Yang Yang, Peng Hu, Qi~Zhang, Fengwei Yu, Wei Wang, and Shi Gu.
\newblock Brecq: Pushing the limit of post-training quantization by block reconstruction.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{lin2023awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock {\em arXiv preprint arXiv:2306.00978}, 2023.

\bibitem{liu2023llm}
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
\newblock Llm-qat: Data-free quantization aware training for large language models.
\newblock {\em arXiv preprint arXiv:2305.17888}, 2023.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv preprint arXiv:1609.07843}, 2016.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{shao2023omniquant}
Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu~Qiao, and Ping Luo.
\newblock Omniquant: Omnidirectionally calibrated quantization for large language models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock {\em arXiv preprint arXiv:2403.08295}, 2024.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{wei2023outlier}
Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu.
\newblock Outlier suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 1648--1665, 2023.

\bibitem{wei2022outlier}
Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi~Zhang, Fengwei Yu, and Xianglong Liu.
\newblock Outlier suppression: Pushing the limit of low-bit transformer language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:17402--17414, 2022.

\bibitem{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In {\em International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}
