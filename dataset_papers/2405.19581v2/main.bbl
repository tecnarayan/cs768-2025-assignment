\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Kaswan et~al.(2023)Al-Kaswan, Ahmed, Izadi, Sawant, Devanbu, and van Deursen]{al2023extending}
Ali Al-Kaswan, Toufique Ahmed, Maliheh Izadi, Anand~Ashok Sawant, Premkumar Devanbu, and Arie van Deursen.
\newblock Extending source code pre-trained language models to summarise decompiled binarie.
\newblock In \emph{2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)}, pages 260--271. IEEE, 2023.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Anderson et~al.(2018)Anderson, Wu, Teney, Bruce, Johnson, S{\"u}nderhauf, Reid, Gould, and Van Den~Hengel]{anderson2018vision}
Peter Anderson, Qi~Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S{\"u}nderhauf, Ian Reid, Stephen Gould, and Anton Van Den~Hengel.
\newblock Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3674--3683, 2018.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and Parikh]{antol2015vqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C~Lawrence Zitnick, and Devi Parikh.
\newblock Vqa: Visual question answering.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 2425--2433, 2015.

\bibitem[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu, Marathe, Bitton, Gadre, Sagawa, et~al.]{awadalla2023openflamingo}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et~al.
\newblock Openflamingo: An open-source framework for training large autoregressive vision-language models.
\newblock \emph{arXiv preprint arXiv:2308.01390}, 2023.

\bibitem[Banerjee et~al.(2021)Banerjee, Pal, Wang, and Baral]{banerjee2021variable}
Pratyay Banerjee, Kuntal~Kumar Pal, Fish Wang, and Chitta Baral.
\newblock Variable name recovery in decompiled binary code using constrained masked language modeling.
\newblock \emph{arXiv preprint arXiv:2103.12801}, 2021.

\bibitem[Banerjee and Lavie(2005)]{banerjee2005meteor}
Satanjeev Banerjee and Alon Lavie.
\newblock Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
\newblock In \emph{Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization}, pages 65--72, 2005.

\bibitem[Bryant(2012)]{bryant2012understanding}
Adam~R Bryant.
\newblock \emph{Understanding how reverse engineers make sense of programs from assembly language representations}.
\newblock Air Force Institute of Technology, 2012.

\bibitem[Burk et~al.(2022)Burk, Pagani, Kruegel, and Vigna]{burk2022decomperson}
Kevin Burk, Fabio Pagani, Christopher Kruegel, and Giovanni Vigna.
\newblock Decomperson: How humans decompile and what we can learn from it.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pages 2765--2782, 2022.

\bibitem[Chai et~al.(2023)Chai, Zhang, Wu, Han, Hu, Huang, and Yang]{chai2023graphllm}
Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, and Yang Yang.
\newblock Graphllm: Boosting graph reasoning ability of large language model.
\newblock \emph{arXiv preprint arXiv:2310.05845}, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Lacomis, Schwartz, Goues, Neubig, and Vasilescu]{dirty}
Qibin Chen, Jeremy Lacomis, Edward~J. Schwartz, Claire~Le Goues, Graham Neubig, and Bogdan Vasilescu.
\newblock Augmenting decompiler output with learned variable names and types.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pages 4327--4343, Boston, MA, August 2022{\natexlab{a}}. USENIX Association.
\newblock ISBN 978-1-939133-31-1.
\newblock URL \url{https://www.usenix.org/conference/usenixsecurity22/presentation/chen-qibin}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Lacomis, Schwartz, Le~Goues, Neubig, and Vasilescu]{chen2022augmenting}
Qibin Chen, Jeremy Lacomis, Edward~J Schwartz, Claire Le~Goues, Graham Neubig, and Bogdan Vasilescu.
\newblock Augmenting decompiler output with learned variable names and types.
\newblock In \emph{31st USENIX Security Symposium (USENIX Security 22)}, pages 4327--4343, 2022{\natexlab{b}}.

\bibitem[Dai et~al.(2024)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi]{dai2024instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale~N Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Das et~al.(2017)Das, Kottur, Gupta, Singh, Yadav, Moura, Parikh, and Batra]{das2017visual}
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos{\'e}~MF Moura, Devi Parikh, and Dhruv Batra.
\newblock Visual dialog.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 326--335, 2017.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022gpt3}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30318--30332, 2022.

\bibitem[Dettmers et~al.(2024)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2024qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Fried et~al.(2022)Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong, Yih, Zettlemoyer, and Lewis]{fried2022incoder}
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
\newblock Incoder: A generative model for code infilling and synthesis.
\newblock \emph{arXiv preprint arXiv:2204.05999}, 2022.

\bibitem[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li, Luo, Xiong, and Liang]{guo2024deepseekcoder}
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y.~Wu, Y.~K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang.
\newblock Deepseek-coder: When the large language model meets programming -- the rise of code intelligence, 2024.

\bibitem[Guo et~al.(2023)Guo, Du, and Liu]{guo2023gpt4graph}
Jiayan Guo, Lun Du, and Hengyu Liu.
\newblock Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking.
\newblock \emph{arXiv preprint arXiv:2305.15066}, 2023.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020retrieval}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
\newblock Retrieval augmented language model pre-training.
\newblock In \emph{International conference on machine learning}, pages 3929--3938. PMLR, 2020.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 9729--9738, 2020.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Huang et~al.(2024)Huang, Li, Yang, Shi, Chang, Ye, Wu, Hong, Huang, Liu, et~al.]{huang2024audiogpt}
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et~al.
\newblock Audiogpt: Understanding and generating speech, music, sound, and talking head.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 23802--23804, 2024.

\bibitem[Izacard and Grave(2021)]{izacard-grave-2021-leveraging}
Gautier Izacard and Edouard Grave.
\newblock Leveraging passage retrieval with generative models for open domain question answering.
\newblock In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, \emph{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pages 874--880, Online, April 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.eacl-main.74}.
\newblock URL \url{https://aclanthology.org/2021.eacl-main.74}.

\bibitem[Jiang et~al.(2024)Jiang, An, Huang, Tang, Nie, Wu, and Zhang]{jiang2024binaryai}
Ling Jiang, Junwen An, Huihui Huang, Qiyi Tang, Sen Nie, Shi Wu, and Yuqun Zhang.
\newblock Binaryai: Binary software composition analysis via intelligent binary source code matching.
\newblock In \emph{Proceedings of the IEEE/ACM 46th International Conference on Software Engineering}, pages 1--13, 2024.

\bibitem[Jiang et~al.(2023)Jiang, Wang, Liu, Xu, Tan, and Zhang]{jiang2023nova}
Nan Jiang, Chengxiao Wang, Kevin Liu, Xiangzhe Xu, Lin Tan, and Xiangyu Zhang.
\newblock Nova $^{+} $: Generative language models for binaries.
\newblock \emph{arXiv preprint arXiv:2311.13721}, 2023.

\bibitem[Jin et~al.(2022)Jin, Pei, Won, and Lin]{jin2022symlm}
Xin Jin, Kexin Pei, Jun~Yeon Won, and Zhiqiang Lin.
\newblock Symlm: Predicting function names in stripped binaries via context-sensitive execution-aware code embeddings.
\newblock In \emph{Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security}, pages 1631--1645, 2022.

\bibitem[Jin et~al.(2023)Jin, Larson, Yang, and Lin]{jin2023binary}
Xin Jin, Jonathan Larson, Weiwei Yang, and Zhiqiang Lin.
\newblock Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models.
\newblock \emph{arXiv preprint arXiv:2312.09601}, 2023.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih]{karpukhin2020dense}
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 6769--6781, 2020.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0 (13):\penalty0 3521--3526, 2017.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, Allal, Li, Mou, Ferrandis, Jernite, Mitchell, Hughes, Wolf, et~al.]{kocetkov2022stack}
Denis Kocetkov, Raymond Li, Loubna~Ben Allal, Jia Li, Chenghao Mou, Carlos~Mu{\~n}oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et~al.
\newblock The stack: 3 tb of permissively licensed source code.
\newblock \emph{arXiv preprint arXiv:2211.15533}, 2022.

\bibitem[Lacomis et~al.(2019)Lacomis, Yin, Schwartz, Allamanis, Le~Goues, Neubig, and Vasilescu]{lacomis2019dire}
Jeremy Lacomis, Pengcheng Yin, Edward Schwartz, Miltiadis Allamanis, Claire Le~Goues, Graham Neubig, and Bogdan Vasilescu.
\newblock Dire: A neural approach to decompiled identifier naming.
\newblock In \emph{2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, pages 628--639. IEEE, 2019.

\bibitem[Levine et~al.(2022)Levine, Dalmedigos, Ram, Zeldes, Jannai, Muhlgay, Osin, Lieber, Lenz, Shalev-Shwartz, et~al.]{levine2022standing}
Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin, Opher Lieber, Barak Lenz, Shai Shalev-Shwartz, et~al.
\newblock Standing on the shoulders of giant frozen language models.
\newblock \emph{arXiv preprint arXiv:2204.10019}, 2022.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9459--9474, 2020.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{International conference on machine learning}, pages 19730--19742. PMLR, 2023.

\bibitem[Li et~al.(2019)Li, Zhang, Li, Li, and Fu]{li2019visual}
Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu.
\newblock Visual semantic reasoning for image-text matching.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 4654--4662, 2019.

\bibitem[Lin(2004)]{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 74--81, 2004.

\bibitem[Lin et~al.(2018)Lin, Wang, Zettlemoyer, and Ernst]{lin2018nl2bash}
Xi~Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael~D Ernst.
\newblock Nl2bash: A corpus and semantic parser for natural language interface to the linux operating system.
\newblock In \emph{Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)}, 2018.

\bibitem[Liu et~al.(2023)Liu, Li, Li, and Lee]{liu2023improved}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2310.03744}, 2023.

\bibitem[Liu et~al.(2024)Liu, Li, Wu, and Lee]{liu2024visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei, Liu, Tian, Kocetkov, Zucker, Belkada, Wang, Liu, Abulkhanov, Paul, Li, Li, Risdal, Li, Zhu, Zhuo, Zheltonozhskii, Dade, Yu, Krauß, Jain, Su, He, Dey, Abati, Chai, Muennighoff, Tang, Oblokulov, Akiki, Marone, Mou, Mishra, Gu, Hui, Dao, Zebaze, Dehaene, Patry, Xu, McAuley, Hu, Scholak, Paquet, Robinson, Anderson, Chapados, Patwary, Tajbakhsh, Jernite, Ferrandis, Zhang, Hughes, Wolf, Guha, von Werra, and de~Vries]{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry~Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae~Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn~Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos~Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de~Vries.
\newblock Starcoder 2 and the stack v2: The next generation, 2024.

\bibitem[Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{luo2023wizardcoder}
Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.
\newblock Wizardcoder: Empowering code large language models with evol-instruct.
\newblock \emph{arXiv preprint arXiv:2306.08568}, 2023.

\bibitem[Moon et~al.(2023)Moon, Madotto, Lin, Nagarajan, Smith, Jain, Yeh, Murugesan, Heidari, Liu, et~al.]{moon2023anymal}
Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et~al.
\newblock Anymal: An efficient and scalable any-modality augmented language model.
\newblock \emph{arXiv preprint arXiv:2309.16058}, 2023.

\bibitem[OpenAI(2024)]{openai2024gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.

\bibitem[Pal et~al.(2024)Pal, Bajaj, Banerjee, Dutcher, Nakamura, Basque, Gupta, Sawant, Anantheswaran, Shoshitaishvili, et~al.]{varbert}
Kuntal~Kumar Pal, Ati~Priya Bajaj, Pratyay Banerjee, Audrey Dutcher, Mutsumi Nakamura, Zion~Leonahenahe Basque, Himanshu Gupta, Saurabh~Arjun Sawant, Ujjwala Anantheswaran, Yan Shoshitaishvili, et~al.
\newblock “len or index or count, anything but v1”: Predicting variable names in decompilation output with transfer learning.
\newblock In \emph{2024 IEEE Symposium on Security and Privacy (SP)}, pages 152--152. IEEE Computer Society, 2024.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Pei et~al.(2020)Pei, Xuan, Yang, Jana, and Ray]{pei2020trex}
Kexin Pei, Zhou Xuan, Junfeng Yang, Suman Jana, and Baishakhi Ray.
\newblock Trex: Learning execution semantics from micro-traces for binary similarity.
\newblock \emph{arXiv preprint arXiv:2012.08680}, 2020.

\bibitem[Pei et~al.(2021{\natexlab{a}})Pei, Guan, Broughton, Chen, Yao, Williams-King, Ummadisetty, Yang, Ray, and Jana]{pei2021stateformer}
Kexin Pei, Jonas Guan, Matthew Broughton, Zhongtian Chen, Songchen Yao, David Williams-King, Vikas Ummadisetty, Junfeng Yang, Baishakhi Ray, and Suman Jana.
\newblock Stateformer: Fine-grained type recovery from binaries using generative state modeling.
\newblock In \emph{Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering}, pages 690--702, 2021{\natexlab{a}}.

\bibitem[Pei et~al.(2021{\natexlab{b}})Pei, Guan, Williams-King, Yang, and Jana]{pei2020xda}
Kexin Pei, Jonas Guan, David Williams-King, Junfeng Yang, and Suman Jana.
\newblock Xda: Accurate, robust disassembly with transfer learning.
\newblock In \emph{{NDSS}}. The Internet Society, 2021{\natexlab{b}}.

\bibitem[Petroni et~al.(2019)Petroni, Rockt{\"a}schel, Riedel, Lewis, Bakhtin, Wu, and Miller]{petroni2019language}
Fabio Petroni, Tim Rockt{\"a}schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.
\newblock Language models as knowledge bases?
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2463--2473, 2019.

\bibitem[Popovi{\'c}(2015)]{popovic2015chrf}
Maja Popovi{\'c}.
\newblock chrf: character n-gram f-score for automatic mt evaluation.
\newblock In \emph{Proceedings of the tenth workshop on statistical machine translation}, pages 392--395, 2015.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Rozière et~al.(2024)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Sauvestre, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve]{rozière2024code}
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian~Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.
\newblock Code llama: Open foundation models for code, 2024.

\bibitem[Shang et~al.(2024)Shang, Cheng, Chen, Zhang, Hu, Yu, Li, Zhang, and Yu]{shang2024far}
Xiuwei Shang, Shaoyin Cheng, Guoqiang Chen, Yanming Zhang, Li~Hu, Xiao Yu, Gangyang Li, Weiming Zhang, and Nenghai Yu.
\newblock How far have we gone in stripped binary code understanding using large language models.
\newblock \emph{arXiv preprint arXiv:2404.09836}, 2024.

\bibitem[Shi et~al.(2022)Shi, Fried, Ghazvininejad, Zettlemoyer, and Wang]{shi2022natural}
Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida~I Wang.
\newblock Natural language to code translation with execution.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 3533--3546, 2022.

\bibitem[Su et~al.(2024)Su, Xu, Huang, Zhang, Ye, Huang, and Zhang]{su2024codeart}
Zian Su, Xiangzhe Xu, Ziyang Huang, Zhuo Zhang, Yapeng Ye, Jianjun Huang, and Xiangyu Zhang.
\newblock Codeart: Better code models by attention regularization when symbols are lacking.
\newblock \emph{Proceedings of the ACM on Software Engineering}, 1\penalty0 (FSE):\penalty0 562--585, 2024.

\bibitem[Tang et~al.(2023)Tang, Yang, Wei, Shi, Su, Cheng, Yin, and Huang]{tang2023graphgpt}
Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang.
\newblock Graphgpt: Graph instruction tuning for large language models.
\newblock \emph{arXiv preprint arXiv:2310.13023}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Qu, Katz, Zhu, Gao, Qiu, Zhuge, and Zhang]{wang2022jtrans}
Hao Wang, Wenjie Qu, Gilad Katz, Wenyu Zhu, Zeyu Gao, Han Qiu, Jianwei Zhuge, and Chao Zhang.
\newblock jtrans: Jump-aware transformer for binary code similarity.
\newblock \emph{arXiv preprint arXiv:2205.12713}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Feng, He, Tan, Han, and Tsvetkov]{wang2024can}
Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov.
\newblock Can language models solve graph problems in natural language?
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Chen, Chen, Wu, Zhu, Zeng, Luo, Lu, Zhou, Qiao, et~al.]{wang2024visionllm}
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu~Qiao, et~al.
\newblock Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Wang, Joty, and Hoi]{wang2021codet5}
Yue Wang, Weishi Wang, Shafiq Joty, and Steven~CH Hoi.
\newblock Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation.
\newblock \emph{arXiv preprint arXiv:2109.00859}, 2021.

\bibitem[Wang et~al.(2023)Wang, Le, Gotmare, Bui, Li, and Hoi]{wang2023codet5+}
Yue Wang, Hung Le, Akhilesh~Deepak Gotmare, Nghi~DQ Bui, Junnan Li, and Steven~CH Hoi.
\newblock Codet5+: Open code large language models for code understanding and generation.
\newblock \emph{arXiv preprint arXiv:2305.07922}, 2023.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Nie, Qiao, Xiao, Baraniuk, and Anandkumar]{wang2022retrieval}
Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar.
\newblock Retrieval-based controllable molecule generation.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2023)Wei, Durrett, and Dillig]{wei2023coeditor}
Jiayi Wei, Greg Durrett, and Isil Dillig.
\newblock Coeditor: Leveraging contextual changes for multi-round code auto-editing.
\newblock \emph{arXiv preprint arXiv:2305.18584}, 2023.

\bibitem[Weijia et~al.(2023)Weijia, Sewon, Michihiro, Minjoon, Rich, Mike, et~al.]{weijia2023replug}
Shi Weijia, Min Sewon, Yasunaga Michihiro, Seo Minjoon, James Rich, Lewis Mike, et~al.
\newblock Replug: Retrieval-augmented black-box language models.
\newblock \emph{ArXiv: 2301.12652}, 2023.

\bibitem[Xiong et~al.(2023)Xiong, Chen, Chen, Gao, Cheng, and Zhang]{xiong2023hext5}
Jiaqi Xiong, Guoqiang Chen, Kejiang Chen, Han Gao, Shaoyin Cheng, and Weiming Zhang.
\newblock Hext5: Unified pre-training for stripped binary code information inference.
\newblock In \emph{2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, pages 774--786. IEEE, 2023.

\bibitem[Xu et~al.(2015)Xu, Ba, Kiros, Cho, Courville, Salakhudinov, Zemel, and Bengio]{xu2015show}
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio.
\newblock Show, attend and tell: Neural image caption generation with visual attention.
\newblock In \emph{International conference on machine learning}, pages 2048--2057. PMLR, 2015.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Feng, Ye, Shen, Su, Cheng, Tao, Shi, Zhang, and Zhang]{diemph}
Xiangzhe Xu, Shiwei Feng, Yapeng Ye, Guangyu Shen, Zian Su, Siyuan Cheng, Guanhong Tao, Qingkai Shi, Zhuo Zhang, and Xiangyu Zhang.
\newblock Improving binary code similarity transformer models by semantics-driven instruction deemphasis.
\newblock In \emph{Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis}, ISSTA 2023, page 1106–1118, New York, NY, USA, 2023{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9798400702211.
\newblock \doi{10.1145/3597926.3598121}.
\newblock URL \url{https://doi.org/10.1145/3597926.3598121}.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Zhang, Feng, Ye, Su, Jiang, Cheng, Tan, and Zhang]{xu2023lmpa}
Xiangzhe Xu, Zhuo Zhang, Shiwei Feng, Yapeng Ye, Zian Su, Nan Jiang, Siyuan Cheng, Lin Tan, and Xiangyu Zhang.
\newblock Lmpa: Improving decompilation by synergy of large language model and program analysis.
\newblock \emph{arXiv preprint arXiv:2306.02546v1}, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2023{\natexlab{c}})Xu, Zhang, Su, Huang, Feng, Ye, Jiang, Xie, Cheng, Tan, et~al.]{xu2024leveraging}
Xiangzhe Xu, Zhuo Zhang, Zian Su, Ziyang Huang, Shiwei Feng, Yapeng Ye, Nan Jiang, Danning Xie, Siyuan Cheng, Lin Tan, et~al.
\newblock Leveraging generative models to recover variable names from stripped binary.
\newblock \emph{arXiv preprint arXiv:2306.02546v3}, 2023{\natexlab{c}}.

\bibitem[Ye et~al.(2023{\natexlab{a}})Ye, Zhang, Wang, Xu, and Zhang]{ye2023natural}
Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng Zhang.
\newblock Natural language is all a graph needs.
\newblock \emph{arXiv preprint arXiv:2308.07134}, 2023{\natexlab{a}}.

\bibitem[Ye et~al.(2023{\natexlab{b}})Ye, Wu, Ma, Zhang, Du, Liu, Ji, and Wang]{ye2023cp}
Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu, Shouling Ji, and Wenhai Wang.
\newblock Cp-bcs: Binary code summarization guided by control flow graph and pseudo code.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 14740--14752, 2023{\natexlab{b}}.

\bibitem[Yu et~al.(2022)Yu, Iter, Wang, Xu, Ju, Sanyal, Zhu, Zeng, and Jiang]{yu2022generate}
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang.
\newblock Generate rather than retrieve: Large language models are strong context generators.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Zhai et~al.(2023)Zhai, Tong, Li, Cai, Qu, Lee, and Ma]{zhai2023investigating}
Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu~Cai, Qing Qu, Yong~Jae Lee, and Yi~Ma.
\newblock Investigating the catastrophic forgetting in multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2309.10313}, 2023.

\bibitem[Zheng et~al.(2024)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhou et~al.(2022)Zhou, Alon, Xu, Jiang, and Neubig]{zhou2022docprompting}
Shuyan Zhou, Uri Alon, Frank~F Xu, Zhengbao Jiang, and Graham Neubig.
\newblock Docprompting: Generating code by retrieving the docs.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
