\begin{thebibliography}{105}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2020)Abdolmaleki, Huang, Hasenclever, Neunert,
  Song, Zambelli, Martins, Heess, Hadsell, and
  Riedmiller]{abdolmaleki2020distributional}
A.~Abdolmaleki, S.~Huang, L.~Hasenclever, M.~Neunert, F.~Song, M.~Zambelli,
  M.~Martins, N.~Heess, R.~Hadsell, and M.~Riedmiller.
\newblock A distributional view on multi-objective policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  11--22. PMLR, 2020.

\bibitem[Abdolmaleki et~al.(2021)Abdolmaleki, Huang, Vezzani, Shahriari,
  Springenberg, Mishra, TB, Byravan, Bousmalis, Gyorgy,
  et~al.]{abdolmaleki2021multi}
A.~Abdolmaleki, S.~H. Huang, G.~Vezzani, B.~Shahriari, J.~T. Springenberg,
  S.~Mishra, D.~TB, A.~Byravan, K.~Bousmalis, A.~Gyorgy, et~al.
\newblock On multi-objective policy optimization as a tool for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2106.08199}, 2021.

\bibitem[Abdolshah et~al.(2019)Abdolshah, Shilton, Rana, Gupta, and
  Venkatesh]{abdolshah2019multi}
M.~Abdolshah, A.~Shilton, S.~Rana, S.~Gupta, and S.~Venkatesh.
\newblock Multi-objective bayesian optimisation with preferences over
  objectives.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Astudillo and Frazier(2020)]{astudillo2020multi}
R.~Astudillo and P.~Frazier.
\newblock Multi-attribute bayesian optimization with interactive preference
  learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem[Balandat et~al.(2020)Balandat, Karrer, Jiang, Daulton, Letham, Wilson,
  and Bakshy]{balandat2020botorch}
M.~Balandat, B.~Karrer, D.~Jiang, S.~Daulton, B.~Letham, A.~G. Wilson, and
  E.~Bakshy.
\newblock Botorch: A framework for efficient monte-carlo bayesian optimization.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Belakaria and Deshwal(2019)]{belakaria2019max}
S.~Belakaria and A.~Deshwal.
\newblock Max-value entropy search for multi-objective bayesian optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Belakaria et~al.(2020)Belakaria, Deshwal, Jayakodi, and
  Doppa]{belakaria2020uncertainty}
S.~Belakaria, A.~Deshwal, N.~K. Jayakodi, and J.~R. Doppa.
\newblock Uncertainty-aware search framework for multi-objective bayesian
  optimization.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2020.

\bibitem[{Blank} and {Deb}(2020)]{pymoo}
J.~{Blank} and K.~{Deb}.
\newblock pymoo: Multi-objective optimization in python.
\newblock \emph{IEEE Access}, 8:\penalty0 89497--89509, 2020.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
S.~Boyd and L.~Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Bradford et~al.(2018)Bradford, Schweidtmann, and
  Lapkin]{bradford2018efficient}
E.~Bradford, A.~M. Schweidtmann, and A.~Lapkin.
\newblock Efficient multiobjective optimization employing gaussian processes,
  spectral sampling and a genetic algorithm.
\newblock \emph{Journal of global optimization}, 71\penalty0 (2):\penalty0
  407--438, 2018.

\bibitem[Brochu et~al.(2010)Brochu, Cora, and De~Freitas]{brochu2010tutorial}
E.~Brochu, V.~M. Cora, and N.~De~Freitas.
\newblock A tutorial on bayesian optimization of expensive cost functions, with
  application to active user modeling and hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1012.2599}, 2010.

\bibitem[Cheng and Li(1999)]{cheng1999generalized}
F.~Cheng and X.~Li.
\newblock Generalized center method for multiobjective engineering
  optimization.
\newblock \emph{Engineering Optimization}, 31\penalty0 (5):\penalty0 641--661,
  1999.

\bibitem[Choo and Atkins(1983)]{choo1983proper}
E.~U. Choo and D.~Atkins.
\newblock Proper efficiency in nonconvex multicriteria programming.
\newblock \emph{Mathematics of Operations Research}, 8\penalty0 (3):\penalty0
  467--470, 1983.

\bibitem[Daulton et~al.(2020)Daulton, Balandat, and
  Bakshy]{daulton2020differentiable}
S.~Daulton, M.~Balandat, and E.~Bakshy.
\newblock Differentiable expected hypervolume improvement for parallel
  multi-objective bayesian optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Daulton et~al.(2021)Daulton, Balandat, and
  Bakshy]{daulton2021parallel}
S.~Daulton, M.~Balandat, and E.~Bakshy.
\newblock Parallel bayesian optimization of multiple noisy objectives with
  expected hypervolume improvement.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Daulton et~al.(2022{\natexlab{a}})Daulton, Cakmak, Balandat, Osborne,
  Zhou, and Bakshy]{daulton2022robust}
S.~Daulton, S.~Cakmak, M.~Balandat, M.~A. Osborne, E.~Zhou, and E.~Bakshy.
\newblock Robust multi-objective bayesian optimization under input noise.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2022{\natexlab{a}}.

\bibitem[Daulton et~al.(2022{\natexlab{b}})Daulton, Eriksson, Balandat, and
  Bakshy]{daulton2022multi}
S.~Daulton, D.~Eriksson, M.~Balandat, and E.~Bakshy.
\newblock Multi-objective bayesian optimization over high-dimensional search
  spaces.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages 507--517. PMLR, 2022{\natexlab{b}}.

\bibitem[De~Ath et~al.(2021)De~Ath, Everson, Rahat, and Fieldsend]{de2021greed}
G.~De~Ath, R.~M. Everson, A.~A. Rahat, and J.~E. Fieldsend.
\newblock Greed is good: Exploration and exploitation trade-offs in bayesian
  optimisation.
\newblock \emph{ACM Transactions on Evolutionary Learning and Optimization},
  1\penalty0 (1):\penalty0 1--22, 2021.

\bibitem[Deb and Srinivasan(2006)]{deb2006innovization}
K.~Deb and A.~Srinivasan.
\newblock Innovization: Innovating design principles through optimization.
\newblock In \emph{Genetic and Evolutionary Computation Conference (GECCO)},
  2006.

\bibitem[Deb et~al.(2002{\natexlab{a}})Deb, Pratap, Agarwal, and
  Meyarivan]{deb2002fast}
K.~Deb, A.~Pratap, S.~Agarwal, and T.~Meyarivan.
\newblock A fast and elitist multiobjective genetic algorithm: Nsga-ii.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 6\penalty0
  (2):\penalty0 182--197, 2002{\natexlab{a}}.

\bibitem[Deb et~al.(2002{\natexlab{b}})Deb, Thiele, Laumanns, and
  Zitzler]{deb2002scalable}
K.~Deb, L.~Thiele, M.~Laumanns, and E.~Zitzler.
\newblock Scalable multi-objective optimization test problems.
\newblock In \emph{IEEE Congress on Evolutionary Computation (CEC)},
  2002{\natexlab{b}}.

\bibitem[Desautels et~al.(2014)Desautels, Krause, and
  Burdick]{desautels2014parallelizing}
T.~Desautels, A.~Krause, and J.~Burdick.
\newblock Parallelizing exploration-exploitation tradeoffs in gaussian process
  bandit optimization.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 3873--3923, 2014.

\bibitem[Dosovitskiy and Djolonga(2019)]{dosovitskiy2019you}
A.~Dosovitskiy and J.~Djolonga.
\newblock You only train once: Loss-conditional training of deep networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Ehrgott(2005)]{ehrgott2005multicriteria}
M.~Ehrgott.
\newblock \emph{Multicriteria optimization}, volume 491.
\newblock Springer Science \& Business Media, 2005.

\bibitem[Emmerich and Klinkenberg(2008)]{emmerich2008computation}
M.~Emmerich and J.~Klinkenberg.
\newblock The computation of the expected improvement in dominated hypervolume
  of pareto front approximations.
\newblock \emph{Rapport technique, Leiden University}, 34, 2008.

\bibitem[Emmerich et~al.(2006)Emmerich, Giannakoglou, and
  Naujoks]{emmerich2006single}
M.~Emmerich, K.~Giannakoglou, and B.~Naujoks.
\newblock Single-and multiobjective evolutionary optimization assisted by
  gaussian random field metamodels.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 10\penalty0
  (4):\penalty0 421--439, 2006.

\bibitem[Eriksson et~al.(2019)Eriksson, Pearce, Gardner, Turner, and
  Poloczek]{eriksson2019scalable}
D.~Eriksson, M.~Pearce, J.~Gardner, R.~D. Turner, and M.~Poloczek.
\newblock Scalable global optimization via local bayesian optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Eriksson et~al.(2021)Eriksson, Chuang, Daulton, Aly, Babu,
  Shrivastava, Xia, Zhao, Venkatesh, and Balandat]{eriksson2021latency}
D.~Eriksson, P.~I.-J. Chuang, S.~Daulton, A.~Aly, A.~Babu, A.~Shrivastava,
  P.~Xia, S.~Zhao, G.~Venkatesh, and M.~Balandat.
\newblock Latency-aware neural architecture search with multi-objective
  bayesian optimization.
\newblock \emph{arXiv preprint arXiv:2106.11890}, 2021.

\bibitem[Frazier(2018)]{frazier2018tutorial}
P.~I. Frazier.
\newblock A tutorial on bayesian optimization.
\newblock \emph{arXiv preprint arXiv:1807.02811}, 2018.

\bibitem[Garnett(2022)]{garnett2022bayesian}
R.~Garnett.
\newblock \emph{{Bayesian Optimization}}.
\newblock Cambridge University Press, 2022.
\newblock in preparation.

\bibitem[Giagkiozis and Fleming(2014)]{giagkiozis2014pareto}
I.~Giagkiozis and P.~J. Fleming.
\newblock Pareto front estimation for decision making.
\newblock \emph{Evolutionary computation}, 22\penalty0 (4):\penalty0 651--678,
  2014.

\bibitem[G{\'o}mez-Bombarelli et~al.(2018)G{\'o}mez-Bombarelli, Wei, Duvenaud,
  Hern{\'a}ndez-Lobato, S{\'a}nchez-Lengeling, Sheberla, Aguilera-Iparraguirre,
  Hirzel, Adams, and Aspuru-Guzik]{gomez2018automatic}
R.~G{\'o}mez-Bombarelli, J.~N. Wei, D.~Duvenaud, J.~M. Hern{\'a}ndez-Lobato,
  B.~S{\'a}nchez-Lengeling, D.~Sheberla, J.~Aguilera-Iparraguirre, T.~D.
  Hirzel, R.~P. Adams, and A.~Aspuru-Guzik.
\newblock Automatic chemical design using a data-driven continuous
  representation of molecules.
\newblock \emph{ACS central science}, 4\penalty0 (2):\penalty0 268--276, 2018.

\bibitem[Hennig and Schuler(2012)]{hennig2012entropy}
P.~Hennig and C.~J. Schuler.
\newblock Entropy search for information-efficient global optimization.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (6), 2012.

\bibitem[Hern{\'a}ndez-Lobato et~al.(2016)Hern{\'a}ndez-Lobato,
  Hernandez-Lobato, Shah, and Adams]{hernandez2016predictive}
D.~Hern{\'a}ndez-Lobato, J.~Hernandez-Lobato, A.~Shah, and R.~Adams.
\newblock Predictive entropy search for multi-objective bayesian optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Hern{\'a}ndez-Lobato et~al.(2014)Hern{\'a}ndez-Lobato, Hoffman, and
  Ghahramani]{hernandez2014predictive}
J.~M. Hern{\'a}ndez-Lobato, M.~W. Hoffman, and Z.~Ghahramani.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2014.

\bibitem[Hillermeier(2001)]{hillermeier2001generalized}
C.~Hillermeier.
\newblock Generalized homotopy approach to multiobjective optimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 110\penalty0
  (3):\penalty0 557--583, 2001.

\bibitem[Hoffman and Ghahramani(2015)]{hoffman2015output}
M.~W. Hoffman and Z.~Ghahramani.
\newblock Output-space predictive entropy search for flexible global
  optimization.
\newblock In \emph{NeurIPS Workshop on Bayesian Optimization}, 2015.

\bibitem[Jablonka et~al.(2021)Jablonka, Jothiappan, Wang, Smit, and
  Yoo]{jablonka2021bias}
K.~M. Jablonka, G.~M. Jothiappan, S.~Wang, B.~Smit, and B.~Yoo.
\newblock Bias free multiobjective active learning for materials design and
  discovery.
\newblock \emph{Nature Communications}, 12\penalty0 (1):\penalty0 1--10, 2021.

\bibitem[Jones(2001)]{jones2001taxonomy}
D.~R. Jones.
\newblock A taxonomy of global optimization methods based on response surfaces.
\newblock \emph{Journal of global optimization}, 21\penalty0 (4):\penalty0
  345--383, 2001.

\bibitem[Jones et~al.(1998)Jones, Schonlau, and Welch]{jones1998efficient}
D.~R. Jones, M.~Schonlau, and W.~J. Welch.
\newblock Efficient global optimization of expensive black-box functions.
\newblock \emph{Journal of Global Optimization}, 13\penalty0 (4):\penalty0
  455--492, 1998.

\bibitem[Kaliszewski(1987)]{kaliszewski1987modified}
I.~Kaliszewski.
\newblock A modified weighted tchebycheff metric for multiple objective
  programming.
\newblock \emph{Computers \& operations research}, 14\penalty0 (4):\penalty0
  315--323, 1987.

\bibitem[Kawaguchi et~al.(2015)Kawaguchi, Kaelbling, and
  Lozano-P{\'e}rez]{kawaguchi2015bayesian}
K.~Kawaguchi, L.~P. Kaelbling, and T.~Lozano-P{\'e}rez.
\newblock Bayesian optimization with exponential convergence.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2015.

\bibitem[Keane(2006)]{keane2006statistical}
A.~J. Keane.
\newblock Statistical improvement criteria for use in multiobjective design
  optimization.
\newblock \emph{AIAA journal}, 44\penalty0 (4):\penalty0 879--891, 2006.

\bibitem[Khan et~al.(2002)Khan, Goldberg, and Pelikan]{khan2002multi}
N.~Khan, D.~E. Goldberg, and M.~Pelikan.
\newblock Multi-objective bayesian optimization algorithm.
\newblock In \emph{Genetic and Evolutionary Computation Conference (GECCO)},
  pages 684--684. Citeseer, 2002.

\bibitem[Knowles(2006)]{knowles2006parego}
J.~Knowles.
\newblock {ParEGO}: A hybrid algorithm with on-line landscape approximation for
  expensive multiobjective optimization problems.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 10\penalty0
  (1):\penalty0 50--66, 2006.

\bibitem[Kramer(1994)]{kramer1994augmented}
S.~Kramer.
\newblock An augmented lagrange multiplier based method for mixed integer
  discrete continuous optimization and its applications to mechanical design.
\newblock \emph{Journal of Mechanical Design}, 116:\penalty0 405, 1994.

\bibitem[Kushner(1964)]{kushner1964new}
H.~J. Kushner.
\newblock A new method of locating the maximum point of an arbitrary multipeak
  curve in the presence of noise.
\newblock \emph{Journal of Basic Engineering}, 86\penalty0 (1):\penalty0
  97–106, 1964.

\bibitem[Laumanns and Ocenasek(2002)]{laumanns2002bayesian}
M.~Laumanns and J.~Ocenasek.
\newblock Bayesian optimization algorithms for multi-objective optimization.
\newblock In \emph{International Conference on Parallel Problem Solving from
  Nature (PPSN)}, 2002.

\bibitem[Lin et~al.(2019)Lin, Zhen, Li, Zhang, and Kwong]{lin2019pareto}
X.~Lin, H.-L. Zhen, Z.~Li, Q.~Zhang, and S.~Kwong.
\newblock Pareto multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  12060--12070, 2019.

\bibitem[Lin et~al.(2020)Lin, Yang, Zhang, and Kwong]{lin2020controllable}
X.~Lin, Z.~Yang, Q.~Zhang, and S.~Kwong.
\newblock Controllable pareto multi-task learning.
\newblock \emph{arXiv preprint arXiv:2010.06313}, 2020.

\bibitem[Lin et~al.(2022)Lin, Yang, and Zhang]{lin2022pareto}
X.~Lin, Z.~Yang, and Q.~Zhang.
\newblock Pareto set learning for neural multi-objective combinatorial
  optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Lopez-Paz and Sagun(2018)]{lopez2018easing}
D.~Lopez-Paz and L.~Sagun.
\newblock Easing non-convex optimization with neural networks.
\newblock In \emph{International Conference on Learning Representations (ICLR)
  Workshops}, 2018.

\bibitem[Lukovic et~al.(2020)Lukovic, Tian, and Matusik]{lukovic2020diversity}
M.~K. Lukovic, Y.~Tian, and W.~Matusik.
\newblock Diversity-guided multi-objective bayesian optimization with batch
  evaluations.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Ma et~al.(2020)Ma, Du, and Matusik]{ma2020efficient}
P.~Ma, T.~Du, and W.~Matusik.
\newblock Efficient continuous pareto exploration in multi-task learning.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Mahapatra and Rajan(2020)]{mahapatramulti2020multi}
D.~Mahapatra and V.~Rajan.
\newblock Multi-task learning with user preferences: Gradient descent with
  controlled ascent in pareto optimization.
\newblock \emph{Thirty-seventh International Conference on Machine Learning},
  2020.

\bibitem[Malkomes et~al.(2021)Malkomes, Cheng, Lee, and
  Mccourt]{malkomes2021beyond}
G.~Malkomes, B.~Cheng, E.~H. Lee, and M.~Mccourt.
\newblock Beyond the pareto efficient frontier: Constraint active search for
  multiobjective experimental design.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[McKay et~al.(2000)McKay, Beckman, and Conover]{mckay2000comparison}
M.~D. McKay, R.~J. Beckman, and W.~J. Conover.
\newblock A comparison of three methods for selecting values of input variables
  in the analysis of output from a computer code.
\newblock \emph{Technometrics}, 42\penalty0 (1):\penalty0 55--61, 2000.

\bibitem[Miettinen(1998)]{miettinen1998nonlinear}
K.~Miettinen.
\newblock \emph{Nonlinear multiobjective optimization}.
\newblock Springer Science \& Business Media, 1998.

\bibitem[Mo{\v{c}}kus(1975)]{movckus1975bayesian}
J.~Mo{\v{c}}kus.
\newblock On bayesian methods for seeking the extremum.
\newblock In \emph{Optimization techniques IFIP technical conference}, pages
  400--404. Springer, 1975.

\bibitem[Mockus(1989)]{mockus1989bayesian}
J.~Mockus.
\newblock \emph{Bayesian approach to global optimization: theory and
  applications}.
\newblock Kluwer Academic Publishers., 1989.

\bibitem[Navon et~al.(2021)Navon, Shamsian, Chechik, and
  Fetaya]{navon2020learning}
A.~Navon, A.~Shamsian, G.~Chechik, and E.~Fetaya.
\newblock Learning the pareto front with hypernetworks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Paria et~al.(2020)Paria, Kandasamy, and P{\'o}czos]{paria2020flexible}
B.~Paria, K.~Kandasamy, and B.~P{\'o}czos.
\newblock A flexible framework for multi-objective bayesian optimization using
  random scalarizations.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  2020.

\bibitem[Parisi et~al.(2016)Parisi, Pirotta, and Restelli]{parisi2016multi}
S.~Parisi, M.~Pirotta, and M.~Restelli.
\newblock Multi-objective reinforcement learning through continuous pareto
  manifold approximation.
\newblock \emph{Journal of Artificial Intelligence Research (JAIR)},
  57:\penalty0 187--227, 2016.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Pelikan et~al.(1999)Pelikan, Goldberg, Cant{\'u}-Paz,
  et~al.]{pelikan1999boa}
M.~Pelikan, D.~E. Goldberg, E.~Cant{\'u}-Paz, et~al.
\newblock Boa: The bayesian optimization algorithm.
\newblock In \emph{Genetic and Evolutionary Computation Conference (GECCO)},
  1999.

\bibitem[Pirotta et~al.(2015)Pirotta, Parisi, and Restelli]{pirotta2015multi}
M.~Pirotta, S.~Parisi, and M.~Restelli.
\newblock Multi-objective reinforcement learning with continuous pareto
  frontier approximation.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2015.

\bibitem[Ponweiser et~al.(2008)Ponweiser, Wagner, Biermann, and
  Vincze]{ponweiser2008multiobjective}
W.~Ponweiser, T.~Wagner, D.~Biermann, and M.~Vincze.
\newblock Multiobjective optimization on a limited budget of evaluations using
  model-assisted s-metric selection.
\newblock In \emph{International Conference on Parallel Problem Solving from
  Nature (PPSN)}, 2008.

\bibitem[Rakowska et~al.(1991)Rakowska, Haftka, and
  Watson]{rakowska1991tracing}
J.~Rakowska, R.~T. Haftka, and L.~T. Watson.
\newblock Tracing the efficient curve for multi-objective control-structure
  optimization.
\newblock \emph{Computing Systems in Engineering}, 2\penalty0 (5-6):\penalty0
  461--471, 1991.

\bibitem[Rasmussen and Williams(2006)]{rasmussen2006gaussian}
C.~E. Rasmussen and C.~K. Williams.
\newblock \emph{Gaussian processes for machine learning}.
\newblock {MIT} Press, 2006.

\bibitem[Ray and Liew(2002)]{ray2002swarm}
T.~Ray and K.~Liew.
\newblock A swarm metaphor for multiobjective design optimization.
\newblock \emph{Engineering optimization}, 34\penalty0 (2):\penalty0 141--153,
  2002.

\bibitem[Rehbach et~al.(2020)Rehbach, Zaefferer, Naujoks, and
  Bartz-Beielstein]{rehbach2020expected}
F.~Rehbach, M.~Zaefferer, B.~Naujoks, and T.~Bartz-Beielstein.
\newblock Expected improvement versus predicted value in surrogate-based
  optimization.
\newblock In \emph{Genetic and Evolutionary Computation Conference (GECCO)},
  2020.

\bibitem[Romero(2001)]{romero2001extended}
C.~Romero.
\newblock Extended lexicographic goal programming: a unifying approach.
\newblock \emph{Omega}, 29\penalty0 (1):\penalty0 63--71, 2001.

\bibitem[Roussel et~al.(2021)Roussel, Hanuka, and
  Edelen]{roussel2021multiobjective}
R.~Roussel, A.~Hanuka, and A.~Edelen.
\newblock Multiobjective bayesian optimization for online accelerator tuning.
\newblock \emph{Physical Review Accelerators and Beams}, 24\penalty0
  (6):\penalty0 062801, 2021.

\bibitem[Ruchte and Grabocka(2021)]{ruchte2021scalable}
M.~Ruchte and J.~Grabocka.
\newblock Scalable pareto front approximation for deep multi-objective
  learning.
\newblock In \emph{IEEE International Conference on Data Mining (ICDM)}, 2021.

\bibitem[Sener and Koltun(2018)]{sener2018multi}
O.~Sener and V.~Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  525--536, 2018.

\bibitem[Sener and Koltun(2020)]{sener2020learning}
O.~Sener and V.~Koltun.
\newblock Learning to guide random search.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Shahriari et~al.(2016)Shahriari, Swersky, Wang, Adams, and
  De~Freitas]{shahriari2016taking}
B.~Shahriari, K.~Swersky, Z.~Wang, R.~Adams, and N.~De~Freitas.
\newblock Taking the human out of the loop: A review of bayesian optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2016.

\bibitem[Shoshan et~al.(2019)Shoshan, Mechrez, and
  Zelnik-Manor]{shoshan2019dynamic}
A.~Shoshan, R.~Mechrez, and L.~Zelnik-Manor.
\newblock Dynamic-net: Tuning the objective without re-training for synthesis
  tasks.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2019.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
J.~Snoek, H.~Larochelle, and R.~Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2012.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{snoek2015scalable}
J.~Snoek, O.~Rippel, K.~Swersky, R.~Kiros, N.~Satish, N.~Sundaram, M.~Patwary,
  M.~Prabhat, and R.~Adams.
\newblock Scalable bayesian optimization using deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2009gaussian}
N.~Srinivas, A.~Krause, S.~M. Kakade, and M.~Seeger.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2010.

\bibitem[Steponavi{\v{c}}{\.e} et~al.(2017)Steponavi{\v{c}}{\.e}, Hyndman,
  Smith-Miles, and Villanova]{steponavivce2017dynamic}
I.~Steponavi{\v{c}}{\.e}, R.~J. Hyndman, K.~Smith-Miles, and L.~Villanova.
\newblock Dynamic algorithm selection for pareto optimal set approximation.
\newblock \emph{Journal of Global Optimization}, 67\penalty0 (1):\penalty0
  263--282, 2017.

\bibitem[Steuer and Choo(1983)]{steuer1983interactive}
R.~E. Steuer and E.-U. Choo.
\newblock An interactive weighted tchebycheff procedure for multiple objective
  programming.
\newblock \emph{Mathematical Programming}, 26\penalty0 (3):\penalty0 326--344,
  1983.

\bibitem[Suzuki et~al.(2020)Suzuki, Takeno, Tamura, Shitara, and
  Karasuyama]{suzuki2020multi}
S.~Suzuki, S.~Takeno, T.~Tamura, K.~Shitara, and M.~Karasuyama.
\newblock Multi-objective bayesian optimization using pareto-frontier entropy.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Tanabe and Ishibuchi(2020)]{tanabe2020easy}
R.~Tanabe and H.~Ishibuchi.
\newblock An easy-to-use real-world multi-objective optimization problem suite.
\newblock \emph{Applied Soft Computing}, 89:\penalty0 106078, 2020.

\bibitem[Tripp et~al.(2020)Tripp, Daxberger, and
  Hern{\'a}ndez-Lobato]{tripp2020sample}
A.~Tripp, E.~Daxberger, and J.~M. Hern{\'a}ndez-Lobato.
\newblock Sample-efficient optimization in the latent space of deep generative
  models via weighted retraining.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Vaidyanathan et~al.(2003)Vaidyanathan, Tucker, Papila, and
  Shyy]{vaidyanathan2003cfd}
R.~Vaidyanathan, K.~Tucker, N.~Papila, and W.~Shyy.
\newblock Cfd-based design optimization for single element rocket injector.
\newblock In \emph{Aerospace Sciences Meeting and Exhibit}, 2003.

\bibitem[Van~Veldhuizen and Lamont(1999)]{van1999multiobjective}
D.~A. Van~Veldhuizen and G.~B. Lamont.
\newblock Multiobjective evolutionary algorithm test suites.
\newblock In \emph{ACM Symposium on Applied Computing (SAC)}, 1999.

\bibitem[Wang et~al.(2020)Wang, Fonseca, and Tian]{wang2020learning}
L.~Wang, R.~Fonseca, and Y.~Tian.
\newblock Learning search space partition for black-box optimization using
  monte carlo tree search.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Wang and Jegelka(2017)]{wang2017max}
Z.~Wang and S.~Jegelka.
\newblock Max-value entropy search for efficient bayesian optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Wang et~al.(2013)Wang, Zoghi, Hutter, Matheson, and
  De~Freitas]{wang2013bayesian}
Z.~Wang, M.~Zoghi, F.~Hutter, D.~Matheson, and N.~De~Freitas.
\newblock Bayesian optimization in high dimensions via random embeddings.
\newblock In \emph{International Joint Conferences on Artificial Intelligence
  (IJCAI)}, 2013.

\bibitem[Wang et~al.(2018)Wang, Gehring, Kohli, and Jegelka]{wang2018batched}
Z.~Wang, C.~Gehring, P.~Kohli, and S.~Jegelka.
\newblock Batched large-scale bayesian optimization in high-dimensional spaces.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2018.

\bibitem[Wilson et~al.(2018)Wilson, Hutter, and
  Deisenroth]{wilson2018maximizing}
J.~Wilson, F.~Hutter, and M.~Deisenroth.
\newblock Maximizing acquisition functions for bayesian optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~31, 2018.

\bibitem[Wilson et~al.(2017)Wilson, Moriconi, Hutter, and
  Deisenroth]{wilson2017reparameterization}
J.~T. Wilson, R.~Moriconi, F.~Hutter, and M.~P. Deisenroth.
\newblock The reparameterization trick for acquisition functions.
\newblock \emph{arXiv preprint arXiv:1712.00424}, 2017.

\bibitem[Wu and Frazier(2016)]{wu2016parallel}
J.~Wu and P.~Frazier.
\newblock The parallel knowledge gradient method for batch bayesian
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem[Yang et~al.(2019)Yang, Sun, and Narasimhan]{yang2019generalized}
R.~Yang, X.~Sun, and K.~Narasimhan.
\newblock A generalized algorithm for multi-objective reinforcement learning
  and policy adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Zhang and Li(2007)]{zhang2007moea}
Q.~Zhang and H.~Li.
\newblock {MOEA/D}: A multiobjective evolutionary algorithm based on
  decomposition.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 11\penalty0
  (6):\penalty0 712--731, 2007.

\bibitem[Zhang et~al.(2008)Zhang, Zhou, and Jin]{zhang2008rm}
Q.~Zhang, A.~Zhou, and Y.~Jin.
\newblock Rm-meda: A regularity model-based multiobjective estimation of
  distribution algorithm.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 12\penalty0
  (1):\penalty0 41--63, 2008.

\bibitem[Zhang et~al.(2010)Zhang, Liu, Tsang, and Virginas]{zhang2010expensive}
Q.~Zhang, W.~Liu, E.~Tsang, and B.~Virginas.
\newblock Expensive multiobjective optimization by moea/d with gaussian process
  model.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 14\penalty0
  (3):\penalty0 456--474, 2010.

\bibitem[Zhang and Golovin(2020)]{zhang2020random}
R.~Zhang and D.~Golovin.
\newblock Random hypervolume scalarizations for provable multi-objective black
  box optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Zhao et~al.(2022)Zhao, Wang, Yang, Zhang, Guo, and
  Tian]{zhao2022multi}
Y.~Zhao, L.~Wang, K.~Yang, T.~Zhang, T.~Guo, and Y.~Tian.
\newblock Multi-objective optimization by learning space partitions.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Zitzler et~al.(2000)Zitzler, Deb, and Thiele]{zitzler2000comparison}
E.~Zitzler, K.~Deb, and L.~Thiele.
\newblock Comparison of multiobjective evolutionary algorithms: {E}mpirical
  results.
\newblock \emph{Evolutionary Computation}, 8\penalty0 (2):\penalty0 173--195,
  2000.

\bibitem[Zitzler et~al.(2007)Zitzler, Brockhoff, and
  Thiele]{zitzler2007hypervolume}
E.~Zitzler, D.~Brockhoff, and L.~Thiele.
\newblock The hypervolume indicator revisited: On the design of
  pareto-compliant indicators via weighted integration.
\newblock In \emph{International Conference on Evolutionary Multi-Criterion
  Optimization (EMO)}, 2007.

\bibitem[Zuluaga et~al.(2013)Zuluaga, Sergent, Krause, and
  P{\"u}schel]{zuluaga2013active}
M.~Zuluaga, G.~Sergent, A.~Krause, and M.~P{\"u}schel.
\newblock Active learning for multi-objective optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2013.

\bibitem[Zuluaga et~al.(2016)Zuluaga, Krause, and
  P{\"u}schel]{zuluaga2016varepsilon}
M.~Zuluaga, A.~Krause, and M.~P{\"u}schel.
\newblock $\varepsilon$-pal: an active learning approach to the multi-objective
  optimization problem.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 3619--3650, 2016.

\end{thebibliography}
