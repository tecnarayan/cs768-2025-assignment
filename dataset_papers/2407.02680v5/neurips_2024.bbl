\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Add()]{AddressSanitizer}
Kernel address sanitizer.
\newblock \url{https://www.kernel.org/doc/html/latest/dev-tools/kasan.html}.

\bibitem[And()]{Android}
Android.
\newblock \url{https://blog.google/products/android/io22-multideviceworld}.

\bibitem[Bug()]{BugLink}
Syzkaller kasan use-after-free bug.
\newblock \url{https://syzkaller.appspot.com/bug?extid=822d1359297e2694f873}.

\bibitem[CVE()]{CVEDetails}
Cvedetails.
\newblock \url{https://www.cvedetails.com/product/47/Linux-Linux-Kernel.html?vendor_id=33}.

\bibitem[Con()]{ConcurrencySanitizer}
The kernel concurrency sanitizer (kcsan).
\newblock \url{https://www.kernel.org/doc/html/latest/dev-tools/kcsan.html}.

\bibitem[Lin()]{Linux}
Linux.
\newblock \url{https://github.com/torvalds/linux}.

\bibitem[RV6()]{RV63}
Linux 6.3.
\newblock \url{https://lwn.net/Articles/929582/}.

\bibitem[Syz()]{Syzkaller}
Syzkaller.
\newblock \url{https://github.com/google/syzkaller}.

\bibitem[Tri()]{Trinity}
Trinity: Linux system call fuzzer.
\newblock \url{https://github.com/kernelslacker/trinity}.

\bibitem[lla()]{llama3}
Llama3.
\newblock \url{https://github.com/meta-llama/llama3}.

\bibitem[Ahmad et~al.(2021)Ahmad, Chakraborty, Ray, and Chang]{ahmad-etal-2021-unified}
Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
\newblock Unified pre-training for program understanding and generation.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 2655--2668, Online, June 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.211}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.211}.

\bibitem[Amazon(2023)]{amazon-2023-codewhisperer}
Amazon.
\newblock Amazon codewhisperer: Build applications faster and more securely with your ai coding companion.
\newblock \url{https://aws.amazon.com/codewhisperer/}, 2023.

\bibitem[Athiwaratkun et~al.(2023)Athiwaratkun, Gouda, Wang, Li, Tian, Tan, Ahmad, Wang, Sun, Shang, Gonugondla, Ding, Kumar, Fulton, Farahani, Jain, Giaquinto, Qian, Ramanathan, Nallapati, Ray, Bhatia, Sengupta, Roth, and Xiang]{athiwaratkun2022multi}
Ben Athiwaratkun, Sanjay~Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi~Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan~Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali~Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, and Bing Xiang.
\newblock Multi-lingual evaluation of code generation models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Bo7eeXm6An8}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{ArXiv preprint}, abs/2108.07732, 2021.
\newblock URL \url{https://arxiv.org/abs/2108.07732}.

\bibitem[Cassano et~al.(2023)Cassano, Gouwar, Nguyen, Nguyen, Phipps-Costin, Pinckney, Yee, Zi, Anderson, Feldman, Guha, Greenberg, and Jangda]{cassano2023multiple}
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn~Jane Anderson, Molly~Q Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda.
\newblock Multipl-e: A scalable and polyglot approach to benchmarking neural code generation.
\newblock \emph{IEEE Transactions on Software Engineering}, 49\penalty0 (7):\penalty0 3675--3691, 2023.
\newblock \doi{10.1109/TSE.2023.3267446}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.
\newblock Evaluating large language models trained on code, 2021.

\bibitem[Clement et~al.(2021)Clement, Lu, Liu, Tufano, Drain, Duan, Sundaresan, and Svyatkovskiy]{clement-etal-2021-long}
Colin Clement, Shuai Lu, Xiaoyu Liu, Michele Tufano, Dawn Drain, Nan Duan, Neel Sundaresan, and Alexey Svyatkovskiy.
\newblock Long-range modeling of source code files with e{WASH}: Extended window access by syntax hierarchy.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 4713--4722, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.387}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.387}.

\bibitem[CodeGeeX(2022)]{humaneval_x}
CodeGeeX, 2022.
\newblock \url{https://github.com/THUDM/CodeGeeX}.

\bibitem[Ding et~al.(2023{\natexlab{a}})Ding, Kumar, Tian, Wang, Kwiatkowski, Li, Ramanathan, Ray, Bhatia, Sengupta, et~al.]{ding2023static}
Hantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski, Xiaopeng Li, Murali~Krishna Ramanathan, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, et~al.
\newblock A static evaluation of code completion by large language models.
\newblock \emph{arXiv preprint arXiv:2306.03203}, 2023{\natexlab{a}}.

\bibitem[Ding et~al.(2022)Ding, Wang, Ahmad, Ramanathan, Nallapati, Bhatia, Roth, and Xiang]{ding2022cocomic}
Yangruibo Ding, Zijian Wang, Wasi~Uddin Ahmad, Murali~Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang.
\newblock Cocomic: Code completion by jointly modeling in-file and cross-file context.
\newblock \emph{arXiv preprint arXiv:2212.10007}, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.10007}.

\bibitem[Ding et~al.(2023{\natexlab{b}})Ding, Wang, Ahmad, Ding, Tan, Jain, Ramanathan, Nallapati, Bhatia, Roth, and Xiang]{ding2023crosscodeeval}
Yangruibo Ding, Zijian Wang, Wasi~Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali~Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang.
\newblock Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/pdf/2310.11248.pdf}.

\bibitem[Dinh et~al.(2023)Dinh, Zhao, Tan, Negrinho, Lausen, Zha, and Karypis]{dinh2023large}
Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, and George Karypis.
\newblock Large language models of code fail at completing code with potential bugs, 2023.

\bibitem[Feng et~al.(2020)Feng, Guo, Tang, Duan, Feng, Gong, Shou, Qin, Liu, Jiang, and Zhou]{feng-etal-2020-codebert}
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
\newblock {C}ode{BERT}: A pre-trained model for programming and natural languages.
\newblock In Trevor Cohn, Yulan He, and Yang Liu, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2020}, pages 1536--1547, Online, November 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.139}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.139}.

\bibitem[Fried et~al.(2023)Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong, Yih, Zettlemoyer, and Lewis]{fried2022incoder}
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis.
\newblock Incoder: A generative model for code infilling and synthesis.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=hQwb-lbM6EL}.

\bibitem[Gao et~al.(2022)Gao, Noller, and Roychoudhury]{gao2022program}
Xiang Gao, Yannic Noller, and Abhik Roychoudhury.
\newblock Program repair, 2022.

\bibitem[GitHub(2021)]{github-2021-copilot}
GitHub.
\newblock Github copilot: Your ai pair programmer.
\newblock \url{https://copilot.github.com/}, 2021.

\bibitem[Guo et~al.(2022)Guo, Lu, Duan, Wang, Zhou, and Yin]{guo2022unixcoder}
Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin.
\newblock Unixcoder: Unified cross-modal pre-training for code representation.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 7212--7225, 2022.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, and Steinhardt]{hendrycksapps2021}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt.
\newblock Measuring coding challenge competence with apps.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Jimenez et~al.(2024)Jimenez, Yang, Wettig, Yao, Pei, Press, and Narasimhan]{jimenez2024swebench}
Carlos~E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan.
\newblock Swe-bench: Can language models resolve real-world github issues?, 2024.

\bibitem[Kang et~al.(2023)Kang, Yoon, and Yoo]{kang2023large}
Sungmin Kang, Juyeon Yoon, and Shin Yoo.
\newblock Large language models are few-shot testers: Exploring llm-based general bug reproduction, 2023.

\bibitem[Kim et~al.(2020)Kim, Jeong, Kim, Jang, Shin, and Lee]{Kim2020HFLHF}
Kyungtae Kim, Dae~R. Jeong, Chung~Hwan Kim, Yeongjin Jang, Insik Shin, and Byoungyoung Lee.
\newblock Hfl: Hybrid fuzzing on the linux kernel.
\newblock \emph{Proceedings 2020 Network and Distributed System Security Symposium}, 2020.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:211267895}.

\bibitem[Li et~al.(2022)Li, Lu, Guo, Duan, Jannu, Jenks, Majumder, Green, Svyatkovskiy, Fu, and Sundaresan]{li2022automating}
Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, and Neel Sundaresan.
\newblock Automating code review activities by large-scale pre-training, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Xia, Wang, and Zhang]{liu2023code}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Xie, and Liu]{liu2023commitbart}
Shangqing Liu, Yanzhou Li, Xiaofei Xie, and Yang Liu.
\newblock Commitbart: A large pre-trained model for github commits, 2023{\natexlab{b}}.

\bibitem[Lu et~al.(2021)Lu, Guo, Ren, Huang, Svyatkovskiy, Blanco, Clement, Drain, Jiang, Tang, Li, Zhou, Shou, Zhou, Tufano, GONG, Zhou, Duan, Sundaresan, Deng, Fu, and LIU]{lu2021codexglue}
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge~Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao~Kun Deng, Shengyu Fu, and Shujie LIU.
\newblock Code{XGLUE}: A machine learning benchmark dataset for code understanding and generation.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6lE4dQXaUcb}.

\bibitem[Lu et~al.(2022)Lu, Duan, Han, Guo, Hwang, and Svyatkovskiy]{lu-etal-2022-reacc}
Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy.
\newblock {R}e{ACC}: A retrieval-augmented code completion framework.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 6227--6240, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.431}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.431}.

\bibitem[Nijkamp et~al.(2023{\natexlab{a}})Nijkamp, Hayashi, Xiong, Savarese, and Zhou]{nijkamp2023codegen2}
Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou.
\newblock Codegen2: Lessons for training llms on programming and natural languages.
\newblock \emph{arXiv preprint arXiv:2305.02309}, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2305.02309}.

\bibitem[Nijkamp et~al.(2023{\natexlab{b}})Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese, and Xiong]{nijkamp2022codegen}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.
\newblock Codegen: An open large language model for code with multi-turn program synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=iaYcJKpY2B_}.

\bibitem[Ott et~al.(2022)Ott, Barbosa-Silva, Blagec, Brauner, and Samwald]{Ott_2022}
Simon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan Brauner, and Matthias Samwald.
\newblock Mapping global dynamics of benchmark creation and saturation in artificial intelligence.
\newblock \emph{Nature Communications}, 13\penalty0 (1), November 2022.
\newblock ISSN 2041-1723.
\newblock \doi{10.1038/s41467-022-34591-0}.
\newblock URL \url{http://dx.doi.org/10.1038/s41467-022-34591-0}.

\bibitem[Pei et~al.(2023)Pei, Zhao, Lausen, Zha, and Karypis]{pei2023better}
Hengzhi Pei, Jinman Zhao, Leonard Lausen, Sheng Zha, and George Karypis.
\newblock Better context makes better code language models: A case study on function call argument completion.
\newblock In \emph{Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence}, AAAI'23/IAAI'23/EAAI'23. AAAI Press, 2023.
\newblock ISBN 978-1-57735-880-0.
\newblock \doi{10.1609/aaai.v37i4.25653}.
\newblock URL \url{https://doi.org/10.1609/aaai.v37i4.25653}.

\bibitem[Puri et~al.(2021)Puri, Kung, Janssen, Zhang, Domeniconi, Zolotov, Dolby, Chen, Choudhury, Decker, Thost, Buratti, Pujar, Ramji, Finkler, Malaika, and Reiss]{puri2021codenet}
Ruchir Puri, David~S Kung, Geert Janssen, Wei Zhang, Giacomo Domeniconi, Vladimir Zolotov, Julian Dolby, Jie Chen, Mihir Choudhury, Lindsey Decker, Veronika Thost, Luca Buratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss.
\newblock Codenet: A large-scale {AI} for code dataset for learning a diversity of coding tasks.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6vZVBkCDrHT}.

\bibitem[Rozière et~al.(2024)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Sauvestre, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve]{rozière2024code}
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian~Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.
\newblock Code llama: Open foundation models for code, 2024.

\bibitem[Schumilo et~al.(2017)Schumilo, Aschermann, Gawlik, Schinzel, and Holz]{Schumilo2017kAFLHF}
Sergej Schumilo, Cornelius Aschermann, Robert Gawlik, Sebastian Schinzel, and Thorsten Holz.
\newblock kafl: Hardware-assisted feedback fuzzing for os kernels.
\newblock In \emph{USENIX Security Symposium}, 2017.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:12778185}.

\bibitem[Serebryany et~al.(2012)Serebryany, Bruening, Potapenko, and Vyukov]{Serebryany2012AddressSanitizerAF}
Kostya Serebryany, Derek Bruening, Alexander Potapenko, and Dmitriy Vyukov.
\newblock Addresssanitizer: A fast address sanity checker.
\newblock In \emph{USENIX Annual Technical Conference}, 2012.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:11024896}.

\bibitem[Shrivastava et~al.(2023)Shrivastava, Larochelle, and Tarlow]{shrivastava2022repository}
Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
\newblock Repository-level prompt generation for large language models of code.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 31693--31715. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/shrivastava23a.html}.

\bibitem[Stepanov and Serebryany(2015)]{7054186}
Evgeniy Stepanov and Konstantin Serebryany.
\newblock Memorysanitizer: Fast detector of uninitialized memory use in c++.
\newblock In \emph{2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)}, pages 46--55, 2015.
\newblock \doi{10.1109/CGO.2015.7054186}.

\bibitem[Wang et~al.(2024)Wang, Huang, Chen, Liu, Wang, and Wang]{wang2024software}
Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang.
\newblock Software testing with large language models: Survey, landscape, and vision, 2024.

\bibitem[Wang et~al.(2023)Wang, Li, Qian, Yang, Wang, Shang, Kumar, Tan, Ray, Bhatia, Nallapati, Ramanathan, Roth, and Xiang]{wang2023recode}
Shiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar, Samson Tan, Baishakhi Ray, Parminder Bhatia, Ramesh Nallapati, Murali~Krishna Ramanathan, Dan Roth, and Bing Xiang.
\newblock {R}e{C}ode: Robustness evaluation of code generation models.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 13818--13843, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.773}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.773}.

\bibitem[Wang et~al.(2021)Wang, Wang, Joty, and Hoi]{wang-etal-2021-codet5}
Yue Wang, Weishi Wang, Shafiq Joty, and Steven~C.H. Hoi.
\newblock {C}ode{T}5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 8696--8708, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.685}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.685}.

\bibitem[Xia et~al.(2024)Xia, Paltenghi, Tian, Pradel, and Zhang]{xia2024fuzz4all}
Chunqiu~Steven Xia, Matteo Paltenghi, Jia~Le Tian, Michael Pradel, and Lingming Zhang.
\newblock Fuzz4all: Universal fuzzing with large language models, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Zhang, Liu, Zan, Mao, Lou, and Chen]{zhang2023repocoder}
Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi~Mao, Jian-Guang Lou, and Weizhu Chen.
\newblock Repocoder: Repository-level code completion through iterative retrieval and generation.
\newblock \emph{arXiv preprint arXiv:2303.12570}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.12570}.

\end{thebibliography}
