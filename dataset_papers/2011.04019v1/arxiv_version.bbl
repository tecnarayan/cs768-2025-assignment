\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019{\natexlab{a}})Abbasi-Yadkori, Bartlett,
  Bhatia, Lazic, Szepesvari, and Weisz]{abbasi2019politex}
Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba
  Szepesvari, and Gell{\'e}rt Weisz.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  3692--3702, 2019{\natexlab{a}}.

\bibitem[Abbasi-Yadkori et~al.(2019{\natexlab{b}})Abbasi-Yadkori, Lazic,
  Szepesvari, and Weisz]{abbasi2019exploration}
Yasin Abbasi-Yadkori, Nevena Lazic, Csaba Szepesvari, and Gellert Weisz.
\newblock Exploration-enhanced politex.
\newblock \emph{arXiv preprint arXiv:1908.10479}, 2019{\natexlab{b}}.

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Henaff, Kakade, and
  Sun]{agarwal2020pc}
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock \emph{arXiv preprint arXiv:2007.08459}, 2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020theory}
Alekh Agarwal, Sham~M. Kakade, Jason~D. Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift, 2020{\natexlab{b}}.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008fitted}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Fitted {Q}-iteration in continuous action-space {MDP}s.
\newblock In \emph{Advances in neural information processing systems}, pages
  9--16, 2008.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, and Hilbert~J Kappen.
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock \emph{Machine learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Bellman et~al.(1963)Bellman, Kalaba, and Kotkin]{BeKaKo63}
I.~R. Bellman, R.~Kalaba, and B.~Kotkin.
\newblock Polynomial approximation -- a new computational technique in dynamic
  programming.
\newblock \emph{Math. Comp.}, 17\penalty0 (8):\penalty0 155--161, 1963.

\bibitem[Belloni et~al.(2013)Belloni, Chernozhukov, et~al.]{belloni2013least}
Alexandre Belloni, Victor Chernozhukov, et~al.
\newblock Least squares after model selection in high-dimensional sparse
  models.
\newblock \emph{Bernoulli}, 19\penalty0 (2):\penalty0 521--547, 2013.

\bibitem[Bertsekas(1995)]{bertsekas1995dynamic}
Dimitri~P. Bertsekas.
\newblock \emph{Dynamic programming and optimal control}, volume~1.
\newblock Athena Scientific, 1995.

\bibitem[Bertsekas and Tsitsiklis(1996)]{BeTs96}
Dimitri~P. Bertsekas and John~N. Tsitsiklis.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific, 1996.

\bibitem[Bickel et~al.(2009)Bickel, Ritov, Tsybakov,
  et~al.]{bickel2009simultaneous}
Peter~J Bickel, Ya'acov Ritov, Alexandre~B Tsybakov, et~al.
\newblock Simultaneous analysis of {L}asso and {D}antzig selector.
\newblock \emph{The Annals of Statistics}, 37\penalty0 (4):\penalty0
  1705--1732, 2009.

\bibitem[B{\"u}hlmann and Van De~Geer(2011)]{buhlmann2011statistics}
Peter B{\"u}hlmann and Sara Van De~Geer.
\newblock \emph{Statistics for high-dimensional data: methods, theory and
  applications}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[Bunea et~al.(2007)Bunea, Tsybakov, and Wegkamp]{Buneaetal07}
F.~Bunea, A.~Tsybakov, and M.~Wegkamp.
\newblock Sparsity oracle inequalities for the {L}asso.
\newblock \emph{Electronic Journal of Statistics}, 1:\penalty0 169--194, 2007.

\bibitem[Cai et~al.(2019)Cai, Yang, Jin, and Wang]{cai2019provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock \emph{arXiv preprint arXiv:1912.05830}, 2019.

\bibitem[Calandriello et~al.(2014)Calandriello, Lazaric, and
  Restelli]{calandriello2014sparse}
Daniele Calandriello, Alessandro Lazaric, and Marcello Restelli.
\newblock Sparse multi-task reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  819--827, 2014.

\bibitem[Chen et~al.(2001)Chen, Donoho, and Saunders]{chen2001atomic}
Scott~Shaobing Chen, David~L Donoho, and Michael~A Saunders.
\newblock Atomic decomposition by basis pursuit.
\newblock \emph{SIAM review}, 43\penalty0 (1):\penalty0 129--159, 2001.

\bibitem[Duan and Wang(2020)]{duan2020minimax}
Yaqi Duan and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock \emph{Internation Conference on Machine Learning}, 2020.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 503--556, 2005.

\bibitem[Farahmand et~al.(2016)Farahmand, Ghavamzadeh, Szepesv{\'a}ri, and
  Mannor]{farahmand2016:jmlr}
A.m. Farahmand, M.~Ghavamzadeh, Cs. Szepesv{\'a}ri, and S.~Mannor.
\newblock Regularized policy iteration with nonparametric function spaces.
\newblock \emph{JMLR}, 17:\penalty0 1--66, 2016.

\bibitem[Farahmand et~al.(2008)Farahmand, Ghavamzadeh, Szepesv{\'a}ri, and
  Mannor]{massoud2008regularized}
Amir~massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv{\'a}ri, and Shie
  Mannor.
\newblock Regularized fitted {Q}-iteration: Application to planning.
\newblock In \emph{European Workshop on Reinforcement Learning}, pages 55--68.
  Springer, 2008.

\bibitem[Geist and Scherrer(2011)]{geist2011}
Matthieu Geist and Bruno Scherrer.
\newblock $\ell^1$-penalized projected {B}ellman residual.
\newblock In \emph{European Workshop on Reinforcement Learning}, pages 89--101.
  Springer, 2011.

\bibitem[Geist et~al.(2012)Geist, Scherrer, Lazaric, and
  Ghavamzadeh]{geist2012dantzig}
Matthieu Geist, Bruno Scherrer, Alessandro Lazaric, and Mohammad Ghavamzadeh.
\newblock A {D}antzig selector approach to temporal difference learning.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 347--354, 2012.

\bibitem[Ghavamzadeh et~al.(2011)Ghavamzadeh, Lazaric, Munos, and
  Hoffman]{ghavamzadeh2011finite}
Mohammad Ghavamzadeh, Alessandro Lazaric, R{\'e}mi Munos, and Matthew Hoffman.
\newblock Finite-sample analysis of {L}asso-{TD}.
\newblock In \emph{Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, pages 1177--1184, 2011.

\bibitem[Hallak and Mannor(2017)]{hallak2017consistent}
Assaf Hallak and Shie Mannor.
\newblock Consistent on-line off-policy evaluation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1372--1383. JMLR. org, 2017.

\bibitem[Hastie et~al.(2015)Hastie, Tibshirani, and
  Wainwright]{hastie2015statistical}
Trevor Hastie, Robert Tibshirani, and Martin Wainwright.
\newblock \emph{Statistical learning with sparsity: the lasso and
  generalizations}.
\newblock CRC press, 2015.

\bibitem[Hoffman et~al.(2011)Hoffman, Lazaric, Ghavamzadeh, and
  Munos]{hoffman2011regularized}
Matthew~W Hoffman, Alessandro Lazaric, Mohammad Ghavamzadeh, and R{\'e}mi
  Munos.
\newblock Regularized least squares temporal difference learning with nested
  $\ell^2$ and $\ell^1$ penalization.
\newblock In \emph{European Workshop on Reinforcement Learning}, pages
  102--114. Springer, 2011.

\bibitem[Ibrahimi et~al.(2012)Ibrahimi, Javanmard, and
  Roy]{ibrahimi2012efficient}
Morteza Ibrahimi, Adel Javanmard, and Benjamin~V Roy.
\newblock Efficient reinforcement learning for high dimensional linear
  quadratic systems.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2636--2644, 2012.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  652--661, 2016.

\bibitem[Jin et~al.(2019)Jin, Yang, Wang, and Jordan]{jin2019provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1907.05388}, 2019.

\bibitem[Kakade et~al.(2003)]{kakade2003sample}
Sham~Machandranath Kakade et~al.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[Kallus and Uehara(2020)]{kallus2020double}
Nathan Kallus and Masatoshi Uehara.
\newblock Double reinforcement learning for efficient off-policy evaluation in
  markov decision processes.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (167):\penalty0 1--63, 2020.

\bibitem[Kolter and Ng(2009)]{kolter2009regularization}
J~Zico Kolter and Andrew~Y Ng.
\newblock Regularization and feature selection in least-squares temporal
  difference learning.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 521--528, 2009.

\bibitem[Lagoudakis and Parr(2003)]{lagoudakis2003least}
Michail~G Lagoudakis and Ronald Parr.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Dec):\penalty0 1107--1149, 2003.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pages 45--73. Springer, 2012.

\bibitem[Lazic et~al.(2020)Lazic, Yin, Farajtabar, Levine, Gorur, Harris, and
  Schuurmans]{lazic2020maximum}
Nevena Lazic, Dong Yin, Mehrdad Farajtabar, Nir Levine, Dilan Gorur, Chris
  Harris, and Dale Schuurmans.
\newblock A maximum-entropy approach to off-policy evaluation in average-reward
  mdps.
\newblock \emph{Conference on Neural Information Processing Systems}, 2020.

\bibitem[Le et~al.(2019)Le, Voloshin, and Yue]{le2019batch}
Hoang Le, Cameron Voloshin, and Yisong Yue.
\newblock Batch policy learning under constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  3703--3712, 2019.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesvari]{li2015toward}
Lihong Li, Remi Munos, and Csaba Szepesvari.
\newblock Toward minimax off-policy value estimation.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 608--616,
  2015.

\bibitem[Liu et~al.(2012)Liu, Mahadevan, and Liu]{liu2012regularized}
Bo~Liu, Sridhar Mahadevan, and Ji~Liu.
\newblock Regularized off-policy {TD}-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  836--844, 2012.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5356--5366, 2018.

\bibitem[Moore(1990)]{moore1990efficient}
Andrew~William Moore.
\newblock Efficient memory-based learning for robot control.
\newblock 1990.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (May):\penalty0 815--857, 2008.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock {DualDICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2315--2325, 2019.

\bibitem[Painter-Wakefield and Parr(2012)]{painter2012greedy}
Christopher Painter-Wakefield and Ronald Parr.
\newblock Greedy algorithms for sparse reinforcement learning.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 867--874, 2012.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Doina Precup, Richard~S Sutton, and Satinder Singh.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{ICML'00 Proceedings of the Seventeenth International
  Conference on Machine Learning}, 2000.

\bibitem[Rish and Grabarnik(2014)]{RiGr14}
Irina Rish and Genady Grabarnik.
\newblock \emph{Sparse Modeling: Theory, Algorithms, and Applications}.
\newblock CRC Press, 2014.

\bibitem[Schweitzer and Seidmann(1985)]{SchSe85}
Paul~J Schweitzer and Abraham Seidmann.
\newblock Generalized polynomial approximations in {M}arkovian decision
  processes.
\newblock \emph{Journal of Mathematical Analysis and Applications},
  110\penalty0 (2):\penalty0 568--582, 1985.

\bibitem[Szepesv{\'a}ri(2010)]{sze10}
Csaba Szepesv{\'a}ri.
\newblock \emph{Algorithms for Reinforcement Learning}.
\newblock Morgan and Claypool, 2010.

\bibitem[Thomas and Brunskill(2016)]{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2139--2148, 2016.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the {L}asso.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[Uehara and Jiang(2019)]{uehara2019minimax}
Masatoshi Uehara and Nan Jiang.
\newblock Minimax weight and {Q}-function learning for off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:1910.12809}, 2019.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wainwright(2009)]{wainwright2009sharp}
Martin~J Wainwright.
\newblock Sharp thresholds for high-dimensional and noisy sparsity recovery
  using l1-constrained quadratic programming (lasso).
\newblock \emph{IEEE transactions on information theory}, 55\penalty0
  (5):\penalty0 2183--2202, 2009.

\bibitem[Wainwright(2019)]{Wa19}
Martin~J Wainwright.
\newblock \emph{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge University Press, 2019.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9665--9675, 2019.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pages
  6995--7004, 2019.

\bibitem[Yang and Wang(2020)]{yang2019reinforcement}
Lin~F Yang and Mengdi Wang.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \emph{International Conference on Machine Learning}, 2020.

\bibitem[Yang et~al.(2020)Yang, Nachum, Dai, Li, and Schuurmans]{yang2020off}
Mengjiao Yang, Ofir Nachum, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock \emph{arXiv preprint arXiv:2007.03438}, 2020.

\bibitem[Yin and Wang(2020)]{yin2020asymptotically}
Ming Yin and Yu-Xiang Wang.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2001.10742}, 2020.

\bibitem[Yuan and Lin(2006)]{yuan2006model}
Ming Yuan and Yi~Lin.
\newblock Model selection and estimation in regression with grouped variables.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 68\penalty0 (1):\penalty0 49--67, 2006.

\bibitem[Zanette et~al.(2020)Zanette, Brandfonbrener, Brunskill, Pirotta, and
  Lazaric]{zanette2020frequentist}
Andrea Zanette, David Brandfonbrener, Emma Brunskill, Matteo Pirotta, and
  Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1954--1964, 2020.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2020gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock {GenDICE}: Generalized offline estimation of stationary values.
\newblock \emph{arXiv preprint arXiv:2002.09072}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Shangtong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock {GradientDICE}: Rethinking generalized offline estimation of
  stationary values.
\newblock \emph{arXiv preprint arXiv:2001.11113}, 2020{\natexlab{b}}.

\bibitem[Zhao and Yu(2006)]{zhao2006model}
Peng Zhao and Bin Yu.
\newblock On model selection consistency of lasso.
\newblock \emph{Journal of Machine learning research}, 7\penalty0
  (Nov):\penalty0 2541--2563, 2006.

\bibitem[Zhou et~al.(2020)Zhou, He, and Gu]{zhou2020provably}
Dongruo Zhou, Jiafan He, and Quanquan Gu.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock \emph{arXiv preprint arXiv:2006.13165}, 2020.

\end{thebibliography}
