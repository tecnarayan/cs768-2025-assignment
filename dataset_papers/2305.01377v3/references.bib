@book{adlerRandomFieldsGeometry2007,
  title = {Random {{Fields}} and {{Geometry}}},
  author = {Adler, Robert J. and Taylor, Jonathan E.},
  date = {2007},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {Springer New York},
  location = {New York, NY},
  issn = {1439-7382},
  doi = {10.1007/978-0-387-48116-6},
  isbn = {978-0-387-48112-8},
  langid = {english},
  keywords = {Geometry,Mathematical Methods in Physics,Mathematics,Mathematics and Statistics,Probability Theory and Stochastic Processes,Statistics general},
  file = {/Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Applications of Random Fields and Geometry Foundations and Case Studies.pdf;/Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Random Fields and Geometry_old_notes.pdf;/Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Random Fields and Geometry.pdf}
}

@article{agnihotriExploringBayesianOptimization2020,
  title = {Exploring {{Bayesian Optimization}}},
  author = {Agnihotri, Apoorv and Batra, Nipun},
  date = {2020-05-05},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {5},
  number = {5},
  pages = {e26},
  issn = {2476-0757},
  doi = {10.23915/distill.00026},
  abstract = {How to tune hyperparameters for your machine learning model using Bayesian optimization.},
  langid = {english},
  file = {/Users/felix/Zotero/storage/XV7HBLLR/distill-bayesian-optimization.png;/Users/felix/Zotero/storage/YXIIIXCJ/bayesian-optimization.html}
}

@book{andersonIntroductionRandomMatrices2010,
  title = {An {{Introduction}} to {{Random Matrices}}},
  author = {Anderson, Greg W. and Guionnet, Alice and Zeitouni, Ofer},
  date = {2010},
  eprint = {wjHc45dVTpMC},
  eprinttype = {googlebooks},
  publisher = {Cambridge University Press},
  abstract = {The theory of random matrices plays an important role in many areas of pure mathematics and employs a variety of sophisticated mathematical tools (analytical, probabilistic and combinatorial). This diverse array of tools, while attesting to the vitality of the field, presents several formidable obstacles to the newcomer, and even the expert probabilist. This rigorous introduction to the basic theory is sufficiently self-contained to be accessible to graduate students in mathematics or related sciences, who have mastered probability theory at the graduate level, but have not necessarily been exposed to advanced notions of functional analysis, algebra or geometry. Useful background material is collected in the appendices and exercises are also included throughout to test the reader's understanding. Enumerative techniques, stochastic analysis, large deviations, concentration inequalities, disintegration and Lie algebras all are introduced in the text, which will enable readers to approach the research literature with confidence.},
  isbn = {978-0-521-19452-5},
  langid = {english},
  pagetotal = {507},
  keywords = {Mathematics / Algebra / General,Mathematics / Calculus,Mathematics / Probability & Statistics / General},
  file = {/Users/felix/paper/2010_Anderson et al/Anderson et al_2010_An Introduction to Random Matrices.pdf}
}

@online{anEnsembleSimpleConvolutional2020,
  title = {An {{Ensemble}} of {{Simple Convolutional Neural Network Models}} for {{MNIST Digit Recognition}}},
  author = {An, Sanghyeon and Lee, Minjun and Park, Sanglee and Yang, Heerin and So, Jungmin},
  date = {2020-10-04},
  doi = {10.48550/arXiv.2008.10400},
  abstract = {We report that a very high accuracy on the MNIST test set can be achieved by using simple convolutional neural network (CNN) models. We use three different models with 3x3, 5x5, and 7x7 kernel size in the convolution layers. Each model consists of a set of convolution layers followed by a single fully connected layer. Every convolution layer uses batch normalization and ReLU activation, and pooling is not used. Rotation and translation is used to augment training data, which is frequently used in most image classification tasks. A majority voting using the three models independently trained on the training data set can achieve up to 99.87\% accuracy on the test set, which is one of the state-of-the-art results. A two-layer ensemble, a heterogeneous ensemble of three homogeneous ensemble networks, can achieve up to 99.91\% test accuracy. The results can be reproduced by using the code at: https://github.com/ansh941/MnistSimpleCNN},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/felix/paper/2020_An et al/An et al_2020_An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit.pdf;/Users/felix/Zotero/storage/MMVJEDFF/2008.html}
}

@article{arousAgingSphericalSpin2001,
  title = {Aging of Spherical Spin Glasses},
  author = {Arous, G. Ben and Dembo, A. and Guionnet, A.},
  date = {2001-05-01},
  journaltitle = {Probability Theory and Related Fields},
  shortjournal = {Probab Theory Relat Fields},
  volume = {120},
  number = {1},
  pages = {1--67},
  issn = {1432-2064},
  doi = {10.1007/PL00008774},
  abstract = {Sompolinski and Zippelius (1981) propose the study of dynamical systems whose invariant measures are the Gibbs measures for (hard to analyze) statistical physics models of interest. In the course of doing so, physicists often report of an “aging” phenomenon. For example, aging is expected to happen for the Sherrington-Kirkpatrick model, a disordered mean-field model with a very complex phase transition in equilibrium at low temperature. We shall study the Langevin dynamics for a simplified spherical version of this model. The induced rotational symmetry of the spherical model reduces the dynamics in question to an N-dimensional coupled system of Ornstein-Uhlenbeck processes whose random drift parameters are the eigenvalues of certain random matrices. We obtain the limiting dynamics for N approaching infinity and by analyzing its long time behavior, explain what is aging (mathematically speaking), what causes this phenomenon, and what is its relationship with the phase transition of the corresponding equilibrium invariant measures.},
  langid = {english},
  keywords = {Key words or phrases: Interacting random processes – Disordered systems – Large deviations – Statistical mechanics – Langevin dynamics – Eigenvalues Random matrices,Mathematics Subject Classification (2000): 60H10 82B44 60F10 60K35 82C44 82C31 82C22 15A18 15A52},
  file = {/Users/felix/paper/2001_Arous et al/Arous et al_2001_Aging of spherical spin glasses.pdf}
}

@article{arousExponentialGrowthRandom2022,
  title = {Exponential Growth of Random Determinants beyond Invariance},
  author = {Arous, Gérard Ben and Bourgade, Paul and McKenna, Benjamin},
  date = {2022-12-31},
  journaltitle = {Probability and Mathematical Physics},
  shortjournal = {Prob. Math. Phys.},
  volume = {3},
  number = {4},
  eprint = {2105.05000},
  eprinttype = {arXiv},
  eprintclass = {math-ph},
  pages = {731--789},
  issn = {2690-1005, 2690-0998},
  doi = {10.2140/pmp.2022.3.731},
  abstract = {We give simple criteria to identify the exponential order of magnitude of the absolute value of the determinant for wide classes of random matrix models, not requiring the assumption of invariance. These include Gaussian matrices with covariance profiles, Wigner matrices and covariance matrices with subexponential tails, Erd\textbackslash H\{o\}s-R\textbackslash 'enyi and \$d\$-regular graphs for any polynomial sparsity parameter, and non-mean-field random matrix models, such as random band matrices for any polynomial bandwidth. The proof builds on recent tools, including the theory of the Matrix Dyson Equation as developed in [Ajanki, Erd\textbackslash H\{o\}s, Kr\textbackslash "uger 2019]. We use these asymptotics as an important input to identify the complexity of classes of Gaussian random landscapes in our companion papers [Ben Arous, Bourgade, McKenna 2021; McKenna 2021].},
  keywords = {60B20,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2022_Arous et al/Arous et al_2022_Exponential growth of random determinants beyond invariance.pdf;/Users/felix/Zotero/storage/73L2FDKP/2105.html}
}

@online{arousLandscapeComplexityInvariance2021,
  title = {Landscape Complexity beyond Invariance and the Elastic Manifold},
  author = {Arous, Gérard Ben and Bourgade, Paul and McKenna, Benjamin},
  date = {2021-05-11},
  eprint = {2105.05051},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, physics:math-ph},
  doi = {10.48550/arXiv.2105.05051},
  abstract = {This paper characterizes the annealed, topological complexity (both of total critical points and of local minima) of the elastic manifold. This classical model of a disordered elastic system captures point configurations with self-interactions in a random medium. We establish the simple-vs.-glassy phase diagram in the model parameters, with these phases separated by a physical boundary known as the Larkin mass, confirming formulas of Fyodorov and Le Doussal. One essential, dynamical, step of the proof also applies to a general signal-to-noise model of soft spins in an anisotropic well, for which we prove a negative-second-moment threshold distinguishing positive from zero complexity. A universal near-critical behavior appears within this phase portrait, namely quadratic near-critical vanishing of the complexity of total critical points, and cubic near-critical vanishing of the complexity of local minima. These two models serve as a paradigm of complexity calculations for Gaussian landscapes exhibiting few distributional symmetries, i.e. beyond the invariant setting. The two main inputs for the proof are determinant asymptotics for non-invariant random matrices from our companion paper [Ben Arous, Bourgade, McKenna 2021], and the atypical convexity and integrability of the limiting variational problems.},
  pubstate = {prepublished},
  keywords = {60B20 60G15 82B44,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2021_Arous et al/Arous et al_2021_Landscape complexity beyond invariance and the elastic manifold.pdf;/Users/felix/Zotero/storage/URQTNKCY/2105.html}
}

@article{arousLargeDeviationsCircular1998,
  title = {Large Deviations from the Circular Law},
  author = {Arous, Gérard Ben and Zeitouni, Ofer},
  date = {1998-01},
  journaltitle = {ESAIM: Probability and Statistics},
  volume = {2},
  pages = {123--134},
  publisher = {EDP Sciences},
  issn = {1292-8100, 1262-3318},
  doi = {10.1051/ps:1998104},
  abstract = {We prove a full large deviations principle, in the scale N2,  for the empirical measure of the eigenvalues of an N x N  (non self-adjoint) matrix composed of i.i.d. zero mean random  variables with variance N-1. The (good) rate function which  governs this rate function possesses as unique minimizer the  circular law, providing an alternative proof of convergence to  the latter. The techniques are related to recent work by Ben  Arous and Guionnet, who treat the self-adjoint case. A crucial  role is played by precise determinant computations due to Edelman  and to Lehmann and Sommers.},
  langid = {english},
  keywords = {Large deviations / circular law / non commutative entropy.},
  file = {/Users/felix/paper/1998_Arous_Zeitouni/Arous_Zeitouni_1998_Large deviations from the circular law.pdf}
}

@article{arousLargeDeviationsWigner1997,
  title = {Large Deviations for {{Wigner}}'s Law and {{Voiculescu}}'s Non-Commutative Entropy},
  author = {Arous, Gérard Ben and Guionnet, A.},
  date = {1997-08-01},
  journaltitle = {Probability Theory and Related Fields},
  shortjournal = {Probab Theory Relat Fields},
  volume = {108},
  number = {4},
  pages = {517--542},
  issn = {1432-2064},
  doi = {10.1007/s004400050119},
  abstract = {We study the spectral measure of Gaussian Wigner's matrices and prove that it satisfies a large deviation principle. We show that the good rate function which governs this principle achieves its minimum value at Wigner's semicircular law, which entails the convergence of the spectral measure to the semicircular law. As a conclusion, we give some further examples of random matrices with spectral measure satisfying a large deviation principle and argue about Voiculescu's non commutative entropy.},
  langid = {english},
  keywords = {15A18,15A52,Mathematics Subject of Classification: 60F10},
  file = {/Users/felix/paper/1997_Arous_Guionnet/Arous_Guionnet_1997_Large deviations for Wigner's law and Voiculescu's non-commutative entropy.pdf}
}

@inreference{askeyGautschiInequality,
  title = {Gautschi’s {{Inequality}}},
  booktitle = {Digital {{Library}} of {{Mathematical Functions}} ({{DLMF}})},
  author = {Askey, Richard A. and Roy, Ranjan},
  url = {http://dlmf.nist.gov/5.6.E4},
  urldate = {2024-03-15}
}

@article{auffingerComplexityGaussianRandom2023,
  title = {Complexity of {{Gaussian Random Fields}} with {{Isotropic Increments}}},
  author = {Auffinger, Antonio and Zeng, Qiang},
  date = {2023-08-01},
  journaltitle = {Communications in Mathematical Physics},
  shortjournal = {Commun. Math. Phys.},
  volume = {402},
  number = {1},
  pages = {951--993},
  issn = {1432-0916},
  doi = {10.1007/s00220-023-04739-0},
  abstract = {We study the energy landscape of a model of a single particle on a random potential, that is, we investigate the topology of level sets of smooth random fields on \$\$\{\textbackslash mathbb \{R\}\}\textasciicircum\{N\}\$\$of the form \$\$X\_N(x) +\textbackslash frac\{\textbackslash mu \}\{2\} \textbackslash Vert x\textbackslash Vert \textasciicircum 2,\$\$where \$\$X\_\{N\}\$\$is a Gaussian process with isotropic increments. We derive asymptotic formulas for the mean number of critical points with critical values in an open set as the dimension N goes to infinity. In a companion paper, we provide the same analysis for the number of critical points with a given index.},
  langid = {english},
  file = {/Users/felix/paper/2023_Auffinger_Zeng/Auffinger_Zeng_2023_Complexity of Gaussian Random Fields with Isotropic Increments.pdf}
}

@article{auffingerComplexityRandomSmooth2013,
  title = {Complexity of Random Smooth Functions on the High-Dimensional Sphere},
  author = {Auffinger, Antonio and Arous, Gerard Ben},
  date = {2013-11},
  journaltitle = {The Annals of Probability},
  volume = {41},
  number = {6},
  pages = {4214--4247},
  publisher = {Institute of Mathematical Statistics},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/13-AOP862},
  abstract = {We analyze the landscape of general smooth Gaussian functions on the sphere in dimension \$N\$, when \$N\$ is large. We give an explicit formula for the asymptotic complexity of the mean number of critical points of finite and diverging index at any level of energy and for the mean Euler characteristic of level sets. We then find two possible scenarios for the bottom landscape, one that has a layered structure of critical values and a strong correlation between indexes and critical values and another where even at levels below the limiting ground state energy the mean number of local minima is exponentially large. We end the paper by discussing how these results can be interpreted in the language of spin glasses models.},
  keywords = {15A52,60G60,82D30,critical points,Parisi formula,random matrices,Sample,Spin glasses},
  file = {/Users/felix/paper/2013_Auffinger_Arous/Auffinger_Arous_2013_Complexity of random smooth functions on the high-dimensional sphere.pdf}
}

@online{auffingerNumberSaddlesSpherical2020,
  title = {The Number of Saddles of the Spherical \$p\$-Spin Model},
  author = {Auffinger, Antonio and Gold, Julian},
  date = {2020-07-17},
  eprint = {2007.09269},
  eprinttype = {arXiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/2007.09269},
  urldate = {2023-06-15},
  abstract = {We show that the quenched complexity of saddles of the spherical pure \$p\$-spin model agrees with the annealed complexity when both are positive. Precisely, we show that the second moment of the number of critical values of a given finite index in a given interval has twice the growth rate of the first moment.},
  pubstate = {prepublished},
  version = {1},
  keywords = {60F10 82D30,Mathematics - Probability},
  file = {/Users/felix/paper/2020_Auffinger_Gold/Auffinger_Gold_2020_The number of saddles of the spherical $p$-spin model.pdf;/Users/felix/Zotero/storage/I9VYX3Z4/2007.html}
}

@incollection{auffingerOptimizationRandomHighDimensional2023,
  title = {Optimization of {{Random High-Dimensional Functions}}: {{Structure}} and {{Algorithms}}},
  shorttitle = {Optimization of {{Random High-Dimensional Functions}}},
  booktitle = {Spin {{Glass Theory}} and {{Far Beyond}}},
  author = {Auffinger, Antonio and Montanari, Andrea and Subag, Eliran},
  date = {2023-02-09},
  pages = {609--633},
  publisher = {WORLD SCIENTIFIC},
  location = {5 Toh Tuck Link, Singapore},
  doi = {10.1142/9789811273926_0029},
  abstract = {Replica symmetry breaking postulates that near optima of spin glass Hamiltonians have an ultrametric structure. Namely, near optima can be associated to leaves of a tree, and the Euclidean distance between them corresponds to the distance along this tree. We survey recent progress towards a rigorous proof of this picture in the context of mixed \$p\$-spin spin glass models. We focus in particular on the following topics: \$(i)\$\textasciitilde The structure of critical points of the Hamiltonian; \$(ii)\$\textasciitilde The realization of the ultrametric tree as near optima of a suitable TAP free energy; \$(iii)\$\textasciitilde The construction of efficient optimization algorithm that exploits this picture.},
  isbn = {9789811273919},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Probability},
  file = {/Users/felix/paper/2023_Auffinger et al/Auffinger et al_2023_Optimization of Random High-Dimensional Functions.pdf;/Users/felix/Zotero/storage/LUIYQ2E7/2206.html}
}

@article{auffingerParisiFormulaGround2017,
  title = {Parisi Formula for the Ground State Energy in the Mixed \$p\$-Spin Model},
  author = {Auffinger, Antonio and Chen, Wei-Kuo},
  date = {2017-11},
  journaltitle = {The Annals of Probability},
  volume = {45},
  pages = {4617--4631},
  publisher = {Institute of Mathematical Statistics},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/16-AOP1173},
  abstract = {We show that the thermodynamic limit of the ground state energy in the mixed \$p\$-spin model can be identified as a variational problem. This gives a natural generalization of the Parisi formula at zero temperature.},
  issue = {6B},
  keywords = {60K35,Ground state energy,Parisi formula,Spin glasses},
  file = {/Users/felix/paper/2017_Auffinger_Chen/Auffinger_Chen_2017_Parisi formula for the ground state energy in the mixed $p$-spin model.pdf}
}

@article{auffingerRandomMatricesComplexity2013,
  title = {Random {{Matrices}} and {{Complexity}} of {{Spin Glasses}}},
  author = {Auffinger, Antonio and Arous, Gérard Ben and Černý, Jiří},
  date = {2013},
  journaltitle = {Communications on Pure and Applied Mathematics},
  volume = {66},
  number = {2},
  pages = {165--201},
  issn = {1097-0312},
  doi = {10.1002/cpa.21422},
  abstract = {We give an asymptotic evaluation of the complexity of spherical p-spin spin glass models via random matrix theory. This study enables us to obtain detailed information about the bottom of the energy landscape, including the absolute minimum (the ground state), and the other local minima, and describe an interesting layered structure of the low critical values for the Hamiltonians of these models. We also show that our approach allows us to compute the related TAPcomplexity and extend the results known in the physics literature. As an independent tool, we prove a large deviation principle for the kth-largest eigenvalue of the Gaussian orthogonal ensemble, extending the results of Ben Arous, Dembo, and Guionnet. © 2012 Wiley Periodicals, Inc.},
  langid = {english},
  file = {/Users/felix/paper/2013_Auffinger et al/Auffinger et al_2013_Random Matrices and Complexity of Spin Glasses.pdf;/Users/felix/paper/2013_Auffinger et al/Auffinger et al_2013_Random Matrices and Complexity of Spin Glasses2.pdf;/Users/felix/Zotero/storage/AYD3PWWD/cpa.html}
}

@book{azaisLevelSetsExtrema2009,
  title = {Level {{Sets}} and {{Extrema}} of {{Random Processes}} and {{Fields}}},
  author = {Azais, Jean-Marc and Wschebor, Mario},
  date = {2009-02-17},
  eprint = {vF36BsG32CEC},
  eprinttype = {googlebooks},
  publisher = {John Wiley \& Sons},
  abstract = {A timely and comprehensive treatment of random field theory with applications across diverse areas of study Level Sets and Extrema of Random Processes and Fields discusses how to understand the properties of the level sets of paths as well as how to compute the probability distribution of its extremal values, which are two general classes of problems that arise in the study of random processes and fields and in related applications. This book provides a unified and accessible approach to these two topics and their relationship to classical theory and Gaussian processes and fields, and the most modern research findings are also discussed. The authors begin with an introduction to the basic concepts of stochastic processes, including a modern review of Gaussian fields and their classical inequalities. Subsequent chapters are devoted to Rice formulas, regularity properties, and recent results on the tails of the distribution of the maximum. Finally, applications of random fields to various areas of mathematics are provided, specifically to systems of random equations and condition numbers of random matrices. Throughout the book, applications are illustrated from various areas of study such as statistics, genomics, and oceanography while other results are relevant to econometrics, engineering, and mathematical physics. The presented material is reinforced by end-of-chapter exercises that range in varying degrees of difficulty. Most fundamental topics are addressed in the book, and an extensive, up-to-date bibliography directs readers to existing literature for further study. Level Sets and Extrema of Random Processes and Fields is an excellent book for courses on probability theory, spatial statistics, Gaussian fields, and probabilistic methods in real computation at the upper-undergraduate and graduate levels. It is also a valuable reference for professionals in mathematics and applied fields such as statistics, engineering, econometrics, mathematical physics, and biology.},
  isbn = {978-0-470-43463-5},
  langid = {english},
  pagetotal = {407},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  file = {/Users/felix/paper/2009_Azais_Wschebor/Azais_Wschebor_2009_Level Sets and Extrema of Random Processes and Fields.pdf}
}

@article{bahriStatisticalMechanicsDeep2020,
  title = {Statistical {{Mechanics}} of {{Deep Learning}}},
  author = {Bahri, Yasaman and Kadmon, Jonathan and Pennington, Jeffrey and Schoenholz, Sam S. and Sohl-Dickstein, Jascha and Ganguli, Surya},
  date = {2020},
  journaltitle = {Annual Review of Condensed Matter Physics},
  volume = {11},
  number = {1},
  pages = {501--528},
  doi = {10.1146/annurev-conmatphys-031119-050745},
  abstract = {The recent striking success of deep neural networks in machine learning raises profound questions about the theoretical principles underlying their success. For example, what can such deep networks compute? How can we train them? How does information propagate through them? Why can they generalize? And how can we teach them to imagine? We review recent work in which methods of physical analysis rooted in statistical mechanics have begun to provide conceptual insights into these questions. These insights yield connections between deep learning and diverse physical and mathematical topics, including random landscapes, spin glasses, jamming, dynamical phase transitions, chaos, Riemannian geometry, random matrix theory, free probability, and nonequilibrium statistical mechanics. Indeed, the fields of statistical mechanics and machine learning have long enjoyed a rich history of strongly coupled interactions, and recent advances at the intersection of statistical mechanics and deep learning suggest these interactions will only deepen going forward.},
  keywords = {chaos,dynamical phase transitions,interacting particle systems,jamming,machine learning,neural networks,nonequilibrium statistical mechanics,random matrix theory,spin glasses},
  file = {/Users/felix/paper/2020_Bahri et al/Bahri et al_2020_Statistical Mechanics of Deep Learning.pdf}
}

@article{baldiNeuralNetworksPrincipal1989,
  title = {Neural Networks and Principal Component Analysis: {{Learning}} from Examples without Local Minima},
  shorttitle = {Neural Networks and Principal Component Analysis},
  author = {Baldi, Pierre and Hornik, Kurt},
  date = {1989-01-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {2},
  number = {1},
  pages = {53--58},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90014-2},
  abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  langid = {english},
  keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
  file = {/Users/felix/paper/1989_Baldi_Hornik/Baldi_Hornik_1989_Neural networks and principal component analysis.pdf;/Users/felix/Zotero/storage/PAYJZGVA/0893608089900142.html}
}

@inproceedings{barbierOptimalErrorsPhase2018,
  title = {Optimal {{Errors}} and {{Phase Transitions}} in {{High-Dimensional Generalized Linear Models}}},
  booktitle = {Proceedings of the 31st  {{Conference On Learning Theory}}},
  author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, Léo and Zdeborová, Lenka},
  date = {2018-07-03},
  pages = {728--731},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v75/barbier18a.html},
  urldate = {2023-12-21},
  abstract = {Generalized linear models (GLMs) arise in high-dimensional machine learning, statistics, communications and signal processing. \% In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes or benchmarks models in neural networks. \% We evaluate the mutual information (or “free entropy”) from which we deduce the Bayes-optimal inference and generalization errors.  Our analysis applies to the high-dimensional limit where both the number of samples and dimensions are large and their ratio is fixed. \% Non-rigorous predictions for the optimal inference and generalization errors existed for special cases of GLMs, e.g. for the perceptron in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. \% Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance, and locate the associated sharp phase transitions separating learnable and non-learnable regions.},
  eventtitle = {Conference {{On Learning Theory}}},
  langid = {english},
  file = {/Users/felix/paper/2018_Barbier et al/Barbier et al_2018_Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear.pdf}
}

@article{barbierOptimalErrorsPhase2019,
  title = {Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models},
  author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, Léo and Zdeborová, Lenka},
  date = {2019-03-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {12},
  pages = {5451--5460},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1802705116},
  abstract = {Generalized linear models (GLMs) are used in high-dimensional machine learning, statistics, communications, and signal processing. In this paper we analyze GLMs when the data matrix is random, as relevant in problems such as compressed sensing, error-correcting codes, or benchmark models in neural networks. We evaluate the mutual information (or “free entropy”) from which we deduce the Bayes-optimal estimation and generalization errors. Our analysis applies to the high-dimensional limit where both the number of samples and the dimension are large and their ratio is fixed. Nonrigorous predictions for the optimal errors existed for special cases of GLMs, e.g., for the perceptron, in the field of statistical physics based on the so-called replica method. Our present paper rigorously establishes those decades-old conjectures and brings forward their algorithmic interpretation in terms of performance of the generalized approximate message-passing algorithm. Furthermore, we tightly characterize, for many learning problems, regions of parameters for which this algorithm achieves the optimal performance and locate the associated sharp phase transitions separating learnable and nonlearnable regions. We believe that this random version of GLMs can serve as a challenging benchmark for multipurpose algorithms.},
  file = {/Users/felix/paper/2019_Barbier et al/Barbier et al_2019_Optimal errors and phase transitions in high-dimensional generalized linear.pdf}
}

@thesis{baskervilleRandomMatrixTheory2023,
  type = {phdthesis},
  title = {Random Matrix Theory and the Loss Surfaces of Neural Networks},
  author = {Baskerville, Nicholas P.},
  date = {2023-06-03},
  eprint = {2306.02108},
  eprinttype = {arXiv},
  eprintclass = {math-ph},
  institution = {University of Bristol},
  url = {http://arxiv.org/abs/2306.02108},
  urldate = {2023-07-10},
  abstract = {Neural network models are one of the most successful approaches to machine learning, enjoying an enormous amount of development and research over recent years and finding concrete real-world applications in almost any conceivable area of science, engineering and modern life in general. The theoretical understanding of neural networks trails significantly behind their practical success and the engineering heuristics that have grown up around them. Random matrix theory provides a rich framework of tools with which aspects of neural network phenomenology can be explored theoretically. In this thesis, we establish significant extensions of prior work using random matrix theory to understand and describe the loss surfaces of large neural networks, particularly generalising to different architectures. Informed by the historical applications of random matrix theory in physics and elsewhere, we establish the presence of local random matrix universality in real neural networks and then utilise this as a modeling assumption to derive powerful and novel results about the Hessians of neural network loss surfaces and their spectra. In addition to these major contributions, we make use of random matrix models for neural network loss surfaces to shed light on modern neural network training approaches and even to derive a novel and effective variant of a popular optimisation algorithm. Overall, this thesis provides important contributions to cement the place of random matrix theory in the theoretical study of modern neural networks, reveals some of the limits of existing approaches and begins the study of an entirely new role for random matrix theory in the theory of deep learning with important experimental discoveries and novel theoretical results based on local random matrix universality.},
  pagetotal = {320},
  keywords = {Computer Science - Machine Learning,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2023_Baskerville/Baskerville_2023_Random matrix theory and the loss surfaces of neural networks.pdf;/Users/felix/Zotero/storage/QJRARCVH/2306.html}
}

@article{bassettMaximumPosterioriEstimators2019,
  title = {Maximum a Posteriori Estimators as a Limit of {{Bayes}} Estimators},
  author = {Bassett, Robert and Deride, Julio},
  date = {2019-03-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Math. Program.},
  volume = {174},
  number = {1},
  pages = {129--144},
  issn = {1436-4646},
  doi = {10.1007/s10107-018-1241-0},
  abstract = {Maximum a posteriori and Bayes estimators are two common methods of point estimation in Bayesian statistics. It is commonly accepted that maximum a posteriori estimators are a limiting case of Bayes estimators with 0–1 loss. In this paper, we provide a counterexample which shows that in general this claim is false. We then correct the claim that by providing a level-set condition for posterior densities such that the result holds. Since both estimators are defined in terms of optimization problems, the tools of variational analysis find a natural application to Bayesian point estimation.},
  langid = {english},
  keywords = {62C10,62F10,62F15,65K10},
  file = {/Users/felix/paper/2019_Bassett_Deride/Bassett_Deride_2019_Maximum a posteriori estimators as a limit of Bayes estimators.pdf}
}

@article{benarousCountingEquilibriaLarge2021,
  title = {Counting Equilibria of Large Complex Systems by Instability Index},
  author = {Ben Arous, Gérard and Fyodorov, Yan V. and Khoruzhenko, Boris A.},
  date = {2021-08-24},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {34},
  eprint = {2008.00690},
  eprinttype = {arXiv},
  eprintclass = {math-ph},
  pages = {e2023719118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2023719118},
  abstract = {We consider a nonlinear autonomous system of 𝑁≫1 degrees of freedom randomly coupled by both relaxational (“gradient”) and nonrelaxational (“solenoidal”) random interactions. We show that with increased interaction strength, such systems generically undergo an abrupt transition from a trivial phase portrait with a single stable equilibrium into a topologically nontrivial regime of “absolute instability” where equilibria are on average exponentially abundant, but typically, all of them are unstable, unless the dynamics is purely gradient. When interactions increase even further, the stable equilibria eventually become on average exponentially abundant unless the interaction is purely solenoidal. We further calculate the mean proportion of equilibria that have a fixed fraction of unstable directions.},
  file = {/Users/felix/paper/2021_Ben Arous et al/Ben Arous et al_2021_Counting equilibria of large complex systems by instability index.pdf}
}

@online{benningGradientSpanAlgorithms2024,
  title = {Gradient {{Span Algorithms Make Predictable Progress}} in {{High Dimension}}},
  author = {Benning, Felix and Döring, Leif},
  date = {2024-10-13},
  doi = {10.48550/arXiv.2410.09973},
  abstract = {We prove that all 'gradient span algorithms' have asymptotically deterministic behavior on scaled Gaussian random functions as the dimension tends to infinity. In particular, this result explains the counterintuitive phenomenon that different training runs of many large machine learning models result in approximately equal cost curves despite random initialization on a complicated non-convex landscape. The distributional assumption of (non-stationary) isotropic Gaussian random functions we use is sufficiently general to serve as realistic model for machine learning training but also encompass spin glasses and random quadratic functions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Probability,Statistics - Machine Learning},
  file = {/Users/felix/paper/2024_Benning_Döring/Benning_Döring_2024_Gradient Span Algorithms Make Predictable Progress in High Dimension.pdf;/Users/felix/Zotero/storage/B2SP78C5/2410.html}
}

@inproceedings{benningRandomFunctionDescent2024,
  title = {Random {{Function Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Benning, Felix and Döring, Leif},
  date = {2024-12},
  volume = {37},
  eprint = {2305.01377},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  publisher = {Curran Associates, Inc.},
  location = {Vancouver, Canada},
  abstract = {Classical worst-case optimization theory neither explains the success of optimization in machine learning, nor does it help with step size selection. In this paper we demonstrate the viability and advantages of replacing the classical ``convex function'' framework with a ``random function'' framework. With complexity \textbackslash (\textbackslash bigO(n\textasciicircum 3\textbackslash dims\textasciicircum 3)\textbackslash ), where \textbackslash (n\textbackslash ) is the number of steps and \textbackslash (\textbackslash dims\textbackslash ) the number of dimensions, Bayesian optimization with gradients has not been viable in large dimension so far. By bridging the gap between Bayesian optimization (i.e. random function optimization theory) and classical optimization we establish viability. Specifically, we use a `stochastic Taylor approximation' to rediscover gradient descent, which is scalable in high dimension due to \textbackslash (\textbackslash bigO(n\textbackslash dims)\textbackslash ) complexity. This rediscovery yields a specific step size schedule we call Random Function Descent (RFD). The advantage of this random function framework is that RFD is scale invariant and that it provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup.},
  eventtitle = {{{NeurIPS}}},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/paper/2024_Benning_Döring/Benning_Döring_2024_Random Function Descent.pdf;/Users/felix/Zotero/storage/SR3IPX7X/2305.html}
}

@book{bergerStatisticalDecisionTheory1985,
  title = {Statistical {{Decision Theory}} and {{Bayesian Analysis}}},
  author = {Berger, James O.},
  date = {1985},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4757-4286-2},
  isbn = {978-1-4419-3074-3 978-1-4757-4286-2},
  keywords = {Analysis,Bayessches Verfahren,decision theory,Entscheidung (Math.),Excel},
  file = {/Users/felix/paper/1985_Berger/Berger_1985_Statistical Decision Theory and Bayesian Analysis.pdf}
}

@article{berkenkampNoRegretBayesianOptimization2019,
  title = {No-{{Regret Bayesian Optimization}} with {{Unknown Hyperparameters}}},
  author = {Berkenkamp, Felix and Schoellig, Angela P. and Krause, Andreas},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {50},
  pages = {1--24},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/18-213.html},
  urldate = {2024-08-15},
  abstract = {Bayesian optimization (BO) based on Gaussian process models is a powerful paradigm to optimize black-box functions that are expensive to evaluate. While several BO algorithms provably converge to the global optimum of the unknown function, they assume that the hyperparameters of the kernel are known in advance. This is not the case in practice and misspecification often causes these algorithms to converge to poor local optima. In this paper, we present the first BO algorithm that is provably no-regret and converges to the optimum without knowledge of the hyperparameters. During optimization we slowly adapt the hyperparameters of stationary kernels and thereby expand the associated function class over time, so that the BO algorithm considers more complex function candidates. Based on the theoretical insights, we propose several practical algorithms that achieve the empirical sample efficiency of BO with online hyperparameter estimation, but retain theoretical convergence guarantees. We evaluate our method on several benchmark problems.},
  file = {/Users/felix/paper/2019_Berkenkamp et al/Berkenkamp et al_2019_No-Regret Bayesian Optimization with Unknown Hyperparameters.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  date = {2017-01-01},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  volume = {59},
  number = {1},
  pages = {65--98},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be “laws of nature" by practitioners of numerical computing: \textbackslash beginlist \textbackslash item High-level dynamic programs have to be slow. \textbackslash item One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.},
  keywords = {65Y05,68N15,97P40,Julia,numerical,parallel,scientific computing},
  file = {/Users/felix/paper/2017_Bezanson et al/Bezanson et al_2017_Julia.pdf}
}

@book{borgwardtSimplexMethodProbabilistic1986,
  title = {The {{Simplex Method}}: {{A Probabilistic Analysis}}},
  shorttitle = {The {{Simplex Method}}},
  author = {Borgwardt, Karl Heinz},
  date = {1986-11-01},
  edition = {Softcover reprint of the original 1st ed. 1987 edition},
  publisher = {Springer},
  location = {Berlin Heidelberg},
  abstract = {For more than 35 years now, George B. Dantzig's Simplex-Method has been the most efficient mathematical tool for solving linear programming problems. It is proba­ bly that mathematical algorithm for which the most computation time on computers is spent. This fact explains the great interest of experts and of the public to understand the method and its efficiency. But there are linear programming problems which will not be solved by a given variant of the Simplex-Method in an acceptable time. The discrepancy between this (negative) theoretical result and the good practical behaviour of the method has caused a great fascination for many years. While the "worst-case analysis" of some variants of the method shows that this is not a "good" algorithm in the usual sense of complexity theory, it seems to be useful to apply other criteria for a judgement concerning the quality of the algorithm. One of these criteria is the average computation time, which amounts to an anal­ ysis of the average number of elementary arithmetic computations and of the number of pivot steps. A rigid analysis of the average behaviour may be very helpful for the decision which algorithm and which variant shall be used in practical applications. The subject and purpose of this book is to explain the great efficiency in prac­ tice by assuming certain distributions on the "real-world" -problems. Other stochastic models are realistic as well and so this analysis should be considered as one of many possibilities.},
  isbn = {978-3-540-17096-9},
  langid = {english},
  pagetotal = {282},
  file = {/Users/felix/paper/1986_Borgwardt/Borgwardt_1986_The Simplex Method.pdf}
}

@article{borotLargeDeviationsMaximal2011,
  title = {Large Deviations of the Maximal Eigenvalue of Random Matrices},
  author = {Borot, G. and Eynard, B. and Majumdar, S. N. and Nadal, C.},
  date = {2011-11},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2011},
  number = {11},
  pages = {P11024},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2011/11/P11024},
  abstract = {We present detailed computations of the nondecaying terms (three dominant orders) of the free energy in a one-cut matrix model with a hard edge a, in β ensembles, with any polynomial potential. β {$>$} 0 is not restricted to the standard values β = 1 (Hermitian matrices), β = 1/2 (symmetric matrices) and β = 2 (quaternionic self-dual matrices). This model allows us to study the statistics of the maximum eigenvalue of random matrices. We compute the large deviation function to the left of the expected maximum. We specialize our results to the Gaussian β ensembles and check them numerically. Our method is based on general results and procedures already developed in the literature to solve the Pastur equations (also called ‘loop equations’). It allows us to compute the left tail of the analog of Tracy–Widom laws for any β, including the constant term.},
  langid = {english},
  file = {/Users/felix/paper/2011_Borot et al/Borot et al_2011_Large deviations of the maximal eigenvalue of random matrices.pdf}
}

@online{bradleyMatrixFreeApproximateEquilibration2012,
  title = {Matrix-{{Free Approximate Equilibration}}},
  author = {Bradley, Andrew M. and Murray, Walter},
  date = {2012-06-19},
  eprint = {1110.2805},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1110.2805},
  abstract = {The condition number of a diagonally scaled matrix, for appropriately chosen scaling matrices, is often less than that of the original. Equilibration scales a matrix so that the scaled matrix's row and column norms are equal. Scaling can be approximate. We develop approximate equilibration algorithms for nonsymmetric and symmetric matrices having signed elements that access a matrix only by matrix-vector products.},
  pubstate = {prepublished},
  keywords = {15A12 15B51 65F35,Mathematics - Numerical Analysis},
  file = {/Users/felix/paper/2012_Bradley_Murray/Bradley_Murray_2012_Matrix-Free Approximate Equilibration.pdf;/Users/felix/Zotero/storage/SW4JY8E2/1110.html}
}

@article{brayStatisticsCriticalPoints2007,
  title = {The Statistics of Critical Points of {{Gaussian}} Fields on Large-Dimensional Spaces},
  author = {Bray, Alan J. and Dean, David S.},
  date = {2007-04-10},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {98},
  number = {15},
  eprint = {cond-mat/0611023},
  eprinttype = {arXiv},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.98.150201},
  abstract = {We calculate the average number of critical points of a Gaussian field on a high-dimensional space as a function of their energy and their index. Our results give a complete picture of the organization of critical points and are of relevance to glassy and disordered systems, and to landscape scenarios coming from the anthropic approach to string theory.},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/felix/paper/2007_Bray_Dean/[journal] Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces.pdf;/Users/felix/paper/2007_Bray_Dean/Bray_Dean_2007_The statistics of critical points of Gaussian fields on large-dimensional spaces.pdf;/Users/felix/Zotero/storage/RV2A6TST/0611023.html}
}

@inproceedings{brutzkusGloballyOptimalGradient2017,
  title = {Globally {{Optimal Gradient Descent}} for a {{ConvNet}} with {{Gaussian Inputs}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Brutzkus, Alon and Globerson, Amir},
  date = {2017-07-17},
  eprint = {1702.07966},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  pages = {605--614},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/brutzkus17a.html},
  urldate = {2023-10-16},
  abstract = {Deep learning models are often successfully trained using gradient descent, despite the worst case hardness of the underlying non-convex optimization problem. The key question is then under what conditions can one prove that optimization will succeed. Here we provide a strong result of this kind. We consider a neural net with one hidden layer and a convolutional structure with no overlap and a ReLU activation function. For this architecture we show that learning is NP-complete in the general case, but that when the input distribution is Gaussian, gradient descent converges to the global optimum in polynomial time. To the best of our knowledge, this is the first global optimality guarantee of gradient descent on a convolutional neural network with ReLU activations.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2017_Brutzkus_Globerson/[Supplemental] Brutzkus_Globerson_2017_Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs.pdf;/Users/felix/paper/2017_Brutzkus_Globerson/Brutzkus_Globerson_2017_Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs.pdf}
}

@unpublished{bubeckGeometricAlternativeNesterov2015,
  title = {A Geometric Alternative to {{Nesterov}}'s Accelerated Gradient Descent},
  author = {Bubeck, Sébastien and Lee, Yin Tat and Singh, Mohit},
  date = {2015-06-26},
  eprint = {1506.08187},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/1506.08187},
  urldate = {2021-07-07},
  abstract = {We propose a new method for unconstrained optimization of a smooth and strongly convex function, which attains the optimal rate of convergence of Nesterov’s accelerated gradient descent. The new algorithm has a simple geometric interpretation, loosely inspired by the ellipsoid method. We provide some numerical evidence that the new method can be superior to Nesterov’s accelerated gradient descent.},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/Users/felix/paper/2015_Bubeck et al/Bubeck et al_2015_A geometric alternative to Nesterov's accelerated gradient descent.pdf}
}

@inproceedings{bubeckUniversalLawRobustness2021,
  title = {A {{Universal Law}} of {{Robustness}} via {{Isoperimetry}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bubeck, Sebastien and Sellke, Mark},
  date = {2021},
  volume = {34},
  eprint = {2105.12806},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {28811--28822},
  publisher = {Curran Associates, Inc.},
  location = {Virtual Event},
  url = {https://proceedings.neurips.cc/paper/2021/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html},
  urldate = {2023-09-22},
  file = {/Users/felix/paper/2021_Bubeck_Sellke/[Arxiv] Bubeck_Sellke_2021_A Universal Law of Robustness via Isoperimetry.pdf;/Users/felix/paper/2021_Bubeck_Sellke/[Supplemental] Bubeck_Sellke_2021_A Universal Law of Robustness via Isoperimetry.pdf;/Users/felix/paper/2021_Bubeck_Sellke/Bubeck_Sellke_2021_A Universal Law of Robustness via Isoperimetry.pdf}
}

@online{bulatovAnswerItPossible2023,
  title = {Answer to "{{Is}} It Possible to Use the {{Laplace Transform}} to Calculate Eigenvalues?"},
  shorttitle = {Answer to "{{Is}} It Possible to Use the {{Laplace Transform}} to Calculate Eigenvalues?},
  author = {Bulatov, Yaroslav},
  date = {2023-03-28},
  url = {https://mathoverflow.net/a/443644/122659},
  urldate = {2023-03-28},
  organization = {MathOverflow},
  file = {/Users/felix/Zotero/storage/VXEMJTVX/443644.html}
}

@article{bullConvergenceRatesEfficient2011,
  title = {Convergence {{Rates}} of {{Efficient Global Optimization Algorithms}}},
  author = {Bull, Adam D.},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {88},
  pages = {2879--2904},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v12/bull11a.html},
  urldate = {2024-07-23},
  abstract = {In the efficient global optimization problem, we minimize an unknown function f, using as few observations f(x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour. Implementing these algorithms requires a choice of Gaussian-process prior, which determines an associated space of functions, its reproducing-kernel Hilbert space (RKHS). When the prior is fixed, expected improvement is known to converge on the minimum of any function in its RKHS. We provide convergence rates for this procedure, optimal for functions of low smoothness, and describe a modified algorithm attaining optimal rates for smoother functions. In practice, however, priors are typically estimated sequentially from the data. For standard estimators, we show this procedure may never find the minimum of f. We then propose alternative estimators, chosen to minimize the constants in the rate of convergence, and show these estimators retain the convergence rates of a fixed prior.},
  file = {/Users/felix/paper/2011_Bull/Bull_2011_Convergence Rates of Efficient Global Optimization Algorithms.pdf}
}

@book{charbonneauSpinGlassTheory2023,
  title = {Spin {{Glass Theory And Far Beyond}}: {{Replica Symmetry Breaking After}} 40 {{Years}}},
  shorttitle = {Spin {{Glass Theory And Far Beyond}}},
  author = {Charbonneau, Patrick and Marinari, Enzo and Parisi, Giorgio and Ricci-tersenghi, Federico and Sicuro, Gabriele and Zamponi, Francesco and Mezard, Marc},
  date = {2023-07-26},
  eprint = {gIrVEAAAQBAJ,},
  eprinttype = {googlebooks},
  publisher = {World Scientific},
  location = {5 Toh Tuck Link, Singapore},
  doi = {10.1142/13341},
  abstract = {About sixty years ago, the anomalous magnetic response of certain magnetic alloys drew the attention of theoretical physicists. It soon became clear that understanding these systems, now called spin glasses, would give rise to a new branch of statistical physics. As physical materials, spin glasses were found to be as useless as they were exotic. They have nevertheless been recognized as paradigmatic examples of complex systems with applications to problems as diverse as neural networks, amorphous solids, biological molecules, social and economic interactions, information theory and constraint satisfaction problems.This book presents an encyclopaedic overview of the broad range of these applications. More than 30 contributions are compiled, written by many of the leading researchers who have contributed to these developments over the last few decades. Some timely and cutting-edge applications are also discussed. This collection serves well as an introduction and summary of disordered and glassy systems for advanced undergraduates, graduate students and practitioners interested in the topic.},
  isbn = {9789811273933},
  langid = {english},
  pagetotal = {740},
  keywords = {Science / Mechanics / Thermodynamics,Science / Physics / Condensed Matter,Science / Physics / General,Science / Physics / Mathematical & Computational},
  file = {/Users/felix/paper/2023_Charbonneau et al/Charbonneau et al_2023_Spin Glass Theory And Far Beyond.pdf}
}

@article{chenAizenmanSimsStarrSchemeParisi2013,
  title = {The {{Aizenman-Sims-Starr}} Scheme and {{Parisi}} Formula for Mixed \$p\$-Spin Spherical Models},
  author = {Chen, Wei-Kuo},
  date = {2013-01},
  journaltitle = {Electronic Journal of Probability},
  volume = {18},
  pages = {1--14},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1083-6489, 1083-6489},
  doi = {10.1214/EJP.v18-2580},
  abstract = {The Parisi formula for the free energy in the spherical models with mixed even \$p\$-spin interactions was proven in Michel Talagrand. In this paper we study the general mixed \$p\$-spin spherical models including \$p\$-spin interactions for odd \$p\$. We establish the Aizenman Sims-Starr scheme and from this together with many well-known results and Dmitry Panchenko's recent proof on the Parisi ultrametricity conjecture, we prove the Parisi formula.},
  issue = {none},
  file = {/Users/felix/paper/2013_Chen/Chen_2013_The Aizenman-Sims-Starr scheme and Parisi formula for mixed $p$-spin spherical.pdf}
}

@inproceedings{chenFasterPerturbedStochastic2022,
  title = {Faster {{Perturbed Stochastic Gradient Methods}} for {{Finding Local Minima}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Algorithmic Learning Theory}}},
  author = {Chen, Zixiang and Zhou, Dongruo and Gu, Quanquan},
  date = {2022-03-20},
  pages = {176--204},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v167/chen22b.html},
  urldate = {2022-10-17},
  abstract = {Escaping from saddle points and finding local minimum is a central problem in nonconvex optimization. Perturbed gradient methods are perhaps the simplest approach for this problem. However, to find (ϵ,ϵ√)(ϵ,ϵ)(\textbackslash epsilon, \textbackslash sqrt\{\textbackslash epsilon\})-approximate local minima, the existing best stochastic gradient complexity for this type of algorithms is O\textasciitilde (ϵ−3.5)O\textasciitilde (ϵ−3.5)\textbackslash tilde O(\textbackslash epsilon\textasciicircum\{-3.5\}), which is not optimal. In this paper, we propose LENA (Last stEp shriNkAge), a faster perturbed stochastic gradient framework for finding local minima. We show that LENA with stochastic gradient estimators such as SARAH/SPIDER and STORM can find (ϵ,ϵH)(ϵ,ϵH)(\textbackslash epsilon, \textbackslash epsilon\_\{H\})-approximate local minima within O\textasciitilde (ϵ−3+ϵ−6H)O\textasciitilde (ϵ−3+ϵH−6)\textbackslash tilde O(\textbackslash epsilon\textasciicircum\{-3\} + \textbackslash epsilon\_\{H\}\textasciicircum\{-6\}) stochastic gradient evaluations (or O\textasciitilde (ϵ−3)O\textasciitilde (ϵ−3)\textbackslash tilde O(\textbackslash epsilon\textasciicircum\{-3\}) when ϵH=ϵ√ϵH=ϵ\textbackslash epsilon\_H = \textbackslash sqrt\{\textbackslash epsilon\}). The core idea of our framework is a step-size shrinkage scheme to control the average movement of the iterates, which leads to faster convergence to the local minima.},
  eventtitle = {International {{Conference}} on {{Algorithmic Learning Theory}}},
  langid = {english},
  file = {/Users/felix/paper/2022_Chen et al/Chen et al_2022_Faster Perturbed Stochastic Gradient Methods for Finding Local Minima.pdf}
}

@online{chengDifferentiationIntegralSign2013,
  title = {Differentiation under the Integral Sign},
  author = {Cheng, Steve},
  date = {2013-03-22},
  url = {https://planetmath.org/differentiationundertheintegralsign},
  urldate = {2022-12-20},
  annotation = {username: stevecheng (10074)},
  file = {/Users/felix/Zotero/storage/24I7IQN7/differentiationundertheintegralsign.html}
}

@article{chengExpectedNumberHeight2018,
  title = {Expected {{Number}} and {{Height Distribution}} of {{Critical Points}} of {{Smooth Isotropic Gaussian Random Fields}}},
  author = {Cheng, Dan and Schwartzman, Armin},
  date = {2018-11},
  journaltitle = {Bernoulli : official journal of the Bernoulli Society for Mathematical Statistics and Probability},
  shortjournal = {Bernoulli (Andover)},
  volume = {24},
  eprint = {31511762},
  eprinttype = {pmid},
  pages = {3422--3446},
  issn = {1350-7265},
  doi = {10.3150/17-BEJ964},
  abstract = {We obtain formulae for the expected number and height distribution of critical points of smooth isotropic Gaussian random fields parameterized on Euclidean space or spheres of arbitrary dimension. The results hold in general in the sense that there are no restrictions on the covariance function of the field except for smoothness and isotropy. The results are based on a characterization of the distribution of the Hessian of the Gaussian field by means of the family of Gaussian orthogonally invariant (GOI) matrices, of which the Gaussian orthogonal ensemble (GOE) is a special case. The obtained formulae depend on the covariance function only through a single parameter (Euclidean space) or two parameters (spheres), and include the special boundary case of random Laplacian eigenfunctions.},
  issue = {4B},
  pmcid = {PMC6738978},
  file = {/Users/felix/paper/2018_Cheng_Schwartzman/Cheng_Schwartzman_2018_Expected Number and Height Distribution of Critical Points of Smooth Isotropic.pdf}
}

@article{chenParisiFormulaDisorder2017,
  title = {Parisi {{Formula}}, {{Disorder Chaos}} and {{Fluctuation}} for the {{Ground State Energy}} in the {{Spherical Mixed}} p-{{Spin Models}}},
  author = {Chen, Wei-Kuo and Sen, Arnab},
  date = {2017-02-01},
  journaltitle = {Communications in Mathematical Physics},
  shortjournal = {Commun. Math. Phys.},
  volume = {350},
  number = {1},
  pages = {129--173},
  issn = {1432-0916},
  doi = {10.1007/s00220-016-2808-3},
  abstract = {We show that the limiting ground state energy of the spherical mixed p-spin model can be identified as the infimum of certain variational problem. This complements the well-known Parisi formula for the limiting free energy in the spherical model. As an application, we obtain explicit formulas for the limiting ground state energy in the replica symmetry, one level of replica symmetry breaking and full replica symmetry breaking phases at zero temperature. In addition, our approach leads to new results on disorder chaos in spherical mixed even p-spin models. In particular, we prove that when there is no external field, the location of the ground state energy is chaotic under small perturbations of the disorder. We also establish that in the spherical mixed even p-spin model, the ground state energy superconcentrates in the absence of external field, while it obeys a central limit theorem if the external field is present.},
  langid = {english},
  file = {/Users/felix/paper/2017_Chen_Sen/Chen_Sen_2017_Parisi Formula, Disorder Chaos and Fluctuation for the Ground State Energy in.pdf}
}

@book{chiuStochasticGeometryIts2013,
  title = {Stochastic {{Geometry}} and {{Its Applications}}},
  author = {Chiu, Sung Nok and Stoyan, Dietrich and Kendall, Wilfrid S. and Mecke, Joseph},
  date = {2013-06-27},
  publisher = {John Wiley \& Sons},
  abstract = {An extensive update to a classic text Stochastic geometry and spatial statistics play a fundamental role in many modern branches of physics, materials sciences, engineering, biology and environmental sciences. They offer successful models for the description of random two- and three-dimensional micro and macro structures and statistical methods for their analysis. The previous edition of this book has served as the key reference in its field for over 18 years and is regarded as the best treatment of the subject of stochastic geometry, both as a subject with vital applications to spatial statistics and as a very interesting field of mathematics in its own right. This edition:  Presents a wealth of models for spatial patterns and related statistical methods. Provides a great survey of the modern theory of random tessellations, including many new models that became tractable only in the last few years. Includes new sections on random networks and random graphs to review the recent ever growing interest in these areas. Provides an excellent introduction to theory and modelling of point processes, which covers some very latest developments. Illustrate the forefront theory of random sets, with many applications. Adds new results to the discussion of fibre and surface processes. Offers an updated collection of useful stereological methods. Includes 700 new references. Is written in an accessible style enabling non-mathematicians to benefit from this book. Provides a companion website hosting information on recent developments in the field www.wiley.com/go/cskm   Stochastic Geometry and its Applications is ideally suited for researchers in physics, materials science, biology and ecological sciences as well as mathematicians and statisticians. It should also serve as a valuable introduction to the subject for students of mathematics and statistics.},
  isbn = {978-1-118-65825-3},
  langid = {english},
  pagetotal = {518},
  keywords = {Mathematics / General,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  file = {/Users/felix/paper/2013_Chiu et al/Chiu et al_2013_Stochastic Geometry and Its Applications.pdf}
}

@online{choAnalysisExtensionArcCosine2011,
  title = {Analysis and {{Extension}} of {{Arc-Cosine Kernels}} for {{Large Margin Classification}}},
  author = {Cho, Youngmin and Saul, Lawrence K.},
  date = {2011-12-16},
  eprint = {1112.3712},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1112.3712},
  abstract = {We investigate a recently proposed family of positive-definite kernels that mimic the computation in large neural networks. We examine the properties of these kernels using tools from differential geometry; specifically, we analyze the geometry of surfaces in Hilbert space that are induced by these kernels. When this geometry is described by a Riemannian manifold, we derive results for the metric, curvature, and volume element. Interestingly, though, we find that the simplest kernel in this family does not admit such an interpretation. We explore two variations of these kernels that mimic computation in neural networks with different activation functions. We experiment with these new kernels on several data sets and highlight their general trends in performance for classification.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/paper/2011_Cho_Saul/Cho_Saul_2011_Analysis and Extension of Arc-Cosine Kernels for Large Margin Classification.pdf;/Users/felix/Zotero/storage/QRWJ6KLH/1112.html}
}

@inproceedings{choKernelMethodsDeep2009,
  title = {Kernel {{Methods}} for {{Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cho, Youngmin and Saul, Lawrence},
  date = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2009/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html},
  urldate = {2023-04-03},
  abstract = {We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets.  These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs).  We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures.  On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.},
  file = {/Users/felix/paper/2009_Cho_Saul/Cho_Saul_2009_Kernel Methods for Deep Learning.pdf}
}

@inproceedings{choromanskaLossSurfacesMultilayer2015,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  booktitle = {Proceedings of the {{Eighteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Choromanska, Anna and family=Henaff, given=Mikael, given-i={{Mi}} and Mathieu, Michael and Arous, Gerard Ben and LeCun, Yann},
  date = {2015-02-21},
  eprint = {1412.0233},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {192--204},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v38/choromanska15.html},
  urldate = {2023-01-23},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/paper/2015_Choromanska et al/Choromanska et al_2015_The Loss Surfaces of Multilayer Networks.pdf}
}

@inproceedings{choromanskaOpenProblemLandscape2015,
  title = {Open {{Problem}}: {{The}} Landscape of the Loss Surfaces of Multilayer Networks},
  shorttitle = {Open {{Problem}}},
  booktitle = {Proceedings of {{The}} 28th {{Conference}} on {{Learning Theory}}},
  author = {Choromanska, Anna and LeCun, Yann and Arous, Gérard Ben},
  date = {2015-06-26},
  pages = {1756--1760},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v40/Choromanska15.html},
  urldate = {2023-01-23},
  abstract = {Deep learning has enjoyed a resurgence of interest in the last few years for such applications as image and speech recognition, or natural language processing. The vast majority of practical applications of deep learning focus on supervised learning, where the supervised loss function is minimized using stochastic gradient descent. The properties of this highly non-convex loss function, such as its landscape and the behavior of critical points (maxima, minima, and saddle points), as well as the reason why large- and small-size networks achieve radically different practical performance, are however very poorly understood. It was only recently shown that new results in spin-glass theory potentially may provide an explanation for these problems by establishing a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models. The connection between both models relies on a number of possibly unrealistic assumptions, yet the empirical evidence suggests that the connection may exist in real. The question we pose is whether it is possible to drop some of these assumptions to establish a stronger connection between both models.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  file = {/Users/felix/paper/2015_Choromanska et al/Choromanska et al_2015_Open Problem.pdf}
}

@inbook{crisantiDynamicalMeanFieldTheory2023,
  title = {Dynamical {{Mean-Field Theory}} and the {{Aging Dynamics}}},
  booktitle = {Spin {{Glass Theory}} and {{Far Beyond}}},
  author = {Crisanti, Andrea and Franz, Silvio and Kurchan, Jorge and Maiorano, Andrea},
  date = {2023-08},
  pages = {157--186},
  publisher = {WORLD SCIENTIFIC},
  doi = {10.1142/9789811273926_0009},
  bookauthor = {Charbonneau, Patrick and Marinari, Enzo and Mézard, Marc and Parisi, Giorgio and Ricci-Tersenghi, Federico and Sicuro, Gabriele and Zamponi, Francesco},
  isbn = {9789811273919 9789811273926},
  langid = {english}
}

@article{crisantiSphericalPspinInteraction1992,
  title = {The Spherical P-Spin Interaction Spin Glass Model: The Statics},
  shorttitle = {The Sphericalp-Spin Interaction Spin Glass Model},
  author = {Crisanti, A. and Sommers, H. -J.},
  date = {1992-10-01},
  journaltitle = {Zeitschrift für Physik B Condensed Matter},
  shortjournal = {Z. Physik B - Condensed Matter},
  volume = {87},
  number = {3},
  pages = {341--354},
  issn = {1431-584X},
  doi = {10.1007/BF01309287},
  abstract = {The static properties of the sphericalp-spin interaction spin glass model are calculated using the replica method. It is shown that within the Parisi scheme the most general solution is the one-step replica symmetry breaking. The transition from the replica symmetric solution to the replica replica symmetry broken one is either continuous or discontinuous inq1−q0 depending on the strength of the external magnetic field. The model can be solved explicitly for anyp at any temperature and magnetic field. Below the transition we find an infinite number of metastable states.},
  langid = {english},
  keywords = {Complex System,Magnetic Field,Neural Network,Spectroscopy,State Physics},
  file = {/Users/felix/paper/1992_Crisanti_Sommers/Crisanti_Sommers_1992_The spherical p-spin interaction spin glass model.pdf}
}

@article{cugliandoloAnalyticalSolutionOffEquilibrium1993,
  title = {Analytical {{Solution}} of the {{Off-Equilibrium Dynamics}} of a {{Long Range Spin-Glass Model}}},
  author = {Cugliandolo, L. F. and Kurchan, J.},
  date = {1993-07-05},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {71},
  number = {1},
  eprint = {cond-mat/9303036},
  eprinttype = {arXiv},
  pages = {173--176},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.71.173},
  abstract = {We study the non-equilibrium relaxation of the spherical spin-glass model with p-spin interactions in the \$N \textbackslash rightarrow \textbackslash infty\$ limit. We analytically solve the asymptotics of the magnetization and the correlation and response functions for long but finite times. Even in the thermodynamic limit the system exhibits `weak' (as well as `true') ergodicity breaking and aging effects. We determine a functional Parisi-like order parameter \$P\_d(q)\$ which plays a similar role for the dynamics to that played by the usual function for the statics.},
  keywords = {Condensed Matter},
  file = {/Users/felix/Zotero/storage/IFXACUJ4/Cugliandolo and Kurchan - 1993 - Analytical Solution of the Off-Equilibrium Dynamic.pdf;/Users/felix/Zotero/storage/Q77VMMNU/9303036.html}
}

@inproceedings{cunhaOnlyTailsMatter2022,
  title = {Only Tails Matter: {{Average-Case Universality}} and {{Robustness}} in the {{Convex Regime}}},
  shorttitle = {Only Tails Matter},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Cunha, Leonardo and Gidel, Gauthier and Pedregosa, Fabian and Scieur, Damien and Paquette, Courtney},
  date = {2022-06-28},
  pages = {4474--4491},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/cunha22a.html},
  urldate = {2023-11-09},
  abstract = {The recently developed average-case analysis of optimization methods allows a more fine-grained and representative convergence analysis than usual worst-case results. In exchange, this analysis requires a more precise hypothesis over the data generating process, namely assuming knowledge of the expected spectral distribution (ESD) of the random matrix associated with the problem. This work shows that the concentration of eigenvalues near the edges of the ESD determines a problem’s asymptotic average complexity. This a priori information on this concentration is a more grounded assumption than complete knowledge of the ESD. This approximate concentration is effectively a middle ground between the coarseness of the worst-case scenario convergence and the restrictive previous average-case analysis. We also introduce the Generalized Chebyshev method, asymptotically optimal under a hypothesis on this concentration and globally optimal when the ESD follows a Beta distribution. We compare its performance to classical optimization algorithms, such as gradient descent or Nesterov’s scheme, and we show that, in the average-case context, Nesterov’s method is universally nearly optimal asymptotically.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2022_Cunha et al/Cunha et al_2022_Only tails matter.pdf}
}

@article{cutkoskyMechanicLearningRate2023,
  title = {Mechanic: {{A Learning Rate Tuner}}},
  shorttitle = {Mechanic},
  author = {Cutkosky, Ashok and Defazio, Aaron and Mehta, Harsh},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  eprint = {2306.00144},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  pages = {47828--47848},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/955499a8e2860ed746717c1374224c43-Abstract-Conference.html},
  urldate = {2024-03-31},
  langid = {english},
  file = {/Users/felix/paper/2023_Cutkosky et al/Cutkosky et al_2023_Mechanic.pdf}
}

@inproceedings{dauphinEquilibratedAdaptiveLearning2015,
  title = {Equilibrated Adaptive Learning Rates for Non-Convex Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dauphin, Yann and family=Vries, given=Harm, prefix=de, useprefix=true and Bengio, Yoshua},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/430c3626b879b4005d41b8a46172e0c0-Abstract.html},
  urldate = {2024-03-30},
  abstract = {Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of thecritical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments demonstrate that both schemes yield very similar step directions but that ESGD sometimes surpasses RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.},
  file = {/Users/felix/paper/2015_Dauphin et al/Dauphin et al_2015_Equilibrated adaptive learning rates for non-convex optimization.pdf}
}

@inproceedings{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  date = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  location = {Montréal, Canada},
  url = {https://proceedings.neurips.cc/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html},
  urldate = {2022-06-10},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
  file = {/Users/felix/paper/2014_Dauphin et al/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional.pdf;/Users/felix/paper/2014_Dauphin et al/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional2.pdf;/Users/felix/Zotero/storage/ZICT2QCI/1406.html}
}

@article{deanExtremeValueStatistics2008,
  title = {Extreme Value Statistics of Eigenvalues of {{Gaussian}} Random Matrices},
  author = {Dean, David S. and Majumdar, Satya N.},
  date = {2008-04-10},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {77},
  number = {4},
  pages = {041108},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.77.041108},
  abstract = {We compute exact asymptotic results for the probability of the occurrence of large deviations of the largest (smallest) eigenvalue of random matrices belonging to the Gaussian orthogonal, unitary, and symplectic ensembles. In particular, we show that the probability that all the eigenvalues of an (N×N) random matrix are positive (negative) decreases for large N as ∼exp [−βθ(0)N2] where the Dyson index β characterizes the ensemble and the exponent θ(0)=(ln 3)/4=0.274653… is universal. We compute the probability that the eigenvalues lie in the interval [ζ1,ζ2] which allows us to calculate the joint probability distribution of the minimum and the maximum eigenvalue. As a by-product, we also obtain exactly the average density of states in Gaussian ensembles whose eigenvalues are restricted to lie in the interval [ζ1,ζ2], thus generalizing the celebrated Wigner semi-circle law to these restricted ensembles. It is found that the density of states generically exhibits an inverse square-root singularity at the location of the barriers. These results are confirmed by numerical simulations. Some of the results presented in detail here were announced in a previous paper [D. S. Dean and S. N. Majumdar, Phys. Rev. Lett. 97, 160201 (2006)].},
  file = {/Users/felix/paper/2008_Dean_Majumdar/Dean_Majumdar_2008_Extreme value statistics of eigenvalues of Gaussian random matrices.pdf;/Users/felix/Zotero/storage/CQ9ZGLTS/PhysRevE.77.html}
}

@article{deanLargeDeviationsExtreme2006,
  title = {Large {{Deviations}} of {{Extreme Eigenvalues}} of {{Random Matrices}}},
  author = {Dean, David S. and Majumdar, Satya N.},
  date = {2006-10-20},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {97},
  number = {16},
  pages = {160201},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.97.160201},
  abstract = {We calculate analytically the probability of large deviations from its mean of the largest (smallest) eigenvalue of random matrices belonging to the Gaussian orthogonal, unitary, and symplectic ensembles. In particular, we show that the probability that all the eigenvalues of an (N×N) random matrix are positive (negative) decreases for large N as ∼exp [−βθ(0)N2] where the parameter β characterizes the ensemble and the exponent θ(0)=(ln 3)/4=0.274 653… is universal. We also calculate exactly the average density of states in matrices whose eigenvalues are restricted to be larger than a fixed number ζ, thus generalizing the celebrated Wigner semicircle law. The density of states generically exhibits an inverse square-root singularity at ζ.},
  file = {/Users/felix/paper/2006_Dean_Majumdar/Dean_Majumdar_2006_Large Deviations of Extreme Eigenvalues of Random Matrices.pdf;/Users/felix/Zotero/storage/VESJLMKA/PhysRevLett.97.html}
}

@article{debickiExtremesHomogeneousGaussian2015,
  title = {Extremes of {{Homogeneous Gaussian Random Fields}}},
  author = {Dębicki, Krzysztof and Hashorva, Enkelejd and Soja-Kukieła, Natalia},
  date = {2015-03},
  journaltitle = {Journal of Applied Probability},
  volume = {52},
  number = {1},
  pages = {55--67},
  publisher = {Cambridge University Press},
  issn = {0021-9002, 1475-6072},
  doi = {10.1239/jap/1429282606},
  abstract = {Let \{X(s, t): s, t ≥ 0\} be a centred homogeneous Gaussian field with almost surely continuous sample paths and correlation function r(s, t) = cov(X(s, t), X(0, 0)) such that r(s, t) = 1 - |s|α1 - |t|α2 + o(|s|α1 + |t|α2), s, t → 0, with α1, α2 ∈ (0, 2], and r(s, t) {$<$} 1 for (s, t) ≠ (0, 0). In this contribution we derive an asymptotic expansion (as u → ∞) of P(sup(sn1(u),tn2(u)) ∈[0,x]∙[0,y]X(s, t) ≤ u), where n1(u)n2(u) = u2/α1+2/α2Ψ(u), which holds uniformly for (x, y) ∈ [A, B]2 with A, B two positive constants and Ψ the survival function of an N(0, 1) random variable. We apply our findings to the analysis of extremes of homogeneous Gaussian fields over more complex parameter sets and a ball of random radius. Additionally, we determine the extremal index of the discretised random field determined by X(s, t).},
  isbn = {9781429282604},
  langid = {english},
  keywords = {60G15,60G70,Berman condition,extremal index,Gaussian random field,strong dependence,supremum,tail asymptoticy},
  file = {/Users/felix/paper/2015_Dębicki et al/Dębicki et al_2015_Extremes of Homogeneous Gaussian Random Fields.pdf}
}

@inproceedings{defazioLearningRateFreeLearningDAdaptation2023,
  title = {Learning-{{Rate-Free Learning}} by {{D-Adaptation}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Defazio, Aaron and Mishchenko, Konstantin},
  date = {2023-07-03},
  eprint = {2301.07733},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  pages = {7449--7479},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/defazio23a.html},
  urldate = {2024-03-31},
  abstract = {The speed of gradient descent for convex Lipschitz functions is highly dependent on the choice of learning rate. Setting the learning rate to achieve the optimal convergence rate requires knowing the distance D from the initial point to the solution set. In this work, we describe a single-loop method, with no back-tracking or line searches, which does not require knowledge of D yet asymptotically achieves the optimal rate of convergence for the complexity class of convex Lipschitz functions. Our approach is the first parameter-free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. Our method is practical, efficient and requires no additional function value or gradient evaluations each step. An implementation is provided in the supplementary material.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2023_Defazio_Mishchenko/Defazio_Mishchenko_2023_Learning-Rate-Free Learning by D-Adaptation.pdf}
}

@book{definettiTheoryProbabilityCritical2017,
  title = {Theory of {{Probability}}: {{A Critical Introductory Treatment}}},
  shorttitle = {Theory of {{Probability}}},
  author = {family=Finetti, given=Bruno, prefix=de, useprefix=true},
  date = {2017-04-17},
  edition = {1st edition},
  publisher = {Wiley},
  location = {Chichester, UK ; Hoboken, NJ},
  abstract = {First issued in translation as a two-volume work in 1975, this classic book provides the first complete development of the theory of probability from a subjectivist viewpoint. It proceeds from a detailed discussion of the philosophical mathematical aspects to a detailed mathematical treatment of probability and statistics.De Finetti's theory of probability is one of the foundations of Bayesian theory. De Finetti stated that probability is nothing but a subjective analysis of the likelihood that something will happen and that that probability does not exist outside the mind. It is the rate at which a person is willing to bet on something happening. This view is directly opposed to the classicist/ frequentist view of the likelihood of a particular outcome of an event, which assumes that the same event could be identically repeated many times over, and the 'probability' of a particular outcome has to do with the fraction of the time that outcome results from the repeated trials.},
  isbn = {978-1-119-28637-0},
  langid = {english},
  pagetotal = {608},
  file = {/Users/felix/paper/2017_Finetti/Finetti_2017_Theory of Probability.pdf}
}

@article{deiftConjugateGradientAlgorithm2021,
  title = {The Conjugate Gradient Algorithm on Well-Conditioned {{Wishart}} Matrices Is Almost Deterministic},
  author = {Deift, Percy and Trogdon, Thomas},
  date = {2021-03},
  journaltitle = {Quarterly of Applied Mathematics},
  shortjournal = {Quart. Appl. Math.},
  volume = {79},
  number = {1},
  eprint = {1901.09007},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  pages = {125--161},
  issn = {0033-569X, 1552-4485},
  doi = {10.1090/qam/1574},
  abstract = {We prove that the number of iterations required to solve a random positive definite linear system with the conjugate gradient algorithm is almost deterministic for large matrices. We treat the case of Wishart matrices \$W = XX\textasciicircum *\$ where \$X\$ is \$n \textbackslash times m\$ and \$n/m \textbackslash sim d\$ for \$0 {$<$} d {$<$} 1\$. Precisely, we prove that for most choices of error tolerance, as the matrix increases in size, the probability that the iteration count deviates from an explicit deterministic value tends to zero. In addition, for a fixed iteration count, we show that the norm of the error vector and the norm of the residual converge exponentially fast in probability, converge in mean, and converge almost surely.},
  langid = {english},
  file = {/Users/felix/paper/2021_Deift_Trogdon/Deift_Trogdon_2021_The conjugate gradient algorithm on well-conditioned Wishart matrices is almost.pdf}
}

@inproceedings{deiftUniversalityNumericalComputation2018,
  title = {Universality in {{Numerical Computation}} with {{Random Data}}: {{Case Studies}}, {{Analytical Results}} and {{Some Speculations}}},
  shorttitle = {Universality in {{Numerical Computation}} with {{Random Data}}},
  booktitle = {Computation and {{Combinatorics}} in {{Dynamics}}, {{Stochastics}} and {{Control}}},
  author = {Deift, Percy and Trogdon, Thomas},
  editor = {Celledoni, Elena and Di Nunno, Giulia and Ebrahimi-Fard, Kurusch and Munthe-Kaas, Hans Zanna},
  date = {2018},
  series = {Abel {{Symposia}}},
  pages = {221--231},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-01593-0_8},
  abstract = {We discuss various universality aspects of numerical computations using standard algorithms. These aspects include empirical observations and rigorous results. We also make various speculations about computation in a broader sense.},
  isbn = {978-3-030-01593-0},
  langid = {english},
  file = {/Users/felix/paper/2018_Deift_Trogdon/Deift_Trogdon_2018_Universality in Numerical Computation with Random Data.pdf}
}

@article{deuflhardAffineInvariantConvergence1979,
  title = {Affine {{Invariant Convergence Theorems}} for {{Newton}}’s {{Method}} and {{Extensions}} to {{Related Methods}}},
  author = {Deuflhard, P. and Heindl, G.},
  date = {1979-02},
  journaltitle = {SIAM Journal on Numerical Analysis},
  shortjournal = {SIAM J. Numer. Anal.},
  volume = {16},
  number = {1},
  pages = {1--10},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1429},
  doi = {10.1137/0716001},
  abstract = {A classical algorithm for solving the system of nonlinear equations \$F(x) = 0\$ is Newton’s method \textbackslash [ x\_\{k + 1\}  = x\_k  + s\_k ,\textbackslash quad \{\textbackslash text\{where \}\}F'(x\_k )s\_k  =  - F(x\_k ),\textbackslash quad x\_0 \{\textbackslash text\{ given\}\}.\textbackslash ] The method is attractive because it converges rapidly from any sufficiently good initial guess \$x\_0 \$. However, solving a system of linear equations (the Newton equations) at each stage can be expensive if the number of unknowns is large and may not be justified when \$x\_k \$ is far from a solution. Therefore, we consider the class of inexact Newton methods: \textbackslash [ x\_\{k + 1\}  = x\_k  + s\_k ,\textbackslash quad \{\textbackslash text\{where \}\}F'(x\_k )s\_k  =  - F(x\_k ) + r\_k ,\textbackslash quad \{\{\textbackslash left\textbackslash | \{r\_k \} \textbackslash right\textbackslash |\} / \{\textbackslash left\textbackslash | \{F(x\_k )\} \textbackslash right\textbackslash |\}\} \textbackslash leqq \textbackslash eta \_k \textbackslash ] which solve the Newton equations only approximately and in some unspecified manner. Under the natural assumption that the forcing sequence \$\textbackslash\{ n\_k \textbackslash\} \$ is uniformly less than one, we show that all such methods are locally convergent and characterize the order of convergence in terms of the rate of convergence of the relative residuals \$\textbackslash\{ \{\{\textbackslash |r\_k \textbackslash |\} / \{\textbackslash |F(x\_k )\textbackslash |\}\}\textbackslash\} \$.Finally, we indicate how these general results can be used to construct and analyze specific methods for solving systems of nonlinear equations.},
  file = {/Users/felix/paper/1979_Deuflhard_Heindl/Deuflhard_Heindl_1979_Affine Invariant Convergence Theorems for Newton’s Method and Extensions to.pdf}
}

@book{deuflhardNewtonMethodsNonlinear2011,
  title = {Newton {{Methods}} for {{Nonlinear Problems}}: {{Affine Invariance}} and {{Adaptive Algorithms}}},
  shorttitle = {Newton {{Methods}} for {{Nonlinear Problems}}},
  author = {Deuflhard, Peter},
  date = {2011},
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  edition = {1},
  volume = {35},
  publisher = {Springer Nature},
  location = {Berlin, Heidelberg},
  issn = {0179-3632},
  doi = {10.1007/978-3-642-23899-4},
  abstract = {Dealing with the efficient numerical solution of challenging nonlinear problems in science and engineering, this book focuses on both finite local and global Newton methods for direct problems or Gauss-Newton methods for inverse problems.},
  isbn = {978-3-642-23899-4},
  langid = {english},
  keywords = {Algebras Linear,Computational Mathematics and Numerical Analysis,Computational Science and Engineering,Equations Theory of,Math Applications in Computer Science,Mathematical and Computational Engineering,Mathematics,Mathematics and Statistics,Nonlinear theories,Numerical analysis,Optimization,Ordinary Differential Equations},
  file = {/Users/felix/paper/2011_Deuflhard/Deuflhard_2011_Newton Methods for Nonlinear Problems.pdf}
}

@inproceedings{dozatIncorporatingNesterovMomentum2016,
  title = {Incorporating {{Nesterov Momentum}} into {{Adam}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Learning Representations}}},
  author = {Dozat, Timothy},
  date = {2016-02-18},
  location = {San Juan},
  url = {https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ},
  urldate = {2021-11-16},
  abstract = {This work aims to improve upon the recently proposed and rapidly popular- ized optimization algorithm Adam (Kingma \& Ba, 2014). Adam has two main components—a momentum component and an adaptive...},
  eventtitle = {{{ICLR}}},
  langid = {english},
  file = {/Users/felix/paper/2016_Dozat/Dozat_2016_Incorporating Nesterov Momentum into Adam.pdf;/Users/felix/Zotero/storage/L89XBWYX/forum.html}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  date = {2011-07-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {12},
  pages = {2121--2159},
  issn = {1532-4435},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  file = {/Users/felix/paper/2011_Duchi et al/Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf}
}

@thesis{duvenaudAutomaticModelConstruction2014,
  type = {phdthesis},
  title = {Automatic Model Construction with {{Gaussian}} Processes},
  author = {Duvenaud, David},
  date = {2014},
  institution = {University of Cambridge},
  file = {/Users/felix/paper/2014_Duvenaud/Duvenaud_2014_Automatic model construction with Gaussian processes.pdf}
}

@book{eatonMultivariateStatisticsVector2007,
  title = {Multivariate {{Statistics}}: {{A Vector Space Approach}}},
  shorttitle = {Multivariate {{Statistics}}},
  author = {Eaton, Morris L.},
  editor = {Vitale, R. A.},
  date = {2007},
  series = {Lecture {{Notes-Monograph Series}}},
  volume = {53},
  publisher = {Institute of Mathematical Statistics},
  location = {Beachwood, Ohio, USA},
  issn = {0749-2170},
  doi = {10.1214/lnms/1196285102},
  isbn = {978-0-940600-69-0},
  langid = {english},
  pagetotal = {519},
  keywords = {Coordinate systems,Covariance,Eigenvalues,Gaussian distributions,Inner products,Linear models,Linear transformations,Mathematical vectors,Matrices,Vector spaces},
  file = {/Users/felix/paper/2007_Eaton/Eaton_2007_Multivariate Statistics.pdf}
}

@article{edelmanEigenvaluesConditionNumbers1988,
  title = {Eigenvalues and {{Condition Numbers}} of {{Random Matrices}}},
  author = {Edelman, Alan},
  date = {1988-10},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  volume = {9},
  number = {4},
  pages = {543--560},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0895-4798},
  doi = {10.1137/0609045},
  abstract = {Given a random matrix, what condition number should be expected? This paper presents a proof that for real or complex  𝑛×𝑛 n×n  matrices with elements from a standard normal distribution, the expected value of the log of the 2-norm condition number is asymptotic to  log𝑛 log⁡n  as  𝑛→∞ n→∞ . In fact, it is roughly  log𝑛+1.537 log⁡n+1.537  for real matrices and  log𝑛+0.982 log⁡n+0.982  for complex matrices as  𝑛→∞ n→∞ . The paper discusses how the distributions of the condition numbers behave for large n for real or complex and square or rectangular matrices. The exact distributions of the condition numbers of  2×𝑛 2×n  matrices are also given. Intimately related to this problem is the distribution of the eigenvalues of Wishart matrices. This paper studies in depth the largest and smallest eigenvalues, giving exact distributions in some cases. It also describes the behavior of all the eigenvalues, giving an exact formula for the expected characteristic polynomial.},
  keywords = {15A52,characteristic polynomial,condition number,eigenvalues,random matrices,singular values,Wishart distribution},
  file = {/Users/felix/paper/1988_Edelman/Edelman_1988_Eigenvalues and Condition Numbers of Random Matrices.pdf}
}

@article{elalaouiOptimizationMeanfieldSpin2021,
  title = {Optimization of Mean-Field Spin Glasses},
  author = {El Alaoui, Ahmed and Montanari, Andrea and Sellke, Mark},
  date = {2021-11},
  journaltitle = {The Annals of Probability},
  volume = {49},
  number = {6},
  pages = {2922--2960},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/21-AOP1519},
  file = {/Users/felix/paper/2021_El Alaoui et al/[Arxiv] El Alaoui et al_2021_Optimization of mean-field spin glasses.pdf;/Users/felix/paper/2021_El Alaoui et al/El Alaoui et al_2021_Optimization of mean-field spin glasses.pdf}
}

@article{estradeCovarianceFunctionsSpheres2019,
  title = {Covariance Functions on Spheres Cross Time: {{Beyond}} Spatial Isotropy and Temporal Stationarity},
  shorttitle = {Covariance Functions on Spheres Cross Time},
  author = {Estrade, Anne and Fariñas, Alessandra and Porcu, Emilio},
  date = {2019-08-01},
  journaltitle = {Statistics \& Probability Letters},
  shortjournal = {Statistics \& Probability Letters},
  volume = {151},
  pages = {1--7},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2019.03.011},
  abstract = {Spectral representations uniquely define the covariance functions associated to random fields defined over spheres or spheres cross time. Covariance functions on spheres cross time are usually modeled under the assumptions of either spatial isotropy or axial symmetry, and the assumption of temporal stationarity. This paper goes beyond these assumptions. In particular, we consider the problem of spatially anisotropic covariance functions on spheres. The crux of our criterion is to escape from the addition theorem for spherical harmonics. We also challenge the problem of temporal nonstationarity in nonseparable space–time covariance functions, where space is the n-dimensional sphere.},
  keywords = {Gegenbauer polynomial,Positive definite,Space–time random field,Spectral representation,Spherical harmonic},
  file = {/Users/felix/paper/2019_Estrade et al/Estrade et al_2019_Covariance functions on spheres cross time.pdf;/Users/felix/Zotero/storage/LCQIM7F8/S0167715219300872.html}
}

@article{forresterSpectralDensityAsymptotics2012,
  title = {Spectral Density Asymptotics for {{Gaussian}} and {{Laguerre}} β-Ensembles in the Exponentially Small Region},
  author = {Forrester, Peter J.},
  date = {2012-02},
  journaltitle = {Journal of Physics A: Mathematical and Theoretical},
  shortjournal = {J. Phys. A: Math. Theor.},
  volume = {45},
  number = {7},
  pages = {075206},
  publisher = {IOP Publishing},
  issn = {1751-8121},
  doi = {10.1088/1751-8113/45/7/075206},
  abstract = {The first two terms in the large N asymptotic expansion of the β moment of the characteristic polynomial for the Gaussian and Laguerre β-ensembles are calculated. This is used to compute the asymptotic expansion of the spectral density in these ensembles, in the exponentially small region outside the leading support, up to terms o(1) . The leading form of the right tail of the distribution of the largest eigenvalue is given by the density in this regime. It is demonstrated that there is a scaling from this, to the right tail asymptotics for the distribution of the largest eigenvalue at the soft edge.},
  langid = {english},
  file = {/Users/felix/paper/2012_Forrester/Forrester_2012_Spectral density asymptotics for Gaussian and Laguerre β-ensembles in the.pdf}
}

@article{forsytheBestConditionedMatrices1955,
  title = {On {{Best Conditioned Matrices}}},
  author = {Forsythe, G. E. and Straus, E. G.},
  date = {1955},
  journaltitle = {Proceedings of the American Mathematical Society},
  volume = {6},
  number = {3},
  eprint = {2032772},
  eprinttype = {jstor},
  pages = {340--345},
  publisher = {American Mathematical Society},
  issn = {0002-9939},
  doi = {10.2307/2032772},
  file = {/Users/felix/paper/1955_Forsythe_Straus/Forsythe_Straus_1955_On Best Conditioned Matrices.pdf}
}

@incollection{frazierBayesianOptimization2018,
  title = {Bayesian {{Optimization}}},
  booktitle = {Recent {{Advances}} in {{Optimization}} and {{Modeling}} of {{Contemporary Problems}}},
  author = {Frazier, Peter I.},
  date = {2018-10},
  series = {{{INFORMS TutORials}} in {{Operations Research}}},
  eprint = {1807.02811},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  pages = {255--278},
  publisher = {INFORMS},
  location = {Phoenix, Arizona, USA},
  doi = {10.1287/educ.2018.0188},
  isbn = {978-0-9906153-2-3},
  keywords = {Bayesian optimization,Computer Science - Machine Learning,derivative-free optimization,entropy search,expected improvement,Gaussian processes,knowledge-gradient methods,Mathematics - Optimization and Control,optimization of expensive functions,Statistics - Machine Learning,surrogate-based optimization},
  file = {/Users/felix/paper/2018_Frazier/Frazier_2018_A Tutorial on Bayesian Optimization.pdf;/Users/felix/paper/2018_Frazier/Frazier_2018_Bayesian Optimization.pdf}
}

@article{frazierKnowledgeGradientPolicyCorrelated2009,
  title = {The {{Knowledge-Gradient Policy}} for {{Correlated Normal Beliefs}}},
  author = {Frazier, Peter and Powell, Warren and Dayanik, Savas},
  date = {2009-11},
  journaltitle = {INFORMS Journal on Computing},
  volume = {21},
  number = {4},
  pages = {599--613},
  publisher = {INFORMS},
  issn = {1091-9856},
  doi = {10.1287/ijoc.1080.0314},
  abstract = {We consider a Bayesian ranking and selection problem with independent normal rewards and a correlated multivariate normal belief on the mean values of these rewards. Because this formulation of the ranking and selection problem models dependence between alternatives' mean values, algorithms may use this dependence to perform efficiently even when the number of alternatives is very large. We propose a fully sequential sampling policy called the knowledge-gradient policy, which is provably optimal in some special cases and has bounded suboptimality in all others. We then demonstrate how this policy may be applied to efficiently maximize a continuous function on a continuous domain while constrained to a fixed number of noisy measurements.},
  keywords = {Bayesian,decision analysis,design of experiments,sequential,simulation,statistics},
  file = {/Users/felix/paper/2009_Frazier et al/Frazier et al_2009_The Knowledge-Gradient Policy for Correlated Normal Beliefs.pdf}
}

@article{fyodorovComplexityRandomEnergy2004,
  title = {Complexity of {{Random Energy Landscapes}}, {{Glass Transition}}, and {{Absolute Value}} of the {{Spectral Determinant}} of {{Random Matrices}}},
  author = {Fyodorov, Yan V.},
  date = {2004-06-15},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {92},
  number = {24},
  eprint = {cond-mat/0401287},
  eprinttype = {arXiv},
  pages = {240601},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.92.240601},
  abstract = {Finding the mean of the total number Ntot of stationary points for N-dimensional random energy landscapes is reduced to averaging the absolute value of the characteristic polynomial of the corresponding Hessian. For any finite N we provide the exact solution to the problem for a class of landscapes corresponding to the “toy model” of manifolds in a random environment. For N≫1 our asymptotic analysis reveals a phase transition at some critical value μc of a control parameter μ from a phase with a finite landscape complexity: Ntot∼eNΣ, Σ(μ{$<$}μc){$>$}0 to the phase with vanishing complexity: Σ(μ{$>$}μc)=0. Finally, we discuss a method of dealing with the modulus of the spectral determinant applicable to a broad class of problems.},
  file = {/Users/felix/paper/2004_Fyodorov/[journal] Fyodorov_2004_Complexity of Random Energy Landscapes, Glass Transition, and Absolute Value of.pdf;/Users/felix/paper/2004_Fyodorov/Fyodorov_2004_Complexity of Random Energy Landscapes, Glass Transition, and Absolute Value of.pdf;/Users/felix/Zotero/storage/PH78I8GF/PhysRevLett.92.html;/Users/felix/Zotero/storage/SDFWLHS5/PhysRevLett.92.html}
}

@article{fyodorovHighDimensionalRandomFields2015,
  title = {High-{{Dimensional Random Fields}} and {{Random Matrix Theory}}},
  author = {Fyodorov, Yan V.},
  date = {2015},
  journaltitle = {Markov Processes And Related Fields},
  volume = {21},
  number = {3},
  eprint = {1307.2379},
  eprinttype = {arXiv},
  pages = {483--518},
  issn = {1024-2953},
  abstract = {Our goal is to discuss in detail the calculation of the mean number of stationary points and minima for random isotropic Gaussian fields on a sphere as well as for stationary Gaussian random fields in a background parabolic confinement. After developing the general formalism based on the high-dimensional Kac-Rice formulae we combine it with the Random Matrix Theory (RMT) techniques to perform analysis of the random energy landscape of \$p-\$spin spherical spinglasses and a related glass model, both displaying a zero-temperature one-step replica symmetry breaking glass transition as a function of control parameters (e.g. a magnetic field or curvature of the confining potential). A particular emphasis of the presented analysis is on understanding in detail the picture of "topology trivialization" (in the sense of drastic reduction of the number of stationary points) of the landscape which takes place in the vicinity of the zero-temperature glass transition in both models. We will reveal the important role of the GOE "edge scaling" spectral region and the Tracy-Widom distribution of the maximal eigenvalue of GOE matrices for providing an accurate quantitative description of the universal features of the topology trivialization scenario.},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2015_Fyodorov/Fyodorov_2015_High-Dimensional Random Fields and Random Matrix Theory.pdf;/Users/felix/Zotero/storage/CWGQ4C9V/1307.html}
}

@article{fyodorovTopologyTrivializationLarge2014,
  title = {Topology Trivialization and Large Deviations for the Minimum in the Simplest Random Optimization},
  author = {Fyodorov, Yan V. and Doussal, Pierre Le},
  date = {2014-01},
  journaltitle = {Journal of Statistical Physics},
  shortjournal = {J Stat Phys},
  volume = {154},
  number = {1-2},
  eprint = {1304.0024},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, physics:math-ph},
  pages = {466--490},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-013-0838-1},
  abstract = {Finding the global minimum of a cost function given by the sum of a quadratic and a linear form in N real variables over (N-1)- dimensional sphere is one of the simplest, yet paradigmatic problems in Optimization Theory known as the "trust region subproblem" or "constraint least square problem". When both terms in the cost function are random this amounts to studying the ground state energy of the simplest spherical spin glass in a random magnetic field. We first identify and study two distinct large-N scaling regimes in which the linear term (magnetic field) leads to a gradual topology trivialization, i.e. reduction in the total number N\_\{tot\} of critical (stationary) points in the cost function landscape. In the first regime N\_\{tot\} remains of the order \$N\$ and the cost function (energy) has generically two almost degenerate minima with the Tracy-Widom (TW) statistics. In the second regime the number of critical points is of the order of unity with a finite probability for a single minimum. In that case the mean total number of extrema (minima and maxima) of the cost function is given by the Laplace transform of the TW density, and the distribution of the global minimum energy is expected to take a universal scaling form generalizing the TW law. Though the full form of that distribution is not yet known to us, one of its far tails can be inferred from the large deviation theory for the global minimum. In the rest of the paper we show how to use the replica method to obtain the probability density of the minimum energy in the large-deviation approximation by finding both the rate function and the leading pre-exponential factor.},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Optimization and Control},
  file = {/Users/felix/paper/2014_Fyodorov_Doussal/Fyodorov_Doussal_2014_Topology trivialization and large deviations for the minimum in the simplest.pdf;/Users/felix/Zotero/storage/FTNBSEDZ/1304.html}
}

@incollection{gallicchioDeepRandomizedNeural2020,
  title = {Deep {{Randomized Neural Networks}}},
  booktitle = {Recent {{Trends}} in {{Learning From Data}}: {{Tutorials}} from the {{INNS Big Data}} and {{Deep Learning Conference}} ({{INNSBDDL2019}})},
  author = {Gallicchio, Claudio and Scardapane, Simone},
  editor = {Oneto, Luca and Navarin, Nicolò and Sperduti, Alessandro and Anguita, Davide},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {43--68},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-43883-8_3},
  abstract = {Randomized Neural Networks explore the behavior of neural systems where the majority of connections are fixed, either in a stochastic or a deterministic fashion. Typical examples of such systems consist of multi-layered neural network architectures where the connections to the hidden layer(s) are left untrained after initialization. Limiting the training algorithms to operate on a reduced set of weights inherently characterizes the class of Randomized Neural Networks with a number of intriguing features. Among them, the extreme efficiency of the resulting learning processes is undoubtedly a striking advantage with respect to fully trained architectures. Besides, despite the involved simplifications, randomized neural systems possess remarkable properties both in practice, achieving state-of-the-art results in multiple domains, and theoretically, allowing to analyze intrinsic properties of neural architectures (e.g. before training of the hidden layers’ connections). In recent years, the study of Randomized Neural Networks has been extended towards deep architectures, opening new research directions to the design of effective yet extremely efficient deep learning models in vectorial as well as in more complex data domains. This chapter surveys all the major aspects regarding the design and analysis of Randomized Neural Networks, and some of the key results with respect to their approximation capabilities. In particular, we first introduce the fundamentals of randomized neural models in the context of feed-forward networks (i.e., Random Vector Functional Link and equivalent models) and convolutional filters, before moving to the case of recurrent systems (i.e., Reservoir Computing networks). For both, we focus specifically on recent results in the domain of deep randomized systems, and (for recurrent models) their application to structured domains.},
  isbn = {978-3-030-43883-8},
  langid = {english},
  file = {/Users/felix/paper/2020_Gallicchio_Scardapane/Gallicchio_Scardapane_2020_Deep Randomized Neural Networks.pdf}
}

@article{gaoImplementingNelderMeadSimplex2012,
  title = {Implementing the {{Nelder-Mead}} Simplex Algorithm with~Adaptive Parameters},
  author = {Gao, Fuchang and Han, Lixing},
  date = {2012-01-01},
  journaltitle = {Computational Optimization and Applications},
  shortjournal = {Comput Optim Appl},
  volume = {51},
  number = {1},
  pages = {259--277},
  issn = {1573-2894},
  doi = {10.1007/s10589-010-9329-3},
  abstract = {In this paper, we first prove that the expansion and contraction steps of the Nelder-Mead simplex algorithm possess a descent property when the objective function is uniformly convex. This property provides some new insights on why the standard Nelder-Mead algorithm becomes inefficient in high dimensions. We then propose an implementation of the Nelder-Mead method in which the expansion, contraction, and shrink parameters depend on the dimension of the optimization problem. Our numerical experiments show that the new implementation outperforms the standard Nelder-Mead method for high dimensional problems.},
  langid = {english},
  keywords = {Adaptive parameter,Nelder-Mead method,Optimization,Polytope,Simplex},
  file = {/Users/felix/paper/2012_Gao_Han/Gao_Han_2012_Implementing the Nelder-Mead simplex algorithm with adaptive parameters.pdf}
}

@online{garrigosHandbookConvergenceTheorems2023,
  title = {Handbook of {{Convergence Theorems}} for ({{Stochastic}}) {{Gradient Methods}}},
  author = {Garrigos, Guillaume and Gower, Robert M.},
  date = {2023-02-17},
  doi = {10.48550/arXiv.2301.11235},
  abstract = {This is a handbook of simple proofs of the convergence of gradient and stochastic gradient descent type methods. We consider functions that are Lipschitz, smooth, convex, strongly convex, and/or Polyak-\{\textbackslash L\}ojasiewicz functions. Our focus is on ``good proofs'' that are also simple. Each section can be consulted separately. We start with proofs of gradient descent, then on stochastic variants, including minibatching and momentum. Then move on to nonsmooth problems with the subgradient method, the proximal gradient descent and their stochastic variants. Our focus is on global convergence rates and complexity rates. Some slightly less common proofs found here include that of SGD (Stochastic gradient descent) with a proximal step, with momentum, and with mini-batching without replacement.},
  pubstate = {prepublished},
  keywords = {65K05 68T99,G.1.6,Mathematics - Optimization and Control},
  file = {/Users/felix/paper/2023_Garrigos_Gower/Garrigos_Gower_2023_Handbook of Convergence Theorems for (Stochastic) Gradient Methods.pdf;/Users/felix/Zotero/storage/RU3KXUGB/2301.html}
}

@article{gautschiElementaryInequalitiesRelating1959,
  title = {Some {{Elementary Inequalities Relating}} to the {{Gamma}} and {{Incomplete Gamma Function}}},
  author = {Gautschi, Walter},
  date = {1959},
  journaltitle = {Journal of Mathematics and Physics},
  volume = {38},
  number = {1-4},
  pages = {77--81},
  issn = {1467-9590},
  doi = {10.1002/sapm195938177},
  langid = {english},
  file = {/Users/felix/paper/1959_Gautschi/Gautschi_1959_Some Elementary Inequalities Relating to the Gamma and Incomplete Gamma Function.pdf;/Users/felix/Zotero/storage/EYAMIMVL/sapm195938177.html}
}

@book{gelfandGeneralizedFunctionsVolume1964,
  title = {Generalized {{Functions}}, {{Volume}} 4: {{Applications}} of {{Harmonic Analysis}}},
  shorttitle = {Generalized {{Functions}}, {{Volume}} 4},
  author = {Gelfand, I. M. and Vilenkin, N. Ya},
  translator = {Feinstein, Amiel},
  date = {1964},
  series = {{{AMS Chelsea Publishing}}},
  publisher = {American Mathematical Society},
  location = {Providence, Rhode Island},
  url = {https://lccn.loc.gov/2015040021},
  urldate = {2023-12-04},
  isbn = {978-1-4704-2662-0},
  langid = {english},
  pagetotal = {384},
  file = {/Users/felix/paper/1964_Gelfand_Vilenkin/Gelfand_Vilenkin_1964_Generalized Functions, Volume 4.pdf}
}

@book{gihmanTheoryStochasticProcesses1974,
  title = {The {{Theory}} of {{Stochastic Processes I}}},
  author = {Gihman, Iosif Il’ich and Skorokhod, Anatoliĭ Vladimirovich},
  date = {1974},
  series = {Classics in {{Mathematics}}},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-61943-4},
  isbn = {978-3-540-20284-4 978-3-642-61943-4},
  keywords = {60-02,60Jxx,60Kxx,Gaussian measure,Gaussian process,Markov chain,Markov process,Martingal,Martingale,probability,probability theory,random function,Random variable,Random Walk,stochastic processes,YellowSale2006},
  file = {/Users/felix/paper/1974_Gihman_Skorokhod/Gihman_Skorokhod_1974_The Theory of Stochastic Processes I.pdf}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  date = {2010-03-31},
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  location = {Sardinia, Italy},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate = {2023-04-11},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  eventtitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/paper/2010_Glorot_Bengio/Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf}
}

@article{gohWhyMomentumReally2017,
  title = {Why {{Momentum Really Works}}},
  author = {Goh, Gabriel},
  date = {2017-04-04},
  journaltitle = {Distill},
  shortjournal = {Distill},
  doi = {10.23915/distill.00006},
  abstract = {We often think of optimization with momentum as a ball rolling down a hill. This isn't wrong, but there is much more to the story.},
  langid = {english},
  file = {/Users/felix/Zotero/storage/SQTH5E9A/momentum.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2016-11-10},
  eprint = {omivDQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {MIT Press},
  abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
  isbn = {978-0-262-33737-3},
  langid = {english},
  pagetotal = {801},
  keywords = {Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Machine Learning},
  file = {/Users/felix/paper/2016_Goodfellow et al/Goodfellow et al_2016_Deep Learning.pdf}
}

@article{gotzeRateConvergenceSemicircular2003,
  title = {Rate of {{Convergence}} to the {{Semicircular Law}} for the {{Gaussian Unitary Ensemble}}},
  author = {Gotze, F. and Tikhomirov, A. N.},
  date = {2003-01},
  journaltitle = {Theory of Probability \& Its Applications},
  shortjournal = {Theory Probab. Appl.},
  volume = {47},
  number = {2},
  pages = {323--330},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0040-585X},
  doi = {10.1137/TPRBAU000047000002000323000001},
  abstract = {It is shown that the Kolmogorov distance between the expected spectral distribution function of an  𝑛×𝑛 n×n  Wigner matrix with Gaussian elements and the distribution function of the semicircular law is of order  𝑂( 𝑛 −2/3 ) O(n−2/3) .},
  keywords = {independent random variables,random matrix,spectral distribution},
  file = {/Users/felix/paper/2003_Gotze_Tikhomirov/Gotze_Tikhomirov_2003_Rate of Convergence to the Semicircular Law for the Gaussian Unitary Ensemble.pdf}
}

@article{gotzeRateConvergenceSemicircular2003a,
  title = {Rate of Convergence to the Semi-Circular Law},
  author = {Götze, F. and Tikhomirov, A.},
  date = {2003-10-01},
  journaltitle = {Probability Theory and Related Fields},
  shortjournal = {Probab. Theory Relat. Fields},
  volume = {127},
  number = {2},
  pages = {228--276},
  issn = {1432-2064},
  doi = {10.1007/s00440-003-0285-z},
  abstract = {A stochastic bound of order OP(n−1/2) for the Kolmogorov distance between the spectral distribution function of an n×n matrix from Wigner ensemble and the distribution function of the semi-circular law is obtained. The result holds assuming that the twelfth moment of the entries of the matrix is uniformly bounded.},
  langid = {english},
  keywords = {Independent random variables,Random matrix,Spectral distribution},
  file = {/Users/felix/paper/2003_Götze_Tikhomirov/Götze_Tikhomirov_2003_Rate of convergence to the semi-circular law.pdf}
}

@report{goyalAccurateLargeMinibatch2018,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  date = {2018-04-30},
  eprint = {1706.02677},
  eprinttype = {arXiv},
  eprintclass = {cs},
  institution = {arXiv},
  url = {http://arxiv.org/abs/1706.02677},
  urldate = {2024-04-02},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {/Users/felix/paper/2018_Goyal et al/Goyal et al_2018_Accurate, Large Minibatch SGD.pdf;/Users/felix/Zotero/storage/GVK5FVSB/1706.html}
}

@online{grossmannAnswerHowShow2016,
  title = {Answer to "{{How}} Do {{I}} Show That a Linear Operator Commuting with All Isometries Is a Scalar Multiple of the Identity?"},
  shorttitle = {Answer to "{{How}} Do {{I}} Show That a Linear Operator Commuting with All Isometries Is a Scalar Multiple of the Identity?},
  author = {Grossmann, Ben},
  date = {2016-12-01},
  url = {https://math.stackexchange.com/a/2038924/445105},
  urldate = {2023-12-20},
  organization = {Mathematics Stack Exchange},
  file = {/Users/felix/Zotero/storage/LXIG9463/how-do-i-show-that-a-linear-operator-commuting-with-all-isometries-is-a-scalar-m.html}
}

@article{guerraBrokenReplicaSymmetry2003,
  title = {Broken {{Replica Symmetry Bounds}} in the {{Mean Field Spin Glass Model}}},
  author = {Guerra, Francesco},
  date = {2003-02-01},
  journaltitle = {Communications in Mathematical Physics},
  shortjournal = {Commun. Math. Phys.},
  volume = {233},
  number = {1},
  pages = {1--12},
  issn = {1432-0916},
  doi = {10.1007/s00220-002-0773-5},
  abstract = {By using a simple interpolation argument, in previous work we have proven the existence of the thermodynamic limit, for mean field disordered models, including the Sherrington-Kirkpatrick model, and the Derrida p-spin model. Here we extend this argument in order to compare the limiting free energy with the expression given by the Parisi Ansatz, and including full spontaneous replica symmetry breaking. Our main result is that the quenched average of the free energy is bounded from below by the value given in the Parisi Ansatz, uniformly in the size of the system. Moreover, the difference between the two expressions is given in the form of a sum rule, extending our previous work on the comparison between the true free energy and its replica symmetric Sherrington-Kirkpatrick approximation. We give also a variational bound for the infinite volume limit of the ground state energy per site.},
  langid = {english},
  keywords = {Free Energy,Ground State Energy,State Energy,Symmetry Breaking,Thermodynamic Limit},
  file = {/Users/felix/paper/2003_Guerra/Guerra_2003_Broken Replica Symmetry Bounds in the Mean Field Spin Glass Model.pdf}
}

@book{gusakTheoryStochasticProcesses2010,
  title = {Theory of {{Stochastic Processes}}: {{With Applications}} to {{Financial Mathematics}} and {{Risk Theory}}},
  shorttitle = {Theory of {{Stochastic Processes}}},
  author = {Gusak, Dmytro and Kukush, Alexander and Kulik, Alexey and Mishura, Yuliya and Pilipenko, Andrey},
  date = {2010},
  series = {Problem {{Books}} in {{Mathematics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-0-387-87862-1},
  isbn = {978-0-387-87861-4 978-0-387-87862-1},
  langid = {english},
  keywords = {diffusion process,filtration,finite-dimensional distribution,Gaussian process,Markov chain,Martingale,Poisson process,queueing theory,renewal theory,Stochastic Differential Equations,stochastic process,Stochastic Processes},
  file = {/Users/felix/paper/2010_Gusak et al/Gusak et al_2010_Theory of Stochastic Processes.pdf}
}

@book{hanssonOptimizationLearningControl2023,
  title = {Optimization for Learning and Control},
  author = {Hansson, Anders},
  namea = {Andersen, Martin},
  nameatype = {collaborator},
  date = {2023},
  publisher = {John Wiley \& Sons, Inc.},
  location = {Hoboken, New Jersey},
  abstract = {Optimization for Learning and Control Comprehensive resource providing a masters’ level introduction to optimization theory and algorithms for learning and control. Optimization for Learning and Control describes how optimization is used in these domains, giving a thorough introduction to both Unsupervised Learning, Supervised Learning, and Reinforcement Learning, with an emphasis on optimization methods for large-scale learning and control problems. Several applications areas are also discussed, including signal processing, system identification, optimal control, and Machine Learning. Today, most of the material on the optimization aspects of Deep Learning that is accessible for students at a Masters’ level is focused on surface-level computer programming; deeper knowledge about the optimization methods and the trade-offs that are behind these methods is not provided. The objective of this book is to make this scattered knowledge, currently mainly available in publications in academic journals, accessible for Masters’ students in a coherent way. The focus is on basic algorithmic principles and trade-offs. We are now going to discuss Unsupervised Learning. This is about finding lower-dimensional descriptions of a set of data \{x1, … , xN\}. One simple such lower-dimensional description is the mean of the data. Another one could be to find a probability function from which the data are the outcome. We will see that there are many more lower-dimensional descriptions of data. We will start the chapter by defining entropy, and we will see that many of the probability density functions that are of interest in learning can be derived from the so-called “maximum entropy principle.” Specifically, we will derive the categorical distribution, the Ising distribution, and the normal distribution. There is a close relationship between the Lagrange dual function of the maximum entropy problem and maximum likelihood (ML) estimation, which will also be investigated. Other topics that we cover are prediction, graphical models, cross entropy, the expectation maximization algorithm, the Boltzmann machine, principal component analysis, mutual information, and cluster analysis. As a prelude to entropy we will start by discussing the so-called Chebyshev bounds. The CVX modeling package for MATLAB has pioneered what is referred to as disciplined convex programming. It requires that user inputs a problem in a form that allows the software to verify convexity via a number of known composition rules. The problem is then reformulated as a conic optimization problem and passed to one of several possible solvers. The software packages CVXPY, Convex.jl, and CVXR make similar modeling functionality available in the programming languages Python, Julia, and R, respectively. Optimization for Learning and Control covers sample topics such as: Optimization theory and optimization methods, covering classes of optimization problems like least squares problems, quadratic problems, conic optimization problems and rank optimization. First-order methods, second-order methods, variable metric methods, and methods for nonlinear least squares problems. Stochastic optimization methods, augmented Lagrangian methods, interior-point methods, and conic optimization methods. Dynamic programming for solving optimal control problems and its generalization to Reinforcement learning. How optimization theory is used to develop theory and tools of statistics and learning, e.g., the maximum likelihood method, expectation maximization, k-means clustering, and support vector machines. How calculus of variations is used in optimal control and for deriving the family of exponential distributions. Optimization for Learning and Control is an ideal resource on the subject for scientists and engineers learning about which optimization methods are useful for learning and control problems; the text will also appeal to industry professionals using Machine Learning for different practical applications.},
  isbn = {978-1-119-80914-2},
  langid = {english},
  keywords = {Mathematics; Mathematical optimization; Machine learning,Mathematics; MATLAB,Mathematics; Signal processing,System analysis},
  file = {/Users/felix/paper/2023_Hansson/Hansson_2023_Optimization for learning and control.pdf}
}

@article{hestenesMethodsConjugateGradients1952,
  title = {Methods of {{Conjugate Gradients}} for {{Solving Linear Systems}}},
  author = {Hestenes, Magnus R. and Stiefel, Eduard},
  date = {1952-12},
  journaltitle = {Journal of Research of the National Bureau of Standards},
  volume = {49},
  number = {6},
  pages = {409--436},
  doi = {10.6028/jres.049.044.},
  abstract = {An iterative algorithm is given for solving a system Ax=k of n linear equations in n unknowns. The solution is given in n steps. It is shown that this method is a special case of a very general method which also includes Gaussian elimination. These general algorithms are essentially algorithms for finding an n dimensional ellipsoid. Connections are made with the theory of orthogonal polynomials and continued fractions.},
  keywords = {Conjugate gradient method},
  file = {/Users/felix/paper/1952_Hestenes_Stiefel/Hestenes_Stiefel_1952_Methods of Conjugate Gradients for Solving Linear Systems1.pdf}
}

@unpublished{hintonNeuralNetworksMachine2012,
  type = {Massive Open Online Course},
  title = {Neural {{Networks}} for {{Machine Learning}}},
  author = {Hinton, Geoffrey},
  namea = {Sirvastava, Nitish and Swersky, Kevin},
  nameatype = {collaborator},
  date = {2012},
  url = {https://www.cs.toronto.edu/~hinton/coursera_lectures.html},
  urldate = {2021-11-16},
  venue = {Coursera},
  annotation = {Lecture 6 Slides: http://www.cs.toronto.edu/\textasciitilde tijmen/csc321/slides/lecture\_slides\_lec6.pdf},
  file = {/Users/felix/paper/2012_Hinton/Hinton_2012_Neural Networks for Machine Learning.pdf}
}

@book{hirschDifferentialTopology1976,
  title = {Differential {{Topology}}},
  author = {Hirsch, Morris W.},
  date = {1976},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {33},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4684-9449-5},
  isbn = {978-1-4684-9451-8 978-1-4684-9449-5},
  keywords = {differential topology,Differentialtopologie,Immersion,manifold,topology},
  file = {/Users/felix/paper/1976_Hirsch/Hirsch_1976_Differential Topology.pdf}
}

@article{hoareQuicksort1962,
  title = {Quicksort},
  author = {Hoare, C. A. R.},
  date = {1962-01-01},
  journaltitle = {The Computer Journal},
  shortjournal = {The Computer Journal},
  volume = {5},
  number = {1},
  pages = {10--16},
  issn = {0010-4620},
  doi = {10.1093/comjnl/5.1.10},
  abstract = {A description is given of a new method of sorting in the random-access store of a computer. The method compares very favourably with other known methods in speed, in economy of storage, and in ease of programming. Certain refinements of the method, which may be useful in the optimization of inner loops, are described in the second part of the paper.},
  file = {/Users/felix/paper/1962_Hoare/Hoare_1962_Quicksort.pdf;/Users/felix/Zotero/storage/NH6FLD4F/395338.html}
}

@online{huangConstructiveProofSpherical2024,
  title = {A {{Constructive Proof}} of the {{Spherical Parisi Formula}}},
  author = {Huang, Brice and Sellke, Mark},
  date = {2024-05-01},
  eprint = {2311.15495},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, physics:math-ph},
  doi = {10.48550/arXiv.2311.15495},
  abstract = {The Parisi formula for the free energy is among the crown jewels in the theory of spin glasses. We present a simpler proof of the lower bound in the case of the spherical mean-field model. Our method follows the TAP approach developed recently in e.g. (Subag, 2018): we obtain an ultrametric tree of pure states, each with approximately the same free energy as the entire model, which are hierarchically arranged in accordance with the Parisi ansatz. We construct this tree ``layer by layer'' given the minimizer to Parisi's variational problem. On overlap intervals with full RSB, the tree is built by an optimization algorithm due to Subag. On overlap intervals with finite RSB, the tree is constructed by a new truncated second moment argument; a similar argument also characterizes the free energy of the resulting pure states. Notably we do not use the Aizenman--Sims--Starr scheme, and require interpolation bounds only up to the 1RSB level. Our methods also yield results for large deviations of the ground state, including the entire upper tail rate function for all 1RSB models without external field.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2024_Huang_Sellke/Huang_Sellke_2024_A Constructive Proof of the Spherical Parisi Formula.pdf;/Users/felix/Zotero/storage/ZXI24ZIA/2311.html}
}

@inproceedings{huangTightLipschitzHardness2022,
  title = {Tight {{Lipschitz Hardness}} for Optimizing {{Mean Field Spin Glasses}}},
  booktitle = {2022 {{IEEE}} 63rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  author = {Huang, Brice and Sellke, Mark},
  date = {2022-10},
  pages = {312--322},
  issn = {2575-8454},
  doi = {10.1109/FOCS54457.2022.00037},
  abstract = {We study the problem of algorithmically optimizing the Hamiltonian of a spherical or Ising mean field spin glass. The maximum asymptotic value OPT of this random function is characterized by a variational principle known as the Parisi formula, proved first by Talagrand and in more generality by Panchenko. Recently developed approximate message passing algorithms efficiently optimize these functions up to a value ALG given by an extended Parisi formula, which minimizes over a larger space of functional order parameters. These two objectives are equal for spin glasses exhibiting a no overlap gap property. However, ALG can be strictly smaller than OPT, and no efficient algorithm producing a value exceeding ALG is known. We prove that when all interactions have even degree, no algorithm satisfying an overlap concentration property can produce an objective larger than ALG with non-negligible probability. This property holds for all algorithms with suitably Lipschitz dependence on the random disorder coefficients of the objective. It encompasses natural formulations of gradient descent, approximate message passing, and Langevin dynamics run for bounded time and in particular includes the algorithms achieving ALG mentioned above. To prove this result, we substantially generalize the overlap gap property framework introduced by Gamarnik and Sudan to arbitrary ultrametric forbidden structures of solutions.},
  eventtitle = {2022 {{IEEE}} 63rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  keywords = {Approximation algorithms,Computer science,Glass,Heuristic algorithms,Message passing,non-convex optimization,Optimized production technology,overlap gap property,spin glass,statistical physics},
  file = {/Users/felix/paper/2022_Huang_Sellke/[ArXivExtended] Huang_Sellke_2022_Tight Lipschitz Hardness for optimizing Mean Field Spin Glasses.pdf;/Users/felix/paper/2022_Huang_Sellke/Huang_Sellke_2022_Tight Lipschitz Hardness for optimizing Mean Field Spin Glasses.pdf;/Users/felix/Zotero/storage/7SY7GREN/9996802.html}
}

@online{InferringShapeOptimization,
  title = {Inferring Shape of Optimization Surface from Shape of Loss Curve - {{Online Technical Discussion Groups}}—{{Wolfram Community}}},
  url = {https://community.wolfram.com/groups/-/m/t/2362355},
  urldate = {2023-03-28},
  abstract = {Wolfram Community forum discussion about Inferring shape of optimization surface from shape of loss curve. Stay on top of important topics and build connections by joining Wolfram Community groups relevant to your interests.},
  langid = {american}
}

@article{innesFashionableModellingFlux2018,
  title = {Fashionable Modelling with Flux},
  author = {Innes, Michael and Saba, Elliot and Fischer, Keno and Gandhi, Dhairya and Rudilosso, Marco Concetto and Joy, Neethu Mariya and Karmali, Tejan and Pal, Avik and Shah, Viral},
  date = {2018},
  journaltitle = {CoRR},
  doi = {10.48550/arXiv.1811.01457},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/abs-1811-01457},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  file = {/Users/felix/paper/2018_Innes et al/Innes et al_2018_Fashionable modelling with flux.pdf}
}

@inproceedings{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  date = {2015-06-01},
  eprint = {1502.03167},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {448--456},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/ioffe15.html},
  urldate = {2021-10-06},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/paper/2015_Ioffe_Szegedy/Ioffe_Szegedy_2015_Batch Normalization.pdf;/Users/felix/paper/2015_Ioffe_Szegedy/Ioffe_Szegedy_2015_Batch Normalization2.pdf;/Users/felix/Zotero/storage/56DUMX6R/1502.html}
}

@online{ismiguzelUnderstandingGradientDescent2023,
  title = {Understanding {{Gradient Descent}} for {{Machine Learning}}},
  author = {Ismiguzel, Idil},
  date = {2023-05-21T15:42:50},
  url = {https://towardsdatascience.com/understanding-gradient-descent-for-machine-learning-246e324c229},
  urldate = {2024-02-23},
  abstract = {A deep dive into Batch, Stochastic, and Mini-Batch Gradient Descent algorithms using Python},
  langid = {english},
  organization = {Medium},
  file = {/Users/felix/Zotero/storage/5RPG743E/Ismiguzel - 2023 - Understanding Gradient Descent for Machine Learnin.jpg;/Users/felix/Zotero/storage/X7M98MFA/understanding-gradient-descent-for-machine-learning-246e324c229.html}
}

@article{jagannathLowTemperatureAsymptotics2017,
  title = {Low {{Temperature Asymptotics}} of {{Spherical Mean Field Spin Glasses}}},
  author = {Jagannath, Aukosh and Tobasco, Ian},
  date = {2017-06-01},
  journaltitle = {Communications in Mathematical Physics},
  shortjournal = {Commun. Math. Phys.},
  volume = {352},
  number = {3},
  pages = {979--1017},
  issn = {1432-0916},
  doi = {10.1007/s00220-017-2864-3},
  abstract = {In this paper, we study the low temperature limit of the spherical Crisanti–Sommers variational problem. We identify the \$\$\{\textbackslash Gamma\}\$\$-limit of the Crisanti–Sommers functionals, thereby establishing a rigorous variational problem for the ground state energy of spherical mixed p-spin glasses. As an application, we compute moderate deviations of the corresponding minimizers in the low temperature limit. In particular, for a large class of models this yields moderate deviations for the overlap distribution as well as providing sharp interpolation estimates between models. We then analyze the ground state energy problem. We show that this variational problem is dual to an obstacle-type problem. This duality is at the heart of our analysis. We present the regularity theory of the optimizers of the primal and dual problems. This culminates in a simple method for constructing a finite dimensional space in which these optimizers live for any model. As a consequence of these results, we unify independent predictions of Crisanti–Leuzzi and Auffinger–Ben Arous regarding the one-step Replica Symmetry Breaking (1RSB) phase in this limit. We find that the “positive replicon eigenvalue” and “pure-like” conditions are together necessary for optimality, but that neither are themselves sufficient, answering a question of Auffinger and Ben Arous in the negative. We end by proving that these conditions completely characterize the 1RSB phase in 2~+~p-spin models.},
  langid = {english},
  file = {/Users/felix/paper/2017_Jagannath_Tobasco/Jagannath_Tobasco_2017_Low Temperature Asymptotics of Spherical Mean Field Spin Glasses.pdf}
}

@article{jaynesInformationTheoryStatistical1957,
  title = {Information {{Theory}} and {{Statistical Mechanics}}},
  author = {Jaynes, E. T.},
  date = {1957-05-15},
  journaltitle = {Physical Review},
  shortjournal = {Phys. Rev.},
  volume = {106},
  number = {4},
  pages = {620--630},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.106.620},
  abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.},
  file = {/Users/felix/paper/1957_Jaynes/Jaynes_1957_Information Theory and Statistical Mechanics.pdf;/Users/felix/Zotero/storage/KP5UJW3J/PhysRev.106.html}
}

@article{jaynesInformationTheoryStatistical1957a,
  title = {Information {{Theory}} and {{Statistical Mechanics}}. {{II}}},
  author = {Jaynes, E. T.},
  date = {1957-10-15},
  journaltitle = {Physical Review},
  shortjournal = {Phys. Rev.},
  volume = {108},
  number = {2},
  pages = {171--190},
  issn = {0031-899X},
  doi = {10.1103/PhysRev.108.171},
  langid = {english}
}

@inproceedings{jinHowEscapeSaddle2017,
  title = {How to {{Escape Saddle Points Efficiently}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M. and Jordan, Michael I.},
  date = {2017-07-17},
  pages = {1724--1732},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/jin17a.html},
  urldate = {2022-10-17},
  abstract = {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost “dimension-free”). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2017_Jin et al/Jin et al_2017_How to Escape Saddle Points Efficiently.pdf;/Users/felix/Zotero/storage/JZEMXI5Z/Jin et al. - 2017 - How to Escape Saddle Points Efficiently.pdf}
}

@book{johnsonAppliedMultivariateStatistical2007,
  title = {Applied {{Multivariate Statistical Analysis}}},
  author = {Johnson, Richard Arnold and Wichern, Dean W.},
  date = {2007},
  edition = {6},
  publisher = {Pearson College Div},
  location = {Upper Saddle River, N.J},
  abstract = {This market-leading book offers a readable introduction to the statistical analysis of multivariate observations. Its overarching goal is to provide readers with the knowledge necessary to make proper interpretations and select appropriate techniques for analyzing multivariate data. Chapter topics include aspects of multivariate analysis, matrix algebra and random vectors, sample geometry and random sampling, the multivariate normal distribution, inferences about a mean vector, comparisons of several multivariate means, multivariate linear regression models, principal components, factor analysis and inference for structured covariance matrices, canonical correlation analysis, and discrimination and classification. For experimental scientists in a variety of disciplines.},
  isbn = {978-0-13-187715-3},
  langid = {english},
  pagetotal = {767}
}

@book{johnsonFeynmanIntegralFeynman2000,
  title = {The {{Feynman Integral}} and {{Feynman}}'s {{Operational Calculus}}},
  author = {Johnson, Gerald and Lapidus, Michel},
  date = {2000-03-19},
  journaltitle = {Physics Today - PHYS TODAY},
  volume = {54},
  abstract = {This research book (which is 790 = 772+(xviii) page-long) provides the most comprehensive mathematical treatment to date of the mathematically beautiful but difficult subjects of the Feynman path integral and Feynman's operational calculus. It is accessible to mathematicians, mathematical physicists and theoretical physicists. Including new results and much material previously available only in the research literature, this book discusses both the mathematics and physics background that motivate the study of the Feynman integral and Feynman's operational calculus, and also provides more detailed proofs of the central results. From reviews of the hardback edition: "I would recommend this book to serious students of the subject." Physics Today "The second one [the main part of the last chapter] is a most welcome presentation of recent extensions and applications of Feynman's approach to a whole range of physical models of major interest...it is here that the power of Feynman's approach of inspiring both mathematicians and physicists is best evidentiated." Zentralblatt Math. Enclosed as a PDF attachment please find the front matter, the introduction, and the detailed table of contents of this book.},
  isbn = {978-0-19-851572-2},
  file = {/Users/felix/paper/2000_Johnson_Lapidus/Johnson_Lapidus_2000_The Feynman Integral and Feynman's Operational Calculus.pdf}
}

@book{johnsonInformationTheoryCentral2004,
  title = {Information {{Theory}} and the {{Central Limit Theorem}}},
  author = {Johnson, Oliver},
  date = {2004-07},
  publisher = {Imperial College Press},
  doi = {10.1142/p341},
  isbn = {978-1-86094-473-6},
  langid = {english},
  file = {/Users/felix/paper/2004_Johnson/Johnson_2004_Information Theory and the Central Limit Theorem.pdf}
}

@article{jonesEfficientGlobalOptimization1998,
  title = {Efficient {{Global Optimization}} of {{Expensive Black-Box Functions}}},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  date = {1998-12-01},
  journaltitle = {Journal of Global Optimization},
  shortjournal = {Journal of Global Optimization},
  volume = {13},
  number = {4},
  pages = {455--492},
  issn = {1573-2916},
  doi = {10.1023/A:1008306431147},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  langid = {english},
  keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
  file = {/Users/felix/paper/1998_Jones et al/Jones et al_1998_Efficient Global Optimization of Expensive Black-Box Functions.pdf}
}

@book{jonesTheoryGeneralisedFunctions1982,
  title = {The {{Theory}} of {{Generalised Functions}}},
  author = {Jones, D. S.},
  date = {1982-02-25},
  publisher = {Cambridge University Press},
  location = {Cambridge ; New York},
  abstract = {Starting from an elementary level Professor Jones discusses generalised functions and their applications. He aims to supply the simplest introduction for those who wish to learn to use generalised functions and there is liberal provision of exercises with which to gain experience. The study of more advanced topics such as partial differential equations, Laplace transforms and ultra-distributions should also make it a valuable source for researchers. The demands placed upon the reader's analytical background are the minimum required to approach this topic. Therefore, by selecting chapters it is possible to construct a short introductory course for students, a final-year option for honours undergraduates or a comprehensive postgraduate course.},
  isbn = {978-0-521-23723-9},
  langid = {english},
  pagetotal = {552},
  file = {/Users/felix/paper/1982_Jones/Jones_1982_The Theory of Generalised Functions.pdf}
}

@inproceedings{kawaguchiBayesianOptimizationExponential2015,
  title = {Bayesian {{Optimization}} with {{Exponential Convergence}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2015/hash/0ebcc77dc72360d0eb8e9504c78d38bd-Abstract.html},
  urldate = {2024-02-14},
  abstract = {This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate.},
  file = {/Users/felix/paper/2015_Kawaguchi et al/[Appendix] Kawaguchi et al_2015_Bayesian Optimization with Exponential Convergence.pdf;/Users/felix/paper/2015_Kawaguchi et al/Kawaguchi et al_2015_Bayesian Optimization with Exponential Convergence.pdf}
}

@inproceedings{kawaguchiDeepLearningPoor2016,
  title = {Deep {{Learning}} without {{Poor Local Minima}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kawaguchi, Kenji},
  date = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2016/hash/f2fc990265c712c49d51a18a32b39f0c-Abstract.html},
  urldate = {2022-09-02},
  abstract = {In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. For an expected loss function of a deep nonlinear neural network, we prove the following statements under the independence assumption adopted from recent work: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) the property of saddle points differs for shallow networks (with three layers) and deeper networks (with more than three layers). Moreover, we prove that the same four statements hold for deep linear neural networks with any depth, any widths and no unrealistic assumptions. As a result, we present an instance, for which we can answer to the following question: how difficult to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima and the property of the saddle points). We note that even though we have advanced the theoretical foundations of deep learning, there is still a gap between theory and practice.},
  file = {/Users/felix/paper/2016_Kawaguchi/[Appendix] Kawaguchi_2016_Deep Learning without Poor Local Minima.pdf;/Users/felix/paper/2016_Kawaguchi/Kawaguchi_2016_Deep Learning without Poor Local Minima.pdf}
}

@book{kayFundamentalsStatisticalSignal1993,
  title = {Fundamentals of Statistical Signal Processing: Estimation Theory},
  shorttitle = {Fundamentals of Statistical Signal Processing},
  author = {Kay, Steven M.},
  date = {1993-02},
  publisher = {Prentice-Hall, Inc.},
  location = {USA},
  isbn = {978-0-13-345711-7},
  pagetotal = {595},
  file = {/Users/felix/paper/1993_Kay/Kay_1993_Fundamentals of statistical signal processing.pdf}
}

@online{khaledTuningFreeStochasticOptimization2024,
  title = {Tuning-{{Free Stochastic Optimization}}},
  author = {Khaled, Ahmed and Jin, Chi},
  date = {2024-03-18},
  eprint = {2402.07793},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2402.07793},
  abstract = {Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is sufficiently well-behaved. For the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of SGD that matches the best-known high-probability convergence rate for tuned SGD at only an additional polylogarithmic cost. However, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned SGD with high probability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/paper/2024_Khaled_Jin/Khaled_Jin_2024_Tuning-Free Stochastic Optimization.pdf;/Users/felix/Zotero/storage/Z4GDEXVU/2402.html}
}

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Learning Representations}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2015},
  eprint = {1412.6980},
  eprinttype = {arXiv},
  location = {San Diego},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  eventtitle = {{{ICLR}}},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/paper/2015_Kingma_Ba/Kingma_Ba_2015_Adam.pdf;/Users/felix/Zotero/storage/6WJBPWQT/1412.html}
}

@book{klenkeProbabilityTheoryComprehensive2014,
  title = {Probability {{Theory}}: {{A Comprehensive Course}}},
  shorttitle = {Probability {{Theory}}},
  author = {Klenke, Achim},
  date = {2014},
  series = {Universitext},
  publisher = {Springer},
  location = {London},
  doi = {10.1007/978-1-4471-5361-0},
  isbn = {978-1-4471-5360-3 978-1-4471-5361-0},
  langid = {english},
  keywords = {Brownian Motion,Integration Theory,Limit Theorems,Markov Chains,Martingales,Measure Theory,Percolation,Poisson Point Process,Statistical Physics,Stochastic Differential Equations,Stochastic Integral,Stochastic Processes},
  file = {/Users/felix/paper/2014_Klenke/Klenke_2014_Probability Theory.pdf}
}

@inproceedings{kohlerExponentialConvergenceRates2019,
  title = {Exponential Convergence Rates for {{Batch Normalization}}: {{The}} Power of Length-Direction Decoupling in Non-Convex Optimization},
  shorttitle = {Exponential Convergence Rates for {{Batch Normalization}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kohler, Jonas and Daneshmand, Hadi and Lucchi, Aurelien and Hofmann, Thomas and Zhou, Ming and Neymeyr, Klaus},
  date = {2019-04-11},
  eprint = {1805.11604},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  pages = {806--815},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/kohler19a.html},
  urldate = {2023-10-16},
  abstract = {Normalization techniques such as Batch Normalization have been applied very successfully for training deep neural networks. Yet, despite its apparent empirical benefits, the reasons behind the success of Batch Normalization are mostly hypothetical. We here aim to provide a more thorough theoretical understanding from a classical optimization perspective. Our main contribution towards this goal is the identification of various problem instances in the realm of machine learning where Batch Normalization can provably accelerate optimization. We argue that this acceleration is due to the fact that Batch Normalization splits the optimization task into optimizing length and direction of the parameters separately. This allows gradient-based methods to leverage a favourable global structure in the loss landscape that we prove to exist in Learning Halfspace problems and neural network training with Gaussian inputs. We thereby turn Batch Normalization from an effective practical heuristic into a provably converging algorithm for these settings. Furthermore, we substantiate our analysis with empirical evidence that suggests the validity of our theoretical results in a broader context.},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/paper/2019_Kohler et al/[ArXiv] Kohler et al_2019_Exponential convergence rates for Batch Normalization.pdf;/Users/felix/paper/2019_Kohler et al/[Supplemental] Kohler et al_2019_Exponential convergence rates for Batch Normalization.pdf;/Users/felix/paper/2019_Kohler et al/Kohler et al_2019_Exponential convergence rates for Batch Normalization.pdf}
}

@report{krizhevskyLearningMultipleLayers2009,
  title = {Learning Multiple Layers of Features from Tiny Images},
  author = {Krizhevsky, Alex},
  date = {2009},
  url = {https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf},
  urldate = {2024-05-21},
  file = {/Users/felix/Zotero/storage/W25N4ZA2/1370861707142497920.html}
}

@article{kushnerNewMethodLocating1964,
  title = {A {{New Method}} of {{Locating}} the {{Maximum Point}} of an {{Arbitrary Multipeak Curve}} in the {{Presence}} of {{Noise}}},
  author = {Kushner, H. J.},
  date = {1964-03-01},
  journaltitle = {Journal of Basic Engineering},
  shortjournal = {Journal of Basic Engineering},
  volume = {86},
  number = {1},
  pages = {97--106},
  issn = {0021-9223},
  doi = {10.1115/1.3653121},
  abstract = {A versatile and practical method of searching a parameter space is presented. Theoretical and experimental results illustrate the usefulness of the method for such problems as the experimental optimization of the performance of a system with a very general multipeak performance function when the only available information is noise-distributed samples of the function. At present, its usefulness is restricted to optimization with respect to one system parameter. The observations are taken sequentially; but, as opposed to the gradient method, the observation may be located anywhere on the parameter interval. A sequence of estimates of the location of the curve maximum is generated. The location of the next observation may be interpreted as the location of the most likely competitor (with the current best estimate) for the location of the curve maximum. A Brownian motion stochastic process is selected as a model for the unknown function, and the observations are interpreted with respect to the model. The model gives the results a simple intuitive interpretation and allows the use of simple but efficient sampling procedures. The resulting process possesses some powerful convergence properties in the presence of noise; it is nonparametric and, despite its generality, is efficient in the use of observations. The approach seems quite promising as a solution to many of the problems of experimental system optimization.},
  file = {/Users/felix/paper/1964_Kushner/Kushner_1964_A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in.pdf;/Users/felix/Zotero/storage/SHIJUFR9/A-New-Method-of-Locating-the-Maximum-Point-of-an.html}
}

@inproceedings{lacotteOptimalRandomizedFirstOrder2020,
  title = {Optimal {{Randomized First-Order Methods}} for {{Least-Squares Problems}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Lacotte, Jonathan and Pilanci, Mert},
  date = {2020-11-21},
  pages = {5587--5597},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/lacotte20a.html},
  urldate = {2023-11-09},
  abstract = {We provide an exact analysis of a class of randomized algorithms for solving overdetermined least-squares problems. We consider first-order methods, where the gradients are pre-conditioned by an approximation of the Hessian, based on a subspace embedding of the data matrix. This class of algorithms encompasses several randomized methods among the fastest solvers for least-squares problems. We focus on two classical embeddings, namely, Gaussian projections and subsampled randomized Hadamard transforms (SRHT). Our key technical innovation is the derivation of the limiting spectral density of SRHT embeddings. Leveraging this novel result, we derive the family of normalized orthogonal polynomials of the SRHT density and we find the optimal pre-conditioned first-order method along with its rate of convergence. Our analysis of Gaussian embeddings proceeds similarly, and leverages classical random matrix theory results. In particular, we show that for a given sketch size, SRHT embeddings exhibits a faster rate of convergence than Gaussian embeddings. Then, we propose a new algorithm by optimizing the computational complexity over the choice of the sketching dimension. To our knowledge, our resulting algorithm yields the best known complexity for solving least-squares problems with no condition number dependence.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2020_Lacotte_Pilanci/[Supplemental] Lacotte_Pilanci_2020_Optimal Randomized First-Order Methods for Least-Squares Problems.pdf;/Users/felix/paper/2020_Lacotte_Pilanci/Lacotte_Pilanci_2020_Optimal Randomized First-Order Methods for Least-Squares Problems.pdf}
}

@article{langIsotropicGaussianRandom2015,
  title = {Isotropic {{Gaussian}} Random Fields on the Sphere: {{Regularity}}, Fast Simulation and Stochastic Partial Differential Equations},
  shorttitle = {Isotropic {{Gaussian}} Random Fields on the Sphere},
  author = {Lang, Annika and Schwab, Christoph},
  date = {2015-12},
  journaltitle = {The Annals of Applied Probability},
  volume = {25},
  number = {6},
  eprint = {1305.1170},
  eprinttype = {arXiv},
  eprintclass = {math.PR},
  pages = {3047--3094},
  publisher = {Institute of Mathematical Statistics},
  issn = {1050-5164, 2168-8737},
  doi = {10.1214/14-AAP1067},
  abstract = {Isotropic Gaussian random fields on the sphere are characterized by Karhunen–Loève expansions with respect to the spherical harmonic functions and the angular power spectrum. The smoothness of the covariance is connected to the decay of the angular power spectrum and the relation to sample Hölder continuity and sample differentiability of the random fields is discussed. Rates of convergence of their finitely truncated Karhunen–Loève expansions in terms of the covariance spectrum are established, and algorithmic aspects of fast sample generation via fast Fourier transforms on the sphere are indicated. The relevance of the results on sample regularity for isotropic Gaussian random fields and the corresponding lognormal random fields on the sphere for several models from environmental sciences is indicated. Finally, the stochastic heat equation on the sphere driven by additive, isotropic Wiener noise is considered, and strong convergence rates for spectral discretizations based on the spherical harmonic functions are proven.},
  keywords = {33C55,41A25,60G15,60G17,60G60,60H15,60H35,65C30,65N30,Gaussian random fields,isotropic random fields,Karhunen–Loève expansion,Kolmogorov–Chentsov theorem,sample differentiability,sample Hölder continuity,spectral Galerkin methods,spherical harmonic functions,Stochastic partial differential equations,strong convergence rates},
  file = {/Users/felix/paper/2015_Lang_Schwab/Lang_Schwab_2015_Isotropic Gaussian random fields on the sphere.pdf}
}

@dataset{lecunMNISTDATABASEHandwritten2010,
  title = {{{THE MNIST DATABASE}} of Handwritten Digits},
  author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J.C.},
  date = {2010},
  url = {http://yann.lecun.com/exdb/mnist/},
  urldate = {2024-01-24},
  abstract = {The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from MNIST. The digits have been size-normalized and centered in a fixed-size image},
  file = {/Users/felix/Zotero/storage/BD62568A/mnist.html}
}

@inproceedings{ledouxConcentrationMeasureLogarithmic1999,
  title = {Concentration of Measure and Logarithmic {{Sobolev}} Inequalities},
  booktitle = {Séminaire de {{Probabilités XXXIII}}},
  author = {Ledoux, Michel},
  editor = {Azéma, Jacques and Émery, Michel and Ledoux, Michel and Yor, Marc},
  date = {1999},
  pages = {120--216},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/BFb0096511},
  isbn = {978-3-540-48407-3},
  langid = {english},
  keywords = {Dirichlet Form,Gaussian Measure,Isoperimetric Inequality,Lipschitz Function,Logarithmic Sobolev Inequality},
  file = {/Users/felix/paper/1999_Ledoux/Ledoux_1999_Concentration of measure and logarithmic Sobolev inequalities.pdf}
}

@inproceedings{leeDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  date = {2018},
  eprint = {1711.00165},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  location = {Vancouver, Canada},
  url = {https://openreview.net/forum?id=B1EA-M-0Z},
  urldate = {2023-04-03},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide, deep, networks and GPs with a particular covariance function. We further develop a computationally efficient pipeline to compute this covariance function. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We observe that the trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and that the GP-based predictions typically outperform those of finite-width networks. Finally we connect the prior distribution over weights and variances in our GP formulation to the recent development of signal propagation in random neural networks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/felix/paper/2018_Lee et al/Lee et al_2018_Deep Neural Networks as Gaussian Processes.pdf}
}

@inproceedings{leeGradientDescentOnly2016,
  title = {Gradient {{Descent Only Converges}} to {{Minimizers}}},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  date = {2016-06-06},
  eprint = {1602.04915},
  eprinttype = {arXiv},
  eprintclass = {stat.ML},
  pages = {1246--1257},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v49/lee16.html},
  urldate = {2023-06-20},
  abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  file = {/Users/felix/paper/2016_Lee et al/Lee et al_2016_Gradient Descent Only Converges to Minimizers.pdf}
}

@article{lessardAnalysisDesignOptimization2016,
  title = {Analysis and {{Design}} of {{Optimization Algorithms}} via {{Integral Quadratic Constraints}}},
  author = {Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  date = {2016-01-01},
  journaltitle = {SIAM Journal on Optimization},
  shortjournal = {SIAM J. Optim.},
  volume = {26},
  number = {1},
  pages = {57--95},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1052-6234},
  doi = {10.1137/15M1009597},
  abstract = {This paper develops a new framework to analyze and design iterative optimization algorithms built on the notion of integral quadratic constraints (IQCs) from robust control theory. IQCs provide sufficient conditions for the stability of complicated interconnected systems, and these conditions can be checked by semidefinite programming. We discuss how to adapt IQC theory to study optimization algorithms, proving new inequalities about convex functions and providing a version of IQC theory adapted for use by optimization researchers. Using these inequalities, we derive numerical upper bounds on convergence rates for the Gradient method, the Heavy-ball method, Nesterov's accelerated method, and related variants by solving small, simple semidefinite programming problems. We also briefly show how these techniques can be used to search for optimization algorithms with desired performance characteristics, establishing a new methodology for algorithm design.},
  keywords = {90C22,90C25,90C30,93C10,93D99,control theory,convex optimization,first-order methods,Heavy-ball method,integral quadratic constraints,Nesterov's method,proximal gradient methods,semidefinite programming},
  file = {/Users/felix/paper/2016_Lessard et al/Lessard et al_2016_Analysis and Design of Optimization Algorithms via Integral Quadratic.pdf}
}

@inproceedings{liConvergenceStochasticGradient2019a,
  title = {On the {{Convergence}} of {{Stochastic Gradient Descent}} with {{Adaptive Stepsizes}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Li, Xiaoyu and Orabona, Francesco},
  date = {2019-04-11},
  pages = {983--992},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/li19c.html},
  urldate = {2023-08-11},
  abstract = {Stochastic gradient descent is the method of choice for large scale optimization of machine learning objective functions. Yet, its performance is greatly variable and heavily depends on the choice of the stepsizes. This has motivated a large body of research on adaptive stepsizes. However, there is currently a gap in our theoretical understanding of these methods, especially in the non-convex setting. In this paper, we start closing this gap: we theoretically analyze in the convex and non-convex settings a generalized version of the AdaGrad stepsizes. We show sufficient conditions for these stepsizes to achieve almost sure asymptotic convergence of the gradients to zero, proving the first guarantee for generalized AdaGrad stepsizes in the non-convex setting. Moreover, we show that these stepsizes allow to automatically adapt to the level of noise of the stochastic gradients in both the convex and non-convex settings, interpolating between O(1/T) and O(1/sqrt(T)), up to logarithmic terms.},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/paper/2019_Li_Orabona/[Supplemental] Li_Orabona_2019_On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes.pdf;/Users/felix/paper/2019_Li_Orabona/Li_Orabona_2019_On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes2.pdf}
}

@book{lindellIntroductionModernCryptography2007,
  title = {Introduction to {{Modern Cryptography}}: {{Principles}} and {{Protocols}}},
  shorttitle = {Introduction to {{Modern Cryptography}}},
  author = {Lindell, Yehuda, Jonathan Katz},
  date = {2007-08-31},
  publisher = {{Chapman and Hall/CRC}},
  location = {New York},
  doi = {10.1201/9781420010756},
  abstract = {Cryptography plays a key role in ensuring the privacy and integrity of data and the security of computer networks. Introduction to Modern Cryptography provides a rigorous yet accessible treatment of modern cryptography, with a focus on formal definitions, precise assumptions, and rigorous proofs.The authors introduce the core principles of},
  isbn = {978-0-429-14380-9},
  pagetotal = {552}
}

@online{liSSRGDSimpleStochastic2019,
  title = {{{SSRGD}}: {{Simple Stochastic Recursive Gradient Descent}} for {{Escaping Saddle Points}}},
  shorttitle = {{{SSRGD}}},
  author = {Li, Zhize},
  date = {2019-06-20},
  eprint = {1904.09265},
  eprinttype = {arXiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.1904.09265},
  abstract = {We analyze stochastic gradient algorithms for optimizing nonconvex problems. In particular, our goal is to find local minima (second-order stationary points) instead of just finding first-order stationary points which may be some bad unstable saddle points. We show that a simple perturbed version of stochastic recursive gradient descent algorithm (called SSRGD) can find an \$(\textbackslash epsilon,\textbackslash delta)\$-second-order stationary point with \$\textbackslash widetilde\{O\}(\textbackslash sqrt\{n\}/\textbackslash epsilon\textasciicircum 2 + \textbackslash sqrt\{n\}/\textbackslash delta\textasciicircum 4 + n/\textbackslash delta\textasciicircum 3)\$ stochastic gradient complexity for nonconvex finite-sum problems. As a by-product, SSRGD finds an \$\textbackslash epsilon\$-first-order stationary point with \$O(n+\textbackslash sqrt\{n\}/\textbackslash epsilon\textasciicircum 2)\$ stochastic gradients. These results are almost optimal since Fang et al. [2018] provided a lower bound \$\textbackslash Omega(\textbackslash sqrt\{n\}/\textbackslash epsilon\textasciicircum 2)\$ for finding even just an \$\textbackslash epsilon\$-first-order stationary point. We emphasize that SSRGD algorithm for finding second-order stationary points is as simple as for finding first-order stationary points just by adding a uniform perturbation sometimes, while all other algorithms for finding second-order stationary points with similar gradient complexity need to combine with a negative-curvature search subroutine (e.g., Neon2 [Allen-Zhu and Li, 2018]). Moreover, the simple SSRGD algorithm gets a simpler analysis. Besides, we also extend our results from nonconvex finite-sum problems to nonconvex online (expectation) problems, and prove the corresponding convergence results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/paper/2019_Li/Li_2019_SSRGD.pdf;/Users/felix/Zotero/storage/L3TBY39G/1904.html}
}

@inproceedings{liuLinearityLargeNonlinear2020,
  title = {On the Linearity of Large Non-Linear Models: When and Why the Tangent Kernel Is Constant},
  shorttitle = {On the Linearity of Large Non-Linear Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
  date = {2020},
  volume = {33},
  eprint = {2010.01092},
  eprinttype = {arXiv},
  eprintclass = {cs.LG},
  pages = {15954--15964},
  publisher = {Curran Associates, Inc.},
  location = {Vancouver, Canada},
  url = {https://proceedings.neurips.cc/paper/2020/hash/b7ae8fecf15b8b6c3c69eceae636d203-Abstract.html},
  urldate = {2023-03-27},
  abstract = {The goal of this work is to shed light on the remarkable phenomenon of "transition to linearity" of certain neural networks as their width approaches infinity. We show that the "transition to linearity'' of the model and, equivalently, constancy of the (neural) tangent kernel (NTK) result from the scaling properties of the norm of the Hessian matrix of the network as a function of the network width. We present a general framework for understanding the constancy of the tangent kernel via Hessian scaling applicable to the standard classes of neural networks. Our analysis provides a new perspective on the phenomenon of constant tangent kernel, which is different from the widely accepted "lazy training''. Furthermore, we show that the "transition to linearity" is not a general property of wide neural networks and does not hold when the last layer of the network is non-linear.  It is also not necessary for successful optimization by gradient descent.},
  file = {/Users/felix/paper/2020_Liu et al/[Arxiv] Liu et al_2020_On the linearity of large non-linear models.pdf;/Users/felix/paper/2020_Liu et al/Liu et al_2020_On the linearity of large non-linear models.pdf}
}

@online{liuSteinVariationalGradient2019,
  title = {Stein {{Variational Gradient Descent}}: {{A General Purpose Bayesian Inference Algorithm}}},
  shorttitle = {Stein {{Variational Gradient Descent}}},
  author = {Liu, Qiang and Wang, Dilin},
  date = {2019-09-09},
  eprint = {1608.04471},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1608.04471},
  abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/paper/2019_Liu_Wang/Liu_Wang_2019_Stein Variational Gradient Descent.pdf;/Users/felix/Zotero/storage/N45LM2NS/1608.html}
}

@thesis{lizottePracticalBayesianOptimization2008,
  type = {phdthesis},
  title = {Practical Bayesian Optimization},
  author = {Lizotte, Daniel James},
  date = {2008},
  institution = {University of Alberta},
  location = {Alberta, Canada},
  abstract = {Global optimization of non-convex functions over real vector spaces is a problem of widespread theoretical and practical interest. In the past fifty years, research in global optimization has produced many important approaches including Lipschitz optimization, simulated annealing, homotopy methods, genetic algorithms, and Bayesian response-surface methods. This work examines the last of these approaches. The Bayesian response-surface approach to global optimization maintains a posterior model of the function being optimized by combining a prior over functions with accumulating function evaluations. The model is then used to compute which point the method should acquire next in its search for the optimum of the function. Bayesian methods can be some of the most efficient approaches to optimization in terms of the number of function evaluations required, but they have significant drawbacks: Current approaches are needlessly data-inefficient, approximations to the Bayes-optimal acquisition criterion are poorly studied, and current approaches do not take advantage of the small-scale properties of differentiable functions near local optima. This work addresses each of these problems to make Bayesian methods more widely applicable.},
  pagetotal = {168},
  file = {/Users/felix/paper/2008_Lizotte/Lizotte_2008_Practical bayesian optimization.pdf}
}

@article{maAdequacyUntunedWarmup2021,
  title = {On the {{Adequacy}} of {{Untuned Warmup}} for {{Adaptive Optimization}}},
  author = {Ma, Jerry and Yarats, Denis},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {10},
  pages = {8828--8836},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i10.17069},
  abstract = {Adaptive optimization algorithms such as Adam (Kingma and Ba, 2014) are widely used in deep learning. The stability of such algorithms is often improved with a warmup schedule for the learning rate. Motivated by the difficulty of choosing and tuning warmup schedules, recent work proposes automatic variance rectification of Adam's adaptive learning rate, claiming that this rectified approach ("RAdam") surpasses the vanilla Adam algorithm and reduces the need for expensive tuning of Adam with warmup. In this work, we refute this analysis and provide an alternative explanation for the necessity of warmup based on the magnitude of the update term, which is of greater relevance to training stability. We then provide some "rule-of-thumb" warmup schedules, and we demonstrate that simple untuned warmup of Adam performs more-or-less identically to RAdam in typical practical settings. We conclude by suggesting that practitioners stick to linear warmup with Adam, with a sensible default being linear warmup over 2 / (1 - β₂) training iterations.},
  issue = {10},
  langid = {english},
  keywords = {(Deep) Neural Network Algorithms},
  file = {/Users/felix/paper/2021_Ma_Yarats/Ma_Yarats_2021_On the Adequacy of Untuned Warmup for Adaptive Optimization.pdf}
}

@article{mackayComparisonApproximateMethods1999,
  title = {Comparison of {{Approximate Methods}} for {{Handling Hyperparameters}}},
  author = {MacKay, David J. C.},
  date = {1999-07},
  journaltitle = {Neural Computation},
  volume = {11},
  number = {5},
  pages = {1035--1068},
  issn = {0899-7667},
  doi = {10.1162/089976699300016331},
  abstract = {I examine two approximate methods for computational implementation of Bayesian hierarchical models, that is, models that include unknown hyperparameters such as regularization constants and noise levels. In the evidence framework, the model parameters are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized hyperparameters are used to define a gaussian approximation to the posterior distribution. In the alternative MAP method, the true posterior probability is found by integrating over the hyperparameters. The true posterior is then maximized over the model parameters, and a gaussian approximation is made. The similarities of the two approaches and their relative merits are discussed, and comparisons are made with the ideal hierarchical Bayesian solution. In moderately ill-posed problems, integration over hyperparameters yields a probability distribution with a skew peak, which causes signifi-cant biases to arise in the MAP method. In contrast, the evidence framework is shown to introduce negligible predictive error under straightforward conditions. General lessons are drawn concerning inference in many dimensions.},
  eventtitle = {Neural {{Computation}}},
  file = {/Users/felix/paper/1999_MacKay/MacKay_1999_Comparison of Approximate Methods for Handling Hyperparameters.pdf;/Users/felix/Zotero/storage/S7NCCGWB/6790802.html}
}

@thesis{maillardFundamentalLimitsHighdimensional2021,
  type = {phdthesis},
  title = {Fundamental Limits of High-Dimensional Estimation : A Stroll between Statistical Physics, Probability and Random Matrix Theory},
  shorttitle = {Fundamental Limits of High-Dimensional Estimation},
  author = {Maillard, Antoine},
  date = {2021-08-30},
  institution = {Université Paris sciences et lettres},
  url = {https://theses.hal.science/tel-03474190},
  urldate = {2024-02-20},
  abstract = {The past decade saw an intensification of the deluge of data available to learning algorithms, which allowed for the development of modern artificial intelligence techniques. These methods rely on the optimization of a very large number of internal parameters using gigantic amounts of data, and now provide state-of-the-art algorithms for tasks as diverse as image classification, natural language processing, or speech recognition, and regularly achieve super-human performances. This exacerbated research efforts to build a mathematically sound theory of data science able to explain the extraordinary efficiency of these procedures, and has led to a surge of interest for high-dimensional statistics (i.e. when the amount of data and the number of parameters are both very large). In this dissertation we analyze a few pieces of this immense puzzle through the prism of statistical physics, borrowing also often from probability and random matrix theory, and we propose three approaches to the high-dimensional learning problem. In the first one, we revisit high-temperature expansions, an archetypal method of statistical physics. We show how this classical approach is related to modern algorithms, and use it to pave the way towards an exact theory of extensive-rank matrix factorization. Our theory leverages the intimate relation between the statistical physics of disordered systems and high-dimensional statistics, a connection which has been a growing subject of research since the 1990s. Our second approach pushes further this correspondence as we leverage heuristic tools of theoretical physics such as the replica method, along with modern probabilistic methods and message-passing algorithms, to describe the fundamental limits of a wide class of high-dimensional learning problems. We apply our analysis to neural networks, phase retrieval, and to study the influence of data structure on the optimal learning procedures. In a third part, we take an alternative route and consider a topological approach to the problem of learning in high dimension. Using tools of random differential geometry and random matrix theory, we prove exact formulas describing the structure of the high-dimensional landscapes optimized by learning algorithms.},
  langid = {english},
  file = {/Users/felix/paper/2021_Maillard/Maillard_2021_Fundamental limits of high-dimensional estimation.pdf}
}

@online{maillardPhaseRetrievalHigh2020,
  title = {Phase Retrieval in High Dimensions: {{Statistical}} and Computational Phase Transitions},
  shorttitle = {Phase Retrieval in High Dimensions},
  author = {Maillard, Antoine and Loureiro, Bruno and Krzakala, Florent and Zdeborová, Lenka},
  date = {2020-10-23},
  eprint = {2006.05228},
  eprinttype = {arXiv},
  eprintclass = {cond-mat, stat},
  doi = {10.48550/arXiv.2006.05228},
  abstract = {We consider the phase retrieval problem of reconstructing a \$n\$-dimensional real or complex signal \$\textbackslash mathbf\{X\}\textasciicircum\{\textbackslash star\}\$ from \$m\$ (possibly noisy) observations \$Y\_\textbackslash mu = | \textbackslash sum\_\{i=1\}\textasciicircum n \textbackslash Phi\_\{\textbackslash mu i\} X\textasciicircum\{\textbackslash star\}\_i/\textbackslash sqrt\{n\}|\$, for a large class of correlated real and complex random sensing matrices \$\textbackslash mathbf\{\textbackslash Phi\}\$, in a high-dimensional setting where \$m,n\textbackslash to\textbackslash infty\$ while \$\textbackslash alpha = m/n=\textbackslash Theta(1)\$. First, we derive sharp asymptotics for the lowest possible estimation error achievable statistically and we unveil the existence of sharp phase transitions for the weak- and full-recovery thresholds as a function of the singular values of the matrix \$\textbackslash mathbf\{\textbackslash Phi\}\$. This is achieved by providing a rigorous proof of a result first obtained by the replica method from statistical mechanics. In particular, the information-theoretic transition to perfect recovery for full-rank matrices appears at \$\textbackslash alpha=1\$ (real case) and \$\textbackslash alpha=2\$ (complex case). Secondly, we analyze the performance of the best-known polynomial time algorithm for this problem -- approximate message-passing -- establishing the existence of a statistical-to-algorithmic gap depending, again, on the spectral properties of \$\textbackslash mathbf\{\textbackslash Phi\}\$. Our work provides an extensive classification of the statistical and algorithmic thresholds in high-dimensional phase retrieval for a broad class of random matrices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/Users/felix/paper/2020_Maillard et al/Maillard et al_2020_Phase retrieval in high dimensions.pdf;/Users/felix/Zotero/storage/Y2BANZJE/2006.html}
}

@article{majumdarExtremeValueStatistics2020,
  title = {Extreme Value Statistics of Correlated Random Variables: {{A}} Pedagogical Review},
  shorttitle = {Extreme Value Statistics of Correlated Random Variables},
  author = {Majumdar, Satya N. and Pal, Arnab and Schehr, Grégory},
  date = {2020-01-22},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  series = {Extreme Value Statistics of Correlated Random Variables: {{A}} Pedagogical Review},
  volume = {840},
  pages = {1--32},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2019.10.005},
  abstract = {Extreme value statistics (EVS) concerns the study of the statistics of the maximum or the minimum of a set of random variables. This is an important problem for any time-series and has applications in climate, finance, sports, all the way to physics of disordered systems where one is interested in the statistics of the ground state energy. While the EVS of ‘uncorrelated’ variables are well understood, little is known for strongly correlated random variables. Only recently this subject has gained much importance both in statistical physics and in probability theory. In this review, we will first recall the classical EVS for uncorrelated variables and discuss the three universality classes of extreme value limiting distribution, known as the Gumbel, Fréchet and Weibull distribution. We then show that, for weakly correlated random variables with a finite correlation length/time, the limiting extreme value distribution can still be inferred from that of the uncorrelated variables using a renormalization group-like argument. Finally, we consider the most interesting examples of strongly correlated variables for which there are very few exact results for the EVS. We discuss few examples of such strongly correlated systems (such as the Brownian motion and the eigenvalues of a random matrix) where some analytical progress can be made. We also discuss other observables related to extremes, such as the density of near-extreme events, time at which an extreme value occurs, order and record statistics, etc.},
  langid = {english},
  file = {/Users/felix/paper/2020_Majumdar et al/Majumdar et al_2020_Extreme value statistics of correlated random variables.pdf;/Users/felix/Zotero/storage/43NS7XHP/S0370157319303291.html}
}

@article{majumdarLargeDeviationsMaximum2009,
  title = {Large {{Deviations}} of the {{Maximum Eigenvalue}} for {{Wishart}} and {{Gaussian Random Matrices}}},
  author = {Majumdar, Satya N. and Vergassola, Massimo},
  date = {2009-02-12},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {102},
  number = {6},
  pages = {060601},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.102.060601},
  abstract = {We present a Coulomb gas method to calculate analytically the probability of rare events where the maximum eigenvalue of a random matrix is much larger than its typical value. The large deviation function that characterizes this probability is computed explicitly for Wishart and Gaussian ensembles. The method is general and applies to other related problems, e.g., the joint large deviation function for large fluctuations of top eigenvalues. Our results are relevant to widely employed data compression techniques, namely, the principal components analysis. Analytical predictions are verified by extensive numerical simulations.},
  file = {/Users/felix/paper/2009_Majumdar_Vergassola/Majumdar_Vergassola_2009_Large Deviations of the Maximum Eigenvalue for Wishart and Gaussian Random.pdf;/Users/felix/Zotero/storage/4R89DVY4/PhysRevLett.102.html}
}

@article{majumdarTopEigenvalueRandom2014,
  title = {Top Eigenvalue of a Random Matrix: Large Deviations and Third Order Phase Transition},
  shorttitle = {Top Eigenvalue of a Random Matrix},
  author = {Majumdar, Satya N. and Schehr, Grégory},
  date = {2014-01},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2014},
  number = {1},
  pages = {P01012},
  publisher = {{IOP Publishing and SISSA}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2014/01/P01012},
  abstract = {We study the fluctuations of the largest eigenvalue λmax of N × N random matrices in the limit of large N. The main focus is on Gaussian β ensembles, including in particular the Gaussian orthogonal (β = 1), unitary (β = 2) and symplectic (β = 4) ensembles. The probability density function (PDF) of λmax consists, for large N, of a central part described by Tracy–Widom distributions flanked, on both sides, by two large deviation tails. While the central part characterizes the typical fluctuations of λmax—of order —the large deviation tails are instead associated with extremely rare fluctuations—of order . Here we review some recent developments in the theory of these extremely rare events using a Coulomb gas approach. We discuss in particular the third order phase transition which separates the left tail from the right tail, a transition akin to the so-called Gross–Witten–Wadia phase transition found in 2-d lattice quantum chromodynamics. We also discuss the occurrence of similar third order transitions in various physical problems, including non-intersecting Brownian motions, conductance fluctuations in mesoscopic physics and entanglement in a bipartite system.},
  langid = {english},
  file = {/Users/felix/paper/2014_Majumdar_Schehr/Majumdar_Schehr_2014_Top eigenvalue of a random matrix.pdf}
}

@article{mandtStochasticGradientDescent2017,
  title = {Stochastic Gradient Descent as Approximate Bayesian Inference},
  author = {Mandt, Stephan and Hoffman, Matthew D. and Blei, David M.},
  date = {2017},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {JMLR},
  volume = {18},
  url = {https://jmlr.org/papers/v18/17-214.html},
  urldate = {2023-10-23},
  file = {/Users/felix/paper/2017_Mandt et al/Mandt et al_2017_Stochastic gradient descent as approximate bayesian inference2.pdf}
}

@book{marinucciRandomFieldsSphere2011,
  title = {Random {{Fields}} on the {{Sphere}}: {{Representation}}, {{Limit Theorems}} and {{Cosmological Applications}}},
  shorttitle = {Random {{Fields}} on the {{Sphere}}},
  author = {Marinucci, Domenico and Peccati, Giovanni},
  date = {2011-10-10},
  edition = {Illustrated edition},
  publisher = {Cambridge University Press},
  location = {Cambridge, UK ; New York},
  abstract = {Random Fields on the Sphere presents a comprehensive analysis of isotropic spherical random fields. The main emphasis is on tools from harmonic analysis, beginning with the representation theory for the group of rotations SO(3). Many recent developments on the method of moments and cumulants for the analysis of Gaussian subordinated fields are reviewed. This background material is used to analyse spectral representations of isotropic spherical random fields and then to investigate in depth the properties of associated harmonic coefficients. Properties and statistical estimation of angular power spectra and polyspectra are addressed in full. The authors are strongly motivated by cosmological applications, especially the analysis of cosmic microwave background (CMB) radiation data, which has initiated a challenging new field of mathematical and statistical research. Ideal for mathematicians and statisticians interested in applications to cosmology, it will also interest cosmologists and mathematicians working in group representations, stochastic calculus and spherical wavelets.},
  isbn = {978-0-521-17561-6},
  langid = {english},
  pagetotal = {356},
  file = {/Users/felix/paper/2011_Marinucci_Peccati/Marinucci_Peccati_2011_Random Fields on the Sphere.pdf}
}

@article{matheronIntrinsicRandomFunctions1973,
  title = {The Intrinsic Random Functions and Their Applications},
  author = {Matheron, G.},
  date = {1973-12},
  journaltitle = {Advances in Applied Probability},
  volume = {5},
  number = {3},
  pages = {439--468},
  publisher = {Cambridge University Press},
  issn = {0001-8678, 1475-6064},
  doi = {10.2307/1425829},
  abstract = {The intrinsic random functions (IRF) are a particular case of the Guelfand generalized processes with stationary increments. They constitute a much wider class than the stationary RF, and are used in practical applications for representing non-stationary phenomena. The most important topics are: existence of a generalized covariance (GC) for which statistical inference is possible from a unique realization; theory of the best linear intrinsic estimator (BLIE) used for contouring and estimating problems; the turning bands method for simulating IRF; and the models with polynomial GC, for which statistical inference may be performed by automatic procedures.},
  langid = {english},
  keywords = {GENERALIZED COVARIANCES,GUELFAND GENERALIZED STOCHASTIC PROCESS,INTRINSIC RANDOM FUNCTION,POLYNOMIAL COVARIANCE,TURNING BANDS METHOD},
  file = {/Users/felix/paper/1973_Matheron/Matheron_1973_The intrinsic random functions and their applications.pdf}
}

@article{meiMeanFieldView2018,
  title = {A Mean Field View of the Landscape of Two-Layer Neural Networks},
  author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  date = {2018-08-14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {115},
  number = {33},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1806579115},
  abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that –in a suitable scaling limit– SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearlyideal generalization error. This description allows to ‘average-out’ some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
  langid = {english},
  file = {/Users/felix/Zotero/storage/I62LH4HD/Mei et al. - 2018 - A mean field view of the landscape of two-layer ne.pdf}
}

@article{mezardReplicaFieldTheory1991,
  title = {Replica Field Theory for Random Manifolds},
  author = {Mézard, Marc and Parisi, Giorgio},
  date = {1991-06-01},
  journaltitle = {Journal de Physique I},
  shortjournal = {J. Phys. I France},
  volume = {1},
  number = {6},
  pages = {809--836},
  publisher = {EDP Sciences},
  issn = {1155-4304, 1286-4862},
  doi = {10.1051/jp1:1991171},
  abstract = {Journal de Physique I, Journal de Physique Archives représente une mine d informations facile à consulter sur la manière dont la physique a été publiée depuis 1872.},
  langid = {english},
  file = {/Users/felix/paper/1991_Mézard_Parisi/Mézard_Parisi_1991_Replica field theory for random manifolds.pdf;/Users/felix/Zotero/storage/5NEYZNW7/jp1v1p809.html}
}

@incollection{mezardSpinGlassesIntroduction1994,
  title = {Spin {{Glasses}}: {{An Introduction}}},
  shorttitle = {Spin {{Glasses}}},
  booktitle = {From {{Statistical Physics}} to {{Statistical Inference}} and {{Back}}},
  author = {Mézard, Marc},
  editor = {Grassberger, Peter and Nadal, Jean-Pierre},
  date = {1994},
  series = {{{NATO ASI Series}}},
  pages = {183--193},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/978-94-011-1068-6_11},
  abstract = {This is a short introduction to spin glass theory for non physicists. It is not at all a review paper. More detailed presentations can be found in the references ([1]),([2]),([3]), and an excellent introduction to random systems for non specialists is ([4]). I also refer to these reviews for mentioning the original works and their references, which are systematically avoided here.},
  isbn = {978-94-011-1068-6},
  langid = {english},
  keywords = {Free Energy Density,Pure State,Spin Glass,Symmetry Breaking,Ultrametric Space}
}

@book{mezardSpinGlassTheory1987,
  title = {Spin Glass Theory and beyond: {{An Introduction}} to the {{Replica Method}} and {{Its Applications}}},
  shorttitle = {Spin Glass Theory and Beyond},
  author = {Mézard, Marc and Parisi, Giorgio and Virasoro, Miguel Angel},
  date = {1987},
  volume = {9},
  publisher = {World Scientific Publishing Company},
  doi = {10.1142/0271},
  isbn = {978-981-310-391-7}
}

@inproceedings{mikolovEmpiricalEvaluationCombination2011,
  title = {Empirical Evaluation and Combination of Advanced Language Modeling Techniques},
  booktitle = {Interspeech 2011},
  author = {Mikolov, Tomáš and Deoras, Anoop and Kombrink, Stefan and Burget, Lukáš and Černocký, Jan},
  date = {2011-08-27},
  pages = {605--608},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2011-242},
  abstract = {We present results obtained with several advanced language modeling techniques, including class based model, cache model, maximum entropy model, structured language model, random forest language model and several types of neural network based language models. We show results obtained after combining all these models by using linear interpolation. We conclude that for both small and moderately sized tasks, we obtain new state of the art results with combination of models, that is significantly better than performance of any individual model. Obtained perplexity reductions against Good-Turing trigram baseline are over 50\% and against modified Kneser-Ney smoothed 5-gram over 40\%.},
  eventtitle = {Interspeech 2011},
  langid = {english},
  file = {/Users/felix/paper/2011_Mikolov et al/Mikolov et al_2011_Empirical evaluation and combination of advanced language modeling techniques.pdf}
}

@article{mockusBayesianApproachGlobal1991,
  title = {Bayesian Approach to Global Optimization and Application to Multiobjective and Constrained Problems},
  author = {Mockus, J. B. and Mockus, L. J.},
  date = {1991-07-01},
  journaltitle = {Journal of Optimization Theory and Applications},
  shortjournal = {J Optim Theory Appl},
  volume = {70},
  number = {1},
  pages = {157--172},
  issn = {1573-2878},
  doi = {10.1007/BF00940509},
  abstract = {In this paper, the Bayesian methods of global optimization are considered. They provide the minimal expected deviation from the global minimum. It is shown that, using the Bayesian methods, the asymptotic density of calculations of the objective function is much greater around the point of global minimum. The relation of this density to the parameters of the method and to the function is defined.},
  langid = {english},
  keywords = {Bayesian approach,density of observations,Global optimization,linear and nonlinear constraints,multiobjective optimization},
  file = {/Users/felix/paper/1991_Mockus_Mockus/Mockus_Mockus_1991_Bayesian approach to global optimization and application to multiobjective and.pdf}
}

@article{montanariOptimizationSherringtonKirkpatrick2021,
  title = {Optimization of the {{Sherrington--Kirkpatrick Hamiltonian}}},
  author = {Montanari, Andrea},
  date = {2021-01-07},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  pages = {FOCS19-1},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/20M132016X},
  abstract = {Multigrid modeling algorithms are a technique used to accelerate iterative method models running on a hierarchy of similar graphlike structures. We introduce and demonstrate a new method for training neural networks which uses multilevel methods. Using an objective function derived from a graph-distance metric, we perform orthogonally-constrained optimization to find optimal prolongation and restriction maps between graphs. We compare and contrast several methods for performing this numerical optimization, and additionally present some new theoretical results on upper bounds of this type of objective function. Once calculated, these optimal maps between graphs form the core of multiscale artificial neural network (MsANN) training, a new procedure we present which simultaneously trains a hierarchy of neural network models of varying spatial resolution. Parameter information is passed between members of this hierarchy according to standard coarsening and refinement schedules from the multiscale modeling literature. In our machine learning experiments, these models are able to learn faster than training at the fine scale alone, achieving a comparable level of error with fewer weight updates (by an order of magnitude).},
  pagetotal = {FOCS19-38},
  file = {/Users/felix/paper/2021_Montanari/Montanari_2021_Optimization of the Sherrington--Kirkpatrick Hamiltonian.pdf}
}

@online{montanariSolvingOverparametrizedSystems2023,
  title = {Solving Overparametrized Systems of Random Equations: {{I}}. {{Model}} and Algorithms for Approximate Solutions},
  shorttitle = {Solving Overparametrized Systems of Random Equations},
  author = {Montanari, Andrea and Subag, Eliran},
  date = {2023-06-23},
  eprint = {2306.13326},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2306.13326},
  abstract = {Consider the problem of solving a system of equations \$\{\textbackslash boldsymbol F\}(\{\textbackslash boldsymbol x\})= \{\textbackslash boldsymbol 0\}\$, subject to \$\textbackslash |\{\textbackslash boldsymbol x\}\textbackslash |\_2=1\$, whereby \$\{\textbackslash boldsymbol F\}:\{\textbackslash mathbb R\}\textasciicircum d\textbackslash to\{\textbackslash mathbb R\}\textasciicircum n\$ is a random nonlinear map. More precisely, \$ \{\textbackslash boldsymbol F\}(\{\textbackslash boldsymbol x\}) = (F\_1(\{\textbackslash boldsymbol x\}),\textbackslash dots,F\_n(\{\textbackslash boldsymbol x\})) \$ where the \$ F\_i(\textbackslash,\textbackslash cdot\textbackslash, ) \$'s are i.i.d. rotationally invariant Gaussian processes. We study this problem under the proportional asymptotics \$n,d\textbackslash to\textbackslash infty\$, \$n/d\textbackslash to\textbackslash alpha\textbackslash in [0,1)\$ and establish results about the existence of solutions and polynomial-time algorithms to find them. First, we establish upper and lower bounds \$\textbackslash alpha\_\{UB\}\$, \$\textbackslash alpha\_\{LB\}\$ on the threshold for existence of solutions. Namely, if the number of equations per variable satisfies \$\textbackslash alpha{$<\backslash$}alpha\_\{LB\}\$, then the system admits exact solutions with high probability, while for \$\textbackslash alpha{$>\backslash$}alpha\_\{UB\}\$, no solutions exist, even in an approximate sense. We then analyze several algorithms to find solutions: gradient descent, Hessian descent, and a two-phase algorithm. In particular, for Hessian descent and the two-phase algorithm, we characterize their thresholds \$\textbackslash alpha\_\{HD\}\$, \$\textbackslash alpha\_\{TP\}\$. Namely, for \$\textbackslash alpha{$<\backslash$}alpha\_\{HD\}\$ (or \$\textbackslash alpha{$<\backslash$}alpha\_\{TP\}\$) the algorithm finds an approximate solution with high probability, while for \$\textbackslash alpha{$>\backslash$}alpha\_\{HD\}\$ (respectively \$\textbackslash alpha{$>\backslash$}alpha\_\{TP\}\$), it does not. Finally, we compare the theoretical predictions within this model to empirical results obtained with structured systems of nonlinear equations.},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability},
  file = {/Users/felix/paper/2023_Montanari_Subag/Montanari_Subag_2023_Solving overparametrized systems of random equations.pdf;/Users/felix/Zotero/storage/W8T2JI5X/2306.html}
}

@book{mullerCollectingSpatialData2007,
  title = {Collecting {{Spatial Data}}: {{Optimum Design}} of {{Experiments}} for {{Random Fields}}},
  shorttitle = {Collecting {{Spatial Data}}},
  author = {Muller, Werner},
  date = {2007},
  edition = {3. Aufl., Third Revised and Extended Edition},
  publisher = {Springer-Verlag},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-31175-1},
  abstract = {The book is concerned with the statistical theory for locating spatial sensors. It bridges the gap between spatial statistics and optimum design theory. After introductions to those two fields the topics of exploratory designs and designs for spatial trend and variogram estimation are treated. Special attention is devoted to describing new methodologies to cope with the problem of correlated observations. A great number of relevant references are collected and put into a common perspective. The theoretical investigations are accompanied by a practical example, the redesign of an Upper-Austrian air pollution monitoring network. A reader should be able to find respective theory and recommendations on how to efficiently plan a specific purpose spatial monitoring network. The third edition takes into account the rapid development in the area of spatial statistics by including new relevant research and references. The revised edition contains additional material on design for detecting spatial dependence and for estimating parametrized covariance functions.},
  isbn = {978-3-540-31174-4},
  langid = {english},
  keywords = {Earth Sciences general,Economics,Economics and Finance,Economics_xStatistics,Experimental design,Math. Appl. in Environmental Science,Random fields,Regional/Spatial Science,Statistics for Business Management Economics Finance Insurance,Statistics for Engineering Physics Computer Science Chemistry and Earth Sciences},
  file = {/Users/felix/paper/2007_Muller/Muller_2007_Collecting Spatial Data.pdf}
}

@online{murrayRandomizedNumericalLinear2023,
  title = {Randomized {{Numerical Linear Algebra}} : {{A Perspective}} on the {{Field With}} an {{Eye}} to {{Software}}},
  shorttitle = {Randomized {{Numerical Linear Algebra}}},
  author = {Murray, Riley and Demmel, James and Mahoney, Michael W. and Erichson, N. Benjamin and Melnichenko, Maksim and Malik, Osman Asif and Grigori, Laura and Luszczek, Piotr and Dereziński, Michał and Lopes, Miles E. and Liang, Tianyu and Luo, Hengrui and Dongarra, Jack},
  date = {2023-04-12},
  eprint = {2302.11474},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2302.11474},
  abstract = {Randomized numerical linear algebra - RandNLA, for short - concerns the use of randomization as a resource to develop improved algorithms for large-scale linear algebra computations. The origins of contemporary RandNLA lay in theoretical computer science, where it blossomed from a simple idea: randomization provides an avenue for computing approximate solutions to linear algebra problems more efficiently than deterministic algorithms. This idea proved fruitful in the development of scalable algorithms for machine learning and statistical data analysis applications. However, RandNLA's true potential only came into focus upon integration with the fields of numerical analysis and "classical" numerical linear algebra. Through the efforts of many individuals, randomized algorithms have been developed that provide full control over the accuracy of their solutions and that can be every bit as reliable as algorithms that might be found in libraries such as LAPACK. Recent years have even seen the incorporation of certain RandNLA methods into MATLAB, the NAG Library, NVIDIA's cuSOLVER, and SciKit-Learn. For all its success, we believe that RandNLA has yet to realize its full potential. In particular, we believe the scientific community stands to benefit significantly from suitably defined "RandBLAS" and "RandLAPACK" libraries, to serve as standards conceptually analogous to BLAS and LAPACK. This 200-page monograph represents a step toward defining such standards. In it, we cover topics spanning basic sketching, least squares and optimization, low-rank approximation, full matrix decompositions, leverage score sampling, and sketching data with tensor product structures (among others). Much of the provided pseudo-code has been tested via publicly available MATLAB and Python implementations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Mathematical Software,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/Users/felix/paper/2023_Murray et al/Murray et al_2023_Randomized Numerical Linear Algebra.pdf;/Users/felix/Zotero/storage/MDR2WHKT/2302.html}
}

@online{nagwaLessonExplainerFunction,
  title = {Lesson {{Explainer}}: {{Function Transformations}}: {{Translations}} | {{Nagwa}}},
  shorttitle = {Lesson {{Explainer}}},
  author = {Nagwa},
  url = {https://www.nagwa.com/en/explainers/707102751905/},
  urldate = {2024-02-23},
  abstract = {In this explainer, we will learn how to identify function transformations involving horizontal and vertical shifts.},
  langid = {english},
  file = {/Users/felix/Zotero/storage/WT7XE9FS/707102751905.html}
}

@book{nealBayesianLearningNeural1996,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  date = {1996},
  series = {Lecture {{Notes}} in {{Statistics}}},
  edition = {1},
  volume = {118},
  publisher = {Springer New York, NY},
  doi = {10.1007/978-1-4612-0745-0},
  isbn = {978-0-387-94724-2},
  langid = {english},
  pagetotal = {204},
  file = {/Users/felix/paper/1996_Neal/Neal_1996_Bayesian Learning for Neural Networks.pdf;/Users/felix/Zotero/storage/GC37WL36/978-1-4612-0745-0.html}
}

@book{nesterovLecturesConvexOptimization2018,
  title = {Lectures on {{Convex Optimization}}},
  author = {Nesterov, Yurii Evgen'evič},
  date = {2018},
  series = {Springer Optimization and {{Its}} Applications; Volume 137},
  edition = {Second edition},
  publisher = {Springer},
  location = {Cham},
  doi = {10.1007/978-3-319-91578-4},
  abstract = {This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics, Introduction -- Part I Black-Box Optimization -- 1 Nonlinear Optimization -- 2 Smooth Convex Optimization -- 3 Nonsmooth Convex Optimization -- 4 Second-Order Methods -- Part II Structural Optimization -- 5 Polynomial-time Interior-Point Methods -- 6 Primal-Dual Model of Objective Function -- 7 Optimization in Relative Scale -- Bibliographical Comments -- Appendix A. Solving some Auxiliary Optimization Problems -- References -- Index},
  isbn = {978-3-319-91578-4},
  langid = {english},
  keywords = {Computer software; Optimization; Mathematical optimization; Algorithms,Konvexe Optimierung},
  file = {/Users/felix/paper/2018_Nesterov/Nesterov_2018_Lectures on Convex Optimization.pdf}
}

@article{nesterovMethodSolvingConvex1983,
  title = {A Method for Solving the Convex Programming Problem with Convergence Rate \(O(1/k^2)\)},
  author = {Nesterov, Yurii Evgen'evič},
  date = {1983},
  journaltitle = {Dokl. Akad. Nauk SSSR},
  volume = {269},
  pages = {543--547},
  url = {http://m.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng},
  urldate = {2021-07-17},
  file = {/Users/felix/paper/1983_Nesterov/Nesterov_1983_A method for solving the convex programming problem with convergence rate.pdf;/Users/felix/Zotero/storage/LUQ6H5UH/10029946121.html}
}

@software{nogueiraBayesianOptimizationOpen2014,
  title = {Bayesian {{Optimization}}: {{Open}} Source Constrained Global Optimization Tool for {{Python}}},
  author = {Nogueira, Fernando},
  date = {2014/},
  origdate = {2014-06-06T08:18:56Z},
  url = {https://github.com/bayesian-optimization/BayesianOptimization},
  urldate = {2024-02-23},
  abstract = {A Python implementation of global optimization with gaussian processes.},
  organization = {bayesian-optimization},
  keywords = {bayesian-optimization,gaussian-processes,optimization,python,simple},
  file = {/Users/felix/Zotero/storage/7P5RXHFR/BayesianOptimization.png}
}

@article{oneillExchangeabilityCorrelationBayes2009,
  title = {Exchangeability, {{Correlation}}, and {{Bayes}}' {{Effect}}},
  author = {O'Neill, Ben},
  date = {2009},
  journaltitle = {International Statistical Review},
  volume = {77},
  number = {2},
  pages = {241--250},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2008.00059.x},
  abstract = {We examine the difference between Bayesian and frequentist statistics in making statements about the relationship between observable values. We show how standard models under both paradigms can be based on an assumption of exchangeability and we derive useful covariance and correlation results for values from an exchangeable sequence. We find that such values are never negatively correlated, and are generally positively correlated under the models used in Bayesian statistics. We discuss the significance of this result as well as a phenomenon which often follows from the differing methodologies and practical applications of these paradigms – a phenomenon we call Bayes' effect.},
  langid = {english},
  keywords = {Bayes' effect,Bayesian statistics,correlation,exchangeability,frequentist statistics,independence,pseudo-correlation},
  file = {/Users/felix/paper/2009_O'Neill/O'Neill_2009_Exchangeability, Correlation, and Bayes' Effect.pdf;/Users/felix/Zotero/storage/EA3CDCFX/j.1751-5823.2008.00059.html}
}

@inproceedings{padidarScalingGaussianProcesses2021,
  title = {Scaling {{Gaussian Processes}} with {{Derivative Information Using Variational Inference}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Padidar, Misha and Zhu, Xinran and Huang, Leo and Gardner, Jacob and Bindel, David},
  date = {2021},
  volume = {34},
  pages = {6442--6453},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/32bbf7b2bc4ed14eb1e9c2580056a989-Abstract.html},
  urldate = {2023-05-17},
  file = {/Users/felix/paper/2021_Padidar et al/Padidar et al_2021_Scaling Gaussian Processes with Derivative Information Using Variational.pdf;/Users/felix/paper/2021_Padidar et al/Supplemental_Padidar et al_2021_Scaling Gaussian Processes with Derivative Information Using Variational.pdf}
}

@article{panchenkoParisiUltrametricityConjecture2013,
  title = {The {{Parisi}} Ultrametricity Conjecture},
  author = {Panchenko, Dmitry},
  date = {2013},
  journaltitle = {Annals of Mathematics},
  volume = {177},
  number = {1},
  pages = {383--393},
  doi = {10.4007/annals.2013.177.1.8},
  abstract = {In this paper we prove that the support of a random measure on the unit ball of a separable Hilbert space that satisfies the Ghirlanda-Guerra identities must be ultrametric with probability one. This implies the Parisi ultrametricity conjecture in mean-field spin glass models, such as the Sherrington-Kirkpatrick and mixed 𝑝-spin models, for which Gibbs measures are known to satisfy the Ghirlanda-Guerra identities in the thermodynamic limit.},
  langid = {american},
  file = {/Users/felix/paper/2013_Panchenko/Panchenko_2013_The Parisi ultrametricity conjecture.pdf;/Users/felix/Zotero/storage/JE8HDIXU/p08.html}
}

@book{panchenkoSherringtonKirkpatrickModel2013,
  title = {The {{Sherrington-Kirkpatrick Model}}},
  author = {Panchenko, Dmitry},
  date = {2013},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4614-6289-7},
  isbn = {978-1-4614-6288-0 978-1-4614-6289-7},
  langid = {english},
  keywords = {Aizenman-Sims-Starr scheme,Aldous-Hoover representation,Dovbysh-Sudakov representation,exchangeability,Gaussian processes,Ghirlanda-Guerra identities,Guerra replica symmetry breaking,p-spin models,Parisi ansatz,Parisi formula,Poisson processes,Poisson-Dirichlet processes,replica symmetry breaking,Ruelle probability cascades,Sherrington-Kirkpatrick model,spin glass models,Talagrand positivity principle,ultrametricity},
  file = {/Users/felix/paper/2013_Panchenko/Panchenko_2013_The Sherrington-Kirkpatrick Model.pdf}
}

@article{paquetteHaltingTimePredictable2022,
  title = {Halting {{Time}} Is {{Predictable}} for {{Large Models}}: {{A Universality Property}} and {{Average-Case Analysis}}},
  shorttitle = {Halting {{Time}} Is {{Predictable}} for {{Large Models}}},
  author = {Paquette, Courtney and family=Merriënboer, given=Bart, prefix=van, useprefix=true and Paquette, Elliot and Pedregosa, Fabian},
  date = {2022-02},
  journaltitle = {Foundations of Computational Mathematics},
  shortjournal = {Found Comput Math},
  volume = {23},
  number = {2},
  eprint = {2006.04299},
  eprinttype = {arXiv},
  eprintclass = {math, stat},
  pages = {597--673},
  issn = {1615-3383},
  doi = {10.1007/s10208-022-09554-y},
  abstract = {Average-case analysis computes the complexity of an algorithm averaged over all possible inputs. Compared to worst-case analysis, it is more representative of the typical behavior of an algorithm, but remains largely unexplored in optimization. One difficulty is that the analysis can depend on the probability distribution of the inputs to the model. However, we show that this is not the case for a class of large-scale problems trained with first-order methods including random least squares and one-hidden layer neural networks with random weights. In fact, the halting time exhibits a universality property: it is independent of the probability distribution. With this barrier for average-case analysis removed, we provide the first explicit average-case convergence rates showing a tighter complexity not captured by traditional worst-case analysis. Finally, numerical simulations suggest this universality property holds for a more general class of algorithms and problems.},
  langid = {english},
  keywords = {60B20,65K10,68T07,90C06,90C25,Optimization,Random matrix theory,Universality},
  file = {/Users/felix/paper/2022_Paquette et al/[ArXiv] Paquette et al_2022_Halting Time is Predictable for Large Models.pdf;/Users/felix/paper/2022_Paquette et al/Paquette et al_2022_Halting Time is Predictable for Large Models.pdf}
}

@article{paquetteUniversalityConjugateGradient2022,
  title = {Universality for the {{Conjugate Gradient}} and {{MINRES Algorithms}} on {{Sample Covariance Matrices}}},
  author = {Paquette, Elliot and Trogdon, Thomas},
  date = {2022-09-01},
  journaltitle = {Communications on Pure and Applied Mathematics},
  volume = {76},
  number = {5},
  pages = {1085--1136},
  issn = {1097-0312},
  doi = {10.1002/cpa.22081},
  abstract = {We present a probabilistic analysis of two Krylov subspace methods for solving linear systems. We prove a central limit theorem for norms of the residual vectors that are produced by the conjugate gradient and MINRES algorithms when applied to a wide class of sample covariance matrices satisfying some standard moment conditions. The proof involves establishing a four-moment theorem for the so-called spectral measure, implying, in particular, universality for the matrix produced by the Lanczos iteration. The central limit theorem then implies an almost-deterministic iteration count for the iterative methods in question. © 2022 Wiley Periodicals LLC.},
  langid = {english},
  keywords = {conjugate gradient,MINRES,Sample covariance matrices,Wishart distribution},
  file = {/Users/felix/paper/2022_Paquette_Trogdon/Paquette_Trogdon_2022_Universality for the Conjugate Gradient and MINRES Algorithms on Sample.pdf;/Users/felix/Zotero/storage/5TA4BUYX/cpa.html}
}

@article{parisiSequenceApproximatedSolutions1980,
  title = {A Sequence of Approximated Solutions to the {{S-K}} Model for Spin Glasses},
  author = {Parisi, G.},
  date = {1980-04},
  journaltitle = {Journal of Physics A: Mathematical and General},
  shortjournal = {J. Phys. A: Math. Gen.},
  volume = {13},
  number = {4},
  pages = {L115},
  issn = {0305-4470},
  doi = {10.1088/0305-4470/13/4/009},
  abstract = {In the framework of the new version of the replica theory, a sequence of approximated solutions is computed for the Sherrington-Kirkpatrick model (see Phys. Rev. Lett., vol.35, p.1972, 1975) of spin glasses.},
  langid = {english},
  file = {/Users/felix/paper/1980_Parisi/Parisi_1980_A sequence of approximated solutions to the S-K model for spin glasses.pdf}
}

@inproceedings{pascanuDifficultyTrainingRecurrent2013,
  title = {On the Difficulty of Training Recurrent Neural Networks},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  date = {2013-05-26},
  pages = {1310--1318},
  publisher = {PMLR},
  location = {Atlanta},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v28/pascanu13.html},
  urldate = {2024-04-02},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2013_Pascanu et al/[Supplemental] Pascanu et al_2013_On the difficulty of training recurrent neural networks.pdf;/Users/felix/paper/2013_Pascanu et al/Pascanu et al_2013_On the difficulty of training recurrent neural networks.pdf}
}

@online{pascanuSaddlePointProblem2014,
  title = {On the Saddle Point Problem for Non-Convex Optimization},
  author = {Pascanu, Razvan and Dauphin, Yann N. and Ganguli, Surya and Bengio, Yoshua},
  date = {2014-05-27},
  eprint = {1405.4604},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.1405.4604},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for the ability of these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, and neural network theory, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new algorithm, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep neural network training, and provide preliminary numerical evidence for its superior performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/felix/paper/2014_Pascanu et al/Pascanu et al_2014_On the saddle point problem for non-convex optimization.pdf;/Users/felix/Zotero/storage/7B28YP5G/1405.html}
}

@inproceedings{pedregosaAccelerationSpectralDensity2020,
  title = {Acceleration through Spectral Density Estimation},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Pedregosa, Fabian and Scieur, Damien},
  date = {2020-11-21},
  pages = {7553--7562},
  publisher = {PMLR},
  location = {Virtual Event (formerly Vienna)},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/pedregosa20a.html},
  urldate = {2023-11-09},
  abstract = {We develop a framework for the average-case analysis of random quadratic problems and derive algorithms that are optimal under this analysis. This yields a new class of methods that achieve acceleration given a model of the Hessian’s eigenvalue distribution. We develop explicit algorithms for the uniform, Marchenko-Pastur, and exponential distributions. These methods have a simple momentum-like update, in which each update only makes use on the current gradient and previous two iterates. Furthermore, the momentum and step-size parameters can be estimated without knowledge of the Hessian’s smallest singular value, in contrast with classical accelerated methods like Nesterov acceleration and Polyak momentum. Through empirical benchmarks on quadratic and logistic regression problems, we identify regimes in which the the proposed methods improve over classical (worst-case) accelerated methods.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2020_Pedregosa_Scieur/Pedregosa_Scieur_2020_Acceleration through spectral density estimation.pdf;/Users/felix/Zotero/storage/IFQE5S5M/Pedregosa and Scieur - 2020 - Acceleration through spectral density estimation.pdf}
}

@inproceedings{penningtonGeometryNeuralNetwork2017,
  title = {Geometry of {{Neural Network Loss Surfaces}} via {{Random Matrix Theory}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Pennington, Jeffrey and Bahri, Yasaman},
  date = {2017-07-17},
  pages = {2798--2806},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/pennington17a.html},
  urldate = {2022-04-12},
  abstract = {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, 𝜙ϕ\textbackslash phi, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1/2(1−𝜙)21/2(1−ϕ)21/2(1-\textbackslash phi)\textasciicircum 2.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2017_Pennington_Bahri/Pennington_Bahri_2017_Geometry of Neural Network Loss Surfaces via Random Matrix Theory.pdf;/Users/felix/Zotero/storage/BV2L2GUY/Pennington and Bahri - 2017 - Geometry of Neural Network Loss Surfaces via Rando.pdf}
}

@book{piterbargAsymptoticMethodsTheory1996,
  title = {Asymptotic {{Methods}} in the {{Theory}} of {{Gaussian Processes}} and {{Fields}}},
  author = {Piterbarg, Vladimir I.},
  date = {1996},
  series = {Translations of {{Mathematical Monographs}}},
  volume = {148},
  eprint = {Tia46nGNFiAC},
  eprinttype = {googlebooks},
  publisher = {American Mathematical Soc.},
  isbn = {978-0-8218-0423-0},
  langid = {english},
  pagetotal = {452},
  keywords = {Mathematics / General,Mathematics / Numerical Analysis,Mathematics / Probability & Statistics / General},
  file = {/Users/felix/paper/1996_Piterbarg/Piterbarg_1996_Asymptotic Methods in the Theory of Gaussian Processes and Fields.pdf;/Users/felix/paper/1996_Piterbarg/Piterbarg_1996_Asymptotic Methods in the Theory of Gaussian Processes and Fields2.pdf}
}

@article{polyakMethodsSpeedingConvergence1964,
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, Boris T.},
  date = {1964-01-01},
  journaltitle = {USSR Computational Mathematics and Mathematical Physics},
  shortjournal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  number = {5},
  pages = {1--17},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(64)90137-5},
  abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
  langid = {english},
  file = {/Users/felix/paper/1964_Polyak/Polyak_1964_Some methods of speeding up the convergence of iteration methods.pdf;/Users/felix/Zotero/storage/YHYZHVCI/0041555364901375.html}
}

@article{quOptimalDiagonalPreconditioning2024,
  title = {Optimal {{Diagonal Preconditioning}}},
  author = {Qu, Zhaonan and Gao, Wenzhi and Hinder, Oliver and Ye, Yinyu and Zhou, Zhengyuan},
  date = {2024-03-04},
  journaltitle = {Operations Research},
  eprint = {2209.00809},
  eprinttype = {arXiv},
  eprintclass = {math.OC},
  publisher = {INFORMS,},
  issn = {0030-364X},
  doi = {10.1287/opre.2022.0592},
  abstract = {Preconditioning has long been a staple technique in optimization, often applied to reduce the condition number of a matrix and speed up the convergence of algorithms. Although there are many popular preconditioning techniques in practice, most lack guarantees on reductions in condition number, and the degree to which we can improve over existing heuristic preconditioners remains an important question. In this paper, we study the problem of optimal diagonal preconditioning that achieves maximal reduction in the condition number of any full-rank matrix by scaling its rows and/or columns with positive numbers. We first reformulate the problem as a quasiconvex optimization problem and provide a simple algorithm based on bisection. Then, we develop an interior point algorithm with  𝑂(log(1/𝜖)) O(log(1/ϵ))  iteration complexity, where each iteration consists of a Newton update based on the Nesterov-Todd direction. Next, we specialize in one-sided optimal diagonal preconditioning problems and demonstrate that they can be formulated as standard dual semidefinite program (SDP) problems. We then develop efficient customized solvers for the SDP approach and study the empirical performance of our optimal diagonal preconditioning procedures through extensive experiments. Our findings suggest that optimal diagonal preconditioners can significantly improve upon existing heuristics-based diagonal preconditioners at reducing condition numbers, and our SDP approach can find such optimal preconditioners efficiently. We also extend our SDP approach to compute optimal mixtures of heuristic preconditioners, which further improves its scalability and applicability. Funding: Z. Qu was supported by a Stanford Interdisciplinary Graduate Fellowship; W. Gao was supported in part by the National Natural Science Foundation of China (NSFC) [Grants NSFC-72150001, NSFC-72225009, NSFC-72394360, and NSFC72394365]; O. Hinder was supported by Pitt momentum funds and a Mascaro center for sustainable innovation faculty fellowship; Z. Zhou was supported by NSF Grant CCF [Grant 2312205] and the New York University Research Catalyst Prize. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0592.},
  keywords = {condition number,diagonal preconditioning,interior point algorithms,iterative methods,Optimization},
  file = {/Users/felix/paper/2024_Qu et al/Qu et al_2024_Optimal Diagonal Preconditioning.pdf}
}

@article{rainforthModernBayesianExperimental2024,
  title = {Modern {{Bayesian Experimental Design}}},
  author = {Rainforth, Tom and Foster, Adam and Ivanova, Desi R. and Smith, Freddie Bickford},
  date = {2024-02},
  journaltitle = {Statistical Science},
  volume = {39},
  number = {1},
  pages = {100--114},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/23-STS915},
  abstract = {Bayesian experimental design (BED) provides a powerful and general framework for optimizing the design of experiments. However, its deployment often poses substantial computational challenges that can undermine its practical use. In this review, we outline how recent advances have transformed our ability to overcome these challenges and thus utilize BED effectively, before discussing some areas for future development in the field.},
  keywords = {Active learning,Bayesian adaptive design,Bayesian optimal design,information maximization},
  file = {/Users/felix/paper/2024_Rainforth et al/Rainforth et al_2024_Modern Bayesian Experimental Design.pdf}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K.I.},
  date = {2006},
  series = {Adaptive Computation and Machine Learning},
  edition = {2},
  number = {3},
  publisher = {MIT Press},
  location = {Cambridge, Massachusetts},
  url = {http://gaussianprocess.org/gpml/chapters/RW.pdf},
  urldate = {2023-04-06},
  isbn = {0-262-18253-X},
  langid = {english},
  pagetotal = {248},
  file = {/Users/felix/paper/2006_Rasmussen_Williams/Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf}
}

@inproceedings{reddiConvergenceAdam2018,
  title = {On the {{Convergence}} of {{Adam}} and {{Beyond}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  date = {2018},
  eprint = {1904.09237},
  eprinttype = {arXiv},
  location = {Vancouver, Canada},
  abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  eventtitle = {6th {{International Conference}} on {{Learning Representations}}},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/paper/2019_Reddi et al/Reddi et al_2019_On the Convergence of Adam and Beyond.pdf;/Users/felix/Zotero/storage/7HJCRKB3/1904.html}
}

@article{roburinSphericalPerspectiveLearning2022,
  title = {Spherical Perspective on Learning with Normalization Layers},
  author = {Roburin, Simon and family=Mont-Marin, given=Yann, prefix=de, useprefix=true and Bursuc, Andrei and Marlet, Renaud and Pérez, Patrick and Aubry, Mathieu},
  date = {2022-05-28},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {487},
  eprint = {2006.13382},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {66--74},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.02.021},
  abstract = {Normalization Layers (NLs) are widely used in modern deep-learning architectures. Despite their apparent simplicity, their effect on optimization is not yet fully understood. This paper introduces a spherical framework to study the optimization of neural networks with NLs from a geometric perspective. Concretely, the radial invariance of groups of parameters, such as filters for convolutional neural networks, allows to translate the optimization steps on the L2 unit hypersphere. This formulation and the associated geometric interpretation shed new light on the training dynamics. Firstly, the first effective learning rate expression of Adam is derived. Then the demonstration that, in the presence of NLs, performing Stochastic Gradient Descent (SGD) alone is actually equivalent to a variant of Adam constrained to the unit hypersphere, stems from the framework. Finally, this analysis outlines phenomena that previous variants of Adam act on and their importance in the optimization process are experimentally validated.},
  keywords = {Batch normalization,Deep learning,Normalization layers,Optimization},
  file = {/Users/felix/paper/2022_Roburin et al/Roburin et al_2022_Spherical perspective on learning with normalization layers2.pdf;/Users/felix/Zotero/storage/NFLAFME2/S092523122200159X.html}
}

@book{rockafellarVariationalAnalysis2009,
  title = {Variational {{Analysis}}},
  author = {Rockafellar, R. Tyrrell and Wets, Maria and Wets, Roger J.-B.},
  date = {2009},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}, {{A Series}} of {{Comprehensive Studies}} in {{Mathematics}}},
  volume = {317},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  issn = {0072-7830},
  doi = {10.1007/978-3-642-02431-3},
  abstract = {From its origins in the minimization of integral functionals, the notion of 'variations' has evolved greatly in connection with applications in optimization, equilibrium, and control. It refers not only to constrained movement away from a point, but also to modes of perturbation and approximation that are best describable by 'set convergence', variational convergence of functions and the like. This book develops a unified framework and, in finite dimension, provides a detailed exposition of variational geometry and subdifferential calculus in their current forms beyond classical and convex analysis. Also covered are set-convergence, set-valued mappings, epi-convergence, duality, maximal monotone mappings, second-order subderivatives, measurable selections and normal integrands. The changes in this 3rd printing mainly concern various typographical corrections, and reference omissions that came to light in the previous printings. Many of these reached the authors' notice through their own re-reading, that of their students and a number of colleagues mentioned in the Preface. The authors also included a few telling examples as well as improved a few statements, with slightly weaker assumptions or have strengthened the conclusions in a couple of instances.},
  isbn = {978-3-540-62772-2},
  langid = {english},
  keywords = {Calculus of variations,Calculus of Variations and Optimal Control; Optimization,Mathematical optimization,Mathematics,Mathematics and Statistics,Systems theory,Systems Theory Control},
  file = {/Users/felix/paper/2009_Rockafellar et al/Rockafellar et al_2009_Variational Analysis.pdf}
}

@inproceedings{roosHighDimensionalGaussianProcess2021,
  title = {High-{{Dimensional Gaussian Process Inference}} with {{Derivatives}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {family=Roos, given=Filip, prefix=de, useprefix=false and Gessner, Alexandra and Hennig, Philipp},
  date = {2021-07-01},
  pages = {2535--2545},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/de-roos21a.html},
  urldate = {2023-05-15},
  abstract = {Although it is widely known that Gaussian processes can be conditioned on observations of the gradient, this functionality is of limited use due to the prohibitive computational cost of O(N\textasciicircum 3D\textasciicircum 3) in data points N and dimension D. The dilemma of gradient observations is that a single one of them comes at the same cost as D independent function evaluations, so the latter are often preferred. Careful scrutiny reveals, however, that derivative observations give rise to highly structured kernel Gram matrices for very general classes of kernels (inter alia, stationary kernels). We show that in the low-data regime N {$<$} D, the Gram matrix can be decomposed in a manner that reduces the cost of inference to O(N\textasciicircum 2D +(N\textasciicircum 2)\textasciicircum 3) (i.e., linear in the number of dimensions) and, in special cases, to O(N\textasciicircum 2D+N\textasciicircum 3). This reduction in complexity opens up new use-cases for inference with gradients especially in the high-dimensional regime, where the information-to-cost ratio of gradient observations significantly increases. We demonstrate this potential in a variety of tasks relevant for machine learning, such as optimization and Hamiltonian Monte Carlo with predictive gradients.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2021_Roos et al/Roos et al_2021_High-Dimensional Gaussian Process Inference with Derivatives.pdf;/Users/felix/paper/2021_Roos et al/Supplemental_Roos et al_2021_High-Dimensional Gaussian Process Inference with Derivatives.pdf}
}

@online{rosHighdLandscapesParadigm2023,
  title = {The High-d Landscapes Paradigm: Spin-Glasses, and Beyond},
  shorttitle = {The High-d Landscapes Paradigm},
  author = {Ros, Valentina and Fyodorov, Yan V.},
  date = {2023-04-11},
  eprint = {2209.07975},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  doi = {10.48550/arXiv.2209.07975},
  abstract = {We review recent developments on the characterization of random landscapes in high-dimension. We focus in particular on the problem of characterizing the landscape topology and geometry, discussing techniques to count and classify its stationary points and stressing connections with the statistical physics of disordered systems and with random matrix theory.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/Users/felix/Zotero/storage/T8P9VB9F/Ros and Fyodorov - 2023 - The high-d landscapes paradigm spin-glasses, and .pdf;/Users/felix/Zotero/storage/JZ2MN47P/2209.html}
}

@inproceedings{safranEffectsMildOverparameterization2021,
  title = {The {{Effects}} of {{Mild Over-parameterization}} on the {{Optimization Landscape}} of {{Shallow ReLU Neural Networks}}},
  booktitle = {Proceedings of {{Thirty Fourth Conference}} on {{Learning Theory}}},
  author = {Safran, Itay M. and Yehudai, Gilad and Shamir, Ohad},
  date = {2021-07-21},
  pages = {3889--3934},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v134/safran21a.html},
  urldate = {2023-10-26},
  abstract = {We study the effects of mild over-parameterization on the optimization landscape of a simple ReLU neural network of the form 𝐱↦∑𝑘𝑖=1max\{0,𝐰⊤𝑖𝐱\}x↦∑i=1kmax\{0,wi⊤x\}\textbackslash mathbf\{x\}\textbackslash mapsto\textbackslash sum\_\{i=1\}\textasciicircum k\textbackslash max\textbackslash\{0,\textbackslash mathbf\{w\}\_i\textasciicircum\{\textbackslash top\}\textbackslash mathbf\{x\}\textbackslash\}, in a well-studied teacher-student setting where the target values are generated by the same architecture, and when directly optimizing over the population squared loss with respect to Gaussian inputs. We prove that while the objective is strongly convex around the global minima when the teacher and student networks possess the same number of neurons, it is not even \textbackslash emph\{locally convex\} after any amount of over-parameterization. Moreover, related desirable properties (e.g., one-point strong convexity and the Polyak-\{Ł\}ojasiewicz condition) also do not hold even locally. On the other hand, we establish that the objective remains one-point strongly convex in \textbackslash emph\{most\} directions (suitably defined), and show an optimization guarantee under this property. For the non-global minima, we prove that adding even just a single neuron will turn a non-global minimum into a saddle point. This holds under some technical conditions which we validate empirically.  These results provide a possible explanation for why recovering a global minimum becomes significantly easier when we over-parameterize, even if the amount of over-parameterization is very moderate.},
  eventtitle = {Conference on {{Learning Theory}}},
  langid = {english},
  file = {/Users/felix/paper/2021_Safran et al/Safran et al_2021_The Effects of Mild Over-parameterization on the Optimization Landscape of.pdf}
}

@inproceedings{safranSpuriousLocalMinima2018,
  title = {Spurious {{Local Minima}} Are {{Common}} in {{Two-Layer ReLU Neural Networks}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Safran, Itay and Shamir, Ohad},
  date = {2018-07-03},
  pages = {4433--4441},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/safran18a.html},
  urldate = {2023-10-26},
  abstract = {We consider the optimization problem associated with training simple ReLU neural networks of the form 𝐱↦∑𝑘𝑖=1max\{0,𝐰⊤𝑖𝐱\}x↦∑i=1kmax\{0,wi⊤x\}\textbackslash mathbf\{x\}\textbackslash mapsto \textbackslash sum\_\{i=1\}\textasciicircum\{k\}\textbackslash max\textbackslash\{0,\textbackslash mathbf\{w\}\_i\textasciicircum\textbackslash top \textbackslash mathbf\{x\}\textbackslash\} with respect to the squared loss. We provide a computer-assisted proof that even if the input distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal parameter vectors, the problem can still have spurious local minima once 6≤𝑘≤206≤k≤206\textbackslash le k\textbackslash le 20. By a concentration of measure argument, this implies that in high input dimensions, nearly all target networks of the relevant sizes lead to spurious local minima. Moreover, we conduct experiments which show that the probability of hitting such local minima is quite high, and increasing with the network size. On the positive side, mild over-parameterization appears to drastically reduce such local minima, indicating that an over-parameterization assumption is necessary to get a positive result in this setting.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2018_Safran_Shamir/Safran_Shamir_2018_Spurious Local Minima are Common in Two-Layer ReLU Neural Networks.pdf;/Users/felix/Zotero/storage/LZ4EPCXL/Safran and Shamir - 2018 - Spurious Local Minima are Common in Two-Layer ReLU.pdf}
}

@article{sagunUniversalHaltingTimes2018,
  title = {Universal Halting Times in Optimization and Machine Learning},
  author = {Sagun, Levent and Trogdon, Thomas and LeCun, Yann},
  date = {2018-06},
  journaltitle = {Quarterly of Applied Mathematics},
  shortjournal = {Quart. Appl. Math.},
  volume = {76},
  number = {2},
  pages = {289--301},
  issn = {0033-569X, 1552-4485},
  doi = {10.1090/qam/1483},
  abstract = {We present empirical evidence that the halting times for a class of optimization algorithms are universal. The algorithms we consider come from quadratic optimization, spin glasses and machine learning. A universality theorem is given in the case of the quadratic gradient descent flow. More precisely, given an algorithm, which we take to be both the optimization routine and the form of the random landscape, the fluctuations of the halting time of the algorithm follow a distribution that, after centering and scaling, appears invariant under changes in the distribution on the landscape — universality is present.},
  langid = {english},
  file = {/Users/felix/paper/2018_Sagun et al/Sagun et al_2018_Universal halting times in optimization and machine learning.pdf}
}

@inproceedings{salimansWeightNormalizationSimple2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Salimans, Tim and Kingma, Durk P},
  date = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  location = {Barcelona, Spain},
  url = {https://proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html},
  urldate = {2023-10-16},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  file = {/Users/felix/paper/2016_Salimans_Kingma/Salimans_Kingma_2016_Weight Normalization.pdf}
}

@article{sampsonNonparametricEstimationNonstationary1992,
  title = {Nonparametric {{Estimation}} of {{Nonstationary Spatial Covariance Structure}}},
  author = {Sampson, Paul D. and Guttorp, Peter},
  date = {1992-03-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {87},
  number = {417},
  pages = {108--119},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1992.10475181},
  abstract = {Estimation of the covariance structure of spatial processes is a fundamental prerequisite for problems of spatial interpolation and the design of monitoring networks. We introduce a nonparametric approach to global estimation of the spatial covariance structure of a random function Z(x, t) observed repeatedly at times ti (i = 1, …, T) at a finite number of sampling stations xi (i = 1, 2, …, N) in the plane. Our analyses assume temporal stationarity but do not assume spatial stationarity (or isotropy). We analyze the spatial dispersions var(Z(xi, t) − Z(xj, t)) as a natural metric for the spatial covariance structure and model these as a general smooth function of the geographic coordinates of station pairs (xi, xj ). The model is constructed in two steps. First, using nonmetric multidimensional scaling (MDS) we compute a two-dimensional representation of the sampling stations for which a monotone function of interpoint distances δij approximates the spatial dispersions. MDS transforms the problem into one for which the covariance structure, expressed in terms of spatial dispersions, is stationary and isotropic. Second, we compute thin-plate splines to provide smooth mappings of the geographic representation of the sampling stations into their MDS representation. The composition of this mapping f and a monotone function g derived from MDS yields a nonparametric estimator of var(Z(xa, t) − Z(xb, t)) for any two geographic locations xa and xb (monitored or not) of the form g(|f(xa ) − f(xb )|). By restricting the monotone function g to a class of conditionally nonpositive definite variogram functions, we ensure that the resulting nonparametric model corresponds to a nonnegative definite covariance model. We use biorthogonal grids, introduced by Bookstein in the field of morphometrics, to depict the thin-plate spline mappings that embody the nature of the anisotropy and nonstationarity in the sample covariance matrix. An analysis of mesoscale variability in solar radiation monitored in southwestern British Columbia demonstrates this methodology.},
  keywords = {Biorthogonal grids,Dispersion,Kriging,Multidimensional scaling,Thin-plate spline,Variogram},
  file = {/Users/felix/paper/1992_Sampson_Guttorp/Sampson_Guttorp_1992_Nonparametric Estimation of Nonstationary Spatial Covariance Structure.pdf}
}

@book{sasvariMultivariateCharacteristicCorrelation2013,
  title = {Multivariate {{Characteristic}} and {{Correlation Functions}}},
  author = {Sasvári, Zoltán},
  date = {2013-03-22},
  series = {De {{Gruyter Studies}} in {{Mathematics}}},
  number = {50},
  publisher = {Walter de Gruyter},
  location = {Berlin/Boston},
  abstract = {In a certain sense characteristic functions and correlation functions are the same, the common underlying concept is positive definiteness. Many results in probability theory, mathematical statistics and stochastic processes can be derived by using these functions. While there are books on characteristic functions of one variable, books devoting some sections to the multivariate case, and books treating the general case of locally compact groups, interestingly there is no book devoted entirely to the multidimensional case which is extremely important for applications. This book is intended to fill this gap at least partially. It makes the basic concepts and results on multivariate characteristic and correlation functions easily accessible to both students and researchers in a comprehensive manner. The first chapter presents basic results and should be read carefully since it is essential for the understanding of the subsequent chapters. The second chapter is devoted to correlation functions, their applications to stationary processes and some connections to harmonic analysis. In Chapter 3 we deal with several special properties, Chapter 4 is devoted to the extension problem while Chapter 5 contains a few applications. A relatively large appendix comprises topics like infinite products, functional equations, special functions or compact operators.},
  isbn = {978-3-11-022399-6},
  langid = {english},
  pagetotal = {376},
  keywords = {Mathematics / General,Mathematics / Mathematical Analysis,Mathematics / Probability & Statistics / General},
  file = {/Users/felix/paper/2013_Sasvári/Sasvári_2013_Multivariate Characteristic and Correlation Functions.pdf}
}

@book{schervishTheoryStatistics1995,
  title = {Theory of {{Statistics}}},
  author = {Schervish, Mark J.},
  date = {1995},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-4250-5},
  isbn = {978-1-4612-8708-7 978-1-4612-4250-5},
  langid = {english},
  keywords = {ANOVA,bayesian statistics,best fit,Estimator,Likelihood,Probability distribution,Probability theory,Variance},
  file = {/Users/felix/paper/1995_Schervish/Schervish_1995_Theory of Statistics.pdf}
}

@article{scheuererCovarianceModelsDivergenceFree2012,
  title = {Covariance {{Models}} for {{Divergence-Free}} and {{Curl-Free Random Vector Fields}}},
  author = {Scheuerer, Michael and Schlather, Martin},
  date = {2012-07-01},
  journaltitle = {Stochastic Models},
  volume = {28},
  number = {3},
  pages = {433--451},
  publisher = {Taylor \& Francis},
  issn = {1532-6349},
  doi = {10.1080/15326349.2012.699756},
  abstract = {We construct matrix-valued covariance functions and in ℝ2 and ℝ3, starting from an arbitrary scalar-valued variogram. It is shown that sufficiently smooth random vector fields (RVFs) with these covariance functions have divergence-free and curl-free sample paths, respectively. Conversely, essentially all models with such sample paths can be obtained via our construction. Extensions to space-time RVFs are possible. RVFs with divergence-free and curl-free sample paths can be utilised in meteorological applications e.g. for modelling and interpolating wind fields.},
  keywords = {Curl-free,Divergence-free,Matrix-valued covariance function,Primary 60G60,Sample path properties,Secondary 60G17 62H11 86A32,Vector-valued random field},
  file = {/Users/felix/Zotero/storage/KDSCGVKT/15326349.2012.html}
}

@thesis{scheurerComparisonModelsMethods2009,
  title = {A {{Comparison}} of {{Models}} and {{Methods}} for {{Spatial Interpolation}} in {{Statistics}} and {{Numerical Analysis}}},
  author = {Scheurer, Michael},
  date = {2009},
  institution = {Göttingen},
  doi = {10.53846/goediss-2461},
  langid = {english},
  pagetotal = {142},
  file = {/Users/felix/paper/2009_Scheurer/Scheurer_2009_A Comparison of Models and Methods for Spatial Interpolation in Statistics and.pdf}
}

@article{schlatherParametricModelBridging2017,
  title = {A Parametric Model Bridging between Bounded and Unbounded Variograms},
  author = {Schlather, Martin and Moreva, Olga},
  date = {2017},
  journaltitle = {Stat},
  volume = {6},
  number = {1},
  pages = {47--52},
  issn = {2049-1573},
  doi = {10.1002/sta4.134},
  abstract = {A simple variogram model with two parameters is presented that includes the power variogram for fractional Brownian motion, a modified De Wijsian model, the generalized Cauchy model and the multiquadric model. One parameter controls the sample path roughness of the process. The other parameter allows for a smooth transition between bounded and unbounded variograms, that is, between stationary and intrinsically stationary processes in a Gaussian framework, or between mixing and non-ergodic Brown–Resnick processes when modeling spatial extremes. Copyright © 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {geostatistics,simulation,spatial statistics,stochastic processes},
  file = {/Users/felix/paper/2017_Schlather_Moreva/Schlather_Moreva_2017_A parametric model bridging between bounded and unbounded variograms.pdf;/Users/felix/Zotero/storage/QV7VKKRM/sta4.html}
}

@inproceedings{schmidtDescendingCrowdedValley2021,
  title = {Descending through a {{Crowded Valley}} - {{Benchmarking Deep Learning Optimizers}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
  date = {2021-07-01},
  eprint = {2007.01547},
  eprinttype = {arXiv},
  pages = {9367--9376},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/schmidt21a.html},
  urldate = {2023-03-23},
  abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than 50,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/paper/2021_Schmidt et al/Schmidt et al_2021_Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers.pdf;/Users/felix/paper/2021_Schmidt et al/Schmidt et al_2021_Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers2.pdf;/Users/felix/paper/2021_Schmidt et al/Schmidt et al_2021_Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers.pdf;/Users/felix/Zotero/storage/TJ3EARHJ/2007.html}
}

@article{schnelliConvergenceRateTracy2022,
  title = {Convergence {{Rate}} to the {{Tracy}}–{{Widom Laws}} for the {{Largest Eigenvalue}} of {{Wigner Matrices}}},
  author = {Schnelli, Kevin and Xu, Yuanyuan},
  date = {2022-07-01},
  journaltitle = {Communications in Mathematical Physics},
  shortjournal = {Commun. Math. Phys.},
  volume = {393},
  number = {2},
  pages = {839--907},
  issn = {1432-0916},
  doi = {10.1007/s00220-022-04377-y},
  abstract = {We show that the fluctuations of the largest eigenvalue of a real symmetric or complex Hermitian Wigner matrix of size N converge to the Tracy–Widom laws at a rate \$\$O(N\textasciicircum\{-1/3+\textbackslash omega \})\$\$, as N tends to infinity. For Wigner matrices this improves the previous rate \$\$O(N\textasciicircum\{-2/9+\textbackslash omega \})\$\$obtained by Bourgade~(J Eur Math Soc, 2021) for generalized Wigner matrices. Our result follows from a Green function comparison theorem, originally introduced by Erdős et al. (Adv Math 229(3):1435–1515, 2012) to prove edge universality, on a finer spectral parameter scale with improved error estimates. The proof relies on the continuous Green function flow induced by a matrix-valued Ornstein–Uhlenbeck process. Precise estimates on leading contributions from the third and fourth order moments of the matrix entries are obtained using iterative cumulant expansions and recursive comparisons for correlation functions, along with uniform convergence estimates for correlation kernels of the Gaussian invariant ensembles.},
  langid = {english},
  file = {/Users/felix/paper/2022_Schnelli_Xu/Schnelli_Xu_2022_Convergence Rate to the Tracy–Widom Laws for the Largest Eigenvalue of Wigner.pdf}
}

@online{schnelliQuantitativeTracyWidomLaws2022,
  title = {Quantitative {{Tracy-Widom}} Laws for the Largest Eigenvalue of Generalized {{Wigner}} Matrices},
  author = {Schnelli, Kevin and Xu, Yuanyuan},
  date = {2022-08-03},
  eprint = {2207.00546},
  eprinttype = {arXiv},
  eprintclass = {math-ph},
  doi = {10.48550/arXiv.2207.00546},
  abstract = {We show that the fluctuations of the largest eigenvalue of any generalized Wigner matrix \$H\$ converge to the Tracy-Widom laws at a rate nearly \$O(N\textasciicircum\{-1/3\})\$, as the matrix dimension \$N\$ tends to infinity. We allow the variances of the entries of \$H\$ to have distinct values but of comparable sizes such that \$\textbackslash sum\_\{i\} \textbackslash mathbb\{E\}|h\_\{ij\}|\textasciicircum 2=1\$. Our result improves the previous rate \$O(N\textasciicircum\{-2/9\})\$ by Bourgade [8] and the proof relies on the first long-time Green function comparison theorem near the edges without the second moment matching restriction.},
  pubstate = {prepublished},
  keywords = {15B52 60B20,Mathematical Physics,Mathematics - Probability},
  file = {/Users/felix/paper/2022_Schnelli_Xu/Schnelli_Xu_2022_Quantitative Tracy-Widom laws for the largest eigenvalue of generalized Wigner.pdf;/Users/felix/Zotero/storage/UM8RRKTN/2207.html}
}

@article{schoenbergPositiveDefiniteFunctions1942,
  title = {Positive Definite Functions on Spheres},
  author = {Schoenberg, Isaac J.},
  date = {1942-03},
  journaltitle = {Duke Mathematical Journal},
  volume = {9},
  number = {1},
  pages = {96--108},
  publisher = {Duke University Press},
  issn = {0012-7094, 1547-7398},
  doi = {10.1215/S0012-7094-42-00908-6},
  abstract = {Duke Mathematical Journal},
  keywords = {42.4X},
  file = {/Users/felix/paper/1942_Schoenberg/Schoenberg_1942_Positive definite functions on spheres.pdf}
}

@inproceedings{scieurUniversalAverageCaseOptimality2020,
  title = {Universal {{Average-Case Optimality}} of {{Polyak Momentum}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Scieur, Damien and Pedregosa, Fabian},
  date = {2020-11-21},
  pages = {8565--8572},
  publisher = {PMLR},
  location = {Virtual Event (formerly Vienna)},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/scieur20a.html},
  urldate = {2023-11-09},
  abstract = {Polyak momentum (PM), also known as the heavy-ball method, is a widely used optimization method that enjoys an asymptotic optimal worst-case complexity on quadratic objectives. However, its remarkable empirical success is not fully explained by this optimality, as the worst-case analysis –contrary to the average-case– is not representative of the expected complexity of an algorithm. In this work we establish a novel link between PM and the average-case analysis. Our main contribution is to prove that any optimal average-case method converges in the number of iterations to PM, under mild assumptions. This brings a new perspective on this classical method, showing that PM is asymptotically both worst-case and average-case optimal.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/felix/paper/2020_Scieur_Pedregosa/Scieur_Pedregosa_2020_Universal Average-Case Optimality of Polyak Momentum.pdf}
}

@thesis{sellkeHighDimensionalProblemsProbability2022,
  type = {phdthesis},
  title = {High-{{Dimensional Problems In Probability}}, {{Optimization}}, and {{Learning}}},
  author = {Sellke, Mark},
  date = {2022},
  institution = {Stanford University},
  location = {United States -- California},
  url = {https://www.proquest.com/docview/2734703062/abstract/6295839FB3E8442CPQ/1},
  urldate = {2024-06-11},
  abstract = {This thesis concerns several problems in probability, optimization, and machine learning. In the first part we study mixing and sampling. We first revisit the riffle shuffle, generalizing the classic ``seven shuffles suffice'' result of Bayer and Diaconis to shuffles with asymmetric cuts. We then study our first spin glass model, aiming to sample from the Sherrington-Kirkpatrick Gibbs measure in the high-temperature phase. We develop a new approach based not on a Markov Chain, but instead on Eldan's stochastic localization. Moreover we prove a hardness result for stable algorithms using Chatterjee's disorder chaos. In the second part we turn to optimization, aiming to find approximate ground states in spin glass models. This problem is intimately related to their low temperature behavior, and the limiting ground state energy is given by the Parisi formula at zero temperature. We determine an exact algorithmic threshold for a natural class of stable algorithms, which is achieved by approximate message passing algorithms. Our hardness results stem from a refined landscape property that we christen the branching overlap gap property. The third part concerns two problems in high-dimensional machine learning. We first study the problem of chasing convex bodies, in which one aims to perform stable convex optimization to obtain robust performance guarantees in changing environments. The solution involves a generalization of the classical Steiner point in convex geometry and its connections to Lipschitz selection. Finally we establish the law of robustness, showing that a natural robustness memorization task in high dimension requires extremely overparametrized machine learning models.},
  isbn = {9798357502827},
  langid = {english},
  pagetotal = {390},
  keywords = {Algorithms,Artificial intelligence,Asymmetry,Computer science,Localization,Machine learning,Markov analysis,Operations research,Optimization,Phase transitions,Symmetry},
  file = {/Users/felix/paper/2022_Sellke/Sellke_2022_High-Dimensional Problems In Probability, Optimization, and Learning.pdf}
}

@incollection{sellkeLecture10Concentration2024,
  title = {Lecture 10: {{Concentration}} for {{Langevin Dynamics}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-22},
  langid = {english},
  annotation = {Scribe: Alan Chung},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 10.pdf}
}

@incollection{sellkeLecture11Bounds2024,
  title = {Lecture 11: {{Bounds}} on {{FN}} (β) in the {{High-Temperature Case}} with {{External Field}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-27},
  langid = {english},
  annotation = {Scribe: Peter Luo},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 11.pdf}
}

@incollection{sellkeLecture12Shattering2024,
  title = {Lecture 12: {{Shattering I}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-29},
  langid = {english},
  annotation = {Scribe: Peter Luo},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 12.pdf}
}

@incollection{sellkeLecture13Shattering2024,
  title = {Lecture 13: {{Shattering II}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-03-03},
  langid = {english},
  annotation = {Scribe: Yufan Li},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 13.pdf}
}

@incollection{sellkeLecture14Shattering2024,
  title = {Lecture 14: {{Shattering III}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-03-07},
  annotation = {Scribe: Alan Chung},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 14.pdf}
}

@incollection{sellkeLecture15Subag2024,
  title = {Lecture 15: {{Subag}}’s {{Optimization Algorithm}} and {{Ultrametricity}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-03-19},
  langid = {english},
  annotation = {Scribe: Kenny Gu},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 15.pdf}
}

@incollection{sellkeLecture16Overlap2024,
  title = {Lecture 16: {{Overlap Gap Property}} and {{Hardness}} of {{Optimization}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-03-21},
  abstract = {Today, we’ll introduce the overlap gap property (OGP), which is a geometric frameework to show algorithmic hardness for random optimization problems. To illustrate this framework, we’ll consider the problem of finding a maximum independent set in a sparse random graph. The OGP will demonstrate that a class of local algorithms will not be able to find such a set.},
  langid = {english},
  annotation = {Scribe: Sabarish Sainathan},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 16.pdf}
}

@incollection{sellkeLecture17MultiOGP2024,
  title = {Lecture 17: {{Multi-OGP}}},
  author = {Sellke, Mark},
  date = {2024-03-26},
  abstract = {STAT 291, Spring 2024: Random High-Dimensional Optimization: Landscapes and Algorithmic Barriers},
  langid = {english},
  annotation = {Scribe: Sabarish Sainathan},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 17.pdf}
}

@incollection{sellkeLecture18OGP2024,
  title = {Lecture 18: {{OGP}} for {{Inference}} ({{Sparse PCA}})},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-03-28},
  langid = {english},
  annotation = {Scribe: Neil Shah},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 18.pdf}
}

@incollection{sellkeLecture19Tight2024,
  title = {Lecture 19: {{Tight Hardness}} for {{Optimizing Spherical Spin Glasses}} via {{Branching OGP}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-04-03},
  langid = {english},
  annotation = {Scribe: Kevin Luo},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 19.pdf}
}

@incollection{sellkeLecture20Spherical2024,
  title = {Lecture 20: {{Towards}} the {{Spherical Parisi Formula}}: {{Ruelle Probability Cascades}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-04-04},
  langid = {english},
  annotation = {Scribe: Kevin Luo},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 20.pdf}
}

@incollection{sellkeLecture21Upper2024,
  title = {Lecture 21: {{Upper Bound}}: {{Guerra}}’s {{Interpolation Bound}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-04-09},
  langid = {english},
  annotation = {Scribe: Neil Shah},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 21.pdf}
}

@incollection{sellkeLecture22Proof2024,
  title = {Lecture 22: {{Proof}} of {{Spherical Parisi Formula}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-04-11},
  langid = {english},
  annotation = {Scribe: Kenny Gu},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 22.pdf}
}

@incollection{sellkeLectureConcentrationEnhancedSecond2024,
  title = {Lecture 3: {{Concentration-Enhanced Second Moment Method}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-01-30},
  langid = {english},
  annotation = {Scribe: Jarell Cheong Tze Wen},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 3.pdf}
}

@incollection{sellkeLectureFreeEnergies2024,
  title = {Lecture 2: {{Free Energies}} and {{Moment Computations}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-01-25},
  url = {https://msellke.com/courses/STAT_291/slides_notes/Stat291Lecture2.pdf},
  urldate = {2024-06-04},
  langid = {english},
  annotation = {Scribe: Jarell Cheong Tze Wen},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 2.pdf}
}

@incollection{sellkeLectureGeometricStatistical2024,
  title = {Lecture 4: {{Geometric}} and Statistical Consequences of Annealed Free Energy},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-01},
  langid = {english},
  annotation = {Scribe: Zad Chin},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 4.pdf}
}

@incollection{sellkeLectureKacRiceGeneral2024,
  title = {Lecture 5: {{Kac-Rice I}}: {{General Formula}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-06},
  langid = {english},
  annotation = {Scribe: Zad Chin},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 5.pdf}
}

@incollection{sellkeLectureKacRiceII2024,
  title = {Lecture 6: {{Kac-Rice II}}: Spherical Spin Glasses without External Field},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-08},
  langid = {english},
  annotation = {Scribe: Rushil Mallarapu},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 6.pdf}
}

@incollection{sellkeLectureKacRiceIII2024,
  title = {Lecture 7: {{Kac-Rice III}}: {{Second Moments}} and {{E}}∞ {{Threshold}}},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-13},
  langid = {english},
  annotation = {Scribe: Rushil Mallarapu},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 7.pdf}
}

@incollection{sellkeLectureKacRiceIV2024,
  title = {Lecture 8: {{Kac-Rice IV}}: Topological Trivialization},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-15},
  langid = {english},
  annotation = {Scribe: Xiaodong Yang},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 8.pdf}
}

@incollection{sellkeLectureLangevinDynamics2024,
  title = {Lecture 9: {{Langevin}} Dynamics},
  booktitle = {{{STAT}} 291, {{Spring}} 2024: {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  author = {Sellke, Mark},
  date = {2024-02-20},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Lecture 9.pdf}
}

@article{sellkeOptimizingMeanField2024,
  title = {Optimizing Mean Field Spin Glasses with External Field},
  author = {Sellke, Mark},
  date = {2024-01},
  journaltitle = {Electronic Journal of Probability},
  volume = {29},
  pages = {1--47},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1083-6489, 1083-6489},
  doi = {10.1214/23-EJP1066},
  abstract = {We consider the Hamiltonians of mean-field spin glasses, which are certain random functions HN defined on high-dimensional cubes or spheres in RN. The asymptotic maximum values of these functions were famously obtained by Talagrand and later by Panchenko and by Chen. The landscape of approximate maxima of HN is described by various forms of replica symmetry breaking exhibiting a broad range of behaviors. We study the problem of efficiently computing an approximate maximizer of HN. We give a two-phase message passing algorithm to approximately maximize HN when a no overlap gap condition holds. This generalizes the recent works [Sub21, Mon19, AMS21] by allowing a non-trivial external field. For even Ising spin glasses with constant external field, our algorithm succeeds exactly when existing methods fail to rule out approximate maximization for a wide class of algorithms. Moreover we give a branching variant of our algorithm which constructs a full ultrametric tree of approximate maxima.},
  issue = {none},
  keywords = {60G15,approximate message passing,optimization,Spin glasses},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_Optimizing mean field spin glasses with external field.pdf}
}

@unpublished{sellkeSTAT291Random2024,
  type = {Lecture Notes},
  title = {{{STAT}} 291, {{Random High-Dimensional Optimization}}: {{Landscapes}} and {{Algorithmic Barriers}}},
  shorttitle = {{{STAT}} 291},
  author = {Sellke, Mark},
  date = {2024},
  url = {https://msellke.com/courses/STAT_291/course_page_website.html},
  urldate = {2024-08-16},
  abstract = {This is a graduate topics course focusing on paradigmatic optimization problems with random objective functions. We will develop the tools needed to understand the geometric behavior of complex random landscapes, the different phase transitions that can occur, and how these transitions are linked to the success and failure of efficient algorithms. Topics will include: - Random constraint satisfaction problems, spin glasses, and tensor PCA - Landscape complexity via critical point counting - The overlap gap property as a geometric barrier to optimization - Implications for Markov chain sampling - Combining these ideas to prove the spherical Parisi formula},
  file = {/Users/felix/paper/2024_Sellke/Sellke_2024_STAT 291_Extra_Credits.pdf;/Users/felix/paper/2024_Sellke/Sellke_2024_STAT 291_HW1.pdf;/Users/felix/paper/2024_Sellke/Sellke_2024_STAT 291_HW2.pdf;/Users/felix/paper/2024_Sellke/Sellke_2024_STAT 291_HW3.pdf}
}

@article{shahriariTakingHumanOut2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2016-01},
  journaltitle = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2015.2494218},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Bayes methods,Big data,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,Linear programming,optimization,Optimization,response surface methodology,Statistical analysis,statistical learning},
  file = {/Users/felix/paper/2016_Shahriari et al/Shahriari et al_2016_Taking the Human Out of the Loop.pdf;/Users/felix/Zotero/storage/HNUAJ9FM/7352306.html}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  date = {1948-07},
  journaltitle = {The Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  eventtitle = {The {{Bell System Technical Journal}}},
  file = {/Users/felix/paper/1948_Shannon/[Original] Shannon_1948_A mathematical theory of communication.pdf;/Users/felix/paper/1948_Shannon/Shannon_1948_A mathematical theory of communication.pdf;/Users/felix/Zotero/storage/A5HFSYR5/6773024.html}
}

@book{skorokhodBasicPrinciplesApplications2005,
  title = {Basic {{Principles}} and {{Applications}} of {{Probability Theory}}},
  author = {Skorokhod, A. V.},
  editor = {family=Prokhorov, given=Yu.V., given-i={{Yu}}V},
  date = {2005},
  publisher = {Springer-Verlag},
  location = {Berlin/Heidelberg},
  doi = {10.1007/b137401},
  isbn = {978-3-540-54686-3},
  langid = {english},
  keywords = {Markov process,probability,probability space,Probability theory,random function,randomness,stochastic processes},
  file = {/Users/felix/paper/2005_Skorokhod_Prokhorov/Skorokhod_Prokhorov_2005_Basic Principles and Applications of Probability Theory.pdf}
}

@article{smaleAverageNumberSteps1983,
  title = {On the Average Number of Steps of the Simplex Method of Linear Programming},
  author = {Smale, Steve},
  date = {1983-10-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Mathematical Programming},
  volume = {27},
  number = {3},
  pages = {241--262},
  issn = {1436-4646},
  doi = {10.1007/BF02591902},
  abstract = {The goal is to give some theoretical explanation for the efficiency of the simplex method of George Dantzig. Fixing the number of constraints and using Dantzig's self-dual parametric algorithm, we show that the number of pivots required to solve a linear programming problem grows in proportion to the number of variables on the average.},
  langid = {english},
  keywords = {Algorithms,Complexity Theory,Linear Complementarity Problem,Linear Programming,Path Following,Simplex Method},
  file = {/Users/felix/paper/1983_Smale/Smale_1983_On the average number of steps of the simplex method of linear programming.pdf}
}

@inproceedings{smithCyclicalLearningRates2017,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  booktitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Smith, Leslie N.},
  date = {2017-03},
  pages = {464--472},
  doi = {10.1109/WACV.2017.58},
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  eventtitle = {2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  keywords = {Computational efficiency,Computer architecture,Neural networks,Schedules,Training,Tuning},
  file = {/Users/felix/paper/2017_Smith/Smith_2017_Cyclical Learning Rates for Training Neural Networks.pdf;/Users/felix/Zotero/storage/F6RH6K44/7926641.html}
}

@online{smithDisciplinedApproachNeural2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  date = {2018-04-24},
  eprint = {1803.09820},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1803.09820},
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/paper/2018_Smith/Smith_2018_A disciplined approach to neural network hyper-parameters.pdf;/Users/felix/Zotero/storage/APBXGKDL/1803.html}
}

@article{spielmanSmoothedAnalysisAlgorithms2004,
  title = {Smoothed Analysis of Algorithms: {{Why}} the Simplex Algorithm Usually Takes Polynomial Time},
  shorttitle = {Smoothed Analysis of Algorithms},
  author = {Spielman, Daniel A. and Teng, Shang-Hua},
  date = {2004-05-01},
  journaltitle = {Journal of the ACM},
  shortjournal = {J. ACM},
  volume = {51},
  number = {3},
  pages = {385--463},
  issn = {0004-5411},
  doi = {10.1145/990308.990310},
  abstract = {We introduce the smoothed analysis of algorithms, which continuously interpolates between the worst-case and average-case analyses of algorithms. In smoothed analysis, we measure the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. We measure this performance in terms of both the input size and the magnitude of the perturbations. We show that the simplex algorithm has smoothed complexity polynomial in the input size and the standard deviation of Gaussian perturbations.},
  keywords = {complexity,perturbation,Simplex method,smoothed analysis},
  file = {/Users/felix/paper/2004_Spielman_Teng/Spielman_Teng_2004_Smoothed analysis of algorithms.pdf}
}

@inproceedings{srinivasGaussianProcessOptimization2010,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}:  {{No Regret}} and {{Experimental Design}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Machine Learning}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  date = {2010},
  eprint = {0912.3995},
  eprinttype = {arXiv},
  location = {Haifa, Israel},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  eventtitle = {{{ICML}}},
  langid = {english},
  file = {/Users/felix/paper/2010_Srinivas et al/[Arxiv Extended] Srinivas et al_2010_Gaussian Process Optimization in the Bandit Setting.pdf;/Users/felix/paper/2010_Srinivas et al/Srinivas et al_2010_Gaussian Process Optimization in the Bandit Setting.pdf}
}

@article{srinivasInformationTheoreticRegretBounds2012,
  title = {Information-{{Theoretic Regret Bounds}} for {{Gaussian Process Optimization}} in the {{Bandit Setting}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M. and Seeger, Matthias W.},
  date = {2012-05},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {58},
  number = {5},
  pages = {3250--3265},
  issn = {1557-9654},
  doi = {10.1109/TIT.2011.2182033},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low norm in a reproducing kernel Hilbert space. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze an intuitive Gaussian process upper confidence bound (GP-UCB) algorithm, and bound its cumulative regret in terms of maximal in- formation gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  keywords = {Bandit problems,Bayesian methods,Bayesian prediction,Convergence,experimental design,Gaussian process (GP),Gaussian processes,information gain,Kernel,Noise,nonparametric statistics,online learning,Optimization,regret bound,statistical learning,Temperature sensors},
  file = {/Users/felix/paper/2012_Srinivas et al/Srinivas et al_2012_Information-Theoretic Regret Bounds for Gaussian Process Optimization in the.pdf;/Users/felix/Zotero/storage/C5AUD33C/6138914.html}
}

@book{steinInterpolationSpatialData1999,
  title = {Interpolation of {{Spatial Data}}},
  author = {Stein, Michael L.},
  date = {1999},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4612-1494-6},
  isbn = {978-1-4612-7166-6 978-1-4612-1494-6},
  keywords = {digital elevation model,geographic data,Kriging,Likelihood,linear optimization,Normal distribution,Spatial Data,Spatial Statistics,STATISTICA,Variance},
  file = {/Users/felix/paper/1999_Stein/Stein_1999_Interpolation of Spatial Data.pdf}
}

@article{subagFollowingGroundStates2021,
  title = {Following the {{Ground States}} of {{Full-RSB Spherical Spin Glasses}}},
  author = {Subag, Eliran},
  date = {2021},
  journaltitle = {Communications on Pure and Applied Mathematics},
  volume = {74},
  number = {5},
  pages = {1021--1044},
  issn = {1097-0312},
  doi = {10.1002/cpa.21922},
  abstract = {We focus on spherical spin glasses whose Parisi distribution has support of the form [0, q]. For such models we construct paths from the origin to the sphere that consistently remain close to the ground-state energy on the sphere of corresponding radius. The construction uses a greedy strategy, which always follows a direction corresponding to the most negative eigenvalues of the Hessian of the Hamiltonian. For finite mixtures ξ(x) it provides an algorithm of time complexity O(Ndeg(ξ)) to find w.h.p. points with the ground-state energy, up to a small error. For the pure spherical models, the same algorithm reaches the energy −E∞, the conjectural terminal energy for gradient descent. Using the TAP formula for the free energy, for full-RSB models with support [0, q], we are able to prove the correct lower bound on the free energy (namely, prove the lower bound from Parisi's formula), assuming the correctness of the Parisi formula only in the replica symmetric case.},
  langid = {english},
  file = {/Users/felix/paper/2021_Subag/Subag_2021_Following the Ground States of Full-RSB Spherical Spin Glasses.pdf;/Users/felix/Zotero/storage/JG5Y44BS/cpa.html}
}

@unpublished{subagFreeEnergyLandscapes2018,
  title = {Free Energy Landscapes in Spherical Spin Glasses},
  author = {Subag, Eliran},
  date = {2018-10},
  file = {/Users/felix/paper/2018_Subag/Subag_2018_Free energy landscapes in spherical spin glasses.pdf}
}

@online{subagFreeEnergyLandscapes2018a,
  title = {Free Energy Landscapes in Spherical Spin Glasses},
  author = {Subag, Eliran},
  date = {2018-04},
  eprint = {1804.10576},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1804.10576},
  abstract = {We introduce and analyze free energy landscapes defined by associating to any point inside the sphere a free energy calculated on a thin spherical band around it, using many orthogonal replicas. This allows us to reinterpret, rigorously prove and extend for general spherical models the main ideas of the Thouless-Anderson-Palmer (TAP) approach originally introduced in the 70s for the Sherrington-Kirkpatrick model. In particular, we establish a TAP representation for the free energy, valid for any overlap value which can be sampled as many times as we wish in an appropriate sense. We call such overlaps multi-samplable. The correction to the Hamiltonian in the TAP representation arises in our analysis as the free energy of a certain model on an overlap dependent band. For the largest multi-samplable overlap it coincides with the Onsager reaction term from physics. For smaller multi-samplable overlaps the formula we obtain is new. We also derive the corresponding TAP equation for critical points. We prove all the above without appealing to the celebrated Parisi formula or the ultrametricity property. We prove that any overlap value in the support of the Parisi measure is multi-samplable. For generic models, we further show that the set of multi-samplable overlaps coincides with a certain set that arises in the characterization for the Parisi measure by Talagrand. The ultrametric tree of pure states can be embedded in the interior of the sphere in a natural way. For this embedding, we show that the points on the tree uniformly maximize the free energies we define. From this we conclude that the Hamiltonian at each point on the tree is approximately maximal over the sphere of same radius, and that points on the tree approximately solve the TAP equations for critical points.},
  pubstate = {prepublished},
  keywords = {Mathematics - Probability},
  file = {/Users/felix/paper/2018_Subag/[v6] Subag_2020_Free energy landscapes in spherical spin glasses.pdf;/Users/felix/paper/2018_Subag/Subag_2018_Free energy landscapes in spherical spin glasses2.pdf;/Users/felix/Zotero/storage/5GGHBFSA/1804.html}
}

@online{surjanovicVirtualLibrarySimulation,
  title = {Virtual Library of Simulation Experiments: {{Test}} Functions and Datasets},
  author = {Surjanovic, S. and Bingham, D.},
  url = {https://www.sfu.ca/~ssurjano/optimization.html},
  urldate = {2023-04-21},
  file = {/Users/felix/Zotero/storage/D6Q89UAL/optimization.html}
}

@unpublished{swirszczLocalMinimaTraining2017,
  title = {Local Minima in Training of Neural Networks},
  author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  date = {2017-02-17},
  eprint = {1611.06310},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.06310},
  urldate = {2021-06-16},
  abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/felix/paper/2017_Swirszcz et al/Swirszcz et al_2017_Local minima in training of neural networks.pdf;/Users/felix/Zotero/storage/NN572T58/1611.html}
}

@article{swirszczLocalMinimaTraining2022,
  title = {Local Minima in Training of Deep Networks},
  author = {Swirszcz, Grzegorz and Czarnecki, Wojciech Marian and Pascanu, Razvan},
  date = {2022-07-21},
  url = {https://openreview.net/forum?id=Syoiqwcxx},
  urldate = {2022-09-02},
  abstract = {There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.},
  langid = {english},
  file = {/Users/felix/paper/2022_Swirszcz et al/Swirszcz et al_2022_Local minima in training of deep networks.pdf;/Users/felix/Zotero/storage/QJME2G3L/forum.html}
}

@article{talagrandFreeEnergySpherical2006a,
  title = {Free Energy of the Spherical Mean Field Model},
  author = {Talagrand, Michel},
  date = {2006-03-01},
  journaltitle = {Probability Theory and Related Fields},
  shortjournal = {Probab. Theory Relat. Fields},
  volume = {134},
  number = {3},
  pages = {339--382},
  issn = {1432-2064},
  doi = {10.1007/s00440-005-0433-8},
  abstract = {We compute at any temperature the free energy of the multi p-spin spherical model when only terms for p even are considered.},
  langid = {english},
  keywords = {Free Energy,Gibbs Measure,Independent Copy,Replica Symmetry,Spherical Model},
  file = {/Users/felix/paper/2006_Talagrand/Talagrand_2006_Free energy of the spherical mean field model2.pdf}
}

@article{talagrandParisiFormula2006,
  title = {The {{Parisi Formula}}},
  author = {Talagrand, Michel},
  date = {2006},
  journaltitle = {Annals of Mathematics},
  volume = {163},
  number = {1},
  eprint = {20159953},
  eprinttype = {jstor},
  pages = {221--263},
  publisher = {Annals of Mathematics},
  issn = {0003-486X},
  url = {https://www.jstor.org/stable/20159953},
  urldate = {2024-05-28},
  abstract = {Using Guerra's interpolation scheme, we compute the free energy of the Sherrington-Kirkpatrick model for spin glasses at any temperature, confirming a celebrated prediction of G. Parisi.},
  file = {/Users/felix/paper/2006_Talagrand/Talagrand_2006_The Parisi Formula.pdf}
}

@online{taoCentralLimitTheorem2012,
  title = {A Central Limit Theorem for the Determinant of a {{Wigner}} Matrix},
  author = {Tao, Terence and Vu, Van},
  date = {2012-03-29},
  eprint = {1111.6300},
  eprinttype = {arXiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1111.6300},
  urldate = {2022-12-20},
  abstract = {We establish a central limit theorem for the log-determinant \$\textbackslash log|\textbackslash det(M\_n)|\$ of a Wigner matrix \$M\_n\$, under the assumption of four matching moments with either the GUE or GOE ensemble. More specifically, we show that this log-determinant is asymptotically distributed like \$N(\textbackslash log \textbackslash sqrt\{n!\} - 1/2 \textbackslash log n, 1/2 \textbackslash log n)\_\textbackslash R\$ when one matches moments with GUE, and \$N(\textbackslash log \textbackslash sqrt\{n!\} - 1/4 \textbackslash log n, 1/4 \textbackslash log n)\_\textbackslash R\$ when one matches moments with GOE.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {15A52,Mathematics - Probability},
  file = {/Users/felix/paper/2012_Tao_Vu/Tao_Vu_2012_A central limit theorem for the determinant of a Wigner matrix.pdf}
}

@book{taylorMeasureTheoryIntegration2006,
  title = {Measure Theory and Integration},
  author = {Taylor, Michael Eugene},
  date = {2006},
  series = {Graduate {{Studies}} in {{Mathematics}}},
  volume = {76},
  publisher = {American Mathematical Society},
  location = {Providence, R.I},
  abstract = {This self-contained treatment of measure and integration begins with a brief review of the Riemann integral and proceeds to a construction of Lebesgue measure on the real line. From there the reader is led to the general notion of measure, to the construction of the Lebesgue integral on a measure space, and to the major limit theorems, such as the Monotone and Dominated Convergence Theorems. The treatment proceeds to \$L\textasciicircum p\$ spaces, normed linear spaces that are shown to be complete (i.e., Banach spaces) due to the limit theorems. Particular attention is paid to \$L\textasciicircum 2\$ spaces as Hilbert spaces, with a useful geometrical structure. Having gotten quickly to the heart of the matter, the text proceeds to broaden its scope.There are further constructions of measures, including Lebesgue measure on \$n\$-dimensional Euclidean space. There are also discussions of surface measure, and more generally of Riemannian manifolds and the measures they inherit, and an appendix on the integration of differential forms. Further geometric aspects are explored in a chapter on Hausdorff measure. The text also treats probabilistic concepts, in chapters on ergodic theory, probability spaces and random variables, Wiener measure and Brownian motion, and martingales. This text will prepare graduate students for more advanced studies in functional analysis, harmonic analysis, stochastic analysis, and geometric measure theory.},
  isbn = {978-0-8218-4180-8},
  langid = {english},
  keywords = {Convergence,Measure theory,Probabilities,Riemann integral},
  file = {/Users/felix/paper/2006_Taylor/Taylor_2006_Measure theory and integration.pdf}
}

@misc{taylorRandomFieldsStationarity2023,
  title = {Random Fields: Stationarity, Ergodicity, and Spectral Behavior},
  shorttitle = {Random Fields},
  author = {Taylor, Michael},
  date = {2023},
  url = {https://mtaylor.web.unc.edu/wp-content/uploads/sites/16915/2018/04/rndfcn.pdf},
  urldate = {2023-11-03},
  file = {/Users/felix/paper/_Taylor/Taylor_Random fields.pdf}
}

@article{toddProbabilisticModelsLinear1991,
  title = {Probabilistic {{Models}} for {{Linear Programming}}},
  author = {Todd, Michael J.},
  date = {1991-11},
  journaltitle = {Mathematics of Operations Research},
  shortjournal = {Mathematics of OR},
  volume = {16},
  number = {4},
  pages = {671--693},
  publisher = {INFORMS},
  issn = {0364-765X},
  doi = {10.1287/moor.16.4.671},
  abstract = {We propose and investigate new probabilistic models for linear programming. In contrast to previous models, ours guarantee the existence of optimal solutions and are symmetric under duality. While in some respects our distributions are very special, there is sufficient flexibility to permit an arbitrary degree of primal and/or dual degeneracy, either just at the optimal solution or throughout the feasible region using null variables. Moreover, the precision of the distributions allows us to compute the probability that the feasible region is bounded as well as the distribution of the distance to a constraint hyperplane and that of the components of a vertex. Interest in these measures stems from Karmarkar's algorithm, and we also introduce a model for generating random linear programming problems on a simplex.},
  keywords = {Gaussian distributions,Karmarkar's algorithm,linear programming,probabilistic models},
  file = {/Users/felix/paper/1991_Todd/Todd_1991_Probabilistic Models for Linear Programming.pdf}
}

@inproceedings{tranCloserLookSpatiotemporal2018,
  title = {A {{Closer Look}} at {{Spatiotemporal Convolutions}} for {{Action Recognition}}},
  author = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  date = {2018},
  pages = {6450--6459},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html},
  urldate = {2024-04-02},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/felix/paper/2018_Tran et al/Tran et al_2018_A Closer Look at Spatiotemporal Convolutions for Action Recognition.pdf}
}

@online{truongBacktrackingGradientDescent2018,
  title = {Backtracking Gradient Descent Method for General $C^1$ Functions, with Applications to {{Deep Learning}}},
  author = {Truong, Tuyen Trung and Nguyen, Tuan Hang},
  date = {2018-08},
  doi = {10.48550/arXiv.1808.05160},
  abstract = {While Standard gradient descent is one very popular optimisation method, its convergence cannot be proven beyond the class of functions whose gradient is globally Lipschitz continuous. As such, it is not actually applicable to realistic applications such as Deep Neural Networks. In this paper, we prove that its backtracking variant behaves very nicely, in particular convergence can be shown for all Morse functions. The main theoretical result of this paper is as follows. Theorem. Let \$f:\textbackslash mathbb\{R\}\textasciicircum k\textbackslash rightarrow \textbackslash mathbb\{R\}\$ be a \$C\textasciicircum 1\$ function, and \$\textbackslash\{z\_n\textbackslash\}\$ a sequence constructed from the Backtracking gradient descent algorithm. (1) Either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_\{n+1\}-z\_n||=0\$. (2) Assume that \$f\$ has at most countably many critical points. Then either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash\{z\_n\textbackslash\}\$ converges to a critical point of \$f\$. (3) More generally, assume that all connected components of the set of critical points of \$f\$ are compact. Then either \$\textbackslash lim \_\{n\textbackslash rightarrow\textbackslash infty\}||z\_n||=\textbackslash infty\$ or \$\textbackslash\{z\_n\textbackslash\}\$ is bounded. Moreover, in the latter case the set of cluster points of \$\textbackslash\{z\_n\textbackslash\}\$ is connected. Some generalised versions of this result, including an inexact version, are included. Another result in this paper concerns the problem of saddle points. We then present a heuristic argument to explain why Standard gradient descent method works so well, and modifications of the backtracking versions of GD, MMT and NAG. Experiments with datasets CIFAR10 and CIFAR100 on various popular architectures verify the heuristic argument also for the mini-batch practice and show that our new algorithms, while automatically fine tuning learning rates, perform better than current state-of-the-art methods such as MMT, NAG, Adagrad, Adadelta, RMSProp, Adam and Adamax.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/felix/paper/2019_Truong_Nguyen/Truong_Nguyen_2019_Backtracking gradient descent method for general $C^1$ functions, with.pdf;/Users/felix/Zotero/storage/43XEVGBC/1808.html}
}

@article{tsagrisFoldedNormalDistribution2014,
  title = {On the {{Folded Normal Distribution}}},
  author = {Tsagris, Michail and Beneki, Christina and Hassani, Hossein},
  date = {2014-03},
  journaltitle = {Mathematics},
  volume = {2},
  number = {1},
  pages = {12--28},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math2010012},
  abstract = {The characteristic function of the folded normal distribution and its moment function are derived. The entropy of the folded normal distribution and the Kullback–Leibler from the normal and half normal distributions are approximated using Taylor series. The accuracy of the results are also assessed using different criteria. The maximum likelihood estimates and confidence intervals for the parameters are obtained using the asymptotic theory and bootstrap method. The coverage of the confidence intervals is also examined.},
  issue = {1},
  langid = {english},
  keywords = {entropy,folded normal distribution,Kullback–Leibler,maximum likelihood estimates},
  file = {/Users/felix/paper/2014_Tsagris et al/Tsagris et al_2014_On the Folded Normal Distribution.pdf}
}

@article{vandersluisConditionNumbersEquilibration1969,
  title = {Condition Numbers and Equilibration of Matrices},
  author = {Van der Sluis, Abraham},
  date = {1969},
  journaltitle = {Numerische Mathematik},
  volume = {14},
  number = {1},
  pages = {14--23},
  publisher = {Springer-Verlag Berlin/Heidelberg},
  url = {https://user.it.uu.se/~maya/Courses/NLA/Projects/2014/VanderSluis1963.pdf},
  urldate = {2024-04-02},
  file = {/Users/felix/paper/1969_Van der Sluis/Van der Sluis_1969_Condition numbers and equilibration of matrices.pdf}
}

@article{wangBayesianOptimizationBillion2016,
  title = {Bayesian {{Optimization}} in a {{Billion Dimensions}} via {{Random Embeddings}}},
  author = {Wang, Ziyu and Hutter, Frank and Zoghi, Masrour and Matheson, David and family=Feitas, given=Nando, prefix=de, useprefix=false},
  date = {2016-02-19},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {55},
  pages = {361--387},
  issn = {1076-9757},
  doi = {10.1613/jair.4806},
  abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.},
  langid = {english},
  file = {/Users/felix/paper/2016_Wang et al/Wang et al_2016_Bayesian Optimization in a Billion Dimensions via Random Embeddings.pdf}
}

@article{wangRecentAdvancesBayesian2023,
  title = {Recent {{Advances}} in {{Bayesian Optimization}}},
  author = {Wang, Xilu and Jin, Yaochu and Schmitt, Sebastian and Olhofer, Markus},
  date = {2023-07-13},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {55},
  pages = {287:1--287:36},
  issn = {0360-0300},
  doi = {10.1145/3582078},
  abstract = {Bayesian optimization has emerged at the forefront of expensive black-box optimization due to its data efficiency. Recent years have witnessed a proliferation of studies on the development of new Bayesian optimization algorithms and their applications. Hence, this article attempts to provide a comprehensive and updated survey of recent advances in Bayesian optimization that are mainly based on Gaussian processes and identify challenging open problems. We categorize the existing work on Bayesian optimization into nine main groups according to the motivations and focus of the proposed algorithms. For each category, we present the main advances with respect to the construction of surrogate models and adaptation of the acquisition functions. Finally, we discuss the open questions and suggest promising future research directions, in particular with regard to heterogeneity, privacy preservation, and fairness in distributed and federated optimization systems.},
  issue = {13s},
  keywords = {acquisition function,Bayesian optimization,Gaussian process},
  file = {/Users/felix/paper/2023_Wang et al/Wang et al_2023_Recent Advances in Bayesian Optimization.pdf}
}

@online{wangTheoreticalAnalysisBayesian2014,
  title = {Theoretical {{Analysis}} of {{Bayesian Optimisation}} with {{Unknown Gaussian Process Hyper-Parameters}}},
  author = {Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2014-06-30},
  eprint = {1406.7758},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1406.7758},
  abstract = {Bayesian optimisation has gained great popularity as a tool for optimising the parameters of machine learning algorithms and models. Somewhat ironically, setting up the hyper-parameters of Bayesian optimisation methods is notoriously hard. While reasonable practical solutions have been advanced, they can often fail to find the best optima. Surprisingly, there is little theoretical analysis of this crucial problem in the literature. To address this, we derive a cumulative regret bound for Bayesian optimisation with Gaussian processes and unknown kernel hyper-parameters in the stochastic setting. The bound, which applies to the expected improvement acquisition function and sub-Gaussian observation noise, provides us with guidelines on how to design hyper-parameter estimation methods. A simple simulation demonstrates the importance of following these guidelines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/paper/2014_Wang_de Freitas/Wang_de Freitas_2014_Theoretical Analysis of Bayesian Optimisation with Unknown Gaussian Process.pdf;/Users/felix/Zotero/storage/LXWHFLGR/1406.html}
}

@article{warrickOptimizationSamplingLocations1987,
  title = {Optimization of Sampling Locations for Variogram Calculations},
  author = {Warrick, A. W. and Myers, D. E.},
  date = {1987},
  journaltitle = {Water Resources Research},
  volume = {23},
  number = {3},
  pages = {496--500},
  issn = {1944-7973},
  doi = {10.1029/WR023i003p00496},
  abstract = {A method is presented and demonstrated for optimizing the selection of sample locations for variogram estimation. It is assumed that the distribution of distance classes is decided a priori and the problem therefore is to closely approximate the preselected distribution, although the dispersion within individual classes can also be considered. All of the locations may be selected or points added to an existing set of sites or to those chosen on regular patterns. In the examples, the sum of squares characterizing the deviation from the desired distribution of couples is reduced by as much as 2 orders of magnitude between random and optimized points. The calculations may be carried out on a microcomputer. Criteria for what constitutes best estimators for variogram are discussed, but a study of variogram estimators is not the object of this paper.},
  langid = {english},
  file = {/Users/felix/paper/1987_Warrick_Myers/Warrick_Myers_1987_Optimization of sampling locations for variogram calculations.pdf;/Users/felix/Zotero/storage/99CUC7HK/WR023i003p00496.html}
}

@article{wasilkowskiAverageComplexityGlobal1992,
  title = {On Average Complexity of Global Optimization Problems},
  author = {Wasilkowski, G. W.},
  date = {1992-05-01},
  journaltitle = {Mathematical Programming},
  shortjournal = {Mathematical Programming},
  volume = {57},
  number = {1},
  pages = {313--324},
  issn = {1436-4646},
  doi = {10.1007/BF01581086},
  abstract = {We discuss the average case complexity of global optimization problems. By the average complexity, we roughly mean the amount of work needed to solve the problem with the expected error not exceeding a preassigned error demand. The expectation is taken with respect to a probability measure on a classF of objective functions.},
  langid = {english},
  keywords = {Case Complexity,Global Optimization,Mathematical Method,Objective Function,Probability Measure},
  file = {/Users/felix/paper/1992_Wasilkowski/Wasilkowski_1992_On average complexity of global optimization problems.pdf}
}

@article{williamsBayesianClassificationGaussian1998,
  title = {Bayesian Classification with {{Gaussian}} Processes},
  author = {Williams, C.K.I. and Barber, D.},
  date = {1998-12},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {20},
  number = {12},
  pages = {1342--1351},
  issn = {1939-3539},
  doi = {10.1109/34.735807},
  abstract = {We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the probability of class one given x is estimated by /spl sigma/(y(x)), where /spl sigma/(y)=1/(1+e/sup -y/). A Gaussian process prior is placed on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation. The method is generalized to multiclass problems (m{$>$}2) using the softmax function. We demonstrate the effectiveness of the method on a number of datasets.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/Users/felix/paper/1998_Williams_Barber/Williams_Barber_1998_Bayesian classification with Gaussian processes.pdf;/Users/felix/Zotero/storage/EJVE6YL8/735807.html}
}

@article{williamsComputationInfiniteNeural1998,
  title = {Computation with {{Infinite Neural Networks}}},
  author = {Williams, Christopher K. I.},
  date = {1998-07-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {10},
  number = {5},
  pages = {1203--1216},
  issn = {0899-7667},
  doi = {10.1162/089976698300017412},
  abstract = {For neural networks with a wide class of weight priors, it can be shown that in the limit of an infinite number of hidden units, the prior over functions tends to a gaussian process. In this article, analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units and shows, somewhat paradoxically, that it may be easier to carry out Bayesian prediction with infinite networks rather than finite ones.},
  file = {/Users/felix/paper/1998_Williams/Williams_1998_Computation with Infinite Neural Networks.pdf;/Users/felix/Zotero/storage/LZEA6EBK/Computation-with-Infinite-Neural-Networks.html}
}

@inproceedings{williamsComputingInfiniteNetworks1996,
  title = {Computing with {{Infinite Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Williams, Christopher},
  date = {1996},
  volume = {9},
  publisher = {MIT Press},
  url = {https://proceedings.neurips.cc/paper/1996/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html},
  urldate = {2022-12-01},
  abstract = {For  neural  networks  with  a  wide  class  of weight-priors,  it  can  be  shown  that  in  the  limit  of an  infinite  number of hidden  units  the  prior over functions  tends to a  Gaussian process.  In  this paper analytic forms are derived for the covariance function of the Gaussian  processes  corresponding  to networks with sigmoidal and Gaussian  hidden  units.  This  allows predictions  to  be  made efficiently  using  networks  with an infinite number of hidden units,  and shows  that,  somewhat paradoxically, it may be  easier  to compute with infinite  networks  than finite  ones.},
  file = {/Users/felix/paper/1996_Williams/Williams_1996_Computing with Infinite Networks.pdf}
}

@inproceedings{wilsonDeepKernelLearning2016,
  title = {Deep {{Kernel Learning}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
  date = {2016-05-02},
  pages = {370--378},
  publisher = {PMLR},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v51/wilson16.html},
  urldate = {2023-10-23},
  abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods.  Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation.  These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability.  We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process.  Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point.  On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/Users/felix/paper/2016_Wilson et al/Wilson et al_2016_Deep Kernel Learning.pdf}
}

@inproceedings{wilsonMarginalValueAdaptive2017,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  date = {2017},
  volume = {30},
  eprint = {1705.08292},
  eprinttype = {arXiv},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html},
  urldate = {2023-03-23},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks.  Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD).  We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half.  We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/paper/2017_Wilson et al/Wilson et al_2017_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf;/Users/felix/paper/2018_Wilson et al/Wilson et al_2018_The Marginal Value of Adaptive Gradient Methods in Machine Learning.pdf;/Users/felix/Zotero/storage/59ZDTSIM/1705.html}
}

@inproceedings{wuBayesianOptimizationGradients2017,
  title = {Bayesian {{Optimization}} with {{Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wu, Jian and Poloczek, Matthias and Wilson, Andrew G and Frazier, Peter},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html},
  urldate = {2022-06-02},
  abstract = {Bayesian optimization has shown success in global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. dKG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the dKG acquisition function and its gradient using a novel fast discretization-free technique. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.},
  file = {/Users/felix/paper/2017_Wu et al/[Supplemental] Wu et al_2017_Bayesian Optimization with Gradients.pdf;/Users/felix/paper/2017_Wu et al/Wu et al_2017_Bayesian Optimization with Gradients.pdf}
}

@online{xiaoFashionMNISTNovelImage2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  date = {2017-09-15},
  eprint = {1708.07747},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1708.07747},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/felix/paper/2017_Xiao et al/Xiao et al_2017_Fashion-MNIST.pdf;/Users/felix/Zotero/storage/TC8E9NTG/1708.html}
}

@inproceedings{yangMeanFieldTheory2018,
  title = {A {{Mean Field Theory}} of {{Batch Normalization}}},
  author = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
  date = {2018-09-27},
  eprint = {1902.08129},
  eprinttype = {arXiv},
  eprintclass = {cs.NE},
  url = {https://openreview.net/forum?id=SyMDXnCcF7},
  urldate = {2023-10-16},
  abstract = {We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/felix/paper/2018_Yang et al/[ArXiv] Yang et al_2018_A Mean Field Theory of Batch Normalization.pdf;/Users/felix/paper/2018_Yang et al/Yang et al_2018_A Mean Field Theory of Batch Normalization.pdf}
}

@unpublished{zeilerADADELTAAdaptiveLearning2012,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  author = {Zeiler, Matthew D.},
  date = {2012-12-22},
  eprint = {1212.5701},
  eprinttype = {arXiv},
  eprintclass = {cs},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/felix/paper/2012_Zeiler/Zeiler_2012_ADADELTA.pdf;/Users/felix/Zotero/storage/KEXJ9289/1212.html}
}

@inproceedings{zhangWhichAlgorithmicChoices2019,
  title = {Which {{Algorithmic Choices Matter}} at {{Which Batch Sizes}}? {{Insights From}} a {{Noisy Quadratic Model}}},
  shorttitle = {Which {{Algorithmic Choices Matter}} at {{Which Batch Sizes}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
  date = {2019},
  volume = {32},
  eprint = {1907.04164},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2019/hash/e0eacd983971634327ae1819ea8b6214-Abstract.html},
  urldate = {2023-11-09},
  abstract = {Increasing the batch size is a popular way to speed up neural network training, but beyond some critical batch size, larger batch sizes yield diminishing returns. In this work, we study how the critical batch size changes based on properties of the optimization algorithm, including acceleration and preconditioning, through two different lenses: large scale experiments and analysis using a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning, specifically Adam and K-FAC, result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training, despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers, previous results with accelerated gradient descent, and other results around optimal learning rates and large batch training, making it a useful tool to generate testable predictions about neural network optimization. We demonstrate empirically that the simple noisy quadratic model (NQM) displays many similarities to neural networks in terms of large-batch training. We prove analytical convergence results for the NQM model that predict such behavior and hence provide possible explanations and a better understanding for many large-batch training phenomena.},
  file = {/Users/felix/paper/2019_Zhang et al/[ArXiv] Zhang et al_2019_Which Algorithmic Choices Matter at Which Batch Sizes.pdf;/Users/felix/paper/2019_Zhang et al/Zhang et al_2019_Which Algorithmic Choices Matter at Which Batch Sizes.pdf}
}

@inproceedings{zhangWhyGradientClipping2019,
  title = {Why {{Gradient Clipping Accelerates Training}}: {{A Theoretical Justification}} for {{Adaptivity}}},
  shorttitle = {Why {{Gradient Clipping Accelerates Training}}},
  author = {Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=BJgnXpVYwS&=KA65jj-wX},
  urldate = {2024-04-02},
  abstract = {We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily faster than gradient descent with fixed stepsize. We further explain why such adaptively scaled gradient methods can accelerate empirical convergence and verify our results empirically in popular neural network training settings.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/felix/paper/2019_Zhang et al/Zhang et al_2019_Why Gradient Clipping Accelerates Training2.pdf}
}
