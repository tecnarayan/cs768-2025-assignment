\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alain and Bengio(2014)]{alain2014regularized}
Guillaume Alain and Yoshua Bengio.
\newblock What regularized auto-encoders learn from the data-generating
  distribution.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 3563--3593, 2014.

\bibitem[Boursier and Flammarion(2023)]{boursier2023penalising}
Etienne Boursier and Nicolas Flammarion.
\newblock Penalising the biases in norm regularisation enforces sparsity.
\newblock \emph{arXiv preprint arXiv:2303.01353}, 2023.

\bibitem[Chen et~al.(2014)Chen, Weinberger, Sha, and Bengio]{pmlr-v32-cheng14}
Minmin Chen, Kilian Weinberger, Fei Sha, and Yoshua Bengio.
\newblock Marginalized denoising auto-encoders for nonlinear representations.
\newblock In Eric~P. Xing and Tony Jebara, editors, \emph{Proceedings of the
  31st International Conference on Machine Learning}, volume~32 of
  \emph{Proceedings of Machine Learning Research}, pages 1476--1484, Bejing,
  China, 22--24 Jun 2014. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v32/cheng14.html}.

\bibitem[Elad et~al.(2023)Elad, Kawar, and Vaksman]{doi:10.1137/23M1545859}
Michael Elad, Bahjat Kawar, and Gregory Vaksman.
\newblock Image denoising: The deep learning revolution and beyond—a survey
  paper.
\newblock \emph{SIAM Journal on Imaging Sciences}, 16\penalty0 (3):\penalty0
  1594--1654, 2023.
\newblock \doi{10.1137/23M1545859}.
\newblock URL \url{https://doi.org/10.1137/23M1545859}.

\bibitem[Ergen and Pilanci(2021)]{ergen2021convex}
Tolga Ergen and Mert Pilanci.
\newblock Convex geometry and duality of over-parameterized neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 9646--9708, 2021.

\bibitem[Hanin(2021)]{hanin2021ridgeless}
Boris Hanin.
\newblock Ridgeless interpolation with shallow relu networks in $1 d $ is
  nearest neighbor curvature extrapolation and provably generalizes on
  lipschitz functions.
\newblock \emph{arXiv preprint arXiv:2109.12960}, 2021.

\bibitem[Hasinoff et~al.(2010)Hasinoff, Durand, and Freeman]{hasinoff2010noise}
Samuel~W Hasinoff, Fr{\'e}do Durand, and William~T Freeman.
\newblock Noise-optimal capture for high dynamic range photography.
\newblock In \emph{2010 IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition}, pages 553--560. IEEE, 2010.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Horrace(2015)]{horrace2015moments}
William~C Horrace.
\newblock Moments of the truncated normal distribution.
\newblock \emph{Journal of Productivity Analysis}, 43:\penalty0 133--138, 2015.

\bibitem[Jacot(2022)]{jacot2022implicit}
Arthur Jacot.
\newblock Implicit bias of large depth networks: a notion of rank for nonlinear
  functions.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Jacot et~al.(2022)Jacot, Golikov, Hongler, and Gabriel]{jacotNeurips}
Arthur Jacot, Eugene Golikov, Clement Hongler, and Franck Gabriel.
\newblock Feature learning in l\_2-regularized dnns: Attraction/repulsion and
  sparsity.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~35, pages 6763--6774, 2022.

\bibitem[Manjunath and Wilhelm(2021)]{Manjunath_Wilhelm_2021}
B.~G. Manjunath and Stefan Wilhelm.
\newblock Moments calculation for the doubly truncated multivariate normal
  density.
\newblock \emph{Journal of Behavioral Data Science}, 1\penalty0 (1):\penalty0
  17–33, May 2021.
\newblock \doi{10.35566/jbds/v1n1/p2}.
\newblock URL \url{https://jbds.isdsa.org/index.php/jbds/article/view/9}.

\bibitem[Mulayoff et~al.(2021)Mulayoff, Michaeli, and
  Soudry]{mulayoff2021implicit}
Rotem Mulayoff, Tomer Michaeli, and Daniel Soudry.
\newblock The implicit bias of minima stability: A view from function space.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  34:\penalty0 17749--17761, 2021.

\bibitem[Nacson et~al.(2023)Nacson, Mulayoff, Ongie, Michaeli, and
  Soudry]{nacsonimplicit}
Mor~Shpigel Nacson, Rotem Mulayoff, Greg Ongie, Tomer Michaeli, and Daniel
  Soudry.
\newblock The implicit bias of minima stability in multivariate shallow relu
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 1376--1401.
  PMLR, 2015.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  30, 2017.

\bibitem[Ongie et~al.(2020)Ongie, Willett, Soudry, and Srebro]{Ongie2020A}
Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro.
\newblock A function space view of bounded norm infinite width relu nets: The
  multivariate case.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1lNPxHKDH}.

\bibitem[Parhi and Nowak(2021)]{parhi2021banach}
Rahul Parhi and Robert~D Nowak.
\newblock Banach space representer theorems for neural networks and ridge
  splines.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 1960--1999, 2021.

\bibitem[Parhi and Nowak(2022)]{parhi2022kinds}
Rahul Parhi and Robert~D Nowak.
\newblock What kinds of functions do deep neural networks learn? insights from
  variational spline theory.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 4\penalty0
  (2):\penalty0 464--489, 2022.

\bibitem[Radhakrishnan et~al.(2018)Radhakrishnan, Yang, Belkin, and
  Uhler]{radhakrishnan2018memorization}
Adityanarayanan Radhakrishnan, Karren Yang, Mikhail Belkin, and Caroline Uhler.
\newblock Memorization in overparameterized autoencoders.
\newblock \emph{arXiv preprint arXiv:1810.10333}, 2018.

\bibitem[Raya and Ambrogioni(2023)]{raya2023spontaneous}
Gabriel Raya and Luca Ambrogioni.
\newblock Spontaneous symmetry breaking in generative diffusion models.
\newblock \emph{arXiv preprint arXiv:2305.19693}, 2023.

\bibitem[Roth and Black(2009)]{cite-key}
Stefan Roth and Michael~J. Black.
\newblock Fields of experts.
\newblock \emph{International Journal of Computer Vision}, 82\penalty0
  (2):\penalty0 205--229, 2009.
\newblock \doi{10.1007/s11263-008-0197-6}.
\newblock URL \url{https://doi.org/10.1007/s11263-008-0197-6}.

\bibitem[Sahiner et~al.(2021)Sahiner, Mardani, Ozturkler, Pilanci, and
  Pauly]{sahiner2021convex}
Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John~M. Pauly.
\newblock Convex regularization behind neural reconstruction.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=VErQxgyrbfn}.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and
  Srebro]{savarese2019infinite}
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro.
\newblock How do infinite width bounded norm networks look in function space?
\newblock In \emph{Conference on Learning Theory}, pages 2667--2690. PMLR,
  2019.

\bibitem[Shenouda et~al.(2023)Shenouda, Parhi, Lee, and
  Nowak]{shenouda2023vector}
Joseph Shenouda, Rahul Parhi, Kangwook Lee, and Robert~D Nowak.
\newblock Vector-valued variation spaces and width bounds for {DNN}s: Insights
  on weight decay regularization.
\newblock \emph{arXiv preprint arXiv:2305.16534}, 2023.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Sonthalia and Nadakuditi(2023)]{sonthalia2023training}
Rishi Sonthalia and Raj~Rao Nadakuditi.
\newblock Training data size induced double descent for denoising feedforward
  neural networks and the role of training noise.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=FdMWtpVT1I}.

\bibitem[Udell and Townsend(2019)]{udell2019big}
Madeleine Udell and Alex Townsend.
\newblock Why are big data matrices approximately low rank?
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 1\penalty0
  (1):\penalty0 144--160, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Zuo, Chen, Meng, and Zhang]{zhang2017beyond}
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.
\newblock Beyond a gaussian denoiser: Residual learning of deep cnn for image
  denoising.
\newblock \emph{IEEE transactions on image processing}, 26\penalty0
  (7):\penalty0 3142--3155, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Li, Zuo, Zhang, Van~Gool, and
  Timofte]{zhang2021plug}
Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van~Gool, and Radu Timofte.
\newblock Plug-and-play image restoration with deep denoiser prior.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 44\penalty0 (10):\penalty0 6360--6376, 2021.

\end{thebibliography}
