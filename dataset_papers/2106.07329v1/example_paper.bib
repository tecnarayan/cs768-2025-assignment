@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={NIPS Deep Learning Workshop},
  year={2013}
}



@article{peters2008reinforcement,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neural networks},
  volume={21},
  number={4},
  pages={682--697},
  year={2008},
  publisher={Elsevier}
}

@article{schulman2016high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={22--31},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={International Conference on Machine Learning},
  volume={2},
  pages={267--274},
  year={2002}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}

@inproceedings{vuong2019supervised,
  title={Supervised Policy Update for Deep Reinforcement Learning},
  author={Vuong, Quan and Zhang, Yiming and Ross, Keith W},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2019}
}



@article{miryoosefi2019reinforcement,
  title={Reinforcement Learning with Convex Constraints},
  author={Miryoosefi, Sobhan and Brantley, Kiant{\'e} and Daum{\'e} III, Hal and Dudik, Miroslav and Schapire, Robert},
  journal={arXiv preprint arXiv:1906.09323},
  year={2019}
}

@article{borkar2005actor,
  title={An actor-critic algorithm for constrained Markov decision processes},
  author={Borkar, Vivek S},
  journal={Systems \& control letters},
  volume={54},
  number={3},
  pages={207--213},
  year={2005},
  publisher={Elsevier}
}

@article{bhatnagar2012online,
  title={An online actor--critic algorithm with function approximation for constrained markov decision processes},
  author={Bhatnagar, Shalabh and Lakshmanan, K},
  journal={Journal of Optimization Theory and Applications},
  volume={153},
  number={3},
  pages={688--708},
  year={2012},
  publisher={Springer}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@book{levin2017markov,
  title={Markov chains and mixing times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}

@book{kemeny1960finite,
  title={Finite {M}arkov {C}hains},
  author={Kemeny, J.G. and Snell, I.J.},
  year={1960},
  publisher={Van Nostrand, New Jersey}
}

@book{puterman1994markov,
  title={Markov {D}ecision {P}rocesses: {D}iscrete {S}tochastic {D}ynamic {P}rogramming},
  author={Puterman, Martin L},
  year={1994},
  publisher={John Wiley \& Sons}
}


@article{zhang2020first,
  title={First Order Constrained Optimization in Policy Space},
  author={Zhang, Yiming and Vuong, Quan and Ross, Keith},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{abdolmaleki2018maximum,
  title={Maximum a posteriori policy optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  journal={International Conference on Learning Representation (ICLR)},
  year={2018}
}

@inproceedings{yang2020projection,
  title={Projection-Based Constrained Policy Optimization},
  author={Yang, Tsung-Yen and Rosca, Justinian and Narasimhan, Karthik and Ramadge, Peter J},
  booktitle={International Conference on Learning Representation (ICLR)},
  year={2020}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{pirotta2013safe,
  title={Safe policy iteration},
  author={Pirotta, Matteo and Restelli, Marcello and Pecorino, Alessio and Calandriello, Daniele},
  booktitle={International Conference on Machine Learning},
  pages={307--315},
  year={2013}
}

@inproceedings{wu2017scalable,
  title={Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation},
  author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  booktitle={Advances in neural information processing systems (NIPS)},
  pages={5285--5294},
  year={2017}
}

@article{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={International Conference on Machine Learning (ICML)},
  year={2018}
}

@misc{brockman2016openai,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


@article{naik2019discounted,
  title={Discounted reinforcement learning is not an optimization problem},
  author={Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Sutton, Richard S},
  journal={NeurIPS Optimization Foundations for Reinforcement Learning Workshop},
  year={2019}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@misc{achiam2017advanced,
  author = "Joshua Achiam",
  title  = "{{UC} {B}erkeley {CS} 285 ({F}all 2017), Advanced Policy Gradients}",
  note   = "URL: \url{http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf}. 
            Last visited on 2020/05/24",
  year   = 2017,
}


@inproceedings{abbasi2019politex,
  title={POLITEX: Regret bounds for policy iteration using expert prediction},
  author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gell{\'e}rt},
  booktitle={International Conference on Machine Learning},
  pages={3692--3702},
  year={2019}
}

@article{wei2019model,
  title={Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes},
  author={Wei, Chen-Yu and Jafarnia-Jahromi, Mehdi and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
  journal={arXiv preprint arXiv:1910.07072},
  year={2019}
}

@article{mahadevan1996average,
  title={Average reward reinforcement learning: Foundations, algorithms, and empirical results},
  author={Mahadevan, Sridhar},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={159--195},
  year={1996},
  publisher={Springer}
}

@article{blackwell1962discrete,
  title={Discrete dynamic programming},
  author={Blackwell, David},
  journal={The Annals of Mathematical Statistics},
  pages={719--726},
  year={1962},
  publisher={JSTOR}
}

@book{tsybakov2008introduction,
  title={Introduction to nonparametric estimation},
  author={Tsybakov, Alexandre B},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@inproceedings{schwartz1993reinforcement,
  title={A reinforcement learning method for maximizing undiscounted rewards},
  author={Schwartz, Anton},
  booktitle={Proceedings of the tenth international conference on machine learning},
  volume={298},
  pages={298--305},
  year={1993}
}


@book{bertsekas1995dynamic,
  title={Dynamic programming and optimal control},
  author={Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P},
  volume={1,2},
  year={1995},
  publisher={Athena scientific Belmont, MA}
}

@book{kallenberg1983linear,
  title={Linear Programming and Finite Markovian Control Problems},
  author={Kallenberg, L.C.M.},
  year={1983},
  publisher={Centrum Voor Wiskunde en Informatica}
}

@article{abounadi2001learning,
  title={Learning algorithms for Markov decision processes with average cost},
  author={Abounadi, Jinane and Bertsekas, D and Borkar, Vivek S},
  journal={SIAM Journal on Control and Optimization},
  volume={40},
  number={3},
  pages={681--698},
  year={2001},
  publisher={SIAM}
}

@book{howard1960dynamic,
  title={Dynamic programming and markov processes.},
  author={Howard, Ronald A},
  year={1960},
  publisher={John Wiley}
}

@article{veinott1966finding,
  title={On finding optimal policies in discrete dynamic programming with no discounting},
  author={Veinott, Arthur F},
  journal={The Annals of Mathematical Statistics},
  volume={37},
  number={5},
  pages={1284--1294},
  year={1966},
  publisher={JSTOR}
}

@book{lehmann2006theory,
  title={Theory of point estimation},
  author={Lehmann, Erich L and Casella, George},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{strang2007computational,
  title={Computational science and engineering},
  author={Strang, Gilbert},
  publisher={Wellesley-Cambridge Press},
  year=2007
}

@article{ross1985constrained,
  title={Constrained Markov decision processes with queueing applications.},
  author={Ross, Keith W},
  journal={Dissertation Abstracts International Part B: Science and Engineering[DISS. ABST. INT. PT. B- SCI. \& ENG.],},
  volume={46},
  number={4},
  year={1985}
}


@inproceedings{zhao2011analysis,
  title={Analysis and improvement of policy gradient estimation},
  author={Zhao, Tingting and Hachiya, Hirotaka and Niu, Gang and Sugiyama, Masashi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={262--270},
  year={2011}
}

@article{tessler2018reward,
  title={Reward constrained policy optimization},
  author={Tessler, Chen and Mankowitz, Daniel J and Mannor, Shie},
  journal={International Conference on Learning Representation (ICLR)},
  year={2019}
}

@article{zhang2020average,
  title={Average Reward Reinforcement Learning with Monotonic Policy Improvement},
  author={Zhang, Yiming and Ross, Keith},
  journal={Preprint},
  year={2020}
}

@article{wan2020learning,
  title={Learning and Planning in Average-Reward Markov Decision Processes},
  author={Wan, Yi and Naik, Abhishek and Sutton, Richard S},
  journal={arXiv preprint arXiv:2006.16318},
  year={2020}
}

@article{agarwal2019theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={arXiv preprint arXiv:1908.00261},
  year={2019}
}

@misc{aldous1995reversible,
  title={Reversible Markov chains and random walks on graphs},
  author={Aldous, David and Fill, James},
  year={1995},
  publisher={Berkeley}
}

@book{grinstead2012introduction,
  title={Introduction to probability},
  author={Grinstead, Charles Miller and Snell, James Laurie},
  year={2012},
  publisher={American Mathematical Soc.}
}

@article{even2009online,
  title={Online Markov decision processes},
  author={Even-Dar, Eyal and Kakade, Sham M and Mansour, Yishay},
  journal={Mathematics of Operations Research},
  volume={34},
  number={3},
  pages={726--736},
  year={2009},
  publisher={INFORMS}
}

@inproceedings{neu2010online,
  title={Online Markov decision processes under bandit feedback},
  author={Neu, Gergely and Antos, Andras and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1804--1812},
  year={2010}
}

@article{cho2001comparison,
  title={Comparison of perturbation bounds for the stationary distribution of a Markov chain},
  author={Cho, Grace E and Meyer, Carl D},
  journal={Linear Algebra and its Applications},
  volume={335},
  number={1-3},
  pages={137--150},
  year={2001},
  publisher={North-Holland}
}

@article{hunter2005stationary,
  title={Stationary distributions and mean first passage times of perturbed Markov chains},
  author={Hunter, Jeffrey J},
  journal={Linear Algebra and its Applications},
  volume={410},
  pages={217--243},
  year={2005},
  publisher={Elsevier}
}

@article{meyer1975role,
  title={The role of the group generalized inverse in the theory of finite Markov chains},
  author={Meyer, Jr, Carl D},
  journal={Siam Review},
  volume={17},
  number={3},
  pages={443--464},
  year={1975},
  publisher={SIAM}}
  
@inproceedings{wei2020model,
  title={Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes},
  author={Wei, Chen-Yu and Jafarnia-Jahromi, Mehdi and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
  booktitle={International conference on machine learning},
  year={2020}
}

@inproceedings{yang2016efficient,
  title={Efficient Average Reward Reinforcement Learning Using Constant Shifting Values.},
  author={Yang, Shangdong and Gao, Yang and An, Bo and Wang, Hao and Chen, Xingguo},
  booktitle={AAAI},
  pages={2258--2264},
  year={2016}
}

@article{baxter2001infinite,
  title={Infinite-horizon policy-gradient estimation},
  author={Baxter, Jonathan and Bartlett, Peter L},
  journal={Journal of Artificial Intelligence Research},
  volume={15},
  pages={319--350},
  year={2001}
}

@inproceedings{kakade2001optimizing,
  title={Optimizing average reward using discounted rewards},
  author={Kakade, Sham},
  booktitle={International Conference on Computational Learning Theory},
  pages={605--615},
  year={2001},
  organization={Springer}
}

@inproceedings{amit2020discount,
  title={Discount Factor as a Regularizer in Reinforcement Learning},
  author={Amit, Ron and Meir, Ron and Ciosek, Kamil},
  booktitle={International conference on machine learning},
  year={2020}
}

@article{andrychowicz2020matters,
  title={What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study},
  author={Andrychowicz, Marcin and Raichuk, Anton and Sta{\'n}czyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphael and Hussenot, L{\'e}onard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and others},
  journal={arXiv preprint arXiv:2006.05990},
  year={2020}
}

@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{song2020v,
  title={V-MPO: on-policy maximum a posteriori policy optimization for discrete and continuous control},
  author={Song, H Francis and Abdolmaleki, Abbas and Springenberg, Jost Tobias and Clark, Aidan and Soyer, Hubert and Rae, Jack W and Noury, Seb and Ahuja, Arun and Liu, Siqi and Tirumala, Dhruva and others},
  journal={International Conference on Learning Representations},
  year={2020}
}

@book{bremaud2020markov,
  title={Markov Chains Gibbs Fields, Monte Carlo Simulation and Queues},
  author={Br{\'e}maud, Pierre},
  year=2020,
  edition=2,
  publisher={Springer}
}

@techreport{tadepalli1994h,
  title={H-learning: A reinforcement learning method to optimize undiscounted average reward},
  author={Tadepalli, Prasad and Ok, DoKyeong},
  number={94-30-01},
  institution = {Oregon State University},
  year={1994}
}

@article{ross1986optimal,
  title={Optimal dynamic routing in Markov queueing networks},
  author={Ross, Keith W},
  journal={Automatica},
  volume={22},
  number={3},
  pages={367--370},
  year={1986},
  publisher={Elsevier}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@article{lillicrap2016continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{petrik2008biasing,
  title={Biasing approximate dynamic programming with a lower discount factor},
  author={Petrik, Marek and Scherrer, Bruno},
  booktitle={Twenty-Second Annual Conference on Neural Information Processing Systems-NIPS 2008},
  year={2008}
}

@inproceedings{jiang2015dependence,
  title={The dependence of effective planning horizon on model accuracy},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
  booktitle={Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
  pages={1181--1189},
  year={2015},
  organization={Citeseer}
}

@inproceedings{jiang2016structural,
  title={On Structural Properties of MDPs that Bound Loss Due to Shallow Planning.},
  author={Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
  booktitle={IJCAI},
  pages={1640--1647},
  year={2016}
}

@inproceedings{lehnert2018value,
  title={On value function representation of long horizon problems},
  author={Lehnert, Lucas and Laroche, Romain and van Seijen, Harm},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}


