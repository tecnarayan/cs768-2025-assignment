\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2022)Agarwal, Schwarzer, Castro, Courville, and Bellemare]{agarwal2022reincarnating}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and Marc Bellemare.
\newblock Reincarnating reinforcement learning: Reusing prior computation to accelerate progress.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 28955--28971, 2022.

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{bacon2017option}
Pierre-Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~31, 2017.

\bibitem[Bain and Sammut(1995)]{Bain1995AFF}
Michael Bain and Claude Sammut.
\newblock A framework for behavioural cloning.
\newblock In \emph{Machine Intelligence 15}, 1995.

\bibitem[Bandura(1977)]{bandura1977social}
A.~Bandura.
\newblock \emph{Social Learning Theory}.
\newblock Prentice-Hall series in social learning theory. Prentice Hall, 1977.
\newblock ISBN 9780138167516.

\bibitem[Chevalier-Boisvert et~al.(2018)Chevalier-Boisvert, Willems, and Pal]{gym_minigrid}
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal.
\newblock Minimalistic gridworld environment for openai gym.
\newblock \url{https://github.com/maximecb/gym-minigrid}, 2018.

\bibitem[Dayan and Hinton(1992)]{dayan1992feudal}
Peter Dayan and Geoffrey~E Hinton.
\newblock Feudal reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 5, 1992.

\bibitem[Degrave et~al.(2022)Degrave, Felici, Buchli, Neunert, Tracey, Carpanese, Ewalds, Hafner, Abdolmaleki, de~Las~Casas, et~al.]{degrave2022magnetic}
Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, Timo Ewalds, Roland Hafner, Abbas Abdolmaleki, Diego de~Las~Casas, et~al.
\newblock Magnetic control of tokamak plasmas through deep reinforcement learning.
\newblock \emph{Nature}, 602\penalty0 (7897):\penalty0 414--419, 2022.

\bibitem[Goecks et~al.(2019)Goecks, Gremillion, Lawhern, Valasek, and Waytowich]{goecks2019integrating}
Vinicius~G Goecks, Gregory~M Gremillion, Vernon~J Lawhern, John Valasek, and Nicholas~R Waytowich.
\newblock Integrating behavior cloning and reinforcement learning for improved performance in dense and sparse reward environments.
\newblock \emph{arXiv preprint arXiv:1910.04281}, 2019.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International conference on machine learning}, pages 1352--1361. PMLR, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{ICML}, 2018.

\bibitem[Hester et~al.(2017)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot, Sendonaris, Dulac-Arnold, Osband, Agapiou, et~al.]{hester2017learning}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, et~al.
\newblock Learning from demonstrations for real world reinforcement learning.
\newblock 2017.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Jiang et~al.(2019)Jiang, Gu, Murphy, and Finn]{jiang2019language}
Yiding Jiang, Shixiang~Shane Gu, Kevin~P Murphy, and Chelsea Finn.
\newblock Language as an abstraction for hierarchical deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kaelbling(2020)]{kaelbling2020foundation}
Leslie~Pack Kaelbling.
\newblock The foundation of efficient robot learning.
\newblock \emph{Science}, 369\penalty0 (6506):\penalty0 915--916, 2020.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog, Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018scalable}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic manipulation.
\newblock In \emph{Conference on Robot Learning}, pages 651--673. PMLR, 2018.

\bibitem[Khetarpal and Precup(2019)]{khetarpal2019learning}
Khimya Khetarpal and Doina Precup.
\newblock Learning options with interest functions.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 9955--9956, 2019.

\bibitem[Kim et~al.(2021)Kim, Park, and Kim]{kim2021unsupervised}
Jaekyeom Kim, Seohong Park, and Gunhee Kim.
\newblock Unsupervised skill discovery with bottleneck option learning.
\newblock \emph{arXiv preprint arXiv:2106.14305}, 2021.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and Tenenbaum]{kulkarni2016hierarchical}
Tejas~D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
\newblock Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[MacKay(2003)]{mackay2003information}
David~JC MacKay.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{nachum2018near}
Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine.
\newblock Near-optimal representation learning for hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1810.01257}, 2018.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and Abbeel]{nair2018overcoming}
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE international conference on robotics and automation (ICRA)}, pages 6292--6299. IEEE, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pfeiffer et~al.(2018)Pfeiffer, Shukla, Turchetta, Cadena, Krause, Siegwart, and Nieto]{pfeiffer2018reinforced}
Mark Pfeiffer, Samarth Shukla, Matteo Turchetta, Cesar Cadena, Andreas Krause, Roland Siegwart, and Juan Nieto.
\newblock Reinforced imitation: Sample efficient deep reinforcement learning for mapless navigation by leveraging prior demonstrations.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (4):\penalty0 4423--4430, 2018.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, McGrew, Baker, Powell, Schneider, Tobin, Chociej, Welinder, et~al.]{plappert2018multi}
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et~al.
\newblock Multi-goal reinforcement learning: Challenging robotics environments and request for research.
\newblock \emph{arXiv preprint arXiv:1802.09464}, 2018.

\bibitem[Qureshi et~al.(2020)Qureshi, Johnson, Qin, Henderson, Boots, and Yip]{Qureshi2020Composing}
Ahmed~H. Qureshi, Jacob~J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, and Michael~C. Yip.
\newblock Composing task-agnostic policies with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1ezFREtwH}.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and Dormann]{stable-baselines3}
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (268):\penalty0 1--8, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-1364.html}.

\bibitem[Rajendran et~al.(2017)Rajendran, Lakshminarayanan, Khapra, P, and Ravindran]{rajendran2017attend}
Janarthanan Rajendran, Aravind Lakshminarayanan, Mitesh~M. Khapra, Prasanna P, and Balaraman Ravindran.
\newblock Attend, adapt and transfer: Attentive deep architecture for adaptive transfer from multiple sources in the same domain.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Sy6iJDqlx}.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman, Todorov, and Levine]{rajeswaran2017learning}
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine.
\newblock Learning complex dexterous manipulation with deep reinforcement learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Song et~al.(2021)Song, Kidzi{\'n}ski, Peng, Ong, Hicks, Levine, Atkeson, and Delp]{song2021deep}
Seungmoon Song, {\L}ukasz Kidzi{\'n}ski, Xue~Bin Peng, Carmichael Ong, Jennifer Hicks, Sergey Levine, Christopher~G Atkeson, and Scott~L Delp.
\newblock Deep reinforcement learning for modeling human locomotion control in neuromechanical simulation.
\newblock \emph{Journal of neuroengineering and rehabilitation}, 18\penalty0 (1):\penalty0 1--17, 2021.

\bibitem[Stolle and Precup(2002)]{stolle2002learning}
Martin Stolle and Doina Precup.
\newblock Learning options in reinforcement learning.
\newblock In \emph{International Symposium on abstraction, reformulation, and approximation}, pages 212--223. Springer, 2002.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tseng et~al.(2021)Tseng, Lin, Feng, and Sun]{tseng2021toward}
Wei-Cheng Tseng, Jin-Siang Lin, Yao-Min Feng, and Min Sun.
\newblock Toward robust long range policy transfer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 9958--9966, 2021.

\bibitem[Vecerik et~al.(2017)Vecerik, Hester, Scholz, Wang, Pietquin, Piot, Heess, Roth{\"o}rl, Lampe, and Riedmiller]{vecerik2017leveraging}
Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth{\"o}rl, Thomas Lampe, and Martin Riedmiller.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Wurman et~al.(2022)Wurman, Barrett, Kawamoto, MacGlashan, Subramanian, Walsh, Capobianco, Devlic, Eckert, Fuchs, et~al.]{wurman2022outracing}
Peter~R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas~J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et~al.
\newblock Outracing champion gran turismo drivers with deep reinforcement learning.
\newblock \emph{Nature}, 602\penalty0 (7896):\penalty0 223--228, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Hao, Wang, Tang, Ma, Duan, and Zheng]{Zhang2020KoGuNAD}
Peng Zhang, Jianye Hao, Weixun Wang, Hongyao Tang, Yi~Ma, Yihai Duan, and Yan Zheng.
\newblock Kogun: Accelerating deep reinforcement learning via integrating human suboptimal knowledge.
\newblock In \emph{International Joint Conference on Artificial Intelligence}, 2020.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Brian~D Ziebart.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\end{thebibliography}
