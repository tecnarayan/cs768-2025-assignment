\begin{thebibliography}{10}

\bibitem{Brown2020GPT3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em NeurIPS}, pages 1--25, 2020.

\bibitem{Ouyang2022InstructGPT}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~F. Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In {\em NeurIPS}, volume~35, pages 27730--27744, 2022.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{OpenAI2023GPT4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock {\em CoRR}, abs/2303.08774, 2023.

\bibitem{Hugo2023LLaMa}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie{-}Anne Lachaux, Timoth{\'{e}}e Lacroix, Baptiste Rozi{\`{e}}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur{\'{e}}lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em CoRR}, abs/2302.13971, 2023.

\bibitem{Rohan2023PaLM2}
Rohan Anil, Andrew~M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan~H. Clark, Laurent~El Shafey, Yanping Huang, Kathy Meier{-}Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi~Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo~Hern{\'{a}}ndez {\'{A}}brego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan~A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher~A. Choquette{-}Choo, Aakanksha Chowdhery, Cl{\'{e}}ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D{\'{\i}}az, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et~al.
\newblock Palm 2 technical report.
\newblock {\em CoRR}, abs/2305.10403, 2023.

\bibitem{gravitas2023auto}
Significant Gravitas.
\newblock Auto-gpt: An autonomous gpt-4 experiment.
\newblock \url{https://github.com/Significant-Gravitas/Auto-GPT}, 2023.

\bibitem{Yongliang2023HuggingGPT}
Yongliang Shen, Kaitao Song, Xu~Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
\newblock Hugginggpt: Solving {AI} tasks with chatgpt and its friends in huggingface.
\newblock {\em CoRR}, abs/2303.17580, 2023.

\bibitem{Yohei2023BabyAGI}
Yohei Nakajima.
\newblock Babyagi.
\newblock \url{https://github.com/yoheinakajima/babyagi}, 2023.

\bibitem{Yaobo2023TaskMatrix}
Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu~Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan.
\newblock Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis.
\newblock {\em CoRR}, abs/2303.16434, 2023.

\bibitem{hongMetaGPTMetaProgramming2023}
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka~Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu.
\newblock {{MetaGPT}}: {{Meta Programming}} for {{Multi-Agent Collaborative Framework}}, August 2023.

\bibitem{liCAMELCommunicativeAgents2023}
Guohao Li, Hasan Abed Al~Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
\newblock {{CAMEL}}: {{Communicative Agents}} for "{{Mind}}" {{Exploration}} of {{Large Scale Language Model Society}}, March 2023.

\bibitem{parkGenerativeAgentsInteractive2023}
Joon~Sung Park, Joseph~C. O'Brien, Carrie~J. Cai, Meredith~Ringel Morris, Percy Liang, and Michael~S. Bernstein.
\newblock Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}, April 2023.

\bibitem{wangVoyagerOpenEndedEmbodied2023}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
\newblock Voyager: {{An Open-Ended Embodied Agent}} with {{Large Language Models}}, May 2023.

\bibitem{Dan2021MMLU}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In {\em ICLR}, 2021.

\bibitem{Karl2021GSM8K}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock {\em CoRR}, abs/2110.14168, 2021.

\bibitem{Wanjun2023AGIEval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: {A} human-centric benchmark for evaluating foundation models.
\newblock {\em CoRR}, abs/2304.06364, 2023.

\bibitem{patil2023gorilla}
Shishir~G Patil, Tianjun Zhang, Xin Wang, and Joseph~E Gonzalez.
\newblock Gorilla: Large language model connected with massive apis.
\newblock {\em arXiv preprint arXiv:2305.15334}, 2023.

\bibitem{Qin2023ToolLLM}
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock {{ToolLLM}}: {{Facilitating Large Language Models}} to {{Master}} 16000+ {{Real-world APIs}}, July 2023.

\bibitem{huang2023metatool}
Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil~Zhenqiang Gong, and Lichao Sun.
\newblock Metatool benchmark: Deciding whether to use tools and which to use.
\newblock {\em arXiv preprint arXiv: 2310.03128}, 2023.

\bibitem{zhuangToolQADatasetLLM2023}
Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang.
\newblock {{ToolQA}}: {{A Dataset}} for {{LLM Question Answering}} with {{External Tools}}, June 2023.

\bibitem{Qiaoyu2023ToolAlpaca}
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le~Sun.
\newblock Toolalpaca: Generalized tool learning for language models with 3000 simulated cases.
\newblock {\em CoRR}, abs/2306.05301, 2023.

\bibitem{liAPIBankBenchmarkToolAugmented2023}
Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li.
\newblock {{API-Bank}}: {{A Benchmark}} for {{Tool-Augmented LLMs}}, April 2023.

\bibitem{Hugo2023LLaMa2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em CoRR}, abs/2307.09288, 2023.

\bibitem{Xiao2023AgentBench}
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu~Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu~Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.
\newblock Agentbench: Evaluating llms as agents.
\newblock {\em CoRR}, abs/2308.03688, 2023.

\bibitem{yangGPT4ToolsTeachingLarge2023}
Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan.
\newblock {{GPT4Tools}}: {{Teaching Large Language Model}} to {{Use Tools}} via {{Self-instruction}}, May 2023.

\bibitem{Timo2023Toolformer}
Timo Schick, Jane Dwivedi{-}Yu, Roberto Dess{\`{\i}}, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock {\em CoRR}, abs/2302.04761, 2023.

\bibitem{Thomas2019HuggingFace}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'{e}}mi Louf, Morgan Funtowicz, and Jamie Brew.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock {\em CoRR}, abs/1910.03771, 2019.

\bibitem{wang-etal-2023-self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 13484--13508, Toronto, Canada, July 2023. Association for Computational Linguistics.

\bibitem{IntroducingNextGeneration}
Introducing the next generation of {{Claude}}.
\newblock https://www.anthropic.com/news/claude-3-family.

\bibitem{IntroducingMetaLlama}
Introducing {{Meta Llama}} 3: {{The}} most capable openly available {{LLM}} to date.
\newblock https://ai.meta.com/blog/meta-llama-3/.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem{roziere2023code}
Baptiste Rozi{\`e}re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al.
\newblock Code llama: Open foundation models for code.
\newblock {\em arXiv preprint arXiv:2308.12950}, 2023.

\bibitem{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex instructions.
\newblock {\em arXiv preprint arXiv:2304.12244}, 2023.

\bibitem{yang2023baichuan}
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, Fan Yang, et~al.
\newblock Baichuan 2: Open large-scale language models.
\newblock {\em arXiv preprint arXiv:2309.10305}, 2023.

\bibitem{2023internlm}
InternLM Team.
\newblock Internlm: A multilingual language model with progressively enhanced capabilities.
\newblock \url{https://github.com/InternLM/InternLM}, 2023.

\bibitem{longchat2023}
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph~E. Gonzalez, Ion Stoica, Xuezhe Ma, , and Hao Zhang.
\newblock How long can open-source llms truly promise on context length?, June 2023.

\bibitem{MosaicML2023Introducing}
MosaicML~NLP Team.
\newblock Introducing mpt-7b: A new standard for open-source, ly usable llms, 2023.
\newblock Accessed: 2023-03-28.

\end{thebibliography}
