Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Wang2018,
abstract = {It has been recently shown that a convolutional neural network can learn optical flow estimation with unsuper-vised learning. However, the performance of the unsuper-vised methods still has a relatively large gap compared to its supervised counterpart. Occlusion and large motion are some of the major factors that limit the current unsuper-vised learning of optical flow methods. In this work we introduce a new method which models occlusion explicitly and a new warping way that facilitates the learning of large motion. Our method shows promising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets. Especially on KITTI dataset where abundant unlabeled samples exist, our unsupervised method outperforms its counterpart trained with supervised learning.},
author = {Wang, Yang and Yang, Yi and Yang, Zhenheng and Zhao, Liang and Wang, Peng and Xu, Wei},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2018 - Occlusion Aware Unsupervised Learning of Optical Flow.pdf:pdf},
pages = {4884--4893},
title = {{Occlusion Aware Unsupervised Learning of Optical Flow}},
year = {2018}
}
@techreport{Chan,
abstract = {Figure 1: "Do as I Do" motion transfer: given a YouTube clip of a ballerina (top), and a video of a graduate student performing various motions, our method transfers the ballerina's performance onto the student (bottom). Abstract This paper presents a simple method for "do as I do" motion transfer: given a source video of a person dancing, we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We approach this problem as video-to-video translation using pose as an intermediate representation. To transfer the motion, we extract poses from the source subject and apply the learned pose-to-appearance mapping to generate the target subject. We predict two consecutive frames for temporally coherent video results and introduce a separate pipeline for realistic face synthesis. Although our method is quite simple, it produces surprisingly compelling results (see video). This motivates us to also provide a forensics tool for reliable synthetic content detection, which is able to distinguish videos synthesized by our system from real data. In addition, we release a first-of-its-kind open-source dataset of videos that can be legally used for training and motion transfer. * C. Chan is currently a graduate student at MIT CSAIL. † T. Zhou is currently affiliated with Humen, Inc.},
annote = {Video to graph to vedea generator

supervised},
archivePrefix = {arXiv},
arxivId = {1808.07371v2},
author = {Chan, Caroline and Ginosar, Shiry and Zhou, Tinghui and Efros, Alexei A},
eprint = {1808.07371v2},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan et al. - Unknown - Everybody Dance Now.pdf:pdf},
title = {{Everybody Dance Now}}
}
@inproceedings{Bengio2009,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions). Copyright 2009.},
address = {New York, New York, USA},
author = {Bengio, Yoshua and Louradour, J{\'{e}}r̂ome and Collobert, Ronan and Weston, Jason},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/1553374.1553380},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2009 - Curriculum learning.pdf:pdf},
isbn = {9781605585161},
pages = {1--8},
publisher = {ACM Press},
title = {{Curriculum learning}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
volume = {382},
year = {2009}
}
@article{szeliski2006tutorial,
annote = {cite book piblication in "Handbook of Mathematical Models in Computer Vision" instead},
author = {Szeliski, Richard},
doi = {10.1201/9780203997536},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szeliski - 2006 - Image Alignment and Stitching A Tutorial.pdf:pdf},
isbn = {9780203997536},
journal = {Foundations and Trends in Computer Graphics and Vision},
keywords = {affine,device coordinates,motion models,normalized coordinates,overview,taylor series,tutorial},
mendeley-tags = {affine,device coordinates,motion models,normalized coordinates,overview,taylor series,tutorial},
number = {1},
pages = {1--104},
title = {{Image Alignment and Stitching: A Tutorial}},
volume = {2},
year = {2006}
}
@techreport{Huang,
abstract = {We propose a real-time intermediate flow estimation algorithm (RIFE) for video frame interpolation (VFI). Most existing methods first estimate the bi-directional optical flows, and then linearly combine them to approximate intermediate flows, leading to artifacts around motion boundaries. We design an intermediate flow model named IFNet that can directly estimate the intermediate flows from coarse to fine. We then warp the input frames according to the estimated intermediate flows and employ a fusion process to compute final results. Based on our proposed leakage distillation , RIFE can be trained end-to-end and achieve excellent performance. Experiments demonstrate that RIFE is significantly faster than existing flow-based VFI methods and achieves state-of-the-art index on several benchmarks. The code is available at https://github. com/hzwer/RIFE.},
annote = {Optical flow based video interpolation

Good review of the topic
Good idea to refine warped imags via Fusion map (fusin forward and backward warps) and residual (added after interpolation)

Trained end-to-end, with 3 loss functions:
- reconstruction loss
- census loss (handles illumination changes between scenes, specific to optical flow)
- distillation loss (intermediate features supervised by a stronger, pre-trained network)},
archivePrefix = {arXiv},
arxivId = {2011.06294v1},
author = {Huang, Zhewei and Zhang, Tianyuan and Heng, Wen and Shi, Boxin and Zhou, Shuchang},
eprint = {2011.06294v1},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - RIFE Real-Time Intermediate Flow Estimation for Video Frame Interpolation.pdf:pdf},
title = {{RIFE: Real-Time Intermediate Flow Estimation for Video Frame Interpolation}},
url = {https://github.}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
publisher = {Springer Verlag},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
volume = {9351},
year = {2015}
}
@techreport{Ranjan,
abstract = {We learn to compute optical flow by combining a classical spatial-pyramid formulation with deep learning. This estimates large motions in a coarse-to-fine approach by warping one image of a pair at each pyramid level by the current flow estimate and computing an update to the flow. Instead of the standard minimization of an objective function at each pyramid level, we train one deep network per level to compute the flow update. Unlike the recent FlowNet approach, the networks do not need to deal with large motions ; these are dealt with by the pyramid. This has several advantages. First, our Spatial Pyramid Network (SPyNet) is much simpler and 96{\%} smaller than FlowNet in terms of model parameters. This makes it more efficient and appropriate for embedded applications. Second, since the flow at each pyramid level is small ({\textless} 1 pixel), a convolutional approach applied to pairs of warped images is appropriate. Third, unlike FlowNet, the learned convolution filters appear similar to classical spatio-temporal filters, giving insight into the method and how to improve it. Our results are more accurate than FlowNet on most standard benchmarks , suggesting a new direction of combining classical flow methods with deep learning.},
annote = {Pyramid-approach to motion estimation

Problem: Estimate two-frame optical flow using an end-to-end deep learning approach},
author = {Ranjan, Anurag and Black, Michael J},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ranjan, Black - Unknown - Optical Flow Estimation using a Spatial Pyramid Network.pdf:pdf},
title = {{Optical Flow Estimation using a Spatial Pyramid Network}},
url = {https://github.com/anuragranj/spynet}
}
@incollection{Hu2019,
abstract = {We propose a Dual-Stream Pyramid Registration Network (referred as Dual-PRNet) for unsupervised 3D medical image registration. Unlike recent CNN-based registration approaches, such as VoxelMorph, which explores a single-stream encoder-decoder network to compute a . Our contributiregistration fields from a pair of 3D volumes, we design a two-stream architecture able to compute multi-scale registration fields from convolutional feature pyramidsons are two-fold: (i) we design a two-stream 3D encoder-decoder network which computes two convolutional feature pyramids separately for a pair of input volumes, resulting in strong deep representations that are meaningful for deformation estimation; (ii) we propose a pyramid registration module able to predict multi-scale registration fields directly from the decoding feature pyramids. This allows it to refine the registration fields gradually in a coarse-to-fine manner via sequential warping, and enable the model with the capability for handling significant deformations between two volumes, such as large displacements in spatial domain or slice space. The proposed Dual-PRNet is evaluated on two standard benchmarks for brain MRI registration, where it outperforms the state-of-the-art approaches by a large margin, e.g., having improvements over recent VoxelMorph [2] with 0.683-{\textgreater}0.778 on the LPBA40, and 0.511-{\textgreater}0.631 on the Mindboggle101, in term of average Dice score.},
annote = {Unsupervised

Using a multi-stage approach to learn a registration map. Two parallel U-Nets interacting with each other

Simple architecture, similar to multi-stage segmentation (coarse to fine), and great results!

supplementary material available separately},
archivePrefix = {arXiv},
arxivId = {1909.11966},
author = {Hu, Xiaojun and Kang, Miao and Huang, Weilin and Scott, Matthew R. and Wiest, Roland and Reyes, Mauricio},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_43},
eprint = {1909.11966},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2019 - Dual-Stream Pyramid Registration Network.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {382--390},
title = {{Dual-Stream Pyramid Registration Network}},
year = {2019}
}
@inproceedings{Isola2017,
abstract = {Labels to Facade BW to Color Aerial to Map Labels to Street Scene Edges to Photo input output input input input input output output output output input output Day to Night Figure 1: Many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image. These problems are often treated with application-specific algorithms, even though the setting is always the same: map pixels to pixels. Conditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. Here we show results of the method on several. In each case we use the same architecture and objective, and simply train on different data. Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pix2pix software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A and Research, Berkeley Ai},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Isola et al. - 2017 - Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
pages = {1125--1134},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {https://github.com/phillipi/pix2pix.},
year = {2017}
}
@inproceedings{Li2010,
abstract = {Topological changes are common in brain MR images for aging or disease studies. For deformable registration algorithms, which are formulated as a variational problem and solved by the minimization of certain energy functional, topological changes can cause false deformation in the resulting vector field, and affect algorithm convergence. In this work, we focus on the effect of topological changes on diffeomorphic and inverse-consistent deformable registration algorithms, specifically, diffeomorphic demons and symmetric LDDMM. We first use a simple example to demonstrate the adverse effect of topological changes on these algorithms. Then, we propose an novel framework that can be imposed onto any existing diffeomorphic and inverse-consistent deformable registration algorithm. Our framework renders these registration algorithms robust to topological changes, where the outputwill consist of two components. The first is a deformation field that presents only the brain structural change which is the expected vector field if the topological change did not exist. The second component is a label map that provides a segmentation of the topological changes appeared in input images. {\textcopyright}2010 IEEE.},
annote = {+ registration with anatomical mismatch
- does not support topological changes (insertion/removal), only mismatches
+ ignores anatomical mismatches during registration
+ finds anatomical mismatch in automatic pre-registration step


TLDR: identify topological changes, ignore them during registration

Goal: fully diffeomorphic registration, ignoring topological changes in the process.
Topological changes are identified and removed from the image, allowing registration without being disturbed by them.

multi-step process:
1) coarse-scale registration
2) identify non-matching areas in coarse sale reg, these are the discontenueties
3) replace these areas by interpolating surrounding tissue
4) another normal registration step, now the topological changes are no longer present},
author = {Li, Xiaoxing and Wyatt, Chritopher},
booktitle = {2010 7th IEEE International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2010.5490334},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Wyatt - 2010 - Modeling topological changes in deformable registration.pdf:pdf},
isbn = {9781424441266},
keywords = {Deformable registration,Segmentation,Topological change},
pages = {360--363},
title = {{Modeling topological changes in deformable registration}},
year = {2010}
}
@article{citationrequired,
author = {CitationRequired},
title = {{TODO}},
year = {9999}
}
@article{Fu2020,
author = {Fu, Yabo and Lei, Yang and Wang, Tonghe and Higgins, Kristin and Bradley, Jeffrey D. and Curran, Walter J. and Liu, Tian and Yang, Xiaofeng},
doi = {10.1002/mp.14065},
issn = {0094-2405},
journal = {Medical Physics},
month = {feb},
publisher = {Wiley},
title = {{LungRegNet: an unsupervised deformable image registration method for 4D‐CT lung}},
year = {2020}
}
@article{Papiez2014,
abstract = {Several biomedical applications require accurate image registration that can cope effectively with complex organ deformations. This paper addresses this problem by introducing a generic deformable registration algorithm with a new regularization scheme, which is performed through bilateral filtering of the deformation field. The proposed approach is primarily designed to handle smooth deformations both between and within body structures, and also more challenging deformation discontinuities exhibited by sliding organs. The conventional Gaussian smoothing of deformation fields is replaced by a bilateral filtering procedure, which compromises between the spatial smoothness and local intensity similarity kernels, and is further supported by a deformation field similarity kernel. Moreover, the presented framework does not require any explicit prior knowledge about the organ motion properties (e.g. segmentation) and therefore forms a fully automated registration technique. Validation was performed using synthetic phantom data and publicly available clinical 4D CT lung data sets. In both cases, the quantitative analysis shows improved accuracy when compared to conventional Gaussian smoothing. In addition, we provide experimental evidence that masking the lungs in order to avoid the problem of sliding motion during registration performs similarly in terms of the target registration error when compared to the proposed approach, however it requires accurate lung segmentation. Finally, quantification of the level and location of detected sliding motion yields visually plausible results by demonstrating noticeable sliding at the pleural cavity boundaries.},
author = {Papiez, Bart{\l}omiej W. and Heinrich, Mattias P. and Fehrenbach, J{\'{e}}rome and Risser, Laurent and Schnabel, Julia A.},
doi = {10.1016/j.media.2014.05.005},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Papiez et al. - 2014 - An implicit sliding-motion preserving regularisation via bilateral filtering for deformable image registration.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Bilateral filtering,Nonrigid registration,Regularisation,Respiratory motion,Sliding motion},
month = {dec},
number = {8},
pages = {1299--1311},
publisher = {Elsevier},
title = {{An implicit sliding-motion preserving regularisation via bilateral filtering for deformable image registration}},
volume = {18},
year = {2014}
}
@article{Sowell2002,
abstract = {Previous in vivo morphometric studies of human brain maturation between childhood and young adulthood have revealed a spatial and temporal pattern of progressive brain changes that is consistent with the post mortem cytoarchitectonic and cognitive developmental literatures. In this study, we mapped age differences in structural asymmetries at the cortical surface in groups of normally developing children (7-11 years), adolescents (12-16 years) and young adults (23-30 years) using novel surface-based mesh modeling image analytic methods. We also assessed relationships between cortical surface sulcal asymmetry and the local density of the underlying cortical gray matter. Results from this study reveal that perisylvian sulcal asymmetries are much more prominent in the adults than in the children studied. The superior posterior extent of the Sylvian fissure in the right hemisphere is ∼7 mm more superior in the average adult than in the average child studied, whereas little difference is observed during this age range in the location of this anatomical structure in the left hemisphere. Age-related differences in Sylvian fissure asymmetry were significant (P = 0.0129, permutation test), showing increased asymmetry with increasing age. We also show age-related increases in local gray matter proportion bilaterally in the temporo-parietal cortices that are anatomically and temporally related to the sulcal asymmetries. Results from this cross-sectional study imply that asymmetries in the Sylvian fissure are dynamically changing into young adulthood and show that variability in brain tissue density is related to asymmetry in this region. These morphological differences may be related to changing cognitive abilities and are relevant in interpreting results from studies of abnormal brain development where perisylvian brain regions are implicated.},
author = {Sowell, Elizabeth R. and Thompson, Paul M. and Rex, David and Kornsand, David and Tessner, Kevin D. and Jernigan, Terry L. and Toga, Arthur W.},
doi = {10.1093/cercor/12.1.17},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sowell et al. - 2002 - Mapping sulcal pattern asymmetry and local cortical surface gray matter distribution in vivo Maturation in perisy.pdf:pdf},
issn = {10473211},
journal = {Cerebral Cortex},
keywords = {adult,brain,child,gray matter,sylvian fissure},
month = {jan},
number = {1},
pages = {17--26},
pmid = {11734529},
publisher = {Oxford University Press},
title = {{Mapping sulcal pattern asymmetry and local cortical surface gray matter distribution in vivo: Maturation in perisylvian cortices}},
url = {http://www.loni.ucla.edu/{~}esowell/sulcvar.html.},
volume = {12},
year = {2002}
}
@article{Dalca2018,
abstract = {Traditional deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates. In this paper, we present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task, and provide an empirical analysis of the algorithm. Our approach results in state of the art accuracy and very fast runtimes, while providing diffeomorphic guarantees and uncertainty estimates. Our implementation is available online at http://voxelmorph.csail.mit.edu .},
annote = {Unsupervised, propabalistic

Voxelpmorph V2.0},
archivePrefix = {arXiv},
arxivId = {1805.04605},
author = {Dalca, Adrian V. and Balakrishnan, Guha and Guttag, John and Sabuncu, Mert R.},
doi = {10.1007/978-3-030-00928-1_82},
eprint = {1805.04605},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalca et al. - 2018 - Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration.pdf:pdf},
journal = {Medical Image Computing and Computer Assisted Intervention},
month = {may},
pages = {729--738},
title = {{Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration}},
url = {http://arxiv.org/abs/1805.04605 http://dx.doi.org/10.1007/978-3-030-00928-1{\_}82},
year = {2018}
}
@article{Rueckert1999,
abstract = {In this paper we present a new approach for the nonrigid registration of contrast-enhanced breast MRI. A hierarchical transformation model of the motion of the breast has been developed. The global motion of the breast is modeled by an affine transformation while the local breast motion is described by a free-form deformation (FFD) based on B-splines. Normalized mutual information is used as a voxel-based similarity measure which is insensitive to intensity changes as a result of the contrast enhancement. Registration is achieved by minimizing a cost function, which represents a combination of the cost associated with the smoothness of the transformation and the cost associated with the image similarity. The algorithm has been applied to the fully automated registration of three-dimensional (3-D) breast MRI in volunteers and patients. In particular, we have compared the results of the proposed nonrigid registration algorithm to those obtained using rigid and affine registration techniques. The results clearly indicate that the nonrigid registration algorithm is much better able to recover the motion and deformation of the breast than rigid or affine registration algorithms. {\textcopyright} 1999 IEEE.},
annote = {B-splines paper},
author = {Rueckert, Daniel and Sonoda, Luke I and Hayes, Carmel and Hill, Derek LG and Leach, Martin O and Hawkes, David J},
doi = {10.1109/42.796284},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rueckert et al. - 1999 - Nonrigid registration using free-form deformations Application to breast mr images.pdf:pdf},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
number = {8},
pages = {712--721},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Nonrigid registration using free-form deformations: Application to breast mr images}},
volume = {18},
year = {1999}
}
@inproceedings{Li2018,
abstract = {A novel non-rigid image registration algorithm is built upon fully convolutional networks (FCNs) to optimize and learn spatial transformations between pairs of images to be registered in a self-supervised learning framework. Different from most existing deep learning based image registration methods that learn spatial transformations from training data with known corresponding spatial transformations, our method directly estimates spatial transformations between pairs of images by maximizing an image-wise similarity metric between fixed and deformed moving images, similar to conventional image registration algorithms. The image registration is implemented in a multi-resolution image registration framework to jointly optimize and learn spatial transformations and FCNs at different spatial resolutions with deep self-supervision through typical feedforward and backpropagation computation. The proposed method has been evaluated for registering 3D structural brain magnetic resonance (MR) images and obtained better performance than state-of-the-art image registration algorithms.},
annote = {more of a schale-space work....},
author = {Li, Hongming and Fan, Yong},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2018.8363757},
isbn = {9781538636367},
issn = {19458452},
keywords = {Fully convolutional networks,Image registration,Multi-resolution,Self-supervision},
month = {may},
pages = {1075--1078},
publisher = {IEEE Computer Society},
title = {{Non-rigid image registration using self-supervised fully convolutional networks without training data}},
volume = {2018-April},
year = {2018}
}
@article{Pielawski2020a,
abstract = {We propose contrastive coding to learn shared, dense image representations, referred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs enable the registration of multimodal images where existing registration methods often fail due to a lack of sufficiently similar image structures. CoMIRs reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods, used for, e.g., classification, our approach generates image-like representations that contain the information shared between modalities. We introduce a novel, hyperparameter-free modification to InfoNCE, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of RGB and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-field and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on CoMIRs significantly outperforms registration of representations created by GAN-based image-to-image translation, as well as a state-of-the-art, application-specific method which takes additional knowledge about the data into account. Code is available at: https://github.com/MIDA-group/CoMIR.},
archivePrefix = {arXiv},
arxivId = {2006.06325},
author = {Pielawski, Nicolas and Wetzer, Elisabeth and {\"{O}}fverstedt, Johan and Lu, Jiahao and W{\"{a}}hlby, Carolina and Lindblad, Joakim and Sladoje, Nata{\v{s}}a},
eprint = {2006.06325},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pielawski et al. - 2020 - CoMIR Contrastive Multimodal Image Representation for Registration(2).pdf:pdf},
journal = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
month = {jun},
publisher = {arXiv},
title = {{CoMIR: Contrastive Multimodal Image Representation for Registration}},
url = {http://arxiv.org/abs/2006.06325},
year = {2020}
}
@inproceedings{Chen2021b,
abstract = {Image registration aims to establish spatial correspondence across pairs, or groups of images, and is a cornerstone of medical image computing and computer-assisted-interventions. Currently, most deep learning-based registration methods assume that the desired deformation fields are globally smooth and continuous, which is not always valid for real-world scenarios, especially in medical image registration (e.g. cardiac imaging and abdominal imaging). Such a global constraint can lead to artefacts and increased errors at discontinuous tissue interfaces. To tackle this issue, we propose a weakly-supervised Deep Discontinuity-preserving Image Registration network (DDIR), to obtain better registration performance and realistic deformation fields. We demonstrate that our method achieves significant improvements in registration accuracy and predicts more realistic deformations, in registration experiments on cardiac magnetic resonance (MR) images from UK Biobank Imaging Study (UKBB), than state-of-the-art approaches.},
annote = {Deep Discontinuity-Preserving Image Registration Network

Explicit Sliding-Motion approach: Each organ registered separately, discontinuities along the border. Uses explicit organ segmentation

Problem:
- entierly smooth and continouus transformation fields not applicable to many medical imaging domains
- Boundary effects due to different physical properties of tissue-types
- enforcing deformation fields to be globally smooth can generate unrealistic deformations and lead increased errors near these boundaries.

Method:
- requires segmentation of organs (weakly supervised approach)
- Register each segmented organ separately (with black background)
- compose fields, each starting only at their segmented areas
- results in smooth transformation fields within the segmentation areas, but discontinuities among the border.
- Loss function:
- similarity loss: NCC
- segmentation match via DICE
- Diffusion reg as in Voxelmorph, for each area separately


Results:
- higher accuracy than SyN, BSpline and Voxelmorph
- not compared to any sliding motion works},
archivePrefix = {arXiv},
arxivId = {2107.04440},
author = {Chen, Xiang and Ravikumar, Nishant and Xia, Yan and Frangi, Alejandro F},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
eprint = {2107.04440},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2021 - A Deep Discontinuity-Preserving Image Registration Network.pdf:pdf},
keywords = {Cardiac,Deep,Discontinuity-preserving,Image,Learning {\textperiodcentered},Reg-istration {\textperiodcentered},Registration,Registration {\textperiodcentered}},
month = {jul},
title = {{A Deep Discontinuity-Preserving Image Registration Network}},
url = {https://arxiv.org/abs/2107.04440v1},
year = {2021}
}
@article{Anthony2020,
abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},
archivePrefix = {arXiv},
arxivId = {2007.03051},
author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
eprint = {2007.03051},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anthony, Kanding, Selvan - 2020 - Carbontracker Tracking and Predicting the Carbon Footprint of Training Deep Learning Models.pdf:pdf},
journal = {ICML Workshop on Challenges in Deploying and monitoring Machine Learning Systems},
month = {jul},
title = {{Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models}},
url = {http://arxiv.org/abs/2007.03051},
year = {2020}
}
@article{Courty2017,
abstract = {Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.},
archivePrefix = {arXiv},
arxivId = {1507.00504},
author = {Courty, Nicolas and Flamary, Remi and Tuia, Devis and Rakotomamonjy, Alain},
doi = {10.1109/TPAMI.2016.2615921},
eprint = {1507.00504},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Unsupervised domain adaptation,classification,optimal transport,transfer learning,visual adaptation},
month = {sep},
number = {9},
pages = {1853--1865},
pmid = {27723579},
publisher = {IEEE Computer Society},
title = {{Optimal Transport for Domain Adaptation}},
volume = {39},
year = {2017}
}
@article{faisalBeg2005,
abstract = {This paper examine the Euler-Lagrange equations for the solution of the large deformation diffeomor-phic metric mapping problem studied in Dupuis et al. (1998) and Trouv{\'{e}} (1995) in which two images I 0 , I 1 are given and connected via the diffeomorphic change of coordinates I 0 • ϕ −1 = I 1 where ϕ = $\phi$ 1 is the end point at t = 1 of curve $\phi$ t , t ∈ [0, 1] satisfying ˙ $\phi$ t = v t ($\phi$ t), t ∈ [0, 1] with $\phi$ 0 = id. The variational problem takes the form argmin v: ˙ $\phi$ t =v t ($\phi$ t) 1 0 v t 2 V dt + I 0 • $\phi$ −1 1 − I 1 2 L 2 , where v t V is an appropriate Sobolev norm on the velocity field v t ({\textperiodcentered}), and the second term enforces matching of the images with {\textperiodcentered}{\textperiodcentered} L 2 representing the squared-error norm. In this paper we derive the Euler-Lagrange equations characterizing the minimizing vector fields v t , t ∈ [0, 1] assuming sufficient smoothness of the norm to guarantee existence of solutions in the space of diffeomorphisms. We describe the implementation of the Euler equations using semi-Lagrangian method of computing particle flows and show the solutions for various examples. We also compute the metric distance on several anatomical configurations as measured by 1 0 v t V dt on the geodesic shortest paths. 140 Beg et al.},
annote = {implementation in cpp: https://github.com/frankyeh/TIPL/blob/master/reg/lddmm.hpp},
author = {{Faisal Beg}, Mirza and Miller, Michael I and Trouv{\'{e}}trouv, Alain and Younes, Laurent},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Faisal Beg et al. - 2005 - Computing Large Deformation Metric Mappings via Geodesic Flows of Diffeomorphisms.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {Euler-Lagrange equation,computational anatomy,deformable template,metrics,variational optimization},
number = {2},
pages = {139--157},
title = {{Computing Large Deformation Metric Mappings via Geodesic Flows of Diffeomorphisms}},
volume = {61},
year = {2005}
}
@article{Chen2021a,
abstract = {In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.},
annote = {E-6

video: https://2021.midl.io/papers/e6

Plausible hypothesis that long-distance correspondences are hard to fund with conv-nets. Experimental results to back up the claim?

To what extend do U-net style skip connections around the Transformer bloks compremise the vision transformer?

Why/Where does Vit achieve better dice scores than voxelmorph?},
archivePrefix = {arXiv},
arxivId = {2104.06468},
author = {Chen, Junyu and He, Yufan and Frey, Eric C. and Li, Ye and Du, Yong},
eprint = {2104.06468},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2021 - ViT-V-Net Vision Transformer for Unsupervised Volumetric Medical Image Registration(2).pdf:pdf},
journal = {Medical Imaging with Deep Learning},
keywords = {convolutional neural networks,image registration,vision transformer},
pages = {2020--2022},
title = {{ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration}},
url = {http://arxiv.org/abs/2104.06468},
year = {2021}
}
@article{Codella2019,
abstract = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10{\%} of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
archivePrefix = {arXiv},
arxivId = {1902.03368},
author = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M. Emre and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
eprint = {1902.03368},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Codella et al. - 2019 - Skin Lesion Analysis Toward Melanoma Detection 2018 A Challenge Hosted by the International Skin Imaging Collabo.pdf:pdf},
journal = {arXiv},
keywords = {Deep learning,Dermoscopy,Melanoma,Skin cancer},
month = {feb},
publisher = {arXiv},
title = {{Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC)}},
url = {http://arxiv.org/abs/1902.03368},
year = {2019}
}
@incollection{Lee2019,
annote = {???

registration based on regions of interest and associated features.},
author = {Lee, David S. and Sahib, Ashish and Wade, Benjamin and Narr, Katherine L. and Hellemann, Gerhard and Woods, Roger P. and Joshi, Shantanu H.},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_42},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2019 - Multimodal Data Registration for Brain Structural Association Networks.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {373--381},
title = {{Multimodal Data Registration for Brain Structural Association Networks}},
year = {2019}
}
@techreport{Fechter2020,
abstract = {Deformable image registration is a very important field of research in medical imaging. Recently multiple deep learning approaches were published in this area showing promising results. However, drawbacks of deep learning methods are the need for a large amount of training datasets and their inability to register unseen images different from the training datasets. One shot learning comes without the need of large training datasets and has already been proven to be applicable to 3D data. In this work we present a one shot registration approach for periodic motion tracking in 3D and 4D datasets. When applied to a 3D dataset the algorithm calculates the inverse of the registration vector field simultaneously. For registration we employed a U-Net combined with a coarse to fine approach and a differential spatial transformer module. The algorithm was thoroughly tested with multiple 4D and 3D datasets publicly available. The results show that the presented approach is able to track periodic motion and to yield a competitive registration accuracy. Possible applications are the use as a stand-alone algorithm for 3D and 4D motion tracking or in the beginning of studies until enough datasets for a separate training phase are available.},
annote = {Summary:
patch-based, unsupervised, one-shot learning (training one one input image) with good results. avieves an average landmark distance of 1.77mm +- 2.07 on extreme phases of dirlab dataset. Uses multiple scales, compares to many other works on the dirlab dataset.


Data:
31 patients, many images per patient
Prediction: given a time series of N images, we align each image n to the following n+1
Additionally, extreme phase registration is evaluated

This leads to small deformations. Contrast to my approach: register the two extreme images --{\textgreater} much larger deformations

Data pre-processing:
patch based learning, only consider patches of the body (no background)

Patches:
performs patch-based learning with non-overlapping. Predicts a SVF for each patch, then appends them to the whole image. Patches taken at different scales, to enable large deformations

Model:
shallow unet (2 layers). 3 Convolutions per layer
3 separet strands (different weights, trained separately), for different resolutions

Training:
Performs one-shot learning. The model is fine-tuned for each pair of images before prediction.

Evaluation:
Landmarks: landmark distance
Segmentation: S{\o}rensen-Dice index, Hausdorff distance, average symmetric surface distance
Diffeomorphism: determinant of the Jacobian matrix

Results:
achieves landmark distance of 1.77mm +- 2.07 on extreme phases of dirlab datasets (our VXM: 3.2mm on test data)},
archivePrefix = {arXiv},
arxivId = {1907.04641v3},
author = {Fechter, Tobias and Baltas, Dimos},
eprint = {1907.04641v3},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fechter, Baltas - 2020 - One Shot Learning for Deformable Medical Image Registration and Periodic Motion Tracking.pdf:pdf},
keywords = {Index Terms-Machine learning,Motion compensation and analysis,Neural network,Registration},
title = {{One Shot Learning for Deformable Medical Image Registration and Periodic Motion Tracking}},
url = {https://github.com/ToFec/OneShotImageRegistration},
year = {2020}
}
@article{jaderberg2015spatial,
abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaderberg et al. - 2015 - Spatial Transformer Networks.pdf:pdf},
journal = {Advances in neural information processing systems},
keywords = {NN},
mendeley-tags = {NN},
title = {{Spatial Transformer Networks}},
year = {2015}
}
@article{Czolbe2020,
abstract = {To train Variational Autoencoders (VAEs) to generate realistic imagery requires a loss function that reflects human perception of image similarity. We propose such a loss function based on Watson's perceptual model, which computes a weighted distance in frequency space and accounts for luminance and contrast masking. We extend the model to color images, increase its robustness to translation by using the Fourier Transform, remove artifacts due to splitting the image into blocks, and make it differentiable. In experiments, VAEs trained with the new loss function generated realistic, high-quality image samples. Compared to using the Euclidean distance and the Structural Similarity Index, the images were less blurry; compared to deep neural network based losses, the new approach required less computational resources and generated images with less artifacts.},
archivePrefix = {arXiv},
arxivId = {2006.15057},
author = {Czolbe, Steffen and Krause, Oswin and Cox, Ingemar and Igel, Christian},
eprint = {2006.15057},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czolbe et al. - 2020 - A Loss Function for Generative Neural Networks Based on Watson's Perceptual Model.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
title = {{A Loss Function for Generative Neural Networks Based on Watson's Perceptual Model}},
url = {http://arxiv.org/abs/2006.15057},
year = {2020}
}
@article{Dosovitskiy2020,
abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
annote = {video explanation: https://youtu.be/TrdevFK{\_}am4?t=320
ViT model

[3] in image transformer reading group

Very data intensive. Calls ImageNet 'small'
when trained on just ImageNet, performance worse than CNN
Only better than CNN when trained on muh larger datasets 


compared to Image Transformer paper:
- computational efficiency via patch embeddings instead of local pixel-whise memory
- large gains in image classification via pre-training on huge dataset
- huge model
- no image generation/imprinting. only classification.
- positional encodings. See appendix D.3



questions:
Exact working of hybrind model: "patch size of one pixel" (page 4) = one pixel on the lower-resolution representation?},
archivePrefix = {arXiv},
arxivId = {2010.11929},
author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
eprint = {2010.11929},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Image Recognition at Scale.pdf:pdf},
journal = {Proceedings of the 38th International Conference on Machine Learning},
month = {oct},
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
url = {http://arxiv.org/abs/2010.11929},
year = {2021}
}
@article{Avants2008,
abstract = {One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals.},
annote = {SyN, ANTs paper
Great evaluation!},
author = {Avants, B. B. and Epstein, C. L. and Grossman, M. and Gee, J. C.},
doi = {10.1016/j.media.2007.06.004},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Avants et al. - 2008 - Symmetric diffeomorphic image registration with cross-correlation Evaluating automated labeling of elderly and ne.pdf:pdf},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Cross-correlation,Deformable image registration,Dementia,Diffeomorphic,Human cortex,Morphometry},
month = {feb},
number = {1},
pages = {26--41},
pmid = {17659998},
title = {{Symmetric diffeomorphic image registration with cross-correlation: Evaluating automated labeling of elderly and neurodegenerative brain}},
volume = {12},
year = {2008}
}
@article{Castillo2013,
abstract = {Landmark point-pairs provide a strategy to assess deformable image registration (DIR) accuracy in terms of the spatial registration of the underlying anatomy depicted in medical images. In this study, we propose to augment a publicly available database (www.dir-lab.com) of medical images with large sets of manually identified anatomic feature pairs between breath-hold computed tomography (BH-CT) images for DIR spatial accuracy evaluation. Ten BH-CT image pairs were randomly selected from the COPDgene study cases. Each patient had received CT imaging of the entire thorax in the supine position at one-fourth dose normal expiration and maximum effort full dose inspiration. Using dedicated in-house software, an imaging expert manually identified large sets of anatomic feature pairs between images. Estimates of inter- and intra-observer spatial variation in feature localization were determined by repeat measurements of multiple observers over subsets of randomly selected features. 7298 anatomic landmark features were manually paired between the 10 sets of images. Quantity of feature pairs per case ranged from 447 to 1172. Average 3D Euclidean landmark displacements varied substantially among cases, ranging from 12.29 (SD: 6.39) to 30.90 (SD: 14.05) mm. Repeat registration of uniformly sampled subsets of 150 landmarks for each case yielded estimates of observer localization error, which ranged in average from 0.58 (SD: 0.87) to 1.06 (SD: 2.38) mm for each case. The additions to the online web database (www.dir-lab.com) described in this work will broaden the applicability of the reference data, providing a freely available common dataset for targeted critical evaluation of DIR spatial accuracy performance in multiple clinical settings. Estimates of observer variance in feature localization suggest consistent spatial accuracy for all observers across both four-dimensional CT and COPDgene patient cohorts. {\textcopyright} 2013 Institute of Physics and Engineering in Medicine.},
author = {Castillo, Richard and Castillo, Edward and Fuentes, David and Ahmad, Moiz and Wood, Abbie M. and Ludwig, Michelle S. and Guerrero, Thomas},
doi = {10.1088/0031-9155/58/9/2861},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Castillo et al. - 2013 - A reference dataset for deformable image registration spatial accuracy evaluation using the COPDgene study arch.pdf:pdf},
issn = {00319155},
journal = {Physics in Medicine and Biology},
month = {may},
number = {9},
pages = {2861--2877},
title = {{A reference dataset for deformable image registration spatial accuracy evaluation using the COPDgene study archive}},
volume = {58},
year = {2013}
}
@article{Tschandl2018,
abstract = {Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50{\%} of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.},
author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
doi = {10.1038/sdata.2018.161},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tschandl, Rosendahl, Kittler - 2018 - Data descriptor The HAM10000 dataset, a large collection of multi-source dermatoscopic images of c.pdf:pdf},
issn = {20524463},
journal = {Scientific Data},
keywords = {Basal cell carcinoma,Cancer imaging,Cancer screening,Melanoma,Squamous cell carcinoma},
month = {aug},
number = {1},
pages = {1--9},
pmid = {30106392},
publisher = {Nature Publishing Groups},
title = {{Data descriptor: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions}},
url = {www.nature.com/sdata/},
volume = {5},
year = {2018}
}
@article{Li2012,
abstract = {This paper presents registration via embedded maps (REM), a deformable registration algorithm for images with varying topology. The algorithm represents 3-D images as 4-D manifolds in a Riemannian space (referred to as embedded maps). Registration is performed as a surface evolution matching one embedded map to another using a diffusion process. The approach differs from those existing in that it takes an apriori estimation of image regions where topological changes are present, for example lesions, and generates a dense vector field representing both the shape and intensity changes necessary to match the images. The algorithm outputs both a diffeomorphic deformation field and an intensity displacement which corrects the intensity difference caused by topological changes. Multiple sets of experiments are conducted on magnetic resonance imaging (MRI) with lesions from OASIS and ADNI datasets. These images are registered to either a brain template or images of healthy individuals. An exemplar case registering a template to an MRI with tumor is also given. The resulting deformation fields were compared with those obtained using diffeomorphic demons, where topological changes are not modeled. These sets of experiments demonstrate the efficacy of our proposed REM method for registration of brain MRI with severe topological differences. {\textcopyright} 2012, IEEE.},
annote = {+ registration with anatomical mismatch
- does not support topological changes (insertion/removal), only mismatches
+ weights intensity gradient vs transformation regularization based on beta-parameter
- beta fron annotations (differences must be annotated)
Predicts both displacement field and intensity adaptation
heavily relies on synthetic data

Topological changes can cause false deformation during the registration process! E.g. false information about local growth and shrinkage

Lots of good citations},
author = {Li, Xiaoxing and Long, Xiaojing and Wyatt, Christopher and Laurienti, Paul},
doi = {10.1109/TMI.2011.2178609},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2012 - Registration of Images with Varying Topology Using Embedded Maps.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Deformable registration,Riemannian embedding,false deformation,topological change},
month = {mar},
number = {3},
pages = {749--765},
pmid = {22194239},
title = {{Registration of Images with Varying Topology Using Embedded Maps}},
volume = {31},
year = {2012}
}
@article{Murphy2011,
abstract = {EMPIRE10 (Evaluation of Methods for Pulmonary Image REgistration 2010) is a public platform for fair and meaningful comparison of registration algorithms which are applied to a database of intrapatient thoracic CT image pairs. Evaluation of nonrigid registration techniques is a nontrivial task. This is compounded by the fact that researchers typically test only on their own data, which varies widely. For this reason, reliable assessment and comparison of different registration algorithms has been virtually impossible in the past. In this work we present the results of the launch phase of EMPIRE10, which comprised the comprehensive evaluation and comparison of 20 individual algorithms from leading academic and industrial research groups. All algorithms are applied to the same set of 30 thoracic CT pairs. Algorithm settings and parameters are chosen by researchers expert in the configuration of their own method and the evaluation is independent, using the same criteria for all participants. All results are published on the EMPIRE10 website (http://empire10.isi.uu.nl). The challenge remains ongoing and open to new participants. Full results from 24 algorithms have been published at the time of writing. This paper details the organization of the challenge, the data and evaluation methods and the outcome of the initial launch with 20 algorithms. The gain in knowledge and future work are discussed. {\textcopyright} 2011 IEEE.},
author = {Murphy, Keelin and {Van Ginneken}, Bram and Reinhardt, Joseph M. and Kabus, Sven and Ding, Kai and Deng, Xiang and Cao, Kunlin and Du, Kaifang and Christensen, Gary E. and Garcia, Vincent and Vercauteren, Tom and Ayache, Nicholas and Commowick, Olivier and Malandain, Grgoire and Glocker, Ben and Paragios, Nikos and Navab, Nassir and Gorbunova, Vladlena and Sporring, Jon and {De Bruijne}, Marleen and Han, Xiao and Heinrich, Mattias P. and Schnabel, Julia A. and Jenkinson, Mark and Lorenz, Cristian and Modat, Marc and McClelland, Jamie R. and Ourselin, Sebastien and Muenzing, Sascha E.A. and Viergever, Max A. and {De Nigris}, Dante and Collins, D. Louis and Arbel, Tal and Peroni, Marta and Li, Rui and Sharp, Gregory C. and Schmidt-Richberg, Alexander and Ehrhardt, Jan and Werner, Ren{\'{e}} and Smeets, Dirk and Loeckx, Dirk and Song, Gang and Tustison, Nicholas and Avants, Brian and Gee, James C. and Staring, Marius and Klein, Stefan and Stoel, Berend C. and Urschler, Martin and Werlberger, Manuel and Vandemeulebroucke, Jef and Rit, Simon and Sarrut, David and Pluim, Josien P.W.},
doi = {10.1109/TMI.2011.2158349},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy et al. - 2011 - Evaluation of registration methods on thoracic CT The EMPIRE10 challenge.pdf:pdf},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Chest,computed tomography,evaluation,registration},
number = {11},
pages = {1901--1920},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Evaluation of registration methods on thoracic CT: The EMPIRE10 challenge}},
volume = {30},
year = {2011}
}
@inproceedings{deng2009imagenet,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
booktitle = {Conference on Computer Vision and Pattern Recognition},
pages = {248--255},
title = {{Imagenet: A large-scale hierarchical image database}},
year = {2009}
}
@techreport{Zanfir2018,
abstract = {The problem of graph matching under node and pairwise constraints is fundamental in areas as diverse as combinatorial optimization, machine learning or computer vision, where representing both the relations between nodes and their neighborhood structure is essential. We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and pairwise node neighborhoods, represented as deep feature extraction hierarchies. The challenge is in the formulation of the different matrix computation layers of the model in a way that enables the consistent, efficient propagation of gradients in the complete pipeline from the loss function, through the combinatorial optimization layer solving the matching problem, and the feature extraction hierarchy. Our computer vision experiments and ablation studies on challenging datasets like PASCAL VOC keypoints, Sintel and CUB show that matching models refined end-to-end are superior to counterparts based on feature hierarchies trained for other problems.},
annote = {presentation: https://youtu.be/NHajBzgnC1Q?t=2166},
author = {Zanfir, Andrei and Sminchisescu, Cristian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00284},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zanfir, Sminchisescu - 2018 - Deep Learning of Graph Matching.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
pages = {2684--2693},
title = {{Deep Learning of Graph Matching}},
year = {2018}
}
@incollection{Lee2019a,
abstract = {Image registration with deep neural networks has become an active field of research and exciting avenue for a long standing problem in medical imaging. The goal is to learn a complex function that maps the appearance of input image pairs to parameters of a spatial transformation in order to align corresponding anatomical structures. We argue and show that the current direct, non-iterative approaches are sub-optimal, in particular if we seek accurate alignment of Structures-of-Interest (SoI). Information about SoI is often available at training time, for example, in form of segmentations or landmarks. We introduce a novel, generic framework, Image-and-Spatial Transformer Networks (ISTNs), to leverage SoI information allowing us to learn new image representations that are optimised for the downstream registration task. Thanks to these representations we can employ a test-specific, iterative refinement over the transformation parameters which yields highly accurate registration even with very limited training data. Performance is demonstrated on pairwise 3D brain registration and illustrative synthetic data.},
annote = {Supervised

Use an intermediate 'Image Transformer Module' to augment images and hightlight structures of interest for the registration.

Make a good point for the power of feature based similarity metrics with synthetic example.

Requires annotations of the structures of interest.},
archivePrefix = {arXiv},
arxivId = {1907.09200},
author = {Lee, Matthew C. H. and Oktay, Ozan and Schuh, Andreas and Schaap, Michiel and Glocker, Ben},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_38},
eprint = {1907.09200},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2019 - Image-and-Spatial Transformer Networks for Structure-Guided Image Registration(2).pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {337--345},
title = {{Image-and-Spatial Transformer Networks for Structure-Guided Image Registration}},
year = {2019}
}
@article{dalca2018unsuperiveddiff,
abstract = {Traditional deformable registration techniques achieve impressive results and offer a rigorous theoretical treatment, but are computationally intensive since they solve an optimization problem for each image pair. Recently, learning-based methods have facilitated fast registration by learning spatial deformation functions. However, these approaches use restricted deformation models, require supervised labels, or do not guarantee a diffeomorphic (topology-preserving) registration. Furthermore, learning-based registration tools have not been derived from a probabilistic framework that can offer uncertainty estimates. In this paper, we present a probabilistic generative model and derive an unsupervised learning-based inference algorithm that makes use of recent developments in convolutional neural networks (CNNs). We demonstrate our method on a 3D brain registration task, and provide an empirical analysis of the algorithm. Our approach results in state of the art accuracy and very fast runtimes, while providing diffeomorphic guarantees and uncertainty estimates. Our implementation is available online at http://voxelmorph.csail.mit.edu .},
archivePrefix = {arXiv},
arxivId = {1805.04605},
author = {Dalca, Adrian V. and Balakrishnan, Guha and Guttag, John and Sabuncu, Mert R.},
doi = {10.1007/978-3-030-00928-1_82},
eprint = {1805.04605},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalca et al. - 2018 - Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration.pdf:pdf},
journal = {Medical Image Computing and Computer Assisted Intervention},
month = {may},
pages = {729--738},
title = {{Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration}},
url = {http://arxiv.org/abs/1805.04605 http://dx.doi.org/10.1007/978-3-030-00928-1{\_}82},
year = {2018}
}
@article{Brats2015,
abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74{\%}-85{\%}), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
annote = {Brats 1/3},
author = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc Andr{\'{e}} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'{e}} and Demiralp, {\c{C}}ağatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'{e}} Ant{\'{o}}nio and Meier, Raphael and Pereira, S{\'{e}}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M.S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and {Van Leemput}, Koen},
doi = {10.1109/TMI.2014.2377694},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Menze et al. - 2015 - The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS).pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Benchmark,Brain,Image segmentation,MRI,Oncology/tumor},
month = {oct},
number = {10},
pages = {1993--2024},
pmid = {25494501},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)}},
url = {https://pubmed.ncbi.nlm.nih.gov/25494501/},
volume = {34},
year = {2015}
}
@article{Lavoie2012,
author = {Lavoie, Caroline and Higgins, Jane and Bissonnette, Jean Pierre and Le, Lisa W. and Sun, Alexander and Brade, Anthony and Hope, Andrew and Cho, John and Bezjak, Andrea},
doi = {10.1016/j.ijrobp.2012.02.012},
issn = {03603016},
journal = {International Journal of Radiation Oncology Biology Physics},
month = {dec},
number = {5},
pages = {1086--1092},
title = {{Volumetric image guidance using carina vs spine as registration landmarks for conventionally fractionated lung radiotherapy}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22494582},
volume = {84},
year = {2012}
}
@inproceedings{Skilling2015,
abstract = {Information H is a unique relationship between probabilities, based on the property of independence which is central to scientific methodology. Information Geometry makes the tempting but fallacious assumption that a local metric (conventionally based on information) can be used to endow the space of probability distributions with a preferred global Riemannian metric. No such global metric can conform to H, which is "from-to" asymmetric whereas geometrical length is by definition symmetric. Accordingly, any Riemannian metric will contradict the required structure of the very distributions which are supposedly being triangulated. It follows that probabilities do not form a metric space. We give counter-examples in which alternative formulations of information, and the use of information geometry, lead to unacceptable results.},
annote = {tldr: use KL-divergence, all other metrics have problems},
author = {Skilling, John},
booktitle = {AIP Conference Proceedings},
doi = {10.1063/1.4905961},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Skilling - 2015 - Failures of information geometry.pdf:pdf},
isbn = {9780735412804},
issn = {15517616},
keywords = {Information geometry,metric space,probability distribution},
month = {feb},
number = {1},
pages = {27--42},
publisher = {American Institute of Physics Inc.},
title = {{Failures of information geometry}},
url = {http://aip.scitation.org/doi/abs/10.1063/1.4905961},
volume = {1641},
year = {2015}
}
@techreport{Hu,
author = {Hu, Xiaojun and Kang, Miao and Huang, Weilin and Scott, Matthew R and Wiest, Roland and Reyes, Mauricio},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - Unknown - Dual-Stream Pyramid Registration Network Supplementary Material.pdf:pdf},
title = {{Dual-Stream Pyramid Registration Network Supplementary Material}}
}
@inproceedings{Rezende2015,
abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, fo-cusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinites-imal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
booktitle = {International conference on machine learning},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rezende, Mohamed - 2015 - Variational Inference with Normalizing Flows.pdf:pdf},
issn = {1938-7228},
month = {jun},
pages = {1530--1538},
publisher = {PMLR},
title = {{Variational Inference with Normalizing Flows}},
url = {http://proceedings.mlr.press/v37/rezende15.html},
year = {2015}
}
@inproceedings{Hauberg2016,
abstract = {Data augmentation is a key element in training high-dimensional models. In this approach , one synthesizes new observations by applying pre-specified transformations to the original training data; e.g. new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Rieman-nian submanifold of the Lie group of dif-feomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.},
author = {Hauberg, S{\o}ren and Freifeld, Oren and Boesen, Anders and Larsen, Lindbo and Fisher, John W and Lars, Iii and Hansen, Kai},
booktitle = {Artificial Intelligence and Statistics},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hauberg et al. - 2016 - Dreaming More Data Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation.pdf:pdf},
issn = {1938-7228},
month = {may},
pages = {342--350},
publisher = {PMLR},
title = {{Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation}},
url = {http://www2.compute.dtu.dk/},
volume = {41},
year = {2016}
}
@inproceedings{Krebs2017,
abstract = {Robust image registration in medical imaging is essential for comparison or fusion of images, acquired from various perspectives, modalities or at different times. Typically, an objective function needs to be minimized assuming specific a priori deformation models and predefined or learned similarity measures. However, these approaches have difficulties to cope with large deformations or a large variability in appearance. Using modern deep learning (DL) methods with automated feature design, these limitations could be resolved by learning the intrinsic mapping solely from experience. We investigate in this paper how DL could help organ-specific (ROI-specific) deformable registration, to solve motion compensation or atlas-based segmentation problems for instance in prostate diagnosis. An artificial agent is trained to solve the task of non-rigid registration by exploring the parametric space of a statistical deformation model built from training data. Since it is difficult to extract trustworthy ground-truth deformation fields, we present a training scheme with a large number of synthetically deformed image pairs requiring only a small number of real inter-subject pairs. Our approach was tested on inter-subject registration of prostate MR data and reached a median DICE score of.88 in 2-D and.76 in 3-D, therefore showing improved results compared to state-of-the-art registration algorithms.},
annote = {Reinforcement learning-based approach.

Trained on synthetic ground truth deformations of real images.

Does not require a similarity metric, as trained on gt transformations.},
author = {Krebs, Julian and Mansi, Tommaso and Delingette, Herv{\'{e}} and Zhang, Li and Ghesu, Florin C. and Miao, Shun and Maier, Andreas K. and Ayache, Nicholas and Liao, Rui and Kamen, Ali},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-66182-7_40},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krebs et al. - 2017 - Robust non-rigid registration through agent-based action learning.pdf:pdf},
isbn = {9783319661810},
issn = {16113349},
month = {sep},
pages = {344--352},
publisher = {Springer Verlag},
title = {{Robust non-rigid registration through agent-based action learning}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-66182-7{\_}40},
volume = {10433},
year = {2017}
}
@incollection{Xu2019,
abstract = {Deep convolutional neural networks (CNNs) are state-of-the-art for semantic image segmentation, but typically require many labeled training samples. Obtaining 3D segmentations of medical images for supervised training is difficult and labor intensive. Motivated by classical approaches for joint segmentation and registration we therefore propose a deep learning framework that jointly learns networks for image registration and image segmentation. In contrast to previous work on deep unsupervised image registration, which showed the benefit of weak supervision via image segmentations, our approach can use existing segmentations when available and computes them via the segmentation network otherwise, thereby providing the same registration benefit. Conversely, segmentation network training benefits from the registration, which essentially provides a realistic form of data augmentation. Experiments on knee and brain 3D magnetic resonance (MR) images show that our approach achieves large simultaneous improvements of segmentation and registration accuracy (over independently trained networks) and allows training high-quality models with very limited training data. Specifically, in a one-shot-scenario (with only one manually labeled image) our approach increases Dice scores ({\%}) over an unsupervised registration network by 2.7 and 1.8 on the knee and brain images respectively.},
annote = {supervised

Train a segmentation and registration network in tandem, based on supervised segmentation data.

weakly supervised setting.

The two networks complement each other:
-Registration of images is a good data augmentation step for image segmentation tasks
- The segmentation network can provide segmentations for weak supervision of the registration network, if no ground truth segmentation is available},
archivePrefix = {arXiv},
arxivId = {1904.08465},
author = {Xu, Zhenlin and Niethammer, Marc},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_47},
eprint = {1904.08465},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Niethammer - 2019 - DeepAtlas Joint Semi-supervised Learning of Image Registration and Segmentation.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {420--429},
title = {{DeepAtlas: Joint Semi-supervised Learning of Image Registration and Segmentation}},
year = {2019}
}
@article{Pace2013a,
abstract = {We propose a deformable image registration algorithm that uses anisotropic smoothing for regularization to find correspondences between images of sliding organs. In particular, we apply the method for respiratory motion estimation in longitudinal thoracic and abdominal computed tomography scans. The algorithm uses locally adaptive diffusion tensors to determine the direction and magnitude with which to smooth the components of the displacement field that are normal and tangential to an expected sliding boundary. Validation was performed using synthetic, phantom, and 14 clinical datasets, including the publicly available DIR-Lab dataset. We show that motion discontinuities caused by sliding can be effectively recovered, unlike conventional regularizations that enforce globally smooth motion. In the clinical datasets, target registration error showed improved accuracy for lung landmarks compared to the diffusive regularization. We also present a generalization of our algorithm to other sliding geometries, including sliding tubes (e.g., needles sliding through tissue, or contrast agent flowing through a vessel). Potential clinical applications of this method include longitudinal change detection and radiotherapy for lung or abdominal tumours, especially those near the chest or abdominal wall. {\textcopyright} 2013 IEEE.},
author = {Pace, Danielle F. and Aylward, Stephen R. and Niethammer, Marc},
doi = {10.1109/TMI.2013.2274777},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pace, Aylward, Niethammer - 2013 - A locally adaptive regularization based on anisotropic diffusion for deformable image registration(2).pdf:pdf},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Abdominal computed tomography (CT),deformable image registration,locally adaptive regularization,respiratory motion,sliding motion,thoracic CT},
number = {11},
pages = {2114--2126},
title = {{A locally adaptive regularization based on anisotropic diffusion for deformable image registration of sliding organs}},
volume = {32},
year = {2013}
}
@article{Brats2018,
abstract = {Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.},
annote = {Brats 3/3},
archivePrefix = {arXiv},
arxivId = {1811.02629},
author = {Bakas, Spyridon and Reyes, Mauricio and Jakab, Andras and Bauer, Stefan and Rempfler, Markus and Crimi, Alessandro and Shinohara, Russell Takeshi and Berger, Christoph and Ha, Sung Min and Rozycki, Martin and Prastawa, Marcel and Alberts, Esther and Lipkova, Jana and Freymann, John and Kirby, Justin and Bilello, Michel and Fathallah-Shaykh, Hassan and Wiest, Roland and Kirschke, Jan and Wiestler, Benedikt and Colen, Rivka and Kotrotsou, Aikaterini and Lamontagne, Pamela and Marcus, Daniel and Milchenko, Mikhail and Nazeri, Arash and Weber, Marc-Andre and Mahajan, Abhishek and Baid, Ujjwal and Gerstner, Elizabeth and Kwon, Dongjin and Acharya, Gagan and Agarwal, Manu and Alam, Mahbubul and Albiol, Alberto and Albiol, Antonio and Albiol, Francisco J. and Alex, Varghese and Allinson, Nigel and Amorim, Pedro H. A. and Amrutkar, Abhijit and Anand, Ganesh and Andermatt, Simon and Arbel, Tal and Arbelaez, Pablo and Avery, Aaron and Azmat, Muneeza and B., Pranjal and Bai, W and Banerjee, Subhashis and Barth, Bill and Batchelder, Thomas and Batmanghelich, Kayhan and Battistella, Enzo and Beers, Andrew and Belyaev, Mikhail and Bendszus, Martin and Benson, Eze and Bernal, Jose and Bharath, Halandur Nagaraja and Biros, George and Bisdas, Sotirios and Brown, James and Cabezas, Mariano and Cao, Shilei and Cardoso, Jorge M. and Carver, Eric N and Casamitjana, Adri{\`{a}} and Castillo, Laura Silvana and Cat{\`{a}}, Marcel and Cattin, Philippe and Cerigues, Albert and Chagas, Vinicius S. and Chandra, Siddhartha and Chang, Yi-Ju and Chang, Shiyu and Chang, Ken and Chazalon, Joseph and Chen, Shengcong and Chen, Wei and Chen, Jefferson W and Chen, Zhaolin and Cheng, Kun and Choudhury, Ahana Roy and Chylla, Roger and Cl{\'{e}}rigues, Albert and Colleman, Steven and Colmeiro, Ramiro German Rodriguez and Combalia, Marc and Costa, Anthony and Cui, Xiaomeng and Dai, Zhenzhen and Dai, Lutao and Daza, Laura Alexandra and Deutsch, Eric and Ding, Changxing and Dong, Chao and Dong, Shidu and Dudzik, Wojciech and Eaton-Rosen, Zach and Egan, Gary and Escudero, Guilherme and Estienne, Th{\'{e}}o and Everson, Richard and Fabrizio, Jonathan and Fan, Yong and Fang, Longwei and Feng, Xue and Ferrante, Enzo and Fidon, Lucas and Fischer, Martin and French, Andrew P. and Fridman, Naomi and Fu, Huan and Fuentes, David and Gao, Yaozong and Gates, Evan and Gering, David and Gholami, Amir and Gierke, Willi and Glocker, Ben and Gong, Mingming and Gonz{\'{a}}lez-Vill{\'{a}}, Sandra and Grosges, T. and Guan, Yuanfang and Guo, Sheng and Gupta, Sudeep and Han, Woo-Sup and Han, Il Song and Harmuth, Konstantin and He, Huiguang and Hern{\'{a}}ndez-Sabat{\'{e}}, Aura and Herrmann, Evelyn and Himthani, Naveen and Hsu, Winston and Hsu, Cheyu and Hu, Xiaojun and Hu, Xiaobin and Hu, Yan and Hu, Yifan and Hua, Rui and Huang, Teng-Yi and Huang, Weilin and {Van Huffel}, Sabine and Huo, Quan and HV, Vivek and Iftekharuddin, Khan M. and Isensee, Fabian and Islam, Mobarakol and Jackson, Aaron S. and Jambawalikar, Sachin R. and Jesson, Andrew and Jian, Weijian and Jin, Peter and Jose, V Jeya Maria and Jungo, Alain and Kainz, B and Kamnitsas, Konstantinos and Kao, Po-Yu and Karnawat, Ayush and Kellermeier, Thomas and Kermi, Adel and Keutzer, Kurt and Khadir, Mohamed Tarek and Khened, Mahendra and Kickingereder, Philipp and Kim, Geena and King, Nik and Knapp, Haley and Knecht, Urspeter and Kohli, Lisa and Kong, Deren and Kong, Xiangmao and Koppers, Simon and Kori, Avinash and Krishnamurthi, Ganapathy and Krivov, Egor and Kumar, Piyush and Kushibar, Kaisar and Lachinov, Dmitrii and Lambrou, Tryphon and Lee, Joon and Lee, Chengen and Lee, Yuehchou and Lee, M and Lefkovits, Szidonia and Lefkovits, Laszlo and Levitt, James and Li, Tengfei and Li, Hongwei and Li, Wenqi and Li, Hongyang and Li, Xiaochuan and Li, Yuexiang and Li, Heng and Li, Zhenye and Li, Xiaoyu and Li, Zeju and Li, XiaoGang and Li, Wenqi and Lin, Zheng-Shen and Lin, Fengming and Lio, Pietro and Liu, Chang and Liu, Boqiang and Liu, Xiang and Liu, Mingyuan and Liu, Ju and Liu, Luyan and Llado, Xavier and Lopez, Marc Moreno and Lorenzo, Pablo Ribalta and Lu, Zhentai and Luo, Lin and Luo, Zhigang and Ma, Jun and Ma, Kai and Mackie, Thomas and Madabushi, Anant and Mahmoudi, Issam and Maier-Hein, Klaus H. and Maji, Pradipta and Mammen, CP and Mang, Andreas and Manjunath, B. S. and Marcinkiewicz, Michal and McDonagh, S and McKenna, Stephen and McKinley, Richard and Mehl, Miriam and Mehta, Sachin and Mehta, Raghav and Meier, Raphael and Meinel, Christoph and Merhof, Dorit and Meyer, Craig and Miller, Robert and Mitra, Sushmita and Moiyadi, Aliasgar and Molina-Garcia, David and Monteiro, Miguel A. B. and Mrukwa, Grzegorz and Myronenko, Andriy and Nalepa, Jakub and Ngo, Thuyen and Nie, Dong and Ning, Holly and Niu, Chen and Nuechterlein, Nicholas K and Oermann, Eric and Oliveira, Arlindo and Oliveira, Diego D. C. and Oliver, Arnau and Osman, Alexander F. I. and Ou, Yu-Nian and Ourselin, Sebastien and Paragios, Nikos and Park, Moo Sung and Paschke, Brad and Pauloski, J. Gregory and Pawar, Kamlesh and Pawlowski, Nick and Pei, Linmin and Peng, Suting and Pereira, Silvio M. and Perez-Beteta, Julian and Perez-Garcia, Victor M. and Pezold, Simon and Pham, Bao and Phophalia, Ashish and Piella, Gemma and Pillai, G. N. and Piraud, Marie and Pisov, Maxim and Popli, Anmol and Pound, Michael P. and Pourreza, Reza and Prasanna, Prateek and Prkovska, Vesna and Pridmore, Tony P. and Puch, Santi and Puybareau, {\'{E}}lodie and Qian, Buyue and Qiao, Xu and Rajchl, Martin and Rane, Swapnil and Rebsamen, Michael and Ren, Hongliang and Ren, Xuhua and Revanuru, Karthik and Rezaei, Mina and Rippel, Oliver and Rivera, Luis Carlos and Robert, Charlotte and Rosen, Bruce and Rueckert, Daniel and Safwan, Mohammed and Salem, Mostafa and Salvi, Joaquim and Sanchez, Irina and S{\'{a}}nchez, Irina and Santos, Heitor M. and Sartor, Emmett and Schellingerhout, Dawid and Scheufele, Klaudius and Scott, Matthew R. and Scussel, Artur A. and Sedlar, Sara and Serrano-Rubio, Juan Pablo and Shah, N. Jon and Shah, Nameetha and Shaikh, Mazhar and Shankar, B. Uma and Shboul, Zeina and Shen, Haipeng and Shen, Dinggang and Shen, Linlin and Shen, Haocheng and Shenoy, Varun and Shi, Feng and Shin, Hyung Eun and Shu, Hai and Sima, Diana and Sinclair, M and Smedby, Orjan and Snyder, James M. and Soltaninejad, Mohammadreza and Song, Guidong and Soni, Mehul and Stawiaski, Jean and Subramanian, Shashank and Sun, Li and Sun, Roger and Sun, Jiawei and Sun, Kay and Sun, Yu and Sun, Guoxia and Sun, Shuang and Suter, Yannick R and Szilagyi, Laszlo and Talbar, Sanjay and Tao, Dacheng and Tao, Dacheng and Teng, Zhongzhao and Thakur, Siddhesh and Thakur, Meenakshi H and Tharakan, Sameer and Tiwari, Pallavi and Tochon, Guillaume and Tran, Tuan and Tsai, Yuhsiang M. and Tseng, Kuan-Lun and Tuan, Tran Anh and Turlapov, Vadim and Tustison, Nicholas and Vakalopoulou, Maria and Valverde, Sergi and Vanguri, Rami and Vasiliev, Evgeny and Ventura, Jonathan and Vera, Luis and Vercauteren, Tom and Verrastro, C. A. and Vidyaratne, Lasitha and Vilaplana, Veronica and Vivekanandan, Ajeet and Wang, Guotai and Wang, Qian and Wang, Chiatse J. and Wang, Weichung and Wang, Duo and Wang, Ruixuan and Wang, Yuanyuan and Wang, Chunliang and Wang, Guotai and Wen, Ning and Wen, Xin and Weninger, Leon and Wick, Wolfgang and Wu, Shaocheng and Wu, Qiang and Wu, Yihong and Xia, Yong and Xu, Yanwu and Xu, Xiaowen and Xu, Peiyuan and Yang, Tsai-Ling and Yang, Xiaoping and Yang, Hao-Yu and Yang, Junlin and Yang, Haojin and Yang, Guang and Yao, Hongdou and Ye, Xujiong and Yin, Changchang and Young-Moxon, Brett and Yu, Jinhua and Yue, Xiangyu and Zhang, Songtao and Zhang, Angela and Zhang, Kun and Zhang, Xuejie and Zhang, Lichi and Zhang, Xiaoyue and Zhang, Yazhuo and Zhang, Lei and Zhang, Jianguo and Zhang, Xiang and Zhang, Tianhao and Zhao, Sicheng and Zhao, Yu and Zhao, Xiaomei and Zhao, Liang and Zheng, Yefeng and Zhong, Liming and Zhou, Chenhong and Zhou, Xiaobing and Zhou, Fan and Zhu, Hongtu and Zhu, Jin and Zhuge, Ying and Zong, Weiwei and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Davatzikos, Christos and van Leemput, Koen and Menze, Bjoern},
eprint = {1811.02629},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakas et al. - 2018 - Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall.pdf:pdf},
journal = {arXiv},
month = {nov},
title = {{Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge}},
url = {http://arxiv.org/abs/1811.02629},
volume = {124},
year = {2018}
}
@article{Popescu2021,
abstract = {We propose a parameter efficient Bayesian layer for hierarchical convolutional Gaussian Processes that incorporates Gaussian Processes operating in Wasserstein-2 space to reliably propagate uncertainty. This directly replaces convolving Gaussian Processes with a distance-preserving affine operator on distributions. Our experiments on brain tissue-segmentation show that the resulting architecture approaches the performance of well-established deterministic segmentation algorithms (U-Net), which has never been achieved with previous hierarchical Gaussian Processes. Moreover, by applying the same segmentation model to out-of-distribution data (i.e., images with pathology such as brain tumors), we show that our uncertainty estimates result in out-of-distribution detection that outperforms the capabilities of previous Bayesian networks and reconstruction-based approaches that learn normative distributions.},
annote = {Method:
- GP layer for CNN architectures
- estimates model uncertainty (epistemic)


Questions/Discussion:
- builds on a lot of GP and baeysian NN knowledge.
- unsure about Fig 2: within data uncertainty vs distributional uncertainty},
archivePrefix = {arXiv},
arxivId = {2104.13756},
author = {Popescu, Sebastian G. and Sharp, David J. and Cole, James H. and Kamnitsas, Konstantinos and Glocker, Ben},
eprint = {2104.13756},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Popescu et al. - 2021 - Distributional Gaussian Process Layers for Outlier Detection in Image Segmentation.pdf:pdf},
journal = {Information Processing in Medical Imaging},
month = {apr},
pages = {415--427},
title = {{Distributional Gaussian Process Layers for Outlier Detection in Image Segmentation}},
url = {http://arxiv.org/abs/2104.13756},
year = {2021}
}
@article{Liu2014,
abstract = {Low-rank image decomposition has the potential to address a broad range of challenges that routinely occur in clinical practice. Its novelty and utility in the context of atlas-based analysis stems from its ability to handle images containing large pathologies and large deformations. Potential applications include atlas-based tissue segmentation and unbiased atlas building from data containing pathologies. In this paper we present atlas-based tissue segmentation of MRI from patients with large pathologies. Specifically, a healthy brain atlas is registered with the low-rank components from the input MRIs, the low-rank components are then re-computed based on those registrations, and the process is then iteratively repeated. Preliminary evaluations are conducted using the brain tumor segmentation challenge data (BRATS '12). {\textcopyright} 2014 Springer International Publishing.},
annote = {Atlast-based segmentation of Brats.
An iterative, low-rank image registration framework. Tolerates the presence of large lesions during image registration.


Fig1 gives good overview:
Images are described as low-rank matrix components, with the aim of reducing the influence of pathologies. Low-rank images can then be registered. Low-rank approach benefits from position of the pathologies being different between images, so only the common "healthy" tissue is represented by the low-rank matrix.

shortcomings compared to our work:
- does not highlight pathologies, only ignores them
- can only be used to register groups of images

Problem very similar to our work, but framing completely different.},
author = {Liu, Xiaoxiao and Niethammer, Marc and Kwitt, Roland and McCormick, Matthew and Aylward, Stephen},
doi = {10.1007/978-3-319-10443-0_13},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2014 - Low-Rank to the Rescue – Atlas-Based Analyses in the Presence of Pathologies.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {97--104},
publisher = {Springer, Cham},
title = {{Low-Rank to the Rescue – Atlas-Based Analyses in the Presence of Pathologies}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-10443-0{\_}13},
volume = {8675 LNCS},
year = {2014}
}
@article{Avants2009,
author = {Avants, Brian B and Tustison, Nick and Song, Gang},
journal = {Insight j},
number = {365},
pages = {1--35},
title = {{Advanced normalization tools (ANTS)}},
volume = {2},
year = {2009}
}
@misc{Szekely2013,
abstract = {Energy distance is a statistical distance between the distributions of random vectors, which characterizes equality of distributions. The name energy derives from Newton's gravitational potential energy, and there is an elegant relation to the notion of potential energy between statistical observations. Energy statistics are functions of distances between statistical observations in metric spaces. Thus even if the observations are complex objects, like functions, one can use their real valued nonnegative distances for inference. Theory and application of energy statistics are discussed and illustrated. Finally, we explore the notion of potential and kinetic energy of goodness-of-fit. {\textcopyright} 2013 Elsevier B.V.},
author = {Sz{\'{e}}kely, G{\'{a}}bor J. and Rizzo, Maria L.},
booktitle = {Journal of Statistical Planning and Inference},
doi = {10.1016/j.jspi.2013.03.018},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sz{\'{e}}kely, Rizzo - 2013 - Energy statistics A class of statistics based on distances.pdf:pdf},
issn = {03783758},
keywords = {Distance correlation,Distance covariance,Energy distance,Goodness-of-fit,Multivariate independence},
month = {aug},
number = {8},
pages = {1249--1272},
publisher = {North-Holland},
title = {{Energy statistics: A class of statistics based on distances}},
volume = {143},
year = {2013}
}
@article{Sabuncu2009,
abstract = {A natural requirement in pairwise image registration is that the resulting deformation is independent of the order of the images. This constraint is typically achieved via a symmetric cost function and has been shown to reduce the effects of local optima. Consequently, symmetric registration has been successfully applied to pairwise image registration as well as the spatial alignment of individual images with a template. However, recent work has shown that the relationship between an image and a template is fundamentally asymmetric. In this paper, we develop a method that reconciles the practical advantages of symmetric registration with the asymmetric nature of image-template registration by adding a simple correction factor to the symmetric cost function. We instantiate our model within a log-domain diffeomorphic registration framework. Our experiments show exploiting the asymmetry in image-template registration improves alignment in the image coordinates.},
author = {Sabuncu, Mert R and Yeo, B T Thomas and {Van Leemput}, Koen and Vercauteren, Tom and Golland, Polina},
doi = {10.1007/978-3-642-04268-3_70},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sabuncu et al. - 2009 - Asymmetric image-template registration.pdf:pdf},
journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
number = {Pt 1},
pages = {565--73},
pmid = {20426033},
title = {{Asymmetric image-template registration.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20426033 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2860756},
volume = {12},
year = {2009}
}
@article{grenander1998computational,
author = {Grenander, Ulf and Miller, Michael I.},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grenander, Miller - 1998 - Computational Anatomy An Emerging Discipline.pdf:pdf},
journal = {Quarterly ofApplied Mathematics},
pages = {617-- 694},
title = {{Computational Anatomy: An Emerging Discipline}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.93.6634},
volume = {56},
year = {1998}
}
@article{Darkner2018,
abstract = {Diffeomorphic deformation is a popular choice in medical image registration. A fundamental property of diffeomorphisms is invertibility, implying that once the relation between two points A to B is found, then the relation B to A is given per definition. Consistency is a measure of a numerical algorithm's ability to mimic this invertibility, and achieving consistency has proven to be a challenge for many state-of-the-art algorithms. We present CDD (Collocation for Diffeomorphic Deformations), a numerical solution to diffeomorphic image registration, which solves for the Stationary Velocity Field (SVF) using an implicit A-stable collocation method. CDD guarantees the preservation of the diffeomorphic properties at all discrete points and is thereby consistent to machine precision. We compared CDD's collocation method with the following standard methods: Scaling and Squaring, Forward Euler, and Runge-Kutta 4, and found that CDD is up to 9 orders of magnitude more consistent. Finally, we evaluated CDD on a number of standard bench-mark data sets and compared the results with current state-of-the-art methods: SPM-DARTEL, Diffeomorphic Demons and SyN. We found that CDD outperforms state-of-the-art methods in consistency and delivers comparable or superior registration precision.},
annote = {proposes a consistent registration model CCD.

Consitent means transformation and its inverse are consistent, thus ||x - W(W-1(x))|| is approx 0. Consistency is required for a truly bijective mapping.

Uses transformation based on a Stationary Velocity Field (SVF)},
author = {Darkner, Sune and Pai, Akshay and Liptrot, Matthew G. and Sporring, Jon},
doi = {10.1109/TPAMI.2017.2730205},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Darkner et al. - 2018 - Collocation for Diffeomorphic Deformations in Medical Image Registration.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Registration,convergence and stability,image processing and computer vision,model validation and analysis,ordinary differential equation},
month = {jul},
number = {7},
pages = {1570--1583},
publisher = {IEEE Computer Society},
title = {{Collocation for Diffeomorphic Deformations in Medical Image Registration}},
volume = {40},
year = {2018}
}
@article{lamontagne2019oasis,
author = {LaMontagne, Pamela J and Benzinger, Tammie L S and Morris, John C and Others},
journal = {medRxiv},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{OASIS-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease}},
year = {2019}
}
@article{Kohl2018,
abstract = {Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.},
annote = {probabalistic segmentation

Concatinates the U-net output with a samples from a learned conditional prior (during testing) or a posterior inferred from supervised data (during training). The combines the sample and u-net output with 1x1 convolution layers.

+ very efficient training set-up
- no real mathematic reasoning of this approach},
archivePrefix = {arXiv},
arxivId = {1806.05034},
author = {Kohl, Simon A. A. and Romera-Paredes, Bernardino and Meyer, Clemens and {De Fauw}, Jeffrey and Ledsam, Joseph R. and Maier-Hein, Klaus H. and Eslami, S. M. Ali and Rezende, Danilo Jimenez and Ronneberger, Olaf},
eprint = {1806.05034},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohl et al. - 2018 - A Probabilistic U-Net for Segmentation of Ambiguous Images(2).pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {6965--6975},
publisher = {Neural information processing systems foundation},
title = {{A Probabilistic U-Net for Segmentation of Ambiguous Images}},
url = {http://arxiv.org/abs/1806.05034},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{hou2017deep,
author = {Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},
booktitle = {Winter Conference on Applications of Computer Vision},
organization = {IEEE},
pages = {1133--1141},
title = {{Deep feature consistent variational autoencoder}},
year = {2017}
}
@incollection{anandan1993hierarchical,
address = {Boston, MA},
author = {Anandan, P. and Bergen, J. R. and Hanna, K. J. and Hingorani, Rajesh},
booktitle = {Motion Analysis and Image Sequence Processing},
doi = {10.1007/978-1-4615-3236-1_1},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anandan et al. - 1993 - Hierarchical Model-Based Motion Estimation.pdf:pdf},
pages = {1--22},
publisher = {Springer US},
title = {{Hierarchical Model-Based Motion Estimation}},
url = {http://link.springer.com/10.1007/978-1-4615-3236-1{\_}1},
year = {1993}
}
@article{di2014autism,
author = {{Di Martino}, Adriana and Yan, Chao-Gan and Li, Qingyang and Others},
journal = {Molecular psychiatry},
number = {6},
pages = {659--667},
publisher = {Nature Publishing Group},
title = {{The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism}},
volume = {19},
year = {2014}
}
@inproceedings{Vos2020,
abstract = {Current unsupervised deep learning-based image registration methods are trained with mean squares or nor- malized cross correlation as a similarity metric. These metrics are suitable for registration of images where a linear relation between image intensities exists. When such a relation is absent knowledge from conventional image registration literature suggests the use of mutual information. In this work we investigate whether mutual information can be used as a loss for unsupervised deep learning image registration by evaluating it on two datasets: breast dynamic contrast-enhanced MR and cardiac MR images. The results show that training with mutual information as a loss gives on par performance compared with conventional image registration in contrast enhanced images, and the results show that it is generally applicable since it has on par performance compared with normalized cross correlation in single-modality registration.},
annote = {MSE and NCC are suitable if linear relation between image intensities exists. In the absence of such a relation, Mutual information is a frequent choice in image registration. It models a probabilistic relation between voxel intensities.

NMI(I, J) = $\backslash$frac{\{}H(I) + H(J){\}}{\{}I(I, J){\}}
with marginal entropy H and mutual information I. To calculate the joint and marginal entropies, the intensity distributions of both images are often approximated by histrograms, making them non-differentiable. $\backslash$citet{\{}Vox2020{\}} suggests instead using parzen-windows with gaussian kernels to approximate the distributions to allow for a differentiable implementation for use in deep learning frameworks.},
author = {de Vos, Bob D. and van der Velden, Bas H. M. and Sander, J{\"{o}}rg and Gilhuijs, Kenneth G. A. and Staring, Marius and I{\v{s}}gum, Ivana},
booktitle = {Medical Imaging 2020: Image Processing},
doi = {10.1117/12.2549729},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vos et al. - 2020 - Mutual information for unsupervised deep learning image registration.pdf:pdf},
keywords = {deep learning,image registration,mutual information,unsupervised machine learning},
month = {mar},
number = {10},
pages = {113130R},
publisher = {International Society for Optics and Photonics},
title = {{Mutual information for unsupervised deep learning image registration}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11313/113130R/Mutual-information-for-unsupervised-deep-learning-image-registration/10.1117/12.2549729.full https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11313/113130R/Mu},
volume = {11313},
year = {2020}
}
@article{Ulman2017,
author = {Ulman, Vladim{\'{i}}r and Ma{\v{s}}ka, Martin and Magnusson, Klas E.G. and Ronneberger, Olaf and Haubold, Carsten and Harder, Nathalie and Matula, Pavel and Matula, Petr and Svoboda, David and Radojevic, Miroslav and Smal, Ihor and Rohr, Karl and Jald{\'{e}}n, Joakim and Blau, Helen M. and Dzyubachyk, Oleh and Lelieveldt, Boudewijn and Xiao, Pengdong and Li, Yuexiang and Cho, Siu Yeung and Dufour, Alexandre C. and Olivo-Marin, Jean Christophe and Reyes-Aldasoro, Constantino C. and Solis-Lemus, Jose A. and Bensch, Robert and Brox, Thomas and Stegmaier, Johannes and Mikut, Ralf and Wolf, Steffen and Hamprecht, Fred A. and Esteves, Tiago and Quelhas, Pedro and Demirel, {\"{O}}mer and Malmstr{\"{o}}m, Lars and Jug, Florian and Tomancak, Pavel and Meijering, Erik and Mu{\~{n}}oz-Barrutia, Arrate and Kozubek, Michal and Ortiz-De-Solorzano, Carlos},
doi = {10.1038/nmeth.4473},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ulman et al. - 2017 - An objective comparison of cell-tracking algorithms.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
keywords = {Cell migration,Image processing},
month = {dec},
number = {12},
pages = {1141--1152},
pmid = {29083403},
publisher = {Nature Publishing Group},
title = {{An objective comparison of cell-tracking algorithms}},
url = {http://celltrackingchallenge.net/},
volume = {14},
year = {2017}
}
@inproceedings{Simonovsky2016,
abstract = {Multimodal registration is a challenging problem due the high variability of tissue appearance under different imaging modalities. The crucial component here is the choice of the right similarity measure. We make a step towards a general learning-based solution that can be adapted to specific situations and present a metric based on a convolutional neural network. Our network can be trained from scratch even from a few aligned image pairs. The metric is validated on intersubject deformable registration on a dataset different from the one used for training,demonstrating good generalization. In this task,we outperform mutual information by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1609.05396},
author = {Simonovsky, Martin and Guti{\'{e}}rrez-Becker, Benjam{\'{i}}n and Mateus, Diana and Navab, Nassir and Komodakis, Nikos},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-46726-9_2},
eprint = {1609.05396},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonovsky et al. - 2016 - A deep metric for multimodal registration.pdf:pdf},
isbn = {9783319467252},
issn = {16113349},
month = {oct},
pages = {10--18},
publisher = {Springer Verlag},
title = {{A deep metric for multimodal registration}},
volume = {9902 LNCS},
year = {2016}
}
@article{Castillo2009,
abstract = {Expert landmark correspondences are widely reported for evaluating deformable image registration (DIR) spatial accuracy. In this report, we present a framework for objective evaluation of DIR spatial accuracy using large sets of expert-determined landmark point pairs. Large samples (over 1100) of pulmonary landmark point pairs were manually generated for five cases. Estimates of inter- and intra-observer variation were determined from repeated registration. Comparative evaluation of DIR spatial accuracy was performed for two algorithms, a gradient-based optical flow algorithm and a landmark-based moving least-squares algorithm. The uncertainty of spatial error estimates was found to be inversely proportional to the square root of the number of landmark point pairs and directly proportional to the standard deviation of the spatial errors. Using the statistical properties of this data, we performed sample size calculations to estimate the average spatial accuracy of each algorithm with 95pct confidence intervals within a 0.5 mm range. For the optical flow and moving least-squares algorithms, the required sample sizes were 1050 and 36, respectively. Comparative evaluation based on fewer than the required validation landmarks results in misrepresentation of the relative spatial accuracy. This study demonstrates that landmark pairs can be used to assess DIR spatial accuracy within a narrow uncertainty range. 2009 Institute of Physics and Engineering in Medicine.},
author = {Castillo, Richard and Castillo, Edward and Guerra, Rudy and Johnson, Valen E. and McPhail, Travis and Garg, Amit K. and Guerrero, Thomas},
doi = {10.1088/0031-9155/54/7/001},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Castillo et al. - 2009 - A framework for evaluation of deformable image registration spatial accuracy using large landmark point sets.pdf:pdf},
issn = {00319155},
journal = {Physics in Medicine and Biology},
number = {7},
pages = {1849--1870},
title = {{A framework for evaluation of deformable image registration spatial accuracy using large landmark point sets}},
volume = {54},
year = {2009}
}
@article{Hua2017,
abstract = {Image registration is an essential technique to obtain point correspondences between anatomical structures from different images. Conventional non-rigid registration methods assume a continuous and smooth deformation field throughout the image. However, the deformation field at the interface of different organs is not necessarily continuous, since the organs may slide over or separate from each other. Therefore, imposing continuity and smoothness ubiquitously would lead to artifacts and increased errors near the discontinuity interface. In computational mechanics, the eXtended Finite Element Method (XFEM) was introduced to handle discontinuities without using computational meshes that conform to the discontinuity geometry. Instead, the interpolation bases themselves were enriched with discontinuous functional terms. Borrowing this concept, we propose a multiresolution eXtented Free-Form Deformation (XFFD) framework that seamlessly integrates within and extends the standard Free-Form Deformation (FFD) approach. Discontinuities are incorporated by enriching the B-spline basis functions coupled with extra degrees of freedom, which are only introduced near the discontinuity interface. In contrast with most previous methods, restricted to sliding motion, no ad hoc penalties or constraints are introduced to reduce gaps and overlaps. This allows XFFD to describe more general discontinuous motions. In addition, we integrate XFFD into a rigorously formulated multiresolution framework by introducing an exact parameter upsampling method. The proposed method has been evaluated in two publicly available datasets: 4D pulmonary CT images from the DIR-Lab dataset and 4D CT liver datasets. The XFFD achieved a Target Registration Error (TRE) of 1.17 ± 0.85 mm in the DIR-lab dataset and 1.94 ± 1.01 mm in the liver dataset, which significantly improves on the performance of the state-of-the-art methods handling discontinuities.},
author = {Hua, Rui and Pozo, Jose M. and Taylor, Zeike A. and Frangi, Alejandro F.},
doi = {10.1016/j.media.2016.10.008},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hua et al. - 2017 - Multiresolution eXtended Free-Form Deformations (XFFD) for non-rigid registration with discontinuous transforms.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Discontinuity,FFD,Non-rigid registration,Sliding motion},
month = {feb},
pages = {113--122},
publisher = {Elsevier B.V.},
title = {{Multiresolution eXtended Free-Form Deformations (XFFD) for non-rigid registration with discontinuous transforms}},
volume = {36},
year = {2017}
}
@article{Long2015,
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2015 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
journal = {CVPR},
pages = {3431--3440},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2015}
}
@incollection{Luo2019,
annote = {Introduces distinction between transformation uncertainty (the transformation field is unclear), and label uncertainty (labels from atlas, under an uncertain transformation). In larger regions of the brain, it it possible to have high transformation uncertainty, but low label uncertainty (Fig 4, Vc). On edges of high contrast (eg: vertices, tumors), it is also possible to have low transformation uncertainty, but high label uncertainty (Fig 4, Ve).

aplication example of uncertainty measurment in brain registration. Pre-marked tumor gets registered to patient during surgery.

Detailed discussion of uncertainty in registration, and how it can be mis-interpreted},
author = {Luo, Jie and Sedghi, Alireza and Popuri, Karteek and Cobzas, Dana and Zhang, Miaomiao and Preiswerk, Frank and Toews, Matthew and Golby, Alexandra and Sugiyama, Masashi and Wells, William M. and Frisken, Sarah},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_46},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo et al. - 2019 - On the Applicability of Registration Uncertainty.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {410--419},
title = {{On the Applicability of Registration Uncertainty}},
year = {2019}
}
@article{Pang2020,
abstract = {Video anomaly detection is of critical practical importance to a variety of real applications because it allows human attention to be focused on events that are likely to be of interest, in spite of an otherwise overwhelming volume of video. We show that applying self-trained deep ordinal regression to video anomaly detection overcomes two key limitations of existing methods, namely, 1) being highly dependent on manually labeled normal training data; and 2) sub-optimal feature learning. By formulating a surrogate two-class ordinal regression task we devise an end-to-end trainable video anomaly detection approach that enables joint representation learning and anomaly scoring without manually labeled normal/abnormal data. Experiments on eight real-world video scenes show that our proposed method outperforms state-of-the-art methods that require no labeled training data by a substantial margin, and enables easy and accurate localization of the identified anomalies. Furthermore, we demonstrate that our method offers effective human-in-the-loop anomaly detection which can be critical in applications where anomalies are rare and the false-negative cost is high.},
archivePrefix = {arXiv},
arxivId = {2003.06780},
author = {Pang, Guansong and Yan, Cheng and Shen, Chunhua and van den Hengel, Anton and Bai, Xiao},
eprint = {2003.06780},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pang et al. - 2020 - Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection.pdf:pdf},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {mar},
pages = {12170--12179},
publisher = {IEEE Computer Society},
title = {{Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection}},
url = {http://arxiv.org/abs/2003.06780},
year = {2020}
}
@article{Avants2008a,
abstract = {Rationale and Objectives: Diffusion tensor (DT) and T1 structural magnetic resonance images provide unique and complementary tools for quantifying the living brain. We leverage both modalities in a diffeomorphic normalization method that unifies analysis of clinical datasets in a consistent and inherently multivariate (MV) statistical framework. We use this technique to study MV effects of traumatic brain injury (TBI). Materials and Methods: We contrast T1 and DT image-based measurements in the thalamus and hippocampus of 12 TBI survivors and nine matched controls normalized to a combined DT and T1 template space. The normalization method uses maps that are topology-preserving and unbiased. Normalization is based on the full tensor of information at each voxel and, simultaneously, the similarity between high-resolution features derived from T1 data. The technique is termed symmetric normalization for MV neuroanatomy (SyNMN). Voxel-wise MV statistics on the local volume and mean diffusion are assessed with Hotelling's T2 test with correction for multiple comparisons. Results: TBI significantly (false discovery rate P {\textless} .05) reduces volume and increases mean diffusion at coincident locations in the mediodorsal thalamus and anterior hippocampus. Conclusions: SyNMN reveals evidence that TBI compromises the limbic system. This TBI morphometry study and an additional performance evaluation contrasting SyNMN with other methods suggest that the DT component may aid normalization quality. {\textcopyright} 2008 AUR.},
annote = {combination of loss function},
author = {Avants, Brian and Duda, Jeffrey T and Kim, Junghoon and Zhang, Hui and Pluta, John and Gee, James C and Whyte, John},
doi = {10.1016/J.ACRA.2008.07.007},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Avants et al. - 2008 - Multivariate analysis of structural and diffusion imaging in traumatic brain injury.pdf:pdf},
issn = {1878-4046},
journal = {Academic radiology},
keywords = {Adult,Brain / pathology*,Brain Injuries / diagnosis*,Brian Avants,Cohort Studies,Computer-Assisted / methods,Diffusion Magnetic Resonance Imaging / methods*,Echo-Planar Imaging / methods,Extramural,Female,Hippocampus / pathology,Humans,Image Processing,Jeffrey T Duda,John Whyte,MEDLINE,Male,Middle Aged,Multivariate Analysis,N.I.H.,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Non-P.H.S.,Non-U.S. Gov't,PMC6372292,PubMed Abstract,Research Support,Thalamus / pathology,U.S. Gov't,doi:10.1016/j.acra.2008.07.007,pmid:18995188},
month = {nov},
number = {11},
pages = {1360--1375},
pmid = {18995188},
publisher = {Acad Radiol},
title = {{Multivariate analysis of structural and diffusion imaging in traumatic brain injury}},
url = {https://pubmed.ncbi.nlm.nih.gov/18995188/},
volume = {15},
year = {2008}
}
@article{Czolbe2021b,
abstract = {We propose a semantic similarity metric for image registration. Existing
metrics like Euclidean Distance or Normalized Cross-Correlation focus on
aligning intensity values, giving difficulties with low intensity contrast or
noise. Our approach learns dataset-specific features that drive the
optimization of a learning-based registration model. We train both an
unsupervised approach using an auto-encoder, and a semi-supervised approach
using supplemental segmentation data to extract semantic features for image
registration. Comparing to existing methods across multiple image modalities
and applications, we achieve consistently high registration accuracy. A learned
invariance to noise gives smoother transformations on low-quality images.},
archivePrefix = {arXiv},
arxivId = {2104.10051},
author = {Czolbe, Steffen and Krause, Oswin and Feragen, Aasa},
eprint = {2104.10051},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czolbe, Krause, Feragen - 2021 - Semantic similarity metrics for learned image registration.pdf:pdf},
journal = {Proceedings of Machine Learning Research},
keywords = {Deep Learning,Image Registration,Representation Learning},
month = {apr},
title = {{Semantic similarity metrics for learned image registration}},
url = {https://arxiv.org/abs/2104.10051v1},
year = {2021}
}
@inproceedings{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Gener-ative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, pres-ence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.},
annote = {Section 4+5 for backround reading on Variational inference + Mutual information

Very good explanation about the intuition of using Mutual information

GAN setting, latent is modelled ad a pair of variables c, z. C encodes MNIST label.
We want to ensure c is used by the generator G(c,z), and not just ignored. We motel a regularization: the mutual information I(c; G(c,z)) shoeld be high.},
archivePrefix = {arXiv},
arxivId = {1606.03657v1},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
eprint = {1606.03657v1},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets(2).pdf:pdf},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
year = {2016}
}
@inproceedings{Quay2018,
abstract = {Today, serial block-face scanning electron microscopy (SBF-SEM) is capable of producing teravoxel-scale 3D images of biological structures at nanometer-scale resolutions. Image segmentation is fundamental to data analysis workflows in biological electron microscopy (EM), but SBF-SEM datasets can greatly exceed the manual segmentation capacity of a laboratory. Fast automated segmentation algorithms would alleviate this problem, but practical solutions remain unavailable for many biological problems of interest. Segmentation algorithms using deep neural networks have recently demonstrated significant performance gains, but designing high-performing networks that effectively solve targeted problems remains challenging. We are developing genenet, a Python package to rapidly discover, train, and deploy high-performing neural network architectures for SBF-SEM segmentation with little user intervention. Here, we demonstrate how to use genenet to train an ensemble of segmentation networks for a human platelet tissue sample. Initial results indicate this approach is viable for accelerating the segmentation process.},
annote = {plant-em dataset},
author = {Quay, Matthew and Emam, Zeyad and Anderson, Adam and Leapman, Richard},
booktitle = {International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2018.8363603},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quay et al. - 2018 - Designing deep neural networks to automate segmentation for serial block-face electron microscopy.pdf:pdf},
isbn = {9781538636367},
issn = {19458452},
keywords = {Automated segmentation,Deep learning,Electron microscopy,Serial block-face imaging},
month = {may},
pages = {405--408},
publisher = {IEEE Computer Society},
title = {{Designing deep neural networks to automate segmentation for serial block-face electron microscopy}},
volume = {2018-April},
year = {2018}
}
@article{Nielsen2020,
annote = {Topology altering diffeomorphic registration.
Method in three steps:
1) manual finding of discontenueties
2) expansion of discontenueties: We insert a "hole" into the image, and push everything else to the side (but do not hide/remove it)
3) normal registration

Step 2 and 3 are optimized alternatingly. Free parameter in 1) is the size of the expansion, free parametert in 2) is the deformation



Transformation in two steps:
1) introduce and expand topological holes
a) discontinuous expansion in the deformation field
b) changing intensity within the resulting hole
2) diffeomorphic registration

For 1)
alpha channel. a=0 is hole. 0{\textless}a{\textless}1
during superimposition, holes are filled with a background image beta

Expansion of holes:},
author = {Nielsen, Rune Kok and Darkner, Sune and Ferangen, Aasa},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielsen, Darkner, Ferangen - 2020 - TopAwaRe Topology-Aware Registration (PaMi version).pdf:pdf},
journal = {PaMi},
pages = {1--11},
title = {{TopAwaRe : Topology-Aware Registration (PaMi version)}},
year = {2020}
}
@article{Jumper2021,
abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort 1-4 , the structures of around 100,000 unique proteins have been determined 5 , but this represents a small fraction of the billions of known protein sequences 6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence-the structure prediction component of the 'protein folding problem' 8-has been an important open research problem for more than 50 years 9. Despite recent progress 10-14 , existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14) 15 , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm. The development of computational methods to predict three-dimensional (3D) protein structures from the protein sequence has proceeded along two complementary paths that focus on either the physical interactions or the evolutionary history. The physical interaction programme heavily integrates our understanding of molecular driving forces into either thermodynamic or kinetic simulation of protein physics 16 or statistical approximations thereof 17. Although theoretically very appealing, this approach has proved highly challenging for even moderate-sized proteins due to the computational intractability of molecular simulation, the context dependence of protein stability and the difficulty of producing sufficiently accurate models of protein physics. The evolutionary programme has provided an alternative in recent years, in which the constraints on protein structure are derived from bioinformatics analysis of the evolutionary history of proteins, homology to solved structures 18,19 and pairwise evolutionary correlations 20-24. This bioinformatics approach has benefited greatly from the steady growth of experimental protein structures deposited in the Protein Data Bank (PDB) 5 , the explosion of genomic sequencing and the rapid development of deep learning techniques to interpret these correlations. Despite these advances, contemporary physical and evolutionary-history-based approaches produce predictions that are far short of experimental accuracy in the majority of cases in which a close homologue has not been solved experimentally and this has limited their utility for many biological applications. In this study, we develop the first, to our knowledge, computational approach capable of predicting protein structures to near experimental accuracy in a majority of cases. The neural network AlphaFold that we developed was entered into the CASP14 assessment (May-July 2020; entered under the team name 'AlphaFold2' and a completely different model from our CASP13 AlphaFold system 10). The CASP assessment is carried out biennially using recently solved structures that have not been deposited in the PDB or publicly disclosed so that it is a blind test https://doi.},
annote = {Interesting methods:

Recycling (multiple model passes), during BOTH training and test. See supplement page 41 for pseudo-code. Shared weights, gradient-stop beween passes.},
author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'{i}}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis and Hassabis, Demis ✉},
doi = {10.1038/s41586-021-03819-2},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf:pdf},
journal = {Nature},
pages = {583},
title = {{Highly accurate protein structure prediction with AlphaFold}},
url = {https://doi.org/10.1038/s41586-021-03819-2},
volume = {596},
year = {2021}
}
@article{Kwon2014,
abstract = {We propose a new method for deformable registration of pre-operative and post-recurrence brain MR scans of glioma patients. Performing this type of intra-subject registration is challenging as tumor, resection, recurrence, and edema cause large deformations, missing correspondences, and inconsistent intensity profiles between the scans. To address this challenging task, our method, called PORTR, explicitly accounts for pathological information. It segments tumor, resection cavity, and recurrence based on models specific to each scan. PORTR then uses the resulting maps to exclude pathological regions from the image-based correspondence term while simultaneously measuring the overlap between the aligned tumor and resection cavity. Embedded into a symmetric registration framework, we determine the optimal solution by taking advantage of both discrete and continuous search methods. We apply our method to scans of 24 glioma patients. Both quantitative and qualitative analysis of the results clearly show that our method is superior to other state-of-the-art approaches. {\textcopyright} 1982-2012 IEEE.},
annote = {Registration of tumors to cavities:
+ registration with anatomical mismatch
- does not support topological changes (insertion/removal), only mismatches
+ Uses different similarity metric depending on health/unhealth tissue
- uses a whole lot of domain specific knowledge
- requires manual point annotation of tumors

Summary:
- register to a probabalistic segmentation atlas
- manually segment tumors/cavities in the atlas:
a) place tumor as a point-annotation, expand with radius
b) expand edema even further around the tumor, masked by white-matter map
- Register the images, using NCC for health regions, L2 loss to match tumor and post-oprtative cavity.},
author = {Kwon, Dongjin and Niethammer, Marc and Akbari, Hamed and Bilello, Michel and Davatzikos, Christos and Pohl, Kilian M.},
doi = {10.1109/TMI.2013.2293478},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwon et al. - 2014 - PORTR Pre-operative and post-recurrence brain tumor registration.pdf:pdf},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Brain tumor magnetic resonance imaging (MRI),Deformable registration,Discrete-continuous optimization,Tumor growth model,Tumor segmentation},
number = {3},
pages = {651--667},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{PORTR: Pre-operative and post-recurrence brain tumor registration}},
volume = {33},
year = {2014}
}
@incollection{Liu2019,
abstract = {Brain image registration transforms a pair of images into one system with the matched imaging contents, which is of essential importance for brain image analysis. This paper presents a novel framework for unsupervised 3D brain image registration by capturing the feature-level transformation relationships between the unaligned image and reference image. To achieve this, we develop a feature-level probabilistic model to provide the direct regularization to the hidden layers of two deep convolutional neural networks, which are constructed from two input images. This model design is developed into multiple layers of these two networks to capture the transformation relationships at different levels. We employ two common benchmark datasets for 3D brain image registration and perform various experiments to evaluate our method. Experimental results show that our method clearly outperforms state-of-the-art methods on both benchmark datasets by a large margin.},
annote = {unsupervised, probabilistic

NN-based brain registration. Using a pyramid of probabilistic transformation maps (mean, std for each of them), then averaging them together.

Similar to Paper Dual-Stream Pyramid Registration network. Proposes a network architecture for same task.},
archivePrefix = {arXiv},
arxivId = {1907.01922},
author = {Liu, Lihao and Hu, Xiaowei and Zhu, Lei and Heng, Pheng-Ann},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_39},
eprint = {1907.01922},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2019 - Probabilistic Multilayer Regularization Network for Unsupervised 3D Brain Image Registration.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {346--354},
title = {{Probabilistic Multilayer Regularization Network for Unsupervised 3D Brain Image Registration}},
year = {2019}
}
@book{schmidt2014registration,
author = {Schmidt-Richberg, Alexander},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt-Richberg - 2014 - Registration methods for pulmonary image analysis integration of morphological and physiological knowledge.pdf:pdf},
isbn = {9783658016616},
keywords = {fissure alignment,lung,registration,segmentation,sliding motion},
publisher = {Springer Science $\backslash${\&} Business Media},
title = {{Registration methods for pulmonary image analysis: integration of morphological and physiological knowledge}},
year = {2014}
}
@techreport{thirion1998,
abstract = {In this paper, we present the concept of diffusing models to perform image-to-image matching. Having two images to match, the main idea is to consider the objects boundaries in one image as semi-permeable membranes and to let the other image, considered as a deformable grid model, diffuse through these interfaces, by the action of effectors situated within the membranes. We illustrate this concept by an analogy with Maxwell's demons. We show that this concept relates to more traditional ones, based on attraction, with an intermediate step being optical flow techniques. We use the concept of diffusing models to derive three different non-rigid matching algorithms, one using all the intensity levels in the static image, one using only contour points, and a last one operating on already segmented images. Finally, we present results with synthesized deformations and real medical images, with applications to heart motion tracking and three-dimensional inter-patients matching.},
author = {Thirion, Jean-Philippe},
booktitle = {Medical Image Analysis},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thirion - 1998 - Image matching as a diffusion process an analogy with Maxwell's demons.pdf:pdf},
keywords = {deformable model,elastic matching,image sequence analysis,inter-patient registration,non-rigid matching},
number = {3},
pages = {243--260},
publisher = {Elsevier},
title = {{Image matching as a diffusion process: an analogy with Maxwell's demons}},
url = {https://hal.inria.fr/inria-00615088},
volume = {2},
year = {1998}
}
@article{DiMartino2017,
abstract = {The second iteration of the Autism Brain Imaging Data Exchange (ABIDE II) aims to enhance the scope of brain connectomics research in Autism Spectrum Disorder (ASD). Consistent with the initial ABIDE effort (ABIDE I), that released 1112 datasets in 2012, this new multisite open-data resource is an aggregate of resting state functional magnetic resonance imaging (MRI) and corresponding structural MRI and phenotypic datasets. ABIDE II includes datasets from an additional 487 individuals with ASD and 557 controls previously collected across 16 international institutions. The combination of ABIDE I and ABIDE II provides investigators with 2156 unique cross-sectional datasets allowing selection of samples for discovery and/or replication. This sample size can also facilitate the identification of neurobiological subgroups, as well as preliminary examinations of sex differences in ASD. Additionally, ABIDE II includes a range of psychiatric variables to inform our understanding of the neural correlates of co-occurring psychopathology; 284 diffusion imaging datasets are also included. It is anticipated that these enhancements will contribute to unraveling key sources of ASD heterogeneity.},
author = {{Di Martino}, Adriana and O'Connor, David and Chen, Bosi and Alaerts, Kaat and Anderson, Jeffrey S. and Assaf, Michal and Balsters, Joshua H. and Baxter, Leslie and Beggiato, Anita and Bernaerts, Sylvie and Blanken, Laura M.E. and Bookheimer, Susan Y. and Braden, B. Blair and Byrge, Lisa and Castellanos, F. Xavier and Dapretto, Mirella and Delorme, Richard and Fair, Damien A. and Fishman, Inna and Fitzgerald, Jacqueline and Gallagher, Louise and Keehn, R. Joanne Jao and Kennedy, Daniel P. and Lainhart, Janet E. and Luna, Beatriz and Mostofsky, Stewart H. and M{\"{u}}ller, Ralph Axel and Nebel, Mary Beth and Nigg, Joel T. and O'Hearn, Kirsten and Solomon, Marjorie and Toro, Roberto and Vaidya, Chandan J. and Wenderoth, Nicole and White, Tonya and Craddock, R. Cameron and Lord, Catherine and Leventhal, Bennett and Milham, Michael P.},
doi = {10.1038/sdata.2017.10},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Di Martino et al. - 2017 - Enhancing studies of the connectome in autism using the autism brain imaging data exchange II.pdf:pdf},
issn = {20524463},
journal = {Scientific Data},
keywords = {Autism spectrum disorders,Brain imaging,Functional magnetic resonance imaging,Magnetic resonance imaging,Neuroscience},
month = {mar},
number = {1},
pages = {1--15},
pmid = {28291247},
publisher = {Nature Publishing Groups},
title = {{Enhancing studies of the connectome in autism using the autism brain imaging data exchange II}},
url = {www.nature.com/scientificdata},
volume = {4},
year = {2017}
}
@incollection{Alven2019,
annote = {NN, only affine and rigid transfomations. Unsupervised.

Affine alignment of images via NN.

Interesting idea: instead of predicting the whole transformation in on step, we use a composition of multiple smaller transformations. Each small transformation is preduced by the same NN, with shared weigths.

Application: spartial normalization of positron emission tomography (PET) images via a NN.

A PET scan does not provide sufficient anatomical information. Registration is required to align the PET scan to a coordinate system. This is often done with supplemental MR images. We learn a NN instead, which does not require MR images.},
author = {Alv{\'{e}}n, Jennifer and Heurling, Kerstin and Smith, Ruben and Strandberg, Olof and Sch{\"{o}}ll, Michael and Hansson, Oskar and Kahl, Fredrik},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_40},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alv{\'{e}}n et al. - 2019 - A Deep Learning Approach to MR-less Spatial Normalization for Tau PET Images.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {355--363},
title = {{A Deep Learning Approach to MR-less Spatial Normalization for Tau PET Images}},
year = {2019}
}
@article{Ulman2014,
author = {Ma{\v{s}}ka, Martin and Ulman, Vladim{\'{i}}r and Svoboda, David and Matula, Pavel and Matula, Petr and Ederra, Cristina and Urbiola, Ainhoa and Espa{\~{n}}a, Tom{\'{a}}s and Venkatesan, Subramanian and Balak, Deepak M W and Karas, Pavel and Bolckov{\'{a}}, Tereza and {\v{S}}treitov{\'{a}}, Mark{\'{e}}ta and Carthel, Craig and Coraluppi, Stefano and Harder, Nathalie and Rohr, Karl and Magnusson, Klas E G and Jald{\'{e}}n, Joakim and Blau, Helen M and Dzyubachyk, Oleh and Kr{\'{i}}{\v{z}}ek, Pavel and Hagen, Guy M and Pastor-Escuredo, David and Jimenez-Carretero, Daniel and Ledesma-Carbayo, Maria J and Mu{\~{n}}oz-Barrutia, Arrate and Meijering, Erik and Kozubek, Michal and Ortiz-de-Solorzano, Carlos},
doi = {10.1093/bioinformatics/btu080},
issn = {1367-4803},
journal = {Bioinformatics},
number = {11},
pages = {1609--1617},
title = {{A benchmark for comparison of cell tracking algorithms}},
url = {https://doi.org/10.1093/bioinformatics/btu080},
volume = {30},
year = {2014}
}
@inproceedings{Han2020,
abstract = {Registration of images with pathologies is challenging due to tissue appearance changes and missing correspondences caused by the pathologies. Moreover, mass effects as observed for brain tumors may displace tissue, creating larger deformations over time than what is observed in a healthy brain. Deep learning models have successfully been applied to image registration to offer dramatic speed up and to use sur-rogate information (e.g., segmentations) during training. However, existing approaches focus on learning registration models using images from healthy patients. They are therefore not designed for the registration of images with strong pathologies for example in the context of brain tumors, and traumatic brain injuries. In this work, we explore a deep learning approach to register images with brain tumors to an atlas. Our model learns an appearance mapping from images with tumors to the atlas, while simultaneously predicting the transformation to atlas space. Using separate decoders, the network disentangles the tumor mass effect from the reconstruction of quasi-normal images. Results on both synthetic and real brain tumor scans show that our approach outperforms cost function masking for registration to the atlas and that reconstructed quasi-normal images can be used for better longitudinal registrations.},
annote = {DL model to:
- reconstruct tumor appearence
- registering to atlas

Shortcomings / I could improve:
- less convoluted model design
- unsupervised, not requiering GT sgmentations
- prettyer evaluation (with pictures)
- show what happens when a tumor image is registered to the atlas without adjustment
- better blending of reconstruction to normal brain. Achieved with perceptual loss or impainting techniques


Questions:
- do not undertsand regularizer. Seems to be similar to lddmm. Or just fancy math notations},
archivePrefix = {arXiv},
arxivId = {2008.07628v1},
author = {Han, Xu and Shen, Zhengyang and Xu, Zhenlin and Bakas, Spyridon and Akbari, Hamed and Bilello, Michel and Davatzikos, Christos and Niethammer, Marc},
booktitle = {11th International Workshop on Machine Learning in Medical Imaging},
eprint = {2008.07628v1},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - Unknown - A Deep Network for Joint Registration and Reconstruction of Images with Pathologies.pdf:pdf},
pages = {342--352},
publisher = {Springer Nature},
title = {{A Deep Network for Joint Registration and Reconstruction of Images with Pathologies}},
year = {2020}
}
@inproceedings{hu2018adversarial,
abstract = {We describe an adversarial learning approach to constrain convolu-tional neural network training for image registration, replacing heuristic smoothness measures of displacement fields often used in these tasks. Using minimally-invasive prostate cancer intervention as an example application, we demonstrate the feasibility of utilizing biomechanical simulations to regularize a weakly-supervised anatomical-label-driven registration network for aligning pre-procedural magnetic resonance (MR) and 3D intra-procedural transrectal ultrasound (TRUS) images. A discriminator network is optimized to distinguish the registration predicted displacement fields from the motion data simulated by finite element analysis. During training, the registration network simultaneously aims to maximize similarity between anatomical labels that drives image alignment and to minimize an adversarial generator loss that measures divergence between the predicted-and simulated deformation. The end-to-end trained network enables efficient and fully-automated registration that only requires an MR and TRUS image pair as input, without anatomical labels or simulated data during inference. 108 pairs of labelled MR and TRUS images from 76 prostate cancer patients and 71,500 nonlinear finite-element simulations from 143 different patients were used for this study. We show that, with only gland segmentation as training labels, the proposed method can help predict physically plausible deformation without any other smoothness penalty. Based on cross-validation experiments using 834 pairs of independent validation landmarks, the proposed adver-sarial-regularized registration achieved a target registration error of 6.3 mm that is significantly lower than those from several other regularization methods.},
author = {Hu, Yipeng and Gibson, Eli and Ghavami, Nooshin and Bonmati, Ester and Moore, Caroline M and Emberton, Mark and Vercauteren, Tom and Noble, J Alison and Barratt, Dean C},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2018 - Adversarial Deformation Regularization for Training Image Registration Neural Networks.pdf:pdf},
pages = {774--782},
title = {{Adversarial Deformation Regularization for Training Image Registration Neural Networks}},
year = {2018}
}
@article{Pielawski2020,
abstract = {We propose contrastive coding to learn shared, dense image representations, referred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs enable the registration of multimodal images where existing registration methods often fail due to a lack of sufficiently similar image structures. CoMIRs reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods, used for, e.g., classification, our approach generates image-like representations that contain the information shared between modalities. We introduce a novel, hyperparameter-free modification to InfoNCE, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of RGB and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-field and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on CoMIRs significantly outperforms registration of representations created by GAN-based image-to-image translation, as well as a state-of-the-art, application-specific method which takes additional knowledge about the data into account. Code is available at: https://github.com/MIDA-group/CoMIR.},
annote = {data augmentation for multi-model image registration.

Using contrastive loss to learn a common representation for both images.},
archivePrefix = {arXiv},
arxivId = {2006.06325},
author = {Pielawski, Nicolas and Wetzer, Elisabeth and {\"{O}}fverstedt, Johan and Lu, Jiahao and W{\"{a}}hlby, Carolina and Lindblad, Joakim and Sladoje, Nata{\v{s}}a},
eprint = {2006.06325},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pielawski et al. - 2020 - CoMIR Contrastive Multimodal Image Representation for Registration.pdf:pdf},
journal = {NeurIPS},
month = {jun},
publisher = {arXiv},
title = {{CoMIR: Contrastive Multimodal Image Representation for Registration}},
url = {http://arxiv.org/abs/2006.06325},
year = {2020}
}
@article{christensen1996,
abstract = {A general automatic approach is presented for accommodating local shape variation when mapping a two-dimensional (2-D) or three-dimensional (3-D) template image into alignment with a topologically similar target image. Local shape variability is accommodated by applying a vector-field transformation to the underlying material coordinate system of the template while constraining the transformation to be smooth (globally positive definite Jacobian). Smoothness is guaranteed without specifically penalizing large-magnitude deformations of small subvolumes by constraining the transformation on the basis of a Stokesian limit of the fluid-dynamical Navier-Stokes equations. This differs fundamentally from quadratic penalty methods, such as those based on linearized elasticity or thin-plate splines, in that stress restraining the motion relaxes over time allowing large-magnitude deformations. Kinematic nonlinearities are inherently necessary to maintain continuity of structures during large-magnitude deformations, and are included in all results. After initial global registration, final mappings are obtained by numerically solving a set of nonlinear partial differential equations associated with the constrained optimization problem. Automatic regridding is performed by propagating templates as the nonlinear transformations evaluated on a finite lattice become singular. Application of the method to intersubject registration of neuroanatomical structures illustrates the ability to account for local anatomical variability. {\textcopyright} 1996 IEEE.},
author = {Christensen, Gary E. and Rabbitt, Richard D. and Miller, Michael I.},
doi = {10.1109/83.536892},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christensen, Rabbitt, Miller - 1996 - Deformable templates using large deformation kinematics.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
number = {10},
pages = {1435--1447},
title = {{Deformable templates using large deformation kinematics}},
volume = {5},
year = {1996}
}
@article{sotiras2013survey,
author = {Sotiras, Aristeidis and Davatzikos, Christos and Paragios, Nikos},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sotiras, Davatzikos, Paragios - 2013 - Deformable Medical Image Registration A Survey.pdf:pdf},
journal = {IEEE Transactions on Medical Imaging},
number = {7},
pages = {1153--1190},
title = {{Deformable Medical Image Registration: A Survey}},
url = {https://hal.inria.fr/hal-00684715v4},
volume = {32},
year = {2013}
}
@techreport{Gee1993,
author = {Gee, James C and Reivich, Martin and Bajcsy, Ruzena},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gee, Reivich, Bajcsy - 1993 - Elastically Deforming a Three-Dimensional Atlas to Match Anatomical Brain Images.pdf:pdf},
title = {{Elastically Deforming a Three-Dimensional Atlas to Match Anatomical Brain Images}},
url = {http://repository.upenn.edu/ircs{\_}reports/192},
year = {1993}
}
@inproceedings{Baur2019,
abstract = {Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model local patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR slices. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning.},
annote = {VAE for anomaly detection

Prostprocessed difference of input and output shows abnormal tumors.

Train VAE decoder with adversarial network (Heavy GAN inspiration all over the paper). VAE latent space keeps spatial dimensions, e.g. 16x16x64 (fully convolutional, called spatial variational autoencoder sVAE)

Experiments on size of latent space. Latent size is critial to application, model needs to be able to reconstruct fine details, but not out-of-distribution anomalies

Only trained on 2d slices extracted from the midline of the brain},
archivePrefix = {arXiv},
arxivId = {1804.04488},
author = {Baur, Christoph and Wiestler, Benedikt and Albarqouni, Shadi and Navab, Nassir},
booktitle = {Medical Image Computing and Computer Assisted Intervention},
doi = {10.1007/978-3-030-11723-8_16},
eprint = {1804.04488},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baur et al. - 2018 - Deep autoencoding models for unsupervised anomaly segmentation in brain MR images.pdf:pdf},
isbn = {9783030117221},
issn = {16113349},
month = {sep},
publisher = {Springer Verlag},
title = {{Deep autoencoding models for unsupervised anomaly segmentation in brain MR images}},
url = {https://link.springer.com/chapter/10.1007/978-3-030-11723-8{\_}16},
year = {2018}
}
@incollection{szeliski2006,
address = {Boston, MA},
author = {Szeliski, Richard},
booktitle = {Handbook of Mathematical Models in Computer Vision},
editor = {Paragios, Nikos and Chen, Yunmei and Faugeras, Olivier},
pages = {273--292},
publisher = {Springer US},
title = {{Image Alignment and Stitching}},
year = {2006}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {International Conference on Learning Representations},
eprint = {1412.6980},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
month = {dec},
title = {{Adam: A method for stochastic optimization}},
url = {https://arxiv.org/abs/1412.6980v9},
year = {2015}
}
@article{Niethammer2019,
abstract = {Image registration is a key technique in medical image analysis to estimate deformations between image pairs. A good deformation model is important for high-quality estimates. However, most existing approaches use ad-hoc deformation models chosen for mathematical convenience rather than to capture observed data variation. Recent deep learning approaches learn deformation models directly from data. However, they provide limited control over the spatial regularity of transformations. Instead of learning the entire registration approach, we learn a spatially-adaptive regularizer within a registration model. This allows controlling the desired level of regularity and preserving structural properties of a registration model. For example, diffeomorphic transformations can be attained. Our approach is a radical departure from existing deep learning approaches to image registration by embedding a deep learning model in an optimization-based registration algorithm to parameterize and data-adapt the registration model itself.},
annote = {Goal is to learn a spatially- varying regularizer which takes as inputs a momentum vector field and an image and computes a smoothed vectorfield.},
archivePrefix = {arXiv},
arxivId = {1904.09524},
author = {Niethammer, Marc and Kwitt, Roland and Vialard, Francois-Xavier},
eprint = {1904.09524},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Niethammer, Kwitt, Vialard - 2019 - Metric Learning for Image Registration.pdf:pdf},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
month = {apr},
pages = {8463----8472},
title = {{Metric Learning for Image Registration}},
url = {http://arxiv.org/abs/1904.09524},
year = {2019}
}
@article{Parmar2018,
abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling tex-tual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
annote = {[2] of image transformer reading group

The original vision transformer paper
Early work
pixels generated one by one

Useus locally-restricted form of self-attention to fill in missing parts of an image
Self-attention on local neighborhood
No conv layers at all},
author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, {\L}ukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parmar et al. - 2018 - Image Transformer.pdf:pdf},
journal = {Proceedings of the 35th International Conference on Machine Learning},
title = {{Image Transformer}},
year = {2018}
}
@article{Risser2013a,
abstract = {In this paper, we propose a new strategy for modelling sliding conditions when registering 3D images in a piecewise-diffeomorphic framework. More specifically, our main contribution is the development of a mathematical formalism to perform Large Deformation Diffeomorphic Metric Mapping registration with sliding conditions. We also show how to adapt this formalism to the LogDemons diffeomorphic registration framework. We finally show how to apply this strategy to estimate the respiratory motion between 3D CT pulmonary images. Quantitative tests are performed on 2D and 3D synthetic images, as well as on real 3D lung images from the MICCAI EMPIRE10 challenge. Results show that our strategy estimates accurate mappings of entire 3D thoracic image volumes that exhibit a sliding motion, as opposed to conventional registration methods which are not capable of capturing discontinuous deformations at the thoracic cage boundary. They also show that although the deformations are not smooth across the location of sliding conditions, they are almost always invertible in the whole image domain. This would be helpful for radiotherapy planning and delivery. {\textcopyright} 2012.},
annote = {sliding motions by masking a gaussian-smoothing regularizer},
author = {Risser, Laurent and Vialard, Fran{\c{c}}ois Xavier and Baluwala, Habib Y. and Schnabel, Julia A.},
doi = {10.1016/j.media.2012.10.001},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Risser et al. - 2013 - Piecewise-diffeomorphic image registration Application to the motion estimation between 3D CT lung images with(2).pdf:pdf},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Diffeomorphic registration,LDDMM,LogDemons,Respiratory motion,Sliding motion},
month = {feb},
number = {2},
pages = {182--193},
publisher = {Elsevier},
title = {{Piecewise-diffeomorphic image registration: Application to the motion estimation between 3D CT lung images with sliding conditions}},
volume = {17},
year = {2013}
}
@inproceedings{Sohn2015,
abstract = {Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient varia-tional Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
author = {Sohn, Kihyuk and Yan, Xinchen and Lee, Honglak},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sohn, Yan, Lee - 2015 - Learning Structured Output Representation using Deep Conditional Generative Models(3).pdf:pdf},
pages = {3483--3491},
title = {{Learning Structured Output Representation using Deep Conditional Generative Models}},
volume = {28},
year = {2015}
}
@article{balakrishnan2019voxel,
abstract = {We present VoxelMorph, a fast learning-based framework for deformable, pairwise medical image registration. Traditional registration methods optimize an objective function for each pair of images, which can be time-consuming for large datasets or rich deformation models. In contrast to this approach, and building on recent learning-based methods, we formulate registration as a function that maps an input image pair to a deformation field that aligns these images. We parameterize the function via a convolutional neural network (CNN), and optimize the parameters of the neural network on a set of images. Given a new pair of scans, VoxelMorph rapidly computes a deformation field by directly evaluating the function. In this work, we explore two different training strategies. In the first (unsupervised) setting, we train the model to maximize standard image matching objective functions that are based on the image intensities. In the second setting, we leverage auxiliary segmentations available in the training data. We demonstrate that the unsupervised model's accuracy is comparable to state-of-the-art methods, while operating orders of magnitude faster. We also show that VoxelMorph trained with auxiliary data improves registration accuracy at test time, and evaluate the effect of training set size on registration. Our method promises to speed up medical image analysis and processing pipelines, while facilitating novel directions in learning-based registration and its applications. Our code is freely available at voxelmorph.csail.mit.edu.},
annote = {supervised + unsupervised approach

Initial vexelmorph paper, later followed up by "Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration"
lots of good reasoning about using ML for registration

good overview over the field

Evaluation: 
Dataset:
- all displayed in 2D, but registered in 3d
3731 T1–weighted brain MRI scans from multiple datasets:
OASIS, ABIDE, ADHD200, MCIC, PPMI, HABS, Harvard GSP, FreeSurfer Buckner40
- 30 class anatomical segmentation used for evaluation
- metrics: dice overlap, smoothness of transformation (via Jacobian), diffeomorphism of transformation (jacobian determinant)
- Comparison to baseline models: Symmetric Normalization (SyN) from ANTs software package


Section V-B:
method: registering to an atlas (one averaged, common model).
VoxelMorph is compared to ANTs SyN, NiftyReg and Affine registration. Compared metrics: Dice overlap of segmentation data, runtime (GPU and CPU)
- resuts: table and plot of metrics, aggregated and per anatomical feature

Section V-C:
hyper parameter tuning plots

Section V-D:
effect of training set size

Section V-E:
crosscheck of results on a dataset of manual segmentations. Nessesary as all other segmentations were automatically obtained by CNN


Section V-F:
evaluation on unseen test set

Secton V-G:
Same model, but now with extra segmentation data used during training},
author = {Balakrishnan, Guha and Zhao, Amy and Sabuncu, Mert R. and Guttag, John and Dalca, Adrian V.},
doi = {10.1109/tmi.2019.2897538},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Balakrishnan et al. - 2019 - VoxelMorph A Learning Framework for Deformable Medical Image Registration.pdf:pdf},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {NN},
mendeley-tags = {NN},
month = {feb},
number = {8},
pages = {1788--1800},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{VoxelMorph: A Learning Framework for Deformable Medical Image Registration}},
volume = {38},
year = {2019}
}
@inproceedings{Qiu2021,
abstract = {We present a deep learning (DL) registration framework for fast mono-modal and multi-modal image registration using differentiable mutual information and diffeomorphic B-spline free-form deformation (FFD). Deep learning registration has been shown to achieve competitive accuracy and significant speedups from traditional iterative registration methods. In this paper, we propose to use a B-spline FFD parameterisation of Stationary Velocity Field (SVF) to in DL registration in order to achieve smooth diffeomorphic deformation while being computationally-efficient. In contrast to most DL registration methods which use intensity similarity metrics that assume linear intensity relationship, we apply a differentiable variant of a classic similarity metric, mutual information, to achieve robust mono-modal and multi-modal registration. We carefully evaluated our proposed framework on mono-and multi-modal registration using 3D brain MR images and 2D cardiac MR images.},
annote = {E-8

video: https://2021.midl.io/papers/e8
implementation: https://github.com/qiuhuaqi/midir/blob/master/model/loss.py

Questions:
- Why less parameters than a full resolution dense flow prediction network?
- Integration over flow field -{\textgreater} B-splines identical to full-resolution transformations downsampled?
- ask for code on mutual information
- Table 1: LNCC performs better than NMI?},
author = {Qiu, Huaqi and Qin, Chen and Schuh, Andreas and Hammernik, Kerstin and Rueckert, Daniel},
booktitle = {Proceedings of Machine Learning Research},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Qiu et al. - 2021 - Learning Diffeomorphic and Modality-invariant Registration using B-splines.pdf:pdf},
title = {{Learning Diffeomorphic and Modality-invariant Registration using B-splines}},
url = {https://openreview.net/forum?id=eSI9Qh2DJhN},
year = {2021}
}
@inproceedings{Arsigny2006,
abstract = {In this article, we focus on the computation of statistics of invertible geometrical deformations (i.e., diffeomorphisms), based on the generalization to this type of data of the notion of principal logarithm. Remarkably, this logarithm is a simple 3D vector field, and is well-defined for diffeomorphisms close enough to the identity. This allows to perform vectorial statistics on diffeomorphisms, while preserving the invertibility constraint, contrary to Euclidean statistics on displacement fields. We also present here two efficient algorithms to compute logarithms of diffeomorphisms and exponentials of vector fields, whose accuracy is studied on synthetic data. Finally, we apply these tools to compute the mean of a set of diffeomorphisms, in the context of a registration experiment between an atlas an a database of 9 T1 MR images of the human brain. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {Proposes logarithms on geometrical deformations, based on matrix exponentials.

The logarithm of a geometric transformation Phi is given by a simple vector field V, also called Stationary Velocity Field (SVF).

V = log(Phi)
Phi = exp(V)
log(Phi) = V

Calculation methods are given in the paper.

Instead of doing statistics on the diffomorphism (complicated functon), we can simply do statistics on its logarithm V (simple vector field)! Those statistics are invertible back to the diffeomorphism (contrary to just doing statistics on displacement fields)},
author = {Arsigny, Vincent and Commowick, Olivier and Pennec, Xavier and Ayache, Nicholas},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/11866565_113},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arsigny et al. - 2006 - A log-euclidean framework for statistics on diffeomorphisms.pdf:pdf},
isbn = {3540447075},
issn = {16113349},
pages = {924--931},
pmid = {17354979},
publisher = {Springer Verlag},
title = {{A log-euclidean framework for statistics on diffeomorphisms}},
year = {2006}
}
@article{ICMJE2015,
author = {ICMJE},
journal = {International Committee of Medical Journal Editors},
title = {{Recommendations for the conduct, reporting, editing, and publication of scholarly work in medical journals}},
year = {2015}
}
@article{Vercauteren2008,
abstract = {Modern morphometric studies use non-linear image registration to compare anatomies and perform group analysis. Recently, log-Euclidean approaches have contributed to promote the use of such computational anatomy tools by permitting simple computations of statistics on a rather large class of invertible spatial transformations. In this work, we propose a non-linear registration algorithm perfectly fit for log-Euclidean statistics on diffeomorphisms. Our algorithm works completely in the log-domain, i.e. it uses a stationary velocity field. This implies that we guarantee the invertibility of the deformation and have access to the true inverse transformation. This also means that our output can be directly used for log-Euclidean statistics without relying on the heavy computation of the log of the spatial transformation. As it is often desirable , our algorithm is symmetric with respect to the order of the input images. Furthermore, we use an alternate optimization approach related to Thirion's demons algorithm to provide a fast non-linear registration algorithm. First results show that our algorithm outperforms both the demons algorithm and the recently proposed diffeomorphic demons algorithm in terms of accuracy of the transformation while remaining com-putationally efficient.},
author = {Vercauteren, Tom and Pennec, Xavier and Perchant, Aymeric and Ayache, Nicholas},
doi = {10.1007/978-3-540-85988-8_90ï},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vercauteren et al. - 2008 - Symmetric Log-Domain Diffeomorphic Registration A Demons-based Approach.pdf:pdf},
journal = {Medical Image Computing and Computer Assisted Intervention},
pages = {754--761},
title = {{Symmetric Log-Domain Diffeomorphic Registration: A Demons-based Approach}},
url = {https://hal.inria.fr/inria-00280602},
year = {2008}
}
@article{Haskins2019a,
abstract = {Purpose: The fusion of transrectal ultrasound (TRUS) and magnetic resonance (MR) images for guiding targeted prostate biopsy has significantly improved the biopsy yield of aggressive cancers. A key component of MR–TRUS fusion is image registration. However, it is very challenging to obtain a robust automatic MR–TRUS registration due to the large appearance difference between the two imaging modalities. The work presented in this paper aims to tackle this problem by addressing two challenges: (i) the definition of a suitable similarity metric and (ii) the determination of a suitable optimization strategy. Methods: This work proposes the use of a deep convolutional neural network to learn a similarity metric for MR–TRUS registration. We also use a composite optimization strategy that explores the solution space in order to search for a suitable initialization for the second-order optimization of the learned metric. Further, a multi-pass approach is used in order to smooth the metric for optimization. Results: The learned similarity metric outperforms the classical mutual information and also the state-of-the-art MIND feature-based methods. The results indicate that the overall registration framework has a large capture range. The proposed deep similarity metric-based approach obtained a mean TRE of 3.86 mm (with an initial TRE of 16 mm) for this challenging problem. Conclusion: A similarity metric that is learned using a deep neural network can be used to assess the quality of any given image registration and can be used in conjunction with the aforementioned optimization framework to perform automatic registration that is robust to poor initialization.},
annote = {Description: 
learn a metric-network for similarity judgements

Good:
good results with deep similarity metric on MR-TRUS images

Bad:
requires prior ground truth registration for training
limited to rigid transformations
no alignment of deep features, instead map from two images to similarity},
author = {Haskins, Grant and Kruecker, Jochen and Kruger, Uwe and Xu, Sheng and Pinto, Peter A. and Wood, Brad J. and Yan, Pingkun},
doi = {10.1007/s11548-018-1875-7},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haskins et al. - 2019 - Learning deep similarity metric for 3D MR–TRUS image registration.pdf:pdf},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {Convolutional neural networks,Image registration,Image-guided interventions,Multimodal image fusion,Prostate cancer},
month = {mar},
number = {3},
pages = {417--425},
pmid = {30382457},
publisher = {Springer Verlag},
title = {{Learning deep similarity metric for 3D MR–TRUS image registration}},
url = {https://doi.org/10.1007/s11548-018-1875-7},
volume = {14},
year = {2019}
}
@inproceedings{Ruan2009,
abstract = {Sliding effects often occur along tissue/organ boundaries. For instance, it is widely observed that the lung and diaphragm slide against the rib cage and the atria during breathing. Conventional homogeneous smooth registration methods fail to address this issue. Some recent studies preserve motion discontinuities by either using joint registration/segmentation or utilizing robust regularization energy on the motion field. However, allowing all types of discontinuities is not strict enough for physical deformations. In particular, flows that generate local vacuums or mass collisions should be discouraged by the energy functional. In this study, we propose a regularization energy that encodes a discriminative treatment of different types of motion discontinuities. The key idea is motivated by the Helmholtz-Hodge decomposition, and regards the underlying motion flow as a superposition of a solenoidal component, an irrotational component and a harmonic part. The proposed method applies a homogeneous penalty on the divergence, discouraging local volume change caused by the irrotational component, thus avoiding local vacuum or collision; it regularizes the curl field with a robust functional so that the resulting solenoidal component vanishes almost everywhere except on a singular set where the large shear values are preserved. This singularity set corresponds to sliding interfaces. Preliminary tests with both simulated and clinical data showed promising results. {\textcopyright} 2009 IEEE.},
annote = {implicit (= sliding surfaces are unknown) sliding-motion registration.},
author = {Ruan, Dan and Esedoĝlu, Selim and Fessler, Jeffrey A.},
booktitle = {Proceedings - 2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro, ISBI 2009},
doi = {10.1109/ISBI.2009.5193076},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruan, Esedoĝlu, Fessler - 2009 - Discriminative sliding preserving regularization in medical image registration.pdf:pdf},
isbn = {9781424439324},
keywords = {Curl,Divergence,Registration},
pages = {430--433},
title = {{Discriminative sliding preserving regularization in medical image registration}},
year = {2009}
}
@article{Heinrich2012,
abstract = {Deformable registration of images obtained from different modalities remains a challenging task in medical image analysis. This paper addresses this important problem and proposes a modality independent neighbourhood descriptor (MIND) for both linear and deformable multi-modal registration. Based on the similarity of small image patches within one image, it aims to extract the distinctive structure in a local neighbourhood, which is preserved across modalities. The descriptor is based on the concept of image self-similarity, which has been introduced for non-local means filtering for image denoising. It is able to distinguish between different types of features such as corners, edges and homogeneously textured regions. MIND is robust to the most considerable differences between modalities: non-functional intensity relations, image noise and non-uniform bias fields. The multi-dimensional descriptor can be efficiently computed in a dense fashion across the whole image and provides point-wise local similarity across modalities based on the absolute or squared difference between descriptors, making it applicable for a wide range of transformation models and optimisation algorithms. We use the sum of squared differences of the MIND representations of the images as a similarity metric within a symmetric non-parametric Gauss-Newton registration framework. In principle, MIND would be applicable to the registration of arbitrary modalities. In this work, we apply and validate it for the registration of clinical 3D thoracic CT scans between inhale and exhale as well as the alignment of 3D CT and MRI scans. Experimental results show the advantages of MIND over state-of-the-art techniques such as conditional mutual information and entropy images, with respect to clinically annotated landmark locations. {\textcopyright} 2012 Elsevier B.V.},
author = {Heinrich, Mattias P. and Jenkinson, Mark and Bhushan, Manav and Matin, Tahreema and Gleeson, Fergus V. and Brady, Sir Michael and Schnabel, Julia A.},
doi = {10.1016/j.media.2012.05.008},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinrich et al. - 2012 - MIND Modality independent neighbourhood descriptor for multi-modal deformable registration.pdf:pdf},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Multi-modal similarity metric,Non-local means,Non-rigid registration,Pulmonary images,Self-similarity},
month = {oct},
number = {7},
pages = {1423--1435},
pmid = {22722056},
publisher = {Elsevier},
title = {{MIND: Modality independent neighbourhood descriptor for multi-modal deformable registration}},
volume = {16},
year = {2012}
}
@article{Christiansen2006,
annote = {example of how to teach a lesson:

1. Awake interest
2. simple Sub-assignment, so students familiarize with the material used in this lesson
3. Explore knowledge as the answer to a question: through concrete, applied problems, not the problem-removed representation from textbooks
4. Students form hypothesis based on assignments
5. Teach helps to validatehypothesis
6. teach generalizes knowledge gained},
author = {Christiansen, Frederik Voetmann and Olsen, Lars},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christiansen, Olsen - 2006 - Analyse og design af didaktiske situationer Analysis and design of didactic situations a pharmaceutical exa.pdf:pdf},
journal = {Mona},
pages = {7--23},
title = {{Analyse og design af didaktiske situationer [Analysis and design of didactic situations: a pharmaceutical example]}},
volume = {3},
year = {2006}
}
@article{Hoopes2021,
abstract = {We present HyperMorph, a learning-based strategy for deformable image registration that removes the need to tune important registration hyperparameters during training. Classical registration methods solve an optimization problem to find a set of spatial correspondences between two images, while learning-based methods leverage a training dataset to learn a function that generates these correspondences. The quality of the results for both types of techniques depends greatly on the choice of hyperparameters. Unfortunately, hyperparameter tuning is time-consuming and typically involves training many separate models with various hyperparameter values, potentially leading to suboptimal results. To address this inefficiency, we introduce amortized hyperparameter learning for image registration, a novel strategy to learn the effects of hyperparameters on deformation fields. The proposed framework learns a hypernetwork that takes in an input hyperparameter and modulates a registration network to produce the optimal deformation field for that hyperparameter value. In effect, this strategy trains a single, rich model that enables rapid, fine-grained discovery of hyperparameter values from a continuous interval at test-time. We demonstrate that this approach can be used to optimize multiple hyperparameters considerably faster than existing search strategies, leading to a reduced computational and human burden as well as increased flexibility. We also show several important benefits, including increased robustness to initialization and the ability to rapidly identify optimal hyperparameter values specific to a registration task, dataset, or even a single anatomical region, all without retraining the HyperMorph model. Our code is publicly available at http://voxelmorph.mit.edu.},
annote = {IPMI 2021 egistration oral

Method:
-Instead of training multiple models with different hyperparameter choices, we:
- train a "Hyper-Model", which inputs hyperparameters lambda and outputs parameters for the registration model
- during training, hyperparameters are drawn at random from distributions
- during validation, we can test different inputs for the hyperparameters
- hyperparameters tuned in this paper: regularizer strength, supervised seg information strength, NCC window size, 
- individual model training time twiche as long as baseline model, but vastly shorter compared to many baseline model runs

Questions:
- Architecture of the Hypernetwork: Paper states 4 fully connected layers of 64 neurons each. Registration network needs 300k parameters. Is last layer mapping from 64-{\textgreater}300k?
- integration (mentioned in talk): also at train time, or just during test?

Discussion:},
archivePrefix = {arXiv},
arxivId = {2101.01035},
author = {Hoopes, Andrew and Hoffmann, Malte and Fischl, Bruce and Guttag, John and Dalca, Adrian V.},
eprint = {2101.01035},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoopes et al. - 2021 - HyperMorph Amortized Hyperparameter Learning for Image Registration(2).pdf:pdf},
journal = {Information Processing in Medical Imaging},
keywords = {Amortized,Deep,Deformable,Hyperparameter,Image,Learning,Learning {\textperiodcentered},Registration {\textperiodcentered},Tuning {\textperiodcentered}},
month = {jan},
title = {{HyperMorph: Amortized Hyperparameter Learning for Image Registration}},
url = {http://arxiv.org/abs/2101.01035},
year = {2021}
}
@misc{De,
author = {Mayo, Deborah G},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mayo - Unknown - Philosophy of statistics.pdf:pdf},
title = {{Philosophy of statistics}}
}
@techreport{thorin95,
author = {Thirion, Jean-Philippe},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thirion - 1995 - Fast Non-Rigid Matching of 3D Medical Images.pdf:pdf},
title = {{Fast Non-Rigid Matching of 3D Medical Images}},
url = {https://hal.inria.fr/inria-00077268},
year = {1995}
}
@article{Vishnevskiy2017,
abstract = {Spatial regularization is essential in image registration, which is an ill-posed problem. Regularization can help to avoid both physically implausible displacement fields and local minima during optimization. Tikhonov regularization (squared ℓ2-norm) is unable to correctly represent non-smooth displacement fields, that can, for example, occur at sliding interfaces in the thorax and abdomen in image time-series during respiration. In this paper, isotropic Total Variation (TV) regularization is used to enable accurate registration near such interfaces. We further develop the TV-regularization for parametric displacement fields and provide an efficient numerical solution scheme using the Alternating Directions Method of Multipliers (ADMM). The proposed method was successfully applied to four clinical databases which capture breathing motion, including CT lung and MR liver images. It provided accurate registration results for the whole volume. A key strength of our proposed method is that it does not depend on organ masks that are conventionally required by many algorithms to avoid errors at sliding interfaces. Furthermore, our method is robust to parameter selection, allowing the use of the same parameters for all tested databases. The average target registration error (TRE) of our method is superior (10{\%} to 40{\%}) to other techniques in the literature. It provides precise motion quantification and sliding detection with sub-pixel accuracy on the publicly available breathing motion databases (mean TREs of 0.95 mm for DIR 4D CT, 0.96 mm for DIR COPDgene, 0.91 mm for POPI databases).},
author = {Vishnevskiy, Valery and Gass, Tobias and Szekely, Gabor and Tanner, Christine and Goksel, Orcun},
doi = {10.1109/TMI.2016.2610583},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {4DCT,4DMR,ADMM,breathing motion,sliding at anatomical interfaces},
month = {feb},
number = {2},
pages = {385--395},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Isotropic Total Variation Regularization of Displacements in Parametric Image Registration}},
volume = {36},
year = {2017}
}
@techreport{Siarohin2019,
abstract = {Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly available 1 .},
annote = {Registration with occlusion mask, into image generation

Can we levearge the occlusion mask for image registration tasks?

Cool end-to-end trained registration+generation network.},
author = {Siarohin, Aliaksandr and Lathuili{\`{e}}re, St{\'{e}}phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siarohin et al. - 2019 - First Order Motion Model for Image Animation.pdf:pdf},
pages = {7137--7147},
title = {{First Order Motion Model for Image Animation}},
url = {https://aliaksandrsiarohin.github.io/first-order-model-website/},
year = {2019}
}
@article{Castillo2010,
abstract = {A four-dimensional deformable image registration (4D DIR) algorithm, referred to as 4D local trajectory modeling (4DLTM), is presented and applied to thoracic 4D computed tomography (4DCT) image sets. The theoretical framework on which this algorithm is built exploits the incremental continuity present in 4DCT component images to calculate a dense set of parameterized voxel trajectories through space as functions of time. The spatial accuracy of the 4DLTM algorithm is compared with an alternative registration approach in which component phase to phase (CPP) DIR is utilized to determine the full displacement between maximum inhale and exhale images. A publically available DIR reference database (http://www.dir-lab.com) is utilized for the spatial accuracy assessment. The database consists of ten 4DCT image sets and corresponding manually identified landmark points between the maximum phases. A subset of points are propagated through the expiratory 4DCT component images. Cubic polynomials were found to provide sufficient flexibility and spatial accuracy for describing the point trajectories through the expiratory phases. The resulting average spatial error between the maximum phases was 1.25 mm for the 4DLTM and 1.44 mm for the CPP. The 4DLTM method captures the long-range motion between 4DCT extremes with high spatial accuracy. 2010 Institute of Physics and Engineering in Medicine.},
author = {Castillo, Edward and Castillo, Richard and Martinez, Josue and Shenoy, Maithili and Guerrero, Thomas},
doi = {10.1088/0031-9155/55/1/018},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Castillo et al. - 2010 - Four-dimensional deformable image registration using trajectory modeling.pdf:pdf},
issn = {00319155},
journal = {Physics in Medicine and Biology},
month = {jan},
number = {1},
pages = {305--327},
title = {{Four-dimensional deformable image registration using trajectory modeling}},
volume = {55},
year = {2010}
}
@article{Czolbe2021a,
abstract = {Geometric alignment appears in a variety of applications, ranging from domain
adaptation, optimal transport, and normalizing flows in machine learning;
optical flow and learned augmentation in computer vision and deformable
registration within biomedical imaging. A recurring challenge is the alignment
of domains whose topology is not the same; a problem that is routinely ignored,
potentially introducing bias in downstream analysis. As a first step towards
solving such alignment problems, we propose an unsupervised topological
difference detection algorithm. The model is based on a conditional variational
auto-encoder and detects topological anomalies with regards to a reference
alongside the registration step. We consider both a) topological changes in the
image under spatial variation and b) unexpected transformations. Our approach
is validated on a proxy task of unsupervised anomaly detection in images.},
archivePrefix = {arXiv},
arxivId = {2106.08233},
author = {Czolbe, Steffen and Feragen, Aasa and Krause, Oswin},
eprint = {2106.08233},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czolbe, Feragen, Krause - 2021 - Spot the Difference Topological Anomaly Detection via Geometric Alignment.pdf:pdf},
month = {jun},
title = {{Spot the Difference: Topological Anomaly Detection via Geometric Alignment}},
url = {https://arxiv.org/abs/2106.08233v1},
year = {2021}
}
@article{Monteiro2020,
abstract = {In image segmentation, there is often more than one plausible solution for a given input. In medical imaging, for example, experts will often disagree about the exact location of object boundaries. Estimating this inherent uncertainty and predicting multiple plausible hypotheses is of great interest in many applications, yet this ability is lacking in most current deep learning methods. In this paper, we introduce stochastic segmentation networks (SSNs), an efficient probabilistic method for modelling aleatoric uncertainty with any image segmentation network architecture. In contrast to approaches that produce pixel-wise estimates, SSNs model joint distributions over entire label maps and thus can generate multiple spatially coherent hypotheses for a single image. By using a low-rank multivariate normal distribution over the logit space to model the probability of the label map given the image, we obtain a spatially consistent probability distribution that can be efficiently computed by a neural network without any changes to the underlying architecture. We tested our method on the segmentation of real-world medical data, including lung nodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform state-of-the-art for modelling correlated uncertainty in ambiguous images while being much simpler, more flexible, and more efficient.},
annote = {Probabilistic segmentation by:
- NN predicts mean and covariance
- Covariance matrix is formed by low-rank approximation (but not just diagonal, but bigger!)
- allows sampling for distribution

evaluated against U-Net, probabilistic U-net and PHiSeg (Miccai 2019)

Evaluated using:
- Expected Dice Overlap of sample sdrawn
- GED (very nice formulation)
- sample diversity as expected distance between samples},
archivePrefix = {arXiv},
arxivId = {2006.06015},
author = {Monteiro, Miguel and Folgoc, Lo{\"{i}}c Le and de Castro, Daniel Coelho and Pawlowski, Nick and Marques, Bernardo and Kamnitsas, Konstantinos and van der Wilk, Mark and Glocker, Ben},
eprint = {2006.06015},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Monteiro et al. - 2020 - Stochastic Segmentation Networks Modelling Spatially Correlated Aleatoric Uncertainty.pdf:pdf},
month = {jun},
title = {{Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty}},
url = {http://arxiv.org/abs/2006.06015},
year = {2020}
}
@inproceedings{Lee2020,
author = {Lee, Dong Bok and Min, Dongchan and Lee, Seanie and Hwang, Sung Ju},
booktitle = {International Conference on Learning Representations},
title = {{Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning}},
year = {2020}
}
@inproceedings{Gal2016,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison , Bayesian models offer a mathematically grounded framework to reason about model uncertainty , but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs-extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predic-tive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
author = {{Gal, Yarin and Ghahramani}, Zoubin},
booktitle = {International Conference on Machine Learning},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal, Yarin and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
pages = {1050----1059},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
year = {2016}
}
@article{Studholme1999,
abstract = {This paper is concerned with the development of entropy-based registration criteria for automated 3D multi-modality medical image alignment. In this application where misalignment can be large with respect to the imaged field of view, invariance to overlap statistics is an important consideration. Current entropy measures are reviewed and a normalised measure is proposed which is simply the ratio of the sum of the marginal entropies and the joint entropy. The effect of changing overlap on current entropy measures and this normalised measure are compared using a simple image model and experiments on clinical image data. Results indicate that the normalised entropy measure provides significantly improved behaviour over a range of imaged fields of view. {\textcopyright} 1999 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Studholme, C. and Hill, D. L.G. and Hawkes, D. J.},
doi = {10.1016/S0031-3203(98)00091-0},
file = {:home/steffen/Downloads/1-s2.0-S0031320398000910-main.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {3D medical images,Entropy,Information theory,Multi-modality,Mutual information,Normalisation,Registration criteria},
month = {jan},
number = {1},
pages = {71--86},
publisher = {Pergamon},
title = {{An overlap invariant entropy measure of 3D medical image alignment}},
volume = {32},
year = {1999}
}
@article{Yang2017,
annote = {fully supervised (by algorithmically obtained shooting LDDMM data)

The method retains all theoretical properties of LDDMM and results in diffeomorphic transformations if appropriately regularized, but computes these transformations an order ofmagnitude faster than a GPU-
based optimization for the LDDMM model. Further speedups can be achieved by using multile GPUs. In effect, our Quicksilver registration approach converts a notoriously slow and memory-hungry registration approach to a fast method, while retaining all of its appealing mathematical properties. General solution that can be applied to many registration algorithms (e.g., elastic registration or SVF), and even parameterized such as B-splines.

First significant registration paper using deep learning.

Utelizes the LDDMM shooting algorithm in two ways:
1) to generate a ground truth of initial momentum, which is then learned by NN
2) to generate the predicted transformation by integrating the shooting equation (Eq. 4) for the prediced initial momentum

This dual-approach between deep learning and classical registration algorithms allows for the preservation of the theoretical guarantee of obtaining a diffeomorphism, even if our NN predicts a non-smooth initial momentum.


Uses a two-step process to estimating the momentum: 
(1) an Initial prediction, and a 
(2) correction prediction. 
(1) is trained first. Only then, (2) is trained. Suprisingly, this provides a VERY significant improvement over just training (1) for longer (Figure 4). Predictions of both networks are summed together for the final prediction.

Evaluation:
- Testing multiple different versions of the proposed method. Eg: different parameterization maps learner, Dropout/No dropout, different strides, different data sizes. Paper framed in a whay which first resents the best performing method, then confirms this method by stating other options tested, wich perform worse.
- Showing examples (Figure 5) of moving image, target image, result using different methods. Slices in all three dimensions (x, y, z) shown.
- compare mean target overlap (mean over all anatomical regions) to a variety of learned and classical algorithms, on multiple datasets (Fig 6). Explicity plotting of outliers. In-depth discussion
- fine tuning of the methods presented
- Study significance of improvement: paired t-tests with respect to mean overlap scores. Formulate null-hypothesis.},
archivePrefix = {arXiv},
arxivId = {1703.10908},
author = {Yang, Xiao and Kwitt, Roland and Styner, Martin and Niethammer, Marc},
doi = {10.1016/j.neuroimage.2017.07.008},
eprint = {1703.10908},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2017 - Quicksilver Fast predictive image registration - A deep learning approach.pdf:pdf},
issn = {10959572},
journal = {NeuroImage},
keywords = {Brain imaging,Deep learning,Image registration},
month = {sep},
pages = {378--396},
publisher = {Academic Press Inc.},
title = {{Quicksilver: Fast predictive image registration - A deep learning approach}},
volume = {158},
year = {2017}
}
@article{Fu2019,
abstract = {This paper presents a review of deep learning (DL) based medical image registration methods. We summarized the latest developments and applications of DL-based registration methods in the medical field. These methods were classified into seven categories according to their methods, functions and popularity. A detailed review of each category was presented, highlighting important contributions and identifying specific challenges. A short assessment was presented following the detailed review of each category to summarize its achievements and future potentials. We provided a comprehensive comparison among DL-based methods for lung and brain deformable registration using benchmark datasets. Lastly, we analyzed the statistics of all the cited works from various aspects, revealing the popularity and future trend of development in medical image registration using deep learning.},
annote = {Good review with statistics over the field},
archivePrefix = {arXiv},
arxivId = {1912.12318},
author = {Fu, Yabo and Lei, Yang and Wang, Tonghe and Curran, Walter J. and Liu, Tian and Yang, Xiaofeng},
eprint = {1912.12318},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fu et al. - 2019 - Deep Learning in Medical Image Registration A Review.pdf:pdf},
month = {dec},
title = {{Deep Learning in Medical Image Registration: A Review}},
url = {http://arxiv.org/abs/1912.12318},
year = {2019}
}
@techreport{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Heinrich2021,
abstract = {Deep learning has the potential to substantially improve inter-subject alignment for shape and atlas analysis. So far most highly accurate supervised approaches require dense manual annotations and complex multi-level architectures but may still be susceptible to label bias. We present a radically di↵erent approach for learning to estimate large deformations without expert supervision. Instead of regressing displacements, we train a 3D DeepLab network to predict automatic supervoxel segmentations. To enable consistent supervoxel labels, we use the warping field of a conventional approach and increase the accuracy by sampling multiple complementary over-segmentations. We experimentally demonstrate that 1) our deformable supervoxels are less sensitive to large initial misalignment and can combine linear and nonlinear registration and 2) using this self-supervised classification loss is more robust to noisy ground truth and leads to better convergence than direct regression as supervision. Public code and dataset: github.com/multimodallearning/slic reg While early work in DL-based registration had explored using the predictions of conventional methods as strong but noisy displacement supervision, more recent work focused on weak metric-or/and label supervision. Label supervision has great promise but may still require a robust pre-alignment, multi-scale architecture and dense set of labels to avoid local minima and structural bias. Yet non-learning methods still excel in comprehensive medical image registration benchmarks and outperform metric-supervised approaches cf. https://{\{}curious2019{\}},{\{}empire10{\}},{\{}learn2reg{\}}.grand-challenge.org. We hence hypothesise that methods using displacement supervision have been outper-formed by metric-supervision despite the fact that conventional (non-learning) algorithms provide better results due to the diculty of training a deep network for voxel-wise regression , which are less robust to optimise than segmentation architectures. Contribution: We propose a novel loss and fitting function for optimising deep networks for inter-patient registration based on automatically deformed and thus anatomically consistent supervoxel segmentations as training data. Deformable registration is thus solved using segmentation of a reasonably small number of supervoxel classes instead of direct regression of transformation parameters. The displacement field is then indirectly estimated through the correspondence of the predicted supervoxels. Related Work: Our work is related to the idea of pairwise supervoxel classification forest (Kanavati et al., 2017), which is however based on handcrafted context features and does neither employ displacements as supervision nor deep CNNs for predictions. Similarly, (Heinrich et al., 2013) used supervoxel alignment to register CT and MRI scans and provided the concept of complementary layers of over-segmentations, but without any learning.},
annote = {E-4

video: https://2021.midl.io/papers/e4

Strong disagreement with abstract: current deep learning based registration does not require manual supervision!

Method:
registration through segmentation: segment abstract "supervoxels" in both images, then calculate transformation fields from them.

Question:
- What are the advantages of the supervoxel-method over just learning to predict transformation fields obtained from the registration algorithm in a supervised setting?
- Can we use supervoxels without this registration step during dataset creation?},
author = {Heinrich, Mattias P},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinrich - 2021 - Rethinking the Design of Learning based Inter-Patient Registration using Deformable Supervoxels.pdf:pdf},
journal = {Medical Imaging with Deep Learning-Under Review},
keywords = {Supervoxel segmentation,inter-patient registration,noisy labels 1 Motivation / Related Work},
title = {{Rethinking the Design of Learning based Inter-Patient Registration using Deformable Supervoxels}},
year = {2021}
}
@article{Avants2011,
abstract = {The United States National Institutes of Health (NIH) commit significant support to open-source data and software resources in order to foment reproducibility in the biomedical imaging sciences. Here, we report and evaluate a recent product of this commitment: Advanced Neuroimaging Tools (ANTs), which is approaching its 2.0 release. The ANTs open source software library consists of a suite of state-of-the-art image registration, segmentation and template building tools for quantitative morphometric analysis. In this work, we use ANTs to quantify, for the first time, the impact of similarity metrics on the affine and deformable components of a template-based normalization study. We detail the ANTs implementation of three similarity metrics: squared intensity difference, a new and faster cross-correlation, and voxel-wise mutual information. We then use two-fold cross-validation to compare their performance on openly available, manually labeled, T1-weighted MRI brain image data of 40 subjects (UCLA's LPBA40 dataset). We report evaluation results on cortical and whole brain labels for both the affine and deformable components of the registration. Results indicate that the best ANTs methods are competitive with existing brain extraction results (Jaccard = 0.958) and cortical labeling approaches. Mutual information affine mapping combined with cross-correlation diffeomorphic mapping gave the best cortical labeling results (Jaccard = 0.669. ±. 0.022). Furthermore, our two-fold cross-validation allows us to quantify the similarity of templates derived from different subgroups. Our open code, data and evaluation scripts set performance benchmark parameters for this state-of-the-art toolkit. This is the first study to use a consistent transformation framework to provide a reproducible evaluation of the isolated effect of the similarity metric on optimal template construction and brain labeling. {\textcopyright} 2010 Elsevier Inc.},
author = {Avants, Brian B. and Tustison, Nicholas J. and Song, Gang and Cook, Philip A. and Klein, Arno and Gee, James C.},
doi = {10.1016/j.neuroimage.2010.09.025},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Avants et al. - 2011 - A reproducible evaluation of ANTs similarity metric performance in brain image registration.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
month = {feb},
number = {3},
pages = {2033--2044},
pmid = {20851191},
publisher = {Academic Press},
title = {{A reproducible evaluation of ANTs similarity metric performance in brain image registration}},
volume = {54},
year = {2011}
}
@article{Liu2015,
abstract = {We present a common framework, for registering images to an atlas and for forming an unbiased atlas, that tolerates the presence of pathologies such as tumors and traumatic brain injury lesions. This common framework is particularly useful when a sufficient number of protocol-matched scans from healthy subjects cannot be easily acquired for atlas formation and when the pathologies in a patient cause large appearance changes. Our framework combines a low-rank-plus-sparse image decomposition technique with an iterative, diffeomorphic, group-wise image registration method. At each iteration of image registration, the decomposition technique estimates a 'healthy' version of each image as its low-rank component and estimates the pathologies in each image as its sparse component. The healthy version of each image is used for the next iteration of image registration. The low-rank and sparse estimates are refined as the image registrations iteratively improve. For unbiased atlas formation, at each iteration, the average of the low-rank images from the patients is used as the atlas image for the next iteration, until convergence. Since each iteration's atlas is comprised of low-rank components, it provides a population-consistent, pathology-free appearance. Evaluations of the proposed methodology are presented using synthetic data as well as simulated and clinical tumor MRI images from the brain tumor segmentation (BRATS) challenge from MICCAI 2012.},
annote = {extension of "Low-Rank to the Rescue – Atlas-Based Analyses in the Presence of Pathologies", for use in atlas creation

Basic framework is the same. We create a common, low rank representation of all images, then use that to form an atlas. Proces sused iteratively, to improve the common representation of registered images step-by-step},
author = {Liu, Xiaoxiao and Niethammer, Marc and Kwitt, Roland and Singh, Nikhil and McCormick, Matt and Aylward, Stephen},
doi = {10.1109/TMI.2015.2448556},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2015 - Low-Rank Atlas Image Analyses in the Presence of Pathologies.pdf:pdf},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Atlas-based segmentation,Low-rank and sparse decomposition,sparse images,unbiased atlas},
month = {dec},
number = {12},
pages = {2583--2591},
pmid = {26111390},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Low-Rank Atlas Image Analyses in the Presence of Pathologies}},
url = {https://sfx.aub.aau.dk/sfxaub?atitle=Low-rank+atlas+image+analyses+in+the+presence+of+pathologies{\&}auinit=X{\&}aulast=Liu{\&}date=2015{\&}id=doi{\%}3A10.1109{\%}2FTMI.2015.2448556{\&}issn=0278-0062{\&}issue=12{\&}req.language=eng{\&}sid=google{\&}spage=2583{\&}title=IEEE+Transactions+on+M},
volume = {34},
year = {2015}
}
@techreport{Nielsen2020a,
abstract = {Normalizing flows and variational autoencoders are powerful generative models that can represent complicated density functions. However, they both impose constraints on the models: Normalizing flows use bijective transformations to model densities whereas VAEs learn stochastic transformations that are non-invertible and thus typically do not provide tractable estimates of the marginal likelihood. In this paper, we introduce SurVAE Flows: A modular framework of composable transformations that encompasses VAEs and normalizing flows. SurVAE Flows bridge the gap between normalizing flows and VAEs with surjective transformations, wherein the transformations are deterministic in one direction-thereby allowing exact likelihood computation, and stochastic in the reverse direction-hence providing a lower bound on the corresponding likelihood. We show that several recently proposed methods, including dequantization and augmented normalizing flows, can be expressed as SurVAE Flows. Finally, we introduce common operations such as the max value, the absolute value, sorting and stochastic permutation as composable layers in SurVAE Flows.},
annote = {domain transformations in normalizing flows, modelled with VAEs

Can probably be applied to image registration

x $\backslash$in $\backslash$domain{\_}S
y $\backslash$in$\backslash$ domain{\_}T

p(y|x)
q(x|y)

the smoothness of the transformation is not guaranteed with these properties},
author = {Nielsen, Didrik and Jaini, Priyank and Hoogeboom, Emiel and Winther, Ole and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielsen et al. - 2020 - SurVAE Flows Surjections to Bridge the Gap between VAEs and Flows.pdf:pdf},
pages = {12685--12696},
title = {{SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows}},
url = {https://github.com/didriknielsen/survae{\_}flows},
volume = {33},
year = {2020}
}
@article{Touvron2021,
abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1{\%} (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a con-vnet teacher. The learned transformers are competitive (85.2{\%} top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
annote = {[4] in image transformer reading group

More data-efficient version of ViT
lots of well tested training optimizations (architecture, augmentation, initialization, optimizer, teacher model)

Improvments over ViT:
- no huge, extra training set
- reasonable computational cost through teacher network, but requires a CNN teacher network
- training at reduced resolution (224), finetuning at full resolution (384)
- initialization of weights via truncated normal distribution
- large amount of data augmentation
- no dropout, no batch norm (allows for easier scaling of batch-size)
- weight decay (0.03)
- AdamW optimizer, 1x10{\^{}}-4 lr


Soft destillation: KL-divergence between teacher and student softmax class prediction
Hard destillation: treat prediction of teacher as hard label, 1-hot
Label smoothing: smoth out class probabilities slightly},
author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Touvron et al. - 2021 - Training data-efficient image transformers {\&} distillation through attention.pdf:pdf},
issn = {2640-3498},
journal = {Proceedings of Machine Learning Research},
keywords = {ICML,Machine Learning},
month = {jul},
pages = {10347--10357},
publisher = {PMLR},
title = {{Training data-efficient image transformers {\&} distillation through attention}},
url = {https://proceedings.mlr.press/v139/touvron21a.html},
volume = {139},
year = {2021}
}
@article{Davatzikos1997,
abstract = {The development of algorithms for the spatial transformation and registration of tomographic brain images is a key issue in several clinical and basic science medical applications, including computer-aided neurosurgery, functional image analysis, and morphometrics. This paper describes a technique for the spatial transformation of brain images, which is based on elastically deformable models. A deformable surface algorithm is used to find a parametric representation of the outer cortical surface and then to define a map between corresponding cortical regions in two brain images. Based on the resulting map, a three-dimensional elastic warping transformation is then determined, which brings two images into register. This transformation models images as inhomogeneous elastic objects which are deformed into registration with each other by external force fields. The elastic properties of the images can vary from one region to the other, allowing more variable brain regions, such as the ventricles, to deform more freely than less variable ones. Finally, the framework of prestrained elasticity is used to model structural irregularities, and in particular the ventricular expansion occurring with aging or diseases, and the growth of tumors. Performance measurements are obtained using magnetic resonance images. {\textcopyright} 1997 Academic Press.},
annote = {elastic registration method},
author = {Davatzikos, Christos},
doi = {10.1006/CVIU.1997.0605},
issn = {1077-3142},
journal = {Computer Vision and Image Understanding},
month = {may},
number = {2},
pages = {207--222},
publisher = {Academic Press},
title = {{Spatial Transformation and Registration of Brain Images Using Elastically Deformable Models}},
volume = {66},
year = {1997}
}
@techreport{Wilcoxon1945,
annote = {statistical significance test work ranking different models},
author = {Wilcoxon, Frank},
booktitle = {Biometrics Bulletin},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilcoxon - 1945 - Individual Comparisons by Ranking Methods.pdf:pdf},
number = {6},
pages = {80--83},
title = {{Individual Comparisons by Ranking Methods}},
url = {http://www.jstor.org/about/terms.html.},
volume = {1},
year = {1945}
}
@article{Czolbe2020a,
abstract = {We propose a semantic similarity metric for image registration. Existing metrics like euclidean distance or normalized cross-correlation focus on aligning intensity values, giving difficulties with low intensity contrast or noise. Our semantic approach learns dataset-specific features that drive the optimization of a learning-based registration model. Comparing to existing unsupervised and supervised methods across multiple image modalities and applications, we achieve consistently high registration accuracy and faster convergence than state of the art, and the learned invariance to noise gives smoother transformations on low-quality images.},
archivePrefix = {arXiv},
arxivId = {2011.05735},
author = {Czolbe, Steffen and Krause, Oswin and Feragen, Aasa},
eprint = {2011.05735},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czolbe, Krause, Feragen - 2020 - DeepSim Semantic similarity metrics for learned image registration.pdf:pdf},
journal = {NeurIPS 2020 workshop on Medical Imaging},
month = {nov},
title = {{DeepSim: Semantic similarity metrics for learned image registration}},
url = {http://arxiv.org/abs/2011.05735},
year = {2020}
}
@article{DeVos2019,
annote = {Traditional deep learning

{\~{}}2000 CT-scans (non-public NLST (The National Lung Screening Trial Research Team) dataset)
+ each possible pair of intra-patient data is used for training
+ data augmentation

Normal NN training

Fig 16: landmark errors

Resuklts:
Dirlab landmar error 2.64 ± 4.32},
archivePrefix = {arXiv},
arxivId = {1809.06130},
author = {de Vos, Bob D. and Berendsen, Floris F. and Viergever, Max A. and Sokooti, Hessam and Staring, Marius and I{\v{s}}gum, Ivana},
doi = {10.1016/j.media.2018.11.010},
eprint = {1809.06130},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Vos et al. - 2019 - A deep learning framework for unsupervised affine and deformable image registration.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Affine image registration,Cardiac cine MRI,Chest CT,Deep learning,Deformable image registration,Unsupervised learning},
month = {feb},
pages = {128--143},
publisher = {Elsevier B.V.},
title = {{A deep learning framework for unsupervised affine and deformable image registration}},
volume = {52},
year = {2019}
}
@inproceedings{Schnabel2001,
abstract = {This work presents a framework for non-rigid registration which extends and generalizes a previously developed technique by Rueckert et al. [1]. We combine multi-resolution optimization with free-form deformations (FFDs) based on multi-level B-splines to simulate a non-uniform control point distribution. We have applied this to a number of different medical registration tasks to demonstrate its wide applicability, including interventional MRI brain tissue deformation compensation, breathing motion compensation in liver MRI, intramodality inter-modality registration of pre-operative brain MRI to CT electrode implant data, and inter-subject registration of brain MRI. Our results demonstrate that the new algorithm can successfully register images with an improved performance, while achieving a significant reduction in run-time.},
author = {Schnabel, Julia A. and Rueckert, Daniel and Quist, Marcel and Blackall, Jane M. and Castellano-Smith, Andy D. and Hartkens, Thomas and Penney, Graeme P. and Hall, Walter A. and Liu, Haiying and Truwit, Charles L. and Gerritsen, Frans A. and Hill, Derek L.G. and Hawkes, David J.},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
doi = {10.1007/3-540-45468-3_69},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schnabel et al. - 2001 - A generic framework for non-rigid registration based on non-uniform multi-level free-form deformations.pdf:pdf},
isbn = {3540426973},
issn = {16113349},
month = {oct},
pages = {573--581},
publisher = {Springer Verlag},
title = {{A generic framework for non-rigid registration based on non-uniform multi-level free-form deformations}},
volume = {2208},
year = {2001}
}
@inproceedings{Trofimova,
annote = {2d to 3d affine probabilistic registration

Ambiguety / distribution of solutions is created very smartly:
- During training: invertible neural network maps from gt solution + input images -{\textgreater} gaussian latent
- During test: invertible neural network maps from gaussian latent + input images -{\textgreater} prediction. Sample gaussian latent to get distribution of predictions},
author = {Trofimova, Darya and Adler, Tim and Kausch, Lisa and Ardizzone, Lynton and Maier-Hein, Klaus and K{\"{o}}the, Ulrich and Rother, Carsten and Maier-Hein, Lena},
booktitle = {Medical Imaging Workshop at NeurIPS},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trofimova et al. - Unknown - Representing Ambiguity in Registration Problems with Conditional Invertible Neural Networks.pdf:pdf},
title = {{Representing Ambiguity in Registration Problems with Conditional Invertible Neural Networks}},
url = {https://github.com/VLL-HD/FrEIA},
year = {2020}
}
@article{rune2019TopAwaRe,
annote = {+ support toplology changing
- requires manual annotation of source of topological change
+ automatic expansion from annotated source

Proposes topologi-aware non-diffeomorphic registration

Evaluation:

MICCAI2012 database: 32 T1-weighted MRI 3D brain volumes. Annotated for anatomical regions.

all samples affine pre-registered to a common space

6 samples with expanded vertices identified

One group of all pairwhise registrations (32*31 = 992 pairs)

Other group of pairwhise registration between normal vertices and expanded vertices (26 * 6 = 156 pairs)

Manual annotation of ventricle discontinuety of one subject with contracted ventricles. Registered to all other subjects with closed ventrices. Used for later expansion via piecewhise-diffeomorphic approach.

Validation of diffeomorphic base-implementation: 
- method: global Dice overlap (a loss function for segmentation) of coregistered regions (segmentation annotation) of subject-to-subject registration
- result: our diffeomorphic code is in line with state of the art

Validation of piecewhise-diffeomorphic model:
- method 1: manual inspection of the (1) diffeomorphic and (2) piecewhise-diffeomorphic registration of 2 samples from contrated to expanded vertricles. Observation of the segmentation annotation for 5 classes locally present
- result 1: (1) streches out other classes than ventrices to fit. (2) expands ventricles from the discontenuety correctly.
- method 2: Dice overlaps of surrounding classes. comparison of mean and std between: 
a) Classes (Global, Ventricles, ...) 
b) samplegoups (all sample pairs, just pairs of contracted and expanded ventricle) 
c) Registration methods (ANTs reference, our diffeomorphic impl, our piecewhise diffeomorpic impl)
- result 2: statistically significant improvement for group of contracted and expanded ventricle pairs
- method 3: healthy to alzheimers patient from ADNI dataset. Inspection of warp grid.},
author = {Nielsen, Rune Kok and Darkner, Sune and Feragen, Aasa},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielsen, Darkner, Feragen - 2019 - TopAwaRe Topology-Aware Registration.pdf:pdf},
howpublished = {Unpublished work. Submitted to: $\backslash$textit{\{}International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI){\}}},
journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
keywords = {diffeomorphisms,image registration,topology-aware},
pages = {364----372},
title = {{TopAwaRe: Topology-Aware Registration}},
year = {2019}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
annote = {[1] of image transformer reading group

The original transformer paper
Great description of multi-head attention

Positional encoding is no langer used nowerdays (eg BERT does not use it)},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
doi = {10.5555/3295222},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vaswani et al. - 2017 - Attention Is All You Need.pdf:pdf},
journal = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000--6010},
title = {{Attention Is All You Need}},
year = {2017}
}
@article{Horn1981,
author = {Horn, Berthold KP and Schunck, Brian G},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horn, Schunck - 1981 - Determining optical flow.pdf:pdf},
journal = {Artificial intelligence},
pages = {185--203},
title = {{Determining optical flow}},
url = {https://dspace.mit.edu/bitstream/handle/1721.1/6337/AIM-572.pdf?sequence=2},
volume = {1},
year = {1981}
}
@incollection{Bhalodia2019,
abstract = {Spatial transformations are enablers in a variety of medical image analysis applications that entail aligning images to a common coordinate systems. Population analysis of such transformations is expected to capture the underlying image and shape variations, and hence these transformations are required to produce anatomically feasible correspondences. This is usually enforced through some smoothness-based generic regularization on deformation field. Alternatively, population-based regularization has been shown to produce anatomically accurate correspondences in cases where anatomically unaware (i.e., data independent) fail. Recently, deep networks have been for unsupervised image registration, these methods are computationally faster and maintains the accuracy of state of the art methods. However, these networks use smoothness penalty on deformation fields and ignores population-level statistics of the transformations. We propose a novel neural network architecture that simultaneously learns and uses the population-level statistics of the spatial transformations to regularize the neural networks for unsupervised image registration. This regularization is in the form of a bottleneck autoencoder, which encodes the population level information of the deformation fields in a low-dimensional manifold. The proposed architecture produces deformation fields that describe the population-level features and associated correspondences in an anatomically relevant manner and are statistically compact relative to the state-of-the-art approaches while maintaining computational efficiency. We demonstrate the efficacy of the proposed architecture on synthetic data sets, as well as 2D and 3D medical data.},
annote = {Unsupervised

Regularization performed by an Auto-Encoder

idea: a Auto-encoder with a low dimensionality latent space can only express simple, sensible transformations. It can further adapt to transformations present in the population it is trained on.},
archivePrefix = {arXiv},
arxivId = {1908.05825},
author = {Bhalodia, Riddhish and Elhabian, Shireen Y. and Kavan, Ladislav and Whitaker, Ross T.},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_44},
eprint = {1908.05825},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhalodia et al. - 2019 - A Cooperative Autoencoder for Population-Based Regularization of CNN Image Registration.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {391--400},
title = {{A Cooperative Autoencoder for Population-Based Regularization of CNN Image Registration}},
year = {2019}
}
@article{Brats2017,
abstract = {Gliomas belong to a group of central nervous system tumors, and consist of various sub-regions. Gold standard labeling of these sub-regions in radiographic imaging is essential for both clinical and computational studies, including radiomic and radiogenomic analyses. Towards this end, we release segmentation labels and radiomic features for all pre-operative multimodal magnetic resonance imaging (MRI) (n=243) of the multi-institutional glioma collections of The Cancer Genome Atlas (TCGA), publicly available in The Cancer Imaging Archive (TCIA). Pre-operative scans were identified in both glioblastoma (TCGA-GBM, n=135) and low-grade-glioma (TCGA-LGG, n=108) collections via radiological assessment. The glioma sub-region labels were produced by an automated state-of-the-art method and manually revised by an expert board-certified neuroradiologist. An extensive panel of radiomic features was extracted based on the manually-revised labels. This set of labels and features should enable i) direct utilization of the TCGA/TCIA glioma collections towards repeatable, reproducible and comparative quantitative studies leading to new predictive, prognostic, and diagnostic assessments, as well as ii) performance evaluation of computer-aided segmentation methods, and comparison to our state-of-the-art method.},
annote = {Brats 2/3},
author = {Bakas, Spyridon and Akbari, Hamed and Sotiras, Aristeidis and Bilello, Michel and Rozycki, Martin and Kirby, Justin S. and Freymann, John B. and Farahani, Keyvan and Davatzikos, Christos},
doi = {10.1038/sdata.2017.117},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakas et al. - 2017 - Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features.pdf:pdf},
issn = {20524463},
journal = {Scientific Data},
keywords = {Brain Neoplasms / diagnostic imaging,Brain Neoplasms / genetics*,Christos Davatzikos,Computer-Assisted,DNA,Dataset,Extramural,Glioma / diagnostic imaging,Glioma / genetics*,Hamed Akbari,Humans,Image Interpretation,MEDLINE,Magnetic Resonance Imaging,Multimodal Imaging,N.I.H.,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,Neoplasm*,PMC5685212,PubMed Abstract,Research Support,Spyridon Bakas,doi:10.1038/sdata.2017.117,pmid:28872634},
month = {sep},
pmid = {28872634},
publisher = {Nature Publishing Groups},
title = {{Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features}},
url = {https://pubmed.ncbi.nlm.nih.gov/28872634/},
volume = {4},
year = {2017}
}
@article{Vandemeulebroucke2007,
author = {Vandemeulebroucke, Jef and Sarrut, David and Clarysse, Patrick},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vandemeulebroucke et al. - 2007 - The POPI model, a pointvalidated pixel-based breathing thorax model.pdf:pdf},
journal = {XVth ICCR},
title = {{The POPI model, a pointvalidated pixel-based breathing thorax model}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.453.3112},
year = {2007}
}
@misc{Blake2014,
author = {Blake, Canan and Scanlon, Eileen},
doi = {10.4018/978-1-4666-5162-3.ch013},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blake, Scanlon - 2014 - Online Teaching.png:png},
pages = {186--200},
title = {{Online Teaching}},
year = {2014}
}
@article{Liu2014a,
abstract = {Low-rank image decomposition has the potential to address a broad range of challenges that routinely occur in clinical practice. Its novelty and utility in the context of atlas-based analysis stems from its ability to handle images containing large pathologies and large deformations. Potential applications include atlas-based tissue segmentation and unbiased atlas building from data containing pathologies. In this paper we present atlas-based tissue segmentation of MRI from patients with large pathologies. Specifically, a healthy brain atlas is registered with the low-rank components from the input MRIs, the low-rank components are then re-computed based on those registrations, and the process is then iteratively repeated. Preliminary evaluations are conducted using the brain tumor segmentation challenge data (BRATS '12). {\textcopyright} 2014 Springer International Publishing.},
annote = {Low-Rank Plus Sparse Decomposition:
split image stack D into components L, S, where L are the images but as a low-rand matrix, and sparse matrix S recods diffrences S = D - L

Good things:
-Sparse component S can be used to find lesions/abnormalities
- correction for pathologies integrated into registration
- iterative approach (Fig 1) can register even vastly deformed images very well (Fig 3a)
- can be used for atlas creation, see paper "Low-Rank Atlas Image Analyses in the Presence of Pathologies"

similarities to Spot the difference paper:
- Both methods can be used to find lesions (even though this is not the explicit intent of this paper)
- Both methods find differences on the cortical surface

Shortcomings: 
- blurrs normal tissue regions, limiting registration performance of common representation
- memory intensive, as matrix operations have to be performed on all images similtaniously
- no probabilistic formulation. Hard to interpret values
-can not register a single image. alsoways needs a batch for finding a common low-rank approximation
-typical shortcomings of non-learned models: iterative optimization, both for finding the low-rank representation, and for solving the registration problem},
author = {Liu, Xiaoxiao and Niethammer, Marc and Kwitt, Roland and McCormick, Matthew and Aylward, Stephen},
doi = {10.1007/978-3-319-10443-0_13},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2014 - Low-Rank to the Rescue – Atlas-Based Analyses in the Presence of Pathologies(2).pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 3},
pages = {97--104},
publisher = {Springer, Cham},
title = {{Low-Rank to the Rescue – Atlas-Based Analyses in the Presence of Pathologies}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-10443-0{\_}13},
volume = {8675 LNCS},
year = {2014}
}
@article{Czolbe2021d,
abstract = {Probabilistic image segmentation encodes varying prediction confidence and
inherent ambiguity in the segmentation problem. While different probabilistic
segmentation models are designed to capture different aspects of segmentation
uncertainty and ambiguity, these modelling differences are rarely discussed in
the context of applications of uncertainty. We consider two common use cases of
segmentation uncertainty, namely assessment of segmentation quality and active
learning. We consider four established strategies for probabilistic
segmentation, discuss their modelling capabilities, and investigate their
performance in these two tasks. We find that for all models and both tasks,
returned uncertainty correlates positively with segmentation error, but does
not prove to be useful for active learning.},
archivePrefix = {arXiv},
arxivId = {2103.16265},
author = {Czolbe, Steffen and Arnavaz, Kasra and Krause, Oswin and Feragen, Aasa},
eprint = {2103.16265},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Czolbe et al. - 2021 - Is segmentation uncertainty useful.pdf:pdf},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Active learning,Image segmentation,Uncertainty quantification},
month = {mar},
pages = {715--726},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Is segmentation uncertainty useful?}},
url = {https://arxiv.org/abs/2103.16265v1},
volume = {12729 LNCS},
year = {2021}
}
@incollection{Fidon2019,
abstract = {Anatomically plausible image registration often requires volumetric preservation. Previous approaches to incompressible image registration have exploited relaxed constraints, ad hoc optimisation methods or practically intractable computational schemes. Divergence-free velocity fields have been used to achieve incompressibility in the continuous domain, although, after discretisation, no guarantees have been provided. In this paper, we introduce stationary velocity fields (SVFs) parameterised by divergence-conforming B-splines in the context of image registration. We demonstrate that sparse linear constraints on the parameters of such divergence-conforming B-Splines SVFs lead to being exactly divergence-free at any point of the continuous spatial domain. In contrast to previous approaches, our framework can easily take advantage of modern solvers for constrained optimisation, symmetric registration approaches, arbitrary image similarity and additional regularisation terms. We study the numerical incompressibility error for the transformation in the case of an Euler integration, which gives theoretical insights on the improved accuracy error over previous methods. We evaluate the proposed framework using synthetically deformed multimodal brain images, and the STACOM11 myocardial tracking challenge. Accuracy measurements demonstrate that our method compares favourably with state-of-the-art methods whilst achieving volume preservation.},
annote = {Proposes an optimization technique for incompressible image registration (= volumes are preserved). Optimization based on a linear solver (with constraints) to determine a SVF based on b-splines},
archivePrefix = {arXiv},
arxivId = {1907.01593},
author = {Fidon, Lucas and Ebner, Michael and Garcia-Peraza-Herrera, Luis C. and Modat, Marc and Ourselin, S{\'{e}}bastien and Vercauteren, Tom},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_49},
eprint = {1907.01593},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fidon et al. - 2019 - Incompressible Image Registration Using Divergence-Conforming B-Splines.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {438--446},
title = {{Incompressible Image Registration Using Divergence-Conforming B-Splines}},
year = {2019}
}
@misc{fischl2012freesurfer,
author = {Fischl, B},
title = {{FreeSurfer. Neurolmage, 62 (2), 774--781}},
year = {2012}
}
@article{Venkatakrishnan2020,
annote = {VAE for anomaly detection



Bad work, but good introduction.},
author = {Venkatakrishnan, Abinav Ravi and Kim, Seong Tae and Eisawy, Rami and Pfister, Franz and Navab, Nassir},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Venkatakrishnan et al. - Unknown - Self-Supervised Out-of-Distribution Detection in Brain CT Scans.pdf:pdf},
journal = {Medical Imaging Meets NeurIPS Workshop},
title = {{Self-Supervised Out-of-Distribution Detection in Brain CT Scans}},
year = {2020}
}
@article{Cheng2018,
abstract = {An effective similarity measure for multi-modal images is crucial for medical image fusion in many clinical applications. The underlining correlation across modalities is usually too complex to be modelled by intensity-based statistical metrics. Therefore, approaches of learning a similarity metric are proposed in recent years. In this work, we propose a novel deep similarity learning method that trains a binary classifier to learn the correspondence of two image patches. The classification output is transformed to a continuous probability value, then used as the similarity score. Moreover, we propose to utilise multi-modal stacked denoising autoencoder to effectively pre-train the deep neural network. We train and test the proposed metric using sampled corresponding/non-corresponding computed tomography and magnetic resonance head image patches from a same subject. Comparison is made with two commonly used metrics: normalised mutual information and local cross correlation. The contributions of the multi-modal stacked denoising autoencoder and the deep structure of the neural network are also evaluated. Both the quantitative and qualitative results from the similarity ranking experiments show the advantage of the proposed metric for a highly accurate and robust similarity measure.},
annote = {supervised cross-modality learning},
author = {Cheng, Xi and Zhang, Li and Zheng, Yefeng},
doi = {10.1080/21681163.2015.1135299},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Zhang, Zheng - 2018 - Deep similarity learning for multimodal medical images.pdf:pdf},
issn = {2168-1163},
journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging {\&} Visualization},
keywords = {Multi-modal medical images,deep neural network,multi-modal denoising autoencoder,similarity metric learning},
month = {may},
number = {3},
pages = {248--252},
publisher = {Taylor and Francis Ltd.},
title = {{Deep similarity learning for multimodal medical images}},
url = {https://www.tandfonline.com/doi/full/10.1080/21681163.2015.1135299},
volume = {6},
year = {2018}
}
@article{Zhang2018,
abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
archivePrefix = {arXiv},
arxivId = {1801.03924},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
eprint = {1801.03924},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.pdf:pdf},
journal = {Conference on Computer Vision and Pattern Recognition},
month = {jan},
pages = {586--595},
publisher = {IEEE Computer Society},
title = {{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}},
url = {http://arxiv.org/abs/1801.03924},
year = {2018}
}
@article{Klein2009,
annote = {Evaluation of 14 classical registration algorithms.

Great overview of Datasets and Algorithms.},
author = {Klein, Arno and Andersson, Jesper and Ardekani, Babak A. and Ashburner, John and Avants, Brian and Chiang, Ming Chang and Christensen, Gary E. and Collins, D. Louis and Gee, James and Hellier, Pierre and Song, Joo Hyun and Jenkinson, Mark and Lepage, Claude and Rueckert, Daniel and Thompson, Paul and Vercauteren, Tom and Woods, Roger P. and Mann, J. John and Parsey, Ramin V.},
doi = {10.1016/j.neuroimage.2008.12.037},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klein et al. - 2009 - Evaluation of 14 nonlinear deformation algorithms applied to human brain MRI registration.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
month = {jul},
number = {3},
pages = {786--802},
title = {{Evaluation of 14 nonlinear deformation algorithms applied to human brain MRI registration}},
volume = {46},
year = {2009}
}
@article{Hansen2021,
abstract = {As in other areas of medical image analysis, e.g. semantic segmentation, deep learning is currently driving the development of new approaches for image registration. Multi-scale encoder-decoder network architectures achieve state-of-the-art accuracy on tasks such as intra-patient alignment of abdominal CT or brain MRI registration, especially when additional supervision, such as anatomical labels, is available. The success of these methods relies to a large extent on the outstanding ability of deep CNNs to extract descriptive visual features from the input images. In contrast to conventional methods, the explicit inclusion of geometric information plays only a minor role, if at all. In this work we take a look at an exactly opposite approach by investigating a deep learning framework for registration based solely on geometric features and optimisation. We combine graph convolutions with loopy belief message passing to enable highly accurate 3D point cloud registration. Our experimental validation is conducted on complex key-point graphs of inner lung structures, strongly outperforming dense encoder-decoder networks and other point set registration methods. Our code is publicly available at https://github.com/multimodallearning/deep-geo-reg.},
annote = {IPMI 2021: paper 296 registration oral

Method:
- Graph/Point cloud registration of point-annotations
- registers two sets of points, sets not same size
steps (iterative):
1. alginment with displacement vectors V (? unsure when transformation is applied)
2. feature extraction via Graph network (GCN)
3. candidate selection based on spatial closeness
4. regularization of displacements: adjacent nodes should have same displacement, L2 penalty
5. supervised training by 300 gground truth annotations

Questions / unclear:
- Discretization step: Did not fully understand. Doesn't this bring displacements back into a voxel-structure?
- unclear: How/When are the displacement vectors V applied during iterative optimization?

Discussion:
- why mismatch in point cloud size: moving point clud 3x larger than fixed point cloud
- Does this work on non-lung data?
- Large displacements: CNNs bad compared to traditional registration (Ants, Syn) and graph/keypoiny based method. Why?
- CLARIFY/READ TEXT: is supervised annotation only used for evaluation? Presented method uses supervised information in 300 hand-placed keypoints. Can intensity-based registration methods reach similar results with sparse gt transformation fields?
- How good are unsupervised methods of obtaining keypoints? In the Lung domain, datasets of hand-annotated keypoints are redily available, but not for most other domains
- Deep-learning/registration with little data: paper uses 34 training, 1 validation split (CV)},
archivePrefix = {arXiv},
arxivId = {2103.00885},
author = {Hansen, Lasse and Heinrich, Mattias P.},
eprint = {2103.00885},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hansen, Heinrich - 2021 - Deep learning based geometric registration for medical images How accurate can we get without visual features.pdf:pdf},
journal = {Information Processing in Medical Imaging},
keywords = {belief prop-,deformable registration,geometric learning},
title = {{Deep learning based geometric registration for medical images: How accurate can we get without visual features?}},
url = {http://arxiv.org/abs/2103.00885},
year = {2021}
}
@article{Wang2021,
abstract = {Image registration as an important basis in signal processing task often encounter the problem of stability and efficiency. Non-learning registration approaches rely on the optimization of the similarity metrics between the fix and moving images. Yet, those approaches are usually costly in both time and space complexity. The problem can be worse when the size of the image is large or the deformations between the images are severe. Recently, deep learning, or precisely saying, the convolutional neural network (CNN) based image registration methods have been widely investigated in the research community and show promising effectiveness to overcome the weakness of non-learning based methods. To explore the advanced learning approaches in image registration problem for solving practical issues, we present in this paper a method of introducing attention mechanism in deformable image registration problem. The proposed approach is based on learning the deformation field with a Transformer framework (AiR) that does not rely on the CNN but can be efficiently trained on GPGPU devices also. In a more vivid interpretation: we treat the image registration problem as the same as a language translation task and introducing a Transformer to tackle the problem. Our method learns an unsupervised generated deformation map and is tested on two benchmark datasets. The source code of the AiR will be released at Gitlab.},
annote = {Vision transformer for image registration
limited to toy datasets (mnist, fashion mnist)

Attention map is the transformation

Room for improvement
- real dataset
- larger model (they use 1 encoder/decoder block, 4 attention heads)
- use of a regularizer},
archivePrefix = {arXiv},
arxivId = {2105.02282},
author = {Wang, Zihao and Delingette, Herv{\'{e}}},
eprint = {2105.02282},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Delingette - 2021 - Attention for Image Registration (AiR) an unsupervised Transformer approach.pdf:pdf},
keywords = {Deep,Images,Learning,Registration {\textperiodcentered},Transformer {\textperiodcentered}},
month = {may},
title = {{Attention for Image Registration (AiR): an unsupervised Transformer approach}},
url = {http://arxiv.org/abs/2105.02282},
year = {2021}
}
@article{pai2016diffeomorphic,
abstract = {In this paper, we propose a multi-scale, multi-kernel shape, compactly supported kernel bundle framework for stationary velocity field-based image registration (Wendland kernel bundle stationary velocity field, wKB-SVF). We exploit the possibility of directly choosing kernels to construct a reproducing kernel Hilbert space (RKHS) instead of imposing it from a differential operator. The proposed framework allows us to minimize computational cost without sacrificing the theoretical foundations of SVF-based diffeomorphic registration. In order to recover deformations occurring at different scales, we use compactly supported Wendland kernels at multiple scales and orders to parameterize the velocity fields, and the framework allows simultaneous optimization over all scales. The performance of wKB-SVF is extensively compared to the 14 non-rigid registration algorithms presented in a recent comparison paper. On both MGH10 and CUMC12 datasets, the accuracy of wKB-SVF is improved when compared to other registration algorithms. In a disease-specific application for intra-subject registration, atrophy scores estimated using the proposed registration scheme separates the diagnostic groups of Alzheimer's and normal controls better than the state-of-the-art segmentation technique. Experimental results show that wKB-SVF is a robust, flexible registration framework that allows theoretically well-founded and computationally efficient multi-scale representation of deformations and is equally well-suited for both inter-and intra-subject image registration.},
annote = {difeomorphic registration has 3 subfields:
1) large deformation diffeomorpic metric mapping (LDDMM)
2) freeform deformation
3) Stationary velocity fields (SVFs)},
author = {Pai, Akshay and Sommer, Stefan and Sorensen, Lauge and Darkner, Sune and Sporring, Jon and Nielsen, Mads},
doi = {10.1109/TMI.2015.2511062},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pai et al. - 2016 - Kernel Bundle Diffeomorphic Image Registration Using Stationary Velocity Fields and Wendland Basis Functions.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Kernel bundle framework,registration,reproducing kernel hilbert spaces,wendland kernels},
month = {jun},
number = {6},
pages = {1369--1380},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Kernel Bundle Diffeomorphic Image Registration Using Stationary Velocity Fields and Wendland Basis Functions}},
volume = {35},
year = {2016}
}
@article{Delmon2013,
abstract = {Sliding motion is a challenge for deformable image registration because it leads to discontinuities in the sought deformation. In this paper, we present a method to handle sliding motion using multiple B-spline transforms. The proposed method decomposes the sought deformation into sliding regions to allow discontinuities at their interfaces, but prevents unrealistic solutions by forcing those interfaces to match. The method was evaluated on 16 lung cancer patients against a single B-spline transform approach and a multi B-spline transforms approach without the sliding constraint at the interface. The target registration error (TRE) was significantly lower with the proposed method (TRE = 1.5 mm) than with the single B-spline approach (TRE = 3.7 mm) and was comparable to the multi B-spline approach without the sliding constraint (TRE = 1.4 mm). The proposed method was also more accurate along region interfaces, with 37{\%} less gaps and overlaps when compared to the multi B-spline transforms without the sliding constraint.},
author = {Delmon, V and Rit, S and Pinho, R and Sarrut, D},
doi = {10.1088/0031-9155/58/5/1303},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Delmon et al. - 2013 - Registration of sliding objects using direction dependent B-splines decomposition Registration of sliding objects.pdf:pdf},
journal = {Phys. Med. Bio},
number = {5},
pages = {1303--1314},
title = {{Registration of sliding objects using direction dependent B-splines decomposition Registration of sliding objects using direction dependent B-splines decomposition *}},
url = {http://iopscience.iop.org/0031-9155/58/5/1303},
volume = {58},
year = {2013}
}
@article{Bajcsy1989,
abstract = {Matching of locally variant data to an explicit 3-dimensional pictorial model is developed for X-ray computed tomography scans of the human brain, where the model is a voxel representation of an anatomical human brain atlas. The matching process is 3-dimensional without any preference given to the slicing plane. After global alignment the brain atlas is deformed like a piece of rubber, without tearing or folding. Deformation proceeds step-by-step in a coarse-to-fine strategy, increasing the local similarity and global coherence. The assumption underlying this approach is that all normal brains, at least at a certain level of representation, have the same topological structure, but may differ in shape details. Results show that we can account for these differences.},
annote = {elastic registration method},
author = {Bajcsy, Ruzena and Kova{\v{c}}i{\v{c}}, Stane},
doi = {10.1016/S0734-189X(89)80014-3},
issn = {0734-189X},
journal = {Computer Vision, Graphics, and Image Processing},
month = {apr},
number = {1},
pages = {1--21},
publisher = {Academic Press},
title = {{Multiresolution elastic matching}},
volume = {46},
year = {1989}
}
@article{Peyre2019,
abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746–1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total e ort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions—two di erent piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a “global” cost to every such transport, using the “local” consideration of how much it costs to move a grain of sand from one place to another. Mathematicians are interested in the properties of that least costly transport, as well as in its e cient computation. That smallest cost not only defines a distance between distributions, but it also entails a rich geometric structure on the space of probability distributions. That structure is canonical in the sense that it borrows key geometric properties of the underlying “ground” space on which these distributions are defined. For instance, when the underlying space is Euclidean, key concepts such as interpolation, barycenters, convexity or gradients of functions extend naturally to the space of distributions endowed with an OT geometry. OT has been (re)discovered in many settings and under different forms, giving it a rich history. While Monge's seminal work was motivated by an engineering problem, Tolstoi in the 1920s and Hitchcock, Kantorovich and Koopmans in the 1940s established its significance to logistics and economics. Dantzig solved it numerically in 1949 within the framework of linear programming, giving OT a firm footing in optimization. OT was later revisited by analysts in the 1990s, notably Brenier, while also gaining fame in computer vision under the name of earth mover's distances. Recent years have witnessed yet another revolution in the spread of OT, thanks to the emergence of approximate solvers that can scale to large problem dimensions. As a consequence, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), graphics (for shape manipulation) or machine learning (for regression, classification and generative modeling). This paper reviews OT with a bias toward numerical methods, and covers the theoretical properties of OT that can guide the design of new algorithms.We focus in particular on the recent wave of efficient algorithms that have helped OT find relevance in data sciences. We give a prominent place to the many generalizations of OT that have been proposed in but a few years, and connect them with related approaches originating from statistical inference, kernel methods and information theory. All of the figures can be reproduced using code made available in a companion website1. This website hosts the book project Computational Optimal Transport. You will also find slides and computational resources.},
archivePrefix = {arXiv},
arxivId = {1803.00567},
author = {Peyr{\'{e}}, Gabriel and Cuturi, Marco},
doi = {10.1561/2200000073},
eprint = {1803.00567},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Peyr{\'{e}}, Cuturi - 2019 - Computational optimal transport.pdf:pdf},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
keywords = {Classification and prediction,Clustering,Machine Learning,Optimization},
number = {5-6},
pages = {1--257},
publisher = {Now Publishers Inc},
title = {{Computational optimal transport}},
url = {http://dx.doi.org/10.1561/2200000073},
volume = {11},
year = {2019}
}
@article{Shen2002,
abstract = {A new approach is presented for elastic registration of medical images, and is applied to magnetic resonance images of the brain. Experimental results demonstrate very high accuracy in superposition of images from different subjects. There are two major novelties in the proposed algorithm. First, it uses an attribute vector, i.e., a set of geometric moment invariants (GMIs) that are defined on each voxel in an image and are calculated from the tissue maps, to reflect the underlying anatomy at different scales. The attribute vector, if rich enough, can distinguish between different parts of an image, which helps establish anatomical correspondences in the deformation procedure; it also helps reduce local minima, by reducing ambiguity in potential matches. This is a fundamental deviation of our method, referred to as the hierarchical attribute matching mechanism for elastic registration (HAMMER), from other volumetric deformation methods, which are typically based on maximizing image similarity. Second, in order to avoid being trapped by local minima, i.e., suboptimal poor matches, HAMMER uses a successive approximation of the energy function being optimized by lower dimensional smooth energy functions, which are constructed to have significantly fewer local minima. This is achieved by hierarchically selecting the driving features that have distinct attribute vectors, thus, drastically reducing ambiguity in finding correspondence. A number of experiments demonstrate that the proposed algorithm results in accurate superposition of image data from individuals with significant anatomical differences.},
annote = {elastic registration method},
author = {Shen, Dinggang and Davatzikos, Christos},
doi = {10.1109/TMI.2002.803111},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Attribute vectors,Average brain,Deformable registration,Geometric moment invariants,Hierarchical deformation mechanism,Multigrid formulation,Statistical atlases},
month = {nov},
number = {11},
pages = {1421--1439},
title = {{HAMMER: Hierarchical attribute matching mechanism for elastic registration}},
volume = {21},
year = {2002}
}
@article{Kist2021,
annote = {C-8

video: https://2021.midl.io/papers/c8},
author = {Kist, Andreas M and Zilker, Julian and D{\"{o}}llinger, Michael and Semmler, Marion},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kist et al. - 2021 - Feature-based image registration in structured light endoscopy.pdf:pdf},
journal = {Medical Imaging with Deep Learning},
keywords = {deformation field,endoscopy,image registration,structured light},
pages = {338--352},
title = {{Feature-based image registration in structured light endoscopy}},
url = {https://openreview.net/forum?id=MzC8X6cMF2r},
year = {2021}
}
@article{Hansen2020,
abstract = {Though, deep learning based medical image registration is currently starting to show promising advances, often, it still fells behind conventional frameworks in terms of registration accuracy. This is especially true for applications where large deformations exist, such as registration of interpatient abdominal MRI or inhale-to-exhale CT lung registration. Most current works use U-Net-like architectures to predict dense displacement fields from the input images in different supervised and unsupervised settings. We believe that the U-Net architecture itself to some level limits the ability to predict large deformations (even when using multilevel strategies) and therefore propose a novel approach, where the input images are mapped into a displacement space and final registrations are reconstructed from this embedding. Experiments on inhale-to-exhale CT lung registration demonstrate the ability of our architecture to predict large deformations in a single forward path through our network (leading to errors below 2 mm).},
archivePrefix = {arXiv},
arxivId = {2005.13338},
author = {Hansen, Lasse and Heinrich, Mattias P.},
eprint = {2005.13338},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hansen, Heinrich - 2020 - Tackling the Problem of Large Deformations in Deep Learning Based Medical Image Registration Using Displacemen.pdf:pdf},
journal = {Medical Imaging with Deep Learning},
keywords = {convolutional neural networks,deformable image registration,thoracic CT},
month = {may},
title = {{Tackling the Problem of Large Deformations in Deep Learning Based Medical Image Registration Using Displacement Embeddings}},
url = {http://arxiv.org/abs/2005.13338},
year = {2020}
}
@book{Gjerris2018,
abstract = {Since 2011 it has been mandatory for all new PhD students at the University of Copenhagen to take a course in RCR (Responsible Conduct of Research). The present book will serve as a textbook for the courses held at the Faculty of Science and at the Faculty of Health and Medical Sciences. The book thus aims to give an accessible presentation of what PhD-students are supposed to learn about RCR; to present a clear and consistent terminology; and to focus on the way RCR is dealt with in Denmark and at the University of Copenhagen. The intended readers are from two faculties where the large majority of research falls under the umbrella of the natural sciences, broadly construed. The book can also be of use to other scientific staff at the University.},
author = {Gjerris, Mickey and Jensen, Karsten Klint and H{\o}j, Jeppe Berggreen and Whiteley*, Louise},
booktitle = {RCR - A Danish texbook for courses in Responsible Conduct of Research},
isbn = {978-87-92591-62-3},
title = {{RCR - A Danish texbook for courses in Responsible Conduct of Research}},
url = {http://www.ifro.ku.dk/rcr.pdf/},
year = {2018}
}
@techreport{An2015,
abstract = {We propose an anomaly detection method using the reconstruction probability from the variational autoencoder. The reconstruction probability is a probabilistic measure that takes into account the variability of the distribution of variables. The reconstruction probability has a theoretical background making it a more principled and objective anomaly score than the reconstruction error, which is used by autoencoder and principal components based anomaly detection methods. Experimental results show that the proposed method outper-forms autoencoder based and principal components based methods. Utilizing the generative characteristics of the variational autoencoder enables deriving the reconstruction of the data to analyze the underlying cause of the anomaly.},
author = {An, Jinwon and Cho, Sungzoon},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/An, Cho - 2015 - SNU Data Mining Center 2015-2 Special Lecture on IE Variational Autoencoder based Anomaly Detection using Reconstructio.pdf:pdf},
title = {{Variational Autoencoder based Anomaly Detection using Reconstruction Probability}},
year = {2015}
}
@article{Haskins2019,
abstract = {The establishment of image correspondence through robust image registration is critical to many clinical tasks such as image fusion, organ atlas creation, and tumor growth monitoring, and is a very challenging problem. Since the beginning of the recent deep learning renaissance, the medical imaging research community has developed deep learning based approaches and achieved the state-of-the-art in many applications, including image registration. The rapid adoption of deep learning for image registration applications over the past few years necessitates a comprehensive summary and outlook, which is the main scope of this survey. This requires placing a focus on the different research areas as well as highlighting challenges that practitioners face. This survey, therefore, outlines the evolution of deep learning based medical image registration in the context of both research challenges and relevant innovations in the past few years. Further, this survey highlights future research directions to show how this field may be possibly moved forward to the next level.},
annote = {Review paper, overview over lots of papers},
archivePrefix = {arXiv},
arxivId = {1903.02026},
author = {Haskins, Grant and Kruger, Uwe and Yan, Pingkun},
eprint = {1903.02026},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haskins, Kruger, Yan - 2019 - Deep Learning in Medical Image Registration A Survey.pdf:pdf},
month = {mar},
title = {{Deep Learning in Medical Image Registration: A Survey}},
url = {http://arxiv.org/abs/1903.02026},
year = {2019}
}
@incollection{CollFont2019,
annote = {Kidney registration based on alignment of K-means clusters. The presented method makes 4D medical images more consistent, leading to an improved diagnosis in downstream tasks.

Very medical-application focused},
author = {Coll-Font, Jaume and Afacan, Onur and Chow, Jeanne and Kurugol, Sila},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_48},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Coll-Font et al. - 2019 - Linear Time Invariant Model Based Motion Correction (LiMo-MoCo) of Dynamic Radial Contrast Enhanced MRI.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {430--437},
title = {{Linear Time Invariant Model Based Motion Correction (LiMo-MoCo) of Dynamic Radial Contrast Enhanced MRI}},
year = {2019}
}
@inproceedings{Risholm2010,
abstract = {We present a 3D extension and validation of an intra-operative registration framework that accommodates tissue resection. The framework is based on the bijective Demons method, but instead of regularizing with the traditional Gaussian smoother, we apply an anisotropic diffusion filter with the resection modeled as a diffusion sink. The diffusion sink prevents unwanted Demon forces that originates from the resected area from diffusing into the surrounding area. Another attractive property of the diffusion sink is the resulting continuous deformation field across the diffusion sink boundary, which allows us to move the boundary of the diffusion sink without changing values in the deformation field. The area of resection is estimated by a level-set method evolving in the space of image intensity disagreements in the intra-operative image domain. A product of using the bijective Demons method is that we can also provide an accurate estimate of the resected tissue in the preoperative image space. Validation of the proposed method was performed on a set of 25 synthetic images. Our experiments show a significant improvement in accommodating resection using the proposed method compared to two other Demons based methods. {\textcopyright} 2010 Copyright SPIE - The International Society for Optical Engineering.},
annote = {Deformable image registration using sources and sinks

Requires the planned incision to be marked in one image (full area annotation). Then alternating, iterative optimization of (1) expansion/movement of the corresponding resection area on the other image(2) the transformation

Uses a anisotropic smoother (similar to gaussian smoothing) for regularization. Does some modifications to allow the transform to ony move into the resection zone (2.2.2 Diffusion sink)

Very similar to Ruunes approach, but instead of a seed that is expanded, we are marking and aligning an area of resection (the topological difference)},
author = {Risholm, Petter and Samset, Eigil and {Wells III}, William},
booktitle = {Medical Imaging 2010: Image Processing},
doi = {10.1117/12.844302},
editor = {Dawant, Benoit M. and Haynor, David R.},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Risholm, Samset, Wells III - 2010 - Validation of a nonrigid registration framework that accommodates tissue resection.pdf:pdf},
keywords = {non-rigid registration,resection,validation},
month = {mar},
pages = {762319},
publisher = {SPIE},
title = {{Validation of a nonrigid registration framework that accommodates tissue resection}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.844302},
volume = {7623},
year = {2010}
}
@article{Risser2013,
abstract = {In this paper, we propose a new strategy for modelling sliding conditions when registering 3D images in a piecewise-diffeomorphic framework. More specifically, our main contribution is the development of a mathematical formalism to perform Large Deformation Diffeomorphic Metric Mapping registration with sliding conditions. We also show how to adapt this formalism to the LogDemons diffeomorphic registration framework. We finally show how to apply this strategy to estimate the respiratory motion between 3D CT pulmonary images. Quantitative tests are performed on 2D and 3D synthetic images, as well as on real 3D lung images from the MICCAI EMPIRE10 challenge. Results show that our strategy estimates accurate mappings of entire 3D thoracic image volumes that exhibit a sliding motion, as opposed to conventional registration methods which are not capable of capturing discontinuous deformations at the thoracic cage boundary. They also show that although the deformations are not smooth across the location of sliding conditions, they are almost always invertible in the whole image domain. This would be helpful for radiotherapy planning and delivery. {\textcopyright} 2012.},
annote = {Aasa: "an instrumental paper of how we think about sliding motions"},
author = {Risser, Laurent and Vialard, Fran{\c{c}}ois Xavier and Baluwala, Habib Y. and Schnabel, Julia A.},
doi = {10.1016/j.media.2012.10.001},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Risser et al. - 2013 - Piecewise-diffeomorphic image registration Application to the motion estimation between 3D CT lung images with sl.pdf:pdf},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Diffeomorphic registration,LDDMM,LogDemons,Respiratory motion,Sliding motion},
month = {feb},
number = {2},
pages = {182--193},
title = {{Piecewise-diffeomorphic image registration: Application to the motion estimation between 3D CT lung images with sliding conditions}},
volume = {17},
year = {2013}
}
@article{Han2017,
abstract = {Registration involving one or more images containing pathologies is challenging, as standard image similarity measures and spatial transforms cannot account for common changes due to pathologies. Low-rank/Sparse (LRS) decomposition removes pathologies prior to registration; however, LRS is memory-demanding and slow, which limits its use on larger data sets. Additionally, LRS blurs normal tissue regions, which may degrade registration performance. This paper proposes an efficient alternative to LRS: (1) normal tissue appearance is captured by principal component analysis (PCA) and (2) blurring is avoided by an integrated model for pathology removal and image reconstruction. Results on synthetic and BRATS 2015 data demonstrate its utility.},
annote = {Like Low Rank plus sparse decomposition, but using PCS
Splits image into quasi-normal part (dominant pca components) and abnormal part (rest)

works very well for separating the tumor and filling the hole (Fig 6)},
author = {Han, Xu and Yang, Xiao and Aylward, Stephen and Kwitt, Roland and Niethammer, Marc},
doi = {10.1109/ISBI.2017.7950456},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2017 - Efficient registration of pathological images A joint PCAimage-reconstruction approach.pdf:pdf},
isbn = {1-5090-1173-0},
issn = {1945-7928},
journal = {4th International Symposium on Biomedical Imaging},
month = {jun},
pages = {10--14},
pmid = {29887971},
publisher = {IEEE Computer Society},
title = {{Efficient registration of pathological images: A joint PCA/image-reconstruction approach}},
url = {https://sfx.aub.aau.dk/sfxaub?atitle=Efficient+registration+of+pathological+images{\%}3A+a+joint+PCA{\%}2Fimage-reconstruction+approach{\&}auinit=X{\&}aulast=Han{\&}id=doi{\%}3A10.1109{\%}2FISBI.2017.7950456{\&}req.language=eng{\&}sid=google},
volume = {2017},
year = {2017}
}
@article{Parisot2014,
abstract = {In this paper, we present a graph-based concurrent brain tumor segmentation and atlas to diseased patient registration framework. Both segmentation and registration problems are modeled using a unified pairwise discrete Markov Random Field model on a sparse grid superimposed to the image domain. Segmentation is addressed based on pattern classification techniques, while registration is performed by maximizing the similarity between volumes and is modular with respect to the matching criterion. The two problems are coupled by relaxing the registration term in the tumor area, corresponding to areas of high classification score and high dissimilarity between volumes. In order to overcome the main shortcomings of discrete approaches regarding appropriate sampling of the solution space as well as important memory requirements, content driven samplings of the discrete displacement set and the sparse grid are considered, based on the local segmentation and registration uncertainties recovered by the min marginal energies. State of the art results on a substantial low-grade glioma database demonstrate the potential of our method, while our proposed approach shows maintained performance and strongly reduced complexity of the model. {\textcopyright} 2014 Elsevier B.V..},
author = {Parisot, Sarah and Wells, William and Chemouny, St{\'{e}}phane and Duffau, Hugues and Paragios, Nikos},
doi = {10.1016/j.media.2014.02.006},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parisot et al. - 2014 - Concurrent tumor segmentation and registration with uncertainty-based sparse non-uniform graphs.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Brain tumors,Concurrent segmentation/registration,Markov Random Fields,Min-marginals},
number = {4},
pages = {647--659},
publisher = {Elsevier},
title = {{Concurrent tumor segmentation and registration with uncertainty-based sparse non-uniform graphs}},
volume = {18},
year = {2014}
}
@inproceedings{Parisot2013,
abstract = {Graph-based methods have become popular in recent years and have successfully addressed tasks like segmen-tation and deformable registration. Their main strength is optimality of the obtained solution while their main limitation is the lack of precision due to the grid-like representations and the discrete nature of the quantized search space. In this paper we introduce a novel approach for combined segmentation/registration of brain tumors that adapts graph and sampling resolution according to the image content. To this end we estimate the segmentation and registration marginals towards adaptive graph resolution and intelligent definition of the search space. This information is considered in a hierarchical framework where uncertainties are propagated in a natural manner. State of the art results in the joint segmentation/registration of brain images with low-grade gliomas demonstrate the potential of our approach.},
author = {Parisot, Sarah and Iii, William Wells and Chemouny, St{\'{e}}phane and Duffau, Hugues and Paragios, Nikos},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2013.85},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Parisot et al. - 2013 - Uncertainty-driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Regis.pdf:pdf},
pages = {641--648},
title = {{Uncertainty-driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration}},
year = {2013}
}
@article{Han2018,
abstract = {Brain extraction from 3D medical images is a common pre-processing step. A variety of approaches exist, but they are frequently only designed to perform brain extraction from images without strong pathologies. Extracting the brain from images exhibiting strong pathologies, for example, the presence of a brain tumor or of a traumatic brain injury (TBI), is challenging. In such cases, tissue appearance may substantially deviate from normal tissue appearance and hence violates algorithmic assumptions for standard approaches to brain extraction; consequently, the brain may not be correctly extracted. This paper proposes a brain extraction approach which can explicitly account for pathologies by jointly modeling normal tissue appearance and pathologies. Specifically, our model uses a three-part image decomposition: (1) normal tissue appearance is captured by principal component analysis (PCA), (2) pathologies are captured via a total variation term, and (3) the skull and surrounding tissue is captured by a sparsity term. Due to its convexity, the resulting decomposition model allows for efficient optimization. Decomposition and image registration steps are alternated to allow statistical modeling of normal tissue appearance in a fixed atlas coordinate system. As a beneficial side effect, the decomposition model allows for the identification of potentially pathological areas and the reconstruction of a quasi-normal image in atlas space. We demonstrate the effectiveness of our approach on four datasets: the publicly available IBSR and LPBA40 datasets which show normal image appearance, the BRATS dataset containing images with brain tumors, and a dataset containing clinical TBI images. We compare the performance with other popular brain extraction models: ROBEX, BEaST, MASS, BET, BSE and a recently proposed deep learning approach. Our model performs better than these competing approaches on all four datasets. Specifically, our model achieves the best median (97.11) and mean (96.88) Dice scores over all datasets. The two best performing competitors, ROBEX and MASS, achieve scores of 96.23/95.62 and 96.67/94.25 respectively. Hence, our approach is an effective method for high quality brain extraction for a wide variety of images.},
annote = {Main objective: skull stripping

PCA-based split of brain into normal/abnormal used for skull-stripping},
author = {Han, Xu and Kwitt, Roland and Aylward, Stephen and Bakas, Spyridon and Menze, Bjoern and Asturias, Alexander and Vespa, Paul and {Van Horn}, John and Niethammer, Marc},
doi = {10.1016/J.NEUROIMAGE.2018.04.073},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Han et al. - 2018 - Brain extraction from normal and pathological images A joint PCAImage-Reconstruction approach.pdf:pdf},
issn = {1053-8119},
journal = {NeuroImage},
keywords = {Brain extraction,Image registration,PCA,Pathology,Total-variation},
month = {aug},
pages = {431--445},
publisher = {Academic Press},
title = {{Brain extraction from normal and pathological images: A joint PCA/Image-Reconstruction approach}},
volume = {176},
year = {2018}
}
@article{Dalca2019,
abstract = {We develop a learning framework for building deformable templates, which play a fundamental role in many image analysis and computational anatomy tasks. Conventional methods for template creation and image alignment to the template have undergone decades of rich technical development. In these frameworks, templates are constructed using an iterative process of template estimation and alignment, which is often computationally very expensive. Due in part to this shortcoming, most methods compute a single template for the entire population of images, or a few templates for specific sub-groups of the data. In this work, we present a probabilistic model and efficient learning strategy that yields either universal or conditional templates, jointly with a neural network that provides efficient alignment of the images to these templates. We demonstrate the usefulness of this method on a variety of domains, with a special focus on neuroimaging. This is particularly useful for clinical applications where a pre-existing template does not exist, or creating a new one with traditional methods can be prohibitively expensive. Our code and atlases are available online as part of the VoxelMorph library at http://voxelmorph.csail.mit.edu.},
archivePrefix = {arXiv},
arxivId = {1908.02738},
author = {Dalca, Adrian V. and Rakic, Marianne and Guttag, John and Sabuncu, Mert R.},
eprint = {1908.02738},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalca et al. - 2019 - Learning Conditional Deformable Templates with Convolutional Networks.pdf:pdf},
journal = {Advances in neural information processing systems},
month = {aug},
pages = {806----818},
title = {{Learning Conditional Deformable Templates with Convolutional Networks}},
url = {http://arxiv.org/abs/1908.02738},
year = {2019}
}
@incollection{Hu2019a,
abstract = {Classical pairwise image registration methods search for a spatial transformation that optimises a numerical measure that indicates how well a pair of moving and fixed images are aligned. Current learning-based registration methods have adopted the same paradigm and typically predict, for any new input image pair, dense correspondences in the form of a dense displacement field or parameters of a spatial transformation model. However, in many applications of registration, the spatial transformation itself is only required to propagate points or regions of interest (ROIs). In such cases, detailed pixel- or voxel-level correspondence within or outside of these ROIs often have little clinical value. In this paper, we propose an alternative paradigm in which the location of corresponding image-specific ROIs, defined in one image, within another image is learnt. This results in replacing image registration by a conditional segmentation algorithm, which can build on typical image segmentation networks and their widely-adopted training strategies. Using the registration of 3D MRI and ultrasound images of the prostate as an example to demonstrate this new approach, we report a median target registration error (TRE) of 2.1 mm between the ground-truth ROIs defined on intraoperative ultrasound images and those propagated from the preoperative MR images. Significantly lower ({\textgreater}34pct) TREs were obtained using the proposed conditional segmentation compared with those obtained from a previously-proposed spatial-transformation-predicting registration network trained with the same multiple ROI labels for individual image pairs. We conclude this work by using a quantitative bias-variance analysis to provide one explanation of the observed improvement in registration accuracy.},
annote = {weak supervision (with segmentation data)

instead of learning a dense displacement field conditioned on two images, we propose to learn a segmentation of ROI of one image, conditioned on the segmentation data of another image

very applicable to non-diffeomorphic registration},
archivePrefix = {arXiv},
arxivId = {1907.00438},
author = {Hu, Yipeng and Gibson, Eli and Barratt, Dean C. and Emberton, Mark and Noble, J. Alison and Vercauteren, Tom},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-030-32245-8_45},
eprint = {1907.00438},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu et al. - 2019 - Conditional Segmentation in Lieu of Image Registration.pdf:pdf},
isbn = {9783030322441},
issn = {16113349},
pages = {401--409},
title = {{Conditional Segmentation in Lieu of Image Registration}},
year = {2019}
}
@article{Blendowski2019,
abstract = {Purpose: Deep convolutional neural networks in their various forms are currently achieving or outperforming state-of-the-art results on several medical imaging tasks. We aim to make these developments available to the so far unsolved task of accurate correspondence finding—especially with regard to image registration. Methods: We propose a two-step hybrid approach to make deep learned features accessible to a discrete optimization-based registration method. In a first step, in order to extract expressive binary local descriptors, we train a deep network architecture on a patch-based landmark retrieval problem as auxiliary task. As second step at runtime within a MRF-regularised dense displacement sampling, their binary nature enables highly efficient similarity computations, thus making them an ideal candidate to replace the so far used handcrafted local feature descriptors during the registration process. Results: We evaluate our approach on finding correspondences between highly non-rigidly deformed lung CT scans from different breathing states. Although the CNN-based descriptors excell at an auxiliary learning task for finding keypoint correspondences, self-similarity-based descriptors yield more accurate registration results. However, a combination of both approaches turns out to generate the most robust features for registration. Conclusion: We present a three-dimensional framework for large lung motion estimation based on the combination of CNN-based and handcrafted descriptors efficiently employed in a discrete registration method. Achieving best results by combining learned and handcrafted features encourages further research in this direction.},
author = {Blendowski, Max and Heinrich, Mattias P.},
doi = {10.1007/s11548-018-1888-2},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blendowski, Heinrich - 2019 - Combining MRF-based deformable registration and deep binary 3D-CNN descriptors for large lung motion estim.pdf:pdf},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {Deep learning,Discrete optimization,Hamming distance,Image registration},
month = {jan},
number = {1},
pages = {43--52},
pmid = {30430361},
publisher = {Springer Verlag},
title = {{Combining MRF-based deformable registration and deep binary 3D-CNN descriptors for large lung motion estimation in COPD patients}},
url = {https://doi.org/10.1007/s11548-018-1888-2},
volume = {14},
year = {2019}
}
@article{Grenander1998,
author = {Grenander, Ulf and Miller, Michael I},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grenander, Miller - 1998 - Computational anatomy An emerging discipline.pdf:pdf},
journal = {Quarterly of applied mathematics},
pages = {617----694},
title = {{Computational anatomy: An emerging discipline}},
url = {https://www.jstor.org/stable/pdf/43638257.pdf?casa{\_}token=8oVusFqrSsYAAAAA:NS{\_}glch{\_}iRtQoilP8VrpI1SJ{\_}Wb0RW1sjeC-mo0p9d18M8swA2qeqw9Qa4by-J-vGj2ha03atSIaM1c7Sjvh57isrNAnok213dUcTAN9oFvfhLNOUaIOPg},
volume = {56},
year = {1998}
}
@article{Nielsen2019,
author = {Nielsen, Rune Kok and Feragen, Aasa and Darkner, Sune},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielsen, Feragen, Darkner - 2019 - Modeling Discontinuities in Diffeomorphic Registration.pdf:pdf},
keywords = {rune thesis,rune2019thesis},
mendeley-tags = {rune thesis},
title = {{Modeling Discontinuities in Diffeomorphic Registration}},
year = {2019}
}
@article{Suetens2009,
abstract = {The expanded and revised edition will split Chapter 4 to include more details and examples in FMRI, DTI, and DWI for MR image modalities. The book will also expand ultrasound imaging to 3-D dynamic contrast ultrasound imaging in a separate chapter. A new chapter on Optical Imaging Modalities elaborating microscopy, confocal microscopy, endoscopy, optical coherent tomography, fluorescence and molecular imaging will be added. Another new chapter on Simultaneous Multi-Modality Medical Imaging including CT-SPECT and CT-PET will also be added. In the image analysis part, chapters on image reconstructions and visualizations will be significantly enhanced to include, respectively, 3-D fast statistical estimation based reconstruction methods, and 3-D image fusion and visualization overlaying multi-modality imaging and information. A new chapter on Computer-Aided Diagnosis and image guided surgery, and surgical and therapeutic intervention will also be added. A companion site containing power point slides, author biography, corrections to the first edition and images from the text can be found here: ftp://ftp.wiley.com/public/sci{\_}tech{\_}med/medical{\_}image/ Send an email to: Pressbooks@ieee.org to obtain a solutions manual. Please include your affiliation in your email.},
author = {Czolbe, Steffen and Krause, Oswin and Feragem, Aasa},
doi = {10.1017/cbo9780511596803.008},
file = {:home/steffen/Downloads/article.pdf:pdf},
keywords = {deep,image registration,learning,representation learning},
pages = {159--189},
title = {{Semantic similarity metrics for image registration}}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {International Conference on Machine Learning},
eprint = {1502.03167},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch normalization Accelerating deep network training by reducing internal covariate shift.pdf:pdf},
isbn = {9781510810587},
month = {feb},
pages = {448--456},
publisher = {International Machine Learning Society (IMLS)},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
year = {2015}
}
@article{Ashburner2007,
abstract = {This paper describes DARTEL, which is an algorithm for diffeomorphic image registration. It is implemented for both 2D and 3D image registration and has been formulated to include an option for estimating inverse consistent deformations. Nonlinear registration is considered as a local optimisation problem, which is solved using a Levenberg-Marquardt strategy. The necessary matrix solutions are obtained in reasonable time using a multigrid method. A constant Eulerian velocity framework is used, which allows a rapid scaling and squaring method to be used in the computations. DARTEL has been applied to intersubject registration of 471 whole brain images, and the resulting deformations were evaluated in terms of how well they encode the shape information necessary to separate male and female subjects and to predict the ages of the subjects. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Ashburner, John},
doi = {10.1016/j.neuroimage.2007.07.007},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashburner - 2007 - A fast diffeomorphic image registration algorithm.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
month = {oct},
number = {1},
pages = {95--113},
title = {{A fast diffeomorphic image registration algorithm}},
volume = {38},
year = {2007}
}
@article{Wolper2020,
annote = {damage of fibrous structures

examples:
strong, stretchy fibers in a steak influence the direction the meat cuts and tears, while a succulent cube of pork belly has decadent layers that peel apart in sheets.},
author = {Wolper, Joshuah and Chen, Yunuo and Li, Minchen and Fang, Yu and Qu, Ziyin and Lu, Jiecong and Cheng, Meggie and Jiang, Chenfanfu},
doi = {10.1145/3386569.3392428},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolper et al. - 2020 - AnisoMPM Animating Anisotropic Damage Mechanics.pdf:pdf},
issn = {0730-0301},
journal = {ACM Transactions on Graphics},
month = {jul},
number = {4},
publisher = {Association for Computing Machinery (ACM)},
title = {{AnisoMPM: Animating Anisotropic Damage Mechanics}},
volume = {39},
year = {2020}
}
@article{Lee2013,
abstract = {We present a novel image registration method based on B-spline free-form deformation that simultaneously optimizes particle correspondence and image similarity metrics. Different from previous B-spline based registration methods optimized w.r.t. the control points, the deformation in our method is estimated from a set of dense unstructured pair of points, which we refer as corresponding particles. As intensity values are matched on the corresponding location, the registration performance is iteratively improved. Moreover, the use of corresponding particles naturally extends our method to a group-wise registration by computing a mean of particles. Motivated by a surface-based group-wise particle correspondence method, we developed a novel system that takes such particles to the image domain, while keeping the spirit of the method similar. The core algorithm both minimizes an entropy based group-wise correspondence metric as well as maximizes the space sampling of the particles. We demonstrate the results of our method in an application of rodent brain structure segmentation and show that our method provides better accuracy in two structures compared to other registration methods. {\textcopyright} 2013 Springer-Verlag.},
annote = {aligns particales, with goal of:
1) positional overlap of particles across images (1-1 matching)
2) intensity match},
author = {Lee, Joohwi and Lyu, Ilwoo and Oǧuz, Ipek and Styner, Martin A.},
doi = {10.1007/978-3-642-40760-4_26},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2013 - Particle-guided image registration.pdf:pdf},
isbn = {9783642407598},
issn = {03029743},
journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
pages = {203--210},
pmid = {24505762},
publisher = {Springer, Berlin, Heidelberg},
title = {{Particle-guided image registration}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-40760-4{\_}26},
year = {2013}
}
@article{Vahdat2020,
abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256{\$}\backslashtimes{\$}256 pixels. The source code is available at https://github.com/NVlabs/NVAE .},
archivePrefix = {arXiv},
arxivId = {2007.03898},
author = {Vahdat, Arash and Kautz, Jan},
eprint = {2007.03898},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vahdat, Kautz - 2020 - NVAE A Deep Hierarchical Variational Autoencoder.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jul},
publisher = {Neural information processing systems foundation},
title = {{NVAE: A Deep Hierarchical Variational Autoencoder}},
url = {https://arxiv.org/abs/2007.03898v3},
year = {2020}
}
@article{Siebert2021,
abstract = {Deep learning based medical image registration and in particular multimodal alignment has remained extremely difficult and often fails to improve over its classical counterparts where comprehensive supervision is not available. The use of unsupervised, metric-based registration networks have become popular for intramodal DL-based registration. For mul-timodal fusion there is the additional challenge that no universally applicable similarity metric is available and a trade-off between local contrast-invariant edge features or more global statistical metrics has to be found. An additional problem of metric-based methods is the difficult tuning of hyperparameters that balance similarity and regularisation weights. Here we propose a radically new approach that relies on geometric instead of metric supervision. We propose synthetic three-way (triangular) cycles that for each pair of images comprise two multimodal transformations to be estimated and one known synthetic monomodal transform. By minimising the cycle discrepancy and adapting the synthetic transformation to be close to the real geometric difference of the image pairs during training , we are able to learn multimodal registration between CT and MRI without metric supervision. Our method learns modality and appearance agnostic features that excels at accurate anatomical alignment.},
annote = {E-7

video: https://2021.midl.io/papers/e7

Questions: 
- what guarantees alignment of Images 2+3 with image 1? The minimization problem is solved as long as the transformation of Image 2+3 are aligned, Image 1 provides the domain, but it's indensities are independent of them. Registration network could just learn to align to a constant orientation.},
author = {Siebert, Hanna and Hansen, Lasse and Heinrich, Mattias P},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Siebert, Hansen, Heinrich - 2021 - Learning without Metric Self supervised Multimodal Registration using Synthetic Cycle Discrepancy.pdf:pdf},
journal = {Medical Imaging with Deep Learning},
keywords = {image registration,multimodal features,self-supervision},
number = {Figure 1},
pages = {1--12},
title = {{Learning without Metric: Self supervised Multimodal Registration using Synthetic Cycle Discrepancy}},
year = {2021}
}
@article{Schmidt-Richberg2012,
abstract = {The accurate estimation of respiratory lung motion by non-linear registration is currently an important topic of research and required for many applications in pulmonary image analysis, e.g. for radiotherapy treatment planning. A special challenge for lung registration is the sliding motion between visceral an parietal pleurae during breathing, which causes discontinuities in the motion field. It has been shown that accounting for this physiological aspect by modeling the sliding motion using a direction-dependent regularization approach can significantly improve registration results. While the potential of such physiology-based regularization methods has been demonstrated in several publications, so far only simple explicit solution schemes were applied due to the computational complexity. In this paper, a numerical solution of the direction-dependent regularization based on Fast Explicit Diffusion (FED) is presented. The approach is tested for motion estimation on 23 thoracic CT images and a significant improvement over the classic explicit solution is shown. {\textcopyright} 2012 Springer-Verlag.},
author = {Schmidt-Richberg, Alexander and Ehrhardt, Jan and Werner, Ren{\'{e}} and Handels, Heinz},
doi = {10.1007/978-3-642-31340-0_23},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt-Richberg et al. - 2012 - Fast explicit diffusion for registration with direction-dependent regularization.pdf:pdf},
isbn = {9783642313394},
issn = {03029743},
journal = {Biomedical Image Registration},
pages = {220--228},
publisher = {Springer, Berlin, Heidelberg},
title = {{Fast explicit diffusion for registration with direction-dependent regularization}},
volume = {7359},
year = {2012}
}
@article{citationrequired,
author = {CitationRequired},
title = {{TODO}},
year = {9999}
}
@article{Chen2021,
abstract = {In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.},
annote = {Combine conv net and vision transformer to remedy issue of low-resolution (downsampled) data for the transformer
3D, same computational cost as U-Net
single attention layer
U-net style shortcut connections

Has code online

Room for improvement:
Uses only self-attention. No encoder-decder attemtion (CHECK)},
archivePrefix = {arXiv},
arxivId = {2104.06468},
author = {Chen, Junyu and He, Yufan and Frey, Eric C. and Li, Ye and Du, Yong},
eprint = {2104.06468},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2021 - ViT-V-Net Vision Transformer for Unsupervised Volumetric Medical Image Registration.pdf:pdf},
keywords = {Convolutional Neural Networks,Image Registration,Vision Transformer},
month = {apr},
title = {{ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration}},
url = {http://arxiv.org/abs/2104.06468},
year = {2021}
}
@article{Pace2013,
abstract = {We propose a deformable image registration algorithm that uses anisotropic smoothing for regularization to find correspondences between images of sliding organs. In particular, we apply the method for respiratory motion estimation in longitudinal thoracic and abdominal computed tomography scans. The algorithm uses locally adaptive diffusion tensors to determine the direction and magnitude with which to smooth the components of the displacement field that are normal and tangential to an expected sliding boundary. Validation was performed using synthetic, phantom, and 14 clinical datasets, including the publicly available DIR-Lab dataset. We show that motion discontinuities caused by sliding can be effectively recovered, unlike conventional regularizations that enforce globally smooth motion. In the clinical datasets, target registration error showed improved accuracy for lung landmarks compared to the diffusive regularization. We also present a generalization of our algorithm to other sliding geometries, including sliding tubes (e.g., needles sliding through tissue, or contrast agent flowing through a vessel). Potential clinical applications of this method include longitudinal change detection and radiotherapy for lung or abdominal tumours, especially those near the chest or abdominal wall. {\textcopyright} 2013 IEEE.},
annote = {Aasa: "an instrumental paper of how we think about sliding motions"},
author = {Pace, Danielle F. and Aylward, Stephen R. and Niethammer, Marc},
doi = {10.1109/TMI.2013.2274777},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - A Locally Adaptive Regularization Based on Anisotropic Diffusion for Deformable Image Registration of Sliding Organs.pdf:pdf},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Abdominal computed tomography (CT),deformable image registration,locally adaptive regularization,respiratory motion,sliding motion,thoracic CT},
number = {11},
pages = {2114--2126},
title = {{A locally adaptive regularization based on anisotropic diffusion for deformable image registration of sliding organs}},
volume = {32},
year = {2013}
}
@inproceedings{Vercauteren2007,
abstract = {We propose a non-parametric diffeomorphic image registration algorithm based on Thirion's demons algorithm. The demons algorithm can be seen as an optimization procedure on the entire space of displacement fields. The main idea of our algorithm is to adapt this procedure to a space of diffeomorphic transformations. In contrast to many diffeomorphic registration algorithms, our solution is computationally efficient since in practice it only replaces an addition of free form deformations by a few compositions. Our experiments show that in addition to being diffeomorphic, our algorithm provides results that are similar to the ones from the demons algorithm but with transformations that are much smoother and closer to the true ones in terms of Jacobians. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Vercauteren, Tom and Pennec, Xavier and Perchant, Aymeric and Ayache, Nicholas},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-540-75759-7_39},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vercauteren et al. - 2007 - Non-parametric diffeomorphic image registration with the demons algorithm.pdf:pdf},
isbn = {9783540757580},
issn = {03029743},
number = {PART 2},
pages = {319--326},
pmid = {18044584},
title = {{Non-parametric diffeomorphic image registration with the demons algorithm}},
volume = {4792 LNCS},
year = {2007}
}
@article{Hoffmann2020,
abstract = {We introduce a learning strategy for contrast-invariant image registration without requiring imaging data. While classical registration methods accurately estimate the spatial correspondence between images, they solve a costly optimization problem for every image pair. Learning-based techniques are fast at test time, but can only register images with image contrast and geometric content that are similar to those available during training. We focus on removing this image-data dependency of learning methods. Our approach leverages a generative model for diverse label maps and images that exposes networks to a wide range of variability during training, forcing them to learn features invariant to image type (contrast). This strategy results in powerful networks trained to generalize to a broad array of real input images. We present extensive experiments, with a focus on 3D neuroimaging, showing that this strategy enables robust registration of arbitrary image contrasts without the need to retrain for new modalities. We demonstrate registration accuracy that most often surpasses the state of the art both within and across modalities, using a single model. Critically, we show that input labels from which we synthesize images need not be of actual anatomy: training on randomly generated geometric shapes also results in competitive registration performance, albeit slightly less accurate, while alleviating the dependency on real data of any kind. Our code is available at: http://voxelmorph.csail.mit.edu},
annote = {- randomly generated segmentation maps
- use generative model to generate images of different modalities/contrasts from the label maps
- train image registration in a supervised manner, also across modalities
- registration model generalizes to real data},
archivePrefix = {arXiv},
arxivId = {2004.10282},
author = {Hoffmann, Malte and Billot, Benjamin and Iglesias, Juan Eugenio and Fischl, Bruce and Dalca, Adrian V.},
eprint = {2004.10282},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffmann et al. - 2020 - Learning Multi-Modal Image Registration without Real Data.pdf:pdf},
month = {apr},
title = {{Learning image registration without images}},
url = {http://arxiv.org/abs/2004.10282},
year = {2020}
}
@article{Cvpr2020,
author = {Cvpr, Anonymous and Id, Paper},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cvpr, Id - 2020 - A fast and memory efficient perceptual similarity metric for deep neural networks v0.pdf:pdf},
title = {{A fast and memory efficient perceptual similarity metric for deep neural networks v0}},
year = {2020}
}
@article{Yang2016,
abstract = {This paper proposes an approach to improve atlas-to-image registration accuracy with large pathologies. Instead of directly registering an atlas to a pathological image, the method learns a mapping from the pathological image to a quasi-normal image, for which more accurate registration is possible. Specifically, the method uses a deep variational convolutional encoder-decoder network to learn the mapping. Furthermore, the method estimates local mapping uncertainty through network inference statistics and uses those estimates to down-weight the image registration similarity measure in areas of high uncertainty. The performance of the method is quantified using synthetic brain tumor images and images from the brain tumor segmentation challenge (BRATS 2015).},
annote = {- Use a denoising VAE to remove tumors prior to registration
- Mask areas of unlikely correspondence during registration (areas with high VAE uncertainty), by excluding them from the similarity measure
- does not require lesion annotations
- detect tumors as std (calculation of std based on reconstruction)},
author = {Yang, Xiao and Han, Xu and Park, Eunbyung and Aylward, Stephen and Kwitt, Roland and Niethammer, Marc},
doi = {10.1007/978-3-319-46630-9_10},
file = {:home/steffen/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2016 - Registration of Pathological Images.pdf:pdf},
journal = {Simulation and synthesis in medical imaging (Workshop)},
pages = {97},
pmid = {29896582},
publisher = {NIH Public Access},
title = {{Registration of Pathological Images}},
url = {/pmc/articles/PMC5994389/ /pmc/articles/PMC5994389/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5994389/},
volume = {9968},
year = {2016}
}
@article{Ehrhardt2011,
abstract = {Modeling of respiratory motion has become increasingly important in various applications of medical imaging (e.g., radiation therapy of lung cancer). Current modeling approaches are usually confined to intra-patient registration of 3D image data representing the individual patient's anatomy at different breathing phases. We propose an approach to generate a mean motion model of the lung based on thoracic 4D computed tomography (CT) data of different patients to extend the motion modeling capabilities. Our modeling process consists of three steps: an intra-subject registration to generate subject-specific motion models, the generation of an average shape and intensity atlas of the lung as anatomical reference frame, and the registration of the subject-specific motion models to the atlas in order to build a statistical 4D mean motion model (4D-MMM). Furthermore, we present methods to adapt the 4D mean motion model to a patient-specific lung geometry. In all steps, a symmetric diffeomorphic nonlinear intensity-based registration method was employed. The Log-Euclidean framework was used to compute statistics on the diffeomorphic transformations. The presented methods are then used to build a mean motion model of respiratory lung motion using thoracic 4D CT data sets of 17 patients. We evaluate the model by applying it for estimating respiratory motion of ten lung cancer patients. The prediction is evaluated with respect to landmark and tumor motion, and the quantitative analysis results in a mean target registration error (TRE) of 3.3 $\backslash$pm 1.6 mm if lung dynamics are not impaired by large lung tumors or other lung disorders (e.g., emphysema). With regard to lung tumor motion, we show that prediction accuracy is independent of tumor size and tumor motion amplitude in the considered data set. However, tumors adhering to non-lung structures degrade local lung dynamics significantly and the model-based prediction accuracy is lower in these cases. The statistical respiratory motion model is capable of providing valuable prior knowledge in many fields of applications. We present two examples of possible applications in radiation therapy and image guided diagnosis. {\textcopyright} 2010 IEEE.},
author = {Ehrhardt, Jan and Werner, Ren{\'{e}} and Schmidt-Richberg, Alexander and Handels, Heinz},
doi = {10.1109/TMI.2010.2076299},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {4D computed tomography (CT),Diffeomorphic registration,motion modeling,respiratory motion,statistical atlas generation},
month = {feb},
number = {2},
pages = {251--265},
pmid = {20876013},
title = {{Statistical modeling of 4D respiratory lung motion using diffeomorphic image registration}},
volume = {30},
year = {2011}
}