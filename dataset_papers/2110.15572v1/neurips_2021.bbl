\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1057--1063, 2000.

\bibitem[Mei et~al.(2020{\natexlab{a}})Mei, Xiao, Szepesvari, and
  Schuurmans]{mei2020global}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  6820--6829. PMLR, 2020{\natexlab{a}}.

\bibitem[Mei et~al.(2020{\natexlab{b}})Mei, Xiao, Dai, Li, Szepesv{\'a}ri, and
  Schuurmans]{mei2020escaping}
Jincheng Mei, Chenjun Xiao, Bo~Dai, Lihong Li, Csaba Szepesv{\'a}ri, and Dale
  Schuurmans.
\newblock Escaping the gravitational pull of softmax.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Li et~al.(2021)Li, Wei, Chi, Gu, and Chen]{li2021softmax}
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.
\newblock Softmax policy gradient methods can take exponential time to
  converge.
\newblock \emph{arXiv preprint arXiv:2102.11270}, 2021.

\bibitem[Cen et~al.(2020)Cen, Cheng, Chen, Wei, and Chi]{cen2020fast}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock \emph{arXiv preprint arXiv:2007.06558}, 2020.

\bibitem[Lan(2021)]{lan2021policy}
Guanghui Lan.
\newblock Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes.
\newblock \emph{arXiv preprint arXiv:2102.00135}, 2021.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 64--66. PMLR, 2020.

\bibitem[Khodadadian et~al.(2021)Khodadadian, Jhunjhunwala, Varma, and
  Maguluri]{khodadadian2021linear}
Sajad Khodadadian, Prakirt~Raj Jhunjhunwala, Sushil~Mahavir Varma, and
  Siva~Theja Maguluri.
\newblock On the linear convergence of natural policy gradient algorithm.
\newblock \emph{arXiv preprint arXiv:2105.01424}, 2021.

\bibitem[Mei et~al.(2021)Mei, Gao, Dai, Szepesvari, and
  Schuurmans]{mei2021leveraging}
Jincheng Mei, Yue Gao, Bo~Dai, Csaba Szepesvari, and Dale Schuurmans.
\newblock Leveraging non-uniformity in first-order non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2105.06072}, 2021.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba
  Szepesvari, and Gell{\'e}rt Weisz.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  3692--3702. PMLR, 2019.

\bibitem[Chung et~al.(2020)Chung, Thomas, Machado, and Roux]{chung2020beyond}
Wesley Chung, Valentin Thomas, Marlos~C Machado, and Nicolas~Le Roux.
\newblock Beyond variance reduction: Understanding the true impact of baselines
  on policy optimization.
\newblock \emph{arXiv preprint arXiv:2008.13773}, 2020.

\bibitem[Denisov and Walton(2020)]{denisov2020regret}
Denis Denisov and Neil Walton.
\newblock Regret analysis of a markov policy gradient algorithm for multi-arm
  bandits.
\newblock \emph{arXiv preprint arXiv:2007.10229}, 2020.

\bibitem[Kakade(2002)]{kakade2002natural}
Sham~M Kakade.
\newblock A natural policy gradient.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1531--1538, 2002.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Benveniste et~al.(1990)Benveniste, M{\'{e}}tivier, and
  Priouret]{BMP90}
Albert Benveniste, Michel M{\'{e}}tivier, and Pierre Priouret.
\newblock \emph{Adaptive Algorithms and Stochastic Approximations}, volume~22.
\newblock Springer, 1990.

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Baker, Chociej, Jozefowicz,
  McGrew, Pachocki, Petron, Plappert, Powell, Ray,
  et~al.]{andrychowicz2020learning}
OpenAI:~Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob
  McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex
  Ray, et~al.
\newblock Learning dexterous in-hand manipulation.
\newblock \emph{The International Journal of Robotics Research}, 39\penalty0
  (1):\penalty0 3--20, 2020.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Kim, O'Donoghue, and
  Boyd]{zhang2020sample}
Junzi Zhang, Jongho Kim, Brendan O'Donoghue, and Stephen Boyd.
\newblock Sample efficient reinforcement learning with reinforce.
\newblock \emph{arXiv preprint arXiv:2010.11364}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2021)Zhang, Ni, Yu, Szepesvari, and
  Wang]{zhang2021convergence}
Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang.
\newblock On the convergence and sample efficiency of variance-reduced policy
  gradient method.
\newblock \emph{arXiv preprint arXiv:2102.08607}, 2021.

\bibitem[Tucker et~al.(2018)Tucker, Bhupatiraju, Gu, Turner, Ghahramani, and
  Levine]{tucker2018baseline}
George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin
  Ghahramani, and Sergey Levine.
\newblock The mirage of action-dependent baselines in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5015--5024, 2018.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2018deep}
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
  and David Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Wiering and Van~Hasselt(2008)]{wiering2008ensemble}
Marco~A Wiering and Hado Van~Hasselt.
\newblock Ensemble algorithms in reinforcement learning.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 38\penalty0 (4):\penalty0 930--936, 2008.

\bibitem[Jung et~al.(2020)Jung, Park, and Sung]{Jung2020Population-Guided}
Whiyoung Jung, Giseung Park, and Youngchul Sung.
\newblock Population-guided parallel policy search for reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Parker-Holder et~al.(2020)Parker-Holder, Pacchiano, Choromanski, and
  Roberts]{parker2020effective}
Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts.
\newblock Effective diversity in population-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.00632}, 2020.

\bibitem[Lazi{\'c} et~al.(2021)Lazi{\'c}, Hao, Abbasi-Yadkori, Schuurmans, and
  Szepesv{\'a}ri]{lazic2021optimization}
Nevena Lazi{\'c}, Botao Hao, Yasin Abbasi-Yadkori, Dale Schuurmans, and Csaba
  Szepesv{\'a}ri.
\newblock Optimization issues in kl-constrained approximate policy iteration.
\newblock \emph{arXiv preprint arXiv:2102.06234}, 2021.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937. PMLR, 2016.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Bhandari and Russo(2020)]{bhandari2020note}
Jalaj Bhandari and Daniel Russo.
\newblock A note on the linear convergence of policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2007.11120}, 2020.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Koppel, Zhu, and
  Basar]{zhang2020global}
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar.
\newblock Global convergence of policy gradient methods to (almost) locally
  optimal policies.
\newblock \emph{SIAM Journal on Control and Optimization}, 58\penalty0
  (6):\penalty0 3586--3612, 2020{\natexlab{b}}.

\bibitem[Knopp(1947)]{knopp1947theory}
Konrad Knopp.
\newblock \emph{Theory and Application of Infinite Series}.
\newblock Hafner Publishing Company, New York, 1947.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, volume~2, pages 267--274, 2002.

\end{thebibliography}
