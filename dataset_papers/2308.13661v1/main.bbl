\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam \& Sastry(2017)Achiam and Sastry]{achiam2017surprise}
Achiam, J. and Sastry, S.
\newblock Surprise-based intrinsic motivation for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.01732}, 2017.

\bibitem[Andersen et~al.(2006)Andersen, Morris, Amaral, Bliss, and
  O'Keefe]{andersen2006hippocampus}
Andersen, P., Morris, R., Amaral, D., Bliss, T., and O'Keefe, J.
\newblock \emph{The hippocampus book}.
\newblock Oxford university press, 2006.

\bibitem[Badia et~al.(2020)Badia, Sprechmann, Vitvitskyi, Guo, Piot,
  Kapturowski, Tieleman, Arjovsky, Pritzel, Bolt, et~al.]{badia2020never}
Badia, A.~P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski,
  S., Tieleman, O., Arjovsky, M., Pritzel, A., Bolt, A., et~al.
\newblock Never give up: Learning directed exploration strategies.
\newblock \emph{arXiv preprint arXiv:2002.06038}, 2020.

\bibitem[Beattie et~al.(2016)Beattie, Leibo, Teplyashin, Ward, Wainwright,
  K{\"u}ttler, Lefrancq, Green, Vald{\'e}s, Sadik, et~al.]{beattie2016deepmind}
Beattie, C., Leibo, J.~Z., Teplyashin, D., Ward, T., Wainwright, M.,
  K{\"u}ttler, H., Lefrancq, A., Green, S., Vald{\'e}s, V., Sadik, A., et~al.
\newblock Deepmind lab.
\newblock \emph{arXiv preprint arXiv:1612.03801}, 2016.

\bibitem[Blundell et~al.(2016)Blundell, Uria, Pritzel, Li, Ruderman, Leibo,
  Rae, Wierstra, and Hassabis]{blundell2016model}
Blundell, C., Uria, B., Pritzel, A., Li, Y., Ruderman, A., Leibo, J.~Z., Rae,
  J., Wierstra, D., and Hassabis, D.
\newblock Model-free episodic control.
\newblock \emph{arXiv preprint arXiv:1606.04460}, 2016.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Chevalier-Boisvert et~al.(2018)Chevalier-Boisvert, Willems, and
  Pal]{gym_minigrid}
Chevalier-Boisvert, M., Willems, L., and Pal, S.
\newblock Minimalistic gridworld environment for openai gym.
\newblock \url{https://github.com/maximecb/gym-minigrid}, 2018.

\bibitem[Chevalier-Boisvert et~al.(2023)Chevalier-Boisvert, Dai, Towers,
  de~Lazcano, Willems, Lahlou, Pal, Castro, and Terry]{MinigridMiniworld23}
Chevalier-Boisvert, M., Dai, B., Towers, M., de~Lazcano, R., Willems, L.,
  Lahlou, S., Pal, S., Castro, P.~S., and Terry, J.
\newblock Minigrid \& miniworld: Modular \& customizable reinforcement learning
  environments for goal-oriented tasks.
\newblock \emph{CoRR}, abs/2306.13831, 2023.

\bibitem[Cobbe et~al.(2019)Cobbe, Klimov, Hesse, Kim, and
  Schulman]{cobbe2019quantifying}
Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J.
\newblock Quantifying generalization in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1282--1289. PMLR, 2019.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2056. PMLR, 2020.

\bibitem[Eichenbaum(2017)]{eichenbaum2017role}
Eichenbaum, H.
\newblock The role of the hippocampus in navigation is memory.
\newblock \emph{Journal of neurophysiology}, 117\penalty0 (4):\penalty0
  1785--1796, 2017.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International conference on machine learning}, pp.\
  1407--1416. PMLR, 2018.

\bibitem[Flet-Berliac et~al.(2021)Flet-Berliac, Ferret, Pietquin, Preux, and
  Geist]{flet2021adversarially}
Flet-Berliac, Y., Ferret, J., Pietquin, O., Preux, P., and Geist, M.
\newblock Adversarially guided actor-critic.
\newblock \emph{arXiv preprint arXiv:2102.04376}, 2021.

\bibitem[Florensa et~al.(2018)Florensa, Held, Geng, and
  Abbeel]{florensa2018automatic}
Florensa, C., Held, D., Geng, X., and Abbeel, P.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In \emph{International conference on machine learning}, pp.\
  1515--1528. PMLR, 2018.

\bibitem[Freeman et~al.(2019)Freeman, Ha, and Metz]{freeman2019learning}
Freeman, D., Ha, D., and Metz, L.
\newblock Learning to predict without looking ahead: World models without
  forward prediction.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Ha \& Schmidhuber(2018{\natexlab{a}})Ha and
  Schmidhuber]{ha2018recurrent}
Ha, D. and Schmidhuber, J.
\newblock Recurrent world models facilitate policy evolution.
\newblock \emph{Advances in neural information processing systems}, 31,
  2018{\natexlab{a}}.

\bibitem[Ha \& Schmidhuber(2018{\natexlab{b}})Ha and Schmidhuber]{ha2018world}
Ha, D. and Schmidhuber, J.
\newblock World models.
\newblock \emph{arXiv preprint arXiv:1803.10122}, 2018{\natexlab{b}}.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2019dream}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock \emph{arXiv preprint arXiv:1912.01603}, 2019.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner2023mastering}
Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019provably}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2681--2691. PMLR, 2019.

\bibitem[Kempka et~al.(2016)Kempka, Wydmuch, Runc, Toczek, and
  Ja{\'s}kowski]{kempka2016vizdoom}
Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Ja{\'s}kowski, W.
\newblock Vizdoom: A doom-based ai research platform for visual reinforcement
  learning.
\newblock In \emph{2016 IEEE conference on computational intelligence and games
  (CIG)}, pp.\  1--8. IEEE, 2016.

\bibitem[Kirk et~al.(2021)Kirk, Zhang, Grefenstette, and
  Rockt{\"a}schel]{kirk2021survey}
Kirk, R., Zhang, A., Grefenstette, E., and Rockt{\"a}schel, T.
\newblock A survey of generalisation in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2111.09794}, 2021.

\bibitem[Kolter \& Ng(2009)Kolter and Ng]{kolter2009near}
Kolter, J.~Z. and Ng, A.~Y.
\newblock Near-bayesian exploration in polynomial time.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pp.\  513--520, 2009.

\bibitem[Laskin et~al.(2020)Laskin, Lee, Stooke, Pinto, Abbeel, and
  Srinivas]{laskin2020reinforcement}
Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A.
\newblock Reinforcement learning with augmented data.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 19884--19895, 2020.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019efficient}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov,
  R.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Liu \& Abbeel(2021)Liu and Abbeel]{liu2021behavior}
Liu, H. and Abbeel, P.
\newblock Behavior from the void: Unsupervised active pre-training.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 18459--18473, 2021.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937. PMLR, 2016.

\bibitem[Okada et~al.(2020)Okada, Kosaka, and Taniguchi]{okada2020planet}
Okada, M., Kosaka, N., and Taniguchi, T.
\newblock Planet of the bayesians: Reconsidering and improving deep planning
  network by incorporating bayesian inference.
\newblock In \emph{2020 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  5611--5618. IEEE, 2020.

\bibitem[Parisi et~al.(2021)Parisi, Dean, Pathak, and
  Gupta]{parisi2021interesting}
Parisi, S., Dean, V., Pathak, D., and Gupta, A.
\newblock Interesting object, curious agent: Learning task-agnostic
  exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International conference on machine learning}, pp.\
  2778--2787. PMLR, 2017.

\bibitem[Pritzel et~al.(2017)Pritzel, Uria, Srinivasan, Badia, Vinyals,
  Hassabis, Wierstra, and Blundell]{pritzel2017neural}
Pritzel, A., Uria, B., Srinivasan, S., Badia, A.~P., Vinyals, O., Hassabis, D.,
  Wierstra, D., and Blundell, C.
\newblock Neural episodic control.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2827--2836. PMLR, 2017.

\bibitem[Raileanu \& Rockt{\"a}schel(2020)Raileanu and
  Rockt{\"a}schel]{raileanu2020ride}
Raileanu, R. and Rockt{\"a}schel, T.
\newblock Ride: Rewarding impact-driven exploration for procedurally-generated
  environments.
\newblock \emph{arXiv preprint arXiv:2002.12292}, 2020.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave,
  Wiele, Mnih, Heess, and Springenberg]{riedmiller2018learning}
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Wiele, T.,
  Mnih, V., Heess, N., and Springenberg, J.~T.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In \emph{International conference on machine learning}, pp.\
  4344--4353. PMLR, 2018.

\bibitem[Savinov et~al.(2018)Savinov, Raichuk, Marinier, Vincent, Pollefeys,
  Lillicrap, and Gelly]{savinov2018episodic}
Savinov, N., Raichuk, A., Marinier, R., Vincent, D., Pollefeys, M., Lillicrap,
  T., and Gelly, S.
\newblock Episodic curiosity through reachability.
\newblock \emph{arXiv preprint arXiv:1810.02274}, 2018.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sekar et~al.(2020)Sekar, Rybkin, Daniilidis, Abbeel, Hafner, and
  Pathak]{sekar2020planning}
Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D.
\newblock Planning to explore via self-supervised world models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8583--8592. PMLR, 2020.

\bibitem[Seo et~al.(2021)Seo, Chen, Shin, Lee, Abbeel, and
  Lee]{pmlr-v139-seo21a}
Seo, Y., Chen, L., Shin, J., Lee, H., Abbeel, P., and Lee, K.
\newblock State entropy maximization with random encoders for efficient
  exploration.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  9443--9454. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/seo21a.html}.

\bibitem[Stadie et~al.(2015)Stadie, Levine, and
  Abbeel]{stadie2015incentivizing}
Stadie, B.~C., Levine, S., and Abbeel, P.
\newblock Incentivizing exploration in reinforcement learning with deep
  predictive models.
\newblock \emph{arXiv preprint arXiv:1507.00814}, 2015.

\bibitem[Strehl \& Littman(2008)Strehl and Littman]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \emph{Journal of Computer and System Sciences}, 74\penalty0
  (8):\penalty0 1309--1331, 2008.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Xi~Chen, Duan,
  Schulman, DeTurck, and Abbeel]{tang2017exploration}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Xi~Chen, O., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Tunyasuvunakool et~al.(2020)Tunyasuvunakool, Muldal, Doron, Liu,
  Bohez, Merel, Erez, Lillicrap, Heess, and Tassa]{tunyasuvunakool2020}
Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S., Merel, J.,
  Erez, T., Lillicrap, T., Heess, N., and Tassa, Y.
\newblock dm\_control: Software and tasks for continuous control.
\newblock \emph{Software Impacts}, 6:\penalty0 100022, 2020.
\newblock ISSN 2665-9638.
\newblock \doi{https://doi.org/10.1016/j.simpa.2020.100022}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S2665963820300099}.

\bibitem[Wang et~al.(2019)Wang, Bao, Clavera, Hoang, Wen, Langlois, Zhang,
  Zhang, Abbeel, and Ba]{wang2019benchmarking}
Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Langlois, E., Zhang, S.,
  Zhang, G., Abbeel, P., and Ba, J.
\newblock Benchmarking model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.02057}, 2019.

\bibitem[Yarats et~al.(2021{\natexlab{a}})Yarats, Fergus, Lazaric, and
  Pinto]{yarats2021reinforcement}
Yarats, D., Fergus, R., Lazaric, A., and Pinto, L.
\newblock Reinforcement learning with prototypical representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11920--11931. PMLR, 2021{\natexlab{a}}.

\bibitem[Yarats et~al.(2021{\natexlab{b}})Yarats, Zhang, Kostrikov, Amos,
  Pineau, and Fergus]{yarats2021improving}
Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  10674--10681, 2021{\natexlab{b}}.

\bibitem[Zha et~al.(2021)Zha, Ma, Yuan, Hu, and Liu]{zha2021rank}
Zha, D., Ma, W., Yuan, L., Hu, X., and Liu, J.
\newblock Rank the episodes: A simple approach for exploration in
  procedurally-generated environments.
\newblock \emph{arXiv preprint arXiv:2101.08152}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Xu, Wang, Wu, Keutzer, Gonzalez, and
  Tian]{zhang2021noveld}
Zhang, T., Xu, H., Wang, X., Wu, Y., Keutzer, K., Gonzalez, J.~E., and Tian, Y.
\newblock Noveld: A simple yet effective exploration criterion.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\end{thebibliography}
