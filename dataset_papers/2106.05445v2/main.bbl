\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Allen Zhu}(2017)]{Allenzhu2017-katyusha}
Z.~{Allen Zhu}.
\newblock Katyusha: the first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of the 49th Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC} 2017, Montreal, QC, Canada, June 19-23, 2017},
  pages 1200--1205, 2017.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
P.~L. Bartlett, M.~I. Jordan, and J.~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Bousquet(2002)]{bousquet2002concentration}
O.~Bousquet.
\newblock Concentration inequalities and empirical processes theory applied to
  the analysis of learning algorithms.
\newblock \emph{PhD thesis, Ecole Polytechnique}, 2002.

\bibitem[Bousquet and Bottou(2011)]{bousquet2008tradeoffs}
O.~Bousquet and L.~Bottou.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  161--168, 2011.

\bibitem[Byrd et~al.(1987)Byrd, Nocedal, and Yuan]{byrd1987global}
R.~H. Byrd, J.~Nocedal, and Y.-X. Yuan.
\newblock Global convergence of a class of {quasi-Newton} methods on convex
  problems.
\newblock \emph{SIAM Journal on Numerical Analysis}, 24\penalty0 (5):\penalty0
  1171--1190, 1987.

\bibitem[Byrd et~al.(2016)Byrd, Hansen, Nocedal, and
  Singer]{byrd2016stochastic}
R.~H. Byrd, S.~Hansen, J.~Nocedal, and Y.~Singer.
\newblock A stochastic quasi-{Newton} method for large-scale optimization.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (2):\penalty0
  1008--1031, 2016.

\bibitem[Chang and Lin(2011)]{libsvm}
C.-C. Chang and C.-J. Lin.
\newblock Libsvm: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology, Article
  27}, 2011.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
A.~Cutkosky and F.~Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste{-}Julien]{defazio2014saga}
A.~Defazio, F.~Bach, and S.~Lacoste{-}Julien.
\newblock {SAGA:} {A} fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Derezinski et~al.(2018)Derezinski, Mahajan, Keerthi, Vishwanathan, and
  Weimer]{DMKVW18}
M.~Derezinski, D.~Mahajan, S.~S. Keerthi, S.~V.~N. Vishwanathan, and M.~Weimer.
\newblock Batch-expansion training: an efficient optimization framework.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics, pp. 736-744}, 2018.

\bibitem[Epsilon(2008)]{epsilon}
Epsilon.
\newblock Pascal large scale learning challenge.
\newblock \emph{http://www.k4all.org/project/large-scale-learning-challenge},
  2008.

\bibitem[Erdogdu and Montanari(2015)]{erdogdu2015convergence}
M.~A. Erdogdu and A.~Montanari.
\newblock Convergence rates of sub-sampled {Newton} methods.
\newblock In \emph{Advances in Neural Information Processing Systems 28: Annual
  Conference on Neural Information Processing Systems 2015, Montreal, Quebec,
  Canada}, pages 3052--3060, 2015.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018near}
C.~Fang, C.~Li, Z.~Lin, and T.~Zhang.
\newblock Near-optimal non-convex optimization via stochastic path integrated
  differential estimator.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 689, 2018.

\bibitem[F{\'e}raud et~al.(2009)F{\'e}raud, Boull{\'e}, Cl{\'e}rot, Fessant,
  and Lemaire]{orange}
R.~F{\'e}raud, M.~Boull{\'e}, F.~Cl{\'e}rot, F.~Fessant, and V.~Lemaire.
\newblock The orange customer analysis platform.
\newblock \emph{JMLR WCP, Volume 7, KDD cup 2009, Paris}, 2009.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and Sidford]{frostig15}
R.~Frostig, R.~Ge, S.~M. Kakade, and A.~Sidford.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In \emph{The Conference on Learning Theory}, pages 728--763, 2015.

\bibitem[G{\"u}rb{\"u}zbalaban et~al.(2015)G{\"u}rb{\"u}zbalaban, Ozdaglar, and
  Parrilo]{gurbuzbalaban2015globally}
M.~G{\"u}rb{\"u}zbalaban, A.~Ozdaglar, and P.~Parrilo.
\newblock A globally convergent incremental {Newton} method.
\newblock \emph{Mathematical Programming}, 151\penalty0 (1):\penalty0 283--313,
  2015.

\bibitem[Isabelle et~al.(2005)Isabelle, Steve, Asa, and Gideon]{gisette}
G.~Isabelle, G.~Steve, H.~Asa, Ben, and D.~Gideon.
\newblock Result analysis of the nips 2003 feature selection challenge.
\newblock \emph{Advances In Neural Information Processing Systems, Volumn 17},
  2005.

\bibitem[Jin and Mokhtari(2020)]{qiujiang2020quasinewton1}
Q.~Jin and A.~Mokhtari.
\newblock Non-asymptotic superlinear convergence of standard quasi-newton
  methods.
\newblock \emph{arXiv preprint arXiv:2003.13607}, 2020.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages
  315--323, 2013.

\bibitem[{Le Roux} et~al.(2012){Le Roux}, Schmidt, and
  Bach]{roux2012stochastic}
N.~{Le Roux}, M.~Schmidt, and F.~Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Advances in neural information processing systems}, pages
  2663--2671, 2012.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Y.~LeCun, C.~Cortes, and C.~J. Burges.
\newblock {MNIST} handwritten digit database.
\newblock \emph{AT\&T Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2010.

\bibitem[Lee et~al.(1998)Lee, Bartlett, and Williamson]{lee1998importance}
W.~S. Lee, P.~L. Bartlett, and R.~C. Williamson.
\newblock The importance of convexity in learning with squared loss.
\newblock \emph{IEEE Transactions on Information Theory}, 44\penalty0
  (5):\penalty0 1974--1980, 1998.

\bibitem[Liu and Nocedal(1989)]{liu1989limited}
D.~C. Liu and J.~Nocedal.
\newblock On the limited memory {BFGS} method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1-3):\penalty0
  503--528, 1989.

\bibitem[Mokhtari and Ribeiro(2014)]{mokhtari2014res}
A.~Mokhtari and A.~Ribeiro.
\newblock Res: Regularized stochastic {BFGS} algorithm.
\newblock \emph{IEEE Transactions on Signal Processing}, 62\penalty0
  (23):\penalty0 6089--6104, 2014.

\bibitem[Mokhtari and Ribeiro(2015)]{mokhtari2015global}
A.~Mokhtari and A.~Ribeiro.
\newblock Global convergence of online limited memory bfgs.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3151--3181, 2015.

\bibitem[Mokhtari and Ribeiro(2017)]{mokhtari2017first}
A.~Mokhtari and A.~Ribeiro.
\newblock First-order adaptive sample size methods to reduce complexity of
  empirical risk minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2060--2068, 2017.

\bibitem[Mokhtari et~al.(2016)Mokhtari, Daneshmand, Lucchi, Hofmann, and
  Ribeiro]{mokhtari2016adaptive}
A.~Mokhtari, H.~Daneshmand, A.~Lucchi, T.~Hofmann, and A.~Ribeiro.
\newblock Adaptive {Newton} method for empirical risk minimization to
  statistical accuracy.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4062--4070, 2016.

\bibitem[Mokhtari et~al.(2018)Mokhtari, Eisen, and Ribeiro]{mokhtari2017iqn}
A.~Mokhtari, M.~Eisen, and A.~Ribeiro.
\newblock {IQN}: An incremental quasi-{Newton} method with local superlinear
  convergence rate.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1670--1698, 2018.

\bibitem[Moritz et~al.(2016)Moritz, Nishihara, and Jordan]{MoritzNJ16}
P.~Moritz, R.~Nishihara, and M.~I. Jordan.
\newblock A linearly-convergent stochastic {L-BFGS} algorithm.
\newblock In \emph{Proceedings of Int. Conference on Artificial Intelligence
  and Statistics, {AISTATS}}, pages 249--258, 2016.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization}, volume~87.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{International Conference on Machine Learning}, pages
  2613--2621. PMLR, 2017.

\bibitem[Rodomanov and Nesterov(2021{\natexlab{a}})]{rodomanov2020rates}
A.~Rodomanov and Y.~Nesterov.
\newblock Rates of superlinear convergence for classical quasi-newton methods.
\newblock \emph{Mathematical Programming}, pages 1--32, 2021{\natexlab{a}}.

\bibitem[Rodomanov and Nesterov(2021{\natexlab{b}})]{rodomanov2020ratesnew}
A.~Rodomanov and Y.~Nesterov.
\newblock New results on superlinear convergence of classical quasi-newton
  methods.
\newblock \emph{Journal of Optimization Theory and Applications}, 188\penalty0
  (3):\penalty0 744--769, 2021{\natexlab{b}}.

\bibitem[Schraudolph et~al.(2007)Schraudolph, Yu, and
  G{\"{u}}nter]{Schraudolph}
N.~N. Schraudolph, J.~Yu, and S.~G{\"{u}}nter.
\newblock A stochastic quasi-{Newton} method for online convex optimization.
\newblock In \emph{Proceedings of the Eleventh International Conference on
  Artificial Intelligence and Statistics, {AISTATS} 2007, San Juan, Puerto
  Rico, March 21-24, 2007}, pages 436--443, 2007.

\bibitem[Shalev-Shwartz and Zhang(2013)]{shalev2013stochastic}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss.
\newblock \emph{The Journal of Machine Learning Research}, 14:\penalty0
  567--599, 2013.

\bibitem[Tropp(2015)]{matrixconcentration}
J.~A. Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{Volume 8, Issues 1-2 of Foundations and trends in machine
  learning}, 2015.

\bibitem[Vapnik(2013)]{vapnik1998statistical}
V.~Vapnik.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer Science \& Business Media, 2013.

\end{thebibliography}
