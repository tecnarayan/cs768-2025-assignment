\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2019)]{allen2019can}
Z.~Allen-Zhu and Y.~Li.
\newblock Can {SGD} learn recurrent neural networks with provable
  generalization?
\newblock \emph{arXiv preprint arXiv:1902.01028}, 2019.

\bibitem[Allen-Zhu et~al.(2018{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2018learning}
Z.~Allen-Zhu, Y.~Li, and Y.~Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{arXiv preprint arXiv:1811.04918}, 2018{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2018{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2018convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018{\natexlab{b}}.

\bibitem[Andoni et~al.(2014)Andoni, Panigrahy, Valiant, and
  Zhang]{andoni2014learning}
A.~Andoni, R.~Panigrahy, G.~Valiant, and L.~Zhang.
\newblock Learning polynomials with neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1908--1916, 2014.

\bibitem[Barron(1993)]{barron1993universal}
A.~R. Barron.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock \emph{IEEE Transactions on Information theory}, 39\penalty0
  (3):\penalty0 930--945, 1993.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Brutzkus and Globerson(2018)]{brutzkus2018over}
A.~Brutzkus and A.~Globerson.
\newblock Over-parameterization improves generalization in the xor detection
  problem.
\newblock \emph{arXiv preprint arXiv:1810.03037}, 2018.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2017sgd}
A.~Brutzkus, A.~Globerson, E.~Malach, and S.~Shalev-Shwartz.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{arXiv preprint arXiv:1710.10174}, 2017.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Y.~Cao and Q.~Gu.
\newblock A generalization theory of gradient descent for learning
  over-parameterized deep {ReLU} networks.
\newblock \emph{arXiv preprint arXiv:1902.01384}, 2019.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
L.~Chizat and F.~Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \emph{Advances in neural information processing systems}, pages
  3040--3050, 2018.

\bibitem[Daniely(2017)]{daniely2017sgd}
A.~Daniely.
\newblock {SGD} learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2017.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
A.~Daniely, R.~Frostig, and Y.~Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  2253--2261, 2016.

\bibitem[Du and Lee(2018)]{du2018power}
S.~S. Du and J.~D. Lee.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock \emph{arXiv preprint arXiv:1803.01206}, 2018.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2018gradient}
S.~S. Du, J.~D. Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient2}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018{\natexlab{b}}.

\bibitem[Gelbaum et~al.(1961)Gelbaum, De~Lamadrid, et~al.]{gelbaum1961bases}
B.~R. Gelbaum, J.~G. De~Lamadrid, et~al.
\newblock Bases of tensor products of banach spaces.
\newblock \emph{Pacific Journal of Mathematics}, 11\penalty0 (4):\penalty0
  1281--1286, 1961.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019linearized}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{arXiv preprint arXiv:1904.12191}, 2019.

\bibitem[Ghorbani et~al.(2021)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2021linearized}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (2):\penalty0
  1029--1054, 2021.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem[Klusowski and Barron(2018)]{klusowski2018approximation}
J.~M. Klusowski and A.~R. Barron.
\newblock Approximation by combinations of {ReLU} and squared {ReLU} ridge
  functions with $l^1$ and $l^0$ controls.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (12):\penalty0 7649--7656, 2018.

\bibitem[Ledoux(2001)]{ledoux2001concentration}
M.~Ledoux.
\newblock \emph{The concentration of measure phenomenon}.
\newblock Number~89. American Mathematical Soc., 2001.

\bibitem[Li and Liang(2018)]{li2018learning}
Y.~Li and Y.~Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8168--8177, 2018.

\bibitem[Li et~al.(2017)Li, Ma, and Zhang]{li2017algorithmic}
Y.~Li, T.~Ma, and H.~Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock \emph{arXiv preprint arXiv:1712.09203}, 2017.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
R.~Livni, S.~Shalev-Shwartz, and O.~Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  855--863, 2014.

\bibitem[Mei et~al.(2016)Mei, Bai, and Montanari]{mei2016landscape}
S.~Mei, Y.~Bai, and A.~Montanari.
\newblock The landscape of empirical risk for non-convex losses.
\newblock \emph{arXiv preprint arXiv:1607.06534}, 2016.

\bibitem[Rahimi and Recht(2008{\natexlab{a}})]{rahimi2008random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pages
  1177--1184, 2008{\natexlab{a}}.

\bibitem[Rahimi and Recht(2008{\natexlab{b}})]{rahimi2008uniform}
A.~Rahimi and B.~Recht.
\newblock Uniform approximation of functions with random bases.
\newblock In \emph{2008 46th Annual Allerton Conference on Communication,
  Control, and Computing}, pages 555--561. IEEE, 2008{\natexlab{b}}.

\bibitem[Rahimi and Recht(2009)]{rahimi2009weighted}
A.~Rahimi and B.~Recht.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1313--1320, 2009.

\bibitem[Safran and Shamir(2016)]{safran2016quality}
I.~Safran and O.~Shamir.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  774--782, 2016.

\bibitem[Safran and Shamir(2017)]{safran2017spurious}
I.~Safran and O.~Shamir.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock \emph{arXiv preprint arXiv:1712.08968}, 2017.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shamir(2018)]{shamir2018distribution}
O.~Shamir.
\newblock Distribution-specific hardness of learning neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1135--1163, 2018.

\bibitem[Soltanolkotabi(2017)]{soltanolkotabi2017learning}
M.~Soltanolkotabi.
\newblock Learning relus via gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2007--2017, 2017.

\bibitem[Soltanolkotabi et~al.(2019)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2019theoretical}
M.~Soltanolkotabi, A.~Javanmard, and J.~D. Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (2):\penalty0 742--769, 2019.

\bibitem[Soudry and Carmon(2016)]{soudry2016no}
D.~Soudry and Y.~Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Soudry and Hoffer(2017)]{soudry2017exponentially}
D.~Soudry and E.~Hoffer.
\newblock Exponentially vanishing sub-optimal local minima in multilayer neural
  networks.
\newblock \emph{arXiv preprint arXiv:1702.05777}, 2017.

\bibitem[Sun et~al.(2018)Sun, Gilbert, and Tewari]{sun2018random}
Y.~Sun, A.~Gilbert, and A.~Tewari.
\newblock Random {ReLU} features: Universality, approximation, and composition.
\newblock \emph{arXiv preprint arXiv:1810.04374}, 2018.

\bibitem[Wang et~al.(2018)Wang, Giannakis, and Chen]{wang2018learning}
G.~Wang, G.~B. Giannakis, and J.~Chen.
\newblock Learning relu networks on linearly separable data: Algorithm,
  optimality, and generalization.
\newblock \emph{arXiv preprint arXiv:1808.04685}, 2018.

\bibitem[Zhang et~al.(2016{\natexlab{a}})Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016{\natexlab{a}}.

\bibitem[Zhang et~al.(2016{\natexlab{b}})Zhang, Lee, and Jordan]{zhang2016l1}
Y.~Zhang, J.~D. Lee, and M.~I. Jordan.
\newblock l1-regularized neural networks are improperly learnable in polynomial
  time.
\newblock In \emph{International Conference on Machine Learning}, pages
  993--1001, 2016{\natexlab{b}}.

\end{thebibliography}
