\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[pok(2016)]{poking}
Learning to poke by poking: Experiential learning of intuitive physics.
\newblock In D.~D. Lee, M.~Sugiyama, U.~von Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{NeurIPS}, 2016.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
M.~Andrychowicz, F.~Wolski, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, P.~Abbeel, and W.~Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:1707.01495}, 2017.

\bibitem[Berkenkamp et~al.(2017)Berkenkamp, Turchetta, Schoellig, and
  Krause]{berkenkamp2017safe}
F.~Berkenkamp, M.~Turchetta, A.~P. Schoellig, and A.~Krause.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock \emph{arXiv preprint arXiv:1705.08551}, 2017.

\bibitem[Chebotar et~al.(2017)Chebotar, Hausman, Zhang, Sukhatme, Schaal, and
  Levine]{chebotar2017combining}
Y.~Chebotar, K.~Hausman, M.~Zhang, G.~Sukhatme, S.~Schaal, and S.~Levine.
\newblock Combining model-based and model-free updates for trajectory-centric
  reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  703--711. PMLR, 2017.

\bibitem[Chow et~al.(2018)Chow, Nachum, Duenez-Guzman, and
  Ghavamzadeh]{chow2018lyapunov}
Y.~Chow, O.~Nachum, E.~Duenez-Guzman, and M.~Ghavamzadeh.
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1805.07708}, 2018.

\bibitem[Eysenbach et~al.(2017)Eysenbach, Gu, Ibarz, and
  Levine]{eysenbach2017leave}
B.~Eysenbach, S.~Gu, J.~Ibarz, and S.~Levine.
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.06782}, 2017.

\bibitem[Finn and Levine(2017)]{finn2017deep}
C.~Finn and S.~Levine.
\newblock Deep visual foresight for planning robot motion.
\newblock In \emph{2017 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 2786--2793. IEEE, 2017.

\bibitem[Finn et~al.(2016)Finn, Tan, Duan, Darrell, Levine, and
  Abbeel]{finn2016deep}
C.~Finn, X.~Y. Tan, Y.~Duan, T.~Darrell, S.~Levine, and P.~Abbeel.
\newblock Deep spatial autoencoders for visuomotor learning.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 512--519. IEEE, 2016.

\bibitem[Florensa et~al.(2017)Florensa, Held, Wulfmeier, Zhang, and
  Abbeel]{florensa2017reverse}
C.~Florensa, D.~Held, M.~Wulfmeier, M.~Zhang, and P.~Abbeel.
\newblock Reverse curriculum generation for reinforcement learning.
\newblock In \emph{Conference on robot learning}, pages 482--495. PMLR, 2017.

\bibitem[Florensa et~al.(2018)Florensa, Held, Geng, and
  Abbeel]{florensa2018automatic}
C.~Florensa, D.~Held, X.~Geng, and P.~Abbeel.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In \emph{International conference on machine learning}, pages
  1515--1528. PMLR, 2018.

\bibitem[Garc{\i}a and Fern{\'a}ndez(2015)]{garcia2015comprehensive}
J.~Garc{\i}a and F.~Fern{\'a}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Ghadirzadeh et~al.(2017)Ghadirzadeh, Maki, Kragic, and
  Bj{\"o}rkman]{ghadirzadeh2017deep}
A.~Ghadirzadeh, A.~Maki, D.~Kragic, and M.~Bj{\"o}rkman.
\newblock Deep predictive policy training using reinforcement learning.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 2351--2358. IEEE, 2017.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
S.~Gu, E.~Holly, T.~Lillicrap, and S.~Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pages 3389--3396. IEEE, 2017.

\bibitem[Gupta et~al.(2021)Gupta, Yu, Zhao, Kumar, Rovinsky, Xu, Devlin, and
  Levine]{Gupta2021ResetFreeRL}
A.~Gupta, J.~Yu, T.~Zhao, V.~Kumar, A.~Rovinsky, K.~Xu, T.~Devlin, and
  S.~Levine.
\newblock Reset-free reinforcement learning via multi-task learning: Learning
  dexterous manipulation behaviors without human intervention.
\newblock \emph{ArXiv}, abs/2104.11203, 2021.

\bibitem[Ha et~al.(2020)Ha, Xu, Tan, Levine, and Tan]{ha2020learning}
S.~Ha, P.~Xu, Z.~Tan, S.~Levine, and J.~Tan.
\newblock Learning to walk in the real world with minimal human effort, 2020.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Ha, Zhou, Tan, Tucker,
  and Levine]{haarnoja2018learning}
T.~Haarnoja, S.~Ha, A.~Zhou, J.~Tan, G.~Tucker, and S.~Levine.
\newblock Learning to walk via deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.11103}, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018{\natexlab{b}}.

\bibitem[Hafner et~al.(2017)Hafner, Davidson, and
  Vanhoucke]{hafner2017tensorflow}
D.~Hafner, J.~Davidson, and V.~Vanhoucke.
\newblock Tensorflow agents: Efficient batched reinforcement learning in
  tensorflow.
\newblock \emph{arXiv preprint arXiv:1709.02878}, 2017.

\bibitem[Han et~al.(2015)Han, Levine, and Abbeel]{han2015learning}
W.~Han, S.~Levine, and P.~Abbeel.
\newblock Learning compound multi-step controllers under unknown dynamics.
\newblock In \emph{2015 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 6435--6442. IEEE, 2015.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
L.~P. Kaelbling.
\newblock Learning to achieve goals.
\newblock In \emph{IJCAI}, pages 1094--1099. Citeseer, 1993.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
S.~Kakade and J.~Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
D.~Kalashnikov, A.~Irpan, P.~Pastor, J.~Ibarz, A.~Herzog, E.~Jang, D.~Quillen,
  E.~Holly, M.~Kalakrishnan, V.~Vanhoucke, et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Kalashnikov et~al.(2021)Kalashnikov, Varley, Chebotar, Swanson,
  Jonschkowski, Finn, Levine, and Hausman]{kalashnikov2021mt}
D.~Kalashnikov, J.~Varley, Y.~Chebotar, B.~Swanson, R.~Jonschkowski, C.~Finn,
  S.~Levine, and K.~Hausman.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale.
\newblock \emph{arXiv preprint arXiv:2104.08212}, 2021.

\bibitem[Khetarpal et~al.(2020)Khetarpal, Riemer, Rish, and
  Precup]{khetarpal2020towards}
K.~Khetarpal, M.~Riemer, I.~Rish, and D.~Precup.
\newblock Towards continual reinforcement learning: A review and perspectives.
\newblock \emph{arXiv preprint arXiv:2012.13490}, 2020.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
J.~Kober, J.~A. Bagnell, and J.~Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Kohl and Stone(2004)]{kohl2004policy}
N.~Kohl and P.~Stone.
\newblock Policy gradient reinforcement learning for fast quadrupedal
  locomotion.
\newblock In \emph{IEEE International Conference on Robotics and Automation,
  2004. Proceedings. ICRA'04. 2004}, volume~3, pages 2619--2624. IEEE, 2004.

\bibitem[Konidaris and Barto(2009)]{konidaris2009skill}
G.~Konidaris and A.~Barto.
\newblock Skill discovery in continuous reinforcement learning domains using
  skill chaining.
\newblock \emph{Advances in neural information processing systems},
  22:\penalty0 1015--1023, 2009.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
S.~Levine, C.~Finn, T.~Darrell, and P.~Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lu et~al.(2020)Lu, Grover, Abbeel, and Mordatch]{lu2020reset}
K.~Lu, A.~Grover, P.~Abbeel, and I.~Mordatch.
\newblock Reset-free lifelong learning with skill-space planning.
\newblock \emph{arXiv preprint arXiv:2012.03548}, 2020.

\bibitem[Matiisen et~al.(2019)Matiisen, Oliver, Cohen, and
  Schulman]{matiisen2019teacher}
T.~Matiisen, A.~Oliver, T.~Cohen, and J.~Schulman.
\newblock Teacher--student curriculum learning.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  31\penalty0 (9):\penalty0 3732--3740, 2019.

\bibitem[Moldovan and Abbeel(2012)]{moldovan2012safe}
T.~M. Moldovan and P.~Abbeel.
\newblock Safe exploration in markov decision processes.
\newblock \emph{arXiv preprint arXiv:1205.4810}, 2012.

\bibitem[Nagabandi et~al.(2020)Nagabandi, Konolige, Levine, and
  Kumar]{nagabandi2020deep}
A.~Nagabandi, K.~Konolige, S.~Levine, and V.~Kumar.
\newblock Deep dynamics models for learning dexterous manipulation.
\newblock In \emph{Conference on Robot Learning}, pages 1101--1112. PMLR, 2020.

\bibitem[Narvekar and Stone(2018)]{narvekar2018learning}
S.~Narvekar and P.~Stone.
\newblock Learning curriculum policies for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.00285}, 2018.

\bibitem[Ng et~al.(2003)Ng, Kim, Jordan, Sastry, and
  Ballianda]{ng2003autonomous}
A.~Y. Ng, H.~J. Kim, M.~I. Jordan, S.~Sastry, and S.~Ballianda.
\newblock Autonomous helicopter flight via reinforcement learning.
\newblock In \emph{NIPS}, volume~16. Citeseer, 2003.

\bibitem[Pinto and Gupta(2016)]{pinto2016supersizing}
L.~Pinto and A.~Gupta.
\newblock Supersizing self-supervision: Learning to grasp from 50k tries and
  700 robot hours.
\newblock In \emph{2016 IEEE international conference on robotics and
  automation (ICRA)}, pages 3406--3413. IEEE, 2016.

\bibitem[Pong et~al.(2018)Pong, Gu, Dalal, and Levine]{pong2018temporal}
V.~Pong, S.~Gu, M.~Dalal, and S.~Levine.
\newblock Temporal difference models: Model-free deep rl for model-based
  control.
\newblock \emph{arXiv preprint arXiv:1802.09081}, 2018.

\bibitem[Puterman(1990)]{puterman1990markov}
M.~L. Puterman.
\newblock Markov decision processes.
\newblock \emph{Handbooks in operations research and management science},
  2:\penalty0 331--434, 1990.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{schaul2015universal}
T.~Schaul, D.~Horgan, K.~Gregor, and D.~Silver.
\newblock Universal value function approximators.
\newblock In \emph{International conference on machine learning}, pages
  1312--1320. PMLR, 2015.

\bibitem[Sharma et~al.(2020)Sharma, Ahn, Levine, Kumar, Hausman, and
  Gu]{sharma2020emergent}
A.~Sharma, M.~Ahn, S.~Levine, V.~Kumar, K.~Hausman, and S.~Gu.
\newblock Emergent real-world robotic skills via unsupervised off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.12974}, 2020.

\bibitem[Sukhbaatar et~al.(2017)Sukhbaatar, Lin, Kostrikov, Synnaeve, Szlam,
  and Fergus]{sukhbaatar2017intrinsic}
S.~Sukhbaatar, Z.~Lin, I.~Kostrikov, G.~Synnaeve, A.~Szlam, and R.~Fergus.
\newblock Intrinsic motivation and automatic curricula via asymmetric
  self-play.
\newblock \emph{arXiv preprint arXiv:1703.05407}, 2017.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Thananjeyan et~al.(2020)Thananjeyan, Balakrishna, Nair, Luo,
  Srinivasan, Hwang, Gonzalez, Ibarz, Finn, and
  Goldberg]{thananjeyan2020recovery}
B.~Thananjeyan, A.~Balakrishna, S.~Nair, M.~Luo, K.~Srinivasan, M.~Hwang, J.~E.
  Gonzalez, J.~Ibarz, C.~Finn, and K.~Goldberg.
\newblock Recovery rl: Safe reinforcement learning with learned recovery zones.
\newblock \emph{arXiv preprint arXiv:2010.15920}, 2020.

\bibitem[Xu et~al.(2020)Xu, Verma, Finn, and Levine]{Xu2020ContinualLO}
K.~Xu, S.~Verma, C.~Finn, and S.~Levine.
\newblock Continual learning of control primitives: Skill discovery via
  reset-games.
\newblock \emph{ArXiv}, abs/2011.05286, 2020.

\bibitem[Yu et~al.(2020{\natexlab{a}})Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
T.~Yu, S.~Kumar, A.~Gupta, S.~Levine, K.~Hausman, and C.~Finn.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{arXiv preprint arXiv:2001.06782}, 2020{\natexlab{a}}.

\bibitem[Yu et~al.(2020{\natexlab{b}})Yu, Quillen, He, Julian, Hausman, Finn,
  and Levine]{yu2020meta}
T.~Yu, D.~Quillen, Z.~He, R.~Julian, K.~Hausman, C.~Finn, and S.~Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pages 1094--1100. PMLR,
  2020{\natexlab{b}}.

\bibitem[Zeng et~al.(2020)Zeng, Song, Lee, Rodriguez, and
  Funkhouser]{zeng2020tossingbot}
A.~Zeng, S.~Song, J.~Lee, A.~Rodriguez, and T.~Funkhouser.
\newblock Tossingbot: Learning to throw arbitrary objects with residual
  physics.
\newblock \emph{IEEE Transactions on Robotics}, 36\penalty0 (4):\penalty0
  1307--1319, 2020.

\bibitem[Zhu et~al.(2019)Zhu, Gupta, Rajeswaran, Levine, and
  Kumar]{zhu2019dexterous}
H.~Zhu, A.~Gupta, A.~Rajeswaran, S.~Levine, and V.~Kumar.
\newblock Dexterous manipulation with deep reinforcement learning: Efficient,
  general, and low-cost.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pages 3651--3657. IEEE, 2019.

\bibitem[Zhu et~al.(2020)Zhu, Yu, Gupta, Shah, Hartikainen, Singh, Kumar, and
  Levine]{zhu2020ingredients}
H.~Zhu, J.~Yu, A.~Gupta, D.~Shah, K.~Hartikainen, A.~Singh, V.~Kumar, and
  S.~Levine.
\newblock The ingredients of real-world robotic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.12570}, 2020.

\end{thebibliography}
