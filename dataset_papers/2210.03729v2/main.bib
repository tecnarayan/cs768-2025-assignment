=== Introduction ===

@article{agarwal2022reincarnating,
  title={Reincarnating reinforcement learning: Reusing prior computation to accelerate progress},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28955--28971},
  year={2022}
}

@inproceedings{
Qureshi2020Composing,
title={Composing Task-Agnostic Policies with Deep Reinforcement Learning},
author={Ahmed H. Qureshi and Jacob J. Johnson and Yuzhe Qin and Taylor Henderson and Byron Boots and Michael C. Yip},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1ezFREtwH}
}

@article{sakellaridi2019intrinsic,
  title={Intrinsic variable learning for brain-machine interface control by human anterior intraparietal cortex},
  author={Sakellaridi, Sofia and Christopoulos, Vassilios N and Aflalo, Tyson and Pejsa, Kelsie W and Rosario, Emily R and Ouellette, Debra and Pouratian, Nader and Andersen, Richard A},
  journal={Neuron},
  volume={102},
  number={3},
  pages={694--705},
  year={2019},
  publisher={Elsevier}
}

@book{bandura1977social,
  title={Social Learning Theory},
  author={Bandura, A.},
  isbn={9780138167516},
  lccn={76343024},
  series={Prentice-Hall series in social learning theory},
  year={1977},
  publisher={Prentice Hall}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

@article{bellemare2020autonomous,
  title={Autonomous navigation of stratospheric balloons using reinforcement learning},
  author={Bellemare, Marc G and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C and Moitra, Subhodeep and Ponda, Sameera S and Wang, Ziyu},
  journal={Nature},
  volume={588},
  number={7836},
  pages={77--82},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{degrave2022magnetic,
  title={Magnetic control of tokamak plasmas through deep reinforcement learning},
  author={Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and de Las Casas, Diego and others},
  journal={Nature},
  volume={602},
  number={7897},
  pages={414--419},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{wurman2022outracing,
  title={Outracing champion Gran Turismo drivers with deep reinforcement learning},
  author={Wurman, Peter R and Barrett, Samuel and Kawamoto, Kenta and MacGlashan, James and Subramanian, Kaushik and Walsh, Thomas J and Capobianco, Roberto and Devlic, Alisa and Eckert, Franziska and Fuchs, Florian and others},
  journal={Nature},
  volume={602},
  number={7896},
  pages={223--228},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{song2021deep,
  title={Deep reinforcement learning for modeling human locomotion control in neuromechanical simulation},
  author={Song, Seungmoon and Kidzi{\'n}ski, {\L}ukasz and Peng, Xue Bin and Ong, Carmichael and Hicks, Jennifer and Levine, Sergey and Atkeson, Christopher G and Delp, Scott L},
  journal={Journal of neuroengineering and rehabilitation},
  volume={18},
  number={1},
  pages={1--17},
  year={2021},
  publisher={Springer}
}

@article{zhang2019leveraging,
  title={Leveraging human guidance for deep reinforcement learning tasks},
  author={Zhang, Ruohan and Torabi, Faraz and Guan, Lin and Ballard, Dana H and Stone, Peter},
  journal={arXiv preprint arXiv:1909.09906},
  year={2019}
}

@article{billard2016learning,
  title={Learning from humans},
  author={Billard, Aude G and Calinon, Sylvain and Dillmann, R{\"u}diger},
  journal={Springer handbook of robotics},
  pages={1995--2014},
  year={2016},
  publisher={Springer}
}

@article{kaelbling2020foundation,
  title={The foundation of efficient robot learning},
  author={Kaelbling, Leslie Pack},
  journal={Science},
  volume={369},
  number={6506},
  pages={915--916},
  year={2020},
  publisher={American Association for the Advancement of Science}
}

@article{anderson2008asimov,
  title={Asimov’s “three laws of robotics” and machine metaethics},
  author={Anderson, Susan Leigh},
  journal={Ai \& Society},
  volume={22},
  number={4},
  pages={477--493},
  year={2008},
  publisher={Springer}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{heess2017emergence,
  title={Emergence of locomotion behaviours in rich environments},
  author={Heess, Nicolas and TB, Dhruva and Sriram, Srinivasan and Lemmon, Jay and Merel, Josh and Wayne, Greg and Tassa, Yuval and Erez, Tom and Wang, Ziyu and Eslami, SM and others},
  journal={arXiv preprint arXiv:1707.02286},
  year={2017}
}

@inproceedings{kalashnikov2018scalable,
  title={Scalable deep reinforcement learning for vision-based robotic manipulation},
  author={Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and others},
  booktitle={Conference on Robot Learning},
  pages={651--673},
  year={2018},
  organization={PMLR}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}

@article{wang2016sample,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@article{kaiser2019model,
  title={Model-based reinforcement learning for atari},
  author={Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and others},
  journal={arXiv preprint arXiv:1903.00374},
  year={2019}
}

@article{tao2020novelty,
  title={Novelty search in representational space for sample efficient exploration},
  author={Tao, Ruo Yu and Fran{\c{c}}ois-Lavet, Vincent and Pineau, Joelle},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8114--8126},
  year={2020}
}

@inproceedings{scheller2020sample,
  title={Sample efficient reinforcement learning through learning from demonstrations in minecraft},
  author={Scheller, Christian and Schraner, Yanick and Vogel, Manfred},
  booktitle={NeurIPS 2019 Competition and Demonstration Track},
  pages={67--76},
  year={2020},
  organization={PMLR}
}

@article{landolfi2019model,
  title={A model-based approach for sample-efficient multi-task reinforcement learning},
  author={Landolfi, Nicholas C and Thomas, Garrett and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.04964},
  year={2019}
}

=== Method ===
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{vaswani2017attention,
  title={Attention is All you Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{dong2018speech,
  title={Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition},
  author={Dong, Linhao and Xu, Shuang and Xu, Bo},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5884--5888},
  year={2018},
  organization={IEEE}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{lewis2020bart,
  title={BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7871--7880},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}


@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2018}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

=== Explainable Machine Learning Works ===

@book{molnar2020interpretable,
  title={Interpretable Machine Learning},
  author={Molnar, Christoph},
  year={2020},
  publisher={Lulu. com}
}

@inproceedings{alvarez2018towards,
  title={Towards Robust Interpretability with Self-Explaining Neural Networks},
  author={Alvarez-Melis, David and Jaakkola, Tommi S},
  booktitle={NeurIPS},
  year={2018}
}

@article{hao2020self,
  title={Self-attention attribution: Interpreting information interactions inside transformer},
  author={Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
  journal={arXiv preprint arXiv:2004.11207},
  year={2020}
}

@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}
=== Baselines ===
@inproceedings{Bain1995AFF,
  title={A Framework for Behavioural Cloning},
  author={Michael Bain and Claude Sammut},
  booktitle={Machine Intelligence 15},
  year={1995}
}

@inproceedings{Zhang2020KoGuNAD,
  title={KoGuN: Accelerating Deep Reinforcement Learning via Integrating Human Suboptimal Knowledge},
  author={Peng Zhang and Jianye Hao and Weixun Wang and Hongyao Tang and Yi Ma and Yihai Duan and Yan Zheng},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{
rajendran2017attend,
title={Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain},
author={Janarthanan Rajendran and Aravind Lakshminarayanan and Mitesh M. Khapra and Prasanna P and Balaraman Ravindran},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Sy6iJDqlx}
}


=== Related Work ===
%External strategies
@inproceedings{kartal2019action,
  title={Action guidance with MCTS for deep reinforcement learning},
  author={Kartal, Bilal and Hernandez-Leal, Pablo and Taylor, Matthew E},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  volume={15},
  number={1},
  pages={153--159},
  year={2019}
}

@inproceedings{zhang2021deep,
  title={Deep Q-learning with Explainable and Transferable Domain Rules},
  author={Zhang, Yichuan and Ren, Junkai and Li, Junxiang and Fang, Qiang and Xu, Xin},
  booktitle={International Conference on Intelligent Computing},
  pages={259--273},
  year={2021},
  organization={Springer}
}

@inproceedings{li2016deep,
  title={Deep Reinforcement Learning for Dialogue Generation},
  author={Li, Jiwei and Monroe, Will and Ritter, Alan and Jurafsky, Dan and Galley, Michel and Gao, Jianfeng},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={1192--1202},
  year={2016}
}

@inproceedings{gao2020knowledge,
  title={Knowledge-guided reinforcement learning control for robotic lower limb prosthesis},
  author={Gao, Xiang and Si, Jennie and Wen, Yue and Li, Minhan and Huang, He Helen},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={754--760},
  year={2020},
  organization={IEEE}
}

@inproceedings{wiewiora2003principled,
  title={Principled methods for advising reinforcement learning agents},
  author={Wiewiora, Eric and Cottrell, Garrison W and Elkan, Charles},
  booktitle={Proceedings of the 20th international conference on machine learning (ICML-03)},
  pages={792--799},
  year={2003}
}

@inproceedings{konidaris2006autonomous,
  title={Autonomous shaping: Knowledge transfer in reinforcement learning},
  author={Konidaris, George and Barto, Andrew},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={489--496},
  year={2006}
}

@inproceedings{tang2020neuroevolution,
  title={Neuroevolution of self-interpretable agents},
  author={Tang, Yujin and Nguyen, Duong and Ha, David},
  booktitle={Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
  pages={414--424},
  year={2020}
}

@article{sorokin2015deep,
  title={Deep attention recurrent Q-network},
  author={Sorokin, Ivan and Seleznev, Alexey and Pavlov, Mikhail and Fedorov, Aleksandr and Ignateva, Anastasiia},
  journal={arXiv preprint arXiv:1512.01693},
  year={2015}
}

@article{milani2022survey,
  title={A Survey of Explainable Reinforcement Learning},
  author={Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
  journal={arXiv preprint arXiv:2202.08434},
  year={2022}
}

@article{mott2019towards,
  title={Towards interpretable reinforcement learning using attention augmented agents},
  author={Mott, Alexander and Zoran, Daniel and Chrzanowski, Mike and Wierstra, Daan and Jimenez Rezende, Danilo},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{guo2021edge,
  title={EDGE: Explaining Deep Reinforcement Learning Policies},
  author={Guo, Wenbo and Wu, Xian and Khan, Usmann and Xing, Xinyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

%Imitation learning
@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{duan2017one,
  title={One-shot imitation learning},
  author={Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly and Jonathan Ho, OpenAI and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{peng2018deepmimic,
  title={Deepmimic: Example-guided deep reinforcement learning of physics-based character skills},
  author={Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and Van de Panne, Michiel},
  journal={ACM Transactions On Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--14},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@article{peng2018variational,
  title={Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow},
  author={Peng, Xue Bin and Kanazawa, Angjoo and Toyer, Sam and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.00821},
  year={2018}
}

@article{ding2019goal,
  title={Goal-conditioned imitation learning},
  author={Ding, Yiming and Florensa, Carlos and Abbeel, Pieter and Phielipp, Mariano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

%safe RL
@inproceedings{ammar2015safe,
  title={Safe policy search for lifelong reinforcement learning with sublinear regret},
  author={Ammar, Haitham Bou and Tutunov, Rasul and Eaton, Eric},
  booktitle={International Conference on Machine Learning},
  pages={2361--2369},
  year={2015},
  organization={PMLR}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={22--31},
  year={2017},
  organization={PMLR}
}

@article{chow2018lyapunov,
  title={A lyapunov-based approach to safe reinforcement learning},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{stooke2020responsive,
  title={Responsive safety in reinforcement learning by pid lagrangian methods},
  author={Stooke, Adam and Achiam, Joshua and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={9133--9143},
  year={2020},
  organization={PMLR}
}

@inproceedings{ding2021provably,
  title={Provably efficient safe exploration via primal-dual policy optimization},
  author={Ding, Dongsheng and Wei, Xiaohan and Yang, Zhuoran and Wang, Zhaoran and Jovanovic, Mihailo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3304--3312},
  year={2021},
  organization={PMLR}
}

@article{garcia2012safe,
  title={Safe exploration of state and action spaces in reinforcement learning},
  author={Garcia, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Artificial Intelligence Research},
  volume={45},
  pages={515--564},
  year={2012}
}

@article{berkenkamp2017safe,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{alshiekh2018safe,
  title={Safe reinforcement learning via shielding},
  author={Alshiekh, Mohammed and Bloem, Roderick and Ehlers, R{\"u}diger and K{\"o}nighofer, Bettina and Niekum, Scott and Topcu, Ufuk},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{cheng2019end,
  title={End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks},
  author={Cheng, Richard and Orosz, G{\'a}bor and Murray, Richard M and Burdick, Joel W},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3387--3395},
  year={2019}
}

@article{thananjeyan2021recovery,
  title={Recovery rl: Safe reinforcement learning with learned recovery zones},
  author={Thananjeyan, Brijen and Balakrishna, Ashwin and Nair, Suraj and Luo, Michael and Srinivasan, Krishnan and Hwang, Minho and Gonzalez, Joseph E and Ibarz, Julian and Finn, Chelsea and Goldberg, Ken},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={3},
  pages={4915--4922},
  year={2021},
  publisher={IEEE}
}

@inproceedings{liu2022robot,
  title={Robot reinforcement learning on the constraint manifold},
  author={Liu, Puze and Tateo, Davide and Ammar, Haitham Bou and Peters, Jan},
  booktitle={Conference on Robot Learning},
  pages={1357--1366},
  year={2022},
  organization={PMLR}
}

=== Knowledge Transfer in RL ===
@article{rusu2015policy,
  title={Policy distillation},
  author={Rusu, Andrei A and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  journal={arXiv preprint arXiv:1511.06295},
  year={2015}
}

@article{parisotto2015actor,
  title={Actor-mimic: Deep multitask and transfer reinforcement learning},
  author={Parisotto, Emilio and Ba, Jimmy Lei and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1511.06342},
  year={2015}
}

@inproceedings{yin2017knowledge,
  title={Knowledge transfer for deep reinforcement learning with hierarchical experience replay},
  author={Yin, Haiyan and Pan, Sinno Jialin},
  booktitle={Thirty-First AAAI conference on artificial intelligence},
  year={2017}
}

@article{xu2020knowledge,
  title={Knowledge transfer in multi-task deep reinforcement learning for continuous control},
  author={Xu, Zhiyuan and Wu, Kun and Che, Zhengping and Tang, Jian and Ye, Jieping},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15146--15155},
  year={2020}
}

@inproceedings{tao2021repaint,
  title={REPAINT: Knowledge Transfer in Deep Reinforcement Learning},
  author={Tao, Yunzhe and Genc, Sahika and Chung, Jonathan and Sun, Tao and Mallya, Sunil},
  booktitle={International Conference on Machine Learning},
  pages={10141--10152},
  year={2021},
  organization={PMLR}
}

@inproceedings{finn2017model,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2017}
}

@article{gupta2018meta,
  title={Meta-reinforcement learning of structured exploration strategies},
  author={Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{gupta2018unsupervised,
  title={Unsupervised meta-learning for reinforcement learning},
  author={Gupta, Abhishek and Eysenbach, Benjamin and Finn, Chelsea and Levine, Sergey},
  journal={arXiv preprint arXiv:1806.04640},
  year={2018}
}

@article{nagabandi2018learning,
  title={Learning to adapt in dynamic, real-world environments through meta-reinforcement learning},
  author={Nagabandi, Anusha and Clavera, Ignasi and Liu, Simin and Fearing, Ronald S and Abbeel, Pieter and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1803.11347},
  year={2018}
}

@inproceedings{liu2019taming,
  title={Taming maml: Efficient unbiased meta-reinforcement learning},
  author={Liu, Hao and Socher, Richard and Xiong, Caiming},
  booktitle={International conference on machine learning},
  pages={4061--4071},
  year={2019},
  organization={PMLR}
}

@inproceedings{glatt2016towards,
  title={Towards knowledge transfer in deep reinforcement learning},
  author={Glatt, Ruben and Da Silva, Felipe Leno and Costa, Anna Helena Reali},
  booktitle={2016 5th Brazilian Conference on Intelligent Systems (BRACIS)},
  pages={91--96},
  year={2016},
  organization={IEEE}
}


=== Related Work - Real Robot ===
@inproceedings{gao2020knowledge,
  title={Knowledge-guided reinforcement learning control for robotic lower limb prosthesis},
  author={Gao, Xiang and Si, Jennie and Wen, Yue and Li, Minhan and Huang, He Helen},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={754--760},
  year={2020},
  organization={IEEE}
}

@inproceedings{chiu2021bimanual,
  title={Bimanual regrasping for suture needles using reinforcement learning for rapid motion planning},
  author={Chiu, Zih-Yun and Richter, Florian and Funk, Emily K and Orosco, Ryan K and Yip, Michael C},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={7737--7743},
  year={2021},
  organization={IEEE}
}


=== HRL ===
@article{dayan1992feudal,
  title={Feudal reinforcement learning},
  author={Dayan, Peter and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@article{sutton1999between,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{kulkarni2016hierarchical,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{levy2017learning,
  title={Learning multi-level hierarchies with hindsight},
  author={Levy, Andrew and Konidaris, George and Platt, Robert and Saenko, Kate},
  journal={arXiv preprint arXiv:1712.00948},
  year={2017}
}

@article{nachum2018data,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{jiang2019language,
  title={Language as an abstraction for hierarchical deep reinforcement learning},
  author={Jiang, Yiding and Gu, Shixiang Shane and Murphy, Kevin P and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{stolle2002learning,
  title={Learning options in reinforcement learning},
  author={Stolle, Martin and Precup, Doina},
  booktitle={International Symposium on abstraction, reformulation, and approximation},
  pages={212--223},
  year={2002},
  organization={Springer}
}

@inproceedings{bacon2017option,
  title={The option-critic architecture},
  author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={3540--3549},
  year={2017},
  organization={PMLR}
}

@article{nachum2018near,
  title={Near-optimal representation learning for hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.01257},
  year={2018}
}

@article{riemer2018learning,
  title={Learning abstract options},
  author={Riemer, Matthew and Liu, Miao and Tesauro, Gerald},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{eysenbach2018diversity,
  title={Diversity is all you need: Learning skills without a reward function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018}
}

@inproceedings{khetarpal2019learning,
  title={Learning options with interest functions},
  author={Khetarpal, Khimya and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={9955--9956},
  year={2019}
}

@article{zhang2019dac,
  title={DAC: The double actor-critic architecture for learning options},
  author={Zhang, Shangtong and Whiteson, Shimon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{kim2021unsupervised,
  title={Unsupervised skill discovery with bottleneck option learning},
  author={Kim, Jaekyeom and Park, Seohong and Kim, Gunhee},
  journal={arXiv preprint arXiv:2106.14305},
  year={2021}
}

@inproceedings{konidaris2007building,
  title={Building Portable Options: Skill Transfer in Reinforcement Learning.},
  author={Konidaris, George Dimitri and Barto, Andrew G},
  booktitle={IJCAI},
  volume={7},
  pages={895--900},
  year={2007}
}

@article{frans2017meta,
  title={Meta learning shared hierarchies},
  author={Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  journal={arXiv preprint arXiv:1710.09767},
  year={2017}
}

@inproceedings{tessler2017deep,
  title={A deep hierarchical approach to lifelong learning in minecraft},
  author={Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{peng2019mcp,
  title={Mcp: Learning composable hierarchical control with multiplicative compositional policies},
  author={Peng, Xue Bin and Chang, Michael and Zhang, Grace and Abbeel, Pieter and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{qureshi2019composing,
  title={Composing task-agnostic policies with deep reinforcement learning},
  author={Qureshi, Ahmed H and Johnson, Jacob J and Qin, Yuzhe and Henderson, Taylor and Boots, Byron and Yip, Michael C},
  journal={arXiv preprint arXiv:1905.10681},
  year={2019}
}

@inproceedings{tseng2021toward,
  title={Toward Robust Long Range Policy Transfer},
  author={Tseng, Wei-Cheng and Lin, Jin-Siang and Feng, Yao-Min and Sun, Min},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9958--9966},
  year={2021}
}

=== Program-Guided RL ===
@inproceedings{sun2019program,
  title={Program guided agent},
  author={Sun, Shao-Hua and Wu, Te-Lin and Lim, Joseph J},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{trivedi2021learning,
  title={Learning to synthesize programs as interpretable and generalizable policies},
  author={Trivedi, Dweep and Zhang, Jesse and Sun, Shao-Hua and Lim, Joseph J},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25146--25163},
  year={2021}
}

@article{zhao2021proto,
  title={Proto: Program-guided transformer for program-guided tasks},
  author={Zhao, Zelin and Samel, Karan and Chen, Binghong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{brooks2021reinforcement,
  title={Reinforcement learning of implicit and explicit control flow instructions},
  author={Brooks, Ethan and Rajendran, Janarthanan and Lewis, Richard L and Singh, Satinder},
  booktitle={International Conference on Machine Learning},
  pages={1082--1091},
  year={2021},
  organization={PMLR}
}

@article{yang2021program,
  title={Program Synthesis Guided Reinforcement Learning for Partially Observed Environments},
  author={Yang, Yichen and Inala, Jeevana Priya and Bastani, Osbert and Pu, Yewen and Solar-Lezama, Armando and Rinard, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

=== Learning from Demonstration ===
@article{rajeswaran2017learning,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
}

@article{hester2017learning,
  title={Learning from demonstrations for real world reinforcement learning},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Sendonaris, Andrew and Dulac-Arnold, Gabriel and Osband, Ian and Agapiou, John and others},
  year={2017}
}

@inproceedings{nair2018overcoming,
  title={Overcoming exploration in reinforcement learning with demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={6292--6299},
  year={2018},
  organization={IEEE}
}

@article{pfeiffer2018reinforced,
  title={Reinforced imitation: Sample efficient deep reinforcement learning for mapless navigation by leveraging prior demonstrations},
  author={Pfeiffer, Mark and Shukla, Samarth and Turchetta, Matteo and Cadena, Cesar and Krause, Andreas and Siegwart, Roland and Nieto, Juan},
  journal={IEEE Robotics and Automation Letters},
  volume={3},
  number={4},
  pages={4423--4430},
  year={2018},
  publisher={IEEE}
}

@article{goecks2019integrating,
  title={Integrating behavior cloning and reinforcement learning for improved performance in dense and sparse reward environments},
  author={Goecks, Vinicius G and Gremillion, Gregory M and Lawhern, Vernon J and Valasek, John and Waytowich, Nicholas R},
  journal={arXiv preprint arXiv:1910.04281},
  year={2019}
}

@article{zhu2018reinforcement,
  title={Reinforcement and imitation learning for diverse visuomotor skills},
  author={Zhu, Yuke and Wang, Ziyu and Merel, Josh and Rusu, Andrei and Erez, Tom and Cabi, Serkan and Tunyasuvunakool, Saran and Kram{\'a}r, J{\'a}nos and Hadsell, Raia and de Freitas, Nando and others},
  journal={arXiv preprint arXiv:1802.09564},
  year={2018}
}

=== Experiments ===
@misc{gym_minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for OpenAI Gym},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/maximecb/gym-minigrid}},
}

@article{plappert2018multi,
  title={Multi-goal reinforcement learning: Challenging robotics environments and request for research},
  author={Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and others},
  journal={arXiv preprint arXiv:1802.09464},
  year={2018}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

=== Appendix ===
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

% Maximizing entropy in RL
@book{ziebart2010modeling,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010},
  publisher={Carnegie Mellon University}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1352--1361},
  year={2017},
  organization={PMLR}
}

%Other
@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC},
  year={2003},
  publisher={Cambridge university press}
}

@inproceedings{huber2008entropy,
  title={On entropy approximation for Gaussian mixture random vectors},
  author={Huber, Marco F and Bailey, Tim and Durrant-Whyte, Hugh and Hanebeck, Uwe D},
  booktitle={2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
  pages={181--188},
  year={2008},
  organization={IEEE}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{hirschman1957note,
  title={A note on entropy},
  author={Hirschman, Isidore I},
  journal={American journal of mathematics},
  volume={79},
  number={1},
  pages={152--156},
  year={1957},
  publisher={JSTOR}
}

@inproceedings{hershey2007approximating,
  title={Approximating the Kullback Leibler divergence between Gaussian mixture models},
  author={Hershey, John R and Olsen, Peder A},
  booktitle={2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07},
  volume={4},
  pages={IV--317},
  year={2007},
  organization={IEEE}
}

@inproceedings{wu2018low,
  title={A low-cost ethics shaping approach for designing reinforcement learning agents},
  author={Wu, Yueh-Hua and Lin, Shou-De},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{ecoffet2021reinforcement,
  title={Reinforcement learning under moral uncertainty},
  author={Ecoffet, Adrien and Lehman, Joel},
  booktitle={International conference on machine learning},
  pages={2926--2936},
  year={2021},
  organization={PMLR}
}
