\begin{thebibliography}{10}

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{liang2022holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  et~al.
\newblock Holistic evaluation of language models.
\newblock {\em arXiv preprint arXiv:2211.09110}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock {\em arXiv preprint arXiv:2202.12837}, 2022.

\bibitem{chan2022data}
Stephanie~CY Chan, Adam Santoro, Andrew~Kyle Lampinen, Jane~X Wang, Aaditya~K
  Singh, Pierre~Harvey Richemond, James McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{pope2022efficiently}
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
  Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.
\newblock Efficiently scaling transformer inference, 2022.

\bibitem{sheng2023highthroughput}
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel~Y.
  Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph~E. Gonzalez, Percy Liang,
  Christopher Ré, Ion Stoica, and Ce~Zhang.
\newblock High-throughput generative inference of large language models with a
  single gpu, 2023.

\bibitem{yao2022zeroquant}
Zhewei Yao, Reza~Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
  Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock {\em arXiv preprint arXiv:2206.01861}, 2022.

\bibitem{park2022nuqmm}
Gunho Park, Baeseong Park, Se~Jung Kwon, Byeongwook Kim, Youngjoo Lee, and
  Dongsoo Lee.
\newblock nuqmm: Quantized matmul for efficient inference of large-scale
  generative language models.
\newblock {\em arXiv preprint arXiv:2206.09557}, 2022.

\bibitem{dettmers2022llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock {\em arXiv preprint arXiv:2208.07339}, 2022.

\bibitem{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{frantar2023massive}
Elias Frantar and Dan Alistarh.
\newblock Massive language models can be accurately pruned in one-shot.
\newblock {\em arXiv preprint arXiv:2301.00774}, 2023.

\bibitem{bansal2022rethinking}
Hritik Bansal, Karthik Gopalakrishnan, Saket Dingliwal, Sravan Bodapati, Katrin
  Kirchhoff, and Dan Roth.
\newblock Rethinking the role of scale for in-context learning: An
  interpretability-based case study at 66 billion scale.
\newblock {\em arXiv preprint arXiv:2212.09095}, 2022.

\bibitem{xiao2022smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock {\em arXiv preprint arXiv:2211.10438}, 2022.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{kitaevreformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{chen2021mongoose}
Beidi Chen, Zichang Liu, Binghui Peng, Zhaozhuo Xu, Jonathan~Lingjie Li, Tri
  Dao, Zhao Song, Anshumali Shrivastava, and Christopher Re.
\newblock Mongoose: A learnable lsh framework for efficient neural network
  training.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{chen2021scatterbrain}
Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher
  R{\'e}.
\newblock Scatterbrain: Unifying sparse and low-rank attention.
\newblock {\em Advances in Neural Information Processing Systems},
  34:17413--17426, 2021.

\bibitem{choromanskirethinking}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared~Quincy Davis,
  Afroz Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J. Colwell, and
  Adrian Weller.
\newblock Rethinking attention with performers.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness, 2022.

\bibitem{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv e-prints}, 2019.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.

\bibitem{OpenBookQA2018}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In {\em EMNLP}, 2018.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models, 2016.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, 2019.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{Bisk2020}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In {\em Thirty-Fourth AAAI Conference on Artificial Intelligence},
  2020.

\bibitem{ai2:winogrande}
Sakaguchi Keisuke, Le~Bras Ronan, Bhagavatula Chandra, and Choi Yejin.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In {\em Communications of the ACM}, 2019.

\bibitem{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou.
\newblock A framework for few-shot language model evaluation.
\newblock In {\em Version v0. 0.1. Sept}. Zenodo, September 2021.

\end{thebibliography}
