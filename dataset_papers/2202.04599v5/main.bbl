\begin{thebibliography}{10}

\bibitem{barrejon2021medical}
D.~Barrej{\'o}n, P.~M. Olmos, and A.~Art{\'e}s-Rodr{\'\i}guez.
\newblock Medical data wrangling with sequential {V}ariational {A}utoencoders.
\newblock {\em arXiv preprint arXiv:2103.07206}, 2021.

\bibitem{bengio2009learning}
Y.~Bengio.
\newblock {\em Learning deep architectures for {AI}}.
\newblock Now Publishers Inc, 2009.

\bibitem{bernardo1979expected}
J.~M. Bernardo.
\newblock Expected information as expected utility.
\newblock {\em the Annals of Statistics}, pages 686--690, 1979.

\bibitem{betancourt2017conceptual}
M.~Betancourt.
\newblock A conceptual introduction to {H}amiltonian {M}onte {C}arlo.
\newblock {\em arXiv preprint arXiv:1701.02434}, 2017.

\bibitem{betancourt2015hamiltonian}
M.~Betancourt and M.~Girolami.
\newblock Hamiltonian {M}onte {C}arlo for hierarchical models.
\newblock {\em Current trends in Bayesian methodology with applications},
  79(30):2--4, 2015.

\bibitem{campbell2021gradient}
A.~Campbell, W.~Chen, V.~Stimper, J.~M. Hernandez-Lobato, and Y.~Zhang.
\newblock A gradient {B}ased {S}trategy for {H}amiltonian {M}onte {C}arlo
  {H}yperparameter optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1238--1248. PMLR, 2021.

\bibitem{caterini2018hamiltonian}
A.~L. Caterini, A.~Doucet, and D.~Sejdinovic.
\newblock Hamiltonian {V}ariational {A}uto-{E}ncoder.
\newblock {\em arXiv preprint arXiv:1805.11328}, 2018.

\bibitem{child2020very}
R.~Child.
\newblock Very deep {VAE}s generalize autoregressive models and can outperform
  them on images.
\newblock {\em arXiv preprint arXiv:2011.10650}, 2020.

\bibitem{collier2020vaes}
M.~Collier, A.~Nazabal, and C.~K. Williams.
\newblock {VAE}s in the presence of missing data.
\newblock {\em arXiv preprint arXiv:2006.05301}, 2020.

\bibitem{cremer2018inference}
C.~Cremer, X.~Li, and D.~Duvenaud.
\newblock Inference {S}uboptimality in {V}ariational {A}utoencoders.
\newblock In {\em International Conference on Machine Learning}, pages
  1078--1086. PMLR, 2018.

\bibitem{dua2017uci}
D.~Dua, C.~Graff, et~al.
\newblock {UCI} machine learning repository.
\newblock 2017.

\bibitem{duane1987hybrid}
S.~Duane, A.~D. Kennedy, B.~J. Pendleton, and D.~Roweth.
\newblock Hybrid {M}onte {C}arlo.
\newblock {\em Physics letters B}, 195(2):216--222, 1987.

\bibitem{eduardo2020robust}
S.~Eduardo, A.~Naz{\'a}bal, C.~K. Williams, and C.~Sutton.
\newblock Robust {V}ariational {A}utoencoders for outlier detection and repair
  of mixed-type data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4056--4066. PMLR, 2020.

\bibitem{garnelo2018conditional}
M.~Garnelo, D.~Rosenbaum, C.~Maddison, T.~Ramalho, D.~Saxton, M.~Shanahan,
  Y.~W. Teh, D.~Rezende, and S.~A. Eslami.
\newblock Conditional {N}eural {P}rocesses.
\newblock In {\em International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2018.

\bibitem{ghahramani1995learning}
Z.~Ghahramani and M.~I. Jordan.
\newblock Learning from incomplete data.
\newblock 1995.

\bibitem{girolami2011riemann}
M.~Girolami and B.~Calderhead.
\newblock Riemann manifold langevin and hamiltonian monte carlo methods.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 73(2):123--214, 2011.

\bibitem{gong2020sliced}
W.~Gong, Y.~Li, and J.~M. Hern{\'a}ndez-Lobato.
\newblock Sliced {K}ernelized {S}tein {D}iscrepancy.
\newblock {\em arXiv preprint arXiv:2006.16531}, 2020.

\bibitem{gong2021variational}
Y.~Gong, H.~Hajimirsadeghi, J.~He, T.~Durand, and G.~Mori.
\newblock Variational {S}elective {A}utoencoder: Learning from
  partially-observed heterogeneous data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2377--2385. PMLR, 2021.

\bibitem{hoffman2017learning}
M.~D. Hoffman.
\newblock Learning deep latent {G}aussian models with {M}arkov {C}hain {M}onte
  {C}arlo.
\newblock In {\em International conference on machine learning}, pages
  1510--1519. PMLR, 2017.

\bibitem{huang2018active}
S.-J. Huang, M.~Xu, M.-K. Xie, M.~Sugiyama, G.~Niu, and S.~Chen.
\newblock Active feature acquisition with supervised matrix completion.
\newblock In {\em Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1571--1579, 2018.

\bibitem{ipsen2020deal}
N.~Ipsen, P.-A. Mattei, and J.~Frellsen.
\newblock How to deal with missing data in supervised deep learning?
\newblock In {\em Artemiss-ICML Workshop on the Art of Learning with Missing
  Values}, 2020.

\bibitem{joy2021capturing}
T.~Joy, S.~Schmon, P.~Torr, S.~Narayanaswamy, and T.~Rainforth.
\newblock Capturing label characteristics in vaes.
\newblock In {\em Proceedings of the ICLR Conference 2021}. OpenReview, 2021.

\bibitem{kingma2016improved}
D.~P. Kingma, T.~Salimans, R.~Jozefowicz, X.~Chen, I.~Sutskever, and
  M.~Welling.
\newblock Improved {V}ariational {I}nference with {I}nverse {A}utoregressive
  {F}low.
\newblock {\em Advances in neural information processing systems},
  29:4743--4751, 2016.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{kingma2019introduction}
D.~P. Kingma and M.~Welling.
\newblock An introduction to {V}ariational {A}utoencoders.
\newblock {\em arXiv preprint arXiv:1906.02691}, 2019.

\bibitem{kraskov2004estimating}
A.~Kraskov, H.~St{\"o}gbauer, and P.~Grassberger.
\newblock Estimating mutual information.
\newblock {\em Physical review E}, 69(6):066138, 2004.

\bibitem{lecun1998mnist}
Y.~LeCun.
\newblock The {MNIST} database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{li2019generative}
Y.~Li, J.~Bradshaw, and Y.~Sharma.
\newblock Are generative classifiers more robust to adversarial attacks?
\newblock In {\em International Conference on Machine Learning}, pages
  3804--3814. PMLR, 2019.

\bibitem{lindley1956measure}
D.~V. Lindley.
\newblock On a measure of the information provided by an experiment.
\newblock {\em The Annals of Mathematical Statistics}, pages 986--1005, 1956.

\bibitem{little2019statistical}
R.~J. Little and D.~B. Rubin.
\newblock {\em Statistical analysis with missing data}, volume 793.
\newblock John Wiley \& Sons, 2019.

\bibitem{liu2016kernelized}
Q.~Liu, J.~Lee, and M.~Jordan.
\newblock A {K}ernelized {S}tein {D}iscrepancy for goodness-of-fit tests.
\newblock In {\em International conference on machine learning}, pages
  276--284. PMLR, 2016.

\bibitem{liu2015faceattributes}
Z.~Liu, P.~Luo, X.~Wang, and X.~Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{ma2020vaem}
C.~Ma, S.~Tschiatschek, J.~M. Hern{\'a}ndez-Lobato, R.~Turner, and C.~Zhang.
\newblock {VAEM}: a {D}eep {G}enerative {M}odel for {H}eterogeneous {M}ixed
  {T}ype {D}ata.
\newblock {\em arXiv preprint arXiv:2006.11941}, 2020.

\bibitem{ma2018eddi}
C.~Ma, S.~Tschiatschek, K.~Palla, J.~M. Hern{\'a}ndez-Lobato, S.~Nowozin, and
  C.~Zhang.
\newblock Ed{DI}: {E}fficient {D}ynamic {D}iscovery of {H}igh-{V}alue
  {I}nformation with {P}artial {VAE}.
\newblock {\em arXiv preprint arXiv:1809.11142}, 2018.

\bibitem{maaloe2019biva}
L.~Maal{\o}e, M.~Fraccaro, V.~Li{\'e}vin, and O.~Winther.
\newblock {BIVA}: A very deep hierarchy of latent variables for generative
  modeling.
\newblock {\em arXiv preprint arXiv:1902.02102}, 2019.

\bibitem{mattei2019miwae}
P.-A. Mattei and J.~Frellsen.
\newblock {MIWAE}: Deep generative modelling and imputation of incomplete data
  sets.
\newblock In {\em International Conference on Machine Learning}, pages
  4413--4423. PMLR, 2019.

\bibitem{melville2004active}
P.~Melville, M.~Saar-Tsechansky, F.~Provost, and R.~Mooney.
\newblock Active feature-value acquisition for classifier induction.
\newblock In {\em Fourth IEEE International Conference on Data Mining
  (ICDM'04)}, pages 483--486. IEEE, 2004.

\bibitem{nazabal2020handling}
A.~Nazabal, P.~M. Olmos, Z.~Ghahramani, and I.~Valera.
\newblock Handling {I}ncomplete {H}eterogeneous {D}ata using {VAE}s.
\newblock {\em Pattern Recognition}, 107:107501, 2020.

\bibitem{neal2011mcmc}
R.~M. Neal et~al.
\newblock {MCMC} using {H}amiltonian dynamics.
\newblock {\em Handbook of markov chain monte carlo}, 2(11):2, 2011.

\bibitem{razavi2019preventing}
A.~Razavi, A.~v.~d. Oord, B.~Poole, and O.~Vinyals.
\newblock Preventing posterior collapse with delta-{VAE}s.
\newblock {\em arXiv preprint arXiv:1901.03416}, 2019.

\bibitem{rezende2014stochastic}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em International conference on machine learning}, pages
  1278--1286. PMLR, 2014.

\bibitem{ross2014mutual}
B.~C. Ross.
\newblock Mutual information between discrete and continuous data sets.
\newblock {\em PloS one}, 9(2):e87357, 2014.

\bibitem{saar2009active}
M.~Saar-Tsechansky, P.~Melville, and F.~Provost.
\newblock Active feature-value acquisition.
\newblock {\em Management Science}, 55(4):664--684, 2009.

\bibitem{salakhutdinov2015learning}
R.~Salakhutdinov.
\newblock Learning deep generative models.
\newblock {\em Annual Review of Statistics and Its Application}, 2:361--385,
  2015.

\bibitem{salakhutdinov2009deep}
R.~Salakhutdinov and G.~Hinton.
\newblock Deep {B}oltzmann machines.
\newblock In {\em Artificial intelligence and statistics}, pages 448--455.
  PMLR, 2009.

\bibitem{salimans2015markov}
T.~Salimans, D.~Kingma, and M.~Welling.
\newblock Markov {C}hain {M}onte {C}arlo and {V}ariational {I}nference:
  Bridging the gap.
\newblock In {\em International Conference on Machine Learning}, pages
  1218--1226. PMLR, 2015.

\bibitem{smieja2018processing}
M.~{\'S}mieja, {\L}.~Struski, J.~Tabor, B.~Zieli{\'n}ski, and P.~Spurek.
\newblock Processing of missing data by neural networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{sonderby2016ladder}
C.~K. S{\o}nderby, T.~Raiko, L.~Maal{\o}e, S.~K. S{\o}nderby, and O.~Winther.
\newblock Ladder {V}ariational {A}utoencoders.
\newblock {\em Advances in neural information processing systems},
  29:3738--3746, 2016.

\bibitem{stekhoven2012missforest}
D.~J. Stekhoven and P.~B{\"u}hlmann.
\newblock Missforest—non-parametric missing value imputation for mixed-type
  data.
\newblock {\em Bioinformatics}, 28(1):112--118, 2012.

\bibitem{thahir2012efficient}
M.~Thahir, T.~Sharma, and M.~K. Ganapathiraju.
\newblock An efficient heuristic method for active feature acquisition and its
  application to protein-protein interaction prediction.
\newblock In {\em BMC proceedings}, volume~6, pages 1--9. BioMed Central, 2012.

\bibitem{thin2021monte}
A.~Thin, N.~Kotelevskii, A.~Doucet, A.~Durmus, E.~Moulines, and M.~Panov.
\newblock Monte carlo variational auto-encoders.
\newblock In {\em International Conference on Machine Learning}, pages
  10247--10257. PMLR, 2021.

\bibitem{tresp1993training}
V.~Tresp, S.~Ahmad, and R.~Neuneier.
\newblock Training neural networks with deficient data.
\newblock {\em Advances in neural information processing systems}, 6, 1993.

\bibitem{vahdat2020nvae}
A.~Vahdat and J.~Kautz.
\newblock {NVAE}: A {D}eep {H}ierarchical {V}ariational {A}utoencoder.
\newblock {\em arXiv preprint arXiv:2007.03898}, 2020.

\bibitem{wang2020posterior}
Y.~Wang and J.~P. Cunningham.
\newblock Posterior collapse and latent variable non-identifiability.
\newblock In {\em Third Symposium on Advances in Approximate Bayesian
  Inference}, 2020.

\bibitem{xiao2017fashion}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{zhang2018advances}
C.~Zhang, J.~B{\"u}tepage, H.~Kjellstr{\"o}m, and S.~Mandt.
\newblock Advances in {V}ariational {I}nference.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  41(8):2008--2026, 2018.

\end{thebibliography}
