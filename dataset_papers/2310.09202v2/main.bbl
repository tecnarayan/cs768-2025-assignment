\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmed et~al.(2020)Ahmed, Bodwin, Sahneh, Hamm, Jebelli, Kobourov, and Spence]{spanner}
Ahmed, A.~R., Bodwin, G., Sahneh, F.~D., Hamm, K., Jebelli, M. J.~L., Kobourov, S.~G., and Spence, R.
\newblock Graph spanners: {A} tutorial review.
\newblock \emph{Comput. Sci. Rev.}, 37:\penalty0 100253, 2020.

\bibitem[Bo et~al.(2021)Bo, Wang, Shi, and Shen]{FAGCN}
Bo, D., Wang, X., Shi, C., and Shen, H.
\newblock Beyond low-frequency information in graph convolutional networks.
\newblock In \emph{{AAAI}}, pp.\  3950--3957. {AAAI} Press, 2021.

\bibitem[Bo et~al.(2023)Bo, Shi, Wang, and Liao]{Specformer}
Bo, D., Shi, C., Wang, L., and Liao, R.
\newblock Specformer: Spectral graph neural networks meet transformers.
\newblock In \emph{{ICLR}}, 2023.

\bibitem[Cazenavette et~al.(2022)Cazenavette, Wang, Torralba, Efros, and Zhu]{DDMTT}
Cazenavette, G., Wang, T., Torralba, A., Efros, A.~A., and Zhu, J.
\newblock Dataset distillation by matching training trajectories.
\newblock In \emph{{CVPR}}, pp.\  10708--10717. {IEEE}, 2022.

\bibitem[Chien et~al.(2021)Chien, Peng, Li, and Milenkovic]{GPR-GNN}
Chien, E., Peng, J., Li, P., and Milenkovic, O.
\newblock Adaptive universal generalized pagerank graph neural network.
\newblock In \emph{{ICLR}}. OpenReview.net, 2021.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and Vandergheynst]{ChebNet}
Defferrard, M., Bresson, X., and Vandergheynst, P.
\newblock Convolutional neural networks on graphs with fast localized spectral filtering.
\newblock In \emph{{NIPS}}, pp.\  3837--3845, 2016.

\bibitem[Gao et~al.(2023)Gao, Chen, Zang, Zhang, Nguyen, Zheng, and Yin]{MCond}
Gao, X., Chen, T., Zang, Y., Zhang, W., Nguyen, Q. V.~H., Zheng, K., and Yin, H.
\newblock Graph condensation for inductive node representation learning.
\newblock \emph{ArXiv}, abs/2307.15967, 2023.

\bibitem[Gao et~al.(2024)Gao, Yu, Jiang, Chen, Zhang, and Yin]{gd_survey}
Gao, X., Yu, J., Jiang, W., Chen, T., Zhang, W., and Yin, H.
\newblock Graph condensation: A survey.
\newblock \emph{ArXiv}, abs/2401.11720, 2024.

\bibitem[Geng et~al.(2023)Geng, Chen, Wang, Woisetschlaeger, Schimmler, Mayer, Zhao, and Rong]{dd_survey3}
Geng, J., Chen, Z., Wang, Y., Woisetschlaeger, H., Schimmler, S., Mayer, R., Zhao, Z., and Rong, C.
\newblock A survey on dataset distillation: Approaches, applications and future directions.
\newblock In \emph{{IJCAI}}, pp.\  6610--6618. ijcai.org, 2023.

\bibitem[Guo et~al.(2023)Guo, Wang, Cazenavette, Li, Zhang, and You]{guo2023towards}
Guo, Z., Wang, K., Cazenavette, G., Li, H., Zhang, K., and You, Y.
\newblock Towards lossless dataset distillation via difficulty-aligned trajectory matching.
\newblock \emph{arXiv preprint arXiv:2310.05773}, 2023.

\bibitem[Gutman \& Zhou(2006)Gutman and Zhou]{TV}
Gutman, I. and Zhou, B.
\newblock Laplacian energy of a graph.
\newblock \emph{Linear Algebra and its applications}, 414\penalty0 (1):\penalty0 29--37, 2006.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{SAGE}
Hamilton, W.~L., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{{NIPS}}, pp.\  1024--1034, 2017.

\bibitem[He et~al.(2021)He, Wei, Huang, and Xu]{BernNet}
He, M., Wei, Z., Huang, Z., and Xu, H.
\newblock Bernnet: Learning arbitrary graph spectral filters via bernstein approximation.
\newblock In \emph{NeurIPS}, pp.\  14239--14251, 2021.

\bibitem[He et~al.(2022)He, Wei, and Wen]{ChebNetII}
He, M., Wei, Z., and Wen, J.
\newblock Convolutional neural networks on graphs with chebyshev approximation, revisited.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and Leskovec]{OGB}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and Leskovec, J.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Jin et~al.(2022{\natexlab{a}})Jin, Tang, Jiang, Li, Zhang, Tang, and Yin]{DosCond}
Jin, W., Tang, X., Jiang, H., Li, Z., Zhang, D., Tang, J., and Yin, B.
\newblock Condensing graphs via one-step gradient matching.
\newblock In \emph{{KDD}}, pp.\  720--730, 2022{\natexlab{a}}.

\bibitem[Jin et~al.(2022{\natexlab{b}})Jin, Zhao, Zhang, Liu, Tang, and Shah]{GCond}
Jin, W., Zhao, L., Zhang, S., Liu, Y., Tang, J., and Shah, N.
\newblock Graph condensation for graph neural networks.
\newblock In \emph{{ICLR}}, 2022{\natexlab{b}}.

\bibitem[Jin et~al.(2020)Jin, Loukas, and J{\'{a}}J{\'{a}}]{k1+k2}
Jin, Y., Loukas, A., and J{\'{a}}J{\'{a}}, J.~F.
\newblock Graph coarsening with preserved spectral properties.
\newblock In \emph{{AISTATS}}, volume 108, pp.\  4452--4462. {PMLR}, 2020.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{GCN}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{{ICLR}}. OpenReview.net, 2017.

\bibitem[Klicpera et~al.(2019)Klicpera, Bojchevski, and G{\"{u}}nnemann]{PPNP}
Klicpera, J., Bojchevski, A., and G{\"{u}}nnemann, S.
\newblock Predict then propagate: Graph neural networks meet personalized pagerank.
\newblock In \emph{{ICLR}}. OpenReview.net, 2019.

\bibitem[Kumar et~al.(2023)Kumar, Sharma, Saxena, and Kumar]{feature_coarsening}
Kumar, M., Sharma, A., Saxena, S., and Kumar, S.
\newblock Featured graph coarsening with similarity guarantees.
\newblock In \emph{{ICML}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  17953--17975. {PMLR}, 2023.

\bibitem[Lei \& Tao(2023)Lei and Tao]{dd_survey2}
Lei, S. and Tao, D.
\newblock A comprehensive survey of dataset distillation.
\newblock \emph{ArXiv}, abs/2301.05603, 2023.

\bibitem[Lim et~al.(2021)Lim, Hohne, Li, Huang, Gupta, Bhalerao, and Lim]{gamers}
Lim, D., Hohne, F., Li, X., Huang, S.~L., Gupta, V., Bhalerao, O., and Lim, S.
\newblock Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.
\newblock In \emph{NeurIPS}, pp.\  20887--20902, 2021.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Li, Chen, and Song]{GCDM}
Liu, M., Li, S., Chen, X., and Song, L.
\newblock Graph condensation via receptive field distribution matching.
\newblock \emph{ArXiv}, abs/2206.13697, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Wang, Yang, Ye, and Wang]{HaBa}
Liu, S., Wang, K., Yang, X., Ye, J., and Wang, X.
\newblock Dataset distillation via factorization.
\newblock In \emph{NeurIPS}, 2022{\natexlab{b}}.

\bibitem[Loukas(2019)]{coarsening1}
Loukas, A.
\newblock Graph reduction with spectral and cut guarantees.
\newblock \emph{J. Mach. Learn. Res.}, 20:\penalty0 116:1--116:42, 2019.

\bibitem[Martinkus et~al.(2022)Martinkus, Loukas, Perraudin, and Wattenhofer]{Spectre}
Martinkus, K., Loukas, A., Perraudin, N., and Wattenhofer, R.
\newblock {SPECTRE:} spectral conditioning helps to overcome the expressivity limits of one-shot graph generators.
\newblock In \emph{{ICML}}, volume 162, pp.\  15159--15179. {PMLR}, 2022.

\bibitem[Quan et~al.(2023)Quan, Ding, Gao, Yi, Jin, and Li]{app3}
Quan, Y., Ding, J., Gao, C., Yi, L., Jin, D., and Li, Y.
\newblock Robust preference-guided denoising for graph based social recommendation.
\newblock In \emph{{WWW}}, pp.\  1097--1108. {ACM}, 2023.

\bibitem[Rozemberczki et~al.(2021)Rozemberczki, Allen, and Sarkar]{squirrel}
Rozemberczki, B., Allen, C., and Sarkar, R.
\newblock Multi-scale attributed node embedding.
\newblock \emph{J. Complex Networks}, 9\penalty0 (2), 2021.

\bibitem[Sachdeva \& McAuley(2023)Sachdeva and McAuley]{dd_survey1}
Sachdeva, N. and McAuley, J.
\newblock Data distillation: A survey.
\newblock \emph{ArXiv}, abs/2301.04272, 2023.

\bibitem[Sener \& Savarese(2018)Sener and Savarese]{core_set2}
Sener, O. and Savarese, S.
\newblock Active learning for convolutional neural networks: {A} core-set approach.
\newblock In \emph{{ICLR}}. OpenReview.net, 2018.

\bibitem[Spielman \& Srivastava(2011)Spielman and Srivastava]{RS}
Spielman, D.~A. and Srivastava, N.
\newblock Graph sparsification by effective resistances.
\newblock \emph{{SIAM} J. Comput.}, 40\penalty0 (6):\penalty0 1913--1926, 2011.

\bibitem[Velickovic et~al.(2018)Velickovic, Cucurull, Casanova, Romero, Li{\`{o}}, and Bengio]{GAT}
Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Li{\`{o}}, P., and Bengio, Y.
\newblock Graph attention networks.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Wang et~al.(2022)Wang, Zhao, Peng, Zhu, Yang, Wang, Huang, Bilen, Wang, and You]{CAFE}
Wang, K., Zhao, B., Peng, X., Zhu, Z., Yang, S., Wang, S., Huang, G., Bilen, H., Wang, X., and You, Y.
\newblock {CAFE:} learning to condense dataset by aligning features.
\newblock In \emph{{CVPR}}, pp.\  12186--12195. {IEEE}, 2022.

\bibitem[Welling(2009)]{core_set1}
Welling, M.
\newblock Herding dynamical weights to learn.
\newblock In \emph{{ICML}}, volume 382, pp.\  1121--1128. {ACM}, 2009.

\bibitem[Wu et~al.(2019)Wu, Jr., Zhang, Fifty, Yu, and Weinberger]{SGC}
Wu, F., Jr., A. H.~S., Zhang, T., Fifty, C., Yu, T., and Weinberger, K.~Q.
\newblock Simplifying graph convolutional networks.
\newblock In \emph{{ICML}}, volume~97, pp.\  6861--6871. {PMLR}, 2019.

\bibitem[Xu et~al.(2023)Xu, Chen, Pan, Chen, Das, Yang, and Tong]{KIDD}
Xu, Z., Chen, Y., Pan, M., Chen, H., Das, M., Yang, H., and Tong, H.
\newblock Kernel ridge regression-based graph dataset distillation.
\newblock In \emph{{KDD}}, pp.\  2850--2861, 2023.

\bibitem[Yang et~al.(2023)Yang, Wang, Sun, Ji, Fu, Tang, You, and Li]{SGDD}
Yang, B., Wang, K., Sun, Q., Ji, C., Fu, X., Tang, H., You, Y., and Li, J.
\newblock Does graph distillation see like vision dataset counterpart?
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Yang et~al.(2017)Yang, Sun, Zhao, Liu, and Chang]{yang2017neural}
Yang, C., Sun, M., Zhao, W.~X., Liu, Z., and Chang, E.~Y.
\newblock A neural network approach to jointly modeling social networks and mobile trajectories.
\newblock \emph{ACM Transactions on Information Systems (TOIS)}, 35\penalty0 (4):\penalty0 1--28, 2017.

\bibitem[Yu et~al.(2023)Yu, Liu, and Wang]{dd_survey4}
Yu, R., Liu, S., and Wang, X.
\newblock Dataset distillation: A comprehensive review.
\newblock \emph{ArXiv}, abs/2301.07014, 2023.

\bibitem[Yu et~al.(2022)Yu, Alesiani, Yin, Jenssen, and Pr{\'{\i}}ncipe]{sparsification2}
Yu, S., Alesiani, F., Yin, W., Jenssen, R., and Pr{\'{\i}}ncipe, J.~C.
\newblock Principle of relevant information for graph sparsification.
\newblock In \emph{{UAI}}, volume 180 of \emph{Proceedings of Machine Learning Research}, pp.\  2331--2341. {PMLR}, 2022.

\bibitem[Zeng et~al.(2020)Zeng, Zhou, Srivastava, Kannan, and Prasanna]{Flickr}
Zeng, H., Zhou, H., Srivastava, A., Kannan, R., and Prasanna, V.~K.
\newblock Graphsaint: Graph sampling based inductive learning method.
\newblock In \emph{{ICLR}}. OpenReview.net, 2020.

\bibitem[Zhao \& Bilen(2023)Zhao and Bilen]{DCDM}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with distribution matching.
\newblock In \emph{{WACV}}, pp.\  6503--6512. {IEEE}, 2023.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{DCGM}
Zhao, B., Mopuri, K.~R., and Bilen, H.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{{ICLR}}. OpenReview.net, 2021.

\bibitem[Zhao et~al.(2023)Zhao, Li, Qin, and Yu]{IDM}
Zhao, G., Li, G., Qin, Y., and Yu, Y.
\newblock Improved distribution matching for dataset condensation.
\newblock In \emph{{CVPR}}, pp.\  7856--7865. {IEEE}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Zhang, Chen, Nguyen, Zhu, and Pan]{SFGC}
Zheng, X., Zhang, M., Chen, C., Nguyen, Q. V.~H., Zhu, X., and Pan, S.
\newblock Structure-free graph condensation: From large-scale graphs to condensed graph-free data.
\newblock \emph{ArXiv}, abs/2306.02664, 2023.

\end{thebibliography}
