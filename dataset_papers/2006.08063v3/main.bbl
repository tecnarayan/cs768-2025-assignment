\begin{thebibliography}{10}

\bibitem{abadi2016tensorflow}
Mart{\'\i}n {Abadi}, Ashish {Agarwal}, Paul {Barham}, Eugene {Brevdo}, Zhifeng
  {Chen}, Craig {Citro}, Greg~S. {Corrado}, Andy {Davis}, Jeffrey {Dean},
  Matthieu {Devin}, Sanjay {Ghemawat}, Ian {Goodfellow}, Andrew {Harp},
  Geoffrey {Irving}, Michael {Isard}, Yangqing {Jia}, Rafal {Jozefowicz},
  Lukasz {Kaiser}, Manjunath {Kudlur}, Josh {Levenberg}, Dan {Mane}, Rajat
  {Monga}, Sherry {Moore}, Derek {Murray}, Chris {Olah}, Mike {Schuster},
  Jonathon {Shlens}, Benoit {Steiner}, Ilya {Sutskever}, Kunal {Talwar}, Paul
  {Tucker}, Vincent {Vanhoucke}, Vijay {Vasudevan}, Fernanda {Viegas}, Oriol
  {Vinyals}, Pete {Warden}, Martin {Wattenberg}, Martin {Wicke}, Yuan {Yu}, and
  Xiaoqiang {Zheng}.
\newblock {TensorFlow: Large-Scale Machine Learning on Heterogeneous
  Distributed Systems}.
\newblock {\em arXiv e-prints}, page arXiv:1603.04467, March 2016.

\bibitem{adams2011ranking}
Ryan~Prescott Adams and Richard~S Zemel.
\newblock Ranking via sinkhorn propagation.
\newblock {\em arXiv preprint arXiv:1106.1925}, 2011.

\bibitem{cvxpylayers2019}
A.~Agrawal, B.~Amos, S.~Barratt, S.~Boyd, S.~Diamond, and Z.~Kolter.
\newblock Differentiable convex optimization layers.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{agrawal2019differentiating}
Akshay Agrawal, Shane Barratt, Stephen Boyd, Enzo Busseti, and Walaa~M Moursi.
\newblock Differentiating through a conic program.
\newblock {\em arXiv preprint arXiv:1904.09043}, 2019.

\bibitem{amos2019differentiable}
Brandon Amos.
\newblock {\em Differentiable optimization-based modeling for machine
  learning}.
\newblock PhD thesis, PhD thesis. Carnegie Mellon University, 2019.

\bibitem{amos2017optnet}
Brandon Amos and J~Zico Kolter.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 136--145. JMLR. org, 2017.

\bibitem{amos2019limited}
Brandon {Amos}, Vladlen {Koltun}, and J.~{Zico Kolter}.
\newblock {The Limited Multi-Label Projection Layer}.
\newblock {\em arXiv e-prints}, page arXiv:1906.08707, June 2019.

\bibitem{asmussen2007stochastic}
S{\o}ren Asmussen and Peter~W Glynn.
\newblock {\em Stochastic simulation: algorithms and analysis}, volume~57.
\newblock Springer Science \& Business Media, 2007.

\bibitem{aueb2015local}
Michalis Titsias~RC AUEB and Miguel L{\'a}zaro-Gredilla.
\newblock Local expectation gradients for black box variational inference.
\newblock In {\em Advances in neural information processing systems}, pages
  2638--2646, 2015.

\bibitem{beck2017fom}
Amir Beck.
\newblock {\em First-Order Methods in Optimization}.
\newblock SIAM, 2017.

\bibitem{2020arXiv200208676B}
Quentin {Berthet}, Mathieu {Blondel}, Olivier {Teboul}, Marco {Cuturi},
  Jean-Philippe {Vert}, and Francis {Bach}.
\newblock {Learning with Differentiable Perturbed Optimizers}.
\newblock {\em arXiv e-prints}, page arXiv:2002.08676, February 2020.

\bibitem{bertsimas1997introduction}
Dimitris Bertsimas and John~N Tsitsiklis.
\newblock {\em Introduction to linear optimization}, volume~6.
\newblock Athena Scientific Belmont, MA, 1997.

\bibitem{blondel2019structured}
Mathieu Blondel.
\newblock Structured prediction with projection oracles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12145--12156, 2019.

\bibitem{blondel2020learning}
Mathieu Blondel, Andr{\'e}~FT Martins, and Vlad Niculae.
\newblock Learning with fenchel-young losses.
\newblock {\em Journal of Machine Learning Research}, 21(35):1--69, 2020.

\bibitem{blondel2020fast}
Mathieu Blondel, Olivier Teboul, Quentin Berthet, and Josip Djolonga.
\newblock Fast differentiable sorting and ranking.
\newblock {\em arXiv preprint arXiv:2002.08871}, 2020.

\bibitem{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, and Skye Wanderman-Milne.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{chen2018learning}
Jianbo Chen, Le~Song, Martin Wainwright, and Michael Jordan.
\newblock Learning to explain: An information-theoretic perspective on model
  interpretation.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{chu1965shortest}
Y.J. Chu and T.~H. Liu.
\newblock On the shortest arborescence of a directed graph.
\newblock {\em Scientia Sinica}, 14:1396--1400, 1965.

\bibitem{corro2018differentiable}
Caio Corro and Ivan Titov.
\newblock Differentiable perturb-and-parse: Semi-supervised parsing with a
  structured variational autoencoder.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{djolonga2017differentiable}
Josip Djolonga and Andreas Krause.
\newblock Differentiable learning of submodular models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1013--1023, 2017.

\bibitem{domke2010impdiff}
Justin Domke.
\newblock Implicit differentiation by perturbation.
\newblock In J.~D. Lafferty, C.~K.~I. Williams, J.~Shawe-Taylor, R.~S. Zemel,
  and A.~Culotta, editors, {\em Advances in Neural Information Processing
  Systems 23}, pages 523--531. Curran Associates, Inc., 2010.

\bibitem{domke2013learning}
Justin Domke.
\newblock Learning graphical model parameters with approximate marginal
  inference.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  35(10):2454--2467, 2013.

\bibitem{duchi2008efficient}
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.
\newblock Efficient projections onto the l 1-ball for learning in high
  dimensions.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 272--279, 2008.

\bibitem{edmonds1967optimum}
Jack Edmonds.
\newblock Optimum branchings''.
\newblock {\em Journal of Research of the National Bureau of Standards:
  Mathematics and mathematical physics. B}, 71:233, 1967.

\bibitem{fruchterman1991graph}
Thomas~MJ Fruchterman and Edward~M Reingold.
\newblock Graph drawing by force-directed placement.
\newblock {\em Software: Practice and experience}, 21(11):1129--1164, 1991.

\bibitem{gal2016uncertainty}
Yarin Gal.
\newblock Uncertainty in deep learning.
\newblock {\em University of Cambridge}, 1:3, 2016.

\bibitem{gane2014learning}
Andreea Gane, Tamir Hazan, and Tommi Jaakkola.
\newblock Learning with maximum a-posteriori perturbation models.
\newblock In {\em Artificial Intelligence and Statistics}, pages 247--256,
  2014.

\bibitem{glynn1990likelihood}
Peter~W Glynn.
\newblock Likelihood ratio gradient estimation for stochastic systems.
\newblock {\em Communications of the ACM}, 33(10):75--84, 1990.

\bibitem{grathwohl2018backpropagation}
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock {\em arXiv preprint arXiv:1410.5401}, 2014.

\bibitem{grover2018stochastic}
Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon.
\newblock Stochastic optimization of sorting networks via continuous
  relaxations.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{DBLP:journals/corr/GuLSM15}
Shixiang Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih.
\newblock Muprop: Unbiased backpropagation for stochastic neural networks.
\newblock In {\em ICLR}, 2016.

\bibitem{hazan2012partition}
Tamir Hazan and Tommi Jaakkola.
\newblock On the partition function and random maximum a-posteriori
  perturbations.
\newblock In {\em International Conference on Machine Learning}, 2012.

\bibitem{hazan2013perturb}
Tamir Hazan, Subhransu Maji, and Tommi Jaakkola.
\newblock {On Sampling from the Gibbs Distribution with Random Maximum
  A-Posteriori Perturbations}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2013.

\bibitem{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em International Conference on Learning Representations}, 2015.

\bibitem{kingma2014auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In {\em International Conference on Learning Representations}, 2014.

\bibitem{kipf2018neural}
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel.
\newblock Neural relational inference for interacting systems.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{kleinberg2006algorithmdesign}
Jon Kleinberg and \'{E}va Tardos.
\newblock {\em {Algorithm Design}}.
\newblock Pearson Education, 2006.

\bibitem{koller2009probabilistic}
Daphne Koller and Nir Friedman.
\newblock {\em Probabilistic graphical models: principles and techniques}.
\newblock 2009.

\bibitem{kolmogorov2006convergent}
Vladimir Kolmogorov.
\newblock Convergent tree-reweighted message passing for energy minimization.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  28(10):1568--1583, 2006.

\bibitem{koo2007matrixtree}
Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins.
\newblock Structured prediction models via the matrix-tree theorem.
\newblock In {\em Proceedings of the 2007 Joint Conference on Empirical Methods
  in Natural Language Processing and Computational Natural Language Learning
  ({EMNLP}-{C}o{NLL})}, pages 141--150, Prague, Czech Republic, June 2007.
  Association for Computational Linguistics.

\bibitem{kool2019buy}
Wouter Kool, Herke van Hoof, and Max Welling.
\newblock Buy 4 reinforce samples, get a baseline for free!
\newblock 2019.

\bibitem{kool2020gumbeltopk}
Wouter Kool, Herke van Hoof, and Max Welling.
\newblock Ancestral gumbel-top-k sampling for sampling without replacement.
\newblock {\em Journal of Machine Learning Research}, 21(47):1--36, 2020.

\bibitem{Kool2020Estimating}
Wouter Kool, Herke van Hoof, and Max Welling.
\newblock Estimating gradients for discrete random variables by sampling
  without replacement.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{kruskal1956shortest}
Joseph~B Kruskal.
\newblock On the shortest spanning subtree of a graph and the traveling
  salesman problem.
\newblock {\em Proceedings of the American Mathematical society}, 7(1):48--50,
  1956.

\bibitem{kuhn1955hungarian}
Harold~W Kuhn.
\newblock The hungarian method for the assignment problem.
\newblock {\em Naval research logistics quarterly}, 2(1-2):83--97, 1955.

\bibitem{lee2018reparameterization}
Wonyeol Lee, Hangyeol Yu, and Hongseok Yang.
\newblock Reparameterization gradient for non-differentiable models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5553--5563, 2018.

\bibitem{lei2016rationalizing}
Tao Lei, Regina Barzilay, and Tommi Jaakkola.
\newblock Rationalizing neural predictions.
\newblock {\em arXiv preprint arXiv:1606.04155}, 2016.

\bibitem{liu2009efficient}
Jun Liu and Jieping Ye.
\newblock Efficient euclidean projections in linear time.
\newblock In {\em Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 657--664, 2009.

\bibitem{lorberbom2019direct}
Guy Lorberbom, Andreea Gane, Tommi Jaakkola, and Tamir Hazan.
\newblock Direct optimization through argmax for discrete variational
  auto-encoder.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6200--6211, 2019.

\bibitem{luce1959individual}
R~Duncan Luce.
\newblock {\em {Individual Choice Behavior: A Theoretical Analysis}}.
\newblock New York: Wiley, 1959.

\bibitem{maddison2016concrete}
Chris~J Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{maddison2014astarsamp}
Chris~J Maddison, Daniel Tarlow, and Tom Minka.
\newblock {A$^\ast$ Sampling}.
\newblock In {\em Advances in Neural Information Processing Systems}, 2014.

\bibitem{martins2016softmax}
Andre Martins and Ramon Astudillo.
\newblock From softmax to sparsemax: A sparse model of attention and
  multi-label classification.
\newblock In {\em International Conference on Machine Learning}, pages
  1614--1623, 2016.

\bibitem{martins2017learning}
Andr{\'e}~FT Martins and Julia Kreutzer.
\newblock Learning what's easy: Fully differentiable neural easy-first taggers.
\newblock In {\em Proceedings of the 2017 conference on empirical methods in
  natural language processing}, pages 349--362, 2017.

\bibitem{mcauley2012learning}
Julian McAuley, Jure Leskovec, and Dan Jurafsky.
\newblock Learning attitudes and attributes from multi-aspect reviews.
\newblock In {\em 2012 IEEE 12th International Conference on Data Mining},
  pages 1020--1025. IEEE, 2012.

\bibitem{mena2018learning}
Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek.
\newblock Learning latent permutations with gumbel-sinkhorn networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{mezuman2013tighter}
Elad Mezuman, Daniel Tarlow, Amir Globerson, and Yair Weiss.
\newblock Tighter linear program relaxations for high order graphical models.
\newblock In {\em Proceedings of the Twenty-Ninth Conference on Uncertainty in
  Artificial Intelligence}, pages 421--430, 2013.

\bibitem{mnih2014neural}
Andriy Mnih and Karol Gregor.
\newblock Neural variational inference and learning in belief networks.
\newblock In {\em International Conference on Machine Learning}, 2014.

\bibitem{mohamed2019gradientest}
Shakir {Mohamed}, Mihaela {Rosca}, Michael {Figurnov}, and Andriy {Mnih}.
\newblock {Monte Carlo Gradient Estimation in Machine Learning}.
\newblock {\em arXiv e-prints}, page arXiv:1906.10652, June 2019.

\bibitem{nangia2018listops}
Nikita Nangia and Samuel~R Bowman.
\newblock Listops: A diagnostic dataset for latent tree learning.
\newblock {\em arXiv preprint arXiv:1804.06028}, 2018.

\bibitem{niculae2018sparsemap}
Vlad Niculae, Andr{\'e}~FT Martins, Mathieu Blondel, and Claire Cardie.
\newblock Sparsemap: Differentiable sparse structured inference.
\newblock {\em arXiv preprint arXiv:1802.04223}, 2018.

\bibitem{papandreou2011perturb}
G.~Papandreou and A.~Yuille.
\newblock {Perturb-and-MAP Random Fields: Using Discrete Optimization to Learn
  and Sample from Energy Models}.
\newblock In {\em International Conference on Computer Vision}, 2011.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{plackett1975analysis}
Robin~L Plackett.
\newblock The analysis of permutations.
\newblock {\em Journal of the Royal Statistical Society: Series C (Applied
  Statistics)}, 24(2):193--202, 1975.

\bibitem{poon2011sum}
Hoifung Poon and Pedro Domingos.
\newblock Sum-product networks: A new deep architecture.
\newblock In {\em 2011 IEEE International Conference on Computer Vision
  Workshops (ICCV Workshops)}, pages 689--690. IEEE, 2011.

\bibitem{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em International Conference on Machine Learning}, 2014.

\bibitem{rockafellar1970convex}
R.~Tyrrell Rockafellar.
\newblock {\em Convex Analysis}.
\newblock Princeton University Press, 1970.

\bibitem{rockafellar1999second}
R~Tyrrell Rockafellar.
\newblock Second-order convex analysis.
\newblock {\em J. Nonlinear Convex Anal}, 1(1-16):84, 1999.

\bibitem{ross-cvpr-11}
Stephane Ross, Daniel Munoz, Martial Hebert, and J.~Andrew Bagnell.
\newblock Learning message-passing inference machines for structured
  prediction.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2011.

\bibitem{ruiz2016generalized}
Francisco~JR Ruiz, Michalis~K Titsias, and David~M Blei.
\newblock The generalized reparameterization gradient.
\newblock In {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem{rush2020torch}
Alexander~M Rush.
\newblock Torch-struct: Deep structured prediction library.
\newblock {\em arXiv preprint arXiv:2002.00876}, 2020.

\bibitem{schrijver2003combinatorial}
Alexander Schrijver.
\newblock {\em Combinatorial optimization: polyhedra and efficiency},
  volume~24.
\newblock Springer Science \& Business Media, 2003.

\bibitem{sinkhorn1967concerning}
Richard Sinkhorn and Paul Knopp.
\newblock Concerning nonnegative matrices and doubly stochastic matrices.
\newblock {\em Pacific Journal of Mathematics}, 21(2):343--348, 1967.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{swersky2012cardinality}
Kevin Swersky, Ilya Sutskever, Daniel Tarlow, Richard~S Zemel, Russ~R
  Salakhutdinov, and Ryan~P Adams.
\newblock Cardinality restricted boltzmann machines.
\newblock In {\em Advances in neural information processing systems}, pages
  3293--3301, 2012.

\bibitem{tarlow2012randoms}
Daniel Tarlow, Ryan Adams, and Richard Zemel.
\newblock Randomized optimum models for structured prediction.
\newblock In Neil~D. Lawrence and Mark Girolami, editors, {\em Proceedings of
  the Fifteenth International Conference on Artificial Intelligence and
  Statistics}, volume~22 of {\em Proceedings of Machine Learning Research},
  pages 1221--1229, La Palma, Canary Islands, 21--23 Apr 2012. PMLR.

\bibitem{tarlow2012fast}
Daniel Tarlow, Kevin Swersky, Richard~S Zemel, Ryan~P Adams, and Brendan~J
  Frey.
\newblock Fast exact inference for recursive cardinality models.
\newblock In {\em 28th Conference on Uncertainty in Artificial Intelligence,
  UAI 2012}, pages 825--834, 2012.

\bibitem{thurstone1927law}
Louis~L Thurstone.
\newblock A law of comparative judgment.
\newblock {\em Psychological review}, 34(4):273, 1927.

\bibitem{tucker2017rebar}
George Tucker, Andriy Mnih, Chris~J Maddison, John Lawson, and Jascha
  Sohl-Dickstein.
\newblock Rebar: Low-variance, unbiased gradient estimates for discrete latent
  variable models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2627--2636, 2017.

\bibitem{tutte1984graphtheory}
William~T. Tutte.
\newblock {\em {G}raph {T}heory}.
\newblock Addison‚ÄêWesley, 1984.

\bibitem{wainwright2008graphical}
Martin~J Wainwright and Michael~I Jordan.
\newblock Graphical models, exponential families, and variational inference.
\newblock {\em Foundations and Trends in Machine Learning}, 1(1--2):1--305,
  2008.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem{wolfe1976finding}
Philip Wolfe.
\newblock Finding the nearest point in a polytope.
\newblock {\em Mathematical Programming}, 11(1):128--149, 1976.

\bibitem{xie2019reparameterizable}
Sang~Michael Xie and Stefano Ermon.
\newblock Reparameterizable subset sampling via continuous relaxations.
\newblock In {\em International Joint Conference on Artificial Intelligence},
  2019.

\bibitem{yin2018arm}
Mingzhang Yin and Mingyuan Zhou.
\newblock {ARM}: Augment-{REINFORCE}-merge gradient for stochastic binary
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\end{thebibliography}
