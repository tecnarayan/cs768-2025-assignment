\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017quantized}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Balles and Hennig(2018)]{balles2017dissecting}
Lukas Balles and Philipp Hennig.
\newblock Dissecting adam: The sign, magnitude and variance of stochastic
  gradients.
\newblock In \emph{Internation Conference on Machine Learning (ICML)}, 2018.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock signsgd: compressed optimisation for non-convex problems.
\newblock In \emph{Internation Conference on Machine Learning (ICML)}, 2018.

\bibitem[Bernstein et~al.(2019)Bernstein, Zhao, Azizzadenesheli, and
  Anandkumar]{bernstein2019iclr}
Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock sign{SGD} with majority vote is communication efficient and fault
  tolerant.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Bottou(2010)]{Bottou2010:sgd}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In Yves Lechevallier and Gilbert Saporta, editors, \emph{Proceedings
  of COMPSTAT'2010}, pages 177--186, Heidelberg, 2010. Physica-Verlag HD.
\newblock ISBN 978-3-7908-2604-3.

\bibitem[Carlson et~al.(2015)Carlson, Cevher, and Carin]{Carlson:2015to}
David Carlson, Volkan Cevher, and Lawrence Carin.
\newblock {Stochastic Spectral Descent for Restricted Boltzmann Machines}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 111--119, February 2015.

\bibitem[Chen and Gu(2019)]{chen2019padam}
Jinghui Chen and Quanquan Gu.
\newblock Padam: Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
Trishul~M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{OSDI}, volume~14, pages 571--582, 2014.

\bibitem[Cortes and Vapnik(1995)]{cortes1995support}
Corinna Cortes and Vladimir Vapnik.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20\penalty0 (3):\penalty0 273--297, 1995.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1223--1231, 2012.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock \emph{arXiv preprint arXiv:1703.04933}, 2017.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Goyal et~al.(2017)Goyal, Dollar, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
  Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Gugger and Howard(2018)]{Gugger2018AdamW}
Sylvain Gugger and Jeremy Howard.
\newblock Adamw and super-convergence is now the fastest way to train neural
  nets.
\newblock \url{https://www.fast.ai/2018/07/02/adam-weight-decay/}, 2018.
\newblock Accessed: 2019-01-17.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition (CVPR)}, pages 770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, pages
  630--645. Springer, 2016{\natexlab{b}}.

\bibitem[Im et~al.(2016)Im, Tao, and Branson]{im2016empirical}
Daniel~Jiwoong Im, Michael Tao, and Kristin Branson.
\newblock An empirical analysis of the optimization of deep network loss
  surfaces.
\newblock \emph{arXiv preprint arXiv:1612.04010}, 2016.

\bibitem[Kawaguchi et~al.(2017)Kawaguchi, Kaelbling, and
  Bengio]{kawaguchi2017generalization}
Kenji Kawaguchi, Leslie~Pack Kaelbling, and Yoshua Bengio.
\newblock Generalization in deep learning.
\newblock \emph{arXiv preprint arXiv:1710.05468}, 2017.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Technical Report, University of Toronto, Toronto.,
  2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1097--1105, 2012.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436, 2015.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{OSDI}, volume~14, pages 583--598, 2014.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Liu et~al.(2018)Liu, Chen, Chen, and Hong]{liu2018signsgd}
Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong.
\newblock signsgd via zeroth-order oracle.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Luo et~al.(2019)Luo, Xiong, and Liu]{luo2019adaptive}
Liangchen Luo, Yuanhao Xiong, and Yan Liu.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, and
  Chanan]{paszke2017pytorch}
Adam Paszke, Sam Gross, Soumith Chintala, and Gregory Chanan.
\newblock Pytorch, 2017.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Riedmiller and Braun(1993)]{riedmiller1993direct}
Martin Riedmiller and Heinrich Braun.
\newblock A direct adaptive method for faster backpropagation learning: The
  rprop algorithm.
\newblock In \emph{Neural Networks, 1993., IEEE International Conference on},
  pages 586--591. IEEE, 1993.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, September 1951.

\bibitem[Schmidhuber(2015)]{schmidhuber2015deep}
J{\"u}rgen Schmidhuber.
\newblock Deep learning in neural networks: An overview.
\newblock \emph{Neural networks}, 61:\penalty0 85--117, 2015.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0 (70), 2018.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Strom(2015)]{strom2015scalable}
Nikko Strom.
\newblock Scalable distributed dnn training using commodity gpu cloud
  computing.
\newblock In \emph{Sixteenth Annual Conference of the International Speech
  Communication Association}, 2015.

\bibitem[Valiant(1984)]{valiant1984theory}
Leslie~G Valiant.
\newblock A theory of the learnable.
\newblock \emph{Communications of the ACM}, 27\penalty0 (11):\penalty0
  1134--1142, 1984.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{wang2018atomo}
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris
  Papailiopoulos, and Stephen Wright.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1509--1519, 2017.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 4148--4158, 2017.

\bibitem[Wu et~al.(2018)Wu, Huang, Huang, and Zhang]{wu2018error}
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.
\newblock Error compensated quantized sgd and its applications to large-scale
  distributed optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  5321--5329, 2018.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9815--9825, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Vinyals, Munos, and Bengio]{zhang2018study}
Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio.
\newblock A study on overfitting in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1804.06893}, 2018.

\end{thebibliography}
