@inproceedings{gpt3,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
title = {Language Models Are Few-Shot Learners},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {159},
numpages = {25},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}


@article{lamda,
  title={LaMDA: Language Models for Dialog Applications},
  author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam M. Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and Yaguang Li and Hongrae Lee and Huaixiu Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Yanqi Zhou and Chung-Ching Chang and I. A. Krivokon and Willard James Rusch and Marc Pickett and Kathleen S. Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Hartz S{\o}raker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark D{\'i}az and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravindran Rajakumar and Alena Butryna and Matthew Lamm and V. O. Kuzmina and Joseph Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Huai-hsin Chi and Quoc Le},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.08239}
}

@article{parti,
  title={Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
  author={Jiahui Yu and Yuanzhong Xu and Jing Yu Koh and Thang Luong and Gunjan Baid and Zirui Wang and Vijay Vasudevan and Alexander Ku and Yinfei Yang and Burcu Karagol Ayan and Benton C. Hutchinson and Wei Han and Zarana Parekh and Xin Li and Han Zhang and Jason Baldridge and Yonghui Wu},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.10789}
}

@article{palm,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02311}
}

@article{gad,
  title={Lossless Speedup of Autoregressive Translation with Generalized Aggressive Decoding},
  author={Heming Xia and Tao Ge and Furu Wei and Zhifang Sui},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.16487}
}

@article{iad_maybe,
  title={Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding},
  author={Xin Sun and Tao Ge and Furu Wei and Houfeng Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04970}
}


@article{The_Efficiency_Misnomer,
  title={The Efficiency Misnomer},
  author={Mostafa Dehghani and Anurag Arnab and Lucas Beyer and Ashish Vaswani and Yi Tay},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.12894}
}

@article{Distilling_the_Knowledge_in_a_Neural_Network,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@article{Quantized_Neural_Networks,
  title={Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations},
  author={Itay Hubara and Matthieu Courbariaux and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.07061}
}

@inproceedings{Sparse_is_Enough_in_Scaling_Transformers,
  title={Sparse is Enough in Scaling Transformers},
  author={Sebastian Jaszczur and Aakanksha Chowdhery and Afroz Mohiuddin and Lukasz Kaiser and Wojciech Gajewski and Henryk Michalewski and Jonni Kanerva},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{Primer,
  title={Primer: Searching for Efficient Transformers for Language Modeling},
  author={David R. So and Wojciech Ma'nke and Hanxiao Liu and Zihang Dai and Noam M. Shazeer and Quoc V. Le},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.08668}
}

@inproceedings{Matching_Model_and_Instance_Complexities,
  title={The Right Tool for the Job: Matching Model and Instance Complexities},
  author={Roy Schwartz and Gabriel Stanovsky and Swabha Swayamdipta and Jesse Dodge and Noah A. Smith},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@inproceedings{Confident_Adaptive_Transformers,
  title={Consistent Accelerated Inference via Confident Adaptive Transformers},
  author={Tal Schuster and Adam Fisch and T. Jaakkola and Regina Barzilay},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

@article{Controlling_Computation_versus_Quality_Sequence_Models,
  title={Controlling Computation versus Quality for Neural Sequence Models},
  author={Ankur Bapna and N. Arivazhagan and Orhan Firat},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.07106}
}

@article{early_exits,
  title={Why should we add early exits to neural networks?},
  author={Scardapane, Simone and Scarpiniti, Michele and Baccarelli, Enzo and Uncini, Aurelio},
  journal={Cognitive Computation},
  volume={12},
  number={5},
  pages={954--966},
  year={2020},
  publisher={Springer}
}

@article{Dynamic_Neural_Networks_Survey,
  title={Dynamic Neural Networks: A Survey},
  author={Yizeng Han and Gao Huang and Shiji Song and Le Yang and Honghui Wang and Yulin Wang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={44},
  pages={7436-7456}
}

@inproceedings{Wisdom_of_Committees,
  title={Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models},
  author={Xiaofang Wang and D. Kondratyuk and Eric Christiansen and Kris M. Kitani and Yair Alon and Elad Eban},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{depth_adaptive_transformer,
  title={Depth-Adaptive Transformer},
  author={Maha Elbayad and Jiatao Gu and Edouard Grave and Michael Auli},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.10073}
}

@inproceedings{Adaptive_Attention_Span_in_Transformers,
  title={Adaptive Attention Span in Transformers},
  author={Sainbayar Sukhbaatar and Edouard Grave and Piotr Bojanowski and Armand Joulin},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@inproceedings{lm1b,
  title={One billion word benchmark for measuring progress in statistical language modeling},
  author={Ciprian Chelba and Tomas Mikolov and Mike Schuster and Qi Ge and T. Brants and Phillip Todd Koehn and Tony Robinson},
  booktitle={Interspeech},
  year={2013}
}


@article{gelu,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}

@article{sad,
  title={Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding},
  author={Xin Sun and Tao Ge and Furu Wei and Houfeng Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.04970}
}

@article{multi-query-attn,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Noam M. Shazeer},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.02150}
}

@article{t5x,
  title={Scaling Up Models and Data with t5x and seqio},
  author={Adam Roberts and Hyung Won Chung and Anselm Levskaya and Gaurav Mishra and James Bradbury and Daniel Andor and Sharan Narang and Brian Lester and Colin Gaffney and Afroz Mohiuddin and Curtis Hawthorne and Aitor Lewkowycz and Alexandru Salcianu and Marc van Zee and Jacob Austin and Sebastian Goodman and Livio Baldini Soares and Haitang Hu and Sasha Tsvyashchenko and Aakanksha Chowdhery and Jasmijn Bastings and Jannis Bulian and Xavier Garc{\'i}a and Jianmo Ni and Andrew Chen and Kathleen Kenealy and J. Clark and Stephan Lee and Daniel H Garrette and James Lee-Thorp and Colin Raffel and Noam M. Shazeer and Marvin Ritter and Maarten Bosma and Alexandre Passos and Jeremy B. Maitin-Shepard and Noah Fiedel and Mark Omernick and Brennan Saeta and Ryan Sepassi and Alexander Spiridonov and Joshua Newlan and Andrea Gesmundo},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.17189}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@ARTICLE{speculative_computation,  author={Burton, F. Warren},  journal={IEEE Transactions on Computers},   title={Speculative computation, parallelism, and functional programming},   year={1985},  volume={C-34},  number={12},  pages={1190-1193},  doi={10.1109/TC.1985.6312218}}

@book{HennessyPatterson12,
  abstract = {The computing world today is in the middle of a revolution: mobile clients and cloud computing have emerged as the dominant paradigms driving programming and hardware innovation today. The Fifth Edition of Computer Architecture focuses on this dramatic shift, exploring the ways in which software and technology in the 'cloud' are accessed by cell phones, tablets, laptops, and other mobile computing devices. Each chapter includes two real-world examples, one mobile and one datacenter, to illustrate this revolutionary change.},
  added-at = {2016-11-04T19:12:58.000+0100},
  address = {Amsterdam},
  author = {Hennessy, John L. and Patterson, David A.},
  biburl = {https://www.bibsonomy.org/bibtex/2d2d024a4ec1fd887aa36482288ca38f9/flint63},
  edition = 5,
  file = {ACM Learning Center eBook:2012/HennessyPatterson12.pdf:PDF;Amazon Search inside:http\://www.amazon.de/gp/reader/012383872X/:URL},
  groups = {public},
  interhash = {83342075ee6946a3cd5ffa87f2337a87},
  intrahash = {d2d024a4ec1fd887aa36482288ca38f9},
  isbn = {978-0-12-383872-8},
  keywords = {01624 103 book acm elsevier computer architecture mobile device cloud intro},
  publisher = {Morgan Kaufmann},
  timestamp = {2017-07-13T18:01:10.000+0200},
  title = {Computer Architecture: A Quantitative Approach},
  username = {flint63},
  year = 2012
}

@article{graves-adaptive-computation-for-rnn,
  title={Adaptive Computation Time for Recurrent Neural Networks},
  author={Alex Graves},
  journal={ArXiv},
  year={2016},
  volume={abs/1603.08983}
}


@article{Blockwise_Parallel_Decoding,
  title={Blockwise parallel decoding for deep autoregressive models},
  author={Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chen2023accelerating,
  title={Accelerating Large Language Model Decoding with Speculative Sampling},
  author={Charlie Chen and Sebastian Borgeaud and Geoffrey Irving and Jean-Baptiste Lespiau and L. Sifre and John M. Jumper},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.01318}
}