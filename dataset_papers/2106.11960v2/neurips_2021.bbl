\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Y.~Abbasi-Yadkori, D.~P{\'a}l, and C.~Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems},
  24:\penalty0 2312--2320, 2011.

\bibitem[Agarwal et~al.(2019)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2019theory}
A.~Agarwal, S.~M. Kakade, J.~D. Lee, and G.~Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{arXiv preprint arXiv:1908.00261}, 2019.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
A.~Ayoub, Z.~Jia, C.~Szepesvari, M.~Wang, and L.~Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  463--474. PMLR, 2020.

\bibitem[Bertoluzzo and Corazza(2012)]{bertoluzzo2012testing}
F.~Bertoluzzo and M.~Corazza.
\newblock Testing different reinforcement learning configurations for financial
  trading: Introduction and applications.
\newblock \emph{Procedia Economics and Finance}, 3:\penalty0 68--77, 2012.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Q.~Cai, Z.~Yang, C.~Jin, and Z.~Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1283--1294. PMLR, 2020.

\bibitem[Charles et~al.(2013)Charles, Chickering, and
  Simard]{charles2013counterfactual}
D.~Charles, M.~Chickering, and P.~Simard.
\newblock Counterfactual reasoning and learning systems: The example of
  computational advertising.
\newblock \emph{Journal of Machine Learning Research}, 14, 2013.

\bibitem[Chen et~al.(2021)Chen, Scherrer, and Bartlett]{chen2021infinite}
L.~Chen, B.~Scherrer, and P.~L. Bartlett.
\newblock Infinite-horizon offline reinforcement learning with linear function
  approximation: Curse of dimensionality and algorithm.
\newblock \emph{arXiv preprint arXiv:2103.09847}, 2021.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesv{\'a}ri, and
  Schuurmans]{dai2020coindice}
B.~Dai, O.~Nachum, Y.~Chow, L.~Li, C.~Szepesv{\'a}ri, and D.~Schuurmans.
\newblock Coindice: Off-policy confidence interval estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Du et~al.(2020)Du, Kakade, Wang, and Yang]{du2019good}
S.~S. Du, S.~M. Kakade, R.~Wang, and L.~F. Yang.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Duan et~al.(2020)Duan, Jia, and Wang]{duan2020minimax}
Y.~Duan, Z.~Jia, and M.~Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  2701--2709. PMLR, 2020.

\bibitem[Dud{\'\i}k et~al.(2011)Dud{\'\i}k, Langford, and Li]{dudik2011doubly}
M.~Dud{\'\i}k, J.~Langford, and L.~Li.
\newblock Doubly robust policy evaluation and learning.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2011.

\bibitem[Farajtabar et~al.(2018)Farajtabar, Chow, and
  Ghavamzadeh]{farajtabar2018more}
M.~Farajtabar, Y.~Chow, and M.~Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1447--1456. PMLR, 2018.

\bibitem[Freedman(1975)]{freedman1975tail}
D.~A. Freedman.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pages 100--118, 1975.

\bibitem[He et~al.(2021)He, Zhou, and Gu]{he2020logarithmic}
J.~He, D.~Zhou, and Q.~Gu.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2021.

\bibitem[Jia et~al.(2020)Jia, Yang, Szepesvari, and Wang]{jia2020model}
Z.~Jia, L.~Yang, C.~Szepesvari, and M.~Wang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{Learning for Dynamics and Control}, pages 666--686. PMLR,
  2020.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
N.~Jiang and L.~Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  652--661. PMLR, 2016.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
C.~Jin, Z.~Yang, Z.~Wang, and M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, and Wang]{jin2020pessimism}
Y.~Jin, Z.~Yang, and Z.~Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock \emph{arXiv preprint arXiv:2012.15085}, 2020{\natexlab{b}}.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
S.~Kakade and J.~Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kakade(2001)]{kakade2001natural}
S.~M. Kakade.
\newblock A natural policy gradient.
\newblock In \emph{Advances in neural information processing systems},
  volume~14, 2001.

\bibitem[Kallus and Uehara(2019)]{kallus2019efficiently}
N.~Kallus and M.~Uehara.
\newblock Efficiently breaking the curse of horizon in off-policy evaluation
  with double reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1909.05850}, 2019.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
R.~Kidambi, A.~Rajeswaran, P.~Netrapalli, and T.~Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and Levine]{kumar2019stabilizing}
A.~Kumar, J.~Fu, G.~Tucker, and S.~Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
S.~Lange, T.~Gabel, and M.~Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pages 45--73. Springer, 2012.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
S.~Levine, A.~Kumar, G.~Tucker, and J.~Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2011)Li, Chu, Langford, and Wang]{li2011unbiased}
L.~Li, W.~Chu, J.~Langford, and X.~Wang.
\newblock Unbiased offline evaluation of contextual-bandit-based news article
  recommendation algorithms.
\newblock In \emph{Proceedings of the fourth ACM international conference on
  Web search and data mining}, pages 297--306, 2011.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesv{\'a}ri]{li2015toward}
L.~Li, R.~Munos, and C.~Szepesv{\'a}ri.
\newblock Toward minimax off-policy value estimation.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 608--616.
  PMLR, 2015.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Q.~Liu, L.~Li, Z.~Tang, and D.~Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Min et~al.(2021)Min, He, Wang, and Gu]{min2021learning}
Y.~Min, J.~He, T.~Wang, and Q.~Gu.
\newblock Learning stochastic shortest path with linear function approximation.
\newblock \emph{arXiv preprint arXiv:2110.12727}, 2021.

\bibitem[Nachum et~al.(2019{\natexlab{a}})Nachum, Chow, Dai, and
  Li]{nachum2019dualdice}
O.~Nachum, Y.~Chow, B.~Dai, and L.~Li.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pages 2318--2328. Curran Associates, Inc., 2019{\natexlab{a}}.

\bibitem[Nachum et~al.(2019{\natexlab{b}})Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
O.~Nachum, B.~Dai, I.~Kostrikov, Y.~Chow, L.~Li, and D.~Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019{\natexlab{b}}.

\bibitem[Neu and Pike-Burke(2020)]{neu2020unifying}
G.~Neu and C.~Pike-Burke.
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Pirotta et~al.(2013)Pirotta, Restelli, Pecorino, and
  Calandriello]{pirotta2013safe}
M.~Pirotta, M.~Restelli, A.~Pecorino, and D.~Calandriello.
\newblock Safe policy iteration.
\newblock In \emph{International Conference on Machine Learning}, pages
  307--315. PMLR, 2013.

\bibitem[Precup(2000)]{precup2000eligibility}
D.~Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series},
  page~80, 2000.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
D.~Precup, R.~S. Sutton, and S.~Dasgupta.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  417--424, 2001.

\bibitem[Quillen et~al.(2018)Quillen, Jang, Nachum, Finn, Ibarz, and
  Levine]{quillen2018deep}
D.~Quillen, E.~Jang, O.~Nachum, C.~Finn, J.~Ibarz, and S.~Levine.
\newblock Deep reinforcement learning for vision-based robotic grasping: A
  simulated comparative evaluation of off-policy methods.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 6284--6291. IEEE, 2018.

\bibitem[Tang et~al.(2013)Tang, Rosales, Singh, and Agarwal]{tang2013automatic}
L.~Tang, R.~Rosales, A.~Singh, and D.~Agarwal.
\newblock Automatic ad format selection via contextual bandits.
\newblock In \emph{Proceedings of the 22nd ACM international conference on
  Information \& Knowledge Management}, pages 1587--1594, 2013.

\bibitem[Tang et~al.(2020)Tang, Feng, Li, Zhou, and Liu]{tang2019doubly}
Z.~Tang, Y.~Feng, L.~Li, D.~Zhou, and Q.~Liu.
\newblock Doubly robust bias reduction in infinite horizon off-policy
  estimation.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Thomas and Brunskill(2016)]{thomas2016data}
P.~Thomas and E.~Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2139--2148. PMLR, 2016.

\bibitem[Thomas et~al.(2017)Thomas, Theocharous, Ghavamzadeh, Durugkar, and
  Brunskill]{thomas2017predictive}
P.~S. Thomas, G.~Theocharous, M.~Ghavamzadeh, I.~Durugkar, and E.~Brunskill.
\newblock Predictive off-policy policy evaluation for nonstationary decision
  problems, with applications to digital marketing.
\newblock In \emph{AAAI}, pages 4740--4745, 2017.

\bibitem[Tropp(2012)]{tropp2012user}
J.~A. Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of computational mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
R.~Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Wu, Salakhutdinov, and
  Kakade]{wang2021instabilities}
R.~Wang, Y.~Wu, R.~Salakhutdinov, and S.~M. Kakade.
\newblock Instabilities of offline rl with pre-trained neural representation.
\newblock In \emph{International Conference on Machine Learning}. PMLR,
  2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Zhou, and Gu]{wang2021provably}
T.~Wang, D.~Zhou, and Q.~Gu.
\newblock Provably efficient reinforcement learning with linear function
  approximation under adaptivity constraints.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{b}}.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
T.~Xie, Y.~Ma, and Y.-X. Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Yang and Wang(2019)]{yang2019sample}
L.~Yang and M.~Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pages
  6995--7004, 2019.

\bibitem[Yin and Wang(2020)]{yin2020asymptotically}
M.~Yin and Y.-X. Wang.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3948--3958. PMLR, 2020.

\bibitem[Yin et~al.(2021{\natexlab{a}})Yin, Bai, and Wang]{yin2020near}
M.~Yin, Y.~Bai, and Y.-X. Wang.
\newblock Near-optimal provable uniform convergence in offline policy
  evaluation for reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1567--1575. PMLR, 2021{\natexlab{a}}.

\bibitem[Yin et~al.(2021{\natexlab{b}})Yin, Bai, and Wang]{yin2021near}
M.~Yin, Y.~Bai, and Y.-X. Wang.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{b}}.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
A.~Zanette, A.~Lazaric, M.~Kochenderfer, and E.~Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pages
  10978--10989. PMLR, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Dai, Li, and Schuurmans]{zhang2019gendice}
R.~Zhang, B.~Dai, L.~Li, and D.~Schuurmans.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Yang, Ji, and Du]{zhang2021variance}
Z.~Zhang, J.~Yang, X.~Ji, and S.~S. Du.
\newblock Variance-aware confidence set: Variance-dependent bound for linear
  bandits and horizon-free bound for linear mixture mdp.
\newblock \emph{arXiv preprint arXiv:2101.12745}, 2021.

\bibitem[Zhou et~al.(2021{\natexlab{a}})Zhou, Gu, and
  Szepesvari]{zhou2020nearly}
D.~Zhou, Q.~Gu, and C.~Szepesvari.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \emph{Conference on Learning Theory}. PMLR, 2021{\natexlab{a}}.

\bibitem[Zhou et~al.(2021{\natexlab{b}})Zhou, He, and Gu]{zhou2020provably}
D.~Zhou, J.~He, and Q.~Gu.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \emph{International Conference on Machine Learning}. PMLR,
  2021{\natexlab{b}}.

\end{thebibliography}
