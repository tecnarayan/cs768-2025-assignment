\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlam and Pennington(2020)]{AdlamLinearize2}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization.
\newblock In \emph{International Conference on Machine Learning}, pages 74--84. PMLR, 2020.

\bibitem[Adlam et~al.(2019)Adlam, Levinson, and Pennington]{AdlamLinearize1}
Ben Adlam, Jake Levinson, and Jeffrey Pennington.
\newblock A random matrix perspective on mixtures of nonlinearities for deep learning.
\newblock \emph{arXiv preprint arXiv:1912.00827}, 2019.

\bibitem[Beaglehole et~al.(2023)Beaglehole, Radhakrishnan, Pandit, and Belkin]{beaglehole2023mechanism}
Daniel Beaglehole, Adityanarayanan Radhakrishnan, Parthe Pandit, and Mikhail Belkin.
\newblock Mechanism of feature learning in convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:2309.00570}, 2023.

\bibitem[Beaglehole et~al.(2024)Beaglehole, Mitliagkas, and Agarwala]{BeagleholeCenteredNFA}
Daniel Beaglehole, Ioannis Mitliagkas, and Atish Agarwala.
\newblock Gradient descent induces alignment between weights and the empirical ntk for deep non-linear networks.
\newblock \emph{arXiv preprint arXiv:2402.05271}, 2024.

\bibitem[Chen et~al.(2023)Chen, Li, Liu, and Ruan]{chen2023kernel}
Yunlu Chen, Yang Li, Keli Liu, and Feng Ruan.
\newblock Kernel learning in ridge regression" automatically" yields exact low rank solution.
\newblock \emph{arXiv preprint arXiv:2310.11736}, 2023.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Youngmin Cho and Lawrence Saul.
\newblock Kernel methods for deep learning.
\newblock \emph{Advances in neural information processing systems}, 22, 2009.

\bibitem[Ciliberto et~al.(2015)Ciliberto, Mroueh, Poggio, and Rosasco]{ciliberto2015convex}
Carlo Ciliberto, Youssef Mroueh, Tomaso Poggio, and Lorenzo Rosasco.
\newblock Convex learning of multiple tasks and their structure.
\newblock In \emph{International Conference on Machine Learning}, pages 1548--1557. PMLR, 2015.

\bibitem[Fang et~al.(2021)Fang, He, Long, and Su]{fang2021exploring}
Cong Fang, Hangfeng He, Qi~Long, and Weijie~J Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training.
\newblock In \emph{Proceedings of the National Academy of Sciences (PNAS)}, volume 118, 2021.

\bibitem[Galanti et~al.(2022)Galanti, Gy{\"o}rgy, and Hutter]{galanti2022improved}
Tomer Galanti, Andr{\'a}s Gy{\"o}rgy, and Marcus Hutter.
\newblock Improved generalization bounds for transfer learning via neural collapse.
\newblock In \emph{First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML}, 2022.

\bibitem[Haas et~al.(2022)Haas, Yolland, and Rabus]{haas2022linking}
Jarrod Haas, William Yolland, and Bernhard~T Rabus.
\newblock Linking neural collapse and l2 normalization with improved out-of-distribution detection in deep neural networks.
\newblock \emph{Transactions on Machine Learning Research (TMLR)}, 2022.

\bibitem[Han et~al.(2022)Han, Papyan, and Donoho]{han2021neural}
X.~Y. Han, Vardan Papyan, and David~L Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the central path.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[He and Su(2022)]{he2022law}
Hangfeng He and Weijie~J Su.
\newblock A law of data separation in deep learning.
\newblock \emph{arXiv preprint arXiv:2210.17020}, 2022.

\bibitem[Hong and Ling(2023)]{hong2023neural}
Wanli Hong and Shuyang Ling.
\newblock Neural collapse for unconstrained feature model under cross-entropy loss with imbalanced data.
\newblock \emph{arXiv preprint arXiv:2309.09725}, 2023.

\bibitem[Hu and Lu(2022)]{YueUniversality}
Hong Hu and Yue~M Lu.
\newblock Universality laws for high-dimensional learning with random features.
\newblock \emph{IEEE Transactions on Information Theory}, 69\penalty0 (3):\penalty0 1932--1964, 2022.

\bibitem[Ji et~al.(2022)Ji, Lu, Zhang, Deng, and Su]{ji2021unconstrained}
Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie~J Su.
\newblock An unconstrained layer-peeled perspective on neural collapse.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Karoui(2010)]{karoui2010spectrum}
Noureddine~El Karoui.
\newblock The spectrum of kernel random matrices.
\newblock \emph{The Annals of Statistics}, pages 1--50, 2010.

\bibitem[Kothapalli(2023)]{kothapalli2022neural}
Vignesh Kothapalli.
\newblock Neural collapse: A review on modelling principles and generalization.
\newblock In \emph{Transactions on Machine Learning Research (TMLR)}, 2023.

\bibitem[Kunin et~al.(2022)Kunin, Yamamura, Ma, and Ganguli]{kunin2022asymmetric}
Daniel Kunin, Atsushi Yamamura, Chao Ma, and Surya Ganguli.
\newblock The asymmetric maximum margin bias of quasi-homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:2210.03820}, 2022.

\bibitem[Li et~al.(2020)Li, Luo, and Lyu]{li2020towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning.
\newblock \emph{arXiv preprint arXiv:2012.09839}, 2020.

\bibitem[Lu and Steinerberger(2022)]{lu2020neural}
Jianfeng Lu and Stefan Steinerberger.
\newblock Neural collapse under cross-entropy loss.
\newblock In \emph{Applied and Computational Harmonic Analysis}, volume~59, 2022.

\bibitem[Micchelli and Pontil(2004)]{micchelli2004kernels}
Charles Micchelli and Massimiliano Pontil.
\newblock Kernels for multi--task learning.
\newblock \emph{Advances in neural information processing systems}, 17, 2004.

\bibitem[Mixon et~al.(2020)Mixon, Parshall, and Pi]{mixon2020neural}
Dustin~G Mixon, Hans Parshall, and Jianzong Pi.
\newblock Neural collapse with unconstrained features.
\newblock \emph{arXiv preprint arXiv:2011.11619}, 2020.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Vardan Papyan, X.~Y. Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep learning training.
\newblock In \emph{Proceedings of the National Academy of Sciences (PNAS)}, volume 117, 2020.

\bibitem[Parker et~al.(2023)Parker, Onal, Stengel, and Intrater]{parker2023neural}
Liam Parker, Emre Onal, Anton Stengel, and Jake Intrater.
\newblock Neural collapse in the intermediate hidden layers of classification neural networks.
\newblock \emph{arXiv preprint arXiv:2308.02760}, 2023.

\bibitem[Poggio and Liao(2020)]{poggio2020explicit}
Tomaso Poggio and Qianli Liao.
\newblock Explicit regularization and implicit bias in deep network classifiers trained with the square loss.
\newblock \emph{arXiv preprint arXiv:2101.00072}, 2020.

\bibitem[Radhakrishnan et~al.(2024{\natexlab{a}})Radhakrishnan, Beaglehole, Pandit, and Belkin]{AGOPScience}
Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin.
\newblock Mechanism for feature learning in neural networks and backpropagation-free machine learning models.
\newblock \emph{Science}, 383\penalty0 (6690):\penalty0 1461--1467, 2024{\natexlab{a}}.

\bibitem[Radhakrishnan et~al.(2024{\natexlab{b}})Radhakrishnan, Belkin, and Drusvyatskiy]{RadhakrishnanLinearAGOP}
Adityanarayanan Radhakrishnan, Mikhail Belkin, and Dmitriy Drusvyatskiy.
\newblock Linear recursive feature machines provably recover low-rank matrices.
\newblock \emph{arXiv preprint arXiv:2401.04553}, 2024{\natexlab{b}}.

\bibitem[Rangamani et~al.(2023)Rangamani, Lindegaard, Galanti, and Poggio]{rangamani2023feature}
Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso Poggio.
\newblock Feature learning in deep classifiers through intermediate neural collapse.
\newblock \emph{Technical Report}, 2023.

\bibitem[Su et~al.(2023)Su, Zhang, Tsilivis, and Kempe]{su2023robustness}
Jingtong Su, Ya~Shi Zhang, Nikolaos Tsilivis, and Julia Kempe.
\newblock On the robustness of neural collapse and the neural collapse of robustness.
\newblock \emph{arXiv preprint arXiv:2311.07444}, 2023.

\bibitem[S{\'u}ken{\'\i}k et~al.(2023)S{\'u}ken{\'\i}k, Mondelli, and Lampert]{sukenik2023deep}
Peter S{\'u}ken{\'\i}k, Marco Mondelli, and Christoph Lampert.
\newblock Deep neural collapse is provably optimal for the deep unconstrained features model.
\newblock \emph{arXiv preprint arXiv:2305.13165}, 2023.

\bibitem[S{\'u}ken{\'\i}k et~al.(2024)S{\'u}ken{\'\i}k, Mondelli, and Lampert]{lowrankbias}
Peter S{\'u}ken{\'\i}k, Marco Mondelli, and Christoph Lampert.
\newblock Neural collapse versus low-rank bias: Is deep neural collapse really optimal?
\newblock \emph{arXiv preprint arXiv:2405.14468}, 2024.

\bibitem[Thrampoulidis et~al.(2022)Thrampoulidis, Kini, Vakilian, and Behnia]{thrampoulidis2022imbalance}
Christos Thrampoulidis, Ganesh~Ramachandra Kini, Vala Vakilian, and Tina Behnia.
\newblock Imbalance trouble: Revisiting neural-collapse geometry.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Tirer and Bruna(2022)]{tirer2022extended}
Tom Tirer and Joan Bruna.
\newblock Extended unconstrained features model for exploring deep neural collapse.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Tirer et~al.(2022)Tirer, Huang, and Niles-Weed]{tirer2022perturbation}
Tom Tirer, Haoxiang Huang, and Jonathan Niles-Weed.
\newblock Perturbation analysis of neural collapse.
\newblock \emph{arXiv preprint arXiv:2210.16658}, 2022.

\bibitem[Tirer et~al.(2023)Tirer, Huang, and Niles-Weed]{tirer2023perturbation}
Tom Tirer, Haoxiang Huang, and Jonathan Niles-Weed.
\newblock Perturbation analysis of neural collapse.
\newblock In \emph{International Conference on Machine Learning}, pages 34301--34329. PMLR, 2023.

\bibitem[Trivedi et~al.(2014)Trivedi, Wang, Kpotufe, and Shakhnarovich]{SamoryEGOP}
Shubhendu Trivedi, Jialei Wang, Samory Kpotufe, and Gregory Shakhnarovich.
\newblock A consistent estimator of the expected gradient outerproduct.
\newblock In \emph{UAI}, pages 819--828, 2014.

\bibitem[Wang et~al.(2022)Wang, Liu, Yaras, Balzano, and Qu]{wang2022linear}
Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu.
\newblock Linear convergence analysis of neural collapse with unconstrained features.
\newblock In \emph{OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}, 2022.

\bibitem[Wang et~al.(2023)Wang, Luo, Zheng, Huang, and Baktashmotlagh]{wang2023far}
Zijian Wang, Yadan Luo, Liang Zheng, Zi~Huang, and Mahsa Baktashmotlagh.
\newblock How far pre-trained models are from neural collapse on the target dataset informs their transferability.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5549--5558, 2023.

\bibitem[Weinan and Wojtowytsch(2022)]{wojtowytsch2020emergence}
E~Weinan and Stephan Wojtowytsch.
\newblock On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers.
\newblock In \emph{Mathematical and Scientific Machine Learning}, 2022.

\bibitem[Woodbury(1950)]{WoodburyInverseFormula}
Max~A Woodbury.
\newblock \emph{Inverting modified matrices}.
\newblock Department of Statistics, Princeton University, 1950.

\bibitem[Xu et~al.(2023)Xu, Rangamani, Liao, Galanti, and Poggio]{xu2023dynamics}
Mengjia Xu, Akshay Rangamani, Qianli Liao, Tomer Galanti, and Tomaso Poggio.
\newblock Dynamics in deep classifiers trained with the square loss: Normalization, low rank, neural collapse, and generalization bounds.
\newblock In \emph{Research}, volume~6, 2023.

\bibitem[Yuan et~al.(2023)Yuan, Xu, Kpotufe, and Hsu]{HsuEGOP}
Gan Yuan, Mingyue Xu, Samory Kpotufe, and Daniel Hsu.
\newblock Efficient estimation of the central mean subspace via smoothed gradient outer products.
\newblock \emph{arXiv preprint arXiv:2312.15469}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Li, Ding, You, Qu, and Zhu]{zhou2022optimization}
Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu.
\newblock On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Liu, Radhakrishnan, and Belkin]{CatapultsAGOP}
Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, and Mikhail Belkin.
\newblock Catapults in sgd: spikes in the training loss and their impact on generalization through feature learning.
\newblock \emph{arXiv preprint arXiv:2306.04815}, 2023.

\end{thebibliography}
