\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bang et~al.(2021)Bang, Kim, Yoo, Ha, and Choi]{rainbowmemory}
Bang, J., Kim, H., Yoo, Y., Ha, J.-W., and Choi, J.
\newblock Rainbow memory: Continual learning with a memory of diverse samples.
\newblock In \emph{CVPR}, 2021.

\bibitem[Bishop(2006)]{bishop}
Bishop, C.~M.
\newblock \emph{Pattern recognition and machine learning}.
\newblock springer, 2006.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{gpt}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Cazenavette et~al.(2022)Cazenavette, Wang, Torralba, Efros, and
  Zhu]{trajectory}
Cazenavette, G., Wang, T., Torralba, A., Efros, A.~A., and Zhu, J.-Y.
\newblock Dataset distillation by matching training trajectories.
\newblock \emph{arXiv preprint arXiv:2203.11932}, 2022.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and Le]{autoaugment}
Cubuk, E.~D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q.~V.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{CVPR}, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Dong et~al.(2016)Dong, Loy, and Tang]{fsrcnn}
Dong, C., Loy, C.~C., and Tang, X.
\newblock Accelerating the super-resolution convolutional neural network.
\newblock In \emph{ECCV}, pp.\  391--407. Springer, 2016.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{gan}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{densenet}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Huang \& Mumford(1999)Huang and Mumford]{huang1999}
Huang, J. and Mumford, D.
\newblock Statistics of natural images and models.
\newblock In \emph{CVPR}, 1999.

\bibitem[Jastrzebski et~al.(2019)Jastrzebski, Kenton, Ballas, Fischer, Bengio,
  and Storkey]{hessian}
Jastrzebski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y., and Storkey,
  A.
\newblock On the relation between the sharpest directions of dnn loss and the
  sgd step length.
\newblock In \emph{ICLR}, 2019.

\bibitem[Kim et~al.(2020)Kim, Choo, and Song]{puzzle}
Kim, J.-H., Choo, W., and Song, H.~O.
\newblock Puzzle mix: Exploiting saliency and local statistics for optimal
  mixup.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Kim et~al.(2021)Kim, Choo, Jeong, and Song]{comix}
Kim, J.-H., Choo, W., Jeong, H., and Song, H.~O.
\newblock Co-mixup: Saliency guided joint mixup with supermodular diversity.
\newblock In \emph{ICLR}, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Citeseer}, 2009.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{deeplearning}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553), 2015.

\bibitem[Li \& Hoiem(2017)Li and Hoiem]{lwf}
Li, Z. and Hoiem, D.
\newblock Learning without forgetting.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12), 2017.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and Adams]{hypergradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{ICML}, 2015.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and Sapiro]{dictionary_coding}
Mairal, J., Bach, F., Ponce, J., and Sapiro, G.
\newblock Online dictionary learning for sparse coding.
\newblock In \emph{ICML}, 2009.

\bibitem[McLachlan \& Krishnan(2007)McLachlan and Krishnan]{em}
McLachlan, G.~J. and Krishnan, T.
\newblock \emph{The EM algorithm and extensions}, volume 382.
\newblock John Wiley \& Sons, 2007.

\bibitem[Nguyen et~al.(2021)Nguyen, Novak, Xiao, and Lee]{kip}
Nguyen, T., Novak, R., Xiao, L., and Lee, J.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{cl_review}
Parisi, G.~I., Kemker, R., Part, J.~L., Kanan, C., and Wermter, S.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural Networks}, 113, 2019.

\bibitem[Patterson et~al.(2021)Patterson, Gonzalez, Le, Liang, Munguia,
  Rothchild, So, Texier, and Dean]{carbon}
Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.-M., Rothchild, D.,
  So, D., Texier, M., and Dean, J.
\newblock Carbon emissions and large neural network training.
\newblock \emph{arXiv preprint arXiv:2104.10350}, 2021.

\bibitem[Phillips(2016)]{phillips2016coresets}
Phillips, J.~M.
\newblock Coresets and sketches.
\newblock \emph{arXiv preprint arXiv:1601.00617}, 2016.

\bibitem[Prabhu et~al.(2020)Prabhu, Torr, and Dokania]{gdumb}
Prabhu, A., Torr, P.~H., and Dokania, P.~K.
\newblock Gdumb: A simple approach that questions our progress in continual
  learning.
\newblock In \emph{ECCV}, 2020.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and Lampert]{icarl}
Rebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C.~H.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{CVPR}, 2017.

\bibitem[Sitzmann et~al.(2020)Sitzmann, Martel, Bergman, Lindell, and
  Wetzstein]{siren}
Sitzmann, V., Martel, J.~N., Bergman, A.~W., Lindell, D.~B., and Wetzstein, G.
\newblock Implicit neural representations with periodic activation functions.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Such et~al.(2020)Such, Rawal, Lehman, Stanley, and
  Clune]{such2020generative}
Such, F.~P., Rawal, A., Lehman, J., Stanley, K., and Clune, J.
\newblock Generative teaching networks: Accelerating neural architecture search
  by learning to generate synthetic training data.
\newblock In \emph{ICML}, 2020.

\bibitem[Sucholutsky \& Schonlau(2021)Sucholutsky and
  Schonlau]{sucholutsky2021soft}
Sucholutsky, I. and Schonlau, M.
\newblock Soft-label dataset distillation and text dataset distillation.
\newblock In \emph{2021 International Joint Conference on Neural Networks
  (IJCNN)}, 2021.

\bibitem[Tan \& Le(2019)Tan and Le]{efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Tian et~al.(2020)Tian, Krishnan, and Isola]{imagenetsubset}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock In \emph{ECCV}, 2020.

\bibitem[Toneva et~al.(2019)Toneva, Sordoni, Combes, Trischler, Bengio, and
  Gordon]{forgetting}
Toneva, M., Sordoni, A., Combes, R. T.~d., Trischler, A., Bengio, Y., and
  Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In \emph{ICLR}, 2019.

\bibitem[Wang et~al.(2018)Wang, Zhu, Torralba, and Efros]{dd}
Wang, T., Zhu, J.-Y., Torralba, A., and Efros, A.~A.
\newblock Dataset distillation.
\newblock \emph{arXiv preprint arXiv:1811.10959}, 2018.

\bibitem[Warden(2018)]{google_speech}
Warden, P.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock \emph{arXiv preprint arXiv:1804.03209}, 2018.

\bibitem[Welling(2009)]{herding}
Welling, M.
\newblock Herding dynamical weights to learn.
\newblock In \emph{ICML}, 2009.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{ICCV}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Pezeshki, Brakel, Zhang, Bengio, and
  Courville]{speech_recog}
Zhang, Y., Pezeshki, M., Brakel, P., Zhang, S., Bengio, C. L.~Y., and
  Courville, A.
\newblock Towards end-to-end speech recognition with deep convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1701.02720}, 2017.

\bibitem[Zhao \& Bilen(2021{\natexlab{a}})Zhao and Bilen]{dm}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with distribution matching.
\newblock \emph{arXiv preprint arXiv:2110.04181}, 2021{\natexlab{a}}.

\bibitem[Zhao \& Bilen(2021{\natexlab{b}})Zhao and Bilen]{dsa}
Zhao, B. and Bilen, H.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{dc}
Zhao, B., Mopuri, K.~R., and Bilen, H.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{ICLR}, 2021.

\bibitem[Zoph \& Le(2017)Zoph and Le]{nas}
Zoph, B. and Le, Q.~V.
\newblock Neural architecture search with reinforcement learning.
\newblock In \emph{ICLR}, 2017.

\end{thebibliography}
