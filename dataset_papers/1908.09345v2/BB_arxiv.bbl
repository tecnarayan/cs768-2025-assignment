\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Bottou(2015)]{agarwal2014}
Alekh Agarwal and Leon Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock In \emph{Proc. Intl. Conf. on Machine Learning}, pages 78--86, Lille,
  France, 2015.

\bibitem[Barzilai and Borwein(1988)]{barzilai1988}
Jonathan Barzilai and Jonathan~M Borwein.
\newblock Two-point step size gradient methods.
\newblock \emph{IMA Journal of Numerical Analysis}, 8\penalty0 (1):\penalty0
  141--148, 1988.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and Nocedal]{bottou2018}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv preprint arXiv:1606.04838}, 2016.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  15210--15219, Vancouver, Canada, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{defazio2014}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  1646--1654, Montreal, Canada, 2014.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  687--697, Montreal, Canada, 2018.

\bibitem[Hu et~al.(2018)Hu, Wright, and Lessard]{hu2018diss}
Bin Hu, Stephen Wright, and Laurent Lessard.
\newblock Dissipativity theory for accelerating stochastic variance reduction:
  A unified analysis of {SVRG} and {K}atyusha using semidefinite programs.
\newblock \emph{arXiv preprint arXiv:1806.03677}, 2018.

\bibitem[Johnson and Zhang(2013)]{johnson2013}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  315--323, Lake Tahoe, Nevada, 2013.

\bibitem[Konecn{\`y} and Richt{\'a}rik(2013)]{konecny2013}
Jakub Konecn{\`y} and Peter Richt{\'a}rik.
\newblock Semi-stochastic gradient descent methods.
\newblock \emph{arXiv preprint arXiv:1312.1666}, 2013.

\bibitem[Kovalev et~al.(2019)Kovalev, Horvath, and Richtarik]{kovalev2019}
Dmitry Kovalev, Samuel Horvath, and Peter Richtarik.
\newblock Don't jump through hoops and remove those loops: {SVRG} and
  {K}atyusha are better without the outer loop.
\newblock \emph{arXiv preprint arXiv:1901.08689}, 2019.

\bibitem[Kulunchakov and Mairal(2019)]{kulunchakov2019}
Andrei Kulunchakov and Julien Mairal.
\newblock Estimate sequences for variance-reduced stochastic composite
  optimization.
\newblock In \emph{Proc. Intl. Conf. Machine Learning}, pages 3541--3550, Long
  Beach, CA, 2019.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{lei2017}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via {SCSG} methods.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  2348--2358, Long Beach, CA, 2017.

\bibitem[Li and Giannakis(2019)]{li2019adaptive}
Bingcong Li and Georgios~B Giannakis.
\newblock Adaptive step sizes in variance reduction via regularization.
\newblock \emph{arXiv preprint arXiv:1910.06532}, 2019.

\bibitem[Li et~al.(2020)Li, Ma, and Giannakis]{li2019l2s}
Bingcong Li, Meng Ma, and Georgios~B Giannakis.
\newblock On the convergence of {SARAH} and beyond.
\newblock In \emph{Proc. Intl. Conf. on Artificial Intelligence and
  Statistics}, Palermo, Italy, 2020.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  3384--3392, Montreal, Canada, 2015.

\bibitem[Liu et~al.()Liu, Han, and Guo]{liuclass}
Yan Liu, Congying Han, and Tiande Guo.
\newblock A class of stochastic variance reduced methods with an adaptive
  stepsize.
\newblock URL
  \url{http://www.optimization-online.org/DB_FILE/2019/04/7170.pdf}.

\bibitem[Mairal(2013)]{mairal2013}
Julien Mairal.
\newblock Optimization with first-order surrogate functions.
\newblock In \emph{Proc. Intl. Conf. on Machine Learning}, pages 783--791,
  Atlanta, 2013.

\bibitem[Nesterov(2004)]{nesterov2004}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course},
  volume~87.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{Proc. Intl. Conf. Machine Learning}, Sydney, Australia,
  2017.

\bibitem[Nitanda(2014)]{nitanda2014}
Atsushi Nitanda.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  1574--1582, Montreal, Canada, 2014.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and Smola]{reddi2016}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proc. Intl. Conf. on Machine Learning}, pages 314--323, New
  York City, NY, 2016.

\bibitem[Robbins and Monro(1951)]{robbins1951}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem[Roux et~al.(2012)Roux, Schmidt, and Bach]{roux2012}
Nicolas~L Roux, Mark Schmidt, and Francis~R Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  2663--2671, Lake Tahoe, Nevada, 2012.

\bibitem[Shalev-Shwartz and Zhang(2013)]{shalev2013}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, vol. 14:\penalty0 pp.
  567--599, Feb. 2013.

\bibitem[Tan et~al.(2016)Tan, Ma, Dai, and Qian]{tan2016}
Conghui Tan, Shiqian Ma, Yu-Hong Dai, and Yuqiu Qian.
\newblock Barzilai-{B}orwein step size for stochastic gradient descent.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  685--693, Barcelona, Spain, 2016.

\bibitem[Xiao and Zhang(2014)]{xiao2014}
Lin Xiao and Tong Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Yang et~al.(2019)Yang, Chen, and Wang]{yang2019}
Zhuang Yang, Zengping Chen, and Cheng Wang.
\newblock Accelerating mini-batch sarah by step size rules.
\newblock \emph{arXiv preprint arXiv:1906.08496}, 2019.

\end{thebibliography}
