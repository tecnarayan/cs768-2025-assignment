@inproceedings{loizou2021stochastic,
  title={Stochastic {Polyak} step-size for {SGD}: An adaptive learning rate for fast convergence},
  author={Loizou, Nicolas and Vaswani, Sharan and Laradji, Issam Hadj and Lacoste-Julien, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021},
}

@article{rolinek2018l4,
  title={L4: Practical loss-based stepsize adaptation for deep learning},
  author={Rolinek, Michal and Martius, Georg},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  year={1951},
  publisher={JSTOR}
}

@article{zhou2020convergence,
  title={On the convergence of mirror descent beyond stochastic convex programming},
  author={Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Boyd, Stephen P and Glynn, Peter W},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={1},
  pages={687--716},
  year={2020},
  publisher={SIAM}
}

@inproceedings{shi2020rmsprop,
  title={RMSprop converges with proper hyper-parameter},
  author={Shi, Naichen and Li, Dawei and Hong, Mingyi and Sun, Ruoyu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{reddi2019convergence,
  title={On the Convergence of {Adam} and Beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes {signSGD} and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  year={2019},
}

@inproceedings{safaryan2021stochastic,
  title={Stochastic Sign Descent Methods: New Algorithms and Better Theory},
  author={Safaryan, Mher and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@article{lacoste2012simpler,
  title={A simpler approach to obtaining an $O(1/t)$ convergence rate for the projected stochastic subgradient method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{davis2018subgradient,
  title={Subgradient methods for sharp weakly convex functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy and MacPhee, Kellie J and Paquette, Courtney},
  journal={Journal of Optimization Theory and Applications},
  volume={179},
  number={3},
  pages={962--982},
  year={2018},
  publisher={Springer}
}

@article{boyd2003subgradient,
  title={Subgradient methods},
  author={Boyd, Stephen and Xiao, Lin and Mutapcic, Almir},
  journal={Lecture Notes of EE392o, Stanford University, Autumn Quarter},
  volume={2004},
  pages={2004--2005},
  year={2003}
}

@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={International Conference on Machine Learning},
  year={2019},
}

@article{lessard2016analysis,
  title={Analysis and design of optimization algorithms via integral quadratic constraints},
  author={Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={1},
  pages={57--95},
  year={2016},
  publisher={SIAM}
}

@inproceedings{berrada2019training,
  title={Training neural networks for and by interpolation},
  author={Berrada, L. and Zisserman, A. and Kumar, M. P.},
  booktitle={International Conference on Machine Learning},
  year={2020},
}

@article{oberman2019stochastic,
  title={Stochastic Gradient Descent with {Polyak}'s Learning Rate},
  author={Oberman, Adam M and Prazeres, Mariana},
  journal={arXiv preprint arXiv:1903.08688},
  year={2019}
}

@article{gower2021stochastic,
  title={Stochastic {Polyak} Stepsize with a Moving Target},
  author={Gower, Robert M and Defazio, Aaron and Rabbat, Michael},
  journal={arXiv preprint arXiv:2106.11851},
  year={2021}
}

@article{loizou2020momentum,
  title={Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods},
  author={Loizou, Nicolas and Richt{\'a}rik, Peter},
  journal={Computational Optimization and Applications},
  volume={77},
  number={3},
  pages={653--710},
  year={2020},
  publisher={Springer}
}

@article{su2014differential,
  title={A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@misc{sebbouh2021almost,
  title={Almost sure convergence rates for Stochastic Gradient Descent and Stochastic Heavy Ball},
  author={Sebbouh, Othmane and Gower, Robert and Defazio, Aaron},
  year={2021}
}

@book{nesterov2018lectures,
  title={Lectures on {C}onvex {O}ptimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  year={2013},
  publisher={SIAM}
}

@article{shi2021ai,
  title={AI-SARAH: Adaptive and Implicit Stochastic Recursive Gradient Methods},
  author={Shi, Zheng and Loizou, Nicolas and Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:2102.09700},
  year={2021}
}

@inproceedings{gower2021sgd,
  title={{SGD} for structured nonconvex functions: Learning rates, minibatching and interpolation},
  author={Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021},
}

@book{polyak1987introduction,
  title={Introduction to {O}ptimization},
  author={Polyak, Boris},
  publisher={Inc., Publications Division, New York},
  year={1987}
}

@article{arnold2019reducing,
  title={Reducing the variance in online optimization by transporting past gradients},
  author={Arnold, S{\'e}bastien and Manzagol, Pierre-Antoine and Babanezhad Harikandeh, Reza and Mitliagkas, Ioannis and Le Roux, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{ghadimi2015global,
  title={Global convergence of the heavy-ball method for convex optimization},
  author={Ghadimi, Euhanna and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  booktitle={2015 European control conference (ECC)},
  year={2015},
  organization={IEEE}
}

@inproceedings{alimisis2021momentum,
  title={Momentum Improves Optimization on Riemannian Manifolds},
  author={Alimisis, Foivos and Orvieto, Antonio and B{\'e}cigneul, Gary and Lucchi, Aurelien},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021},
}

@inproceedings{orvieto2020role,
  title={The role of memory in stochastic optimization},
  author={Orvieto, Antonio and Kohler, Jonas and Lucchi, Aurelien},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2020},
}

@inproceedings{allen2014linear,
  title={Linear Coupling: An Ultimate Unification of Gradient and Mirror Descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  booktitle={8th Innovations in Theoretical Computer Science Conference (ITCS 2017)},
  year={2017},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@article{nesterov2020primal,
  title={Primal--dual accelerated gradient methods with small-dimensional relaxation oracle},
  author={Nesterov, Yurii and Gasnikov, Alexander and Guminov, Sergey and Dvurechensky, Pavel},
  journal={Optimization Methods and Software},
  pages={1--38},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{asi2019stochastic,
  title={Stochastic (approximate) proximal point methods: Convergence, optimality, and adaptivity},
  author={Asi, Hilal and Duchi, John C},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={3},
  pages={2257--2290},
  year={2019},
  publisher={SIAM}
}

@article{asi2019importance,
  title={The importance of better models in stochastic optimization},
  author={Asi, Hilal and Duchi, John C},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={46},
  pages={22924--22930},
  year={2019},
  publisher={National Acad Sciences}
}



@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={7},
  year={2011}
}

@inproceedings{tang20211,
  title={1-bit adam: Communication efficient large-scale training with adam’s convergence speed},
  author={Tang, Hanlin and Gan, Shaoduo and Awan, Ammar Ahmad and Rajbhandari, Samyam and Li, Conglong and Lian, Xiangru and Liu, Ji and Zhang, Ce and He, Yuxiong},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@article{orvieto2021vanishing,
  title={Vanishing Curvature and the Power of Adaptive Methods in Randomly Initialized Deep Networks},
  author={Orvieto, Antonio and Kohler, Jonas and Pavllo, Dario and Hofmann, Thomas and Lucchi, Aurelien},
  journal={International Conference on Artificial Intelligence and Statistics (to appear},
  year={2022}
}

@inproceedings{cutkosky2020momentum,
  title={Momentum improves normalized {SGD}},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={International Conference on Machine Learning},
  year={2020},
}

@article{levy2016power,
  title={The power of normalization: Faster evasion of saddle points},
  author={Levy, Kfir Y},
  journal={arXiv preprint arXiv:1611.04831},
  year={2016}
}

@inproceedings{staib2019escaping,
  title={Escaping saddle points with adaptive gradient methods},
  author={Staib, Matthew and Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  year={2019},
}

@article{ene2020adaptive,
  title={Adaptive and universal algorithms for variational inequalities with optimal convergence},
  author={Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2010.07799},
  year={2020}
}

@book{goodfellow2016deep,
  title={Deep {L}earning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{zhang2019adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}



@book{kushner2003stochastic,
  title={Stochastic {A}pproximation and {R}ecursive {A}lgorithms and {A}pplications},
  author={Kushner, Harold and Yin, G George},
  volume={35},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{tieleman2012lecture,
  title={Lecture 6.5 - {RMSprop}, {Coursera}: Neural networks for machine learning},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={University of Toronto, Technical Report},
  year={2012}
}

@book{lan2020first,
  title={First-order and Stochastic Optimization Methods for Machine Learning},
  author={Lan, Guanghui},
  year={2020},
  publisher={Springer Nature}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{hazan2019revisiting,
  title={Revisiting the {Polyak} step size},
  author={Hazan, Elad and Kakade, Sham},
  journal={arXiv preprint arXiv:1905.00313},
  year={2019}
}

@article{vaswani2020adaptive,
  title={Adaptive Gradient Methods Converge Faster with Over-Parameterization (but you should do a line-search)},
  author={Vaswani, Sharan and Laradji, Issam and Kunstner, Frederik and Meng, Si Yi and Schmidt, Mark and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:2006.06835},
  year={2020}
}

@article{d2021stochastic,
  title={Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic {Polyak} Stepsize},
  author={D'Orazio, Ryan and Loizou, Nicolas and Laradji, Issam and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:2110.15412},
  year={2021}
}

@misc{CC01a,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {LIBSVM: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 year = {2011},
}


@inproceedings{
anonymous2022directional,
title={Directional Bias Helps Stochastic Gradient Descent to Generalize in Nonparametric Model},
author={Anonymous},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=Zk3TwMJNj7},
note={under review}
}

@inproceedings{wu2020direction,
  title={Direction Matters: On the Implicit Bias of Stochastic Gradient Descent with Moderate Learning Rate},
  author={Wu, Jingfeng and Zou, Difan and Braverman, Vladimir and Gu, Quanquan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on Optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}


@article{levy2018online,
  title={Online adaptive methods, universality and acceleration},
  author={Levy, Kfir Y and Yurtsever, Alp and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@inproceedings{xie2020linear,
  title={Linear convergence of adaptive stochastic gradient descent},
  author={Xie, Yuege and Wu, Xiaoxia and Ward, Rachel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2020},
}

@article{ahn2020sgd,
  title={SGD with shuffling: optimal rates without component convexity and large epoch requirements},
  author={Ahn, Kwangjun and Yun, Chulhee and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@misc{Dua:2019,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{chang2011libsvm,
  title={{LIBSVM}: a library for support vector machines},
  author={Chang, Chih-Chung},
  journal={ACM Transactions on Intelligent Systems and Technology},
  year={2011}
}

@inproceedings{gower2019sgd,
  title={{SGD}: General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  year={2019},
}

@article{defossez2020simple,
  title={A Simple Convergence Proof of {Adam} and {Adagrad}},
  author={D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
  journal={arXiv preprint arXiv:2003.02395},
  year={2020}
}

@article{traore2021sequential,
  title={Sequential convergence of {AdaGrad} algorithm for smooth convex optimization},
  author={Traor{\'e}, Cheik and Pauwels, Edouard},
  journal={Operations Research Letters},
  volume={49},
  number={4},
  pages={452--458},
  year={2021},
  publisher={Elsevier}
}

@article{loshchilov2016sgdr,
  title={{SGDR}: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{richtarik2021ef21,
  title={EF21: A new, simpler, theoretically better, and practically faster error feedback},
  author={Richt{\'a}rik, Peter and Sokolov, Igor and Fatkhullin, Ilyas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{dauphin2015equilibrated,
  title={Equilibrated adaptive learning rates for non-convex optimization},
  author={Dauphin, Yann and De Vries, Harm and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv:1412.6980},
  year={2014}
}



@inproceedings{bock2019non,
  title={Non-convergence and limit cycles in the {Adam} optimizer},
  author={Bock, Sebastian and Wei{\ss}, Martin},
  booktitle={International Conference on Artificial Neural Networks},
  year={2019},
  organization={Springer}
}