\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bradshaw et~al.(2017)Bradshaw, Matthews, and
  Ghahramani]{bradshaw2017adversarial}
J.~Bradshaw, A.~G. d.~G. Matthews, and Z.~Ghahramani.
\newblock Adversarial examples, uncertainty, and transfer testing robustness in
  {G}aussian process hybrid deep networks.
\newblock \emph{arXiv preprint arXiv:1707.02476}, 2017.

\bibitem[Burt et~al.(2020)Burt, Rasmussen, and van~der Wilk]{burt2020gpviconv}
D.~R. Burt, C.~E. Rasmussen, and M.~van~der Wilk.
\newblock Convergence of sparse variational inference in {G}aussian processes
  regression.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 21\penalty0
  (131):\penalty0 1--63, 2020.

\bibitem[Csat{\'o} et~al.(1999)Csat{\'o}, Fokou{\'e}, Opper, Schottky, and
  Winther]{csato1999efficient}
L.~Csat{\'o}, E.~Fokou{\'e}, M.~Opper, B.~Schottky, and O.~Winther.
\newblock Efficient approaches to gaussian process classification.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Damianou and Lawrence(2013)]{damianou2013deep}
A.~Damianou and N.~D. Lawrence.
\newblock Deep {G}aussian processes.
\newblock In \emph{Proceedings of the 16th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2013.

\bibitem[Gardner et~al.(2018)Gardner, Pleiss, Wu, Weinberger, and
  Wilson]{pmlr-v84-gardner18a}
J.~Gardner, G.~Pleiss, R.~Wu, K.~Weinberger, and A.~Wilson.
\newblock Product kernel interpolation for scalable gaussian processes.
\newblock In A.~Storkey and F.~Perez-Cruz, editors, \emph{Proceedings of the
  Twenty-First International Conference on Artificial Intelligence and
  Statistics}, volume~84 of \emph{Proceedings of Machine Learning Research},
  pages 1407--1416. PMLR, 09--11 Apr 2018.
\newblock URL \url{https://proceedings.mlr.press/v84/gardner18a.html}.

\bibitem[Gibbs and MacKay(2000)]{gibbs2000variational}
M.~N. Gibbs and D.~J. MacKay.
\newblock Variational gaussian process classifiers.
\newblock \emph{IEEE Transactions on Neural Networks}, 11\penalty0
  (6):\penalty0 1458--1464, 2000.

\bibitem[Hensman et~al.(2013)Hensman, Fusi, and Lawrence]{hensman2013}
J.~Hensman, N.~Fusi, and N.~D. Lawrence.
\newblock Gaussian processes for big data.
\newblock In \emph{Proceedings of the 29th Conference on Uncertainty in
  Artifical Intelligence (UAI)}, 2013.

\bibitem[Hernandez-Lobato and Adams(2015)]{pmlr-v37-hernandez-lobatoc15}
J.~M. Hernandez-Lobato and R.~Adams.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In F.~Bach and D.~Blei, editors, \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pages 1861--1869, Lille, France, 07--09 Jul
  2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v37/hernandez-lobatoc15.html}.

\bibitem[Hildebrandt and Schoenberg(1933)]{hildebrandt1933linear}
T.~Hildebrandt and I.~Schoenberg.
\newblock On linear functional operations and the moment problem for a finite
  interval in one or several dimensions.
\newblock \emph{Annals of Mathematics}, pages 317--328, 1933.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
M.~D. Hoffman, D.~M. Blei, C.~Wang, and J.~Paisley.
\newblock Stochastic variational inference.
\newblock \emph{Journal of Machine Learning Research}, 2013.

\bibitem[Hug et~al.(2020)Hug, H{\"u}bner, and Arens]{hug2020introducing}
R.~Hug, W.~H{\"u}bner, and M.~Arens.
\newblock Introducing probabilistic b{\'e}zier curves for n-step sequence
  prediction.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 10162--10169, 2020.

\bibitem[Hug et~al.(2022)Hug, Becker, H{\"u}bner, Arens, and Beyerer]{hug2022b}
R.~Hug, S.~Becker, W.~H{\"u}bner, M.~Arens, and J.~Beyerer.
\newblock B\'ezier curve gaussian processes.
\newblock \emph{arXiv preprint arXiv:2205.01754}, 2022.

\bibitem[Kapoor et~al.(2021)Kapoor, Finzi, Wang, and Wilson]{kapoor2021skiing}
S.~Kapoor, M.~Finzi, K.~A. Wang, and A.~G.~G. Wilson.
\newblock Skiing on simplices: Kernel interpolation on the permutohedral
  lattice for scalable gaussian processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  5279--5289. PMLR, 2021.

\bibitem[Kim et~al.(2005)Kim, Mallick, and Holmes]{kim2005analyzing}
H.-M. Kim, B.~K. Mallick, and C.~C. Holmes.
\newblock Analyzing nonstationary spatial data using piecewise gaussian
  processes.
\newblock \emph{Journal of the American Statistical Association}, 100\penalty0
  (470):\penalty0 653--668, 2005.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Nguyen-Tuong et~al.(2008)Nguyen-Tuong, Peters, and
  Seeger]{nguyen2008local}
D.~Nguyen-Tuong, J.~Peters, and M.~Seeger.
\newblock Local gaussian process regression for real time online model
  learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Ober et~al.(2021)Ober, Rasmussen, and van~der Wilk]{ober2021promises}
S.~W. Ober, C.~E. Rasmussen, and M.~van~der Wilk.
\newblock The promises and pitfalls of deep kernel learning.
\newblock In \emph{Proceedings of the 37th Conference on Uncertainty in
  Artifical Intelligence (UAI)}, 2021.

\bibitem[Petrone(1999{\natexlab{a}})]{petrone1999bayesian}
S.~Petrone.
\newblock Bayesian density estimation using bernstein polynomials.
\newblock \emph{Canadian Journal of Statistics}, 27\penalty0 (1):\penalty0
  105--126, 1999{\natexlab{a}}.

\bibitem[Petrone(1999{\natexlab{b}})]{petrone1999random}
S.~Petrone.
\newblock Random bernstein polynomials.
\newblock \emph{Scandinavian Journal of Statistics}, 26\penalty0 (3):\penalty0
  373--393, 1999{\natexlab{b}}.

\bibitem[Petrone and Wasserman(2002)]{petrone2002consistency}
S.~Petrone and L.~Wasserman.
\newblock Consistency of bernstein polynomial posteriors.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 64\penalty0 (1):\penalty0 79--100, 2002.

\bibitem[Prautzsch et~al.(2002)Prautzsch, Boehm, and
  Paluszny]{prautzsch2002bezier}
H.~Prautzsch, W.~Boehm, and M.~Paluszny.
\newblock \emph{B{\'e}zier and B-spline techniques}, volume~6.
\newblock Springer, 2002.

\bibitem[Quinonero-Candela and Rasmussen(2005)]{quinonero2005unifying}
J.~Quinonero-Candela and C.~E. Rasmussen.
\newblock A unifying view of sparse approximate gaussian process regression.
\newblock \emph{The Journal of Machine Learning Research}, 6:\penalty0
  1939--1959, 2005.

\bibitem[Salimbeni and Deisenroth(2017)]{salimbeni2017doubly}
H.~Salimbeni and M.~Deisenroth.
\newblock Doubly stochastic variational inference for deep gaussian processes.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Snelson and Ghahramani(2005)]{snelson2005sparse}
E.~Snelson and Z.~Ghahramani.
\newblock Sparse gaussian processes using pseudo-inputs.
\newblock \emph{Advances in neural information processing systems}, 18, 2005.

\bibitem[Titsias(2009)]{pmlr-v5-titsias09a}
M.~Titsias.
\newblock Variational learning of inducing variables in sparse gaussian
  processes.
\newblock In \emph{Proceedings of the 12th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2009.

\bibitem[Tran et~al.(2021)Tran, Milios, Michiardi, and
  Filippone]{tran2021sparse}
G.-L. Tran, D.~Milios, P.~Michiardi, and M.~Filippone.
\newblock Sparse within sparse gaussian processes using neighbor information.
\newblock In \emph{International Conference on Machine Learning}, pages
  10369--10378. PMLR, 2021.

\bibitem[Wang et~al.(2019)Wang, Pleiss, Gardner, Tyree, Weinberger, and
  Wilson]{wang2019exact}
K.~Wang, G.~Pleiss, J.~Gardner, S.~Tyree, K.~Q. Weinberger, and A.~G. Wilson.
\newblock Exact gaussian processes on a million data points.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/01ce84968c6969bdd5d51c5eeaa3946a-Paper.pdf}.

\bibitem[Williams and Rasmussen(2006)]{williams2006gaussian}
C.~K.~I. Williams and C.~E. Rasmussen.
\newblock \emph{Gaussian processes for machine learning}.
\newblock MIT Press Cambridge, MA, 2006.

\bibitem[Wilson and Nickisch(2015)]{wilson2015kernel}
A.~Wilson and H.~Nickisch.
\newblock Kernel interpolation for scalable structured gaussian processes
  (kiss-gp).
\newblock In \emph{International conference on machine learning}, pages
  1775--1784. PMLR, 2015.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016deep}
A.~G. Wilson, Z.~Hu, R.~Salakhutdinov, and E.~P. Xing.
\newblock Deep kernel learning.
\newblock In \emph{Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2016.

\bibitem[Wu et~al.(2021)Wu, Miller, Anderson, Pleiss, Blei, and
  Cunningham]{wu2021hierarchical}
L.~Wu, A.~Miller, L.~Anderson, G.~Pleiss, D.~Blei, and J.~Cunningham.
\newblock Hierarchical inducing point gaussian process for inter-domian
  observations.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2926--2934. PMLR, 2021.

\bibitem[Wu et~al.(2022)Wu, Pleiss, and Cunningham]{wu2022variational}
L.~Wu, G.~Pleiss, and J.~Cunningham.
\newblock Variational nearest neighbor gaussian processes.
\newblock \emph{arXiv preprint arXiv:2202.01694}, 2022.

\end{thebibliography}
