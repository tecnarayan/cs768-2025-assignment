\begin{thebibliography}{10}

\bibitem{mendez2023controlling}
Andres~H Mendez, Chen Yu, and Linda~B Smith.
\newblock Controlling the input: How one-year-old infants sustain visual attention.
\newblock {\em Developmental Science}, page e13445, 2023.

\bibitem{COOK2011341}
Claire Cook, Noah~D. Goodman, and Laura~E. Schulz.
\newblock Where science starts: Spontaneous experiments in preschoolers’ exploratory play.
\newblock {\em Cognition}, 120(3):341--349, 2011.
\newblock Probabilistic models of cognitive development.

\bibitem{conceptualChange}
Susan Carey.
\newblock {\em Conceptual Change In Childhood}.
\newblock MIT Press, 1985.

\bibitem{schulz2012origins}
Laura Schulz.
\newblock The origins of inquiry: Inductive inference and exploration in early childhood.
\newblock {\em Trends in cognitive sciences}, 16(7):382--389, 2012.

\bibitem{gopnik1999scientist}
Alison Gopnik, Andrew~N Meltzoff, and Patricia~K Kuhl.
\newblock {\em The scientist in the crib: Minds, brains, and how children learn.}
\newblock William Morrow \& Co, 1999.

\bibitem{klahr1988dual}
David Klahr and Kevin Dunbar.
\newblock Dual space search during scientific reasoning.
\newblock {\em Cognitive science}, 12(1):1--48, 1988.

\bibitem{chater2008probabilistic}
Nick Chater and Mike Oaksford.
\newblock {\em The probabilistic mind: Prospects for Bayesian cognitive science}.
\newblock Oxford University Press, USA, 2008.

\bibitem{tenenbaum2011grow}
Joshua~B Tenenbaum, Charles Kemp, Thomas~L Griffiths, and Noah~D Goodman.
\newblock How to grow a mind: Statistics, structure, and abstraction.
\newblock {\em Science}, 331(6022):1279--1285, 2011.

\bibitem{qiu2023phenomenal}
Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, and Xiang Ren.
\newblock Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.
\newblock {\em arXiv preprint arXiv:2310.08559}, 2023.

\bibitem{wang2023hypothesis}
Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah~D Goodman.
\newblock Hypothesis search: Inductive reasoning with language models.
\newblock {\em arXiv preprint arXiv:2309.05660}, 2023.

\bibitem{ellis2023human}
Kevin Ellis.
\newblock Human-like few-shot learning via bayesian reasoning over natural language.
\newblock {\em NeurIPS}, 2023.

\bibitem{del2006smcs}
Pierre Del~Moral, Arnaud Doucet, and Ajay Jasra.
\newblock Sequential monte carlo samplers.
\newblock {\em Journal of the Royal Statistical Society Series B: Statistical Methodology}, 68(3):411--436, 2006.

\bibitem{bramley2018grounding}
Neil Bramley, Anselm Rothe, Josh Tenenbaum, Fei Xu, and Todd Gureckis.
\newblock Grounding compositional hypothesis generation in specific instances.
\newblock In {\em Proceedings of the 40th annual conference of the cognitive science society}, 2018.

\bibitem{franken2022algorithms}
Jan-Philipp Fr{\"a}nken, Nikos~C Theodoropoulos, and Neil~R Bramley.
\newblock Algorithms of adaptation in inductive inference.
\newblock {\em Cognitive Psychology}, 137:101506, 2022.

\bibitem{bramley2023active}
Neil~R Bramley and Fei Xu.
\newblock Active inductive inference in children and adults: A constructivist perspective.
\newblock {\em Cognition}, 238:105471, 2023.

\bibitem{gopnik2000detecting}
Alison Gopnik and David~M Sobel.
\newblock Detecting blickets: How young children use information about novel causal powers in categorization and induction.
\newblock {\em Child development}, 71(5):1205--1222, 2000.

\bibitem{zhang2021acre}
Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, and Yixin Zhu.
\newblock Acre: Abstract causal reasoning beyond covariation.
\newblock In {\em Proceedings of the ieee/cvf conference on computer vision and pattern recognition}, pages 10643--10653, 2021.

\bibitem{lindley1956measure}
Dennis~V Lindley.
\newblock On a measure of the information provided by an experiment.
\newblock {\em The Annals of Mathematical Statistics}, 27(4):986--1005, 1956.

\bibitem{rainforth2024modern}
Tom Rainforth, Adam Foster, Desi~R Ivanova, and Freddie Bickford~Smith.
\newblock Modern bayesian experimental design.
\newblock {\em Statistical Science}, 39(1):100--114, 2024.

\bibitem{settles2009active}
Burr Settles.
\newblock Active learning literature survey.
\newblock 2009.

\bibitem{simon1955behavioral}
Herbert~A Simon.
\newblock A behavioral model of rational choice.
\newblock {\em The quarterly journal of economics}, pages 99--118, 1955.

\bibitem{sanborn2010rational}
Adam~N Sanborn, Thomas~L Griffiths, and Daniel~J Navarro.
\newblock Rational approximations to rational models: alternative algorithms for category learning.
\newblock {\em Psychological review}, 117(4):1144, 2010.

\bibitem{https://doi.org/10.1111/tops.12142}
Thomas~L. Griffiths, Falk Lieder, and Noah~D. Goodman.
\newblock Rational use of cognitive resources: Levels of analysis between the computational and the algorithmic.
\newblock {\em Topics in Cognitive Science}, 7(2):217--229, 2015.

\bibitem{levy2008modeling}
Roger Levy, Florencia Reali, and Thomas Griffiths.
\newblock Modeling the effects of memory on human online sentence processing with particle filters.
\newblock {\em Advances in neural information processing systems}, 21, 2008.

\bibitem{sanborn2016bayesian}
Adam~N Sanborn and Nick Chater.
\newblock Bayesian brains without probabilities.
\newblock {\em Trends in cognitive sciences}, 20(12):883--893, 2016.

\bibitem{chen2023teaching}
Xinyun Chen, Maxwell Lin, Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Teaching large language models to self-debug.
\newblock {\em arXiv preprint arXiv:2304.05128}, 2023.

\bibitem{naesseth2015nested}
Christian Naesseth, Fredrik Lindsten, and Thomas Schon.
\newblock Nested sequential monte carlo methods.
\newblock In {\em International Conference on Machine Learning}, pages 1292--1301. PMLR, 2015.

\bibitem{liu2001monte}
Jun~S Liu and Jun~S Liu.
\newblock {\em Monte Carlo strategies in scientific computing}, volume~10.
\newblock Springer, 2001.

\bibitem{le2023betterproof}
Tuan~Anh Le.
\newblock A better proof of unbiasedness of the sequential monte carlo based normalizing constant estimator, 2023.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock {\em arXiv preprint arXiv:2210.03629}, 2022.

\bibitem{goodman2008rational}
Noah~D Goodman, Joshua~B Tenenbaum, Jacob Feldman, and Thomas~L Griffiths.
\newblock A rational analysis of rule-based concept learning.
\newblock {\em Cognitive science}, 32(1):108--154, 2008.

\bibitem{piantadosi2011learning}
Steven~Thomas Piantadosi.
\newblock {\em Learning and the language of thought}.
\newblock PhD thesis, MIT, 2011.

\bibitem{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338, 2015.

\bibitem{erdogan2015sensory}
Goker Erdogan, Ilker Yildirim, and Robert~A Jacobs.
\newblock From sensory signals to modality-independent conceptual representations: A probabilistic language of thought approach.
\newblock {\em PLoS computational biology}, 11(11):e1004610, 2015.

\bibitem{SABLEMEYER2022101527}
Mathias Sablé-Meyer, Kevin Ellis, Josh Tenenbaum, and Stanislas Dehaene.
\newblock A language of thought for the mental representation of geometric shapes.
\newblock {\em Cognitive Psychology}, 139:101527, 2022.

\bibitem{aher2023using}
Gati~V Aher, Rosa~I Arriaga, and Adam~Tauman Kalai.
\newblock Using large language models to simulate multiple humans and replicate human subject studies.
\newblock In {\em International Conference on Machine Learning}, pages 337--371. PMLR, 2023.

\bibitem{dillion2023can}
Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray.
\newblock Can ai language models replace human participants?
\newblock {\em Trends in Cognitive Sciences}, 2023.

\bibitem{dasgupta2022language}
Ishita Dasgupta, Andrew~K Lampinen, Stephanie~CY Chan, Antonia Creswell, Dharshan Kumaran, James~L McClelland, and Felix Hill.
\newblock Language models show human-like content effects on reasoning.
\newblock {\em arXiv preprint arXiv:2207.07051}, 2022.

\bibitem{anderson1990theac}
John~R. Anderson.
\newblock The adaptive character of thought.
\newblock 1990.

\bibitem{handa2024open}
Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas, Alex Tamkin, and Belinda~Z Li.
\newblock Bayesian preference elicitation with language models.
\newblock {\em arXiv preprint arXiv:2403.05534}, 2024.

\bibitem{amalric2017language}
Marie Amalric, Liping Wang, Pierre Pica, Santiago Figueira, Mariano Sigman, and Stanislas Dehaene.
\newblock The language of geometry: Fast comprehension of geometrical primitives and rules in human adults and preschoolers.
\newblock {\em PLoS computational biology}, 13(1):e1005273, 2017.

\bibitem{ellis2022synthesizing}
Kevin Ellis, Adam Albright, Armando Solar-Lezama, Joshua~B Tenenbaum, and Timothy~J O’Donnell.
\newblock Synthesizing theories of human language with bayesian program induction.
\newblock {\em Nature communications}, 13(1):5024, 2022.

\bibitem{10.1145/3453483.3454080}
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sabl\'{e}-Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua~B. Tenenbaum.
\newblock Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning.
\newblock In {\em PLDI}, 2021.

\bibitem{tian2020learning}
Lucas Tian, Kevin Ellis, Marta Kryven, and Josh Tenenbaum.
\newblock Learning abstract structure for drawing by efficient motor program induction.
\newblock {\em Advances in Neural Information Processing Systems}, 33:2686--2697, 2020.

\bibitem{saad2019bayesian}
Feras~A Saad, Marco~F Cusumano-Towner, Ulrich Schaechtle, Martin~C Rinard, and Vikash~K Mansinghka.
\newblock Bayesian synthesis of probabilistic programs for automatic data modeling.
\newblock {\em Proceedings of the ACM on Programming Languages}, 3(POPL):1--32, 2019.

\bibitem{liang11dcs}
Percy Liang, Michael~I. Jordan, and Dan Klein.
\newblock Learning dependency-based compositional semantics.
\newblock In {\em ACL}, pages 590--599, 2011.

\bibitem{wilson2020bayesian}
Andrew~G Wilson and Pavel Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of generalization.
\newblock {\em Advances in neural information processing systems}, 33:4697--4708, 2020.

\bibitem{kumar2022using}
Sreejan Kumar, Carlos~G Correa, Ishita Dasgupta, Raja Marjieh, Michael Hu, Robert~D. Hawkins, Jonathan Cohen, Nathaniel Daw, Karthik~R Narasimhan, and Thomas~L. Griffiths.
\newblock Using natural language and program abstractions to instill human inductive biases in machines.
\newblock In {\em NeurIPS}, 2022.

\bibitem{chollet2019measure}
François Chollet.
\newblock On the measure of intelligence, 2019.

\bibitem{tsividis2021human}
Pedro~A Tsividis, Joao Loula, Jake Burga, Nathan Foss, Andres Campero, Thomas Pouncy, Samuel~J Gershman, and Joshua~B Tenenbaum.
\newblock Human-level reinforcement learning through theory-based modeling, exploration, and planning.
\newblock {\em arXiv preprint arXiv:2107.12544}, 2021.

\bibitem{murphy2012machine}
Kevin~P Murphy.
\newblock {\em Machine learning: a probabilistic perspective}.
\newblock MIT press, 2012.

\bibitem{tenenbaum1999bayesian}
Joshua~Brett Tenenbaum.
\newblock {\em A Bayesian framework for concept learning}.
\newblock PhD thesis, Massachusetts Institute of Technology, 1999.

\bibitem{piantadosi2016logical}
Steven~T Piantadosi, Joshua~B Tenenbaum, and Noah~D Goodman.
\newblock The logical primitives of thought: Empirical foundations for compositional cognitive models.
\newblock {\em Psychological review}, 123(4):392, 2016.

\bibitem{franken2023modeling}
Jan-Philipp Fr{\"a}nken, Christopher~G Lucas, Neil~R Bramley, and Steven~T Piantadosi.
\newblock Modeling infant object perception as program induction.
\newblock {\em arXiv preprint arXiv:2309.07099}, 2023.

\bibitem{zhou2024hypothesis}
Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan.
\newblock Hypothesis generation with large language models.
\newblock {\em arXiv preprint arXiv:2404.04326}, 2024.

\bibitem{schwettmann2023find}
Sarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torralba.
\newblock Find: A function description benchmark for evaluating interpretability methods.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{li2023gate}
Belinda~Z Li, Alex Tamkin, Noah Goodman, and Jacob Andreas.
\newblock Eliciting human preferences with language models.
\newblock {\em arXiv preprint arXiv:2310.11589}, 2023.

\bibitem{piriyakulkij:neuripsfmdm23}
Wasu~Top Piriyakulkij, Volodymyr Kuleshov, and Kevin Ellis.
\newblock Active preference inference using language models and probabilistic reasoning.
\newblock In {\em NeurIPS FMDM Workshop}, 2023.

\bibitem{grand2024battleship}
Gabriel Grand, Valerio Pepe, Jacob Andreas, and Joshua~B Tenenbaum.
\newblock Loose lips sink ships: Asking questions in battleship with language-informed program sampling.
\newblock {\em arXiv preprint arXiv:2402.19471}, 2024.

\bibitem{andukuri2024stargate}
Chinmaya Andukuri, Jan-Philipp Fr{\"a}nken, Tobias Gerstenberg, and Noah~D Goodman.
\newblock Star-gate: Teaching language models to ask clarifying questions.
\newblock {\em arXiv preprint arXiv:2403.19154}, 2024.

\bibitem{dohan2022cascade}
David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael~Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif~A Saurous, Jascha Sohl-Dickstein, et~al.
\newblock Language model cascades.
\newblock {\em arXiv preprint arXiv:2207.10342}, 2022.

\bibitem{nye2021scratchpad}
Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk~Witold Michalewski, Jacob Austin, David Bieber, David~Martin Dohan, Aitor Lewkowycz, Maarten~Paul Bosma, David Luan, Charles Sutton, and Augustus Odena.
\newblock Show your work: Scratchpads for intermediate computation with language models, 2021.
\newblock https://arxiv.org/abs/2112.00114.

\bibitem{wei2022cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824--24837, 2022.

\bibitem{creswell2022selection}
Antonia Creswell, Murray Shanahan, and Irina Higgins.
\newblock Selection-inference: Exploiting large language models for interpretable logical reasoning.
\newblock {\em arXiv preprint arXiv:2205.09712}, 2022.

\bibitem{hoffman2024training}
Matthew~Douglas Hoffman, Du~Phan, David Dohan, Sholto Douglas, Tuan~Anh Le, Aaron Parisi, Pavel Sountsov, Charles Sutton, Sharad Vikram, and Rif A~Saurous.
\newblock Training chain-of-thought via latent-variable inference.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{lew2023smc}
Alexander~K Lew, Tan Zhi-Xuan, Gabriel Grand, and Vikash~K Mansinghka.
\newblock Sequential monte carlo steering of large language models using probabilistic programs.
\newblock {\em arXiv preprint arXiv:2306.03081}, 2023.

\bibitem{zhao2024twistedsmc}
Stephen Zhao, Rob Brekelmans, Alireza Makhzani, and Roger Grosse.
\newblock Probabilistic inference in language models via twisted sequential monte carlo.
\newblock {\em arXiv preprint arXiv:2404.17546}, 2024.

\bibitem{zhao2023lampp}
Zirui Zhao, Wee~Sun Lee, and David Hsu.
\newblock Large language models as commonsense knowledge for large-scale task planning.
\newblock {\em arXiv preprint arXiv:2305.14078}, 2023.

\bibitem{whatmakesussmart}
Elizabeth Spelke.
\newblock {\em What Makes Us Smart? Core Knowledge and Natural Language}, pages 277--312.
\newblock 03 2003.

\bibitem{shapere1964structure}
Thomas~Samuel Kuhn.
\newblock The structure of scientific revolutions.
\newblock 1962.

\end{thebibliography}
