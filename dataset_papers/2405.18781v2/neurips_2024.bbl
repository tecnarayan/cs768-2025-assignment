\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ait-Saada and Nadif(2023)]{AitSaada2023IsAT}
Mira Ait-Saada and Mohamed Nadif.
\newblock Is anisotropy truly harmful? a case study on text clustering.
\newblock In \emph{ACL}, 2023.

\bibitem[Alman and Song(2023)]{Alman2023FastAR}
Josh Alman and Zhao Song.
\newblock Fast attention requires bounded entries.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{Ba2016LayerN}
Jimmy Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization.
\newblock \emph{ArXiv}, abs/1607.06450, 2016.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{Beltagy2020LongformerTL}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{ArXiv}, abs/2004.05150, 2020.

\bibitem[Brody et~al.(2023)Brody, Alon, and Yahav]{Brody2023OnTE}
Shaked Brody, Uri Alon, and Eran Yahav.
\newblock On the expressivity role of layernorm in transformers' attention.
\newblock In \emph{ACL}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{Brown2020LanguageMA}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Bulatov et~al.(2022)Bulatov, Kuratov, and Burtsev]{Bulatov2022RecurrentMT}
Aydar Bulatov, Yuri Kuratov, and Mikhail~S. Burtsev.
\newblock Recurrent memory transformer.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Bulatov et~al.(2023)Bulatov, Kuratov, and Burtsev]{Bulatov2023ScalingTT}
Aydar Bulatov, Yuri Kuratov, and Mikhail~S. Burtsev.
\newblock Scaling transformer to 1m tokens and beyond with rmt.
\newblock \emph{ArXiv}, abs/2304.11062, 2023.

\bibitem[Cai et~al.(2021)Cai, Huang, Bian, and Church]{Cai2021IsotropyIT}
Xingyu Cai, Jiaji Huang, Yu-Lan Bian, and Kenneth~Ward Church.
\newblock Isotropy in the contextual embedding space: Clusters and manifolds.
\newblock In \emph{ICLR}, 2021.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{Child2019GeneratingLS}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{ArXiv}, abs/1904.10509, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin2019BERTPO}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{ACL}, 2019.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{Dong2021AttentionIN}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly exponentially with depth.
\newblock In \emph{ICML}, 2021.

\bibitem[Ethayarajh(2019)]{Ethayarajh2019HowCA}
Kawin Ethayarajh.
\newblock How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Gao et~al.(2019)Gao, He, Tan, Qin, Wang, and Liu]{Gao2019RepresentationDP}
Jun Gao, Di~He, Xu~Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu.
\newblock Representation degeneration problem in training natural language generation models.
\newblock In \emph{ICLR}, 2019.

\bibitem[Geshkovski et~al.(2023{\natexlab{a}})Geshkovski, Letrouit, Polyanskiy, and Rigollet]{Geshkovski2023AMP}
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet.
\newblock A mathematical perspective on transformers.
\newblock \emph{ArXiv}, abs/2312.10794, 2023{\natexlab{a}}.

\bibitem[Geshkovski et~al.(2023{\natexlab{b}})Geshkovski, Letrouit, Polyanskiy, and Rigollet]{Geshkovski2023TheEO}
Borjan Geshkovski, Cyril Letrouit, Yury Polyanskiy, and Philippe Rigollet.
\newblock The emergence of clusters in self-attention dynamics.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Godey et~al.(2024)Godey, de~la Clergerie, and Sagot]{Godey2024AnisotropyII}
Nathan Godey, Eric~Villemonte de~la Clergerie, and Benoit Sagot.
\newblock Anisotropy is inherent to self-attention in transformers.
\newblock \emph{ArXiv}, abs/2401.12143, 2024.

\bibitem[Hartfiel(2002)]{Hartfiel2002NonhomogeneousMP}
Darald~J. Hartfiel.
\newblock \emph{Nonhomogeneous Matrix Products}.
\newblock 2002.

\bibitem[Hassani et~al.(2023)Hassani, Walton, Li, Li, and Shi]{CV3}
Ali Hassani, Steven Walton, Jiacheng Li, Shengjia Li, and Humphrey Shi.
\newblock Neighborhood attention transformer.
\newblock In \emph{CVPR}, 2023.

\bibitem[He et~al.(2023)He, Martens, Zhang, Botev, Brock, Smith, and Teh]{He2023DeepTW}
Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andy Brock, Samuel~L. Smith, and Yee~Whye Teh.
\newblock Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation.
\newblock In \emph{ICLR}, 2023.

\bibitem[Hwang et~al.(2024)Hwang, Wang, Huo, Sim, and Mengibar]{hwang2024transformerfam}
Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe~Chai Sim, and Pedro~Moreno Mengibar.
\newblock Transformerfam: Feedback attention is working memory.
\newblock \emph{ArXiv}, abs/2404.09173, 2024.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{Jiang2023Mistral7}
Albert~Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~Las~Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timoth{\'e}e Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{ArXiv}, 2023.

\bibitem[Joudaki et~al.(2023)Joudaki, Daneshmand, and Bach]{Joudaki2023OnTI}
Amir Joudaki, Hadi Daneshmand, and Francis~R. Bach.
\newblock On the impact of activation and normalization in obtaining isometric embeddings at initialization.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and Soricut]{Lan2019ALBERTAL}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language representations.
\newblock In \emph{ICLR}, 2020.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{CV1}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{ICCV}, 2021.

\bibitem[Mickus et~al.(2024)Mickus, Gr{\"o}nroos, and Attieh]{Mickus2024IsotropyCA}
Timothee Mickus, Stig-Arne Gr{\"o}nroos, and Joseph Attieh.
\newblock Isotropy, clusters, and classifiers.
\newblock \emph{ArXiv}, abs/2402.03191, 2024.

\bibitem[Noci et~al.(2022)Noci, Anagnostidis, Biggio, Orvieto, Singh, and Lucchi]{Noci2022SignalPI}
Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak~Pal Singh, and Aur{\'e}lien Lucchi.
\newblock Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Pan et~al.(2023)Pan, Ye, Xia, Song, and Huang]{CV2}
Xuran Pan, Tianzhu Ye, Zhuofan Xia, Shiji Song, and Gao Huang.
\newblock Slide-transformer: Hierarchical vision transformer with local self-attention.
\newblock In \emph{CVPR}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"o}pf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{Paszke2019PyTorchAI}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K{\"o}pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{Roy2020EfficientCS}
Aurko Roy, Mohammad~Taghi Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 2020.

\bibitem[Shi et~al.(2022)Shi, Gao, Xu, Liang, Li, Kong, Lee, and Kwok]{Shi2022RevisitingOI}
Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M.~S. Lee, and James Tin-Yau Kwok.
\newblock Revisiting over-smoothing in bert from the perspective of graph.
\newblock In \emph{ICLR}, 2022.

\bibitem[Tian et~al.(2023)Tian, Wang, Chen, and Du]{Tian2023ScanAS}
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon~Shaolei Du.
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Vaswani2017AttentionIA}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{Wolf2020TransformersSN}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Wu et~al.(2023)Wu, Ajorlou, Wu, and Jadbabaie]{Wu2023Demystify}
Xinyi Wu, Amir Ajorlou, Zihui Wu, and Ali Jadbabaie.
\newblock Demystifying oversmoothing in attention-based graph neural networks.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Xiao et~al.(2024)Xiao, Tian, Chen, Han, and Lewis]{Xiao2023EfficientSL}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock In \emph{ICLR}, 2024.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{Xiong2020OnLN}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{ICML}, 2020.

\bibitem[Yun et~al.(2020{\natexlab{a}})Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{Yun2019AreTU}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J. Reddi, and Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock In \emph{ICLR}, 2020{\natexlab{a}}.

\bibitem[Yun et~al.(2020{\natexlab{b}})Yun, Chang, Bhojanapalli, Rawat, Reddi, and Kumar]{Yun2020OnCA}
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J. Reddi, and Sanjiv Kumar.
\newblock O(n) connections are expressive enough: Universal approximability of sparse transformers.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Onta{\~n}{\'o}n, Pham, Ravula, Wang, Yang, and Ahmed]{Zaheer2020BigBT}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta{\~n}{\'o}n, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang, and Amr Ahmed.
\newblock Big bird: Transformers for longer sequences.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Zhang and Sennrich(2019)]{Zhang2019RootMS}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock In \emph{NeurIPS}, 2019.

\end{thebibliography}
