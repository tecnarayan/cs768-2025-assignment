\begin{thebibliography}{10}

\bibitem{gpt32020brown}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em NeurIPS}, volume~33, pages 1877--1901, 2020.

\bibitem{bloom2022}
Bloom: A 176b-parameter open-access multilingual language model.
\newblock \url{https://huggingface.co/bigscience/bloom}.

\bibitem{petroni2019language}
Fabio Petroni, Tim Rockt{\"a}schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
  Alexander~H Miller, and Sebastian Riedel.
\newblock Language models as knowledge bases?
\newblock {\em arXiv preprint arXiv:1909.01066}, 2019.

\bibitem{order2021lu}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock In {\em ACL}, 2021.

\bibitem{nie2022improving}
Feng Nie, Meixi Chen, Zhirui Zhang, and Xu~Cheng.
\newblock Improving few-shot performance of language models via nearest
  neighbor calibration.
\newblock {\em arXiv preprint arXiv:2212.02216}, 2022.

\bibitem{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 55(9):1--35, 2023.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation, 2021.

\bibitem{liu2021p}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang.
\newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning
  universally across scales and tasks.
\newblock {\em arXiv preprint arXiv:2110.07602}, 2021.

\bibitem{hambardzumyan2021warp}
Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May.
\newblock Warp: Word-level adversarial reprogramming.
\newblock {\em arXiv preprint arXiv:2101.00121}, 2021.

\bibitem{qin2021learning}
Guanghui Qin and Jason Eisner.
\newblock Learning how to ask: Querying lms with mixtures of soft prompts.
\newblock {\em arXiv preprint arXiv:2104.06599}, 2021.

\bibitem{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
  Jie Tang.
\newblock Gpt understands, too.
\newblock {\em arXiv preprint arXiv:2103.10385}, 2021.

\bibitem{zhang2022automatic}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock {\em arXiv preprint arXiv:2210.03493}, 2022.

\bibitem{gentile2022fast}
Claudio Gentile, Zhilei Wang, and Tong Zhang.
\newblock Fast rates in pool-based batch active learning.
\newblock {\em arXiv preprint arXiv:2202.05448}, 2022.

\bibitem{diao2023active}
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang.
\newblock Active prompting with chain-of-thought for large language models.
\newblock {\em arXiv preprint arXiv:2302.12246}, 2023.

\bibitem{liu2021makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu
  Chen.
\newblock What makes good in-context examples for gpt-$3 $?
\newblock {\em arXiv preprint arXiv:2101.06804}, 2021.

\bibitem{shi2023large}
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed~Chi,
  Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Large language models can be easily distracted by irrelevant context.
\newblock {\em arXiv preprint arXiv:2302.00093}, 2023.

\bibitem{chatgpt}
\url{https://openai.com/blog/chatgpt}.

\bibitem{calibrate2021zhao}
Tony~Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In {\em ICML}, 2021.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{gpt22018}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em Technical Report}, 2018.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock {\em arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{gptnexo2022}
\url{https://huggingface.co/EleutherAI/gpt-neox-20b}.

\end{thebibliography}
