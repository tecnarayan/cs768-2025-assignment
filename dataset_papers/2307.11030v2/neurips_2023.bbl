\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2020)]{allen2020towards}
Z.~Allen-Zhu and Y.~Li.
\newblock Towards understanding ensemble, knowledge distillation and
  self-distillation in deep learning.
\newblock \emph{arXiv preprint arXiv:2012.09816}, 2020.

\bibitem[Ba and Caruana(2014)]{ba2014deep}
J.~Ba and R.~Caruana.
\newblock Do deep nets really need to be deep?
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Bachman et~al.(2014)Bachman, Alsharif, and
  Precup]{bachman2014learning}
P.~Bachman, O.~Alsharif, and D.~Precup.
\newblock Learning with pseudo-ensembles.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Bartlett and Mendelson(2003)]{bartlett2003}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{J. Mach. Learn. Res.}, 3\penalty0 (null):\penalty0 463–482,
  Mar. 2003.
\newblock ISSN 1532-4435.

\bibitem[Belkin and Niyogi(2004)]{belkin2004semi}
M.~Belkin and P.~Niyogi.
\newblock Semi-supervised learning on {Riemannian} manifolds.
\newblock \emph{Machine learning}, 56:\penalty0 209--239, 2004.

\bibitem[Belkin et~al.(2004)Belkin, Matveeva, and
  Niyogi]{belkin2004regularization}
M.~Belkin, I.~Matveeva, and P.~Niyogi.
\newblock Regularization and semi-supervised learning on large graphs.
\newblock In \emph{Learning Theory: 17th Annual Conference on Learning Theory,
  COLT 2004, Banff, Canada, July 1-4, 2004. Proceedings 17}, pages 624--638.
  Springer, 2004.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~A.
  Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Bertozzi and Merkurjev(2019)]{bertozzi2019graph}
A.~L. Bertozzi and E.~Merkurjev.
\newblock Graph-based optimization approaches for machine learning, uncertainty
  quantification and networks.
\newblock In \emph{Handbook of Numerical Analysis}, volume~20, pages 503--531.
  Elsevier, 2019.

\bibitem[Bilmes(2022)]{bilmes-submodularity-in-ai-ml-2022}
J.~Bilmes.
\newblock {Submodularity In Machine Learning and Artificial Intelligence}.
\newblock \emph{Arxiv}, abs/2202.00132, Jan 2022.

\bibitem[Cai et~al.(2021)Cai, Gao, Lee, and Lei]{cai2021theory}
T.~Cai, R.~Gao, J.~Lee, and Q.~Lei.
\newblock A theory of label propagation for subpopulation shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  1170--1182. PMLR, 2021.

\bibitem[Calder et~al.(2020)Calder, Cook, Thorpe, and
  Slep{\v{c}ev}]{calder_poisson_2020}
J.~Calder, B.~Cook, M.~Thorpe, and D.~Slep{\v{c}ev}.
\newblock Poisson learning: Graph-based semi-supervised learning at very low
  label rates.
\newblock In \emph{Proceedings of the 37th {International} {Conference} on
  {Machine} {Learning}}, pages 1306--1316. Proceedings of Machine Learning
  Research, Nov. 2020.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
M.~Caron, I.~Misra, J.~Mairal, P.~Goyal, P.~Bojanowski, and A.~Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock 2020.

\bibitem[Chen et~al.(2021)Chen, Li, Yan, Wang, and Zhang]{chen2021deep}
B.~Chen, P.~Li, Z.~Yan, B.~Wang, and L.~Zhang.
\newblock Deep metric learning with graph consistency.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 982--990, 2021.

\bibitem[Chen et~al.(2017)Chen, Choi, Yu, Han, and
  Chandraker]{chen2017learning}
G.~Chen, W.~Choi, X.~Yu, T.~Han, and M.~Chandraker.
\newblock Learning efficient object detection models with knowledge
  distillation.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chen et~al.(2020)Chen, Dobriban, and Lee]{chen2020group}
S.~Chen, E.~Dobriban, and J.~H. Lee.
\newblock A group-theoretic framework for data augmentation.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 9885--9955, 2020.

\bibitem[Chen and He(2021)]{chen2021exploring}
X.~Chen and K.~He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 15750--15758, 2021.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, pages 702--703, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[DeVries and Taylor(2017)]{devries2017improved}
T.~DeVries and G.~W. Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Eckart and Young(1936)]{eckart1936}
C.~Eckart and G.~Young.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, Sep 1936.
\newblock ISSN 1860-0980.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2018size}
N.~Golowich, A.~Rakhlin, and O.~Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Conference On Learning Theory}, pages 297--299. PMLR, 2018.

\bibitem[Golub and Van~Loan(2013)]{golub2013}
G.~H. Golub and C.~F. Van~Loan.
\newblock \emph{Matrix computations}.
\newblock JHU press, Baltimore, MD, USA, 2013.

\bibitem[Gou et~al.(2021)Gou, Yu, Maybank, and Tao]{gou2021knowledge}
J.~Gou, B.~Yu, S.~J. Maybank, and D.~Tao.
\newblock Knowledge distillation: A survey.
\newblock \emph{International Journal of Computer Vision}, 129:\penalty0
  1789--1819, 2021.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
J.-B. Grill, F.~Strub, F.~Altch{\'e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Avila~Pires, Z.~Guo, M.~Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21271--21284, 2020.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
J.~Z. HaoChen, C.~Wei, A.~Gaidon, and T.~Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5000--5011, 2021.

\bibitem[Harutyunyan et~al.(2023)Harutyunyan, Rawat, Menon, Kim, and
  Kumar]{harutyunyan2023supervision}
H.~Harutyunyan, A.~S. Rawat, A.~K. Menon, S.~Kim, and S.~Kumar.
\newblock Supervision complexity and its role in knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2301.12245}, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hsu et~al.(2021)Hsu, Ji, Telgarsky, and Wang]{hsu2021generalization}
D.~Hsu, Z.~Ji, M.~Telgarsky, and L.~Wang.
\newblock Generalization bounds via distillation.
\newblock \emph{arXiv preprint arXiv:2104.05641}, 2021.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Ji and Zhu(2020)]{ji2020knowledge}
G.~Ji and Z.~Zhu.
\newblock Knowledge distillation in wide neural networks: Risk bound, data
  efficiency and imperfect teacher.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20823--20833, 2020.

\bibitem[Kim et~al.(2018)Kim, Park, and Kwak]{kim2018paraphrasing}
J.~Kim, S.~Park, and N.~Kwak.
\newblock Paraphrasing complex network: Network compression via factor
  transfer.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Krause and Golovin(2014)]{krause14survey}
A.~Krause and D.~Golovin.
\newblock Submodular function maximization.
\newblock In \emph{Tractability: Practical Approaches to Hard Problems}.
  Cambridge University Press, February 2014.

\bibitem[Krizhevsky and Hinton(2009)]{cifar}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2017imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Communications of the ACM}, 60\penalty0 (6):\penalty0 84--90,
  2017.

\bibitem[Laine and Aila(2016)]{laine2016temporal}
S.~Laine and T.~Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem[Ledoux and Talagrand(2013)]{ledoux2013}
M.~Ledoux and M.~Talagrand.
\newblock \emph{Probability in Banach Spaces: isoperimetry and processes}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{lee2021predicting}
J.~D. Lee, Q.~Lei, N.~Saunshi, and J.~Zhuo.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 309--323, 2021.

\bibitem[Liu et~al.(2019)Liu, Cao, Li, Yuan, Hu, Li, and
  Duan]{liu2019knowledge}
Y.~Liu, J.~Cao, B.~Li, C.~Yuan, W.~Hu, Y.~Li, and Y.~Duan.
\newblock Knowledge distillation via instance relationship graph.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7096--7104, 2019.

\bibitem[Liu et~al.(2021)Liu, Zhang, Wang, and Wang]{liu2021data}
Y.~Liu, W.~Zhang, J.~Wang, and J.~Wang.
\newblock Data-free knowledge transfer: A survey.
\newblock \emph{arXiv preprint arXiv:2112.15278}, 2021.

\bibitem[Louis and Makarychev(2014)]{louis2014approximation}
A.~Louis and K.~Makarychev.
\newblock Approximation algorithm for sparsest k-partitioning.
\newblock In \emph{Proceedings of the twenty-fifth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 1244--1255. SIAM, 2014.

\bibitem[Ma et~al.(2022)Ma, Chen, and Akata]{ma2022distilling}
Y.~Ma, Y.~Chen, and Z.~Akata.
\newblock Distilling knowledge from self-supervised teacher by embedding graph
  alignment.
\newblock \emph{arXiv preprint arXiv:2211.13264}, 2022.

\bibitem[Maurer(2016)]{maurer2016vector}
A.~Maurer.
\newblock A vector-contraction inequality for rademacher complexities.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 3--17. Springer, 2016.

\bibitem[Mei et~al.(2021)Mei, Misiakiewicz, and Montanari]{mei2021learning}
S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Learning with invariances in random features and kernel models.
\newblock In \emph{Conference on Learning Theory}, pages 3351--3418. PMLR,
  2021.

\bibitem[Mirzasoleiman(2017)]{mirzasoleiman2015dissertation}
B.~Mirzasoleiman.
\newblock \emph{Big Data Summarization Using Submodular Functions}.
\newblock PhD thesis, ETH Zurich, 2017.

\bibitem[Mirzasoleiman et~al.(2015)Mirzasoleiman, Badanidiyuru, Karbasi,
  Vondr{\'a}k, and Krause]{mirzasoleiman2015lazier}
B.~Mirzasoleiman, A.~Badanidiyuru, A.~Karbasi, J.~Vondr{\'a}k, and A.~Krause.
\newblock Lazier than lazy greedy.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~29, 2015.

\bibitem[Park et~al.(2019)Park, Kim, Lu, and Cho]{park2019relational}
W.~Park, D.~Kim, Y.~Lu, and M.~Cho.
\newblock Relational knowledge distillation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 3967--3976, 2019.

\bibitem[Parulekar et~al.(2023)Parulekar, Collins, Shanmugam, Mokhtari, and
  Shakkottai]{parulekar2023infonce}
A.~Parulekar, L.~Collins, K.~Shanmugam, A.~Mokhtari, and S.~Shakkottai.
\newblock Infonce loss provably learns cluster-preserving representations.
\newblock \emph{arXiv preprint arXiv:2302.07920}, 2023.

\bibitem[Phan(2021)]{huy_phan_2021_4431043}
H.~Phan.
\newblock huyvnphan/pytorch\_cifar10, jan 2021.

\bibitem[Phuong and Lampert(2019)]{phuong2019towards}
M.~Phuong and C.~Lampert.
\newblock Towards understanding knowledge distillation.
\newblock In \emph{International Conference on Machine Learning}, pages
  5142--5151. PMLR, 2019.

\bibitem[Qian et~al.(2022)Qian, Li, and Hu]{qian2022improved}
Q.~Qian, H.~Li, and J.~Hu.
\newblock Improved knowledge distillation via full kernel matrix transfer.
\newblock In \emph{Proceedings of the 2022 SIAM International Conference on
  Data Mining (SDM)}, pages 612--620. SIAM, 2022.

\bibitem[Romero et~al.(2014)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock \emph{arXiv preprint arXiv:1412.6550}, 2014.

\bibitem[Schreiber et~al.(2020)Schreiber, Bilmes, and
  Noble]{schreiber2020apricot}
J.~Schreiber, J.~Bilmes, and W.~S. Noble.
\newblock apricot: Submodular selection for data summarization in python.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (161):\penalty0 1--6, 2020.

\bibitem[Shen et~al.(2022)Shen, Bubeck, and Gunasekar]{shen2022data}
R.~Shen, S.~Bubeck, and S.~Gunasekar.
\newblock Data augmentation as feature manipulation.
\newblock In \emph{International Conference on Machine Learning}, pages
  19773--19808. PMLR, 2022.

\bibitem[Simard et~al.(2002)Simard, LeCun, Denker, and
  Victorri]{simard2002transformation}
P.~Y. Simard, Y.~A. LeCun, J.~S. Denker, and B.~Victorri.
\newblock Transformation invariance in pattern recognition—tangent distance
  and tangent propagation.
\newblock In \emph{Neural networks: tricks of the trade}, pages 239--274.
  Springer, 2002.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel,
  Cubuk, Kurakin, and Li]{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, N.~Carlini, Z.~Zhang, H.~Zhang, C.~A. Raffel, E.~D.
  Cubuk, A.~Kurakin, and C.-L. Li.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 596--608, 2020.

\bibitem[von Luxburg(2007)]{von_luxburg_tutorial_2007}
U.~von Luxburg.
\newblock A tutorial on spectral clustering.
\newblock \emph{Statistics and Computing}, 17\penalty0 (4):\penalty0 395--416,
  Dec. 2007.
\newblock ISSN 1573-1375.

\bibitem[Wainwright(2019)]{wainwright2019}
M.~J. Wainwright.
\newblock \emph{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2019.

\bibitem[Wang et~al.(2022)Wang, Lohit, Jones, and Fu]{wangmakes}
H.~Wang, S.~Lohit, M.~J. Jones, and Y.~Fu.
\newblock What makes a" good" data augmentation in knowledge distillation-a
  statistical perspective.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Wang and Yoon(2021)]{wang2021knowledge}
L.~Wang and K.-J. Yoon.
\newblock Knowledge distillation and student-teacher learning for visual
  intelligence: A review and new outlooks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Wei and Ma(2019)]{wei2019data}
C.~Wei and T.~Ma.
\newblock Data-dependent sample complexity of deep neural networks via
  lipschitz augmentation.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Wei et~al.(2021)Wei, Shen, Chen, and Ma]{wei2021theoretical}
C.~Wei, K.~Shen, Y.~Chen, and T.~Ma.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Welling and Kipf(2016)]{welling2016semi}
M.~Welling and T.~N. Kipf.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{J. International Conference on Learning Representations
  (ICLR 2017)}, 2016.

\bibitem[Yang et~al.(2023)Yang, Dong, Ward, Dhillon, Sanghavi, and
  Lei]{yang2023sample}
S.~Yang, Y.~Dong, R.~Ward, I.~S. Dhillon, S.~Sanghavi, and Q.~Lei.
\newblock Sample efficiency of data augmentation consistency regularization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3825--3853. PMLR, 2023.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\bibitem[Zagoruyko and Komodakis(2016{\natexlab{a}})]{zagoruyko2016paying}
S.~Zagoruyko and N.~Komodakis.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock \emph{arXiv preprint arXiv:1612.03928}, 2016{\natexlab{a}}.

\bibitem[Zagoruyko and Komodakis(2016{\natexlab{b}})]{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016{\natexlab{b}}.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhou et~al.(2003)Zhou, Bousquet, Lal, Weston, and
  Sch\"{o}lkopf]{zhou2003llgc}
D.~Zhou, O.~Bousquet, T.~Lal, J.~Weston, and B.~Sch\"{o}lkopf.
\newblock Learning with local and global consistency.
\newblock In S.~Thrun, L.~Saul, and B.~Sch\"{o}lkopf, editors, \emph{Advances
  in Neural Information Processing Systems}, volume~16. MIT Press, 2003.

\bibitem[Zhou et~al.(2020)Zhou, Cui, Hu, Zhang, Yang, Liu, Wang, Li, and
  Sun]{zhou2018graph}
J.~Zhou, G.~Cui, S.~Hu, Z.~Zhang, C.~Yang, Z.~Liu, L.~Wang, C.~Li, and M.~Sun.
\newblock Graph neural networks: {A} review of methods and applications.
\newblock \emph{AI Open}, 1:\penalty0 57--81, 2020.
\newblock ISSN 2666-6510.

\bibitem[Zhu et~al.(2003)Zhu, Ghahramani, and
  Lafferty]{zhu_semi-supervised_2003}
X.~Zhu, Z.~Ghahramani, and J.~Lafferty.
\newblock Semi-supervised learning using {Gaussian} fields and harmonic
  functions.
\newblock In \emph{Proceedings of the 20th {International} {Conference} on
  {International} {Conference} on {Machine} {Learning}}, pages 912--919,
  Washington, DC, USA, Aug. 2003. AAAI Press.
\newblock ISBN 978-1-57735-189-4.

\end{thebibliography}
