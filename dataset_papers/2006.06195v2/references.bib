@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{tan2019lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={NeurIPS},
  year={2019}
}

@article{chen2019uniter,
  title={Uniter: Learning universal image-text representations},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:1909.11740},
  year={2019}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{goyal2017making,
  title={Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{yu2016modeling,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={ECCV},
  year={2016}
}

@article{zhu2019freelb,
  title={Freelb: Enhanced adversarial training for language understanding},
  author={Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Thomas and Liu, Jingjing},
  journal={arXiv preprint arXiv:1909.11764},
  year={2019}
}

@article{jiang2019smart,
  title={SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization},
  author={Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},
  journal={arXiv preprint arXiv:1911.03437},
  year={2019}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{miyato2016adversarial,
  title={Adversarial training methods for semi-supervised text classification},
  author={Miyato, Takeru and Dai, Andrew M and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1605.07725},
  year={2016}
}

@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={CVPR},
  year={2018}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1706.06083},
  year={2017}
}

@inproceedings{xie2019feature,
  title={Feature denoising for improving adversarial robustness},
  author={Xie, Cihang and Wu, Yuxin and Maaten, Laurens van der and Yuille, Alan L and He, Kaiming},
  booktitle={CVPR},
  year={2019}
}

@article{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric P and Ghaoui, Laurent El and Jordan, Michael I},
  journal={arXiv preprint arXiv:1901.08573},
  year={2019}
}

@inproceedings{shafahi2019adversarial,
  title={Adversarial training for free!},
  author={Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{zhang2019you,
  title={You only propagate once: Accelerating adversarial training via maximal principle},
  author={Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
  booktitle={NeurIPS},
  year={2019}
}

@article{suhr2018corpus,
  title={A corpus for reasoning about natural language grounded in photographs},
  author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
  journal={arXiv preprint arXiv:1811.00491},
  year={2018}
}

@article{xie2019visual,
  title={Visual entailment: A novel task for fine-grained image understanding},
  author={Xie, Ning and Lai, Farley and Doran, Derek and Kadav, Asim},
  journal={arXiv preprint arXiv:1901.06706},
  year={2019}
}

@inproceedings{lee2018stacked,
  title={Stacked cross attention for image-text matching},
  author={Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
  booktitle={ECCV},
  year={2018}
}

@article{alberti2019fusion,
  title={Fusion of detected objects in text for visual question answering},
  author={Alberti, Chris and Ling, Jeffrey and Collins, Michael and Reitter, David},
  journal={arXiv preprint arXiv:1908.05054},
  year={2019}
}

@article{li2019unicoder,
  title={Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training},
  author={Li, Gen and Duan, Nan and Fang, Yuejian and Jiang, Daxin and Zhou, Ming},
  journal={arXiv preprint arXiv:1908.06066},
  year={2019}
}

@article{su2019vl,
  title={VL-BERT: Pre-training of Generic Visual-Linguistic Representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  journal={arXiv preprint arXiv:1908.08530},
  year={2019}
}

@article{li2019visualbert,
  title={Visualbert: A simple and performant baseline for vision and language},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1908.03557},
  year={2019}
}

@article{lu201912,
  title={12-in-1: Multi-Task Vision and Language Representation Learning},
  author={Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
  journal={arXiv preprint arXiv:1912.02315},
  year={2019}
}

@article{li2020oscar,
  title={Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Hu, Xiaowei and Zhang, Pengchuan and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  journal={arXiv preprint arXiv:2004.06165},
  year={2020}
}

@article{huang2020pixel,
  title={Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  journal={arXiv preprint arXiv:2004.00849},
  year={2020}
}

@article{zhou2019unified,
  title={Unified vision-language pre-training for image captioning and vqa},
  author={Zhou, Luowei and Palangi, Hamid and Zhang, Lei and Hu, Houdong and Corso, Jason J and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1909.11059},
  year={2019}
}

@article{xia2020xgpt,
  title={XGPT: Cross-modal Generative Pre-Training for Image Captioning},
  author={Xia, Qiaolin and Huang, Haoyang and Duan, Nan and Zhang, Dongdong and Ji, Lei and Sui, Zhifang and Cui, Edward and Bharti, Taroon and Zhou, Ming},
  journal={arXiv preprint arXiv:2003.01473},
  year={2020}
}

@article{murahari2019large,
  title={Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline},
  author={Murahari, Vishvak and Batra, Dhruv and Parikh, Devi and Das, Abhishek},
  journal={arXiv preprint arXiv:1912.02379},
  year={2019}
}

@article{wang2020vd,
  title={VD-BERT: A Unified Vision and Dialog Transformer with BERT},
  author={Wang, Yue and Joty, Shafiq and Lyu, Michael R and King, Irwin and Xiong, Caiming and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2004.13278},
  year={2020}
}

@article{hao2020towards,
  title={Towards learning a generic agent for vision-and-language navigation via pre-training},
  author={Hao, Weituo and Li, Chunyuan and Li, Xiujun and Carin, Lawrence and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2002.10638},
  year={2020}
}

@article{majumdar2020improving,
  title={Improving Vision-and-Language Navigation with Image-Text Pairs from the Web},
  author={Majumdar, Arjun and Shrivastava, Ayush and Lee, Stefan and Anderson, Peter and Parikh, Devi and Batra, Dhruv},
  journal={arXiv preprint arXiv:2004.14973},
  year={2020}
}

@inproceedings{sun2019videobert,
  title={Videobert: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={ICCV},
  year={2019}
}

@article{sun2019contrastive,
  title={Contrastive bidirectional transformer for temporal representation learning},
  author={Sun, Chen and Baradel, Fabien and Murphy, Kevin and Schmid, Cordelia},
  journal={arXiv preprint arXiv:1906.05743},
  year={2019}
}

@article{luo2020univilm,
  title={UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation},
  author={Luo, Huaishao and Ji, Lei and Shi, Botian and Huang, Haoyang and Duan, Nan and Li, Tianrui and Chen, Xilin and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.06353},
  year={2020}
}

@article{li2020hero,
  title={HERO: Hierarchical Encoder for Video+ Language Omni-representation Pre-training},
  author={Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
  journal={arXiv preprint arXiv:2005.00200},
  year={2020}
}

@article{athalye2018obfuscated,
  title={Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  journal={arXiv preprint arXiv:1802.00420},
  year={2018}
}

@article{tramer2017ensemble,
  title={Ensemble adversarial training: Attacks and defenses},
  author={Tram{\`e}r, Florian and Kurakin, Alexey and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv preprint arXiv:1705.07204},
  year={2017}
}

@inproceedings{mao2019metric,
  title={Metric learning for adversarial robustness},
  author={Mao, Chengzhi and Zhong, Ziyuan and Yang, Junfeng and Vondrick, Carl and Ray, Baishakhi},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{zhang2019defense,
  title={Defense against adversarial attacks using feature scattering-based adversarial training},
  author={Zhang, Haichao and Wang, Jianyu},
  booktitle={NeurIPS},
  year={2019}
}

@article{li2019improving,
  title={Improving the robustness of deep neural networks via adversarial training with triplet loss},
  author={Li, Pengcheng and Yi, Jinfeng and Zhou, Bowen and Zhang, Lijun},
  journal={arXiv preprint arXiv:1905.11713},
  year={2019}
}

@article{stanforth2019labels,
  title={Are labels required for improving adversarial robustness?},
  author={Stanforth, Robert and Fawzi, Alhussein and Kohli, Pushmeet and others},
  journal={arXiv preprint arXiv:1905.13725},
  year={2019}
}

@inproceedings{carmon2019unlabeled,
  title={Unlabeled data improves adversarial robustness},
  author={Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
  booktitle={NeurIPS},
  year={2019}
}

@article{wong2020fast,
  title={Fast is better than free: Revisiting adversarial training},
  author={Wong, Eric and Rice, Leslie and Kolter, J Zico},
  journal={arXiv preprint arXiv:2001.03994},
  year={2020}
}

@article{pang2020boosting,
  title={Boosting adversarial training with hypersphere embedding},
  author={Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Xu, Kun and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2002.08619},
  year={2020}
}

@article{xie2019adversarial,
  title={Adversarial Examples Improve Image Recognition},
  author={Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V},
  journal={arXiv preprint arXiv:1911.09665},
  year={2019}
}

@article{wang2019improving,
  title={Improving neural language modeling via adversarial training},
  author={Wang, Dilin and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:1906.03805},
  year={2019}
}

@article{hendrycks2019using,
  title={Using pre-training can improve model robustness and uncertainty},
  author={Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
  journal={arXiv preprint arXiv:1901.09960},
  year={2019}
}

@article{chen2020adversarial,
  title={Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning},
  author={Chen, Tianlong and Liu, Sijia and Chang, Shiyu and Cheng, Yu and Amini, Lisa and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2003.12862},
  year={2020}
}

@article{liu2020adversarial,
  title={Adversarial Training for Large Neural Language Models},
  author={Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2004.08994},
  year={2020}
}

@article{chen2017attacking,
  title={Attacking visual language grounding with adversarial examples: A case study on neural image captioning},
  author={Chen, Hongge and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1712.02051},
  year={2017}
}

@inproceedings{xu2019exact,
  title={Exact Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables},
  author={Xu, Yan and Wu, Baoyuan and Shen, Fumin and Fan, Yanbo and Zhang, Yong and Shen, Heng Tao and Liu, Wei},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{ribeiro2018semantically,
  title={Semantically equivalent adversarial rules for debugging nlp models},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={ACL},
  year={2018}
}

@inproceedings{ramakrishnan2018overcoming,
  title={Overcoming language priors in visual question answering with adversarial regularization},
  author={Ramakrishnan, Sainandan and Agrawal, Aishwarya and Lee, Stefan},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{yang2016stacked,
  title={Stacked attention networks for image question answering},
  author={Yang, Zichao and He, Xiaodong and Gao, Jianfeng and Deng, Li and Smola, Alex},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{lu2016hierarchical,
  title={Hierarchical question-image co-attention for visual question answering},
  author={Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
  booktitle={NeurIPS},
  year={2016}
}

@inproceedings{yu2019deep,
  title={Deep modular co-attention networks for visual question answering},
  author={Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  booktitle={CVPR},
  year={2019}
}

@article{peng2018dynamic,
  title={Dynamic fusion with intra-and inter-modality attention flow for visual question answering},
  author={Peng, Gao and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven and Wang, Xiaogang and Li, Hongsheng},
  journal={arXiv preprint arXiv:1812.05252},
  year={2018}
}

@article{fukui2016multimodal,
  title={Multimodal compact bilinear pooling for visual question answering and visual grounding},
  author={Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  journal={arXiv preprint arXiv:1606.01847},
  year={2016}
}

@inproceedings{yu2017multi,
  title={Multi-modal factorized bilinear pooling with co-attention learning for visual question answering},
  author={Yu, Zhou and Yu, Jun and Fan, Jianping and Tao, Dacheng},
  booktitle={ICCV},
  year={2017}
}

@article{kim2016hadamard,
  title={Hadamard product for low-rank bilinear pooling},
  author={Kim, Jin-Hwa and On, Kyoung-Woon and Lim, Woosang and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
  journal={arXiv preprint arXiv:1610.04325},
  year={2016}
}

@inproceedings{kim2018bilinear,
  title={Bilinear attention networks},
  author={Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
  booktitle={NeurIPS},
  year={2018}
}

@article{hudson2018compositional,
  title={Compositional attention networks for machine reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={arXiv preprint arXiv:1803.03067},
  year={2018}
}

@article{gan2019multi,
  title={Multi-step reasoning via recurrent dual attention for visual dialog},
  author={Gan, Zhe and Cheng, Yu and Kholy, Ahmed EI and Li, Linjie and Liu, Jingjing and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1902.00579},
  year={2019}
}

@inproceedings{cadene2019murel,
  title={Murel: Multimodal relational reasoning for visual question answering},
  author={Cadene, Remi and Ben-Younes, Hedi and Cord, Matthieu and Thome, Nicolas},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{santoro2017simple,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  booktitle={NeurIPS},
  year={2017}
}

@article{li2019relation,
  title={Relation-aware Graph Attention Network for Visual Question Answering},
  author={Li, Linjie and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  journal={arXiv preprint arXiv:1903.12314},
  year={2019}
}

@inproceedings{norcliffe2018learning,
  title={Learning conditioned graph structures for interpretable visual question answering},
  author={Norcliffe-Brown, Will and Vafeias, Stathis and Parisot, Sarah},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{andreas2016neural,
  title={Neural module networks},
  author={Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{johnson2017inferring,
  title={Inferring and executing programs for visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Hoffman, Judy and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={ICCV},
  year={2017}
}

@inproceedings{hu2017learning,
  title={Learning to reason: End-to-end module networks for visual question answering},
  author={Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  booktitle={ICCV},
  year={2017}
}

@article{chen2019meta,
  title={Meta Module Network for Compositional Visual Reasoning},
  author={Chen, Wenhu and Gan, Zhe and Li, Linjie and Cheng, Yu and Wang, William and Liu, Jingjing},
  journal={arXiv preprint arXiv:1910.03230},
  year={2019}
}

@article{miyato2018virtual,
  title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
  author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  journal={PAMI},
  year={2018}
}

@article{xie2019unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  year={2019}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={CVPR},
  year={2019}
}

@article{cao2020behind,
  title={Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models},
  author={Cao, Jize and Gan, Zhe and Cheng, Yu and Yu, Licheng and Chen, Yen-Chun and Liu, Jingjing},
  journal={arXiv preprint arXiv:2005.07310},
  year={2020}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  year={2014}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={IJCV},
  year={2017}
}

@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={ACL},
  year={2018}
}

@inproceedings{ordonez2011im2text,
  title={Im2text: Describing images using 1 million captioned photographs},
  author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara L},
  booktitle={NeurIPS},
  year={2011}
}

@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{shah2019cycle,
  title={Cycle-consistency for robust visual question answering},
  author={Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
  booktitle={CVPR},
  year={2019}
}