\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba
  Szepesvari, and Gell{\'e}rt Weisz.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  3692--3702. PMLR, 2019.

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Henaff, Kakade, and
  Sun]{agarwal2020pc}
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock \emph{arXiv preprint arXiv:2007.08459}, 2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 64--66. PMLR,
  2020{\natexlab{b}}.

\bibitem[Amir et~al.(2022)Amir, Azov, Koren, and Livni]{amir2022better}
Idan Amir, Guy Azov, Tomer Koren, and Roi Livni.
\newblock Better best of both worlds bounds for bandits with switching costs.
\newblock \emph{arXiv preprint arXiv:2206.03098}, 2022.

\bibitem[Auer and Chiang(2016)]{auer2016algorithm}
Peter Auer and Chao-Kai Chiang.
\newblock An algorithm with nearly optimal pseudo-regret for both stochastic
  and adversarial bandits.
\newblock In \emph{Conference on Learning Theory}, pages 116--120. PMLR, 2016.

\bibitem[Bubeck and Slivkins(2012)]{bubeck2012best}
S{\'e}bastien Bubeck and Aleksandrs Slivkins.
\newblock The best of both worlds: Stochastic and adversarial bandits.
\newblock In \emph{Conference on Learning Theory}, pages 42--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1283--1294. PMLR, 2020.

\bibitem[Chen et~al.(2021)Chen, Luo, and Wei]{chen2021impossible}
Liyu Chen, Haipeng Luo, and Chen-Yu Wei.
\newblock Impossible tuning made possible: A new expert algorithm and its
  applications.
\newblock In \emph{Conference on Learning Theory}, pages 1216--1259. PMLR,
  2021.

\bibitem[Chen et~al.(2022)Chen, Luo, and Rosenberg]{chen2022policy}
Liyu Chen, Haipeng Luo, and Aviv Rosenberg.
\newblock Policy optimization for stochastic shortest path.
\newblock \emph{arXiv preprint arXiv:2202.03334}, 2022.

\bibitem[Erez and Koren(2021)]{erez2021best}
Liad Erez and Tomer Koren.
\newblock Best-of-all-worlds bounds for online learning with feedback graphs.
\newblock \emph{arXiv preprint arXiv:2107.09572}, 2021.

\bibitem[He et~al.(2022)He, Zhou, and Gu]{he2022near}
Jiafan He, Dongruo Zhou, and Quanquan Gu.
\newblock Near-optimal policy optimization algorithms for learning adversarial
  linear mixture mdps.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4259--4280. PMLR, 2022.

\bibitem[Ito(2021)]{ito2021parameter}
Shinji Ito.
\newblock Parameter-free multi-armed bandit algorithms with hybrid
  data-dependent regret bounds.
\newblock In \emph{Conference on Learning Theory}, pages 2552--2583. PMLR,
  2021.

\bibitem[Ito et~al.(2022)Ito, Tsuchiya, and Honda]{ito2022nearly}
Shinji Ito, Taira Tsuchiya, and Junya Honda.
\newblock Nearly optimal best-of-both-worlds algorithms for online learning
  with feedback graphs.
\newblock \emph{arXiv preprint arXiv:2206.00873}, 2022.

\bibitem[Jin et~al.(2020)Jin, Jin, Luo, Sra, and Yu]{jin2019learning}
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Jin and Luo(2020)]{jin2020simultaneously}
Tiancheng Jin and Haipeng Luo.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 16557--16566, 2020.

\bibitem[Jin et~al.(2021)Jin, Huang, and Luo]{jin2021best}
Tiancheng Jin, Longbo Huang, and Haipeng Luo.
\newblock The best of both worlds: stochastic and adversarial episodic mdps
  with unknown transition.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20491--20502, 2021.

\bibitem[Lai et~al.(1985)Lai, Robbins, et~al.]{lai1985asymptotically}
Tze~Leung Lai, Herbert Robbins, et~al.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in applied mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Lattimore and Szepesv{\'a}ri(2018)]{lattimore2018bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press (preprint), 2018.

\bibitem[Lee et~al.(2020)Lee, Luo, Wei, and Zhang]{lee2020bias}
Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang.
\newblock Bias no more: high-probability data-dependent regret bounds for
  adversarial bandits and mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Levine and Koltun(2013)]{levine2013guided}
Sergey Levine and Vladlen Koltun.
\newblock Guided policy search.
\newblock In \emph{International conference on machine learning}, pages 1--9.
  PMLR, 2013.

\bibitem[Luo(2022)]{luo2022homework3}
Haipeng Luo.
\newblock Homework 3 solution, introduction to online optimization/learning.
\newblock
  \url{http://haipeng-luo.net/courses/CSCI659/2022_fall/homework/HW3_solutions.pdf},
  November 2022.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Haipeng Luo, Chen-Yu Wei, and Chung-Wei Lee.
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22931--22942, 2021.

\bibitem[Neu and Olkhovskaya(2021)]{neu2021online}
Gergely Neu and Julia Olkhovskaya.
\newblock Online learning in mdps with linear function approximation and bandit
  feedback.
\newblock \emph{arXiv preprint arXiv:2007.01612v2}, 2021.

\bibitem[Rouyer et~al.(2021)Rouyer, Seldin, and
  Cesa-Bianchi]{rouyer2021algorithm}
Chlo{\'e} Rouyer, Yevgeny Seldin, and Nicol{\`o} Cesa-Bianchi.
\newblock An algorithm for stochastic and adversarial bandits with switching
  costs.
\newblock In \emph{International Conference on Machine Learning}, pages
  9127--9135. PMLR, 2021.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Seldin and Lugosi(2017)]{seldin2017improved}
Yevgeny Seldin and G{\'a}bor Lugosi.
\newblock An improved parametrization and analysis of the exp3++ algorithm for
  stochastic and adversarial bandits.
\newblock In \emph{Conference on Learning Theory}, pages 1743--1759. PMLR,
  2017.

\bibitem[Seldin and Slivkins(2014)]{seldin2014one}
Yevgeny Seldin and Aleksandrs Slivkins.
\newblock One practical algorithm for both stochastic and adversarial bandits.
\newblock In \emph{International Conference on Machine Learning}, pages
  1287--1295. PMLR, 2014.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  8604--8613. PMLR, 2020.

\bibitem[Tsuchiya et~al.(2022)Tsuchiya, Ito, and Honda]{tsuchiya2022best}
Taira Tsuchiya, Shinji Ito, and Junya Honda.
\newblock Best-of-both-worlds algorithms for partial monitoring.
\newblock \emph{arXiv preprint arXiv:2207.14550}, 2022.

\bibitem[Wei and Luo(2018)]{wei2018more}
Chen-Yu Wei and Haipeng Luo.
\newblock More adaptive algorithms for adversarial bandits.
\newblock In \emph{Conference On Learning Theory}, 2018.

\bibitem[Wei et~al.(2021)Wei, Jahromi, Luo, and Jain]{wei2021learning}
Chen-Yu Wei, Mehdi~Jafarnia Jahromi, Haipeng Luo, and Rahul Jain.
\newblock Learning infinite-horizon average-reward mdps with linear function
  approximation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3007--3015. PMLR, 2021.

\bibitem[Xu et~al.(2021)Xu, Ma, and Du]{xu2021fine}
Haike Xu, Tengyu Ma, and Simon Du.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock In \emph{Conference on Learning Theory}, pages 4438--4472. PMLR,
  2021.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pages
  10978--10989. PMLR, 2020.

\bibitem[Zanette et~al.(2021)Zanette, Cheng, and
  Agarwal]{zanette2021cautiously}
Andrea Zanette, Ching-An Cheng, and Alekh Agarwal.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock \emph{arXiv preprint arXiv:2103.12923}, 2021.

\bibitem[Zimmert and Seldin(2019)]{zimmert2019optimal}
Julian Zimmert and Yevgeny Seldin.
\newblock An optimal algorithm for stochastic and adversarial bandits.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 467--475. PMLR, 2019.

\bibitem[Zimmert et~al.(2019)Zimmert, Luo, and Wei]{zimmert2019beating}
Julian Zimmert, Haipeng Luo, and Chen-Yu Wei.
\newblock Beating stochastic and adversarial semi-bandits optimally and
  simultaneously.
\newblock In \emph{International Conference on Machine Learning}, pages
  7683--7692. PMLR, 2019.

\end{thebibliography}
