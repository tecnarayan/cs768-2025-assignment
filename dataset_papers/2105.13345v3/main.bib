% FAIRL
@inproceedings{ghasemipour2020divergence,
  title={A divergence minimization perspective on imitation learning methods},
  author={Ghasemipour, Seyed Kamyar Seyed and Zemel, Richard and Gu, Shixiang},
  booktitle={Conference on Robot Learning},
  pages={1259--1277},
  year={2020},
  organization={PMLR}
}

@book{montgomery2017design,
  title={Design and analysis of experiments},
  author={Montgomery, Douglas C},
  year={2017},
  publisher={John wiley \& sons}
}

@article{liu2020lipschitz,
  title={Lipschitz constrained GANs via boundedness and continuity},
  author={Liu, Kanglin and Qiu, Guoping},
  journal={Neural Computing and Applications},
  pages={1--13},
  year={2020},
  publisher={Springer}
}

@book{williams1991probability,
  title={Probability with martingales},
  author={Williams, David},
  year={1991},
  publisher={Cambridge university press}
}


% Quasimetrics allow lipschitz dual
@phdthesis{jevtic2018combinatorial,
  title={Combinatorial Structure of Finite Metric Spaces},
  author={Jevti{\'c}, Filip},
  school={The University of Texas at Dallas},
  month={August},
  year={2018}
}

@article{liao2018sharpening,
  title={Sharpening Jensen's inequality},
  author={Liao, JG and Berg, Arthur},
  journal={The American Statistician},
  year={2018},
  publisher={Taylor \& Francis}
}

% intrinsic motivation exploration
@inproceedings{csimcsek2006intrinsic,
  title={An intrinsic reward mechanism for efficient exploration},
  author={{\c{S}}im{\c{s}}ek, {\"O}zg{\"u}r and Barto, Andrew G},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={833--840},
  year={2006}
}

@article{torabi_generative_2019,
	title = {Generative {Adversarial} {Imitation} from {Observation}},
	url = {http://arxiv.org/abs/1807.06158},
	abstract = {Imitation from observation (IfO) is the problem of learning directly from state-only demonstrations without having access to the demonstrator’s actions. The lack of action information both distinguishes IfO from most of the literature in imitation learning, and also sets it apart as a method that may enable agents to learn from a large set of previously inapplicable resources such as internet videos. In this paper, we propose both a general framework for IfO approaches and also a new IfO approach based on generative adversarial networks called generative adversarial imitation from observation (GAIfO). We conduct experiments in two different settings: (1) when demonstrations consist of low-dimensional, manuallydeﬁned state features, and (2) when demonstrations consist of high-dimensional, raw visual data. We demonstrate that our approach performs comparably to classical imitation learning approaches (which have access to the demonstrator’s actions) and signiﬁcantly outperforms existing imitation from observation methods in high-dimensional simulation environments.},
	language = {en},
	urldate = {2020-04-19},
	journal = {arXiv:1807.06158 [cs, stat]},
	author = {Torabi, Faraz and Warnell, Garrett and Stone, Peter},
	month = jun,
	year = {2019},
	note = {arXiv: 1807.06158},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Torabi et al. - 2019 - Generative Adversarial Imitation from Observation.pdf:/Users/idurugkar/Zotero/storage/H6GSS98Q/Torabi et al. - 2019 - Generative Adversarial Imitation from Observation.pdf:application/pdf}
}

% GAIL
@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={4565--4573},
  year={2016}
}


@inproceedings{amortila_distributional_2020,
	title = {A {Distributional} {Analysis} of {Sampling}-{Based} {Reinforcement} {Learning} {Algorithms}},
	url = {http://proceedings.mlr.press/v108/amortila20a.html},
	abstract = {We present a distributional approach to theoretical analyses of reinforcement learning algorithms for constant step-sizes. We demonstrate its effectiveness by presenting simple and unified proofs o...},
	language = {en},
	urldate = {2020-07-20},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Amortila, Philip and Precup, Doina and Panangaden, Prakash and Bellemare, Marc G.},
	month = jun,
	year = {2020},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {4357--4366},
	file = {Full Text PDF:/Users/idurugkar/Zotero/storage/QFT9WX83/Amortila et al. - 2020 - A Distributional Analysis of Sampling-Based Reinfo.pdf:application/pdf;Snapshot:/Users/idurugkar/Zotero/storage/II26S55E/amortila20a.html:text/html}
}

@article{peyre_computational_2020,
year = {2019},
volume = {11},
journal = {Foundations and Trends in Machine Learning},
title = {Computational Optimal Transport},
url = {http://arxiv.org/abs/1803.00567},
abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
urldate = {2020-07-31},
number = {5-6},
pages = {355--607},
author = {Peyré, Gabriel and Cuturi, Marco},
}


@article{xiao_wasserstein_2019,
	title = {Wasserstein {Adversarial} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/1906.08113},
	abstract = {Imitation Learning describes the problem of recovering an expert policy from demonstrations. While inverse reinforcement learning approaches are known to be very sample-efficient in terms of expert demonstrations, they usually require problem-dependent reward functions or a (task-)specific reward-function regularization. In this paper, we show a natural connection between inverse reinforcement learning approaches and Optimal Transport, that enables more general reward functions with desirable properties (e.g., smoothness). Based on our observation, we propose a novel approach called Wasserstein Adversarial Imitation Learning. Our approach considers the Kantorovich potentials as a reward function and further leverages regularized optimal transport to enable large-scale applications. In several robotic experiments, our approach outperforms the baselines in terms of average cumulative rewards and shows a significant improvement in sample-efficiency, by requiring just one expert demonstration.},
	urldate = {2020-07-31},
	journal = {arXiv:1906.08113 [cs, stat]},
	author = {Xiao, Huang and Herman, Michael and Wagner, Joerg and Ziesche, Sebastian and Etesami, Jalal and Linh, Thai Hong},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.08113},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/Y2FDUGKD/Xiao et al. - 2019 - Wasserstein Adversarial Imitation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/RCPHAKIW/1906.html:text/html}
}

@article{tolstikhin_wasserstein_2019,
	title = {Wasserstein {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1711.01558},
	abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.},
	urldate = {2020-07-31},
	journal = {arXiv:1711.01558 [cs, stat]},
	author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
	month = dec,
	year = {2019},
	note = {arXiv: 1711.01558},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published at ICLR 2018.. Included much wider hyperparameter sweep: in significant improvements in FIDs on CelebA},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/6CI6998W/Tolstikhin et al. - 2019 - Wasserstein Auto-Encoders.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/WA92HGQ2/1711.html:text/html}
}

@article{bousquet_optimal_2017,
	title = {From optimal transport to generative modeling: the {VEGAN} cookbook},
	shorttitle = {From optimal transport to generative modeling},
	url = {http://arxiv.org/abs/1705.07642},
	abstract = {We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution \$P\_X\$ and the latent variable model distribution \$P\_G\$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from \$P\_X\$ and \$P\_G\$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.},
	urldate = {2020-07-31},
	journal = {arXiv:1705.07642 [stat]},
	author = {Bousquet, Olivier and Gelly, Sylvain and Tolstikhin, Ilya and Simon-Gabriel, Carl-Johann and Schoelkopf, Bernhard},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07642},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/29I88AJZ/Bousquet et al. - 2017 - From optimal transport to generative modeling the.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/FRBFSLR6/1705.html:text/html}
}

@article{dabney_implicit_2018,
	title = {Implicit {Quantile} {Networks} for {Distributional} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.06923},
	abstract = {In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.},
	urldate = {2020-07-31},
	journal = {arXiv:1806.06923 [cs, stat]},
	author = {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Rémi},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.06923},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: ICML 2018},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/2WUNPJ96/Dabney et al. - 2018 - Implicit Quantile Networks for Distributional Rein.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/RFTXBZPX/1806.html:text/html}
}

% bellman dynamic programming and policy iteration
@article{bellman1966dynamic,
  title={Dynamic programming},
  author={Bellman, Richard},
  journal={Science},
  volume={153},
  number={3731},
  pages={34--37},
  year={1966},
  publisher={American Association for the Advancement of Science}
}

% Soroush's masters thesis
@mastersthesis{Nasiriany:EECS-2020-151,
    Author = {Nasiriany, Soroush},
    Title = {DisCo RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Policies},
    School = {EECS Department, University of California, Berkeley},
    Year = {2020},
    Month = {Aug},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-151.html},
    Number = {UCB/EECS-2020-151},
    Abstract = {Reinforcement learning is focused on the problem of learning a near-optimal policy for a given task. But can we use reinforcement learning to instead learn general-purpose policies that can perform a wide range of different tasks, resulting in flexible and reusable skills? Contextual policies provide this capability in principle, but the representation of the context determines the degree of generalization and expressivity. Categorical contexts preclude generalization to entirely new tasks. Goal-conditioned policies may enable some generalization, but cannot capture all tasks that might be desired. In this paper, we propose goal distributions as a general and broadly applicable task representation suitable for contextual policies. Goal distributions are general in the sense that they can represent any state-based reward function when equipped with an appropriate distribution class, while the particular choice of distribution class allows us to trade off expressivity and learnability. We develop an off-policy algorithm called distribution-conditioned reinforcement learning (DisCo RL) to efficiently learn these policies. We evaluate DisCo RL on a variety of robot manipulation tasks and find that it significantly outperforms prior methods on tasks that require generalization to new goal distributions.}
}

% multi-task RL
@inproceedings{wilson2007multi,
  title={Multi-task reinforcement learning: a hierarchical bayesian approach},
  author={Wilson, Aaron and Fern, Alan and Ray, Soumya and Tadepalli, Prasad},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={1015--1022},
  year={2007}
}

% Pong imagined goals
@inproceedings{Nair2018VisualRL,
  title={Visual Reinforcement Learning with Imagined Goals},
  author={Ashvin Nair and Vitchyr H. Pong and Murtaza Dalal and Shikhar Bahl and S. Lin and S. Levine},
  booktitle={NeurIPS},
  year={2018}
}

% RL on manipulation tasks
@article{Schoettler2020DeepRL,
  title={Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards},
  author={Gerrit Schoettler and Ashvin Nair and Jianlan Luo and Shikhar Bahl and J. A. Ojea and Eugen Solowjow and S. Levine},
  journal={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2020},
  pages={5548-5555}
}

% deep features can act as metric?
@article{Zhang2018TheUE,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Richard Zhang and Phillip Isola and Alexei A. Efros and E. Shechtman and O. Wang},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={586-595}
}

% information theoretic credit assignment
@article{Arumugam2021AnIP,
  title={An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning},
  author={Dilip Arumugam and P. Henderson and P. Bacon},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.06224}
}

% nota showing policy gradient is not a gradient
@inproceedings{nota2020PolicyGradient,
  title={Is the Policy Gradient a Gradient?},
  author={Nota, Chris and Thomas, Philip S},
  booktitle={International Conference on Autonomous Agents and Multi-Agent Systems},
  pages={939--947},
  year={2020}
}

% time is distance in an MDP
@article{jaksch_near-optimal_2010,
	title = {Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}},
	volume = {11},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v11/jaksch10a.html},
	language = {en},
	number = {51},
	urldate = {2021-03-11},
	journal = {Journal of Machine Learning Research},
	author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	year = {2010},
	pages = {1563--1600},
	file = {Full Text PDF:/Users/idurugkar/Zotero/storage/TNALI6YA/Jaksch et al. - 2010 - Near-optimal Regret Bounds for Reinforcement Learn.pdf:application/pdf;Snapshot:/Users/idurugkar/Zotero/storage/2KZYTAS5/jaksch10a.html:text/html}
}

% Uses WGAN for synthesizing programs
@inproceedings{ganin2018synthesizing,
  title={Synthesizing programs for images using reinforced adversarial learning},
  author={Ganin, Yaroslav and Kulkarni, Tejas and Babuschkin, Igor and Eslami, SM Ali and Vinyals, Oriol},
  booktitle={International Conference on Machine Learning},
  pages={1666--1675},
  year={2018},
  organization={PMLR}
}


@article{richemond_wasserstein_2017,
	title = {On {Wasserstein} {Reinforcement} {Learning} and the {Fokker}-{Planck} equation},
	url = {http://arxiv.org/abs/1712.07185},
	abstract = {Policy gradients methods often achieve better performance when the change in policy is limited to a small Kullback-Leibler divergence. We derive policy gradients where the change in policy is limited to a small Wasserstein distance (or trust region). This is done in the discrete and continuous multi-armed bandit settings with entropy regularisation. We show that in the small steps limit with respect to the Wasserstein distance \$W\_2\$, policy dynamics are governed by the Fokker-Planck (heat) equation, following the Jordan-Kinderlehrer-Otto result. This means that policies undergo diffusion and advection, concentrating near actions with high reward. This helps elucidate the nature of convergence in the probability matching setup, and provides justification for empirical practices such as Gaussian policy priors and additive gradient noise.},
	urldate = {2020-07-31},
	journal = {arXiv:1712.07185 [cs]},
	author = {Richemond, Pierre H. and Maginnis, Brendan},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.07185},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/CSBN85BF/Richemond and Maginnis - 2017 - On Wasserstein Reinforcement Learning and the Fokk.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/Z7FWPBAX/1712.html:text/html}
}

@inproceedings{arjovsky_wasserstein_2017,
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	url = {http://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse...},
	language = {en},
	urldate = {2020-07-31},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498
Section: Machine Learning},
	pages = {214--223},
	file = {Full Text PDF:/Users/idurugkar/Zotero/storage/8ZVIGK5K/Arjovsky et al. - 2017 - Wasserstein Generative Adversarial Networks.pdf:application/pdf;Snapshot:/Users/idurugkar/Zotero/storage/YEZMV5XL/arjovsky17a.html:text/html}
}

@incollection{gupta_meta-reinforcement_2018,
	title = {Meta-{Reinforcement} {Learning} of {Structured} {Exploration} {Strategies}},
	url = {http://papers.nips.cc/paper/7776-meta-reinforcement-learning-of-structured-exploration-strategies.pdf},
	urldate = {2020-08-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {5302--5311},
	file = {NIPS Full Text PDF:/Users/idurugkar/Zotero/storage/PX4LUDSV/Gupta et al. - 2018 - Meta-Reinforcement Learning of Structured Explorat.pdf:application/pdf;NIPS Snapshot:/Users/idurugkar/Zotero/storage/2APTJHS9/7776-meta-reinforcement-learning-of-structured-exploration-strategies.html:text/html}
}

@incollection{baldassarre_intrinsic_2013,
	address = {Berlin, Heidelberg},
	title = {Intrinsic {Motivation} and {Reinforcement} {Learning}},
	isbn = {978-3-642-32374-4 978-3-642-32375-1},
	url = {http://link.springer.com/10.1007/978-3-642-32375-1_2},
	abstract = {Motivation is a key factor in human learning. We learn best when we are highly motivated to learn. Psychologists distinguish between extrinsicallymotivated behavior, which is behavior undertaken to achieve some externally supplied reward, such as a prize, a high grade, or a high-paying job, and intrinsically-motivated behavior, which is behavior done for its own sake. Is an analogous distinction meaningful for machine learning systems? Can we say of a machine learning system that it is motivated to learn, and if so, is it possible to provide it with an analog of intrinsic motivation? Despite the fact that a formal distinction between extrinsic and intrinsic motivation is elusive, this chapter argues that the answer to both questions is assuredly “yes,” and that the machine learning framework of reinforcement learning is particularly appropriate for bringing learning together with what in animals one would call motivation. Despite the common perception that a reinforcement learning agent’s reward has to be extrinsic because the agent has a distinct input channel for reward, reinforcement learning provides a natural framework for incorporating principles of intrinsic motivation.},
	language = {en},
	urldate = {2020-08-05},
	booktitle = {Intrinsically {Motivated} {Learning} in {Natural} and {Artificial} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Barto, Andrew G.},
	editor = {Baldassarre, Gianluca and Mirolli, Marco},
	year = {2013},
	doi = {10.1007/978-3-642-32375-1_2},
	pages = {17--47},
	file = {Barto - 2013 - Intrinsic Motivation and Reinforcement Learning.pdf:/Users/idurugkar/Zotero/storage/LTGYX8KS/Barto - 2013 - Intrinsic Motivation and Reinforcement Learning.pdf:application/pdf}
}

@article{singh_intrinsically_2010,
	title = {Intrinsically {Motivated} {Reinforcement} {Learning}: {An} {Evolutionary} {Perspective}},
	volume = {2},
	issn = {1943-0612},
	shorttitle = {Intrinsically {Motivated} {Reinforcement} {Learning}},
	doi = {10.1109/TAMD.2010.2051031},
	abstract = {There is great interest in building intrinsic motivation into artificial systems using the reinforcement learning framework. Yet, what intrinsic motivation may mean computationally, and how it may differ from extrinsic motivation, remains a murky and controversial subject. In this paper, we adopt an evolutionary perspective and define a new optimal reward framework that captures the pressure to design good primary reward functions that lead to evolutionary success across environments. The results of two computational experiments show that optimal primary reward signals may yield both emergent intrinsic and extrinsic motivation. The evolutionary perspective and the associated optimal reward framework thus lead to the conclusion that there are no hard and fast features distinguishing intrinsic and extrinsic reward computationally. Rather, the directness of the relationship between rewarding behavior and evolutionary success varies along a continuum.},
	number = {2},
	journal = {IEEE Transactions on Autonomous Mental Development},
	author = {Singh, Satinder and Lewis, Richard L. and Barto, Andrew G. and Sorg, Jonathan},
	month = jun,
	year = {2010},
	note = {Conference Name: IEEE Transactions on Autonomous Mental Development},
	keywords = {artificial system, Cognitive science, Computer science, Genetic algorithms, History, Humans, intrinsic motivation, Intrinsic motivation, Learning, learning (artificial intelligence), optimal primary reward signal, Psychology, reinforcement learning, reward functions, Signal resolution, System performance},
	pages = {70--82},
	file = {IEEE Xplore Full Text PDF:/Users/idurugkar/Zotero/storage/M54VWMRR/Singh et al. - 2010 - Intrinsically Motivated Reinforcement Learning An.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/idurugkar/Zotero/storage/X7UXKA6L/5471106.html:text/html}
}

% DDPG
@inproceedings{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

% Brad Knox reward misdesign
@article{knox2021reward,
  title={Reward (Mis) design for Autonomous Driving},
  author={Knox, W Bradley and Allievi, Alessandro and Banzhaf, Holger and Schmitt, Felix and Stone, Peter},
  journal={arXiv preprint arXiv:2104.13906},
  year={2021}
}

% Hausknecht DDPG stability
@inproceedings{hausknecht2016deep,
  title={Deep reinforcement learning in parameterized action space},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={International Conference on Learning Representations},
  year={2016},
  url={https://arxiv.org/abs/1511.04143}
}

% exploration with options
@inproceedings{
Jinnai2020Exploration,
title={Exploration in Reinforcement Learning with Deep Covering Options},
author={Yuu Jinnai and Jee Won Park and Marlos C. Machado and George Konidaris},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeIyaVtwB}
}

% uncovering options
@inproceedings{machado2017laplacian,
  title={A laplacian framework for option discovery in reinforcement learning},
  author={Machado, Marlos C and Bellemare, Marc G and Bowling, Michael},
  booktitle={International Conference on Machine Learning},
  pages={2295--2304},
  year={2017},
  organization={PMLR}
}

% proto value functions
@article{mahadevan2007proto,
  title={Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes.},
  author={Mahadevan, Sridhar and Maggioni, Mauro},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={10},
  year={2007}
}

% quasi-metrics
@inproceedings{Smyth1987QuasiUR,
  title={Quasi Uniformities: Reconciling Domains with Metric Spaces},
  author={M. Smyth},
  booktitle={MFPS},
  year={1987}
}

% value density estimation
@article{Schroecker2020UniversalVD,
  title={Universal Value Density Estimation for Imitation Learning and Goal-Conditioned Reinforcement Learning},
  author={Yannick Schroecker and C. Isbell},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.06473}
}

% bisimulation metrics
@article{Zhang2020LearningIR,
  title={Learning Invariant Representations for Reinforcement Learning without Reconstruction},
  author={A. Zhang and Rowan McAllister and R. Calandra and Yarin Gal and S. Levine},
  journal={ArXiv},
  year={2020},
  volume={abs/2006.10742}
}

% state similarity
@inproceedings{Castro2020ScalableMF,
  title={Scalable methods for computing state similarity in deterministic Markov Decision Processes},
  author={P. S. Castro},
  booktitle={AAAI},
  year={2020}
}

% metrics comparing MDP
@inproceedings{Ferns2004MetricsFF,
  title={Metrics for Finite Markov Decision Processes},
  author={N. Ferns and P. Panangaden and Doina Precup},
  booktitle={AAAI},
  year={2004}
}

% distance to goal
@article{Venkattaramanujam2019SelfsupervisedLO,
  title={Self-supervised Learning of Distance Functions for Goal-Conditioned Reinforcement Learning},
  author={Srinivas Venkattaramanujam and E. Crawford and T. Doan and Doina Precup},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.02998}
}

% gym
@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

% Mujoco
@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

% distance between nodes of a graph
@inproceedings{fouss_novel_2005,
	address = {USA},
	series = {{WI} '05},
	title = {A {Novel} {Way} of {Computing} {Similarities} between {Nodes} of a {Graph}, with {Application} to {Collaborative} {Recommendation}},
	isbn = {978-0-7695-2415-3},
	url = {http://doi.org/10.1109/WI.2005.9},
	doi = {10.1109/WI.2005.9},
	abstract = {This work presents a new perspective on characterizing the similarity between elements of a database or, more generally, nodes of a weighted, undirected, graph. It is based on a Markov-chain model of random walk through the database. The suggested quantities, representing dissimilarities (or similarities) between any two elements, have the nice property of decreasing (increasing) when the number of paths connecting those elements increases and when the "length" of any path decreases. The model is evaluated on a collaborative recommendation task where suggestions are made about which movies people should watch based upon what they watched in the past. The model, which nicely fits into the so-called "statistical relational learning" framework as well as the "link analysis" paradigm, could also be used to compute document or word similarities, and, more generally, could be applied to other database or web mining tasks.},
	urldate = {2021-03-24},
	booktitle = {Proceedings of the 2005 {IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence}},
	publisher = {IEEE Computer Society},
	author = {Fouss, Francois and Pirotte, Alain and Saerens, Marco},
	month = sep,
	year = {2005},
	pages = {550--556},
	file = {Full Text PDF:/Users/idurugkar/Zotero/storage/83N58HCN/Fouss et al. - 2005 - A Novel Way of Computing Similarities between Node.pdf:application/pdf}
}

% GoalGAIL
@inproceedings{Ding2019GoalconditionedIL,
  title={Goal-conditioned Imitation Learning},
  author={Y. Ding and Carlos Florensa and Mariano Phielipp and P. Abbeel},
  booktitle={NeurIPS},
  year={2019}
}

% UVFA
@inproceedings{Schaul2015UniversalVF,
  title={Universal Value Function Approximators},
  author={T. Schaul and Daniel Horgan and K. Gregor and D. Silver},
  booktitle={ICML},
  year={2015}
}

@article{zheng_learning_2018,
	title = {On {Learning} {Intrinsic} {Rewards} for {Policy} {Gradient} {Methods}},
	url = {http://arxiv.org/abs/1804.06459},
	abstract = {In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem, or close variants thereof, have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of Singh et.al. that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. Our results show improved performance on most but not all of the domains.},
	urldate = {2020-08-05},
	journal = {arXiv:1804.06459 [cs, stat]},
	author = {Zheng, Zeyu and Oh, Junhyuk and Singh, Satinder},
	month = jun,
	year = {2018},
	note = {arXiv: 1804.06459},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/5AYJ8ZCX/Zheng et al. - 2018 - On Learning Intrinsic Rewards for Policy Gradient .pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/3EZUV9AJ/1804.html:text/html}
}

@article{zhang_automatic_2020,
	title = {Automatic {Curriculum} {Learning} through {Value} {Disagreement}},
	url = {http://arxiv.org/abs/2006.09641},
	abstract = {Continually solving new, unsolved tasks is the key to learning diverse behaviors. Through reinforcement learning (RL), we have made massive strides towards solving tasks that have a single goal. However, in the multi-task domain, where an agent needs to reach multiple goals, the choice of training goals can largely affect sample efficiency. When biological agents learn, there is often an organized and meaningful order to which learning happens. Inspired by this, we propose setting up an automatic curriculum for goals that the agent needs to solve. Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a significantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy. This simple technique samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement. We evaluate our method across 13 multi-goal robotic tasks and 5 navigation tasks, and demonstrate performance gains over current state-of-the-art methods.},
	urldate = {2020-09-17},
	journal = {arXiv:2006.09641 [cs, stat]},
	author = {Zhang, Yunzhi and Abbeel, Pieter and Pinto, Lerrel},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.09641},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	annote = {Comment: https://sites.google.com/berkeley.edu/vds/},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/SGLJW7RG/Zhang et al. - 2020 - Automatic Curriculum Learning through Value Disagr.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/2E5V2NQW/2006.html:text/html}
}

@article{fu_learning_2018,
	title = {Learning {Robust} {Rewards} with {Adversarial} {Inverse} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1710.11248},
	abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose adverserial inverse reinforcement learning (AIRL), a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.},
	urldate = {2020-10-03},
	journal = {arXiv:1710.11248 [cs]},
	author = {Fu, Justin and Luo, Katie and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv: 1710.11248},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/ZFWQX6CT/Fu et al. - 2018 - Learning Robust Rewards with Adversarial Inverse R.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/U6RTN695/1710.html:text/html}
}

% Puterman's MDP book
@article{puterman1990markov,
  title={Markov decision processes},
  author={Puterman, Martin L},
  journal={Handbooks in operations research and management science},
  volume={2},
  pages={331--434},
  year={1990},
  publisher={Elsevier}
}

% Goal-conditioned RL basic citation
@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}

% Hindsight Experience Replay
@article{andrychowicz_hindsight_2018,
	title = {Hindsight {Experience} {Replay}},
	url = {http://arxiv.org/abs/1707.01495},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
	urldate = {2020-10-03},
	journal = {arXiv:1707.01495 [cs]},
	author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
	month = feb,
	year = {2018},
	note = {arXiv: 1707.01495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/WN6IP5PH/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/EUGQUI2G/1707.html:text/html}
}

% GAN
@incollection{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Nets}},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {2672--2680}
}

% Optimal transport
@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2008},
  publisher={Springer Science \& Business Media}
}


% Improved WGAN
@incollection{gulrajani_improved_2017,
	title = {Improved {Training} of {Wasserstein} {GANs}},
	url = {http://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf},
	urldate = {2020-10-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {5767--5777},
	file = {NIPS Full Text PDF:/Users/idurugkar/Zotero/storage/A6N6MX3M/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf;NIPS Snapshot:/Users/idurugkar/Zotero/storage/D62IM4YP/7159-improved-training-of-wasserstein-gans.html:text/html}
}

% soft Q learning
@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1352--1361},
  year={2017}
}

% RL book
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

% multitask learning
@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

% A*
@article{hart1968formal,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE transactions on Systems Science and Cybernetics},
  volume={4},
  number={2},
  pages={100--107},
  year={1968},
  publisher={IEEE}
}


% Using classification for density estimation
@inproceedings{
eysenbach2021clearning,
title={C-Learning: Learning to Achieve Goals via Recursive Classification},
author={Benjamin Eysenbach and Ruslan Salakhutdinov and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=tc5qisoB-C}
}

% Hellinger distance minimization to goal distribution
@article{eysenbach2021replacing,
  title={Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification},
  author={Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2103.12656},
  year={2021}
}

% max ent exploration
@inproceedings{hazan2019provably,
  title={Provably efficient maximum entropy exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={International Conference on Machine Learning},
  pages={2681--2691},
  year={2019},
  organization={PMLR}
}

% state marginal matching
@article{lee2019efficient,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019}
}

% state marginal matching but for IRL
@inproceedings{ni2020fIRL,
  title={F-irl: Inverse reinforcement learning via state marginal matching},
  author={Ni, Tianwei and Sikchi, Harshit and Wang, Yufei and Gupta, Tejus and Lee, Lisa and Eysenbach, Benjamin},
  booktitle={Conference on Robot Learning},
  year={2020}
}

% Metric geometry
@book{burago2001course,
  title={A course in metric geometry},
  author={Burago, Dmitri and Burago, Iu D and Burago, Yuri and Ivanov, Sergei and Ivanov, Sergei V and Ivanov, Sergei A},
  volume={33},
  year={2001},
  publisher={American Mathematical Soc.}
}


% Kearns and satinder singh PAC bounds
@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

% Blog post about faulty reward functions
@misc{clark_faulty_2016, title={Faulty Reward Functions in the Wild}, url={https://openai.com/blog/faulty-reward-functions/}, journal={openai.com}, author={Clark, Jack and Amodei, Dario}, year={2016}, month={Dec}}

% zheng icml
@article{zheng_what_2020,
	title = {What {Can} {Learned} {Intrinsic} {Rewards} {Capture}?},
	url = {http://arxiv.org/abs/1912.05500},
	abstract = {The objective of a reinforcement learning agent is to behave so as to maximise the sum of a suitable scalar function of state: the reward. These rewards are typically given and immutable. In this paper, we instead consider the proposition that the reward function itself can be a good locus of learned knowledge. To investigate this, we propose a scalable meta-gradient framework for learning useful intrinsic reward functions across multiple lifetimes of experience. Through several proof-of-concept experiments, we show that it is feasible to learn and capture knowledge about long-term exploration and exploitation into a reward function. Furthermore, we show that unlike policy transfer methods that capture "how" the agent should behave, the learned reward functions can generalise to other kinds of agents and to changes in the dynamics of the environment by capturing "what" the agent should strive to do.},
	urldate = {2020-08-04},
	journal = {arXiv:1912.05500 [cs]},
	author = {Zheng, Zeyu and Oh, Junhyuk and Hessel, Matteo and Xu, Zhongwen and Kroiss, Manuel and van Hasselt, Hado and Silver, David and Singh, Satinder},
	month = jul,
	year = {2020},
	note = {arXiv: 1912.05500},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: ICML 2020},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/C2TNXSTJ/Zheng et al. - 2020 - What Can Learned Intrinsic Rewards Capture.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/PQEB7GTE/1912.html:text/html;ICML 2020 What can intrinsic reward capture_.pdf:/Users/idurugkar/Zotero/storage/LWFUL32W/ICML 2020 What can intrinsic reward capture_.pdf:application/pdf}
}


@inproceedings{sorg2010internal,
  title={Internal rewards mitigate agent boundedness},
  author={Sorg, Jonathan and Singh, Satinder P and Lewis, Richard L},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={1007--1014},
  year={2010}
}

@inproceedings{sorg2010reward,
  title={Reward design via online gradient ascent},
  author={Sorg, Jonathan and Lewis, Richard L and Singh, Satinder P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2190--2198},
  year={2010}
}

@inproceedings{niekum2010evolved,
  title={Evolved intrinsic reward functions for reinforcement learning},
  author={Niekum, Scott},
  booktitle={Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
  pages={1955--1956},
  year={2010}
}

@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}

Stochastic shortest path value iteration
@article{guillot2020stochastic,
  title={The stochastic shortest path problem: a polyhedral combinatorics perspective},
  author={Guillot, Matthieu and Stauffer, Gautier},
  journal={European Journal of Operational Research},
  volume={285},
  number={1},
  pages={148--158},
  year={2020},
  publisher={Elsevier}
}

@misc{rl-zoo,
  author = {Raffin, Antonin},
  title = {RL Baselines Zoo},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/araffin/rl-baselines-zoo}},
}

%Rudder
@inproceedings{arjona2019rudder,
  title={Rudder: Return decomposition for delayed rewards},
  author={Arjona-Medina, Jose A and Gillhofer, Michael and Widrich, Michael and Unterthiner, Thomas and Brandstetter, Johannes and Hochreiter, Sepp},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13566--13577},
  year={2019}
}

% Ng shaped rewards
@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={ICML},
  volume={99},
  pages={278--287},
  year={1999}
}

% biased wasserstein estimation
@article{Bellemare2017CramerD,
  title={The Cramer Distance as a Solution to Biased Wasserstein Gradients},
  author={Marc G. Bellemare and Ivo Danihelka and Will Dabney and S. Mohamed and Balaji Lakshminarayanan and Stephan Hoyer and R. Munos},
  journal={ArXiv},
  year={2017},
  volume={abs/1705.10743},
  url={https://arxiv.org/abs/1705.10743}
}

% wasserstein GANs fail to estimate the distance
@article{Stanczuk2021WassersteinGW,
  title={Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)},
  author={Jan Stanczuk and Christian Etmann and L. Kreusser and C. Sch{\"o}nlieb},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.01678}
}

% count based exploration with intrinsic motivation
@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in neural information processing systems},
  pages={1471--1479},
  year={2016}
}

% oudeyer intrinsic motivation
@article{oudeyer2009intrinsic,
  title={What is intrinsic motivation? A typology of computational approaches},
  author={Oudeyer, Pierre-Yves and Kaplan, Frederic},
  journal={Frontiers in neurorobotics},
  volume={1},
  pages={6},
  year={2009},
  publisher={Frontiers}
}

% gianluca skill IM
@article{santucci2013best,
  title={Which is the best intrinsic motivation signal for learning multiple skills?},
  author={Santucci, Vieri Giuliano and Baldassarre, Gianluca and Mirolli, Marco},
  journal={Frontiers in neurorobotics},
  volume={7},
  pages={22},
  year={2013},
  publisher={Frontiers}
}

% evolving IM gianluca
@inproceedings{schembri2007evolving,
  title={Evolving internal reinforcers for an intrinsically motivated reinforcement-learning robot},
  author={Schembri, Massimiliano and Mirolli, Marco and Baldassarre, Gianluca},
  booktitle={2007 IEEE 6th International Conference on Development and Learning},
  pages={282--287},
  year={2007},
  organization={IEEE}
}

% More biological perspective
@article{baldassarre2014intrinsic,
  title={Intrinsic motivations and open-ended development in animals, humans, and robots: an overview},
  author={Baldassarre, Gianluca and Stafford, Tom and Mirolli, Marco and Redgrave, Peter and Ryan, Richard M and Barto, Andrew},
  journal={Frontiers in psychology},
  volume={5},
  pages={985},
  year={2014},
  publisher={Frontiers}
}

% Gianluca's perspective on IM
@inproceedings{baldassarre2011intrinsic,
  title={What are intrinsic motivations? A biological perspective},
  author={Baldassarre, Gianluca},
  booktitle={2011 IEEE international conference on development and learning (ICDL)},
  volume={2},
  pages={1--8},
  year={2011},
  organization={IEEE}
}

% oudeyer another goal conditioned intrinsic reward
@article{baranes2013active,
  title={Active learning of inverse models with intrinsically motivated goal exploration in robots},
  author={Baranes, Adrien and Oudeyer, Pierre-Yves},
  journal={Robotics and Autonomous Systems},
  volume={61},
  number={1},
  pages={49--73},
  year={2013},
  publisher={Elsevier}
}

% Oudeyer goal intrinsic
@article{forestier2017intrinsically,
  title={Intrinsically motivated goal exploration processes with automatic curriculum learning},
  author={Forestier, S{\'e}bastien and Portelas, R{\'e}my and Mollard, Yoan and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1708.02190},
  year={2017}
}

% oudeyer another definition
@inproceedings{oudeyer2008can,
  title={How can we define intrinsic motivation?},
  author={Oudeyer, Pierre-Yves and Kaplan, Frederic},
  booktitle={the 8th International Conference on Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems},
  year={2008},
  organization={Lund University Cognitive Studies, Lund: LUCS, Brighton}
}

% oudeyer exploration
@article{baranes2009r,
  title={R-iac: Robust intrinsically motivated exploration and active learning},
  author={Baranes, Adrien and Oudeyer, Pierre-Yves},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={1},
  number={3},
  pages={155--169},
  year={2009},
  publisher={IEEE}
}

% biological basis for IM
@article{gottlieb2013information,
  title={Information-seeking, curiosity, and attention: computational and neural mechanisms},
  author={Gottlieb, Jacqueline and Oudeyer, Pierre-Yves and Lopes, Manuel and Baranes, Adrien},
  journal={Trends in cognitive sciences},
  volume={17},
  number={11},
  pages={585--593},
  year={2013},
  publisher={Elsevier}
}

% Barto and simsek
@inproceedings{barto2005intrinsic,
  title={Intrinsic motivation for reinforcement learning systems},
  author={Barto, Andrew G and Simsek, Ozg{\"u}r},
  booktitle={Proceedings of the Thirteenth Yale Workshop on Adaptive and Learning Systems},
  pages={113--118},
  year={2005}
}

% IM for skill learning
@inproceedings{barto2004intrinsically,
  title={Intrinsically motivated learning of hierarchical collections of skills},
  author={Barto, Andrew G and Singh, Satinder and Chentanez, Nuttapong},
  booktitle={Proceedings of the 3rd International Conference on Development and Learning},
  pages={112--19},
  year={2004},
  organization={Cambridge, MA}
}

@inproceedings{singh2005intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Singh, Satinder and Barto, Andrew G and Chentanez, Nuttapong},
  booktitle={Advances in neural information processing systems},
  pages={1281--1288},
  year={2005}
}

@inproceedings{singh2009rewards,
  title={Where do rewards come from},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
  booktitle={Proceedings of the annual conference of the cognitive science society},
  pages={2601--2606},
  year={2009},
  organization={Cognitive Science Society}
}

% TD3
@inproceedings{fujimoto2018TD3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International Conference on Machine Learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@article{dadashi_primal_2020,
	title = {Primal {Wasserstein} {Imitation} {Learning}},
	url = {http://arxiv.org/abs/2006.04678},
	abstract = {Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.},
	urldate = {2021-01-31},
	journal = {arXiv:2006.04678 [cs, stat]},
	author = {Dadashi, Robert and Hussenot, Léonard and Geist, Matthieu and Pietquin, Olivier},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04678},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/idurugkar/Zotero/storage/AEMRBLND/Dadashi et al. - 2020 - Primal Wasserstein Imitation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/idurugkar/Zotero/storage/SX4N7W9F/2006.html:text/html}
}

@inproceedings{zhang_policy_2018,
	title = {Policy {Optimization} as {Wasserstein} {Gradient} {Flows}},
	url = {http://proceedings.mlr.press/v80/zhang18a.html},
	abstract = {Policy optimization is a core component of reinforcement learning (RL), and most existing RL methods directly optimize parameters of a policy based on maximizing the expected total reward, or its s...},
	language = {en},
	urldate = {2021-01-24},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Ruiyi and Chen, Changyou and Li, Chunyuan and Carin, Lawrence},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {5737--5746},
	file = {Snapshot:/Users/idurugkar/Zotero/storage/QPUPFUX8/zhang18a.html:text/html;Full Text PDF:/Users/idurugkar/Zotero/storage/JQ5JVDFW/Zhang et al. - 2018 - Policy Optimization as Wasserstein Gradient Flows.pdf:application/pdf}
}

@inproceedings{burda2018rnd,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{hartikainen2019ddl,
  title={Dynamical Distance Learning for Semi-Supervised and Unsupervised Skill Discovery},
  author={Hartikainen, Kristian and Geng, Xinyang and Haarnoja, Tuomas and Levine, Sergey},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{berseth2019smirl,
  title={SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments},
  author={Berseth, Glen and Geng, Daniel and Devin, Coline and Rhinehart, Nicholas and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  journal={arXiv preprint arXiv:1912.05510},
  year={2019}
}
