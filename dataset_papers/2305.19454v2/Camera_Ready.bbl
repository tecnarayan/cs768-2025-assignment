\begin{thebibliography}{10}

\bibitem{bellec2018deep}
G.~Bellec, D.~Kappel, W.~Maass, and R.~Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,
  G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~Ziegler, J.~Wu, C.~Winter,
  C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark,
  C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{chen2022coarsening}
T.~Chen, X.~Chen, X.~Ma, Y.~Wang, and Z.~Wang.
\newblock Coarsening the granularity: Towards structurally sparse lottery
  tickets.
\newblock {\em arXiv preprint arXiv:2202.04736}, 2022.

\bibitem{chin2019legr}
T.-W. Chin, R.~Ding, C.~Zhang, and D.~Marculescu.
\newblock Legr: Filter pruning via learned global ranking.
\newblock 2019.

\bibitem{dettmers2019sparse}
T.~Dettmers and L.~Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}, 2019.

\bibitem{dietrich2021towards}
A.~Dietrich, F.~Gressmann, D.~Orr, I.~Chelombiev, D.~Justus, and C.~Luschi.
\newblock Towards structured dynamic sparse pre-training of bert.
\newblock {\em arXiv preprint arXiv:2108.06277}, 2021.

\bibitem{dong2019network}
X.~Dong and Y.~Yang.
\newblock Network pruning via transformable architecture search.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{elsen2020fast}
E.~Elsen, M.~Dukhan, T.~Gale, and K.~Simonyan.
\newblock Fast sparse convnets.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 14629--14638, 2020.

\bibitem{evci2020rigging}
U.~Evci, T.~Gale, J.~Menick, P.~S. Castro, and E.~Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning}, pages
  2943--2952. PMLR, 2020.

\bibitem{gale2019state}
T.~Gale, E.~Elsen, and S.~Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv preprint arXiv:1902.09574}, 2019.

\bibitem{gale2020sparse}
T.~Gale, M.~Zaharia, C.~Young, and E.~Elsen.
\newblock Sparse gpu kernels for deep learning.
\newblock {\em arXiv preprint arXiv:2006.10901}, 2020.

\bibitem{garcia2019estimation}
E.~Garc{\'\i}a-Mart{\'\i}n, C.~F. Rodrigues, G.~Riley, and H.~Grahn.
\newblock Estimation of energy consumption in machine learning.
\newblock {\em Journal of Parallel and Distributed Computing}, 134:75--88,
  2019.

\bibitem{gray2017gpu}
S.~Gray, A.~Radford, and D.~P. Kingma.
\newblock Gpu kernels for block-sparse weights.
\newblock {\em arXiv preprint arXiv:1711.09224}, 3:2, 2017.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2019filter}
Y.~He, P.~Liu, Z.~Wang, Z.~Hu, and Y.~Yang.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2019.

\bibitem{he2017channel}
Y.~He, X.~Zhang, and J.~Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1389--1397, 2017.

\bibitem{hestness2017deep}
J.~Hestness, S.~Narang, N.~Ardalani, G.~Diamos, H.~Jun, H.~Kianinejad,
  M.~Patwary, M.~Ali, Y.~Yang, and Y.~Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv preprint arXiv:1712.00409}, 2017.

\bibitem{hou2022chex}
Z.~Hou, M.~Qin, F.~Sun, X.~Ma, K.~Yuan, Y.~Xu, Y.-K. Chen, R.~Jin, Y.~Xie, and
  S.-Y. Kung.
\newblock Chex: Channel exploration for cnn model compression.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12287--12298, 2022.

\bibitem{hubara2021accelerated}
I.~Hubara, B.~Chmiel, M.~Island, R.~Banner, S.~Naor, and D.~Soudry.
\newblock Accelerated sparse neural training: A provable and efficient method
  to find n: M transposable masks.
\newblock {\em arXiv preprint arXiv:2102.08124}, 2021.

\bibitem{iofinova2022well}
E.~Iofinova, A.~Peste, M.~Kurtz, and D.~Alistarh.
\newblock How well do sparse imagenet models transfer?
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12266--12276, 2022.

\bibitem{jaiswal2022training}
A.~K. Jaiswal, H.~Ma, T.~Chen, Y.~Ding, and Z.~Wang.
\newblock Training your sparse neural network better with any mask.
\newblock In {\em International Conference on Machine Learning}, pages
  9833--9844. PMLR, 2022.

\bibitem{jayakumar2020top}
S.~Jayakumar, R.~Pascanu, J.~Rae, S.~Osindero, and E.~Elsen.
\newblock Top-kast: Top-k always sparse training.
\newblock {\em Advances in Neural Information Processing Systems},
  33:20744--20754, 2020.

\bibitem{jia2019dissecting}
Z.~Jia, B.~Tillman, M.~Maggioni, and D.~P. Scarpazza.
\newblock Dissecting the graphcore ipu architecture via microbenchmarking.
\newblock {\em arXiv preprint arXiv:1912.03413}, 2019.

\bibitem{Peng2022exp}
P.~Jiang, L.~Hu, and S.~Song.
\newblock Exposing and exploiting fine-grained block structures for fast and
  accurate sparse training.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child,
  S.~Gray, A.~Radford, J.~Wu, and D.~Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{pmlr-v119-kurtz20a}
M.~Kurtz, J.~Kopinsky, R.~Gelashvili, A.~Matveev, J.~Carr, M.~Goin,
  W.~Leiserson, S.~Moore, B.~Nell, N.~Shavit, and D.~Alistarh.
\newblock Inducing and exploiting activation sparsity for fast inference on
  deep neural networks.
\newblock In H.~D. III and A.~Singh, editors, {\em Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of {\em Proceedings
  of Machine Learning Research}, pages 5533--5543, Virtual, 13--18 Jul 2020.
  PMLR.

\bibitem{lee2018snip}
N.~Lee, T.~Ajanthan, and P.~H. Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock {\em arXiv preprint arXiv:1810.02340}, 2018.

\bibitem{lin2020hrank}
M.~Lin, R.~Ji, Y.~Wang, Y.~Zhang, B.~Zhang, Y.~Tian, and L.~Shao.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 1529--1538, 2020.

\bibitem{liu2021group}
L.~Liu, S.~Zhang, Z.~Kuang, A.~Zhou, J.-H. Xue, X.~Wang, Y.~Chen, W.~Yang,
  Q.~Liao, and W.~Zhang.
\newblock Group fisher pruning for practical network compression.
\newblock In {\em International Conference on Machine Learning}, pages
  7021--7032. PMLR, 2021.

\bibitem{liu2021sparse}
S.~Liu, T.~Chen, X.~Chen, Z.~Atashgahi, L.~Yin, H.~Kou, L.~Shen,
  M.~Pechenizkiy, Z.~Wang, and D.~C. Mocanu.
\newblock Sparse training via boosting pruning plasticity with
  neuroregeneration.
\newblock {\em Advances in Neural Information Processing Systems},
  34:9908--9922, 2021.

\bibitem{liu2022more}
S.~Liu, T.~Chen, X.~Chen, X.~Chen, Q.~Xiao, B.~Wu, M.~Pechenizkiy, D.~Mocanu,
  and Z.~Wang.
\newblock More convnets in the 2020s: Scaling up kernels beyond 51x51 using
  sparsity.
\newblock {\em arXiv preprint arXiv:2207.03620}, 2022.

\bibitem{liu2022unreasonable}
S.~Liu, T.~Chen, X.~Chen, L.~Shen, D.~C. Mocanu, Z.~Wang, and M.~Pechenizkiy.
\newblock The unreasonable effectiveness of random pruning: Return of the most
  naive baseline for sparse training.
\newblock {\em arXiv preprint arXiv:2202.02643}, 2022.

\bibitem{liu2021SET}
S.~Liu, D.~C. Mocanu, A.~R.~R. Matavalam, Y.~Pei, and M.~Pechenizkiy.
\newblock Sparse evolutionary deep learning with over one million artificial
  neurons on commodity hardware.
\newblock {\em Neural Computing and Applications}, 33(7):2589--2604, 2021.

\bibitem{liu2021we}
S.~Liu, L.~Yin, D.~C. Mocanu, and M.~Pechenizkiy.
\newblock Do we actually need dense over-parameterization? in-time
  over-parameterization in sparse training.
\newblock In {\em Proceedings of the 39th International Conference on Machine
  Learning}, pages 6989--7000. PMLR, 2021.

\bibitem{liu2022swin}
Z.~Liu, H.~Hu, Y.~Lin, Z.~Yao, Z.~Xie, Y.~Wei, J.~Ning, Y.~Cao, Z.~Zhang,
  L.~Dong, et~al.
\newblock Swin transformer v2: Scaling up capacity and resolution.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12009--12019, 2022.

\bibitem{liu2017learning}
Z.~Liu, J.~Li, Z.~Shen, G.~Huang, S.~Yan, and C.~Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2736--2744, 2017.

\bibitem{mariet2015diversity}
Z.~Mariet and S.~Sra.
\newblock Diversity networks: Neural network compression using determinantal
  point processes.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{mocanu2018scalable}
D.~C. Mocanu, E.~Mocanu, P.~Stone, P.~H. Nguyen, M.~Gibescu, and A.~Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em arXiv:1707.04780. Nature communications.}, 9(1):2383, 2018.

\bibitem{mostafa2019parameter}
H.~Mostafa and X.~Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock {\em International Conference on Machine Learning}, 2019.

\bibitem{narang2017exploring}
S.~Narang, E.~Elsen, G.~Diamos, and S.~Sengupta.
\newblock Exploring sparsity in recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1704.05119}, 2017.

\bibitem{nvidia2020}
Nvidia.
\newblock Nvidia a100 tensor core gpu architecture.
\newblock {\em
  https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf},
  2020.

\bibitem{patterson2021carbon}
D.~Patterson, J.~Gonzalez, Q.~Le, C.~Liang, L.-M. Munguia, D.~Rothchild, D.~So,
  M.~Texier, and J.~Dean.
\newblock Carbon emissions and large neural network training.
\newblock {\em arXiv preprint arXiv:2104.10350}, 2021.

\bibitem{peste2021ac}
A.~Peste, E.~Iofinova, A.~Vladu, and D.~Alistarh.
\newblock Ac/dc: Alternating compressed/decompressed training of deep neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:8557--8570, 2021.

\bibitem{NEURIPS2021_6e8404c3}
J.~Pool and C.~Yu.
\newblock Channel permutations for n:m sparsity.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 13316--13327. Curran Associates, Inc., 2021.

\bibitem{rumi2020accelerating}
M.~A. Rumi, X.~Ma, Y.~Wang, and P.~Jiang.
\newblock Accelerating sparse cnn inference on gpus with performance-aware
  weight pruning.
\newblock In {\em Proceedings of the ACM International Conference on Parallel
  Architectures and Compilation Techniques}, pages 267--278, 2020.

\bibitem{schwarz2021powerpropagation}
J.~Schwarz, S.~Jayakumar, R.~Pascanu, P.~E. Latham, and Y.~Teh.
\newblock Powerpropagation: A sparsity inducing weight reparameterisation.
\newblock {\em Advances in Neural Information Processing Systems},
  34:28889--28903, 2021.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em International Conference on Learning Representations}, 2014.

\bibitem{srivastava2022beyond}
A.~Srivastava, A.~Rastogi, A.~Rao, A.~A.~M. Shoeb, A.~Abid, A.~Fisch, A.~R.
  Brown, A.~Santoro, A.~Gupta, A.~Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{strubell2019energy}
E.~Strubell, A.~Ganesh, and A.~McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock {\em arXiv preprint arXiv:1906.02243}, 2019.

\bibitem{su2021locally}
X.~Su, S.~You, T.~Huang, F.~Wang, C.~Qian, C.~Zhang, and C.~Xu.
\newblock Locally free weight sharing for network width search.
\newblock {\em arXiv preprint arXiv:2102.05258}, 2021.

\bibitem{sui2021chip}
Y.~Sui, M.~Yin, Y.~Xie, H.~Phan, S.~Aliari~Zonouz, and B.~Yuan.
\newblock Chip: Channel independence-based pruning for compact neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:24604--24616, 2021.

\bibitem{tanaka2020pruning}
H.~Tanaka, D.~Kunin, D.~L. Yamins, and S.~Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock {\em Advances in Neural Information Processing Systems.
  arXiv:2006.05467}, 2020.

\bibitem{tang2020scop}
Y.~Tang, Y.~Wang, Y.~Xu, D.~Tao, C.~Xu, C.~Xu, and C.~Xu.
\newblock Scop: Scientific control for reliable neural network pruning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:10936--10947, 2020.

\bibitem{2020SciPy-NMeth}
P.~Virtanen, R.~Gommers, T.~E. Oliphant, M.~Haberland, T.~Reddy, D.~Cournapeau,
  E.~Burovski, P.~Peterson, W.~Weckesser, J.~Bright, S.~J. {van der Walt},
  M.~Brett, J.~Wilson, K.~J. Millman, N.~Mayorov, A.~R.~J. Nelson, E.~Jones,
  R.~Kern, E.~Larson, C.~J. Carey, {\.I}.~Polat, Y.~Feng, E.~W. Moore,
  J.~{VanderPlas}, D.~Laxalde, J.~Perktold, R.~Cimrman, I.~Henriksen, E.~A.
  Quintero, C.~R. Harris, A.~M. Archibald, A.~H. Ribeiro, F.~Pedregosa, P.~{van
  Mulbregt}, and {SciPy 1.0 Contributors}.
\newblock {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock {\em Nature Methods}, 17:261--272, 2020.

\bibitem{Wang2020Picking}
C.~Wang, G.~Zhang, and R.~Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{you2019gate}
Z.~You, K.~Yan, J.~Ye, M.~Ma, and P.~Wang.
\newblock Gate decorator: Global filter pruning method for accelerating deep
  convolutional neural networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{yu2019autoslim}
J.~Yu and T.~Huang.
\newblock Autoslim: Towards one-shot architecture search for channel numbers.
\newblock {\em arXiv preprint arXiv:1903.11728}, 2019.

\bibitem{yuan2021mest}
G.~Yuan, X.~Ma, W.~Niu, Z.~Li, Z.~Kong, N.~Liu, Y.~Gong, Z.~Zhan, C.~He,
  Q.~Jin, et~al.
\newblock Mest: Accurate and fast memory-economic sparse training framework on
  the edge.
\newblock {\em Advances in Neural Information Processing Systems},
  34:20838--20850, 2021.

\bibitem{zhou2021learning}
A.~Zhou, Y.~Ma, J.~Zhu, J.~Liu, Z.~Zhang, K.~Yuan, W.~Sun, and H.~Li.
\newblock Learning n:m fine-grained structured sparse neural networks from
  scratch.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhu2017prune}
M.~H. Zhu and S.~Gupta.
\newblock To prune, or not to prune: Exploring the efficacy of pruning for
  model compression, 2018.

\end{thebibliography}
