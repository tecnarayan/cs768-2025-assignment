\begin{thebibliography}{10}

\bibitem{ali2020implicit}
Alnur Ali, Edgar Dobriban, and Ryan Tibshirani.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock In {\em International Conference on Machine Learning}, pages
  233--244. PMLR, 2020.

\bibitem{arnold1986lyapunov}
Ludwig Arnold, W~Kliemann, and E~Oeljeklaus.
\newblock Lyapunov exponents of linear stochastic systems.
\newblock In {\em Lyapunov exponents}, pages 85--125. Springer, 1986.

\bibitem{arnold1987large}
Ludwig Arnold and Wolfgang Kliemann.
\newblock Large deviations of linear stochastic differential equations.
\newblock In {\em Stochastic differential systems}, pages 115--151. Springer,
  1987.

\bibitem{arora2020dropout}
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro.
\newblock Dropout: Explicit forms and capacity control.
\newblock In {\em International Conference on Machine Learning}, pages
  351--361. PMLR, 2021.

\bibitem{azencot2020forecasting}
Omri Azencot, N~Benjamin Erichson, Vanessa Lin, and Michael~W. Mahoney.
\newblock Forecasting sequential data using consistent {K}oopman autoencoders.
\newblock In {\em International Conference on Machine Learning}, pages
  475--485. PMLR, 2020.

\bibitem{balakrishnan2020deep}
Kaushik Balakrishnan and Devesh Upadhyay.
\newblock Deep adversarial {K}oopman model for reaction-diffusion systems.
\newblock {\em arXiv preprint arXiv:2006.05547}, 2020.

\bibitem{bishop1995training}
Chris~M Bishop.
\newblock Training with noise is equivalent to {T}ikhonov regularization.
\newblock {\em Neural Computation}, 7(1):108--116, 1995.

\bibitem{blagoveshchenskii1961some}
Yurii~Nikolaevich Blagoveshchenskii and Mark~Iosifovich Freidlin.
\newblock Some properties of diffusion processes depending on a parameter.
\newblock In {\em Doklady Akademii Nauk}, volume 138, pages 508--511. Russian
  Academy of Sciences, 1961.

\bibitem{camuto2020explicit}
Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen~J Roberts, and
  Chris~C Holmes.
\newblock Explicit regularisation in {G}aussian noise injections.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 16603--16614, 2020.

\bibitem{chang2019antisymmetricrnn}
Bo~Chang, Minmin Chen, Eldad Haber, and Ed~H. Chi.
\newblock Antisymmetric{RNN}: A dynamical system view on recurrent neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{chen2018dynamical}
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz.
\newblock Dynamical isometry and a mean field theory of {R}{N}{N}s: Gating
  enables signal propagation in recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  873--882. PMLR, 2018.

\bibitem{chen2018neural}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6571--6583, 2018.

\bibitem{chen2019symplectic}
Zhengdao Chen, Jianyu Zhang, Martin Arjovsky, and L{\'e}on Bottou.
\newblock Symplectic recurrent neural networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{Das98}
S.~Das and O.~Olurotimi.
\newblock Noisy recurrent neural networks: the continuous-time case.
\newblock {\em IEEE Transactions on Neural Networks}, 9(5):913--936, 1998.

\bibitem{DLM19_Exact_TR}
Michal Derezinski, Feynman~T Liang, and Michael~W Mahoney.
\newblock Exact expressions for double descent and implicit regularization via
  surrogate random design.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 5152--5164, 2020.

\bibitem{dieng2018noisin}
Adji~Bousso Dieng, Rajesh Ranganath, Jaan Altosaar, and David Blei.
\newblock Noisin: Unbiased regularization for recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1252--1261. PMLR, 2018.

\bibitem{dogra2020optimizing}
Akshunna~S. Dogra and William Redman.
\newblock Optimizing neural networks via {K}oopman operator theory.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 2087--2097, 2020.

\bibitem{emami2021implicit}
Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, and
  Alyson~K Fletcher.
\newblock Implicit bias of linear {R}{N}{N}s.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, volume 139, pages 2982--2992. PMLR, 2021.

\bibitem{engelken2020lyapunov}
Rainer Engelken, Fred Wolf, and LF~Abbott.
\newblock Lyapunov spectra of chaotic recurrent neural networks.
\newblock {\em arXiv preprint arXiv:2006.02427}, 2020.

\bibitem{erichson2020lipschitz}
N.~Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and
  Michael~W. Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{erichson2019physics}
N~Benjamin Erichson, Michael Muehlebach, and Michael~W Mahoney.
\newblock Physics-informed autoencoders for {L}yapunov-stable fluid flow
  prediction.
\newblock {\em arXiv preprint arXiv:1905.10866}, 2019.

\bibitem{fang2019adaptive}
Wei Fang.
\newblock {\em Adaptive timestepping for {S}{D}{E}s with non-globally
  {L}ipschitz drift}.
\newblock PhD thesis, University of Oxford, 2019.

\bibitem{fang2020adaptive}
Wei Fang, Michael~B Giles, et~al.
\newblock Adaptive {E}uler--{M}aruyama method for {S}{D}{E}s with nonglobally
  {L}ipschitz drift.
\newblock {\em Annals of Applied Probability}, 30(2):526--560, 2020.

\bibitem{fraccaro2016sequential}
Marco Fraccaro, S\o ren~Kaae S\o~nderby, Ulrich Paquet, and Ole Winther.
\newblock Sequential neural models with stochastic layers.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~29, 2016.

\bibitem{freidlin1998random}
Mark~Iosifovich Freidlin and Alexander~D Wentzell.
\newblock {\em Random Perturbations of Dynamical Systems}.
\newblock Springer, 1998.

\bibitem{gall2016brownian}
J.F.L. Gall.
\newblock {\em Brownian Motion, Martingales, and Stochastic Calculus}.
\newblock Graduate Texts in Mathematics. Springer International Publishing,
  2016.

\bibitem{GM14_ICML}
D.~F. Gleich and M.~W. Mahoney.
\newblock Anti-differentiating approximation algorithms: A case study with
  min-cuts, spectral, and flow.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning}, pages 1018--1025, 2014.

\bibitem{goldberger2000physiobank}
Ary~L Goldberger, Luis~AN Amaral, Leon Glass, Jeffrey~M Hausdorff, Plamen~Ch
  Ivanov, Roger~G Mark, Joseph~E Mietus, George~B Moody, Chung-Kang Peng, and
  H~Eugene Stanley.
\newblock Physiobank, physiotoolkit, and physionet: components of a new
  research resource for complex physiologic signals.
\newblock {\em Circulation}, 101(23):e215--e220, 2000.

\bibitem{gong2020maxup}
Chengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu.
\newblock Maxup: Lightweight adversarial training with data augmentation
  improves neural network training.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2474--2483, 2021.

\bibitem{NEURIPS2019_26cd8eca}
Samuel Greydanus, Misko Dzamba, and Jason Yosinski.
\newblock Hamiltonian neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem{haber2017stable}
Eldad Haber and Lars Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock {\em Inverse Problems}, 34(1):014004, 2017.

\bibitem{higham2002strong}
Desmond~J Higham, Xuerong Mao, and Andrew~M Stuart.
\newblock Strong convergence of {E}uler-type methods for nonlinear stochastic
  differential equations.
\newblock {\em SIAM Journal on Numerical Analysis}, 40(3):1041--1063, 2002.

\bibitem{hodgkinson2020stochastic}
Liam Hodgkinson, Chris van~der Heide, Fred Roosta, and Michael~W Mahoney.
\newblock Stochastic continuous normalizing flows: Training {S}{D}{E}s as
  {O}{D}{E}s.
\newblock In {\em Uncertainty in Artificial Intelligence (UAI)}, 2021.

\bibitem{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em European Conference on Computer Vision}, pages 646--661.
  Springer, 2016.

\bibitem{hutzenthaler2015numerical}
Martin Hutzenthaler and Arnulf Jentzen.
\newblock {\em Numerical approximations of stochastic differential equations
  with non-globally {L}ipschitz continuous coefficients}, volume 236.
\newblock American Mathematical Society, 2015.

\bibitem{hutzenthaler2012strong}
Martin Hutzenthaler, Arnulf Jentzen, Peter~E Kloeden, et~al.
\newblock Strong convergence of an explicit numerical method for {S}{D}{E}s
  with nonglobally {L}ipschitz continuous coefficients.
\newblock {\em The Annals of Applied Probability}, 22(4):1611--1641, 2012.

\bibitem{jim1996analysis}
Kam-Chuen Jim, C~Lee Giles, and Bill~G Horne.
\newblock An analysis of noise in recurrent neural networks: Convergence and
  generalization.
\newblock {\em IEEE Transactions on Neural Networks}, 7(6):1424--1438, 1996.

\bibitem{Kag2020RNNs}
Anil Kag, Ziming Zhang, and Venkatesh Saligrama.
\newblock R{N}{N}s incrementally evolving on an equilibrium manifold: A panacea
  for vanishing and exploding gradients?
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{karatzas1998brownian}
Ioannis Karatzas and Steven~E Shreve.
\newblock {\em Brownian {M}otion}.
\newblock Springer, 1998.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{khalil2002nonlinear}
Hassan~K Khalil and Jessy~W Grizzle.
\newblock {\em Nonlinear Systems}, volume~3.
\newblock Prentice hall Upper Saddle River, NJ, 2002.

\bibitem{kidger2020neural}
Patrick Kidger, James Morrill, James Foster, and Terry Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 6696--6707, 2020.

\bibitem{kloeden2013numerical}
Peter~E Kloeden and Eckhard Platen.
\newblock {\em Numerical Solution of Stochastic Differential Equations},
  volume~23.
\newblock Springer Science \& Business Media, 2013.

\bibitem{kunita1984stochastic}
Hiroshi Kunita.
\newblock Stochastic differential equations and stochastic flows of
  diffeomorphisms.
\newblock In {\em Ecole d'{\'e}t{\'e} de Probabilit{\'e}s de Saint-Flour
  XII-1982}, pages 143--303. Springer, 1984.

\bibitem{le2015simple}
Quoc~V Le, Navdeep Jaitly, and Geoffrey~E Hinton.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock {\em arXiv preprint arXiv:1504.00941}, 2015.

\bibitem{lezcano2019cheap}
Mario Lezcano-Casado and David Mart{\i}nez-Rubio.
\newblock Cheap orthogonal constraints in neural networks: A simple
  parametrization of the orthogonal and unitary group.
\newblock In {\em International Conference on Machine Learning}, pages
  3794--3803, 2019.

\bibitem{li2019learning}
Yunzhu Li, Hao He, Jiajun Wu, Dina Katabi, and Antonio Torralba.
\newblock Learning compositional {K}oopman operators for model-based control.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{lim2020understanding}
Soon~Hoe Lim.
\newblock Understanding recurrent neural networks using nonequilibrium response
  theory.
\newblock {\em J. Mach. Learn. Res.}, 22:47--1, 2021.

\bibitem{liu2019neural}
Xuanqing Liu, Tesi Xiao, Si~Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh.
\newblock How does noise help robustness? {E}xplanation and exploration under
  the neural {S}{D}{E} framework.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 282--290, 2020.

\bibitem{lu2018beyond}
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock In {\em International Conference on Machine Learning}, pages
  3276--3285. PMLR, 2018.

\bibitem{lutter2019deep}
Michael Lutter, Christian Ritter, and Jan Peters.
\newblock Deep {L}agrangian networks: Using physics as model prior for deep
  learning.
\newblock {\em arXiv preprint arXiv:1907.04490}, 2019.

\bibitem{ma2020towards}
Chao Ma, Stephan Wojtowytsch, and Lei Wu.
\newblock Towards a mathematical understanding of neural network-based machine
  learning: What we know and what we don't.
\newblock {\em arXiv preprint arXiv:2009.10713}, 2020.

\bibitem{Mah12}
M.~W. Mahoney.
\newblock Approximate computation and implicit regularization for very
  large-scale data analysis.
\newblock In {\em Proceedings of the 31st ACM Symposium on Principles of
  Database Systems}, pages 143--154, 2012.

\bibitem{MO11-implementing}
M.~W. Mahoney and L.~Orecchia.
\newblock Implementing regularization implicitly via approximate eigenvector
  computation.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning}, pages 121--128, 2011.

\bibitem{malham2010introduction}
Simon~JA Malham and Anke Wiese.
\newblock An introduction to {S}{D}{E} simulation.
\newblock {\em arXiv preprint arXiv:1004.0646}, 2010.

\bibitem{mao1994exponential}
Xuerong Mao.
\newblock {\em Exponential Stability of Stochastic Differential Equations}.
\newblock Marcel Dekker, 1994.

\bibitem{mao2007stochastic}
Xuerong Mao.
\newblock {\em Stochastic Differential Equations and Applications}.
\newblock Elsevier, 2007.

\bibitem{miller2018stable}
John Miller and Moritz Hardt.
\newblock Stable recurrent models.
\newblock {\em arXiv preprint arXiv:1805.10369}, 2018.

\bibitem{morton2019deep}
Jeremy Morton, Freddie~D Witherden, and Mykel~J Kochenderfer.
\newblock Deep variational {K}oopman models: Inferring {K}oopman observations
  for uncertainty-aware dynamics modeling and control.
\newblock {\em arXiv preprint arXiv:1902.09742}, 2019.

\bibitem{noh2017regularizing}
Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, and Bohyung Han.
\newblock Regularizing deep neural networks by noise: Its interpretation and
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5109--5118, 2017.

\bibitem{pan2020physics}
Shaowu Pan and Karthik Duraisamy.
\newblock Physics-informed probabilistic learning of linear embeddings of
  nonlinear dynamics with guaranteed stability.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 19(1):480--509,
  2020.

\bibitem{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1310--1318. PMLR, 2013.

\bibitem{poggio2017theory}
Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco,
  Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar.
\newblock {Theory of deep learning III: Explaining the non-overfitting puzzle}.
\newblock {\em arXiv preprint arXiv:1801.00173}, 2017.

\bibitem{queiruga2020continuous}
Alejandro~F Queiruga, N~Benjamin Erichson, Dane Taylor, and Michael~W Mahoney.
\newblock Continuous-in-depth neural networks.
\newblock {\em arXiv preprint arXiv:2008.02389}, 2020.

\bibitem{rusch2021coupled}
T.~Konstantin Rusch and Siddhartha Mishra.
\newblock Coupled oscillatory recurrent neural network (co{R}{N}{N}): An
  accurate and (gradient) stable architecture for learning long time
  dependencies.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{sarkka2019applied}
Simo S{\"a}rkk{\"a} and Arno Solin.
\newblock {\em Applied Stochastic Differential Equations}, volume~10.
\newblock Cambridge University Press, 2019.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem{smith2021origin}
Samuel~L Smith, Benoit Dherin, David~GT Barrett, and Soham De.
\newblock On the origin of implicit regularization in stochastic gradient
  descent.
\newblock {\em arXiv preprint arXiv:2101.12176}, 2021.

\bibitem{sokolic2017generalization}
Jure Sokoli{\'c}, Raja Giryes, Guillermo Sapiro, and Miguel~RD Rodrigues.
\newblock Generalization error of deep neural networks: Role of classification
  margin and data structure.
\newblock In {\em 2017 International Conference on Sampling Theory and
  Applications (SampTA)}, pages 147--151. IEEE, 2017.

\bibitem{sokolic2017robust}
Jure Sokoli{\'c}, Raja Giryes, Guillermo Sapiro, and Miguel~RD Rodrigues.
\newblock Robust large margin deep neural networks.
\newblock {\em IEEE Transactions on Signal Processing}, 65(16):4265--4280,
  2017.

\bibitem{stutz2019disentangling}
David Stutz, Matthias Hein, and Bernt Schiele.
\newblock Disentangling adversarial robustness and generalization.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6976--6987, 2019.

\bibitem{sun2018stochastic}
Qi~Sun, Yunzhe Tao, and Qiang Du.
\newblock Stochastic training of residual networks: A differential equation
  viewpoint.
\newblock {\em arXiv preprint arXiv:1812.00174}, 2018.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{takeishi2017learning}
Naoya Takeishi, Yoshinobu Kawahara, and Takehisa Yairi.
\newblock Learning {K}oopman invariant subspaces for dynamic mode
  decomposition.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 1130--1140, 2017.

\bibitem{talathi2015improving}
Sachin~S Talathi and Aniket Vartak.
\newblock Improving performance of recurrent neural network with {R}e{L}{U}
  nonlinearity.
\newblock {\em arXiv preprint arXiv:1511.03771}, 2015.

\bibitem{toth2019hamiltonian}
Peter Toth, Danilo~J Rezende, Andrew Jaegle, S{\'e}bastien Racani{\`e}re,
  Aleksandar Botev, and Irina Higgins.
\newblock Hamiltonian generative networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{tzen2019neural}
Belinda Tzen and Maxim Raginsky.
\newblock Neural stochastic differential equations: Deep latent {G}aussian
  models in the diffusion limit.
\newblock {\em arXiv preprint arXiv:1905.09883}, 2019.

\bibitem{vogt2020lyapunov}
Ryan Vogt, Maximilian~Puelma Touzel, Eli Shlizerman, and Guillaume Lajoie.
\newblock On {L}yapunov exponents for {R}{N}{N}s: Understanding information
  propagation using dynamical systems tools.
\newblock {\em arXiv preprint arXiv:2006.14123}, 2020.

\bibitem{wei2020implicit}
Colin Wei, Sham Kakade, and Tengyu Ma.
\newblock The implicit and explicit regularization effects of dropout.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, pages 10181--10192, 2020.

\bibitem{wei2019data}
Colin Wei and Tengyu Ma.
\newblock Data-dependent sample complexity of deep neural networks via
  {L}ipschitz augmentation.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem{weinan2017proposal}
E~Weinan.
\newblock A proposal on machine learning via dynamical systems.
\newblock {\em Communications in Mathematics and Statistics}, 5(1):1--11, 2017.

\bibitem{xu2012robustness}
Huan Xu and Shie Mannor.
\newblock Robustness and generalization.
\newblock {\em Machine Learning}, 86(3):391--423, 2012.

\bibitem{yang2019dynamical}
Yibo Yang, Jianlong Wu, Hongyang Li, Xia Li, Tiancheng Shen, and Zhouchen Lin.
\newblock Dynamical system inspired adaptive time stepping controller for
  residual network families.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  34(04):6648--6655, 2020.

\bibitem{yao2018hessian}
Zhewei Yao, Amir Gholami, Qi~Lei, Kurt Keutzer, and Michael~W. Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4954--4964, 2018.

\bibitem{zaremba2014recurrent}
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
\newblock Recurrent neural network regularization.
\newblock {\em arXiv preprint arXiv:1409.2329}, 2014.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock {\em Communications of the ACM}, 64(3):107--115, 2021.

\bibitem{zhang2019stability}
Huishuai Zhang, Da~Yu, Mingyang Yi, Wei Chen, and Tie-yan Liu.
\newblock Stability and convergence theory for learning {R}es{N}et: A full
  characterization.
\newblock 2019.

\bibitem{zhang2019towards}
Jingfeng Zhang, Bo~Han, Laura Wynter, Bryan Kian~Hsiang Low, and Mohan
  Kankanhalli.
\newblock Towards robust {R}es{N}et: A small step but a giant leap.
\newblock In {\em Proceedings of the Twenty-Eighth International Joint
  Conference on Artificial Intelligence}, 2019.

\bibitem{zhao2020rnn}
Jingyu Zhao, Feiqing Huang, Jia Lv, Yanjie Duan, Zhen Qin, Guodong Li, and
  Guangjian Tian.
\newblock Do {R}{N}{N} and {L}{S}{T}{M} have long memory?
\newblock In {\em International Conference on Machine Learning}, pages
  11365--11375. PMLR, 2020.

\bibitem{zhong2019symplectic}
Yaofeng~Desmond Zhong, Biswadip Dey, and Amit Chakraborty.
\newblock Symplectic {O}{D}{E}-{N}et: Learning {H}amiltonian dynamics with
  control.
\newblock In {\em International Conference on Learning Representations}, 2019.

\end{thebibliography}
