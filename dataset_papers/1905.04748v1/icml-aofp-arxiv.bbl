\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Asl \& Yu(2017)Abbasi-Asl and Yu]{abbasi2017structural}
Abbasi-Asl, R. and Yu, B.
\newblock Structural compression of convolutional neural networks based on
  greedy filter pruning.
\newblock \emph{arXiv preprint arXiv:1705.07356}, 2017.

\bibitem[Alvarez \& Salzmann(2016)Alvarez and Salzmann]{alvarez2016learning}
Alvarez, J.~M. and Salzmann, M.
\newblock Learning the number of neurons in deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2270--2278, 2016.

\bibitem[Anwar et~al.(2017)Anwar, Hwang, and Sung]{anwar2017structured}
Anwar, S., Hwang, K., and Sung, W.
\newblock Structured pruning of deep convolutional neural networks.
\newblock \emph{ACM Journal on Emerging Technologies in Computing Systems
  (JETC)}, 13\penalty0 (3):\penalty0 32, 2017.

\bibitem[Castellano et~al.(1997)Castellano, Fanelli, and
  Pelillo]{castellano1997iterative}
Castellano, G., Fanelli, A.~M., and Pelillo, M.
\newblock An iterative pruning algorithm for feedforward neural networks.
\newblock \emph{IEEE transactions on Neural networks}, 8\penalty0 (3):\penalty0
  519--531, 1997.

\bibitem[Collobert \& Weston(2008)Collobert and Weston]{collobert2008unified}
Collobert, R. and Weston, J.
\newblock A unified architecture for natural language processing: Deep neural
  networks with multitask learning.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  160--167. ACM, 2008.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Computer Vision and Pattern Recognition, 2009. CVPR 2009.
  IEEE Conference on}, pp.\  248--255. IEEE, 2009.

\bibitem[Ding et~al.(2018)Ding, Ding, Han, and Tang]{ding2018auto}
Ding, X., Ding, G., Han, J., and Tang, S.
\newblock Auto-balanced filter pruning for efficient convolutional neural
  networks.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Ding et~al.(2019)Ding, Ding, Guo, and Han]{ding2019centripetal}
Ding, X., Ding, G., Guo, Y., and Han, J.
\newblock Centripetal sgd for pruning very deep convolutional networks with
  complicated structure.
\newblock \emph{arXiv preprint arXiv:1904.03837}, 2019.

\bibitem[GoogLe(2017{\natexlab{a}})]{Tensorflow-AlexNet}
GoogLe.
\newblock Tensorflow-alexnet.
\newblock
  \url{https://github.com/tensorflow/models/blob/master/research/slim/nets/alexnet.py},
  2017{\natexlab{a}}.

\bibitem[GoogLe(2017{\natexlab{b}})]{Tensorflow-slim}
GoogLe.
\newblock Tensorflow-slim.
\newblock \url{https://github.com/tensorflow/models/tree/master/research/slim},
  2017{\natexlab{b}}.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  1379--1387, 2016.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1135--1143, 2015{\natexlab{b}}.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993second}
Hassibi, B. and Stork, D.~G.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  164--171, 1993.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{International Conference on Computer Vision (ICCV)},
  volume~2, pp.\ ~6, 2017.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hu et~al.(2016)Hu, Peng, Tai, and Tang]{hu2016network}
Hu, H., Peng, R., Tai, Y.-W., and Tang, C.-K.
\newblock Network trimming: A data-driven neuron pruning approach towards
  efficient deep architectures.
\newblock \emph{arXiv preprint arXiv:1607.03250}, 2016.

\bibitem[Huang et~al.(2018)Huang, Zhou, You, and Neumann]{huang2018learning}
Huang, Q., Zhou, K., You, S., and Neumann, U.
\newblock Learning to prune filters in convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1801.07365}, 2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456, 2015.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and
  Zisserman]{jaderberg2014speeding}
Jaderberg, M., Vedaldi, A., and Zisserman, A.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock \emph{arXiv preprint arXiv:1405.3866}, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[LeCun et~al.(1990{\natexlab{a}})LeCun, Boser, Denker, Henderson,
  Howard, Hubbard, and Jackel]{lecun1990handwritten}
LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard,
  W.~E., and Jackel, L.~D.
\newblock Handwritten digit recognition with a back-propagation network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  396--404, 1990{\natexlab{a}}.

\bibitem[LeCun et~al.(1990{\natexlab{b}})LeCun, Denker, and
  Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990{\natexlab{b}}.

\bibitem[LeCun et~al.(1995)LeCun, Bengio, et~al.]{lecun1995convolutional}
LeCun, Y., Bengio, Y., et~al.
\newblock Convolutional networks for images, speech, and time series.
\newblock \emph{The handbook of brain theory and neural networks},
  3361\penalty0 (10):\penalty0 1995, 1995.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{2017 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  2755--2763. IEEE, 2017.

\bibitem[Luo \& Wu(2018)Luo and Wu]{luo2018autopruner}
Luo, J.-H. and Wu, J.
\newblock Autopruner: An end-to-end trainable filter pruning method for
  efficient deep model inference.
\newblock \emph{arXiv preprint arXiv:1805.08941}, 2018.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017thinet}
Luo, J.-H., Wu, J., and Lin, W.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  5058--5066, 2017.

\bibitem[Luo et~al.(2018)Luo, Zhang, Zhou, Xie, Wu, and Lin]{luo2018thinet}
Luo, J.-H., Zhang, H., Zhou, H.-Y., Xie, C.-W., Wu, J., and Lin, W.
\newblock Thinet: pruning cnn filters for a thinner net.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2498--2507. JMLR. org, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock 2016.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989using}
Mozer, M.~C. and Smolensky, P.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16, 1989.

\bibitem[Polyak \& Wolf(2015)Polyak and Wolf]{polyak2015channel}
Polyak, A. and Wolf, L.
\newblock Channel-level acceleration of deep face representations.
\newblock \emph{IEEE Access}, 3:\penalty0 2163--2175, 2015.

\bibitem[Roth \& Fischer(2008)Roth and Fischer]{roth2008group}
Roth, V. and Fischer, B.
\newblock The group-lasso for generalized linear models: uniqueness of
  solutions and efficient algorithms.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  848--855. ACM, 2008.

\bibitem[Sharma et~al.(2017)Sharma, Wolfe, and Raj]{sharma2017incredible}
Sharma, A., Wolfe, N., and Raj, B.
\newblock The incredible shrinking neural network: New perspectives on learning
  representations through the lens of pruning.
\newblock \emph{arXiv preprint arXiv:1701.04465}, 2017.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G.~E., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Wang et~al.(2017{\natexlab{a}})Wang, Zhang, Wang, and
  Hu]{wang2017structured}
Wang, H., Zhang, Q., Wang, Y., and Hu, H.
\newblock Structured probabilistic pruning for convolutional neural network
  acceleration.
\newblock \emph{arXiv preprint arXiv:1709.06994}, 2017{\natexlab{a}}.

\bibitem[Wang et~al.(2016)Wang, Xu, You, Tao, and Xu]{wang2016cnnpack}
Wang, Y., Xu, C., You, S., Tao, D., and Xu, C.
\newblock Cnnpack: Packing convolutional neural networks in the frequency
  domain.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  253--261, 2016.

\bibitem[Wang et~al.(2017{\natexlab{b}})Wang, Xu, Xu, and Tao]{wang2017beyond}
Wang, Y., Xu, C., Xu, C., and Tao, D.
\newblock Beyond filters: Compact feature map for portable deep model.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3703--3711, 2017{\natexlab{b}}.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2074--2082, 2016.

\bibitem[Xu et~al.(2018)Xu, Wang, Jia, An, and Wang]{xu2018globally}
Xu, K., Wang, X., Jia, Q., An, J., and Wang, D.
\newblock Globally soft filter pruning for efficient convolutional neural
  networks.
\newblock 2018.

\bibitem[Ye et~al.(2018)Ye, Lu, Lin, and Wang]{ye2018rethinking}
Ye, J., Lu, X., Lin, Z., and Wang, J.~Z.
\newblock Rethinking the smaller-norm-less-informative assumption in channel
  pruning of convolution layers.
\newblock \emph{arXiv preprint arXiv:1802.00124}, 2018.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{yu2018nisp}
Yu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V.~I., Han, X., Gao, M., Lin,
  C.-Y., and Davis, L.~S.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  9194--9203, 2018.

\bibitem[Zhou et~al.(2018)Zhou, Zhou, Hong, and Li]{zhou2018online}
Zhou, Z., Zhou, W., Hong, R., and Li, H.
\newblock Online filter weakening and pruning for efficient convnets.
\newblock In \emph{2018 IEEE International Conference on Multimedia and Expo
  (ICME)}, pp.\  1--6. IEEE, 2018.

\end{thebibliography}
