\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Per(2019)]{Personalizer}
Personalizer azure cognitive service, 2019.

\bibitem[Agarwal et~al.(2014)Agarwal, Beygelzimer, Hsu, Langford, and
  Telgarsky]{NIPS2014_8f1d4362}
Agarwal, A., Beygelzimer, A., Hsu, D.~J., Langford, J., and Telgarsky, M.~J.
\newblock Scalable non-linear learning with adaptive polynomial expansions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Bergstra \& Bengio(2012)Bergstra and Bengio]{Bergstra2012rs}
Bergstra, J. and Bengio, Y.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of Machine Learning Research}, 13:\penalty0 281--305,
  February 2012.

\bibitem[Bergstra et~al.(2015)Bergstra, Komer, Eliasmith, Yamins, and
  Cox]{Bergstra2015hyperopt}
Bergstra, J., Komer, B., Eliasmith, C., Yamins, D., and Cox, D.~D.
\newblock Hyperopt: a python library for model selection and hyperparameter
  optimization.
\newblock \emph{Computational Science {\&} Discovery}, 8\penalty0 (1):\penalty0
  014008, July 2015.

\bibitem[Blum et~al.()Blum, Kalai, and Langford]{progressive}
Blum, A., Kalai, A., and Langford, J.
\newblock Beating the hold-out: Bounds for k-fold and progressive
  cross-validation.
\newblock In \emph{Proceedings of the Twelfth Annual Conference on
  Computational Learning Theory (COLT) 1999}.

\bibitem[Cesa{-}Bianchi \& Lugosi(2006)Cesa{-}Bianchi and Lugosi]{PLG}
Cesa{-}Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge University Press, 2006.
\newblock ISBN 978-0-521-84108-5.

\bibitem[Chaudhuri et~al.(2009)Chaudhuri, Freund, and
  Hsu]{chaudhuri2009parameter}
Chaudhuri, K., Freund, Y., and Hsu, D.
\newblock A parameter-free hedging algorithm.
\newblock \emph{arXiv preprint arXiv:0903.2851}, 2009.

\bibitem[Cutkosky \& Boahen(2017)Cutkosky and Boahen]{cutkosky2017stochastic}
Cutkosky, A. and Boahen, K.~A.
\newblock Stochastic and adversarial online learning without hyperparameters.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Dai et~al.(2020)Dai, Chandar, Fazelnia, Carterette, and
  Lalmas]{dai2020model}
Dai, Z., Chandar, P., Fazelnia, G., Carterette, B., and Lalmas, M.
\newblock Model selection for production system via automated online
  experiments.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Domingos(2012)]{domingos2012few}
Domingos, P.
\newblock A few useful things to know about machine learning.
\newblock \emph{Communications of the ACM}, 55\penalty0 (10):\penalty0 78--87,
  2012.

\bibitem[Elsken et~al.(2019)Elsken, Metzen, and Hutter]{JMLR2019NAS}
Elsken, T., Metzen, J.~H., and Hutter, F.
\newblock Neural architecture search: A survey.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (55):\penalty0 1--21, 2019.

\bibitem[Eriksson et~al.(2019)Eriksson, Pearce, Gardner, Turner, and
  Poloczek]{eriksson2019scalable}
Eriksson, D., Pearce, M., Gardner, J., Turner, R.~D., and Poloczek, M.
\newblock Scalable global optimization via local bayesian optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Falkner et~al.(2018)Falkner, Klein, and Hutter]{falkner2018}
Falkner, S., Klein, A., and Hutter, F.
\newblock {BOHB}: Robust and efficient hyperparameter optimization at scale.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Feurer et~al.(2015)Feurer, Klein, Eggensperger, Springenberg, Blum,
  and Hutter]{feurer2015efficient}
Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M., and
  Hutter, F.
\newblock Efficient and robust automated machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Foster et~al.(2017)Foster, Kale, Mohri, and
  Sridharan]{foster2017parameter}
Foster, D.~J., Kale, S., Mohri, M., and Sridharan, K.
\newblock Parameter-free online learning via model selection.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Huang et~al.(2019)Huang, Wang, Ding, and
  Chaudhuri]{huang2019efficient}
Huang, S., Wang, C., Ding, B., and Chaudhuri, S.
\newblock Efficient identification of approximate best configuration of
  training in large datasets.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Koch et~al.(2018)Koch, Golovidov, Gardner, Wujek, Griffin, and
  Xu]{koch2018autotune}
Koch, P., Golovidov, O., Gardner, S., Wujek, B., Griffin, J., and Xu, Y.
\newblock Autotune: A derivative-free optimization framework for hyperparameter
  tuning.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, 2018.

\bibitem[Li et~al.(2017)Li, Jamieson, DeSalvo, Rostamizadeh, and
  Talwalkar]{ICLR:li2017hyperband}
Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A.
\newblock Hyperband: A novel bandit-based approach to hyperparameter
  optimization.
\newblock In \emph{The International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Luo \& Schapire(2015)Luo and Schapire]{luo2015achieving}
Luo, H. and Schapire, R.~E.
\newblock Achieving all with no parameters: Adanormalhedge.
\newblock In \emph{Conference on Learning Theory}, 2015.

\bibitem[Luo et~al.(2019)Luo, Wang, Zhou, Yao, Tu, Chen, Dai, and
  Yang]{luo2019autocross}
Luo, Y., Wang, M., Zhou, H., Yao, Q., Tu, W.-W., Chen, Y., Dai, W., and Yang,
  Q.
\newblock Autocross: Automatic feature crossing for tabular data in real-world
  applications.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, 2019.

\bibitem[Medress et~al.(1977)Medress, Cooper, Forgie, Green, Klatt, O'Malley,
  Neuburg, Newell, Reddy, Ritea, et~al.]{medress1977speech}
Medress, M.~F., Cooper, F.~S., Forgie, J.~W., Green, C., Klatt, D.~H.,
  O'Malley, M.~H., Neuburg, E.~P., Newell, A., Reddy, D., Ritea, B., et~al.
\newblock Speech understanding systems: Report of a steering committee.
\newblock \emph{Artificial Intelligence}, 9\penalty0 (3):\penalty0 307--316,
  1977.

\bibitem[Muthukumar et~al.(2019)Muthukumar, Ray, Sahai, and
  Bartlett]{muthukumar2019best}
Muthukumar, V., Ray, M., Sahai, A., and Bartlett, P.
\newblock Best of many worlds: Robust model selection for online supervised
  learning.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, 2019.

\bibitem[Orabona \& P{\'a}l(2016)Orabona and P{\'a}l]{orabona2016coin}
Orabona, F. and P{\'a}l, D.
\newblock Coin betting and parameter-free online learning.
\newblock \emph{arXiv preprint arXiv:1602.04128}, 2016.

\bibitem[Real et~al.(2020)Real, Liang, So, and Le]{real2020automl}
Real, E., Liang, C., So, D., and Le, Q.
\newblock Automl-zero: evolving machine learning algorithms from scratch.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Sabharwal et~al.(2016)Sabharwal, Samulowitz, and
  Tesauro]{sabharwal2016selecting}
Sabharwal, A., Samulowitz, H., and Tesauro, G.
\newblock Selecting near-optimal learners via incremental data allocation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2016.

\bibitem[Sato(2001)]{sato2001online}
Sato, M.-A.
\newblock Online model selection based on the variational bayes.
\newblock \emph{Neural computation}, 13\penalty0 (7):\penalty0 1649--1681,
  2001.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{10.5555/2621980}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, USA, 2014.
\newblock ISBN 1107057132.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Vanschoren et~al.(2014)Vanschoren, van Rijn, Bischl, and
  Torgo]{Vanschoren2014}
Vanschoren, J., van Rijn, J.~N., Bischl, B., and Torgo, L.
\newblock Openml: Networked science in machine learning.
\newblock \emph{SIGKDD Explor. Newsl.}, 15\penalty0 (2):\penalty0 49â€“60, June
  2014.

\bibitem[Vapnik(2013)]{vapnik2013nature}
Vapnik, V.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Wu, Huang, and
  Saied]{wang2021economical}
Wang, C., Wu, Q., Huang, S., and Saied, A.
\newblock Economical hyperparameter optimization with blended search strategy.
\newblock In \emph{The International Conference on Learning Representations
  (ICLR)}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Wu, Weimer, and
  Zhu]{wang2021flaml}
Wang, C., Wu, Q., Weimer, M., and Zhu, E.
\newblock Flaml: A fast and lightweight automl library.
\newblock In \emph{The Conference on Machine Learning and Systems (MLSys)},
  2021{\natexlab{b}}.

\bibitem[Wu et~al.(2021)Wu, Wang, and Huang]{wu2021cost}
Wu, Q., Wang, C., and Huang, S.
\newblock Frugal optimization for cost-related hyperparameters.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\end{thebibliography}
