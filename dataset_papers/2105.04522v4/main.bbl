\begin{thebibliography}{10}

\bibitem{Beyer_arXiv_2020_done_ImageNet}
Lucas Beyer, Olivier~J H{\'e}naff, Alexander Kolesnikov, Xiaohua Zhai, and
  A{\"a}ron van~den Oord.
\newblock Are we done with imagenet?
\newblock {\em arXiv preprint arXiv:2006.07159}, 2020.

\bibitem{Ghosh_AAAI_2017_MAE}
Aritra Ghosh, Himanshu Kumar, and PS~Sastry.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In {\em Proceedings of the Thirty-First AAAI Conference on Artificial
  Intelligence}, pages 1919--1925, 2017.

\bibitem{Zhang_NeurIPS_2018_Generalized_CE}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In {\em Advances in neural information processing systems}, pages
  8778--8788, 2018.

\bibitem{Wang_ICCV_2019_Symmetric_CE}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 322--330, 2019.

\bibitem{Ma_ICML_2020_Normalized_Loss}
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James
  Bailey.
\newblock Normalized loss functions for deep learning with noisy labels, 2020.

\bibitem{hendrycks2020augmix}
Dan Hendrycks, Norman Mu, Ekin~D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock In {\em International Conference on Learning Representation}, 2020.

\bibitem{Oliver_arXiv_2018_Realistic_Eval_SSL}
Avital Oliver, Augustus Odena, Colin Raffel, Ekin~D Cubuk, and Ian~J
  Goodfellow.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock {\em arXiv preprint arXiv:1804.09170}, 2018.

\bibitem{Lin_TIT_1991_JS_Divergence}
Jianhua Lin.
\newblock Divergence measures based on the shannon entropy.
\newblock {\em IEEE Transactions on Information theory}, 37(1):145--151, 1991.

\bibitem{Zhang17RethinkingGeneralization}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization, 2017.

\bibitem{Liu_ICML_2020_Peer_Loss}
Yang Liu and Hongyi Guo.
\newblock Peer loss functions: Learning from noisy labels without knowing noise
  rates.
\newblock In Hal~DaumÃ© III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 6226--6236. PMLR, 13--18 Jul
  2020.

\bibitem{Natarajan_NIPS_2013}
Nagarajan Natarajan, Inderjit~S Dhillon, Pradeep~K Ravikumar, and Ambuj Tewari.
\newblock Learning with noisy labels.
\newblock In {\em Advances in neural information processing systems}, pages
  1196--1204, 2013.

\bibitem{Sukhbaatar_ICLR_2015_confusion_matrix}
Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob
  Fergus.
\newblock Training convolutional networks with noisy labels.
\newblock In {\em Proceedings of the international conference on learning
  representation}, 2015.

\bibitem{Patrini_CVPR_2017}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1944--1952, 2017.

\bibitem{Han_NeurIPS_2018}
Bo~Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya~Zhang, and
  Masashi Sugiyama.
\newblock Masking: A new perspective of noisy supervision.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5836--5846, 2018.

\bibitem{Xia_NeurIPS_2019}
Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo~Han, Chen Gong, Gang Niu, and
  Masashi Sugiyama.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6838--6849, 2019.

\bibitem{Li_ICLR_2020_dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In {\em International Conference on Learning Representation}, 2020.

\bibitem{tack2021consistency}
Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung~Ju Hwang, and Jinwoo
  Shin.
\newblock Consistency regularization for adversarial robustness, 2021.

\bibitem{Xu_NeurIPS_2019_Information_Theoretic_Mutual_Info_Loss}
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang.
\newblock L\_dmi: A novel information-theoretic loss function for training deep
  nets robust to label noise.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6225--6236, 2019.

\bibitem{Wei_ICLR_2021_f_Divergence}
Jiaheng Wei and Yang Liu.
\newblock When optimizing f-divergence is robust with label noise.
\newblock In {\em International Conference on Learning Representation}, 2021.

\bibitem{csiszar1967fdiv}
I.~CSISZAR.
\newblock Information-type measures of difference of probability distributions
  and indirect observation.
\newblock {\em Studia Scientiarum Mathematicarum Hungarica}, 2:229--318, 1967.

\bibitem{nielsen2019JSMeans}
Frank Nielsen.
\newblock On the jensen–shannon symmetrization of distances relying on
  abstract means.
\newblock {\em Entropy}, 21(5), 2019.

\bibitem{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Menon, Richard Nock, and Lizhen Qu.
\newblock Making deep neural networks robust to label noise: a loss correction
  approach, 2017.

\bibitem{Lukasik_ICML_2020_label_smoothing_label_noisy}
Michal Lukasik, Srinadh Bhojanapalli, Aditya~Krishna Menon, and Sanjiv Kumar.
\newblock Does label smoothing mitigate label noise?
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{Reed_arXiv_2014_bootstrapping}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock {\em arXiv preprint arXiv:1412.6596}, 2014.

\bibitem{zhang2021learning}
Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, and Chao Chen.
\newblock Learning with feature-dependent label noise: A progressive approach,
  2021.

\bibitem{zheltonozhskii2021contrast}
Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex~M. Bronstein, and
  Or~Litany.
\newblock Contrast to divide: Self-supervised pre-training for learning with
  noisy labels, 2021.

\bibitem{liu2020earlylearning}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels,
  2020.

\bibitem{li2017webvision}
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc~Van Gool.
\newblock Webvision database: Visual learning and understanding from web data,
  2017.

\bibitem{Jiang18MentorNet}
Lu~Jiang, Zhenyuan Zhou, Thomas Leung, Jia Li, and Fei-Fei Li.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In {\em ICML}, 2018.

\bibitem{song2019selfie}
Hwanjun Song, Minseok Kim, and Jae-Gil Lee.
\newblock {SELFIE}: Refurbishing unclean samples for robust deep learning.
\newblock In {\em ICML}, 2019.

\bibitem{lee2017cleannet}
Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang.
\newblock Cleannet: Transfer learning for scalable image classifier training
  with label noise.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition ({CVPR})}, 2018.

\bibitem{cubuk2019randaugment}
Ekin~D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space, 2019.

\bibitem{devries2017cutout}
Terrance DeVries and Graham~W. Taylor.
\newblock Improved regularization of convolutional neural networks with cutout,
  2017.

\bibitem{li2019learning}
Junnan Li, Yongkang Wong, Qi~Zhao, and Mohan Kankanhalli.
\newblock Learning to learn from noisy labeled data, 2019.

\bibitem{Nguyen_ICLR_2020_self_ensemble}
Duc~Tam Nguyen, Chaithanya~Kumar Mummadi, Thi Phuong~Nhung Ngo, Thi Hoai~Phuong
  Nguyen, Laura Beggel, and Thomas Brox.
\newblock Self: Learning to filter noisy labels with self-ensembling.
\newblock In {\em International Conference on Learning Representation}, 2019.

\bibitem{Northcutt_arXiv_2017}
Curtis~G Northcutt, Tailin Wu, and Isaac~L Chuang.
\newblock Learning with confident examples: Rank pruning for robust
  classification with noisy labels.
\newblock {\em arXiv preprint arXiv:1705.01936}, 2017.

\bibitem{Tanaka_CVPR_2018_Joint_Optimization}
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5552--5560, 2018.

\bibitem{Vahdat_NeurIPS_2017_CRF}
Arash Vahdat.
\newblock Toward robustness against label noise in training deep discriminative
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5596--5605, 2017.

\bibitem{Iscen_ECCV_2020}
Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum, and Cordelia Schmid.
\newblock Graph convolutional networks for learning with few clean and many
  noisy labels.
\newblock In {\em Proceedings of the European Conference on Computer Vision},
  2020.

\bibitem{Seo_NeurIPS_2019}
Paul~Hongsuck Seo, Geeho Kim, and Bohyung Han.
\newblock Combinatorial inference against label noise.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1173--1183, 2019.

\bibitem{Szegedy_CVPR_2016_inception_label_smoothing}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{Miyato_PAMI_2018_VAT}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  41(8):1979--1993, 2018.

\bibitem{Berthelot_NeurIPS_2019_mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5049--5059, 2019.

\bibitem{Tarvainen_NIPS_2017_mean_teacher}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In {\em Advances in neural information processing systems}, pages
  1195--1204, 2017.

\end{thebibliography}
