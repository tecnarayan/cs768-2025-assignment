\begin{thebibliography}{10}

\bibitem{bello2021lambdanetworks}
Irwan Bello.
\newblock Lambdanetworks: Modeling long-range interactions without attention.
\newblock {\em arXiv preprint arXiv:2102.08602}, 2021.

\bibitem{beyer2020we}
Lucas Beyer, Olivier~J H{\'e}naff, Alexander Kolesnikov, Xiaohua Zhai, and
  A{\"a}ron van~den Oord.
\newblock Are we done with imagenet?
\newblock {\em arXiv preprint arXiv:2006.07159}, 2020.

\bibitem{brock2021high}
Andrew Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock {\em arXiv preprint arXiv:2102.06171}, 2021.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock {\em arXiv preprint arXiv:2005.12872}, 2020.

\bibitem{chefer2020transformer}
Hila Chefer, Shir Gur, and Lior Wolf.
\newblock Transformer interpretability beyond attention visualization.
\newblock {\em arXiv preprint arXiv:2012.09838}, 2020.

\bibitem{chen2021crossvit}
Chun-Fu Chen, Quanfu Fan, and Rameswar Panda.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image
  classification.
\newblock {\em arXiv preprint arXiv:2103.14899}, 2021.

\bibitem{chen2020pre}
Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei
  Ma, Chunjing Xu, Chao Xu, and Wen Gao.
\newblock Pre-trained image processing transformer.
\newblock {\em arXiv preprint arXiv:2012.00364}, 2020.

\bibitem{chen2018encoder}
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
  Adam.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 801--818, 2018.

\bibitem{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em International Conference on Machine Learning}, pages
  1691--1703. PMLR, 2020.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{dai2020up}
Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.
\newblock Up-detr: Unsupervised pre-training for object detection with
  transformers.
\newblock {\em arXiv preprint arXiv:2011.09094}, 2020.

\bibitem{d2021convit}
St{\'e}phane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio
  Biroli, and Levent Sagun.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock {\em arXiv preprint arXiv:2103.10697}, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{han2021transformer}
Kai Han, An~Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
\newblock Transformer in transformer.
\newblock {\em arXiv preprint arXiv:2103.00112}, 2021.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2961--2969, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{he2019bag}
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu~Li.
\newblock Bag of tricks for image classification with convolutional neural
  networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 558--567, 2019.

\bibitem{heo2021rethinking}
Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and
  Seong~Joon Oh.
\newblock Rethinking spatial dimensions of vision transformers.
\newblock {\em arXiv preprint arXiv:2103.16302}, 2021.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hou2020strip}
Qibin Hou, Li~Zhang, Ming-Ming Cheng, and Jiashi Feng.
\newblock Strip pooling: Rethinking spatial pooling for scene parsing.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4003--4012, 2020.

\bibitem{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em European conference on computer vision}, pages 646--661.
  Springer, 2016.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em arXiv preprint arXiv:2103.14030}, 2021.

\bibitem{long2015fully}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3431--3440, 2015.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{parmar2018image}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, {\L}ukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock {\em arXiv preprint arXiv:1802.05751}, 2018.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in neural information processing systems}, pages
  8026--8037, 2019.

\bibitem{srinivas2021bottleneck}
Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel,
  and Ashish Vaswani.
\newblock Bottleneck transformers for visual recognition.
\newblock {\em arXiv preprint arXiv:2101.11605}, 2021.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{sun2020rethinking}
Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.
\newblock Rethinking transformer-based set prediction for object detection.
\newblock {\em arXiv preprint arXiv:2011.10881}, 2020.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc~V Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock {\em arXiv preprint arXiv:1905.11946}, 2019.

\bibitem{tolstikhin2021mlp}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario
  Lucic, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em arXiv preprint arXiv:2105.01601}, 2021.

\bibitem{touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{touvron2021going}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock {\em arXiv preprint arXiv:2103.17239}, 2021.

\bibitem{touvron2019fixing}
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Fixing the train-test resolution discrepancy.
\newblock {\em arXiv preprint arXiv:1906.06423}, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems},
  30:5998--6008, 2017.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock {\em arXiv preprint arXiv:2102.12122}, 2021.

\bibitem{wang2020end}
Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen,
  and Huaxia Xia.
\newblock End-to-end video instance segmentation with transformers.
\newblock {\em arXiv preprint arXiv:2011.14503}, 2020.

\bibitem{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock {\em arXiv preprint arXiv:2103.15808}, 2021.

\bibitem{xiao2018unified}
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
\newblock Unified perceptual parsing for scene understanding.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 418--434, 2018.

\bibitem{yang2020learning}
Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo.
\newblock Learning texture transformer network for image super-resolution.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5791--5800, 2020.

\bibitem{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis~EH Tay, Jiashi
  Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock {\em arXiv preprint arXiv:2101.11986}, 2021.

\bibitem{yuan2020revisiting}
Li~Yuan, Francis~EH Tay, Guilin Li, Tao Wang, and Jiashi Feng.
\newblock Revisiting knowledge distillation via label smoothing regularization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3903--3911, 2020.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6023--6032, 2019.

\bibitem{yun2021relabel}
Sangdoo Yun, Seong~Joon Oh, Byeongho Heo, Dongyoon Han, Junsuk Choe, and
  Sanghyuk Chun.
\newblock Re-labeling imagenet: from single to multi-labels, from global to
  localized labels.
\newblock {\em arXiv preprint arXiv:2101.05022}, 2021.

\bibitem{zeng2020learning}
Yanhong Zeng, Jianlong Fu, and Hongyang Chao.
\newblock Learning joint spatial-temporal transformations for video inpainting.
\newblock In {\em European Conference on Computer Vision}, pages 528--543.
  Springer, 2020.

\bibitem{zhang2020resnest}
Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi~Zhu, Zhi Zhang, Haibin Lin, Yue
  Sun, Tong He, Jonas Muller, R.~Manmatha, Mu~Li, and Alexander Smola.
\newblock Resnest: Split-attention networks.
\newblock {\em arXiv preprint arXiv:2004.08955}, 2020.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{zhao2020point}
Hengshuang Zhao, Li~Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun.
\newblock Point transformer.
\newblock {\em arXiv preprint arXiv:2012.09164}, 2020.

\bibitem{zhao2017pyramid}
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
\newblock Pyramid scene parsing network.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2881--2890, 2017.

\bibitem{zheng2020end}
Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong.
\newblock End-to-end object detection with adaptive clustering transformer.
\newblock {\em arXiv preprint arXiv:2011.09315}, 2020.

\bibitem{zheng2020rethinking}
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang,
  Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip~HS Torr, et~al.
\newblock Rethinking semantic segmentation from a sequence-to-sequence
  perspective with transformers.
\newblock {\em arXiv preprint arXiv:2012.15840}, 2020.

\bibitem{zhong2020random}
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi~Yang.
\newblock Random erasing data augmentation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 13001--13008, 2020.

\bibitem{zhou2019semantic}
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
  and Antonio Torralba.
\newblock Semantic understanding of scenes through the ade20k dataset.
\newblock {\em International Journal of Computer Vision}, 127(3):302--321,
  2019.

\bibitem{zhou2021deepvit}
Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou,
  and Jiashi Feng.
\newblock Deepvit: Towards deeper vision transformer.
\newblock {\em arXiv preprint arXiv:2103.11886}, 2021.

\bibitem{zhou2018end}
Luowei Zhou, Yingbo Zhou, Jason~J Corso, Richard Socher, and Caiming Xiong.
\newblock End-to-end dense video captioning with masked transformer.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8739--8748, 2018.

\bibitem{zhu2020deformable}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable detr: Deformable transformers for end-to-end object
  detection.
\newblock {\em arXiv preprint arXiv:2010.04159}, 2020.

\end{thebibliography}
