@article{mcmahan2017survey,
  title={A survey of algorithms and analysis for adaptive online learning},
  author={McMahan, H Brendan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3117--3166},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{ma2009identifying,
  title={Identifying suspicious URLs: an application of large-scale online learning},
  author={Ma, Justin and Saul, Lawrence K and Savage, Stefan and Voelker, Geoffrey M},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={681--688},
  year={2009}
}

@inproceedings{mcmahan2013ad,
  title={Ad click prediction: a view from the trenches},
  author={McMahan, H Brendan and Holt, Gary and Sculley, David and Young, Michael and Ebner, Dietmar and Grady, Julian and Nie, Lan and Phillips, Todd and Davydov, Eugene and Golovin, Daniel and others},
  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1222--1230},
  year={2013}
}

@inproceedings{DBLP:journals/corr/abs-2107-01173,
  author    = {Guanghui Wang and
               Ming Yang and
               Lijun Zhang and
               Tianbao Yang},
  title     = {Momentum Accelerates the Convergence of Stochastic {AUPRC} Maximization},
  booktitle   = {Proceedings of International Conference on AI and Statistics},
  volume    = {abs/2107.01173},
  year      = {2022},
  url       = {https://arxiv.org/abs/2107.01173},
  eprinttype = {arXiv},
  eprint    = {2107.01173},
  timestamp = {Tue, 15 Mar 2022 16:14:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-01173.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{sculley2007relaxed,
  title={Relaxed online SVMs for spam filtering},
  author={Sculley, David and Wachman, Gabriel M},
  booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={415--422},
  year={2007}
}


@inproceedings{zhou2020online,
  title={Online AUC Optimization for Sparse High-Dimensional Datasets},
  author={Zhou, Baojian and Ying, Yiming and Skiena, Steven},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  pages={881--890},
  year={2020},
  organization={IEEE}
}

@article{feldman2018generalization,
  title={Generalization bounds for uniformly stable algorithms},
  author={Feldman, Vitaly and Vondrak, Jan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}


@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}


@inproceedings{zhou2020online,
  title={Online AUC Optimization for Sparse High-Dimensional Datasets},
  author={Zhou, Baojian and Ying, Yiming and Skiena, Steven},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  pages={881--890},
  year={2020},
  organization={IEEE}
}

@inproceedings{Steck2007HingeRL,
  title={Hinge Rank Loss and the Area Under the ROC Curve},
  author={Harald Steck},
  booktitle={ECML},
  year={2007}
}

@inproceedings{Cortes2003AUCOV,
  title={AUC Optimization vs. Error Rate Minimization},
  author={Corinna Cortes and Mehryar Mohri},
  booktitle={NIPS},
  year={2003}
}

@inproceedings{Bostrm2004PruningAE,
  title={Pruning and Exclusion Criteria for Unordered Incremental Reduced Error Pruning},
  author={Henrik Bostr{\"o}m},
  year={2004}
}

@article{ledell16,
author = {LeDell, Erin and Laan, Mark and Peterson, Maya},
year = {2016},
month = {05},
pages = {203-218},
title = {AUC-Maximizing Ensembles through Metalearning},
volume = {12},
journal = {The international journal of biostatistics},
doi = {10.1515/ijb-2015-0035}
}

@inproceedings{DBLP:conf/ecai/Gonen16,
  author={Mehmet Gönen},
  title={AUC Maximization in Bayesian Hierarchical Models},
  year={2016},
  cdate={1451606400000},
  pages={21-27},
  url={https://doi.org/10.3233/978-1-61499-672-9-21},
  booktitle={ECAI},
  crossref={conf/ecai/2016}
}

@article{nortonbuffauc,
author = {Norton, Matthew and Uryasev, Stan},
year = {2018},
month = {07},
pages = {1-38},
title = {Maximization of AUC and Buffered AUC in binary classification},
volume = {174},
journal = {Mathematical Programming},
doi = {10.1007/s10107-018-1312-2}
}

@article{10.1016/j.neucom.2010.01.001,
author = {Han, Guang and Zhao, Chunxia},
title = {AUC Maximization Linear Classifier Based on Active Learning and Its Application},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {73},
number = {7–9},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2010.01.001},
doi = {10.1016/j.neucom.2010.01.001},
abstract = {Aiming at labeling and ranking difficulties caused by a large number of samples, as well as uneven distribution of samples in outdoor obstacle detection of the autonomous mobile robot, an AUC maximization linear classifier method based on active learning is proposed in this paper. This method firstly uses dynamic clustering algorithm to select the representative samples and labels these samples, then these labeled samples are put in the training set. Next, a linear classifier is trained using the AUC maximization method on the training set. The above process will be repeated until the AUC converges. The experiments are performed on real outdoor environment image database. The experiment results show that the very good detection results are obtained using the method proposed in this paper with only 120 samples. More importantly, using the proposed method can significantly reduce the workload of labeling the samples and size of the sample set, and AUC maximization proposed also excels the existing methods.},
journal = {Neurocomput.},
month = {mar},
pages = {1272–1280},
numpages = {9},
keywords = {Linear classifier, AUC maximization, Gradient descent method, Dynamic clustering, Obstacle detection, Active learning}
}




@INPROCEEDINGS{4053043,
  author={Culver, Matt and Kun, Deng and Scott, Stephen},
  booktitle={Sixth International Conference on Data Mining (ICDM'06)}, 
  title={Active Learning to Maximize Area Under the ROC Curve}, 
  year={2006},
  volume={},
  number={},
  pages={149-158},
  doi={10.1109/ICDM.2006.12}}

@inproceedings{DBLP:conf/nips/PalaniappanB16,
  author    = {Balamurugan Palaniappan and
               Francis R. Bach},
  editor    = {Daniel D. Lee and
               Masashi Sugiyama and
               Ulrike von Luxburg and
               Isabelle Guyon and
               Roman Garnett},
  title     = {Stochastic Variance Reduction Methods for Saddle-Point Problems},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {1408--1416},
  year      = {2016},
  url       = {https://proceedings.neurips.cc/paper/2016/hash/1aa48fc4880bb0c9b8a3bf979d3b917e-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:22 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/PalaniappanB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
yuan2022compositional,
title={Compositional Training for End-to-End Deep {AUC} Maximization},
author={Zhuoning Yuan and Zhishuai Guo and Nitesh Chawla and Tianbao Yang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gPvB4pdu_Z}
}

@inproceedings{
zhang2018mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

@article{10.5555/1622407.1622416,
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
title = {SMOTE: Synthetic Minority over-Sampling Technique},
year = {2002},
issue_date = {January 2002},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {16},
number = {1},
issn = {1076-9757},
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of oversampling the minority (abnormal)cla ss and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space)tha n only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space)t han varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC)and the ROC convex hull strategy.},
journal = {J. Artif. Int. Res.},
month = {jun},
pages = {321–357},
numpages = {37}
}

@article{zhubenchmark,
  title={Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices},
  author={Dixian Zhu and Xiaodong Wu and   Tianbao Yang},
  journal={arXiv preprint},
  year={2022}
}

johnson2019survey

@article{johnson2019survey,
  title={Survey on deep learning with class imbalance},
  author={Johnson, Justin M and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={27},
  year={2019},
  publisher={Springer}
}

@article{Ba2016LayerN,
  title={Layer Normalization},
  author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@InProceedings{pmlr-v37-ioffe15,
  title = 	 {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author = 	 {Ioffe, Sergey and Szegedy, Christian},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {448--456},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ioffe15.html},
  abstract = 	 {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.}
}



@article{DBLP:journals/corr/abs-1709-01953,
  author    = {Behnam Neyshabur},
  title     = {Implicit Regularization in Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1709.01953},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.01953},
  eprinttype = {arXiv},
  eprint    = {1709.01953},
  timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-01953.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2112-05604,
  author    = {Junchi Yang and
               Antonio Orvieto and
               Aur{\'{e}}lien Lucchi and
               Niao He},
  title     = {Faster Single-loop Algorithms for Minimax Optimization without Strong
               Concavity},
  journal   = {CoRR},
  volume    = {abs/2112.05604},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.05604},
  eprinttype = {arXiv},
  eprint    = {2112.05604},
  timestamp = {Tue, 14 Dec 2021 14:21:31 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-05604.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={315--323},
  year={2013}
}

@Book{GoodBengCour16,
  Title                    = {Deep Learning},
  Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Address                  = {Cambridge, MA, USA},
  Note                     = {\url{http://www.deeplearningbook.org}}
}

@article{DBLP:journals/corr/abs-2005-11074,
  author    = {George Kyriakides and
               Konstantinos G. Margaritis},
  title     = {An Introduction to Neural Architecture Search for Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/2005.11074},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.11074},
  eprinttype = {arXiv},
  eprint    = {2005.11074},
  timestamp = {Thu, 28 May 2020 17:38:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-11074.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}


@InProceedings{pmlr-v70-gilmer17a,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}



@inproceedings{hu2019strategies,
  title={Strategies for Pre-training Graph Neural Networks},
  author={Hu, Weihua and Liu, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
  booktitle={Proceddings of the 7th international conference on learning representations},
  year={2019}
}

@inproceedings{
xu2018how,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={7th International Conference on Learning Representations},
year={2019}
}

@inproceedings{irvin2019chexpert,
  title={Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
  author={Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={590--597},
  year={2019}
}


@article{doi:10.1137/18M1178244,
author = {Davis, Damek and Drusvyatskiy, Dmitriy},
title = {Stochastic Model-Based Minimization of Weakly Convex Functions},
journal = {SIAM Journal on Optimization},
volume = {29},
number = {1},
pages = {207-239},
year = {2019},
doi = {10.1137/18M1178244},

URL = { 
        https://doi.org/10.1137/18M1178244
    
},
eprint = { 
        https://doi.org/10.1137/18M1178244
    
}
,
    abstract = { We consider a family of algorithms that successively sample and minimize simple stochastic models of the objective function. We show that under reasonable conditions on approximation quality and regularity of the models, any such algorithm drives a natural stationarity measure to zero at the rate \$O(k^{-1/4})\$. As a consequence, we obtain the first complexity guarantees for the stochastic proximal point, proximal subgradient, and regularized Gauss--Newton methods for minimizing compositions of convex functions with smooth maps. The guiding principle, underlying the complexity guarantees, is that all algorithms under consideration can be interpreted as approximate descent methods on an implicit smoothing of the problem, given by the Moreau envelope. Specializing to classical circumstances, we obtain the long-sought convergence rate of the stochastic projected gradient method, without batching, for minimizing a smooth function on a closed convex set. }
}




@inproceedings{yang2020-SHT,
  title={Stochastic Hard Thresholding Algorithms for AUC Maximization},
  author={Yang, Zhenhuan and Zhou, Baojian and Lei, Yunwen and Ying, Yiming},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  pages={741--750},
  year={2020},
  organization={IEEE}
}


@book{powell1981approximation,
  title={Approximation theory and methods},
  author={Powell, Michael James David and others},
  year={1981},
  publisher={Cambridge university press}
}


@inproceedings{zhang2015stochastic,
  title={Stochastic primal-dual coordinate method for regularized empirical risk minimization},
  author={Zhang, Yuchen and Lin, Xiao},
  booktitle={International Conference on Machine Learning},
  pages={353--361},
  year={2015},
  organization={PMLR}
}

@article{chen2014optimal,
  title={Optimal primal-dual methods for a class of saddle point problems},
  author={Chen, Yunmei and Lan, Guanghui and Ouyang, Yuyuan},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={1779--1814},
  year={2014},
  publisher={SIAM}
}

@article{sun2018markov,
  title={On markov chain gradient descent},
  author={Sun, Tao and Sun, Yuejiao and Yin, Wotao},
  journal={arXiv preprint arXiv:1809.04216},
  year={2018}
}

@inproceedings{nemirovskistochastic,
  title={Stochastic approximation approach to stochastic programming},
  author={Nemirovski, A and Juditsky, A and Lan, G and Shapiro, A},
  booktitle={SIAM J. Optim},
  organization={Citeseer}
}


@inproceedings{lei2021generalization,
  title={Generalization Guarantee of SGD for Pairwise Learning},
  author={Lei, Yunwen and Liu, Mingrui and Ying, Yiming},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@inproceedings{lei2020sharper,
  title={Sharper Generalization Bounds for Pairwise Learning.},
  author={Lei, Yunwen and Ledent, Antoine and Kloft, Marius},
  booktitle={NeurIPS},
  year={2020}
}


@article{guo2017online,
  title={Online regularized learning with pairwise loss functions},
  author={Guo, Zheng-Chu and Ying, Yiming and Zhou, Ding-Xuan},
  journal={Advances in Computational Mathematics},
  volume={43},
  number={1},
  pages={127--150},
  year={2017},
  publisher={Springer}
}

@inproceedings{shi2020quadruply,
  title={Quadruply Stochastic Gradient Method for Large Scale Nonlinear Semi-Supervised Ordinal Regression AUC Optimization},
  author={Shi, Wanli and Gu, Bin and Li, Xiang and Huang, Heng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5734--5741},
  year={2020}
}


@article{natole2019stochastic,
  title={Stochastic auc optimization algorithms with linear convergence},
  author={Natole Jr, Michael and Ying, Yiming and Lyu, Siwei},
  journal={Frontiers in Applied Mathematics and Statistics},
  volume={5},
  pages={30},
  year={2019},
  publisher={Frontiers}
}


@inproceedings{khalid2020proximal,
  title={Proximal Stochastic AUC Maximization},
  author={Khalid, Majdi and Chitsaz, Hamidreza and Ray, Indrakshi},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2020},
  organization={IEEE}
}


@inproceedings{Yan2003OptimizingCP,
  title={Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic},
  author={Lian Yan and Robert H. Dodier and Michael C. Mozer and Richard H. Wolniewicz},
  booktitle={ICML},
  year={2003}
}

@article{lei2021stochastic,
  title={Stochastic proximal AUC maximization},
  author={Lei, Yunwen and Ying, Yiming},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={61},
  pages={1--45},
  year={2021},
  publisher={Journal of Machine Learning Research}
}


@article{dang2020large,
  title={Large-scale nonlinear auc maximization via triply stochastic gradients},
  author={Dang, Zhiyuan and Li, Xiang and Gu, Bin and Deng, Cheng and Huang, Heng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}

@article{yang2020stochastic,
  title={Stochastic AUC optimization with general loss},
  author={Yang, Zhenhuan and Shen, Wei and Ying, Yiming and Yuan, Xiaoming},
  journal={Communications on Pure \& Applied Analysis},
  volume={19},
  number={8},
  year={2020}
}


@article{ying2016online,
  title={Online pairwise learning algorithms},
  author={Ying, Yiming and Zhou, Ding-Xuan},
  journal={Neural computation},
  volume={28},
  number={4},
  pages={743--777},
  year={2016},
  publisher={MIT Press}
}

@inproceedings{boissier2016fast,
  title={Fast convergence of online pairwise learning algorithms},
  author={Boissier, Martin and Lyu, Siwei and Ying, Yiming and Zhou, Ding-Xuan},
  booktitle={Artificial Intelligence and Statistics},
  pages={204--212},
  year={2016},
  organization={PMLR}
}



@inproceedings{gu2019scalable,
  title={Scalable and efficient pairwise learning to achieve statistical accuracy},
  author={Gu, Bin and Huo, Zhouyuan and Huang, Heng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3697--3704},
  year={2019}
}


@inproceedings{khalid2018scalable,
  title={Scalable nonlinear auc maximization methods},
  author={Khalid, Majdi and Ray, Indrakshi and Chitsaz, Hamidreza},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={292--307},
  year={2018},
  organization={Springer}
}




@inproceedings{yang2021simple,
  title={Simple Stochastic and Online Gradient Descent Algorithms for Pairwise Learning},
  author={Yang, Zhenhuan and Lei, Yunwen and Wang, Puyu and Yang, Tianbao and Ying, Yiming},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}


@inproceedings{shi2020quadruply,
  title={Quadruply Stochastic Gradient Method for Large Scale Nonlinear Semi-Supervised Ordinal Regression AUC Optimization},
  author={Shi, Wanli and Gu, Bin and Li, Xiang and Huang, Heng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5734--5741},
  year={2020}
}


@inproceedings{gu2019scalable,
  title={Scalable and efficient pairwise learning to achieve statistical accuracy},
  author={Gu, Bin and Huo, Zhouyuan and Huang, Heng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3697--3704},
  year={2019}
}



@inproceedings{yang2020stochastic,
  title={Stochastic Hard Thresholding Algorithms for AUC Maximization},
  author={Yang, Zhenhuan and Zhou, Baojian and Lei, Yunwen and Ying, Yiming},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  pages={741--750},
  year={2020},
  organization={IEEE}
}


@article{natole2019stochastic,
  title={Stochastic auc optimization algorithms with linear convergence},
  author={Natole Jr, Michael and Ying, Yiming and Lyu, Siwei},
  journal={Frontiers in Applied Mathematics and Statistics},
  volume={5},
  pages={30},
  year={2019},
  publisher={Frontiers}
}





@article{yang2020stochastic,
  title={Stochastic AUC optimization with general loss},
  author={Yang, Zhenhuan and Shen, Wei and Ying, Yiming and Yuan, Xiaoming},
  journal={Communications on Pure \& Applied Analysis},
  volume={19},
  number={8},
  year={2020}
}

@article{dang2020large,
  title={Large-scale nonlinear auc maximization via triply stochastic gradients},
  author={Dang, Zhiyuan and Li, Xiang and Gu, Bin and Deng, Cheng and Huang, Heng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}


@inproceedings{khalid2018scalable,
  title={Scalable nonlinear auc maximization methods},
  author={Khalid, Majdi and Ray, Indrakshi and Chitsaz, Hamidreza},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={292--307},
  year={2018},
  organization={Springer}
}


@inproceedings{dan2021variance,
  title={Variance reduced stochastic proximal algorithm for auc maximization},
  author={Dan, Soham and Sahoo, Dushyant},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={184--199},
  year={2021},
  organization={Springer}
}



@inproceedings{williams2001using,
  title={Using the Nystr{\"o}m method to speed up kernel machines},
  author={Williams, Christopher and Seeger, Matthias},
  booktitle={Proceedings of the 14th annual conference on neural information processing systems},
  number={CONF},
  pages={682--688},
  year={2001}
}


@inproceedings{rahimi2007random,
  title={Random Features for Large-Scale Kernel Machines.},
  author={Rahimi, Ali and Recht, Benjamin and others},
  booktitle={NIPS},
  volume={3},
  number={4},
  pages={5},
  year={2007},
  organization={Citeseer}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{ding2015adaptive,
  title={An adaptive gradient method for online auc maximization},
  author={Ding, Yi and Zhao, Peilin and Hoi, Steven and Ong, Yew-Soon},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={29},
  number={1},
  year={2015}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{cheng2018adaptive,
  title={An Adaptive Robust Online Method for AUC Maximization},
  author={Cheng, Fan and Zhang, Xia and Zhang, Chuang and Qiu, Jianfeng and Zhang, Lei},
  journal={IEEE Access},
  volume={6},
  pages={52004--52013},
  year={2018},
  publisher={IEEE}
}

@inproceedings{agarwal2013surrogate,
  title={Surrogate regret bounds for the area under the ROC curve via strongly proper losses},
  author={Agarwal, Shivani},
  booktitle={Conference on Learning Theory},
  pages={338--353},
  year={2013},
  organization={PMLR}
}

@article{hu2017online,
  title={Online nonlinear AUC maximization for imbalanced data sets},
  author={Hu, Junjie and Yang, Haiqin and Lyu, Michael R and King, Irwin and So, Anthony Man-Cho},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={4},
  pages={882--895},
  year={2017},
  publisher={IEEE}
}

@inproceedings{ding2017large,
  title={Large scale kernel methods for online auc maximization},
  author={Ding, Yi and Liu, Chenghao and Zhao, Peilin and Hoi, Steven CH},
  booktitle={2017 IEEE International Conference on Data Mining (ICDM)},
  pages={91--100},
  year={2017},
  organization={IEEE}
}

@inproceedings{szorenyi2017non,
  title={Non-parametric online auc maximization},
  author={Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Cohen, Snir and Mannor, Shie},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={575--590},
  year={2017},
  organization={Springer}
}


@article{liu2019adaptive,
  title={An Adaptive Moment estimation method for online AUC maximization},
  author={Liu, Xin and Pan, Zhisong and Yang, Haimin and Zhou, Xingyu and Bai, Wei and Niu, Xianghua},
  journal={Plos one},
  volume={14},
  number={4},
  pages={e0215426},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

@InProceedings{pmlr-v28-kar13,
  title = 	 {On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions},
  author = 	 {Kar, Purushottam and Sriperumbudur, Bharath and Jain, Prateek and Karnick, Harish},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  year = 	 {2013}
  }



@article{wang2013online,
  title={Online learning with pairwise loss functions},
  author={Wang, Yuyang and Khardon, Roni and Pechyony, Dmitry and Jones, Rosie},
  journal={arXiv preprint arXiv:1301.5332},
  year={2013}
}

@inproceedings{wang2012generalization,
  title={Generalization bounds for online learning algorithms with pairwise loss functions},
  author={Wang, Yuyang and Khardon, Roni and Pechyony, Dmitry and Jones, Rosie},
  booktitle={Conference on Learning Theory},
  pages={13--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}


@article{vitter1985random,
  title={Random sampling with a reservoir},
  author={Vitter, Jeffrey S},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={11},
  number={1},
  pages={37--57},
  year={1985},
  publisher={ACM New York, NY, USA}
}


@article{shalev2011online,
  title={Online learning and online convex optimization},
  author={Shalev-Shwartz, Shai and others},
  journal={Foundations and trends in Machine Learning},
  volume={4},
  number={2},
  pages={107--194},
  year={2011}
}

@article{orabona2019modern,
  title={A modern introduction to online learning},
  author={Orabona, Francesco},
  journal={arXiv preprint arXiv:1912.13213},
  year={2019}
}

@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th international conference on machine learning (icml-03)},
  pages={928--936},
  year={2003}
}


@article{hazan2019introduction,
  title={Introduction to online convex optimization},
  author={Hazan, Elad},
  journal={arXiv preprint arXiv:1909.05207},
  year={2019}
}

@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge university press}
}

 

@article{nesterov2005smooth,
  title={Smooth minimization of non-smooth functions},
  author={Nesterov, Yu},
  journal={Mathematical programming},
  volume={103},
  number={1},
  pages={127--152},
  year={2005},
  publisher={Springer}
}

@inproceedings{nesterov1983method,
  title={A method for unconstrained convex minimization problem with the rate of convergence O (1/k\^{} 2)},
  author={Nesterov, Yurii},
  booktitle={Doklady an ussr},
  volume={269},
  pages={543--547},
  year={1983}
}


@inproceedings{brefeld2005auc,
  title={AUC maximizing support vector learning},
  author={Brefeld, Ulf and Scheffer, Tobias and others},
  booktitle={Proceedings of the ICML 2005 workshop on ROC Analysis in Machine Learning},
  year={2005}
}


@article{Xinhua,
	author = {Zhang, X. and Saha, A. and Vishwanathan, SVN},
	date-added = {2017-08-27 16:44:38 +0000},
	date-modified = {2017-08-27 16:45:00 +0000},
	journal = {Journal of Machine Learning Research},
	pages = {3623--3680},
	title = {Smoothing multivariate performance measures},
	volume = {13},
	year = {2012}}


@inproceedings{Joachims,
	author = {Joachims, Thorsten},
	booktitle = {Proceedings of the 22nd international conference on Machine learning},
	date-added = {2017-08-27 14:16:04 +0000},
	date-modified = {2017-08-27 14:16:20 +0000},
	organization = {ACM},
	pages = {377--384},
	title = {A support vector method for multivariate performance measures},
	year = {2005}}



@inproceedings{calders2007efficient,
	author = {Calders, T and Jaroszewicz, S},
	booktitle = {PKDD},
	date-added = {2017-10-07 17:26:05 +0000},
	date-modified = {2017-10-07 17:26:15 +0000},
	organization = {Springer},
	pages = {42--53},
	title = {Efficient AUC optimization for classification},
	volume = {4702},
	year = {2007}}

@inproceedings{Pahikkala2008EfficientAM,
  title={Efficient AUC Maximization with Regularized Least-Squares},
  author={Tapio Pahikkala and Antti Airola and Hanna Suominen and Jorma Boberg and Tapio Salakoski},
  booktitle={SCAI},
  year={2008}
}

@inproceedings{herschtal2004optimising,
	author = {Herschtal, A and Raskutti, B},
	booktitle = {Proceedings of the twenty-first international conference on Machine learning},
	date-added = {2017-10-07 17:24:50 +0000},
	date-modified = {2017-10-07 17:24:59 +0000},
	organization = {ACM},
	pages = {49},
	title = {Optimising area under the ROC curve using gradient descent},
	year = {2004}}





@article{zhang2012smoothing,
  title={Smoothing multivariate performance measures},
  author={Zhang, Xinhua and Saha, Ankan and Vishwanathan, SVN},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={3623--3680},
  year={2012},
  publisher={JMLR. org}
}




@inproceedings{DBLP:conf/icml/HerschtalR04,
  author    = {Alan Herschtal and
               Bhavani Raskutti},
  editor    = {Carla E. Brodley},
  title     = {Optimising area under the {ROC} curve using gradient descent},
  booktitle = {Machine Learning, Proceedings of the Twenty-first International Conference
               {(ICML} 2004), Banff, Alberta, Canada, July 4-8, 2004},
  series    = {{ACM} International Conference Proceeding Series},
  volume    = {69},
  publisher = {{ACM}},
  year      = {2004},
  url       = {https://doi.org/10.1145/1015330.1015366},
  doi       = {10.1145/1015330.1015366},
  timestamp = {Tue, 06 Nov 2018 16:58:29 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/HerschtalR04.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Agarwal2011TheIP,
  title={The Infinite Push: A New Support Vector Ranking Algorithm that Directly Optimizes Accuracy at the Absolute Top of the List},
  author={Shivani Agarwal},
  booktitle={SDM},
  year={2011}
}

@inproceedings{Rakotomamonjy2012SparseSV,
  title={Sparse Support Vector Infinite Push},
  author={Alain Rakotomamonjy},
  booktitle={ICML},
  year={2012}
}

@article{JMLR:v10:rudin09b,
  author  = {Cynthia Rudin},
  title   = {The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  number  = {78},
  pages   = {2233-2271},
  url     = {http://jmlr.org/papers/v10/rudin09b.html}
}

@inproceedings{10.1145/1401890.1401980,
author = {Wu, Shan-Hung and Lin, Keng-Pei and Chen, Chung-Min and Chen, Ming-Syan},
title = {Asymmetric Support Vector Machines: Low False-Positive Learning under the User Tolerance},
year = {2008},
isbn = {9781605581934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401890.1401980},
doi = {10.1145/1401890.1401980},
abstract = {Many practical applications of classification require the classifier to produce a very low false-positive rate. Although the Support Vector Machine (SVM) has been widely applied to these applications due to its superiority in handling high dimensional data, there are relatively little effort other than setting a threshold or changing the costs of slacks to ensure the low false-positive rate. In this paper, we propose the notion of Asymmetric Support VectorMachine (ASVM) that takes into account the false-positives and the user tolerance in its objective. Such a new objective formulation allows us to raise the confidence in predicting the positives, and therefore obtain a lower chance of false-positives. We study the effects of the parameters in ASVM objective and address some implementation issues related to the Sequential Minimal Optimization (SMO) to cope with large-scale data. An extensive simulation is conducted and shows that ASVM is able to yield either noticeable improvement in performance or reduction in training time as compared to the previous arts.},
booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {749–757},
numpages = {9},
keywords = {low false-positive learning, classification, support vectormachine (svm)},
location = {Las Vegas, Nevada, USA},
series = {KDD '08}
}


@article{takashi12,
author = {Takenouchi, Takashi and Komori, Osamu and Eguchi, Shinto},
year = {2012},
month = {06},
pages = {2789-824},
title = {An Extension of the Receiver Operating Characteristic Curve and AUC-Optimal Classification},
volume = {24},
journal = {Neural computation},
doi = {10.1162/NECO_a_00336}
}

@inproceedings{sulam2017maximizing,
  title={Maximizing AUC with Deep Learning for Classification of Imbalanced Mammogram Datasets.},
  author={Sulam, Jeremias and Ben-Ari, Rami and Kisilev, Pavel},
  booktitle={VCBM},
  pages={131--135},
  year={2017}
}


@article{DBLP:journals/corr/abs-1803-06604,
  author    = {Ke Ren and
               Haichuan Yang and
               Yu Zhao and
               Mingshan Xue and
               Hongyu Miao and
               Shuai Huang and
               Ji Liu},
  title     = {A Robust {AUC} Maximization Framework with Simultaneous Outlier Detection
               and Feature Selection for Positive-Unlabeled Classification},
  journal   = {CoRR},
  volume    = {abs/1803.06604},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.06604},
  eprinttype = {arXiv},
  eprint    = {1803.06604},
  timestamp = {Tue, 25 May 2021 12:23:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-06604.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{YANG201774,
title = {Optimizing area under the ROC curve via extreme learning machines},
journal = {Knowledge-Based Systems},
volume = {130},
pages = {74-89},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117302241},
author = {Zhiyong Yang and Taohong Zhang and Jingcheng Lu and Dezheng Zhang and Dorothy Kalui},
keywords = {Extreme learning machine (ELM), Area under the ROC curve (AUC), Imbalanced datasets, Multi-class AUC optimization},
abstract = {Recently, Extreme learning machine (ELM), an efficient training algorithm for single-hidden-layer feedforward neural networks (SLFN), has gained increasing popularity in machine learning communities. In this paper the ELM based Area Under the ROC Curve (AUC) optimization algorithms are studied so as to further improve the performance of ELM for imbalanced datasets. For binary class problems, a novel ELM algorithm is proposed based on an efficient least square method. For multi-class problems, the following works are done in this paper: First of all, theoretical comparison analysis is proposed for the potential multi-class extensions of AUC; Secondly, a unified objective function for multi-class AUC optimization is proposed following the theoretical analysis; Subsequently, two ELM based multi-class AUC optimization algorithms called ELMMAUC and ELMmacroAUC respectively are proposed followed with complexity analyses; Finally, the generalization analysis is established for ELMMAUC in search of theoretical supports. Empirical study on a variety of real-world datasets show the effectiveness of our proposed algorithms.}
}

@book{10.5555/2678054,
author = {Shapiro, Alexander and Dentcheva, Darinka and Ruszczynski, Andrzej},
title = {Lectures on Stochastic Programming: Modeling and Theory, Second Edition},
year = {2014},
isbn = {1611973422},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {Optimization problems involving stochastic models occur in almost all areas of science and engineering, such as telecommunications, medicine, and finance. Their existence compels a need for rigorous ways of formulating, analyzing, and solving such problems. This book focuses on optimization problems involving uncertain parameters and covers the theoretical foundations and recent advances in areas where stochastic models are available. In Lectures on Stochastic Programming: Modeling and Theory, Second Edition, the authors introduce new material to reflect recent developments in stochastic programming, including: an analytical description of the tangent and normal cones of chance constrained sets; analysis of optimality conditions applied to nonconvex problems; a discussion of the stochastic dual dynamic programming method; an extended discussion of law invariant coherent risk measures and their Kusuoka representations; and in-depth analysis of dynamic risk measures and concepts of time consistency, including several new results. Audience: This book is intended for researchers working on theory and applications of optimization. It also is suitable as a text for advanced graduate courses in optimization.}
}



@article{10.1137/070704277,
author = {Nemirovski, A. and Juditsky, A. and Lan, G. and Shapiro, A.},
title = {Robust Stochastic Approximation Approach to Stochastic Programming},
year = {2009},
issue_date = {December 2008},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {19},
number = {4},
issn = {1052-6234},
url = {https://doi.org/10.1137/070704277},
doi = {10.1137/070704277},
abstract = {In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.},
journal = {SIAM J. on Optimization},
month = {jan},
pages = {1574–1609},
numpages = {36},
keywords = {Monte Carlo sampling, stochastic programming, stochastic approximation, sample average approximation method, saddle point, mirror descent algorithm, minimax problems, complexity}
}



@article{komori10,
author={Komori, Osamu and Eguchi, Shinto},
year={2010},
title={A boosting method for maximizing the partial area under the ROC curve},
journal={BMC Bioinformatics},
volume={11}
}


@inproceedings{NIPS2007_c5ff2543,
 author = {Long, Phil and Servedio, Rocco},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Boosting the Area under the ROC Curve},
 url = {https://proceedings.neurips.cc/paper/2007/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf},
 volume = {20},
 year = {2007}
}



@inproceedings{10.1145/279943.279960,
author = {Schapire, Robert E. and Singer, Yoram},
title = {Improved Boosting Algorithms Using Confidence-Rated Predictions},
year = {1998},
isbn = {1581130570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/279943.279960},
doi = {10.1145/279943.279960},
booktitle = {Proceedings of the Eleventh Annual Conference on Computational Learning Theory},
pages = {80–91},
numpages = {12},
location = {Madison, Wisconsin, USA},
series = {COLT' 98}
}



@inproceedings{10.5555/646943.712093,
author = {Freund, Yoav and Schapire, Robert E.},
title = {A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting},
year = {1995},
isbn = {3540591192},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the Second European Conference on Computational Learning Theory},
pages = {23–37},
numpages = {15},
series = {EuroCOLT '95}
}



@article{wang2017scgd,
  title={Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions},
  author={Wang, Mengdi and Fang, Ethan X and Liu, Han},
  journal={Mathematical Programming},
  volume={161},
  number={1-2},
  pages={419--449},
  year={2017},
  publisher={Springer}
}

@article{ghadimi2020nasa,
  title={A single timescale stochastic approximation method for nested stochastic optimization},
  author={Ghadimi, Saeed and Ruszczynski, Andrzej and Wang, Mengdi},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={1},
  pages={960--979},
  year={2020},
  publisher={SIAM}
}


@article{shijunsemiAUC,
author = {Wang, Shijun and Li, Diana and Petrick, Nicholas and Sahiner, Berkman and Linguraru, Marius George and Summers, Ronald},
year = {2015},
month = {01},
pages = {276--287},
title = {Optimizing area under the ROC curve using semi-supervised learning},
volume = {48},
journal = {Pattern Recognition},
doi = {10.1016/j.patcog.2014.07.025}
}

@article{DBLP:journals/ml/SakaiNS18,
  author    = {Tomoya Sakai and
               Gang Niu and
               Masashi Sugiyama},
  title     = {Semi-supervised {AUC} optimization based on positive-unlabeled learning},
  journal   = {Mach. Learn.},
  volume    = {107},
  number    = {4},
  pages     = {767--794},
  year      = {2018},
  url       = {https://doi.org/10.1007/s10994-017-5678-9},
  doi       = {10.1007/s10994-017-5678-9},
  timestamp = {Thu, 04 Feb 2021 08:46:17 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/SakaiNS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1007/978-3-642-13769-3_41,
author = {Lu, Xiaofen and Tang, Ke and Yao, Xin},
title = {Evolving Neural Networks with Maximum AUC for Imbalanced Data Classification},
year = {2010},
isbn = {3642137687},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-13769-3_41},
doi = {10.1007/978-3-642-13769-3_41},
abstract = {Real-world classification problems usually involve imbalanced data sets In such cases, a classifier with high classification accuracy does not necessarily imply a good classification performance for all classes The Area Under the ROC Curve (AUC) has been recognized as a more appropriate performance indicator in such cases Quite a few methods have been developed to design classifiers with the maximum AUC In the context of Neural Networks (NNs), however, it is usually an approximation of AUC rather than the exact AUC itself that is maximized, because AUC is non-differentiable and cannot be directly maximized by gradient-based methods In this paper, we propose to use evolutionary algorithms to train NNs with the maximum AUC The proposed method employs AUC as the objective function An evolutionary algorithm, namely the Self-adaptive Differential Evolution with Neighborhood Search (SaNSDE) algorithm, is used to optimize the weights of NNs with respect to AUC Empirical studies on 19 binary and multi-class imbalanced data sets show that the proposed evolutionary AUC maximization (EAM) method can train NN with larger AUC than existing methods.},
booktitle = {Proceedings of the 5th International Conference on Hybrid Artificial Intelligence Systems - Volume Part I},
pages = {335–342},
numpages = {8},
keywords = {class-imbalance learning, AUC, feed-forward neural networks, differential evolution, ROC, evolutionary algorithms},
location = {San Sebasti\'{a}n, Spain},
series = {HAIS'10}
}





@INPROCEEDINGS{4665906,
  author={Castro, Cristiano Leite and Braga, Antonio Padua},
  booktitle={2008 10th Brazilian Symposium on Neural Networks}, 
  title={Optimization of the Area under the ROC Curve}, 
  year={2008},
  volume={},
  number={},
  pages={141-146},
  doi={10.1109/SBRN.2008.25}}
  
@inproceedings{borkan2019nuanced,
  title={Nuanced metrics for measuring unintended bias with real data for text classification},
  author={Borkan, Daniel and Dixon, Lucas and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
  booktitle={Companion proceedings of the 2019 world wide web conference},
  pages={491--500},
  year={2019}
}

@article{kallus2019fairness,
  title={The fairness of risk scores beyond classification: Bipartite ranking and the x{A}uc metric},
  author={Kallus, Nathan and Zhou, Angela},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={3438--3448},
  year={2019}
}

@misc{zhao2022primaldual,
      title={A Primal-Dual Smoothing Framework for Max-Structured Non-Convex Optimization}, 
      author={Renbo Zhao},
      year={2022},
      eprint={2003.04375},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}


@article{doi:10.1137/17M1135086,
author = {Duchi, John C. and Ruan, Feng},
title = {Stochastic Methods for Composite and Weakly Convex Optimization Problems},
journal = {SIAM Journal on Optimization},
volume = {28},
number = {4},
pages = {3229-3259},
year = {2018},
doi = {10.1137/17M1135086},

URL = { 
    
        https://doi.org/10.1137/17M1135086
    
    

},
eprint = { 
    
        https://doi.org/10.1137/17M1135086
    
    

}
,
    abstract = { We consider minimization of stochastic functionals that are compositions of a (potentially) nonsmooth convex function \$h\$ and smooth function \$c\$ and, more generally, stochastic weakly convex functionals. We develop a family of stochastic methods---including a stochastic prox-linear algorithm and a stochastic (generalized) subgradient procedure---and prove that, under mild technical conditions, each converges to first order stationary points of the stochastic objective. We provide experiments further investigating our methods on nonsmooth phase retrieval problems; the experiments indicate the practical effectiveness of the procedures. }
}



@article{DBLP:journals/mp/DrusvyatskiyP19,
  author       = {Dmitriy Drusvyatskiy and
                  Courtney Paquette},
  title        = {Efficiency of minimizing compositions of convex functions and smooth
                  maps},
  journal      = {Math. Program.},
  volume       = {178},
  number       = {1-2},
  pages        = {503--558},
  year         = {2019},
  url          = {https://doi.org/10.1007/s10107-018-1311-3},
  doi          = {10.1007/s10107-018-1311-3},
  timestamp    = {Thu, 07 Nov 2019 09:20:17 +0100},
  biburl       = {https://dblp.org/rec/journals/mp/DrusvyatskiyP19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{DBLP:journals/corr/abs-2104-08736,
  author    = {Qi Qi and
               Youzhi Luo and
               Zhao Xu and
               Shuiwang Ji and
               Tianbao Yang},
  title     = {Stochastic Optimization of Area Under Precision-Recall Curve for Deep
               Learning with Provable Convergence},
  booktitle  = {Advances in neural information processing systems},
  volume    = {abs/2104.08736},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08736},
  eprinttype = {arXiv},
  eprint    = {2104.08736},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08736.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2102-06764,
  author    = {Valeriia Cherepanova and
               Vedant Nanda and
               Micah Goldblum and
               John P. Dickerson and
               Tom Goldstein},
  title     = {Technical Challenges for Training Fair Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2102.06764},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.06764},
  eprinttype = {arXiv},
  eprint    = {2102.06764},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-06764.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2003-00827,
  author    = {Laleh Seyyed{-}Kalantari and
               Guanxiong Liu and
               Matthew B. A. McDermott and
               Marzyeh Ghassemi},
  title     = {CheXclusion: Fairness gaps in deep chest X-ray classifiers},
  journal   = {CoRR},
  volume    = {abs/2003.00827},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.00827},
  eprinttype = {arXiv},
  eprint    = {2003.00827},
  timestamp = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-00827.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.5555/2503308.2503358,
author = {Zhang, Xinhua and Saha, Ankan and Vishwanathan, S. V. N.},
title = {Smoothing Multivariate Performance Measures},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {Optimizing multivariate performance measure is an important task in Machine Learning.
Joachims (2005) introduced a Support Vector Method whose underlying optimization problem
is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM. It can
be shown that CPMs converge to an ε accurate solution in O(1/λε) iterations, where
λ is the trade-off parameter between the regularizer and the loss function. Motivated
by the impressive convergence rate of CPM on a number of practical problems, it was
conjectured that these rates can be further improved. We disprove this conjecture
in this paper by constructing counter examples. However, surprisingly, we further
discover that these problems are not inherently hard, and we develop a novel smoothing
strategy, which in conjunction with Nesterov's accelerated gradient method, can find
an ε accurate solution in O* (min{1/ε, 1/√λε}) iterations. Computationally, our smoothing
technique is also particularly advantageous for optimizing multivariate performance
scores such as precision/recall break-even point and ROCArea; the cost per iteration
remains the same as that of CPMs. Empirical evaluation on some of the largest publicly
available data sets shows that our method converges significantly faster than CPMs
without sacrificing generalization ability.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {3623–3680},
numpages = {58},
keywords = {smoothing, max-margin methods, support vector machines, multivariate performance measures, non-smooth optimization}
}


@incollection{Herbrich1999d,
abstract = {In contrast to the standard machine learning tasks of classification and metric regression we investigate the problem of predicting variables of ordinal scale, a setting referred to as ordinal regression. This problem arises frequently in the social sciences and in information retrieval where human preferences play a major role. Whilst approaches proposed in statistics rely on a probability model of a latent (unobserved) variable we present a distribution independent risk formulation of ordinal regression which allows us to derive a uniform convergence bound. Applying this bound we present a large margin algorithm that is based on a mapping from objects to scalar utility values thus classifying pairs of objects. We give experimental results for an information retrieval task which show that our algorithm outperforms more naive approaches to ordinal regression such as Support Vector Classification and Support Vector Regression in the case of more than two ranks.},
author = {Herbrich, Ralf and Graepel, Thore and Obermayer, Klause},
booktitle = {Advances in Large Margin Classifiers},
chapter = {7},
file = {:Users/rherb/Code/herbrich.me/papers/nips98\_ordinal.pdf:pdf},
pages = {115--132},
publisher = {The MIT Press},
title = {{Large Margin Rank Boundaries for Ordinal Regression}},
url = {http://www.herbrich.me/papers/nips98\_ordinal.pdf},
year = {1999}
}


@TECHREPORT{Rakotomamonjy04supportvector,
    author = {Alain Rakotomamonjy},
    title = {Support vector machines and area under ROC curves},
    institution = {},
    year = {2004}
}

@article{DBLP:journals/ml/SakaiNS18,
  author    = {Tomoya Sakai and
               Gang Niu and
               Masashi Sugiyama},
  title     = {Semi-supervised {AUC} optimization based on positive-unlabeled learning},
  journal   = {Mach. Learn.},
  volume    = {107},
  number    = {4},
  pages     = {767--794},
  year      = {2018},
  url       = {https://doi.org/10.1007/s10994-017-5678-9},
  doi       = {10.1007/s10994-017-5678-9},
  timestamp = {Thu, 04 Feb 2021 08:46:17 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/SakaiNS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{GreenSwets66,
  added-at = {2008-09-16T23:39:07.000+0200},
  address = {New York},
  author = {Green, David M. and Swets, John A.},
  biburl = {https://www.bibsonomy.org/bibtex/2ac91092fcc1ca05cf7f1297313b893cc/brian.mingus},
  description = {CCNLab BibTeX},
  interhash = {c256c222fd6735a2d69442b4dd9f3ceb},
  intrahash = {ac91092fcc1ca05cf7f1297313b893cc},
  keywords = {cogn},
  publisher = {Wiley},
  timestamp = {2008-09-16T23:40:09.000+0200},
  title = {Signal Detection Theory and Psychophysics},
  year = 1966
}



@inproceedings{10.1145/1102351.1102399,
author = {Joachims, Thorsten},
title = {A Support Vector Method for Multivariate Performance Measures},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102351.1102399},
doi = {10.1145/1102351.1102399},
abstract = {This paper presents a Support Vector Method for optimizing multivariate nonlinear
performance measures like the F1-score. Taking a multivariate prediction approach,
we give an algorithm with which such multivariate SVMs can be trained in polynomial
time for large classes of potentially non-linear performance measures, in particular
ROCArea and all measures that can be computed from the contingency table. The conventional
classification SVM arises as a special case of our method.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {377–384},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}

@article{10.5555/945365.964285,
author = {Freund, Yoav and Iyer, Raj and Schapire, Robert E. and Singer, Yoram},
title = {An Efficient Boosting Algorithm for Combining Preferences},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
abstract = {We study the problem of learning to accurately rank a set of objects by combining
a given collection of ranking or preference functions. This problem of combining preferences
arises in several applications, such as that of combining the results of different
search engines, or the "collaborative-filtering" problem of ranking movies for a user
based on the movie rankings provided by other users. In this work, we begin by presenting
a formal framework for this general problem. We then describe and analyze an efficient
algorithm called RankBoost for combining preferences based on the boosting approach
to machine learning. We give theoretical results describing the algorithm's behavior
both on the training data, and on new test data not seen during training. We also
describe an efficient implementation of the algorithm for a particular restricted
but common case. We next discuss two experiments we carried out to assess the performance
of RankBoost. In the first experiment, we used the algorithm to combine different
web search strategies, each of which is a query expansion for a given domain. The
second experiment is a collaborative-filtering task for making movie recommendations.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {933–969},
numpages = {37}
}

@inproceedings{Li2014TopRO,
  title={Top Rank Optimization in Linear Time},
  author={Nan Li and Rong Jin and Zhi-Hua Zhou},
  booktitle={NIPS},
  year={2014}
}




@article{DBLP:journals/corr/abs-2010-00747,
  author    = {Yuhao Zhang and
               Hang Jiang and
               Yasuhide Miura and
               Christopher D. Manning and
               Curtis P. Langlotz},
  title     = {Contrastive Learning of Medical Visual Representations from Paired
               Images and Text},
  journal   = {CoRR},
  volume    = {abs/2010.00747},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.00747},
  eprinttype = {arXiv},
  eprint    = {2010.00747},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-00747.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2010-05352,
  author    = {Hari Sowrirajan and
               Jingbo Yang and
               Andrew Y. Ng and
               Pranav Rajpurkar},
  title     = {MoCo Pretraining Improves Representation and Transferability of Chest
               X-ray Models},
  journal   = {CoRR},
  volume    = {abs/2010.05352},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.05352},
  eprinttype = {arXiv},
  eprint    = {2010.05352},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-05352.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Iwata2020SemiSupervisedLF,
  title={Semi-Supervised Learning for Maximizing the Partial AUC},
  author={Tomoharu Iwata and Akinori Fujino and Naonori Ueda},
  booktitle={AAAI},
  year={2020}
}

@article{Komori2010ABM,
  title={A boosting method for maximizing the partial area under the ROC curve},
  author={Osamu Komori and Shinto Eguchi},
  journal={BMC Bioinformatics},
  year={2010},
  volume={11},
  pages={314 - 314}
}

@article{Narasimhan2017SupportVA,
  title={Support Vector Algorithms for Optimizing the Partial Area under the ROC Curve},
  author={Harikrishna Narasimhan and Shivani Agarwal},
  journal={Neural Computation},
  year={2017},
  volume={29},
  pages={1919-1963}
}

@inproceedings{10.1145/2487575.2487674,
author = {Narasimhan, Harikrishna and Agarwal, Shivani},
title = {SVMpAUCtight: A New Support Vector Method for Optimizing Partial AUC Based on a Tight Convex Upper Bound},
year = {2013},
isbn = {9781450321747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487575.2487674},
doi = {10.1145/2487575.2487674},
abstract = {The area under the ROC curve (AUC) is a well known performance measure in machine learning and data mining. In an increasing number of applications, however, ranging from ranking applications to a variety of important bioinformatics applications, performance is measured in terms of the partial area under the ROC curve between two specified false positive rates. In recent work, we proposed a structural SVM based approach for optimizing this performance measure (Narasimhan and Agarwal, 2013). In this paper, we develop a new support vector method, SVMpAUCtight, that optimizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced computational complexity. In particular, by rewriting the empirical partial AUC risk as a maximum over subsets of negative instances, we derive a new formulation, where a modified form of the earlier optimization objective is evaluated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the resulting optimization problem can be solved using a cutting-plane algorithm, but the new method has better run time guarantees. We also discuss a projected subgradient method for solving this problem, which offers additional computational savings in certain settings. We demonstrate on a wide variety of bioinformatics tasks, ranging from protein-protein interaction prediction to drug discovery tasks, that the proposed method does, in many cases, perform significantly better on the partial AUC measure than the previous structural SVM approach. In addition, we also develop extensions of our method to learn sparse and group sparse models, often of interest in biological applications.},
booktitle = {Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {167–175},
numpages = {9},
keywords = {cutting-plane method, partial auc, roc curve, svm},
location = {Chicago, Illinois, USA},
series = {KDD '13}
}



@InProceedings{pmlr-v28-narasimhan13,
  title = 	 {A Structural {SVM} Based Approach for Optimizing Partial AUC},
  author = 	 {Narasimhan, Harikrishna and Agarwal, Shivani},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {516--524},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/narasimhan13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/narasimhan13.html},
  abstract = 	 {The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking and biometric screening to medical diagnosis, performance is measured not in terms of the full area under the ROC curve, but instead, in terms of the partial area under the ROC curve between two specified false positive rates. In this paper, we develop a structural SVM framework for directly optimizing the partial AUC between any two false positive rates. Our approach makes use of a cutting plane solver along the lines of the structural SVM based approach for optimizing the full AUC developed by Joachims (2005). Unlike the full AUC, where the combinatorial optimization problem needed to find the most violated constraint in the cutting plane solver can be decomposed easily to yield an efficient algorithm, the corresponding optimization problem in the case of partial AUC is harder to decompose. One of our key technical contributions is an efficient algorithm for solving this combinatorial optimization problem that has the same computational complexity as Joachims’ algorithm for optimizing the usual AUC. This allows us to efficiently optimize the partial AUC in any desired false positive range. We demonstrate the approach on a variety of real-world tasks.}
}



@article{Ueda2018PartialAM,
  title={Partial AUC Maximization via Nonlinear Scoring Functions},
  author={Naonori Ueda and Akinori Fujino},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.04838}
}


@article{Morii2016MachinelearningSO,
  title={Machine-learning selection of optical transients in the Subaru/Hyper Suprime-Cam survey},
  author={Mikio Morii and Shiro Ikeda and Nozomu Tominaga and Masaomi Tanaka and Tomoki Morokuma and Katsuhiko Ishiguro and J. Yamato and Naonori Ueda and Nao Suzuki and Naoki Yasuda and Naoki Yoshida},
  journal={Publications of the Astronomical Society of Japan},
  year={2016},
  volume={68},
  pages={104}
}

@article{Ricamato2011PartialAM,
  title={Partial AUC maximization in a linear combination of dichotomizers},
  author={Maria Teresa Ricamato and Francesco Tortorella},
  journal={Pattern Recognit.},
  year={2011},
  volume={44},
  pages={2669-2677}
}

@misc{charoenphakdee2019symmetric,
      title={On Symmetric Losses for Learning from Corrupted Labels}, 
      author={Nontawat Charoenphakdee and Jongyeong Lee and Masashi Sugiyama},
      year={2019},
      eprint={1901.09314},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{Wang2011MarkerSV,
  title={Marker selection via maximizing the partial area under the ROC curve of linear risk scores.},
  author={Zhu Wang and Y.-C. I. Chang},
  journal={Biostatistics},
  year={2011},
  volume={12 2},
  pages={
          369-85
        }
}

@article{doi:10.1177/0272989X8900900307,
author = {Donna Katzman McClish},
title ={Analyzing a Portion of the ROC Curve},
journal = {Medical Decision Making},
volume = {9},
number = {3},
pages = {190-195},
year = {1989},
doi = {10.1177/0272989X8900900307},
    note ={PMID: 2668680},

URL = { 
        https://doi.org/10.1177/0272989X8900900307
    
},
eprint = { 
        https://doi.org/10.1177/0272989X8900900307
    
}
,
    abstract = { The area under the ROC curve is a common index summarizing the information contained in the curve. When comparing two ROC curves, though, problems arise when interest does not lie in the entire range of false-positive rates (and hence the entire area). Numerical integration is suggested for evaluating the area under a portion of the ROC curve. Variance estimates are derived. The method is applicable for either continuous or rating scale binormal data, from independent or dependent samples. An example is presented which looks at rating scale data of computed tomographic scans of the head with and without concomitant use of clinical history. The areas under the two ROC curves over an a priori range of false- positive rates are examined, as well as the areas under the two curves at a specific point. }
}



@inproceedings{DBLP:journals/corr/abs-2012-03173,
  author    = {Zhuoning Yuan and
               Yan Yan and
               Milan Sonka and
               Tianbao Yang},
  title     = {Robust Deep {AUC} Maximization: {A} New Surrogate Loss and Empirical
               Studies on Medical Image Classification},
 booktitle  = {Interntional Conference on Computer Vision},
  volume    = {abs/2012.03173},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.03173},
  eprinttype = {arXiv},
  eprint    = {2012.03173},
  timestamp = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-03173.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/YuanGXYY21,
  author    = {Zhuoning Yuan and
               Zhishuai Guo and
               Yi Xu and
               Yiming Ying and
               Tianbao Yang},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {Federated Deep {AUC} Maximization for Hetergeneous Data with a Constant
               Communication Complexity},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {12219--12229},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/yuan21a.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/YuanGXYY21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.5555/2968826.2968904,
author = {Kar, Purushottam and Narasimhan, Harikrishna and Jain, Prateek},
title = {Online and Stochastic Gradient Methods for Non-Decomposable Loss Functions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Modern applications in sensitive domains such as biometrics and medicine frequently
require the use of non-decomposable loss functions such as precision@k, F-measure
etc. Compared to point loss functions such as hinge-loss, these offer much more fine
grained control over prediction, but at the same time present novel challenges in
terms of algorithm design and analysis. In this work we initiate a study of online
learning techniques for such non-decomposable loss functions with an aim to enable
incremental learning as well as design scalable solvers for batch problems. To this
end, we propose an online learning framework for such loss functions. Our model enjoys
several nice properties, chief amongst them being the existence of efficient online
learning algorithms with sublinear regret and online to batch conversion bounds. Our
model is a provable extension of existing online learning models for point loss functions.
We instantiate two popular losses, Prec@k and pAUC, in our model and prove sublinear
regret bounds for both of them. Our proofs require a novel structural lemma over ranked
lists which may be of independent interest. We then develop scalable stochastic gradient
descent solvers for non-decomposable loss functions. We show that for a large family
of loss functions satisfying a certain uniform convergence property (that includes
Prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk
minimizer. Such uniform convergence results were not known for these losses and we
establish these using novel proof techniques. We then use extensive experimentation
on real life and benchmark datasets to establish that our method can be orders of
magnitude faster than a recently proposed cutting plane method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {694–702},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}


@misc{zhao2020primal,
      title={A Primal Dual Smoothing Framework for Max-Structured Nonconvex Optimization}, 
      author={Renbo Zhao},
      year={2020},
      eprint={2003.04375},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{luo2021near,
      title={Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced Convex-Concave Minimax Optimization}, 
      author={Luo Luo and Guangzeng Xie and Tong Zhang and Zhihua Zhang},
      year={2021},
      eprint={2106.01761},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@inproceedings{yang2020global,
  author    = {Junchi Yang and
               Negar Kiyavash and
               Niao He},
  title     = {Global Convergence and Variance Reduction for a Class of Nonconvex-Nonconcave
               Minimax Problems},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
  year      = {2020}
}

@article{wu2018moleculenet,
  title={{MoleculeNet}: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@article{DBLP:journals/corr/abs-2005-00687,
  author    = {Weihua Hu and
               Matthias Fey and
               Marinka Zitnik and
               Yuxiao Dong and
               Hongyu Ren and
               Bowen Liu and
               Michele Catasta and
               Jure Leskovec},
  title     = {Open Graph Benchmark: Datasets for Machine Learning on Graphs},
  journal   = {CoRR},
  volume    = {abs/2005.00687},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.00687},
  eprinttype = {arXiv},
  eprint    = {2005.00687},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00687.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bowyer1996digital,
  title={The digital database for screening mammography},
  author={Bowyer, K and Kopans, D and Kegelmeyer, WP and Moore, R and Sallam, M and Chang, K and Woods, K},
  booktitle={Third international workshop on digital mammography},
  volume={58},
  pages={27},
  year={1996}
}

@incollection{heath1998current,
  title={Current status of the digital database for screening mammography},
  author={Heath, Michael and Bowyer, Kevin and Kopans, Daniel and Kegelmeyer, P and Moore, Richard and Chang, Kyong and Munishkumaran, S},
  booktitle={Digital mammography},
  pages={457--460},
  year={1998},
  publisher={Springer}
}

@inproceedings{veeling2018rotation,
  title={Rotation equivariant CNNs for digital pathology},
  author={Veeling, Bastiaan S and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={210--218},
  year={2018},
  organization={Springer}
}

@article{bejnordi2017diagnostic,
  title={Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer},
  author={Bejnordi, Babak Ehteshami and Veta, Mitko and Van Diest, Paul Johannes and Van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and Van Der Laak, Jeroen AWM and Hermsen, Meyke and Manson, Quirine F and Balkenhol, Maschenka and others},
  journal={Jama},
  volume={318},
  number={22},
  pages={2199--2210},
  year={2017},
  publisher={American Medical Association}
}

@article{miller2006melanoma,
  title={Melanoma},
  author={Miller, Arlo J and Mihm Jr, Martin C},
  journal={New England Journal of Medicine},
  volume={355},
  number={1},
  pages={51--65},
  year={2006},
  publisher={Mass Medical Soc}
}

@article{rotemberg2020patient,
  title={A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context},
  author={Rotemberg, Veronica and Kurtansky, Nicholas and Betz-Stablein, Brigid and Caffery, Liam and Chousakos, Emmanouil and Codella, Noel and Combalia, Marc and Dusza, Stephen and Guitera, Pascale and Gutman, David and others},
  journal={arXiv preprint arXiv:2008.07360},
  year={2020}
}

@article{wang2020moleculekit,
  title={Advanced Graph and Sequence Neural Networks for Molecular Property Prediction and Drug Discovery},
  author={Wang, Zhengyang and Liu, Meng and Luo, Youzhi and Xu, Zhao and Xie, Yaochen and Wang, Limei and Cai, Lei and Qi, Qi and Yuan, Zhuoning and Yang, Tianbao and Ji, Shuiwang},
  journal={arXiv preprint arXiv:2012.01981},
  year={2020}
}


@article{he2021performance,
  title={Performance or Trust? Why Not Both. Deep AUC Maximization with Self-Supervised Learning for COVID-19 Chest X-ray Classifications},
  author={He, Siyuan and Xi, Pengcheng and Ebadi, Ashkan and Tremblay, Stephane and Wong, Alexander},
  journal={arXiv preprint arXiv:2112.08363},
  year={2021}
}

@misc{guo2021stochastic,
      title={On Stochastic Moving-Average Estimators for Non-Convex Optimization}, 
      author={Zhishuai Guo and Yi Xu and Wotao Yin and Rong Jin and Tianbao Yang},
      year={2021},
      eprint={2104.14840},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@inproceedings{DBLP:conf/icml/GuoLYSLY20,
  author    = {Zhishuai Guo and
               Mingrui Liu and
               Zhuoning Yuan and
               Li Shen and
               Wei Liu and
               Tianbao Yang},
  title     = {Communication-Efficient Distributed Stochastic {AUC} Maximization
               with Deep Neural Networks},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {3864--3874},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/guo20f.html},
  timestamp = {Tue, 15 Dec 2020 17:40:18 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/GuoLYSLY20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex SGD},
  author={Cutkosky, Ashok and Orabona, Francesco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15210--15219},
  year={2019}
}


@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{a}{{c}}, Martin},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  pages={2613--2621},
  year={2017}
}

@inproceedings{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={689--699},
  year={2018}
}


@article{sun2021algorithms,
  title={Algorithms for Difference-of-Convex (DC) Programs Based on Difference-of-Moreau-Envelopes Smoothing},
  author={Sun, Kaizhao and Sun, Xu Andy},
  journal={arXiv preprint arXiv:2104.01470},
  year={2021}
}

@article{paUCyao,
  title={Large-scale Optimization of Partial AUC in a Range of False Positive Rates},
  author={Yao Yao and Qihang Lin and   Tianbao Yang},
  journal={arXiv preprint},
  volume={arXiv:2203.01505},
  year={2022}
}

@inproceedings{otpaUC,
  title={When AUC meets DRO: Optimizing Partial AUC for Deep Learning with Non-Convex Convergence Guarantee},
  author={Dixian Zhu and Gang Li and Bokun Wang and Xiaodong Wu and   Tianbao Yang},
  booktitle={Proceedings of International Conference of Machine Learning},
  volume={arXiv:2203.00176},
  year={2022}
}

@article{aucgeneral,
author = {Yang, Zhenhuan and Shen, Wei and Ying, Yiming and Yuan, Xiaoming},
year = {2020},
month = {01},
pages = {4191-4212},
title = {Stochastic AUC optimization with general loss},
volume = {19},
journal = {Communications on Pure & Applied Analysis},
doi = {10.3934/cpaa.2020188}
}

@article{DBLP:journals/corr/abs-1904-10112,
  author    = {Yan Yan and
               Yi Xu and
               Qihang Lin and
               Lijun Zhang and
               Tianbao Yang},
  title     = {Stochastic Primal-Dual Algorithms with Faster Convergence than O(1/{\(\surd\)}T)
               for Problems without Bilinear Structure},
  journal   = {CoRR},
  volume    = {abs/1904.10112},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.10112},
  eprinttype = {arXiv},
  eprint    = {1904.10112},
  timestamp = {Thu, 14 Oct 2021 09:18:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-10112.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2006-06889,
  author    = {Zhishuai Guo and
               Zhuoning Yuan and
               Yan Yan and
               Tianbao Yang},
  title     = {Fast Objective and Duality Gap Convergence for Non-convex Strongly-concave
               Min-max Problems},
  journal   = {CoRR},
  volume    = {abs/2006.06889},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.06889},
  eprinttype = {arXiv},
  eprint    = {2006.06889},
  timestamp = {Wed, 17 Jun 2020 14:28:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-06889.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{schutte2021using,
      title={Using StyleGAN for Visual Interpretability of Deep Learning Models on Medical Images}, 
      author={Kathryn Schutte and Olivier Moindrot and Paul Hérent and Jean-Baptiste Schiratti and Simon Jégou},
      year={2021},
      eprint={2101.07563},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@inproceedings{hase2019interpretable,
  title={Interpretable image recognition with hierarchical prototypes},
  author={Hase, Peter and Chen, Chaofan and Li, Oscar and Rudin, Cynthia},
  booktitle={Proceedings of the AAAI Conference on Human Computation and Crowdsourcing},
  volume={7},
  number={1},
  pages={32--40},
  year={2019}
}
@article{arik2020protoattend,
  title={Protoattend: Attention-based prototypical learning},
  author={Ar{\i}k, Sercan O and Pfister, Tomas},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--35},
  year={2020}
}



@article{chen2018looks,
  title={This looks like that: deep learning for interpretable image recognition},
  author={Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
  journal={arXiv preprint arXiv:1806.10574},
  year={2018}
}


@misc{azizi2021big,
      title={Big Self-Supervised Models Advance Medical Image Classification}, 
      author={Shekoofeh Azizi and Basil Mustafa and Fiona Ryan and Zachary Beaver and Jan Freyberg and Jonathan Deaton and Aaron Loh and Alan Karthikesalingam and Simon Kornblith and Ting Chen and Vivek Natarajan and Mohammad Norouzi},
      year={2021},
      eprint={2101.05224},
      archivePrefix={arXiv},
      primaryClass={eess.IV}
}

@inproceedings{luo2020stochastic,
  author    = {Luo Luo and
               Haishan Ye and
               Zhichao Huang and
               Tong Zhang},
  title     = {Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave
               Minimax Problems},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
  year      = {2020}
}

@article{DBLP:journals/corr/abs-2008-08170,
  title={Accelerated zeroth-order momentum methods from mini to minimax optimization},
  author={Huang, Feihu and Gao, Shangqian and Pei, Jian and Huang, Heng},
  journal={arXiv preprint arXiv:2008.08170},
  year={2020}
}

@inproceedings{DBLP:conf/nips/Tran-DinhLN20,
  author    = {Quoc Tran{-}Dinh and
               Deyi Liu and
               Lam M. Nguyen},
  title     = {Hybrid Variance-Reduced {SGD} Algorithms For Minimax Problems with
               Nonconvex-Linear Function},
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS)},
  year      = {2020}
} 

@inproceedings{gao2013one,
  title={One-pass AUC optimization},
  author={Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
  booktitle={International conference on machine learning},
  pages={906--914},
  year={2013}
}


@article{2020arXiv200713605I,
  title={Alternating proximal-gradient steps for (stochastic) nonconvex-concave minimax problems},
  author={Bot, Radu Ioan and Bohm, Axel},
  journal={arXiv preprint arXiv:2007.13605},
  year={2020}
}





@inproceedings{lin2019gradient,
  author    = {Tianyi Lin and
               Chi Jin and
               Michael I. Jordan},
  title     = {On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning (ICML)},
  pages     = {6083--6093},
  year      = {2020}
}





@article{lei2021stochastic,
  title={Stochastic proximal AUC maximization},
  author={Lei, Yunwen and Ying, Yiming},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={61},
  pages={1--45},
  year={2021},
  publisher={Journal of Machine Learning Research}
}


@inproceedings{natole2018stochastic,
  title={Stochastic proximal algorithms for AUC maximization},
  author={Natole, Michael and Ying, Yiming and Lyu, Siwei},
  booktitle={International Conference on Machine Learning},
  pages={3710--3719},
  year={2018}
}


@article{Natole2019StochasticAO,
  title={Stochastic AUC Optimization Algorithms With Linear Convergence},
  author={Michael Natole and Yiming Ying and Siwei Lyu},
  journal={Frontiers in Applied Mathematics and Statistics},
  year={2019},
  volume={5}
}

@inproceedings{liu2018fast,
  title={Fast Stochastic AUC Maximization with $ O (1/n) $-Convergence Rate},
  author={Liu, Mingrui and Zhang, Xiaoxuan and Chen, Zaiyi and Wang, Xiaoyu and Yang, Tianbao},
  booktitle={International Conference on Machine Learning},
  pages={3189--3197},
  year={2018}
}



@inproceedings{ying2016stochastic,
  title={Stochastic online AUC maximization},
  author={Ying, Yiming and Wen, Longyin and Lyu, Siwei},
  booktitle={Advances in neural information processing systems},
  pages={451--459},
  year={2016}
}


@inproceedings{10.5555/2968826.2968904,
author = {Kar, Purushottam and Narasimhan, Harikrishna and Jain, Prateek},
title = {Online and Stochastic Gradient Methods for Non-Decomposable Loss Functions},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Modern applications in sensitive domains such as biometrics and medicine frequently
require the use of non-decomposable loss functions such as precision@k, F-measure
etc. Compared to point loss functions such as hinge-loss, these offer much more fine
grained control over prediction, but at the same time present novel challenges in
terms of algorithm design and analysis. In this work we initiate a study of online
learning techniques for such non-decomposable loss functions with an aim to enable
incremental learning as well as design scalable solvers for batch problems. To this
end, we propose an online learning framework for such loss functions. Our model enjoys
several nice properties, chief amongst them being the existence of efficient online
learning algorithms with sublinear regret and online to batch conversion bounds. Our
model is a provable extension of existing online learning models for point loss functions.
We instantiate two popular losses, Prec@k and pAUC, in our model and prove sublinear
regret bounds for both of them. Our proofs require a novel structural lemma over ranked
lists which may be of independent interest. We then develop scalable stochastic gradient
descent solvers for non-decomposable loss functions. We show that for a large family
of loss functions satisfying a certain uniform convergence property (that includes
Prec@k, pAUC, and F-measure), our methods provably converge to the empirical risk
minimizer. Such uniform convergence results were not known for these losses and we
establish these using novel proof techniques. We then use extensive experimentation
on real life and benchmark datasets to establish that our method can be orders of
magnitude faster than a recently proposed cutting plane method.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 1},
pages = {694–702},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}


@InProceedings{pmlr-v28-kar13,
  title = 	 {On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions},
  author = 	 {Kar, Purushottam and Sriperumbudur, Bharath and Jain, Prateek and Karnick, Harish},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {441--449},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/kar13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/kar13.html},
  abstract = 	 {In this paper, we study the generalization properties of online learning based stochastic methods for supervised learning problems where the loss function is dependent on more than one training sample (e.g., metric learning, ranking). We present a generic decoupling technique that enables us to provide Rademacher complexity-based generalization error bounds. Our bounds are in general tighter than those obtained by Wang et al. (COLT 2012) for the same problem. Using our decoupling technique, we are further able to obtain fast convergence rates for strongly con-vex pairwise loss functions. We are also able to analyze a class of memory efficient on-line learning algorithms for pairwise learning problems that use only a bounded subset of past training samples to update the hypothesis at each step. Finally, in order to complement our generalization bounds, we propose a novel memory efficient online learning algorithm for higher order learning problems with bounded regret guarantees.}
}


@inproceedings{Zhaoicml11,
  author    = {Peilin Zhao and
               Steven C. H. Hoi and
               Rong Jin and
               Tianbao Yang},
  title     = {Online AUC Maximization},
  booktitle = {ICML},
  year      = {2011},
  pages     = {233-240}
  }


@inproceedings{ca3e551a365f44598f995ed69ca9d2bc,
title = "Learning Decision Trees Using the Area Under the ROC Curve",
abstract = "ROC analysis is increasingly being recognised as an important tool for evaluation and comparison of classifiers when the operating characteristics (i.e. class distribution and cost parameters) are not known at training time. Usually, each classifier is characterised by its estimated true and false positive rates and is represented by a single point in the ROC diagram. In this paper, we show how a single decision tree can represent a set of classifiers by choosing different labellings of its leaves, or equivalently, an ordering on the leaves. In this setting, rather than estimating the accuracy of a single tree, it makes more sense to use the area under the ROC curve (AUC) as a quality metric. We also propose a novel splitting criterion which chooses the split with the highest local AUC. To the best of our knowledge, this is the first probabilistic splitting criterion that is not based on weighted average impurity. We present experiments suggesting that the AUC splitting criterion leads to trees with equal or better AUC value, without sacrificing accuracy if a single labelling is chosen. ",
author = "C Ferri and PA Flach and J Hern{\'a}ndez-Orallo",
year = "2002",
language = "English",
isbn = "1558608737",
pages = "139 -- 146",
editor = "Claude Sammut and Achim Hoffmann",
booktitle = "Proceedings of the 19th International Conference on Machine Learning",
publisher = "Morgan Kaufmann",
}



@inproceedings{10.5555/3041838.3041945,
author = {Yan, Lian and Dodier, Robert and Mozer, Michael C. and Wolniewicz, Richard},
title = {Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
abstract = {When the goal is to achieve the best correct classification rate, cross entropy and
mean squared error are typical cost functions used to optimize classifier performance.
However, for many real-world classification problems, the ROC curve is a more meaningful
performance measure. We demonstrate that minimizing cross entropy or mean squared
error does not necessarily maximize the area under the ROC curve (AUC). We then consider
alternative objective functions for training a classifier to maximize the AUC directly.
We propose an objective function that is an approximation to the Wilcoxon-Mann-Whitney
statistic, which is equivalent to the AUC. The proposed objective function is differentiable,
so gradient-based methods can be used to train the classifier. We apply the new objective
function to real-world customer behavior prediction problems for a wireless service
provider and a cable service provider, and achieve reliable improvements in the ROC
curve.},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {848–855},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML'03}
}


@inproceedings{aistats17,
	Author = {Elad Eban and Mariano Schain and Alan Mackey and Ariel Gordon and Rif A. Saurous and Gal Elidan},
	Booktitle = {International Conference on Arti cial Intelligence and Statistics (AISTATS)},
	Date-Added = {2018-09-14 20:35:32 +0000},
	Date-Modified = {2018-09-14 20:35:32 +0000},
	Title = {Scalable Learning of Non-Decomposable Objectives},
	Year = {2017}}
	
	
	
@inproceedings{NIPS2006_af44c4c5,
 author = {Burges, Christopher and Ragno, Robert and Le, Quoc},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Learning to Rank with Nonsmooth Cost Functions},
 volume = {19},
 year = {2007}
}


@article{Mohapatra2018EfficientOF,
  title={Efficient Optimization for Rank-Based Loss Functions},
  author={P. Mohapatra and Michal Rolinek and C. V. Jawahar and V. Kolmogorov and M. Kumar},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018},
  pages={3693-3701}
}


@inproceedings{NEURIPS2020_b2eeb736,
 author = {Oksuz, Kemal and Cam, Baris Can and Akbas, Emre and Kalkan, Sinan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {15534--15545},
 publisher = {Curran Associates, Inc.},
 title = {A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection},
 url = {https://proceedings.neurips.cc/paper/2020/file/b2eeb7362ef83deff5c7813a67e14f0a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@InProceedings{Rolinek_2020_CVPR,
author = {Rolinek, Michal and Musil, Vit and Paulus, Anselm and Vlastelica, Marin and Michaelis, Claudio and Martius, Georg},
title = {Optimizing Rank-Based Metrics With Blackbox Differentiation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

  @article{chen2020ap,
  title={AP-Loss for Accurate One-Stage Object Detection},
  author={Chen, Kean and Lin, Weiyao and See, John and Wang, Ji and Zou, Junni and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2020},
  publisher={IEEE}
}

@incollection{Henderson_2017,
	doi = {10.1007/978-3-319-54193-8_13},
	url = {https://doi.org/10.1007%2F978-3-319-54193-8_13},
	year = 2017,
	publisher = {Springer International Publishing},
	pages = {198--213},
	author = {Paul Henderson and Vittorio Ferrari},
	title = {End-to-End Training of Object Class Detectors for Mean Average Precision},
	booktitle = {Computer Vision {\textendash} {ACCV} 2016}
}

@inproceedings{brown2020smooth,
  title={Smooth-AP: Smoothing the path towards large-scale image retrieval},
  author={Brown, Andrew and Xie, Weidi and Kalogeiton, Vicky and Zisserman, Andrew},
  booktitle={European Conference on Computer Vision},
  pages={677--694},
  year={2020},
  organization={Springer}
}

@InProceedings{Cakir_2019_CVPR,
author = {Cakir, Fatih and He, Kun and Xia, Xide and Kulis, Brian and Sclaroff, Stan},
title = {Deep Metric Learning to Rank},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{Chen_2019_CVPR,
author = {Chen, Kean and Li, Jianguo and Lin, Weiyao and See, John and Wang, Ji and Duan, Lingyu and Chen, Zhibo and He, Changwei and Zou, Junni},
title = {Towards Accurate One-Stage Object Detection With AP-Loss},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@techreport{qin2008a,
author = {Qin, Tao and Liu, Tie-Yan and Li, Hang},
title = {A General Approximation Framework for Direct Optimization of Information Retrieval Measures},
year = {2008},
month = {November},
number = {MSR-TR-2008-164},
}

@inproceedings{10.5555/2984093.2984129,
author = {Chen, Wei and Liu, Tie-Yan and Lan, Yanyan and Ma, Zhiming and Li, Hang},
title = {Ranking Measures and Loss Functions in Learning to Rank},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning to rank has become an important research topic in machine learning. While most learning-to-rank methods learn the ranking functions by minimizing loss functions, it is the ranking measures (such as NDCG and MAP) that are used to evaluate the performance of the learned ranking functions. In this work, we reveal the relationship between ranking measures and loss functions in learning-to-rank methods, such as Ranking SVM, RankBoost, RankNet, and ListMLE. We show that the loss functions of these methods are upper bounds of the measure-based ranking errors. As a result, the minimization of these loss functions will lead to the maximization of the ranking measures. The key to obtaining this result is to model ranking as a sequence of classification tasks, and define a so-called essential loss for ranking as the weighted sum of the classification errors of individual tasks in the sequence. We have proved that the essential loss is both an upper bound of the measure-based ranking errors, and a lower bound of the loss functions in the aforementioned methods. Our proof technique also suggests a way to modify existing loss functions to make them tighter bounds of the measure-based ranking errors. Experimental results on benchmark datasets show that the modifications can lead to better ranking performances, demonstrating the correctness of our theoretical analysis.},
booktitle = {Proceedings of the 22nd International Conference on Neural Information Processing Systems},
pages = {315–323},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@misc{chexpert,
title={CheXpert Competition},
howpublished={\url{https://stanfordmlgroup.github.io/competitions/chexpert/}}
}

@misc{kaggle,
title={Kaggle Competition: SIIM-ISIC Melanoma Classification},
howpublished={\url{https://www.kaggle.com/c/siim-isic-melanoma-classification}}
}


@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}


@article{DBLP:journals/corr/abs-2010-04053,
  author    = {Simon Caton and
               Christian Haas},
  title     = {Fairness in Machine Learning: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2010.04053},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.04053},
  archivePrefix = {arXiv},
  eprint    = {2010.04053},
  timestamp = {Tue, 13 Oct 2020 15:25:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-04053.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{hanley82,
author = {Hanley, J.A. and Mcneil, Barbara},
year = {1982},
month = {05},
pages = {29-36},
title = {The Meaning and Use of the Area Under a Receiver Operating Characteristic (ROC) Curve},
volume = {143},
journal = {Radiology},
doi = {10.1148/radiology.143.1.7063747}
}

@inproceedings{davis06,
    address = {New York, NY, USA},
  author = {Davis, Jesse and Goadrich, Mark},
  booktitle = {ICML '06: Proceedings of the 23rd international conference on Machine learning},
  description = {The relationship between Precision-Recall and ROC curves},
  interhash = {e4ea92aea3ff8bbb3eb04c64867505f2},
  intrahash = {576166847a9527bdf0a0279a3e82b849},
  isbn = {1-59593-383-2},
  keywords = {thesis},
  location = {Pittsburgh, Pennsylvania},
  pages = {233--240},
  publisher = {ACM},
  timestamp = {2011-12-07T11:15:34.000+0100},
  title = {{The Relationship Between Precision-Recall and ROC Curves}},
  year = 2006
}

@article{10.1371/journal.pone.0118432,
    doi = {10.1371/journal.pone.0118432},
    author = {Saito, Takaya AND Rehmsmeier, Marc},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets},
    year = {2015},
    month = {03},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pone.0118432},
    pages = {1-21},
    abstract = {Binary classifiers are routinely evaluated with performance measures such as sensitivity and specificity, and performance is frequently illustrated with Receiver Operating Characteristics (ROC) plots. Alternative measures such as positive predictive value (PPV) and the associated Precision/Recall (PRC) plots are used less frequently. Many bioinformatics studies develop and evaluate classifiers that are to be applied to strongly imbalanced datasets in which the number of negatives outweighs the number of positives significantly. While ROC plots are visually appealing and provide an overview of a classifier's performance across a wide range of specificities, one can ask whether ROC plots could be misleading when applied in imbalanced classification scenarios. We show here that the visual interpretability of ROC plots in the context of imbalanced datasets can be deceptive with respect to conclusions about the reliability of classification performance, owing to an intuitive but wrong interpretation of specificity. PRC plots, on the other hand, can provide the viewer with an accurate prediction of future classification performance due to the fact that they evaluate the fraction of true positives among positive predictions. Our findings have potential implications for the interpretation of a large number of studies that use ROC plots on imbalanced datasets.},
    number = {3},

}

@article{DBLP:journals/corr/abs-1908-09635,
  author    = {Ninareh Mehrabi and
               Fred Morstatter and
               Nripsuta Saxena and
               Kristina Lerman and
               Aram Galstyan},
  title     = {A Survey on Bias and Fairness in Machine Learning},
  journal   = {CoRR},
  volume    = {abs/1908.09635},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09635},
  archivePrefix = {arXiv},
  eprint    = {1908.09635},
  timestamp = {Thu, 29 Aug 2019 16:32:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hanley1982,
  author = {James A. Hanley and Barbara J. McNeil},
  journal = {Radiology},
  number = 1,
  pages = {29-36},
  title = {The meaning and use of the area under a receiver operating characteristic (ROC) curve},
  volume = 143,
  year = 1982
}

@article{doi:10.1177/0962280217718866,
author = {Hanfang Yang and Kun Lu and Xiang Lyu and Feifang Hu},
title ={Two-way partial AUC and its properties},
journal = {Statistical Methods in Medical Research},
volume = {28},
number = {1},
pages = {184-195},
year = {2019},
doi = {10.1177/0962280217718866},
    note ={PMID: 28707503}
    }



@article{dodd03,
author = {Dodd, Lori and Pepe, Margaret},
year = {2003},
month = {10},
pages = {614-23},
title = {Partial AUC Estimation and Regression},
volume = {59},
journal = {Biometrics},
doi = {10.1111/1541-0420.00071}
}


@article{surveyor11,
author = {Waegeman, Willem and De Baets, Bernard},
year = {2011},
month = {01},
pages = {},
title = {A Survey on ROC-based Ordinal Regression},
journal = {Preference Learning},
doi = {10.1007/978-3-642-14125-6_7}
}

@inproceedings{10.5555/2832249.2832379,
author = {Gao, Wei and Zhou, Zhi-Hua},
title = {On the Consistency of AUC Pairwise Optimization},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {AUC (Area Under ROC Curve) has been an important criterion widely used in diverse learning tasks. To optimize AUC, many learning approaches have been developed, most working with pairwise surrogate losses. Thus, it is important to study the AUC consistency based on minimizing pairwise surrogate losses. In this paper, we introduce the generalized calibration for AUC optimization, and prove that it is a necessary condition for AUC consistency. We then provide a sufficient condition for AUC consistency, and show its usefulness in studying the consistency of various surrogate losses, as well as the invention of new consistent losses. We further derive regret bounds for exponential and logistic losses, and present regret bounds for more general surrogate losses in the realizable setting. Finally, we prove regret bounds that disclose the equivalence between the pairwise exponential loss of AUC and univariate exponential loss of accuracy.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {939–945},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@InProceedings{pmlr-v139-yang21k,
  title = 	 {When All We Need is a Piece of the Pie: A Generic Framework for Optimizing Two-way Partial AUC},
  author =       {Yang, Zhiyong and Xu, Qianqian and Bao, Shilong and He, Yuan and Cao, Xiaochun and Huang, Qingming},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11820--11829},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21k/yang21k.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21k.html},
  abstract = 	 {The Area Under the ROC Curve (AUC) is a crucial metric for machine learning, which evaluates the average performance over all possible True Positive Rates (TPRs) and False Positive Rates (FPRs). Based on the knowledge that a skillful classifier should simultaneously embrace a high TPR and a low FPR, we turn to study a more general variant called Two-way Partial AUC (TPAUC), where only the region with $\mathsf{TPR} \ge \alpha, \mathsf{FPR} \le \beta$ is included in the area. Moreover, a recent work shows that the TPAUC is essentially inconsistent with the existing Partial AUC metrics where only the FPR range is restricted, opening a new problem to seek solutions to leverage high TPAUC. Motivated by this, we present the first trial in this paper to optimize this new metric. The critical challenge along this course lies in the difficulty of performing gradient-based optimization with end-to-end stochastic training, even with a proper choice of surrogate loss. To address this issue, we propose a generic framework to construct surrogate optimization problems, which supports efficient end-to-end training with deep-learning. Moreover, our theoretical analyses show that: 1) the objective function of the surrogate problems will achieve an upper bound of the original problem under mild conditions, and 2) optimizing the surrogate problems leads to good generalization performance in terms of TPAUC with a high probability. Finally, empirical studies over several benchmark datasets speak to the efficacy of our framework.}
}


@article{RICAMATO20112669,
title = {Partial AUC maximization in a linear combination of dichotomizers},
journal = {Pattern Recognition},
volume = {44},
number = {10},
pages = {2669-2677},
year = {2011},
note = {Semi-Supervised Learning for Visual Content Analysis and Understanding},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2011.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0031320311001269},
author = {Maria Teresa Ricamato and Francesco Tortorella},
keywords = {Combination of classifiers, ROC analysis, Partial AUC},
abstract = {Classifier combination is a useful and common methodology to design an effective classification system. A large number of combination rules has been proposed hitherto, mostly aimed at minimizing the error rate. Recently, some methods have been presented that are devoted to maximize the area under the ROC curve (AUC), a more suitable performance measure when dealing with two-class problems with imprecise environment and/or imbalanced class priors. However, there are several applications that do not operate in the complete range of the ROC curve, but only in particular regions of it. In these cases, it is better to analyze the performance only in a part of the curve and to use the partial AUC (pAUC). This paper presents a new method that aims at maximizing the pAUC by means of linear combination of classifiers. The effectiveness of the proposed method has been proved on two biometric databases.}
}

@article{DBLP:journals/corr/abs-1810-01943,
  author    = {Rachel K. E. Bellamy and
               Kuntal Dey and
               Michael Hind and
               Samuel C. Hoffman and
               Stephanie Houde and
               Kalapriya Kannan and
               Pranay Lohia and
               Jacquelyn Martino and
               Sameep Mehta and
               Aleksandra Mojsilovic and
               Seema Nagar and
               Karthikeyan Natesan Ramamurthy and
               John T. Richards and
               Diptikalyan Saha and
               Prasanna Sattigeri and
               Moninder Singh and
               Kush R. Varshney and
               Yunfeng Zhang},
  title     = {{AI} Fairness 360: An Extensible Toolkit for Detecting, Understanding,
               and Mitigating Unwanted Algorithmic Bias},
  journal   = {CoRR},
  volume    = {abs/1810.01943},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.01943},
  archivePrefix = {arXiv},
  eprint    = {1810.01943},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-01943.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{xu2019non,
  title={Non-asymptotic analysis of stochastic methods for non-smooth non-convex regularized problems},
  author={Xu, Yi and Jin, Rong and Yang, Tianbao},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={2630--2640},
  year={2019}
}


@misc{arxiv.2011.10076,
  doi = {10.48550/ARXIV.2011.10076},
  
  url = {https://arxiv.org/abs/2011.10076},
  
  author = {Zhang, Zhe and Lan, Guanghui},
  
  keywords = {Optimization and Control (math.OC), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Optimal Algorithms for Convex Nested Stochastic Composite Optimization},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{Chen_2021,
	doi = {10.1109/tsp.2021.3092377},
  
	url = {https://doi.org/10.1109%2Ftsp.2021.3092377},
  
	year = 2021,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {69},
  
	pages = {4937--4948},
  
	author = {Tianyi Chen and Yuejiao Sun and Wotao Yin},
  
	title = {Solving Stochastic Compositional Optimization is Nearly as Easy as Solving Stochastic Optimization},
  
	journal = {{IEEE} Transactions on Signal Processing}
}

@inproceedings{NEURIPS2021_b986700c,
 author = {Hu, Yifan and Chen, Xin and He, Niao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {22119--22131},
 publisher = {Curran Associates, Inc.},
 title = {On the Bias-Variance-Cost Tradeoff of Stochastic Optimization},
 url = {https://proceedings.neurips.cc/paper/2021/file/b986700c627db479a4d9460b75de7222-Paper.pdf},
 volume = {34},
 year = {2021}
}



@ARTICLE{RePEc:spr:aistmt:v:69:y:2017:i:4:d:10.1007_s10463-016-0559-8,
title = {Statistical estimation of composite risk functionals and risk optimization problems},
author = {Dentcheva, Darinka and Penev, Spiridon and Ruszczynski, Andrzej},
year = {2017},
journal = {Annals of the Institute of Statistical Mathematics},
volume = {69},
number = {4},
pages = {737-760},
abstract = {Abstract We address the statistical estimation of composite functionals which may be nonlinear in the probability measure. Our study is motivated by the need to estimate coherent measures of risk, which become increasingly popular in finance, insurance, and other areas associated with optimization under uncertainty and risk. We establish central limit theorems for composite risk functionals. Furthermore, we discuss the asymptotic behavior of optimization problems whose objectives are composite risk functionals and we establish a central limit formula of their optimal values when an estimator of the risk functional is used. While the mathematical structures accommodate commonly used coherent measures of risk, they have more general character, which may be of independent interest.},
keywords = {Risk measures; Composite functionals; Central limit theorem},
url = {https://EconPapers.repec.org/RePEc:spr:aistmt:v:69:y:2017:i:4:d:10.1007_s10463-016-0559-8}
}



@book{nemirovski1983problem,
  title={Problem Complexity and Method Efficiency in Optimization},
  author={A. S. Nemirovsky and D. B. Yudin},
  isbn={9780471103455},
  lccn={82011065},
  series={A Wiley-Interscience publication},
  url={https://books.google.com/books?id=6ULvAAAAMAAJ},
  year={1983},
  publisher={Wiley}
}


@article{yan2019stochastic,
  title={Stochastic Primal-Dual Algorithms with Faster Convergence than  $\mathcal{O}(1/\sqrt {T})$  for Problems without Bilinear Structure},
  author={Yan, Yan and Xu, Yi and Lin, Qihang and Zhang, Lijun and Yang, Tianbao},
  journal={arXiv preprint arXiv:1904.10112},
  year={2019}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@article{qi2020attentional,
  title={Attentional Biased Stochastic Gradient for Imbalanced Classification},
  author={Qi, Qi and Xu, Yi and Jin, Rong and Yin, Wotao and Yang, Tianbao},
  journal={arXiv preprint arXiv:2012.06951},
  year={2020}
}



@article{li2018calculus,
  title={Calculus of the exponent of Kurdyka--{\L}ojasiewicz inequality and its applications to linear convergence of first-order methods},
  author={Li, Guoyin and Pong, Ting Kei},
  journal={Foundations of computational mathematics},
  volume={18},
  number={5},
  pages={1199--1232},
  year={2018},
  publisher={Springer}
}

@article{arjevani2019lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={arXiv preprint arXiv:1912.02365},
  year={2019}
}

@inproceedings{zhu2019robust,
  title={A robust zero-sum game framework for pool-based active learning},
  author={Zhu, Dixian and Li, Zhe and Wang, Xiaoyu and Gong, Boqing and Yang, Tianbao},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={517--526},
  year={2019},
  organization={PMLR}
}

@inproceedings{suh2019stochastic,
  title={Stochastic Class-based Hard Example Mining for Deep Metric Learning},
  author={Suh, Yumin and Han, Bohyung and Kim, Wonsik and Lee, Kyoung Mu},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7251--7259},
  year={2019}
}
@article{zhao2018metric,
  title={Metric-optimized example weights},
  author={Zhao, Sen and Fard, Mahdi Milani and Narasimhan, Harikrishna and Gupta, Maya},
  journal={arXiv preprint arXiv:1805.10582},
  year={2018}
}
@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1885--1894},
  year={2017},
  organization={JMLR. org}
}

@article{pang2019improving,
  title={Improving Adversarial Robustness via Promoting Ensemble Diversity},
  author={Pang, Tianyu and Xu, Kun and Du, Chao and Chen, Ning and Zhu, Jun},
  journal={arXiv preprint arXiv:1901.08846},
  year={2019}
}

@inproceedings{cortes2004auc,
  title={AUC optimization vs. error rate minimization},
  author={Cortes, Corinna and Mohri, Mehryar},
  booktitle={Advances in neural information processing systems},
  pages={313--320},
  year={2004}
}
@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}


@article{goel2022cyclip,
  title={Cyclip: Cyclic contrastive language-image pretraining},
  author={Goel, Shashank and Bansal, Hritik and Bhatia, Sumit and Rossi, Ryan and Vinay, Vishwa and Grover, Aditya},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={6704--6719},
  year={2022}
}


@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{ren2018learning,
  title={Learning to reweight examples for robust deep learning},
  author={Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  journal={arXiv preprint arXiv:1803.09050},
  year={2018}
}

@inproceedings{yuan2022provable,
  title={Provable Stochastic Optimization for Global Contrastive Learning: Small Batch Does Not Harm Performance},
  author={Yuan, Zhuoning and Wu, Yuexin and Qiu, Zi-Hao and Du, Xianzhi and Zhang, Lijun and Zhou, Denny and Yang, Tianbao},
  booktitle={International Conference on Machine Learning},
  pages={25760--25782},
  year={2022},
  organization={PMLR}
}

@article{ahmadi2012entropic,
  title={Entropic value-at-risk: A new coherent risk measure},
  author={Ahmadi-Javid, Amir},
  journal={Journal of Optimization Theory and Applications},
  volume={155},
  pages={1105--1123},
  year={2012},
  publisher={Springer}
}



@inproceedings{wang2019convergence,
  title={On the Convergence and Robustness of Adversarial Training},
  author={Wang, Yisen and Ma, Xingjun and Bailey, James and Yi, Jinfeng and Zhou, Bowen and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={6586--6595},
  year={2019}
}

@article{cai2018curriculum,
  title={Curriculum adversarial training},
  author={Cai, Qi-Zhi and Du, Min and Liu, Chang and Song, Dawn},
  journal={arXiv preprint arXiv:1805.04807},
  year={2018}
}

@inproceedings{DBLP:conf/nips/YingWL16,
  author    = {Yiming Ying and
               Longyin Wen and
               Siwei Lyu},
  title     = {Stochastic Online {AUC} Maximization},
  booktitle = {Advances in Neural Information Processing Systems 29: Annual Conference
               on Neural Information Processing Systems 2016, December 5-10, 2016,
               Barcelona, Spain},
  pages     = {451--459},
  year      = {2016},
  crossref  = {DBLP:conf/nips/2016},
  url       = {http://papers.nips.cc/paper/6065-stochastic-online-auc-maximization},
  timestamp = {Fri, 03 Mar 2017 14:59:41 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/nips/YingWL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{nedic2009subgradient,
  title={Subgradient methods for saddle-point problems},
  author={Nedi{\'c}, Angelia and Ozdaglar, Asuman},
  journal={Journal of optimization theory and applications},
  volume={142},
  number={1},
  pages={205--228},
  year={2009},
  publisher={Springer}
}

@article{carmon2021thinking,
  title={Thinking inside the ball: Near-optimal minimization of the maximal loss},
  author={Carmon, Yair and Jambulapati, Arun and Jin, Yujia and Sidford, Aaron},
  journal={arXiv preprint arXiv:2105.01778},
  year={2021}
}

@article{rockafellar1998variational,
  title={Variational Analysis Springer},
  author={Rockafellar, RT and Wets, RJB},
  journal={MR1491362},
  year={1998}
}

@article{wang2017stochastic,
  title={Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions},
  author={Wang, Mengdi and Fang, Ethan X and Liu, Han},
  journal={Mathematical Programming},
  volume={161},
  number={1-2},
  pages={419--449},
  year={2017},
  publisher={Springer}
}

@article{qi2021online,
  title={An online method for a class of distributionally robust optimization with non-convex objectives},
  author={Qi, Qi and Guo, Zhishuai and Xu, Yi and Jin, Rong and Yang, Tianbao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{zhang2019stochastic,
  title={A stochastic composite gradient method with incremental variance reduction},
  author={Zhang, Junyu and Xiao, Lin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9075--9085},
  year={2019}
}
@article{zhou2019momentum,
  title={Momentum schemes with stochastic variance reduction for nonconvex composite optimization},
  author={Zhou, Yi and Wang, Zhe and Ji, Kaiyi and Liang, Yingbin and Tarokh, Vahid},
  journal={arXiv preprint arXiv:1902.02715},
  year={2019}
}
@article{rafique2021weakly,
  title={Weakly-convex--concave min--max optimization: provable algorithms and applications in machine learning},
  author={Rafique, Hassan and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},
  journal={Optimization Methods and Software},
  pages={1--35},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{Zhang2020OptimalAF,
  title={Optimal Algorithms for Convex Nested Stochastic Composite Optimization},
  author={Zhe Zhang and Guanghui Lan},
  journal = {ArXiv e-prints},
  volume={arXiv:2011.10076},
  year={2021}
}

@article{cutkosky2019momentum,
  title={Momentum-Based Variance Reduction in Non-Convex SGD},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15236--15245},
  year={2019}
}

@article{levy2020large,
  title={Large-Scale Methods for Distributionally Robust Optimization},
  author={Levy, Daniel and Carmon, Yair and Duchi, John C and Sidford, Aaron},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

%%% DRO problem
@article{rahimian2019distributionally,
  title={Distributionally robust optimization: A review},
  author={Rahimian, Hamed and Mehrotra, Sanjay},
  journal={arXiv preprint arXiv:1908.05659},
  year={2019}
}

@article{delage2010distributionally,
  title={Distributionally robust optimization under moment uncertainty with application to data-driven problems},
  author={Delage, Erick and Ye, Yinyu},
  journal={Operations research},
  volume={58},
  number={3},
  pages={595--612},
  year={2010},
  publisher={INFORMS}
}

@article{chen2018robust,
  title={A robust learning approach for regression models based on distributionally robust optimization},
  author={Chen, Ruidi and Paschalidis, Ioannis C},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={13},
  year={2018}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{duchi2021learning,
  title={Learning models with uniform performance via distributionally robust optimization},
  author={Duchi, John C and Namkoong, Hongseok},
  journal={The Annals of Statistics},
  volume={49},
  number={3},
  pages={1378--1406},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

%%Robust optimizaiton
@article{ben2013robust,
  title={Robust solutions of optimization problems affected by uncertain probabilities},
  author={Ben-Tal, Aharon and Den Hertog, Dick and De Waegenaere, Anja and Melenberg, Bertrand and Rennen, Gijs},
  journal={Management Science},
  volume={59},
  number={2},
  pages={341--357},
  year={2013},
  publisher={INFORMS}
}

@article{bertsimas2018data,
  title={Data-driven robust optimization},
  author={Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
  journal={Mathematical Programming},
  volume={167},
  number={2},
  pages={235--292},
  year={2018},
  publisher={Springer}
}

@article{kang2019decoupling,
  title={Decoupling representation and classifier for long-tailed recognition},
  author={Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
  journal={arXiv preprint arXiv:1910.09217},
  year={2019}
}


@article{ghadimi2020single,
  title={A single timescale stochastic approximation method for nested stochastic optimization},
  author={Ghadimi, Saeed and Ruszczynski, Andrzej and Wang, Mengdi},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={1},
  pages={960--979},
  year={2020},
  publisher={SIAM}
}

%DRO related work
@article{staib2019distributionally,
  title={Distributionally robust optimization and generalization in kernel methods},
  author={Staib, Matthew and Jegelka, Stefanie},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={9134--9144},
  year={2019}
}



@article{mccandlish2018empirical,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}





@misc{inaturalist18,
    Howpublished = {~\url{https://github.com/visipedia/inat_comp/tree/master/2018} },
    Title = { {iNaturalist} 2018 competition dataset.},
    Year = {2018},
    key = { {iNaturalist} 2018 competition dataset},
    }
    

@inproceedings{liu2019large,
  title={Large-scale long-tailed recognition in an open world},
  author={Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2537--2546},
  year={2019}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@inproceedings{udell2014convex,
  title={Convex optimization in Julia},
  author={Udell, Madeleine and Mohan, Karanveer and Zeng, David and Hong, Jenny and Diamond, Steven and Boyd, Stephen},
  booktitle={2014 First Workshop for High Performance Technical Computing in Dynamic Languages},
  pages={18--28},
  year={2014},
  organization={IEEE}
}

@inproceedings{namkoong2016stochastic,
  title={Stochastic Gradient Methods for Distributionally Robust Optimization with f-divergences.},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={NIPS},
  volume={29},
  pages={2208--2216},
  year={2016}
}

@inproceedings{namkoong2017variance,
  title={Variance-based regularization with convex objectives},
  author={Namkoong, Hongseok and Duchi, John C},
  booktitle={Advances in neural information processing systems},
  pages={2971--2980},
  year={2017}
}

@article{duchi2016statistics,
	title={Statistics of Robust Optimization: A Generalized Empirical Likelihood Approach},
	author={Duchi, C. John and Glynn, W. Peter and Namkoong, Hongseok},
	journal={Mathematics of Operations Research},
	year={2016}
}

@article{hu2013kullback,
  title={Kullback-Leibler divergence constrained distributionally robust optimization},
  author={Hu, Zhaolin and Hong, L Jeff},
  journal={Available at Optimization Online},
  year={2013}
}

@inproceedings{qi2020simple,
  title={A Simple and Effective Framework for Pairwise Deep Metric Learning},
  author={Qi, Qi and Yan, Yan and Wu, Zixuan and Wang, Xiaoyu and Yang, Tianbao},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XXVII 16},
  pages={375--391},
  year={2020},
  organization={Springer}
}



@article{deng2020distributionally,
  title={Distributionally Robust Federated Averaging},
  author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{li2021tilted,
  title={On Tilted Losses in Machine Learning: Theory and Applications},
  author={Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia},
  journal={arXiv preprint arXiv:2109.06141},
  year={2021}
}

@inproceedings{li2020tilted,
  title={Tilted Empirical Risk Minimization},
  author={Li, Tian and Beirami, Ahmad and Sanjabi, Maziar and Smith, Virginia},
  booktitle={International Conference on Learning Representations},
  year={2020}
}



@article{jin2021non,
  title={Non-convex Distributionally Robust Optimization: Non-asymptotic Analysis},
  author={Jin, Jikai and Zhang, Bohang and Wang, Haiyang and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{wang2021sinkhorn,
  title={Sinkhorn Distributionally Robust Optimization},
  author={Wang, Jie and Gao, Rui and Xie, Yao},
  journal={arXiv preprint arXiv:2109.11926},
  year={2021}
}
@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6083--6093},
  year={2020},
  organization={PMLR}
}

@article{huang2020accelerated,
  title={Accelerated zeroth-order momentum methods from mini to minimax optimization},
  author={Huang, Feihu and Gao, Shangqian and Pei, Jian and Huang, Heng},
  journal={arXiv e-prints},
  pages={arXiv--2008},
  year={2020}
}
@article{luo2020stochastic,
  title={Stochastic Recursive Gradient Descent Ascent for Stochastic Nonconvex-Strongly-Concave Minimax Problems},
  author={Luo, Luo and Ye, Haishan and Huang, Zhichao and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@inproceedings{tran2020hybrid,
  title={Hybrid Variance-Reduced SGD Algorithms For Minimax Problems with Nonconvex-Linear Function.},
  author={Tran-Dinh, Quoc and Liu, Deyi and Nguyen, Lam M},
  booktitle={NeurIPS},
  year={2020}
}

@article{alacaoglu2022complexity,
  title={On the Complexity of a Practical Primal-Dual Coordinate Method},
  author={Alacaoglu, Ahmet and Cevher, Volkan and Wright, Stephen J},
  journal={arXiv preprint arXiv:2201.07684},
  year={2022}
}
@inproceedings{song2021variance,
  title={Variance reduction via primal-dual accelerated dual averaging for nonsmooth convex finite-sums},
  author={Song, Chaobing and Wright, Stephen J and Diakonikolas, Jelena},
  booktitle={International Conference on Machine Learning},
  pages={9824--9834},
  year={2021},
  organization={PMLR}
}