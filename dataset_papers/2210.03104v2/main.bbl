\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2022)Chen, Xu, and Agrawal]{pmlr-v164-chen22a}
T.~Chen, J.~Xu, and P.~Agrawal.
\newblock A system for general in-hand object re-orientation.
\newblock In A.~Faust, D.~Hsu, and G.~Neumann, editors, \emph{Proceedings of
  the 5th Conference on Robot Learning}, volume 164 of \emph{Proceedings of
  Machine Learning Research}, pages 297--307. PMLR, 08--11 Nov 2022.
\newblock URL \url{https://proceedings.mlr.press/v164/chen22a.html}.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
J.~Cohen, E.~Rosenfeld, and Z.~Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{International Conference on Machine Learning}, pages
  1310--1320. PMLR, 2019.

\bibitem[Collins et~al.(2020)Collins, Mokhtari, and
  Shakkottai]{collins2020trmaml}
L.~Collins, A.~Mokhtari, and S.~Shakkottai.
\newblock Distribution-agnostic model-agnostic meta-learning.
\newblock \emph{CoRR}, abs/2002.04766, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.04766}.

\bibitem[De~Boer et~al.(2005)De~Boer, Kroese, Mannor, and
  Rubinstein]{de2005tutorial}
P.-T. De~Boer, D.~P. Kroese, S.~Mannor, and R.~Y. Rubinstein.
\newblock A tutorial on the cross-entropy method.
\newblock \emph{Annals of operations research}, 134\penalty0 (1):\penalty0
  19--67, 2005.

\bibitem[Deleu and Bengio(2018)]{deleu2018effects}
T.~Deleu and Y.~Bengio.
\newblock The effects of negative adaptation in model-agnostic meta-learning.
\newblock \emph{arXiv preprint arXiv:1812.02159}, 2018.

\bibitem[Dorfman et~al.(2020)Dorfman, Shenfeld, and Tamar]{dorfman2020offline}
R.~Dorfman, I.~Shenfeld, and A.~Tamar.
\newblock Offline meta learning of exploration.
\newblock \emph{arXiv preprint arXiv:2008.02598}, 2020.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{duan2016rl}
Y.~Duan, J.~Schulman, X.~Chen, P.~L. Bartlett, I.~Sutskever, and P.~Abbeel.
\newblock Rl2: Fast reinforcement learning via slow reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.02779}, 2016.

\bibitem[Fallah et~al.(2021)Fallah, Mokhtari, and
  Ozdaglar]{fallah2021generalization}
A.~Fallah, A.~Mokhtari, and A.~Ozdaglar.
\newblock Generalization of model-agnostic meta-learning algorithms: Recurring
  and unseen tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Filos et~al.(2020)Filos, Tigkas, McAllister, Rhinehart, Levine, and
  Gal]{filos2020can}
A.~Filos, P.~Tigkas, R.~McAllister, N.~Rhinehart, S.~Levine, and Y.~Gal.
\newblock Can autonomous vehicles identify, recover from, and adapt to
  distribution shifts?
\newblock In \emph{International Conference on Machine Learning}, pages
  3145--3153. PMLR, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Fu et~al.(2021)Fu, Tang, Hao, Chen, Feng, Li, and Liu]{fu2021towards}
H.~Fu, H.~Tang, J.~Hao, C.~Chen, X.~Feng, D.~Li, and W.~Liu.
\newblock Towards effective context for meta-reinforcement learning: an
  approach based on contrastive learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 7457--7465, 2021.

\bibitem[Gupta et~al.(2018{\natexlab{a}})Gupta, Eysenbach, Finn, and
  Levine]{gupta2018unsupervised}
A.~Gupta, B.~Eysenbach, C.~Finn, and S.~Levine.
\newblock Unsupervised meta-learning for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.04640}, 2018{\natexlab{a}}.

\bibitem[Gupta et~al.(2018{\natexlab{b}})Gupta, Mendonca, Liu, Abbeel, and
  Levine]{gupta2018meta}
A.~Gupta, R.~Mendonca, Y.~Liu, P.~Abbeel, and S.~Levine.
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock \emph{Advances in neural information processing systems}, 31,
  2018{\natexlab{b}}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[Hong et~al.(2021)Hong, Wang, Wang, and Zhou]{hong2021federated}
J.~Hong, H.~Wang, Z.~Wang, and J.~Zhou.
\newblock Federated robustness propagation: Sharing adversarial robustness in
  federated learning.
\newblock \emph{arXiv preprint arXiv:2106.10196}, 2021.

\bibitem[Jabri et~al.(2019)Jabri, Hsu, Gupta, Eysenbach, Levine, and
  Finn]{jabri2019unsupervised}
A.~Jabri, K.~Hsu, A.~Gupta, B.~Eysenbach, S.~Levine, and C.~Finn.
\newblock Unsupervised curricula for visual meta-reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Ke et~al.(2021)Ke, Wang, Bhattacharjee, Boots, and
  Srinivasa]{ke2021grasping}
L.~Ke, J.~Wang, T.~Bhattacharjee, B.~Boots, and S.~Srinivasa.
\newblock Grasping with chopsticks: Combating covariate shift in model-free
  imitation learning for fine manipulation.
\newblock In \emph{2021 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 6185--6191. IEEE, 2021.

\bibitem[Kumar et~al.(2021)Kumar, Fu, Pathak, and Malik]{kumar2021rma}
A.~Kumar, Z.~Fu, D.~Pathak, and J.~Malik.
\newblock Rma: Rapid motor adaptation for legged robots.
\newblock \emph{arXiv preprint arXiv:2107.04034}, 2021.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019smm}
L.~Lee, B.~Eysenbach, E.~Parisotto, E.~P. Xing, S.~Levine, and
  R.~Salakhutdinov.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{CoRR}, abs/1906.05274, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.05274}.

\bibitem[Lin et~al.(2020)Lin, Thomas, Yang, and Ma]{lin2021robustmeta}
Z.~Lin, G.~Thomas, G.~Yang, and T.~Ma.
\newblock Model-based adversarial meta-reinforcement learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/73634c1dcbe056c1f7dcf5969da406c8-Abstract.html}.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Margolis et~al.(2022)Margolis, Yang, Paigwar, Chen, and
  Agrawal]{margolis2022rapid}
G.~B. Margolis, G.~Yang, K.~Paigwar, T.~Chen, and P.~Agrawal.
\newblock Rapid locomotion via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2205.02824}, 2022.

\bibitem[Mendonca et~al.(2020)Mendonca, Geng, Finn, and
  Levine]{mendonca2020mier}
R.~Mendonca, X.~Geng, C.~Finn, and S.~Levine.
\newblock Meta-reinforcement learning robust to distributional shift via model
  identification and experience relabeling.
\newblock \emph{CoRR}, abs/2006.07178, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.07178}.

\bibitem[Miki et~al.(2022)Miki, Lee, Hwangbo, Wellhausen, Koltun, and
  Hutter]{miki2022learning}
T.~Miki, J.~Lee, J.~Hwangbo, L.~Wellhausen, V.~Koltun, and M.~Hutter.
\newblock Learning robust perceptive locomotion for quadrupedal robots in the
  wild.
\newblock \emph{Science Robotics}, 7\penalty0 (62):\penalty0 eabk2822, 2022.

\bibitem[Mishra et~al.(2017)Mishra, Rohaninejad, Chen, and
  Abbeel]{mishra2017simple}
N.~Mishra, M.~Rohaninejad, X.~Chen, and P.~Abbeel.
\newblock A simple neural attentive meta-learner.
\newblock \emph{arXiv preprint arXiv:1707.03141}, 2017.

\bibitem[Mitchell et~al.(2021)Mitchell, Rafailov, Peng, Levine, and
  Finn]{mitchell2021offline}
E.~Mitchell, R.~Rafailov, X.~B. Peng, S.~Levine, and C.~Finn.
\newblock Offline meta-reinforcement learning with advantage weighting.
\newblock In \emph{International Conference on Machine Learning}, pages
  7780--7791. PMLR, 2021.

\bibitem[Nagabandi et~al.(2018)Nagabandi, Clavera, Liu, Fearing, Abbeel,
  Levine, and Finn]{nagabandi2018learning}
A.~Nagabandi, I.~Clavera, S.~Liu, R.~S. Fearing, P.~Abbeel, S.~Levine, and
  C.~Finn.
\newblock Learning to adapt in dynamic, real-world environments through
  meta-reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.11347}, 2018.

\bibitem[Nair et~al.(2020)Nair, Gupta, Dalal, and Levine]{nair2020awac}
A.~Nair, A.~Gupta, M.~Dalal, and S.~Levine.
\newblock Awac: Accelerating online reinforcement learning with offline
  datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Nesterov(2009)]{nesterov2009primal}
Y.~Nesterov.
\newblock Primal-dual subgradient methods for convex problems.
\newblock \emph{Mathematical programming}, 120\penalty0 (1):\penalty0 221--259,
  2009.

\bibitem[Ni et~al.(2022)Ni, Eysenbach, Levine, and
  Salakhutdinov]{ni2022recurrent}
T.~Ni, B.~Eysenbach, S.~Levine, and R.~Salakhutdinov.
\newblock Recurrent model-free {RL} is a strong baseline for many {POMDP}s,
  2022.
\newblock URL \url{https://openreview.net/forum?id=E0zOKxQsZhN}.

\bibitem[Oikarinen et~al.(2021)Oikarinen, Zhang, Megretski, Daniel, and
  Weng]{oikarinen2021robbust}
T.~P. Oikarinen, W.~Zhang, A.~Megretski, L.~Daniel, and T.~Weng.
\newblock Robust deep reinforcement learning through adversarial loss.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~N. Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems 34:
  Annual Conference on Neural Information Processing Systems 2021, NeurIPS
  2021, December 6-14, 2021, virtual}, pages 26156--26167, 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/hash/dbb422937d7ff56e049d61da730b3e11-Abstract.html}.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017rarl}
L.~Pinto, J.~Davidson, R.~Sukthankar, and A.~Gupta.
\newblock Robust adversarial reinforcement learning.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning, {ICML} 2017, Sydney, NSW,
  Australia, 6-11 August 2017}, volume~70 of \emph{Proceedings of Machine
  Learning Research}, pages 2817--2826. {PMLR}, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/pinto17a.html}.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Finn, Levine, and
  Quillen]{rakelly2019efficient}
K.~Rakelly, A.~Zhou, C.~Finn, S.~Levine, and D.~Quillen.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In \emph{International conference on machine learning}, pages
  5331--5340. PMLR, 2019.

\bibitem[R{\'e}nyi(1961)]{renyi1961measures}
A.~R{\'e}nyi.
\newblock On measures of entropy and information.
\newblock In \emph{Proceedings of the Fourth Berkeley Symposium on Mathematical
  Statistics and Probability, Volume 1: Contributions to the Theory of
  Statistics}, volume~4, pages 547--562. University of California Press, 1961.

\bibitem[Rothfuss et~al.(2018)Rothfuss, Lee, Clavera, Asfour, and
  Abbeel]{rothfuss2018promp}
J.~Rothfuss, D.~Lee, I.~Clavera, T.~Asfour, and P.~Abbeel.
\newblock Promp: Proximal meta-policy search.
\newblock \emph{arXiv preprint arXiv:1810.06784}, 2018.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.
\newblock URL
  \url{http://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17}.

\bibitem[Sinha et~al.(2017)Sinha, Namkoong, Volpi, and
  Duchi]{sinha2017certifying}
A.~Sinha, H.~Namkoong, R.~Volpi, and J.~Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock \emph{arXiv preprint arXiv:1710.10571}, 2017.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Thrun and Pratt(1998)]{thrun98metalearning}
S.~Thrun and L.~Y. Pratt, editors.
\newblock \emph{Learning to Learn}.
\newblock Springer, 1998.
\newblock ISBN 978-1-4613-7527-2.
\newblock \doi{10.1007/978-1-4615-5529-2}.
\newblock URL \url{https://doi.org/10.1007/978-1-4615-5529-2}.

\bibitem[Vaserstein(1969)]{vaserstein1969markov}
L.~N. Vaserstein.
\newblock Markov processes over denumerable products of spaces, describing
  large systems of automata.
\newblock \emph{Problemy Peredachi Informatsii}, 5\penalty0 (3):\penalty0
  64--72, 1969.

\bibitem[Vinitsky et~al.(2020)Vinitsky, Du, Parvate, Jang, Abbeel, and
  Bayen]{vinitsky2020robust}
E.~Vinitsky, Y.~Du, K.~Parvate, K.~Jang, P.~Abbeel, and A.~M. Bayen.
\newblock Robust reinforcement learning using adversarial populations.
\newblock \emph{CoRR}, abs/2008.01825, 2020.
\newblock URL \url{https://arxiv.org/abs/2008.01825}.

\bibitem[Wolpert and Macready(1997)]{585893}
D.~Wolpert and W.~Macready.
\newblock No free lunch theorems for optimization.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 1\penalty0
  (1):\penalty0 67--82, 1997.
\newblock \doi{10.1109/4235.585893}.

\bibitem[Wu et~al.(2021)Wu, Goodman, Piech, and Finn]{wu2021prototransformer}
M.~Wu, N.~Goodman, C.~Piech, and C.~Finn.
\newblock Prototransformer: A meta-learning approach to providing student
  feedback.
\newblock \emph{arXiv preprint arXiv:2107.14035}, 2021.

\bibitem[Xie et~al.(2022)Xie, Sodhani, Finn, Pineau, and Zhang]{xie2022robust}
A.~Xie, S.~Sodhani, C.~Finn, J.~Pineau, and A.~Zhang.
\newblock Robust policy learning over multiple uncertainty sets.
\newblock \emph{arXiv preprint arXiv:2202.07013}, 2022.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Chen, Boning, and
  Hsieh]{zhang2021robust}
H.~Zhang, H.~Chen, D.~S. Boning, and C.~Hsieh.
\newblock Robust reinforcement learning on state observations with learned
  optimal adversary.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net,
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=sCZbhBvqQaU}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Wang, Hu, Chen, Chen, Fan, and
  Zhang]{zhang2021metacure}
J.~Zhang, J.~Wang, H.~Hu, T.~Chen, Y.~Chen, C.~Fan, and C.~Zhang.
\newblock Metacure: Meta reinforcement learning with empowerment-driven
  exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  12600--12610. PMLR, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2020)Zhao, Nagabandi, Rakelly, Finn, and
  Levine]{zhao2020meld}
T.~Z. Zhao, A.~Nagabandi, K.~Rakelly, C.~Finn, and S.~Levine.
\newblock Meld: Meta-reinforcement learning from images via latent state
  models.
\newblock \emph{arXiv preprint arXiv:2010.13957}, 2020.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2019varibad}
L.~Zintgraf, K.~Shiarlis, M.~Igl, S.~Schulze, Y.~Gal, K.~Hofmann, and
  S.~Whiteson.
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1910.08348}, 2019.

\bibitem[Zintgraf et~al.(2021)Zintgraf, Feng, Lu, Igl, Hartikainen, Hofmann,
  and Whiteson]{zintgraf2021exploration}
L.~M. Zintgraf, L.~Feng, C.~Lu, M.~Igl, K.~Hartikainen, K.~Hofmann, and
  S.~Whiteson.
\newblock Exploration in approximate hyper-state space for meta reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  12991--13001. PMLR, 2021.

\end{thebibliography}
