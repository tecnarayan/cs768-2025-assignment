@article{huang2023metatool,
  title   = {MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use},
  author  = {Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong and Lichao Sun},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2310.03128}
}

@article{patil2023gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@misc{IntroducingMetaLlama,
  title = {Introducing {{Meta Llama}} 3: {{The}} Most Capable Openly Available {{LLM}} to Date},
  shorttitle = {Introducing {{Meta Llama}} 3},
  journal = {Meta AI},
  abstract = {Today, we're introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. In the coming months, we expect to share new capabilities, additional model sizes, and more.},
  howpublished = {https://ai.meta.com/blog/meta-llama-3/},
  langid = {english}
}

@misc{IntroducingNextGeneration,
  title = {Introducing the next Generation of {{Claude}}},
  abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
  howpublished = {https://www.anthropic.com/news/claude-3-family},
  langid = {english}
}



% LLM
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{yang2023baichuan,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and Yang, Fan and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@misc{2023internlm,
    title={InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM}},
    year={2023}
}

@misc{longchat2023,
    title = {How Long Can Open-Source LLMs Truly Promise on Context Length?},
    url = {https://lmsys.org/blog/2023-06-29-longchat},
    author = {Dacheng Li and Rulin Shao and Anze Xie and Ying Sheng and Lianmin Zheng and Joseph E. Gonzalez and Ion Stoica and Xuezhe Ma and and Hao Zhang},
    month = {June},
    year = {2023}
}

@online{MosaicML2023Introducing,
    author    = {MosaicML NLP Team},
    title     = {Introducing MPT-7B: A New Standard for Open-Source,
    ly Usable LLMs},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b},
    note      = {Accessed: 2023-03-28}, 
    urldate   = {2023-03-28}
}

@inproceedings{Brown2020GPT3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {NeurIPS},
  pages        = {1--25},
  year         = {2020},
}

@inproceedings{Ouyang2022InstructGPT,
  author       = {Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe},
  title        = {Training language models to follow instructions with human feedback},
  booktitle    = {NeurIPS},
  year         = {2022},
  pages        = {27730--27744},
  volume       = {35},
}

@article{OpenAI2023GPT4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
}

@article{Hugo2023LLaMa,
  author       = {Hugo Touvron and
                  Thibaut Lavril and
                  Gautier Izacard and
                  Xavier Martinet and
                  Marie{-}Anne Lachaux and
                  Timoth{\'{e}}e Lacroix and
                  Baptiste Rozi{\`{e}}re and
                  Naman Goyal and
                  Eric Hambro and
                  Faisal Azhar and
                  Aur{\'{e}}lien Rodriguez and
                  Armand Joulin and
                  Edouard Grave and
                  Guillaume Lample},
  title        = {LLaMA: Open and Efficient Foundation Language Models},
  journal      = {CoRR},
  volume       = {abs/2302.13971},
  year         = {2023},
}

@article{Hugo2023LLaMa2,
  author       = {Hugo Touvron and 
                  Louis Martin and 
                  Kevin Stone and 
                  Peter Albert and 
                  Amjad Almahairi and 
                  Yasmine Babaei and 
                  Nikolay Bashlykov and 
                  Soumya Batra and 
                  Prajjwal Bhargava and 
                  Shruti Bhosale and 
                  Dan Bikel and 
                  Lukas Blecher and 
                  Cristian Canton Ferrer and 
                  Moya Chen and 
                  Guillem Cucurull and 
                  David Esiobu and 
                  Jude Fernandes and 
                  Jeremy Fu and 
                  Wenyin Fu and 
                  Brian Fuller and 
                  Cynthia Gao and 
                  Vedanuj Goswami and 
                  Naman Goyal and 
                  Anthony Hartshorn and 
                  Saghar Hosseini and 
                  Rui Hou and 
                  Hakan Inan and 
                  Marcin Kardas and 
                  Viktor Kerkez and 
                  Madian Khabsa and 
                  Isabel Kloumann and 
                  Artem Korenev and 
                  Punit Singh Koura and 
                  Marie-Anne Lachaux and 
                  Thibaut Lavril and 
                  Jenya Lee and 
                  Diana Liskovich and 
                  Yinghai Lu and 
                  Yuning Mao and 
                  Xavier Martinet and 
                  Todor Mihaylov and 
                  Pushkar Mishra and 
                  Igor Molybog and 
                  Yixin Nie and 
                  Andrew Poulton and 
                  Jeremy Reizenstein and 
                  Rashi Rungta and 
                  Kalyan Saladi and 
                  Alan Schelten and 
                  Ruan Silva and 
                  Eric Michael Smith and 
                  Ranjan Subramanian and 
                  Xiaoqing Ellen Tan and 
                  Binh Tang and 
                  Ross Taylor and 
                  Adina Williams and 
                  Jian Xiang Kuan and 
                  Puxin Xu and 
                  Zheng Yan and 
                  Iliyan Zarov and 
                  Yuchen Zhang and 
                  Angela Fan and 
                  Melanie Kambadur and 
                  Sharan Narang and 
                  Aurelien Rodriguez and 
                  Robert Stojnic and 
                  Sergey Edunov and 
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
}

@article{Rohan2023PaLM2,
  author       = {Rohan Anil and
                  Andrew M. Dai and
                  Orhan Firat and
                  Melvin Johnson and
                  Dmitry Lepikhin and
                  Alexandre Passos and
                  Siamak Shakeri and
                  Emanuel Taropa and
                  Paige Bailey and
                  Zhifeng Chen and
                  Eric Chu and
                  Jonathan H. Clark and
                  Laurent El Shafey and
                  Yanping Huang and
                  Kathy Meier{-}Hellstern and
                  Gaurav Mishra and
                  Erica Moreira and
                  Mark Omernick and
                  Kevin Robinson and
                  Sebastian Ruder and
                  Yi Tay and
                  Kefan Xiao and
                  Yuanzhong Xu and
                  Yujing Zhang and
                  Gustavo Hern{\'{a}}ndez {\'{A}}brego and
                  Junwhan Ahn and
                  Jacob Austin and
                  Paul Barham and
                  Jan A. Botha and
                  James Bradbury and
                  Siddhartha Brahma and
                  Kevin Brooks and
                  Michele Catasta and
                  Yong Cheng and
                  Colin Cherry and
                  Christopher A. Choquette{-}Choo and
                  Aakanksha Chowdhery and
                  Cl{\'{e}}ment Crepy and
                  Shachi Dave and
                  Mostafa Dehghani and
                  Sunipa Dev and
                  Jacob Devlin and
                  Mark D{\'{\i}}az and
                  Nan Du and
                  Ethan Dyer and
                  Vladimir Feinberg and
                  Fangxiaoyu Feng and
                  Vlad Fienber and
                  Markus Freitag and
                  Xavier Garcia and
                  Sebastian Gehrmann and
                  Lucas Gonzalez and
                  et al.},
  title        = {PaLM 2 Technical Report},
  journal      = {CoRR},
  volume       = {abs/2305.10403},
  year         = {2023},
}

@misc{Rohan2023alpaca,
  author       = {Rohan Taori and 
                  Ishaan Gulrajani and 
                  Tianyi Zhang and 
                  Yann Dubois and 
                  Xuechen Li and 
                  Carlos Guestrin and 
                  Percy Liang and 
                  Tatsunori B. Hashimoto},
  title        = {Stanford Alpaca: An Instruction-following LLaMA model},
  year         = {2023},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{Lianmin2023Vicuna,
  author       = {Lianmin Zheng and
                  Wei{-}Lin Chiang and
                  Ying Sheng and
                  Siyuan Zhuang and
                  Zhanghao Wu and
                  Yonghao Zhuang and
                  Zi Lin and
                  Zhuohan Li and
                  Dacheng Li and
                  Eric P. Xing and
                  Hao Zhang and
                  Joseph E. Gonzalez and
                  Ion Stoica},
  title        = {Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  journal      = {CoRR},
  volume       = {abs/2306.05685},
  year         = {2023},
}

% Autonomous Agents
@article{Yongliang2023HuggingGPT,
  author       = {Yongliang Shen and
                  Kaitao Song and
                  Xu Tan and
                  Dongsheng Li and
                  Weiming Lu and
                  Yueting Zhuang},
  title        = {HuggingGPT: Solving {AI} Tasks with ChatGPT and its Friends in HuggingFace},
  journal      = {CoRR},
  volume       = {abs/2303.17580},
  year         = {2023},
}

@article{Yaobo2023TaskMatrix,
  author       = {Yaobo Liang and
                  Chenfei Wu and
                  Ting Song and
                  Wenshan Wu and
                  Yan Xia and
                  Yu Liu and
                  Yang Ou and
                  Shuai Lu and
                  Lei Ji and
                  Shaoguang Mao and
                  Yun Wang and
                  Linjun Shou and
                  Ming Gong and
                  Nan Duan},
  title        = {TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with
                  Millions of APIs},
  journal      = {CoRR},
  volume       = {abs/2303.16434},
  year         = {2023},
}

@article{Wu2023Visual,
  author       = {Chenfei Wu and
                  Shengming Yin and
                  Weizhen Qi and
                  Xiaodong Wang and
                  Zecheng Tang and
                  Nan Duan},
  title        = {Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation
                  Models},
  journal      = {CoRR},
  volume       = {abs/2303.04671},
  year         = {2023},
}

@article{Zhengyuan2023MMReact,
  author       = {Zhengyuan Yang and
                  Linjie Li and
                  Jianfeng Wang and
                  Kevin Lin and
                  Ehsan Azarnasab and
                  Faisal Ahmed and
                  Zicheng Liu and
                  Ce Liu and
                  Michael Zeng and
                  Lijuan Wang},
  title        = {{MM-REACT:} Prompting ChatGPT for Multimodal Reasoning and Action},
  journal      = {CoRR},
  volume       = {abs/2303.11381},
  year         = {2023},
}

@misc{gravitas2023auto,
  title        = {Auto-gpt: An autonomous gpt-4 experiment},
  author       = {Gravitas, Significant},
  year         = {2023},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/Significant-Gravitas/Auto-GPT}}
}

@misc{Yohei2023BabyAGI,
  title        = {BabyAGI},
  author       = {Yohei Nakajima},
  year         = {2023},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/yoheinakajima/babyagi}}
}



% Dataset for Tools
@article{Qin2023Toolbench,
  author       = {Yujia Qin and
                  Shengding Hu and
                  Yankai Lin and
                  Weize Chen and
                  Ning Ding and
                  Ganqu Cui and
                  Zheni Zeng and
                  Yufei Huang and
                  Chaojun Xiao and
                  Chi Han and
                  Yi Ren Fung and
                  Yusheng Su and
                  Huadong Wang and
                  Cheng Qian and
                  Runchu Tian and
                  Kunlun Zhu and
                  Shihao Liang and
                  Xingyu Shen and
                  Bokai Xu and
                  Zhen Zhang and
                  Yining Ye and
                  Bowen Li and
                  Ziwei Tang and
                  Jing Yi and
                  Yuzhang Zhu and
                  Zhenning Dai and
                  Lan Yan and
                  Xin Cong and
                  Yaxi Lu and
                  Weilin Zhao and
                  Yuxiang Huang and
                  Junxi Yan and
                  Xu Han and
                  Xian Sun and
                  Dahai Li and
                  Jason Phang and
                  Cheng Yang and
                  Tongshuang Wu and
                  Heng Ji and
                  Zhiyuan Liu and
                  Maosong Sun},
  title        = {Tool Learning with Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2304.08354},
  year         = {2023},
}

@article{Shishir2023Gorilla,
  author       = {Shishir G. Patil and
                  Tianjun Zhang and
                  Xin Wang and
                  Joseph E. Gonzalez},
  title        = {Gorilla: Large Language Model Connected with Massive APIs},
  journal      = {CoRR},
  volume       = {abs/2305.15334},
  year         = {2023},
}

@article{Minghao2023APIBank,
  author       = {Minghao Li and
                  Feifan Song and
                  Bowen Yu and
                  Haiyang Yu and
                  Zhoujun Li and
                  Fei Huang and
                  Yongbin Li},
  title        = {API-Bank: {A} Benchmark for Tool-Augmented LLMs},
  journal      = {CoRR},
  volume       = {abs/2304.08244},
  year         = {2023},
}

@article{Qiantong2023Toolbenchv2,
  author       = {Qiantong Xu and
                  Fenglu Hong and
                  Bo Li and
                  Changran Hu and
                  Zhengyu Chen and
                  Jian Zhang},
  title        = {On the Tool Manipulation Capability of Open-source Large Language
                  Models},
  journal      = {CoRR},
  volume       = {abs/2305.16504},
  year         = {2023},
}

@article{Qiaoyu2023ToolAlpaca,
  author       = {Qiaoyu Tang and
                  Ziliang Deng and
                  Hongyu Lin and
                  Xianpei Han and
                  Qiao Liang and
                  Le Sun},
  title        = {ToolAlpaca: Generalized Tool Learning for Language Models with 3000
                  Simulated Cases},
  journal      = {CoRR},
  volume       = {abs/2306.05301},
  year         = {2023},
}

% Other LLM eval
@misc{Xuechen2023alpaca_eval,
  author       = {Xuechen Li and 
                  Tianyi Zhang and 
                  Yann Dubois and 
                  Rohan Taori and 
                  Ishaan Gulrajani and 
                  Carlos Guestrin and 
                  Percy Liang and 
                  Tatsunori B. Hashimoto},
  title        = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year         = {2023},
  publisher    = {GitHub},
  journal      = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/alpaca_eval}}
}

@article{Wanjun2023AGIEval,
  author       = {Wanjun Zhong and
                  Ruixiang Cui and
                  Yiduo Guo and
                  Yaobo Liang and
                  Shuai Lu and
                  Yanlin Wang and
                  Amin Saied and
                  Weizhu Chen and
                  Nan Duan},
  title        = {AGIEval: {A} Human-Centric Benchmark for Evaluating Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2304.06364},
  year         = {2023},
}

@article{Yupeng2023LLMEval,
  author       = {Yupeng Chang and
                  Xu Wang and
                  Jindong Wang and
                  Yuan Wu and
                  Kaijie Zhu and
                  Hao Chen and
                  Linyi Yang and
                  Xiaoyuan Yi and
                  Cunxiang Wang and
                  Yidong Wang and
                  Wei Ye and
                  Yue Zhang and
                  Yi Chang and
                  Philip S. Yu and
                  Qiang Yang and
                  Xing Xie},
  title        = {A Survey on Evaluation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2307.03109},
  year         = {2023},
}

% Planning
@inproceedings{Jason2022CoT,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Brian Ichter and
                  Fei Xia and
                  Ed H. Chi and
                  Quoc V. Le and
                  Denny Zhou},
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  booktitle    = {NeurIPS},
  year         = {2022},
}

@article{Shunyu2023ToT,
  author       = {Shunyu Yao and
                  Dian Yu and
                  Jeffrey Zhao and
                  Izhak Shafran and
                  Thomas L. Griffiths and
                  Yuan Cao and
                  Karthik Narasimhan},
  title        = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2305.10601},
  year         = {2023},
}

@inproceedings{Shunyu2023ReAct,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2023},
}

@article{Noah2023Reflexion,
  author       = {Noah Shinn and
                  Beck Labash and
                  Ashwin Gopinath},
  title        = {Reflexion: an autonomous agent with dynamic memory and self-reflection},
  journal      = {CoRR},
  volume       = {abs/2303.11366},
  year         = {2023},
}

@inproceedings{wang-etal-2023-self-instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508",
}

@inproceedings{Lei2023PlanAndExecute,
  author       = {Lei Wang and
                  Wanyu Xu and
                  Yihuai Lan and
                  Zhiqiang Hu and
                  Yunshi Lan and
                  Roy Ka{-}Wei Lee and
                  Ee{-}Peng Lim},
  title        = {Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning
                  by Large Language Models},
  booktitle    = {ACL},
  pages        = {2609--2634},
  year         = {2023},
}

% Previous datasets
@inproceedings{Alex2019GLUE,
  author       = {Alex Wang and
                  Amanpreet Singh and
                  Julian Michael and
                  Felix Hill and
                  Omer Levy and
                  Samuel R. Bowman},
  title        = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
                  Language Understanding},
  booktitle    = {ICLR},
  year         = {2019},
}

@inproceedings{Alex2019SUperGLUE,
  author       = {Alex Wang and
                  Yada Pruksachatkun and
                  Nikita Nangia and
                  Amanpreet Singh and
                  Julian Michael and
                  Felix Hill and
                  Omer Levy and
                  Samuel R. Bowman},
  title        = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
                  Systems},
  booktitle    = {NeurIPS},
  pages        = {3261--3275},
  year         = {2019},
}

@inproceedings{Dan2021MMLU,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andy Zou and
                  Mantas Mazeika and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Massive Multitask Language Understanding},
  booktitle    = {ICLR},
  year         = {2021},
}

@article{Karl2021GSM8K,
  author       = {Karl Cobbe and
                  Vineet Kosaraju and
                  Mohammad Bavarian and
                  Mark Chen and
                  Heewoo Jun and
                  Lukasz Kaiser and
                  Matthias Plappert and
                  Jerry Tworek and
                  Jacob Hilton and
                  Reiichiro Nakano and
                  Christopher Hesse and
                  John Schulman},
  title        = {Training Verifiers to Solve Math Word Problems},
  journal      = {CoRR},
  volume       = {abs/2110.14168},
  year         = {2021},
}

% Others
@article{Jason2022Emergent,
  author       = {Jason Wei and
                  Yi Tay and
                  Rishi Bommasani and
                  Colin Raffel and
                  Barret Zoph and
                  Sebastian Borgeaud and
                  Dani Yogatama and
                  Maarten Bosma and
                  Denny Zhou and
                  Donald Metzler and
                  Ed H. Chi and
                  Tatsunori Hashimoto and
                  Oriol Vinyals and
                  Percy Liang and
                  Jeff Dean and
                  William Fedus},
  title        = {Emergent Abilities of Large Language Models},
  journal      = {Trans. Mach. Learn. Res.},
  year         = {2022},
}

@inproceedings{Jason2022InstructTuning,
  author       = {Jason Wei and
                  Maarten Bosma and
                  Vincent Y. Zhao and
                  Kelvin Guu and
                  Adams Wei Yu and
                  Brian Lester and
                  Nan Du and
                  Andrew M. Dai and
                  Quoc V. Le},
  title        = {Finetuned Language Models are Zero-Shot Learners},
  booktitle    = {ICLR},
  publisher    = {OpenReview.net},
  year         = {2022},
}


@article{Thomas2019HuggingFace,
  author       = {Thomas Wolf and
                  Lysandre Debut and
                  Victor Sanh and
                  Julien Chaumond and
                  Clement Delangue and
                  Anthony Moi and
                  Pierric Cistac and
                  Tim Rault and
                  R{\'{e}}mi Louf and
                  Morgan Funtowicz and
                  Jamie Brew},
  title        = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/1910.03771},
  year         = {2019},
}

@article{Xiao2023AgentBench,
  author       = {Xiao Liu and
                  Hao Yu and
                  Hanchen Zhang and
                  Yifan Xu and
                  Xuanyu Lei and
                  Hanyu Lai and
                  Yu Gu and
                  Hangliang Ding and
                  Kaiwen Men and
                  Kejuan Yang and
                  Shudan Zhang and
                  Xiang Deng and
                  Aohan Zeng and
                  Zhengxiao Du and
                  Chenhui Zhang and
                  Sheng Shen and
                  Tianjun Zhang and
                  Yu Su and
                  Huan Sun and
                  Minlie Huang and
                  Yuxiao Dong and
                  Jie Tang},
  title        = {AgentBench: Evaluating LLMs as Agents},
  journal      = {CoRR},
  volume       = {abs/2308.03688},
  year         = {2023},
}

@article{Timo2023Toolformer,
  author       = {Timo Schick and
                  Jane Dwivedi{-}Yu and
                  Roberto Dess{\`{\i}} and
                  Roberta Raileanu and
                  Maria Lomeli and
                  Luke Zettlemoyer and
                  Nicola Cancedda and
                  Thomas Scialom},
  title        = {Toolformer: Language Models Can Teach Themselves to Use Tools},
  journal      = {CoRR},
  volume       = {abs/2302.04761},
  year         = {2023},
}


% zotero

@misc{AddonItem,
  title = {Addon {{Item}}}
}

@misc{akataPlayingRepeatedGames2023,
  title = {Playing Repeated Games with {{Large Language Models}}},
  author = {Akata, Elif and Schulz, Lion and {Coda-Forno}, Julian and Oh, Seong Joon and Bethge, Matthias and Schulz, Eric},
  year = {2023},
  month = may,
  number = {arXiv:2305.16867},
  eprint = {2305.16867},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.16867},
  abstract = {Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM's cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures. In a large set of two players-two strategies games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination. We, therefore, further focus on two games from these distinct families. In the canonical iterated Prisoner's Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behavior of the simple convention to alternate between options. We verify that these behavioral signatures are stable across robustness checks. Finally, we show how GPT-4's behavior can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice. These results enrich our understanding of LLM's social behavior and pave the way for a behavioral game theory for machines.},
  archiveprefix = {arxiv}
}

@misc{aksitovReSTMeetsReAct2023,
  title = {{{ReST}} Meets {{ReAct}}: {{Self-Improvement}} for {{Multi-Step Reasoning LLM Agent}}},
  shorttitle = {{{ReST}} Meets {{ReAct}}},
  author = {Aksitov, Renat and Miryoosefi, Sobhan and Li, Zonglin and Li, Daliang and Babayan, Sheila and Kopparapu, Kavya and Fisher, Zachary and Guo, Ruiqi and Prakash, Sushant and Srinivasan, Pranesh and Zaheer, Manzil and Yu, Felix and Kumar, Sanjiv},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10003},
  eprint = {2312.10003},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.10003},
  abstract = {Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.},
  archiveprefix = {arxiv}
}

@misc{akterIndepthLookGemini2023,
  title = {An {{In-depth Look}} at {{Gemini}}'s {{Language Abilities}}},
  author = {Akter, Syeda Nahida and Yu, Zichun and Muhamed, Aashiq and Ou, Tianyue and B{\"a}uerle, Alex and Cabrera, {\'A}ngel Alexander and Dholakia, Krish and Xiong, Chenyan and Neubig, Graham},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11444},
  eprint = {2312.11444},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.11444},
  abstract = {The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark},
  archiveprefix = {arxiv}
}

@misc{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  year = {2022},
  month = nov,
  number = {arXiv:2204.14198},
  eprint = {2204.14198},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.14198},
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  archiveprefix = {arxiv}
}

@misc{alizadehLLMFlashEfficient2023,
  title = {{{LLM}} in a Flash: {{Efficient Large Language Model Inference}} with {{Limited Memory}}},
  shorttitle = {{{LLM}} in a Flash},
  author = {Alizadeh, Keivan and Mirzadeh, Iman and Belenko, Dmitry and Khatamifard, Karen and Cho, Minsik and Del Mundo, Carlo C. and Rastegari, Mohammad and Farajtabar, Mehrdad},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11514},
  eprint = {2312.11514},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their intensive computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters on flash memory but bringing them on demand to DRAM. Our method involves constructing an inference cost model that harmonizes with the flash memory behavior, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this flash memory-informed framework, we introduce two principal techniques. First, "windowing'" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.},
  archiveprefix = {arxiv}
}

@techreport{AlphaZero,
  title = {{{AlphaZero}}}
}

@misc{alshikhBecomingSelfinstructIntroducing2023,
  title = {Becoming Self-Instruct: Introducing Early Stopping Criteria for Minimal Instruct Tuning},
  shorttitle = {Becoming Self-Instruct},
  author = {AlShikh, Waseem and Daaboul, Manhal and Goddard, Kirk and Imel, Brock and Kamble, Kiran and Kulkarni, Parikshith and Russak, Melisa},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03692},
  eprint = {2307.03692},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.03692},
  abstract = {In this paper, we introduce the Instruction Following Score (IFS), a metric that detects language models' ability to follow instructions. The metric has a dual purpose. First, IFS can be used to distinguish between base and instruct models. We benchmark publicly available base and instruct models, and show that the ratio of well formatted responses to partial and full sentences can be an effective measure between those two model classes. Secondly, the metric can be used as an early stopping criteria for instruct tuning. We compute IFS for Supervised Fine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to follow instructions relatively early in the training process, and the further finetuning can result in changes in the underlying base model semantics. As an example of semantics change we show the objectivity of model predictions, as defined by an auxiliary metric ObjecQA. We show that in this particular case, semantic changes are the steepest when the IFS tends to plateau. We hope that decomposing instruct tuning into IFS and semantic factors starts a new trend in better controllable instruct tuning and opens possibilities for designing minimal instruct interfaces querying foundation models.},
  archiveprefix = {arxiv}
}

@misc{AnnotatedDiffusionModel,
  title = {The {{Annotated Diffusion Model}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/blog/annotated-diffusion}
}

@misc{asaiSelfRAGLearningRetrieve2023,
  title = {Self-{{RAG}}: {{Learning}} to {{Retrieve}}, {{Generate}}, and {{Critique}} through {{Self-Reflection}}},
  shorttitle = {Self-{{RAG}}},
  author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11511},
  eprint = {2310.11511},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.11511},
  abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{baiLongAlignRecipeLong2024,
  title = {{{LongAlign}}: {{A Recipe}} for {{Long Context Alignment}} of {{Large Language Models}}},
  shorttitle = {{{LongAlign}}},
  author = {Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi},
  year = {2024},
  month = jan,
  number = {arXiv:2401.18058},
  eprint = {2401.18058},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Extending large language models to effectively handle long contexts requires instruction fine-tuning on input sequences of similar length. To address this, we present LongAlign -- a recipe of the instruction data, training, and evaluation for long context alignment. First, we construct a long instruction-following dataset using Self-Instruct. To ensure the data diversity, it covers a broad range of tasks from various long context sources. Second, we adopt the packing and sorted batching strategies to speed up supervised fine-tuning on data with varied length distributions. Additionally, we develop a loss weighting method to balance the contribution to the loss across different sequences during packing training. Third, we introduce the LongBench-Chat benchmark for evaluating instruction-following capabilities on queries of 10k-100k in length. Experiments show that LongAlign outperforms existing recipes for LLMs in long context tasks by up to 30{\textbackslash}\%, while also maintaining their proficiency in handling short, generic tasks. The code, data, and long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.},
  archiveprefix = {arxiv}
}

@misc{baiQwenVLVersatileVisionLanguage2023,
  title = {Qwen-{{VL}}: {{A Versatile Vision-Language Model}} for {{Understanding}}, {{Localization}}, {{Text Reading}}, and {{Beyond}}},
  shorttitle = {Qwen-{{VL}}},
  author = {Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  year = {2023},
  month = oct,
  number = {arXiv:2308.12966},
  eprint = {2308.12966},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.},
  archiveprefix = {arxiv}
}

@article{BANERNamedEntity,
  title = {{{BA-NER}}: {{A Named Entity Recognition Method Based}} on {{Entity Boundary Awareness}}}
}

@misc{bansalLLMAugmentedLLMs2024,
  title = {{{LLM Augmented LLMs}}: {{Expanding Capabilities}} through {{Composition}}},
  shorttitle = {{{LLM Augmented LLMs}}},
  author = {Bansal, Rachit and Samanta, Bidisha and Dalmia, Siddharth and Gupta, Nitish and Vashishth, Shikhar and Ganapathy, Sriram and Bapna, Abhishek and Jain, Prateek and Talukdar, Partha},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  abstract = {Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13{\textbackslash}\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40{\textbackslash}\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.},
  howpublished = {https://arxiv.org/abs/2401.02412v1},
  langid = {english}
}

@misc{baoAnalyticDPMAnalyticEstimate2022,
  title = {Analytic-{{DPM}}: An {{Analytic Estimate}} of the {{Optimal Reverse Variance}} in {{Diffusion Probabilistic Models}}},
  shorttitle = {Analytic-{{DPM}}},
  author = {Bao, Fan and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
  year = {2022},
  month = may,
  number = {arXiv:2201.06503},
  eprint = {2201.06503},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands of timesteps. A key problem in the inference is to estimate the variance in each timestep of the reverse process. In this work, we present a surprising result that both the optimal reverse variance and the corresponding optimal KL divergence of a DPM have analytic forms w.r.t. its score function. Building upon it, we propose Analytic-DPM, a training-free inference framework that estimates the analytic forms of the variance and KL divergence using the Monte Carlo method and a pretrained score-based model. Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip the estimate for a better result. Empirically, our analytic-DPM improves the log-likelihood of various DPMs, produces high-quality samples, and meanwhile enjoys a 20x to 80x speed up.},
  archiveprefix = {arxiv}
}

@misc{Ber666LlmreasonersLibrary,
  title = {Ber666/Llm-Reasoners: {{A}} Library for Advanced Large Language Model Reasoning},
  howpublished = {https://github.com/Ber666/llm-reasoners/tree/main}
}

@misc{bernsteinCompilingMLModels2023,
  title = {Compiling {{ML}} Models to {{C}} for Fun},
  author = {Bernstein, Max},
  year = {2023},
  month = sep,
  journal = {Max Bernstein},
  abstract = {We make micrograd fly with a little compiler magic. In this post, we'll write a ML compiler from scratch.},
  howpublished = {https://bernsteinbear.com/blog/compiling-ml-models/},
  langid = {english}
}

@misc{bestaGraphThoughtsSolving2023,
  title = {Graph of {{Thoughts}}: {{Solving Elaborate Problems}} with {{Large Language Models}}},
  shorttitle = {Graph of {{Thoughts}}},
  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  year = {2023},
  month = aug,
  number = {arXiv:2308.09687},
  eprint = {2308.09687},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {$>$}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
  archiveprefix = {arxiv}
}

@misc{boehmHowOptimizeCUDA2022,
  title = {How to {{Optimize}} a {{CUDA Matmul Kernel}} for {{cuBLAS-like Performance}}: A {{Worklog}}},
  shorttitle = {How to {{Optimize}} a {{CUDA Matmul Kernel}} for {{cuBLAS-like Performance}}},
  author = {Boehm, Simon},
  year = {2022},
  month = dec,
  abstract = {In this post, I'll iteratively optimize an implementation of matrix multiplication written in CUDA.My goal is not to build a cuBLAS replacement, but to deepl...},
  howpublished = {http://siboehm.com/articles/22/CUDA-MMM}
}

@misc{boigePASTAPretrainedActionState2023,
  title = {{{PASTA}}: {{Pretrained Action-State Transformer Agents}}},
  shorttitle = {{{PASTA}}},
  author = {Boige, Raphael and {Flet-Berliac}, Yannis and Flajolet, Arthur and Richard, Guillaume and Pierrot, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10936},
  eprint = {2307.10936},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.10936},
  abstract = {Self-supervised learning has brought about a revolutionary paradigm shift in various computing domains, including NLP, vision, and biology. Recent approaches involve pre-training transformer models on vast amounts of unlabeled data, serving as a starting point for efficiently solving downstream tasks. In the realm of reinforcement learning, researchers have recently adapted these approaches by developing models pre-trained on expert trajectories, enabling them to address a wide range of tasks, from robotics to recommendation systems. However, existing methods mostly rely on intricate pre-training objectives tailored to specific downstream applications. This paper presents a comprehensive investigation of models we refer to as Pretrained Action-State Transformer Agents (PASTA). Our study uses a unified methodology and covers an extensive set of general downstream tasks including behavioral cloning, offline RL, sensor failure robustness, and dynamics change adaptation. Our goal is to systematically compare various design choices and provide valuable insights to practitioners for building robust models. Key highlights of our study include tokenization at the action and state component level, using fundamental pre-training objectives like next token prediction, training models across diverse domains simultaneously, and using parameter efficient fine-tuning (PEFT). The developed models in our study contain fewer than 10 million parameters and the application of PEFT enables fine-tuning of fewer than 10,000 parameters during downstream adaptation, allowing a broad community to use these models and reproduce our experiments. We hope that this study will encourage further research into the use of transformers with first-principles design choices to represent RL trajectories and contribute to robust policy learning.},
  archiveprefix = {arxiv}
}

@article{borisovLANGUAGEMODELSARE2023,
  title = {{{LANGUAGE MODELS ARE REALISTIC TABULAR DATA GENERATORS}}},
  author = {Borisov, Vadim and Se{\ss}ler, Kathrin and Leemann, Tobias and Pawelczyk, Martin and Kasneci, Gjergji},
  year = {2023},
  abstract = {Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data's characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.},
  langid = {english}
}

@misc{BreakSequentialDependency,
  title = {Break the {{Sequential Dependency}} of {{LLM Inference Using Lookahead Decoding}} {\textbar} {{LMSYS Org}}},
  howpublished = {https://lmsys.org/blog/2023-11-21-lookahead-decoding/}
}

@misc{brooksLargeLanguageModels2023,
  title = {Large {{Language Models}} Can {{Implement Policy Iteration}}},
  author = {Brooks, Ethan and Walls, Logan and Lewis, Richard L. and Singh, Satinder},
  year = {2023},
  month = aug,
  number = {arXiv:2210.03821},
  eprint = {2210.03821},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03821},
  abstract = {This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the "few-shot" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex, a language model with no prior knowledge of the domains on which we evaluate it.},
  archiveprefix = {arxiv}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv}
}

@misc{caiHumanintheLoopChainofThought2023,
  title = {Human-in-the-{{Loop}} through {{Chain-of-Thought}}},
  author = {Cai, Zefan and Chang, Baobao and Han, Wenjuan},
  year = {2023},
  month = jun,
  number = {arXiv:2306.07932},
  eprint = {2306.07932},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While the emergence of powerful language models along with Chain-of-thought prompting has made automation more and more omnipresent, it sometimes demonstrates its weakness in long-term or multi-step logical reasoning. For example, users don't always get desirable answers for complex mathematical problems without human involvement. Against this background, we present the Manual Correction System (MCS) {\textemdash} a human-in-the-loop system enhanced by Chainof-Thought prompting, which explores how manual correction of sub-logics in rationales can improve LLM's reasoning performance. Moving one step forward, considering a system with human-in-the-loop involves more than having humans improve performance but also controlling the cost. Therefore, we post a Costutility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on classical economics theory to analyze, quantify and balance the utility and the corresponding cost. We conduct experiments of MCS and CAMLOP with twelve datasets. A significant advantage w.r.t cost and utility proves its superiority over strong baselines.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{caiLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Tool Makers}}},
  author = {Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun and Zhou, Denny},
  year = {2023},
  month = may,
  number = {arXiv:2305.17126},
  eprint = {2305.17126},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Recent research shows the potential of enhancing the problem-solving ability of large language models (LLMs) through the use of external tools. However, prior work along this line depends on the availability of existing tools. In this work, we take an initial step towards removing this dependency by proposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two key phases: 1) tool making: an LLM acts as the tool maker that crafts tools for given tasks, where a tool is implemented as a Python utility function. 2) tool using: an LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. Tool-making enables an LLM to continually generate tools that can be applied to different requests so that future requests can call the corresponding APIs when beneficial for solving the tasks. Furthermore, the division of labor among LLMs for tool-making and tool-using phases introduces the opportunity to achieve cost effectiveness without degrading the quality of generated tools and problem solutions. For example, recognizing that tool-making demands more sophisticated capabilities than tool-using, we can apply a powerful yet resource-intensive model as the tool maker, and a lightweight while cost-effective model as the tool user. We validate the effectiveness of our approach across a variety of complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM can achieve performance that is on par with using GPT-4 for both tool making and tool using, while the inference cost is significantly reduced.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{caoInstructionMiningHighQuality2023,
  title = {Instruction {{Mining}}: {{High-Quality Instruction Data Selection}} for {{Large Language Models}}},
  shorttitle = {Instruction {{Mining}}},
  author = {Cao, Yihan and Kang, Yanbin and Sun, Lichao},
  year = {2023},
  month = jul,
  number = {arXiv:2307.06290},
  eprint = {2307.06290},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.06290},
  abstract = {Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive finetuning experiments. The experiment results are then applied to estimating parameters in InstructMining. To further investigate its performance, we use InstructMining to select high-quality data from unseen datasets. Results demonstrate that InstructMining can help select relatively high-quality samples from various instruction-following datasets. Compared to models finetuned on unfiltered datasets, models finetuned on InstructMining selected datasets perform better on 42.5\% cases.},
  archiveprefix = {arxiv}
}

@misc{cartaGroundingLargeLanguage2023,
  title = {Grounding {{Large Language Models}} in {{Interactive Environments}} with {{Online Reinforcement Learning}}},
  author = {Carta, Thomas and Romac, Cl{\'e}ment and Wolf, Thomas and Lamprier, Sylvain and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  year = {2023},
  month = sep,
  number = {arXiv:2302.02662},
  eprint = {2302.02662},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.02662},
  abstract = {Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.},
  archiveprefix = {arxiv}
}

@misc{changSurveyEvaluationLarge2023,
  title = {A {{Survey}} on {{Evaluation}} of {{Large Language Models}}},
  author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03109},
  eprint = {2307.03109},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.03109},
  abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
  archiveprefix = {arxiv}
}

@inproceedings{chanHarmsIncreasinglyAgentic2023,
  title = {Harms from {{Increasingly Agentic Algorithmic Systems}}},
  booktitle = {2023 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Chan, Alan and Salganik, Rebecca and Markelius, Alva and Pang, Chris and Rajkumar, Nitarshan and Krasheninnikov, Dmitrii and Langosco, Lauro and He, Zhonghao and Duan, Yawen and Carroll, Micah and Lin, Michelle and Mayhew, Alex and Collins, Katherine and Molamohammadi, Maryam and Burden, John and Zhao, Wanru and Rismani, Shalaleh and Voudouris, Konstantinos and Bhatt, Umang and Weller, Adrian and Krueger, David and Maharaj, Tegan},
  year = {2023},
  month = jun,
  pages = {651--666},
  publisher = {{ACM}},
  address = {{Chicago IL USA}},
  doi = {10.1145/3593013.3594033},
  isbn = {9798400701924},
  langid = {english}
}

@misc{chenAlpaGasusTrainingBetter2023,
  title = {{{AlpaGasus}}: {{Training A Better Alpaca}} with {{Fewer Data}}},
  shorttitle = {{{AlpaGasus}}},
  author = {Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and Jin, Hongxia},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08701},
  eprint = {2307.08701},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.08701},
  abstract = {Large language models{\textasciitilde}(LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant matches \${$>$}90{\textbackslash}\%\$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes {\textbackslash}footnote\{We apply IFT for the same number of epochs as Alpaca(7B) but on fewer data, using 4\${\textbackslash}times\$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.\}. Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models. Our project page is available at: {\textbackslash}url\{https://lichang-chen.github.io/AlpaGasus/\}.},
  archiveprefix = {arxiv}
}

@misc{chenCheXagentFoundationModel2024,
  title = {{{CheXagent}}: {{Towards}} a {{Foundation Model}} for {{Chest X-Ray Interpretation}}},
  shorttitle = {{{CheXagent}}},
  author = {Chen, Zhihong and Varma, Maya and Delbrouck, Jean-Benoit and Paschali, Magdalini and Blankemeier, Louis and Van Veen, Dave and Valanarasu, Jeya Maria Jose and Youssef, Alaa and Cohen, Joseph Paul and Reis, Eduardo Pontes and Tsai, Emily B. and Johnston, Andrew and Olsen, Cameron and Abraham, Tanishq Mathew and Gatidis, Sergios and Chaudhari, Akshay S. and Langlotz, Curtis},
  year = {2024},
  month = jan,
  number = {arXiv:2401.12208},
  eprint = {2401.12208},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.12208},
  abstract = {Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing {\textbackslash}emph\{CheXinstruct\} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present {\textbackslash}emph\{CheXagent\} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce {\textbackslash}emph\{CheXbench\} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at {\textbackslash}url\{https://stanford-aimi.github.io/chexagent.html\}.},
  archiveprefix = {arxiv}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.03374},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arxiv}
}

@misc{chenExtendingContextWindow2023,
  title = {Extending {{Context Window}} of {{Large Language Models}} via {{Positional Interpolation}}},
  author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  year = {2023},
  month = jun,
  number = {arXiv:2306.15595},
  eprint = {2306.15595},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.15595},
  abstract = {We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least \${\textbackslash}sim 600 {\textbackslash}times\$ smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure.},
  archiveprefix = {arxiv}
}

@misc{chenFireActLanguageAgent2023,
  title = {{{FireAct}}: {{Toward Language Agent Fine-tuning}}},
  shorttitle = {{{FireAct}}},
  author = {Chen, Baian and Shu, Chang and Shareghi, Ehsan and Collier, Nigel and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  abstract = {Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77\% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.},
  howpublished = {https://arxiv.org/abs/2310.05915v1},
  langid = {english}
}

@misc{chengAdversarialPreferenceOptimization2023,
  title = {Adversarial {{Preference Optimization}}},
  author = {Cheng, Pengyu and Yang, Yifan and Li, Jian and Dai, Yong and Du, Nan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.08045},
  eprint = {2311.08045},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.08045},
  abstract = {Human preference alignment is a crucial training step to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learning process. In experiments, we empirically verify the effectiveness of APO in improving LLM's helpfulness and harmlessness compared with rejection sampling baselines.},
  archiveprefix = {arxiv}
}

@misc{chengCanWeEdit2023,
  title = {Can {{We Edit Multimodal Large Language Models}}?},
  author = {Cheng, Siyuan and Tian, Bozhong and Liu, Qingbin and Chen, Xi and Wang, Yongheng and Chen, Huajun and Zhang, Ningyu},
  year = {2023},
  month = oct,
  number = {arXiv:2310.08475},
  eprint = {2310.08475},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.08475},
  abstract = {In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights{\textbackslash}footnote\{Code and dataset are available in https://github.com/zjunlp/EasyEdit.\vphantom\}},
  archiveprefix = {arxiv}
}

@misc{chenHowChatGPTBehavior2023,
  title = {How Is {{ChatGPT}}'s Behavior Changing over Time?},
  author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09009},
  eprint = {2307.09009},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6\%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4\%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the ``same'' LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLM quality.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{chenhsingChenHsingAwesomeVideoDiffusionModels2024,
  title = {{{ChenHsing}}/{{Awesome-Video-Diffusion-Models}}},
  author = {ChenHsing},
  year = {2024},
  month = jan,
  abstract = {[Arxiv] A Survey on Video Diffusion Models}
}

@misc{chenMiniGPTv2LargeLanguage2023,
  title = {{{MiniGPT-v2}}: Large Language Model as a Unified Interface for Vision-Language Multi-Task Learning},
  shorttitle = {{{MiniGPT-v2}}},
  author = {Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
  year = {2023},
  month = oct,
  number = {arXiv:2310.09478},
  eprint = {2310.09478},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https://minigpt-v2.github.io/},
  archiveprefix = {arxiv}
}

@misc{chenPutYourMoney2023,
  title = {Put {{Your Money Where Your Mouth Is}}: {{Evaluating Strategic Planning}} and {{Execution}} of {{LLM Agents}} in an {{Auction Arena}}},
  shorttitle = {Put {{Your Money Where Your Mouth Is}}},
  author = {Chen, Jiangjie and Yuan, Siyu and Ye, Rong and Majumder, Bodhisattwa Prasad and Richardson, Kyle},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05746},
  eprint = {2310.05746},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.05746},
  abstract = {Can Large Language Models (LLMs) simulate human behavior in complex environments? LLMs have recently been shown to exhibit advanced reasoning skills but much of NLP evaluation still relies on static benchmarks. Answering this requires evaluation environments that probe strategic reasoning in competitive, dynamic scenarios that involve long-term planning. We introduce AucArena, a novel simulation environment for evaluating LLMs within auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct several controlled simulations using state-of-the-art LLMs as bidding agents. We find that through simple prompting, LLMs do indeed demonstrate many of the skills needed for effectively engaging in auctions (e.g., managing budget, adhering to long-term goals and priorities), skills that we find can be sharpened by explicitly encouraging models to be adaptive and observe strategies in past auctions. These results are significant as they show the potential of using LLM agents to model intricate social dynamics, especially in competitive settings. However, we also observe considerable variability in the capabilities of individual LLMs. Notably, even our most advanced models (GPT-4) are occasionally surpassed by heuristic baselines and human agents, highlighting the potential for further improvements in the design of LLM agents and the important role that our simulation environment can play in further testing and refining agent architectures.},
  archiveprefix = {arxiv}
}

@misc{chenSelfPlayFineTuningConverts2024,
  title = {Self-{{Play Fine-Tuning Converts Weak Language Models}} to {{Strong Language Models}}},
  author = {Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01335},
  eprint = {2401.01335},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.01335},
  abstract = {Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents.},
  archiveprefix = {arxiv}
}

@misc{chenTeachingLargeLanguage2023,
  title = {Teaching {{Large Language Models}} to {{Self-Debug}}},
  author = {Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  year = {2023},
  month = apr,
  number = {arXiv:2304.05128},
  eprint = {2304.05128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose SELF-DEBUGGING, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that SELF-DEBUGGING can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. SELF-DEBUGGING achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-toPython generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, SELF-DEBUGGING with code explanation consistently improves the baseline by 2-3\%, and improves the prediction accuracy on problems of the hardest label by 9\%. On TransCoder and MBPP where unit tests are available, SELF-DEBUGGING improves the baseline accuracy by up to 12\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, SELFDEBUGGING notably improves sample efficiency, and can match or outperform baseline models that generate more than 10{\texttimes} candidate programs.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{chenTEvalEvaluatingTool2023,
  title = {T-{{Eval}}: {{Evaluating}} the {{Tool Utilization Capability Step}} by {{Step}}},
  shorttitle = {T-{{Eval}}},
  author = {Chen, Zehui and Du, Weihua and Zhang, Wenwei and Liu, Kuikun and Liu, Jiangning and Zheng, Miao and Zhuo, Jingming and Zhang, Songyang and Lin, Dahua and Chen, Kai and Zhao, Feng},
  year = {2023},
  month = dec,
  number = {arXiv:2312.14033},
  eprint = {2312.14033},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce {\textbackslash}shortname{\textasciitilde}to evaluate the tool utilization capability step by step. {\textbackslash}shortname{\textasciitilde}disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on {\textbackslash}shortname{\textasciitilde}and in-depth analysis of various LLMs. {\textbackslash}shortname{\textasciitilde} not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available at {\textbackslash}href\{https://github.com/open-compass/T-Eval\}\{https://github.com/open-compass/T-Eval\}.},
  archiveprefix = {arxiv}
}

@misc{chenUniversalSelfConsistencyLarge2023,
  title = {Universal {{Self-Consistency}} for {{Large Language Model Generation}}},
  author = {Chen, Xinyun and Aksitov, Renat and Alon, Uri and Ren, Jie and Xiao, Kefan and Yin, Pengcheng and Prakash, Sushant and Sutton, Charles and Wang, Xuezhi and Zhou, Denny},
  year = {2023},
  month = nov,
  journal = {arXiv.org},
  abstract = {Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.},
  howpublished = {https://arxiv.org/abs/2311.17311v1},
  langid = {english}
}

@misc{chenYangyiChenMultimodalANDLargeLanguageModels2024,
  title = {Yangyi-{{Chen}}/{{Multimodal-AND-Large-Language-Models}}},
  author = {Chen, Yangyi},
  year = {2024},
  month = jan,
  abstract = {Paper list about multimodal and large language models, only used to record papers I read in the daily arxiv for personal needs.}
}

@misc{chernFacToolFactualityDetection2023,
  title = {{{FacTool}}: {{Factuality Detection}} in {{Generative AI}} -- {{A Tool Augmented Framework}} for {{Multi-Task}} and {{Multi-Domain Scenarios}}},
  shorttitle = {{{FacTool}}},
  author = {Chern, I.-Chun and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei},
  year = {2023},
  month = jul,
  number = {arXiv:2307.13528},
  eprint = {2307.13528},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .},
  archiveprefix = {arxiv}
}

@misc{chiaContrastiveChainofThoughtPrompting2023,
  title = {Contrastive {{Chain-of-Thought Prompting}}},
  author = {Chia, Yew Ken and Chen, Guizhen and Tuan, Luu Anh and Poria, Soujanya and Bing, Lidong},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09277},
  eprint = {2311.09277},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.},
  archiveprefix = {arxiv}
}

@article{CoVariancebasedCausalDebiasing2023,
  title = {{{CoVariance-based Causal Debiasing}} for {{Entity}} and {{Relation Extraction}}},
  year = {2023},
  month = jun,
  abstract = {Joint entity and relation extraction tasks aim to recognize named entities and extract relations simultaneously. Suffering from a variety of data biases, such as data selection bias, and distribution bias (out of distribution, long-tail distribution), serious concerns can be witnessed to threaten the model's transferability, robustness, and generalization. In this work, we address the above problems from a causality perspective. We propose a novel causal framework called c\${\textbackslash}underline\{{\textbackslash}textbf\{o\}\}\$variance and \${\textbackslash}underline\{{\textbackslash}textbf\{v\}\}\$ariance \${\textbackslash}underline\{{\textbackslash}textbf\{o\}\}\$ptimization framework (OVO) to optimize feature representations and conduct general debiasing. In particular, the proposed \${\textbackslash}underline\{{\textbackslash}textbf\{c\}\}\$ovariance \${\textbackslash}underline\{{\textbackslash}textbf\{op\}\}\$timizing (COP) minimizes characterizing features' covariance for alleviating the selection and distribution bias and enhances feature representation in the feature space. Furthermore, based on the causal backdoor adjustment, we propose \${\textbackslash}{\textbackslash}underline\{{\textbackslash}textbf\{v\}\}\$ariance \${\textbackslash}underline\{{\textbackslash}textbf\{op\}\}\$timizing (VOP) separates samples in terms of label information and minimizes the variance of each dimension in the feature vectors of the same class label for mitigating the distribution bias further. By applying it to three strong baselines in two widely used datasets, the results demonstrate the effectiveness and generalization of OVO for joint entity and relation extraction tasks. Furthermore, a fine-grained analysis reveals that OVO possesses the capability to mitigate the impact of long-tail distribution.},
  langid = {english}
}

@misc{daiInstructBLIPGeneralpurposeVisionLanguage2023,
  title = {{{InstructBLIP}}: {{Towards General-purpose Vision-Language Models}} with {{Instruction Tuning}}},
  shorttitle = {{{InstructBLIP}}},
  author = {Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
  year = {2023},
  month = jun,
  number = {arXiv:2305.06500},
  eprint = {2305.06500},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7\% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{daiWhyCanGPT2023,
  title = {Why {{Can GPT Learn In-Context}}? {{Language Models Implicitly Perform Gradient Descent}} as {{Meta-Optimizers}}},
  shorttitle = {Why {{Can GPT Learn In-Context}}?},
  author = {Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  year = {2023},
  month = may,
  number = {arXiv:2212.10559},
  eprint = {2212.10559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at {\textbackslash}url\{https://aka.ms/icl\}.},
  archiveprefix = {arxiv}
}

@misc{DaMoXingTuiLiXingNengYouHuaZhiKVCacheJieDu,
  title = {{KV Cache}},
  journal = {},
  abstract = {0. KV Cache KV CacheSelf-AttentionKV CacheMLPKV Cacheblock{\dots}},
  howpublished = {https://zhuanlan.zhihu.com/p/630832593},
  langid = {chinese}
}

@article{daoFasterAttentionBetter,
  title = {Faster {{Attention}} with {{Better Parallelism}} and {{Work Partitioning}}},
  author = {Dao, Tri},
  langid = {english}
}

@misc{daoFlashAttentionFastMemoryEfficient2022,
  title = {{{FlashAttention}}: {{Fast}} and {{Memory-Efficient Exact Attention}} with {{IO-Awareness}}},
  shorttitle = {{{FlashAttention}}},
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14135},
  eprint = {2205.14135},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IOaware{\textemdash}accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15\% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3{\texttimes} speedup on GPT-2 (seq. length 1K), and 2.4{\texttimes} speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4\% accuracy) and Path-256 (seq. length 64K, 63.1\% accuracy).},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{dasguptaCollaboratingLanguageModels2023,
  title = {Collaborating with Language Models for Embodied Reasoning},
  author = {Dasgupta, Ishita and {Kaeser-Chen}, Christine and Marino, Kenneth and Ahuja, Arun and Babayan, Sheila and Hill, Felix and Fergus, Rob},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00763},
  eprint = {2302.00763},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.00763},
  abstract = {Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.},
  archiveprefix = {arxiv}
}

@misc{DeepspeedXiangJieYuanMaFenXi,
  title = {Deepspeed -}
}

@misc{dengMind2WebGeneralistAgent2023,
  title = {{{Mind2Web}}: {{Towards}} a {{Generalist Agent}} for the {{Web}}},
  shorttitle = {{{Mind2Web}}},
  author = {Deng, Xiang and Gu, Yu and Zheng, Boyuan and Chen, Shijie and Stevens, Samuel and Wang, Boshi and Sun, Huan and Su, Yu},
  year = {2023},
  month = jun,
  number = {arXiv:2306.06070},
  eprint = {2306.06070},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.06070},
  abstract = {We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.},
  archiveprefix = {arxiv}
}

@misc{dengUnifiedViewAnswer2023,
  title = {Towards {{A Unified View}} of {{Answer Calibration}} for {{Multi-Step Reasoning}}},
  author = {Deng, Shumin and Zhang, Ningyu and Oo, Nay and Hooi, Bryan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09101},
  eprint = {2311.09101},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. Usually, answer calibration strategies such as step-level or path-level calibration play a vital role in multi-step reasoning. While effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.},
  archiveprefix = {arxiv}
}

@misc{dingEverythingThoughtsDefying2023,
  title = {Everything of {{Thoughts}}: {{Defying}} the {{Law}} of {{Penrose Triangle}} for {{Thought Generation}}},
  shorttitle = {Everything of {{Thoughts}}},
  author = {Ding, Ruomeng and Zhang, Chaoyun and Wang, Lu and Xu, Yong and Ma, Minghua and Zhang, Wei and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
  year = {2023},
  month = nov,
  number = {arXiv:2311.04254},
  eprint = {2311.04254},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.04254},
  abstract = {Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as ``thoughts''. An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called ``Everything of Thoughts'' (XoT) to defy the law of ``Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions. We evaluate XoT on several challenging multi-solution problem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our results demonstrate that XoT significantly outperforms existing approaches. Notably, XoT can yield multiple solutions with just one LLM call, showcasing its remarkable proficiency in addressing complex problems across diverse domains.},
  archiveprefix = {arxiv}
}

@misc{dingLongNetScalingTransformers2023,
  title = {{{LongNet}}: {{Scaling Transformers}} to 1,000,000,000 {{Tokens}}},
  shorttitle = {{{LongNet}}},
  author = {Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02486},
  eprint = {2307.02486},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.},
  archiveprefix = {arxiv}
}

@misc{dingSparseLowrankAdaptation2023,
  title = {Sparse {{Low-rank Adaptation}} of {{Pre-trained Language Models}}},
  author = {Ding, Ning and Lv, Xingtai and Wang, Qiaosen and Chen, Yulin and Zhou, Bowen and Liu, Zhiyuan and Sun, Maosong},
  year = {2023},
  month = nov,
  number = {arXiv:2311.11696},
  eprint = {2311.11696},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.11696},
  abstract = {Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model's memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70\% retained parameters and 70\% training time.},
  archiveprefix = {arxiv}
}

@misc{dingStaticEvaluationCode2023,
  title = {A {{Static Evaluation}} of {{Code Completion}} by {{Large Language Models}}},
  author = {Ding, Hantian and Kumar, Varun and Tian, Yuchen and Wang, Zijian and Kwiatkowski, Rob and Li, Xiaopeng and Ramanathan, Murali Krishna and Ray, Baishakhi and Bhatia, Parminder and Sengupta, Sudipta and Roth, Dan and Xiang, Bing},
  year = {2023},
  month = jun,
  number = {arXiv:2306.03203},
  eprint = {2306.03203},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models trained on code have shown great potential to increase productivity of software developers. Several executionbased benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the contrary, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with executionbased evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models. Through extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{dongC3ZeroshotTexttoSQL2023,
  title = {C3: {{Zero-shot Text-to-SQL}} with {{ChatGPT}}},
  shorttitle = {C3},
  author = {Dong, Xuemei and Zhang, Chao and Ge, Yuhang and Mao, Yuren and Gao, Yunjun and Chen, lu and Lin, Jinshu and Lou, Dongfang},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07306},
  eprint = {2307.07306},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.07306},
  abstract = {This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3, which achieves 82.3{\textbackslash}\% in terms of execution accuracy on the holdout test set of Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the Spider Challenge. C3 consists of three key components: Clear Prompting (CP), Calibration with Hints (CH), and Consistent Output (CO), which are corresponding to the model input, model bias and model output respectively. It provides a systematic treatment for zero-shot Text-to-SQL. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposed method.},
  archiveprefix = {arxiv}
}

@misc{dongDreamLLMSynergisticMultimodal2023,
  title = {{{DreamLLM}}: {{Synergistic Multimodal Comprehension}} and {{Creation}}},
  shorttitle = {{{DreamLLM}}},
  author = {Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and Kong, Xiangwen and Zhang, Xiangyu and Ma, Kaisheng and Yi, Li},
  year = {2023},
  month = sep,
  number = {arXiv:2309.11499},
  eprint = {2309.11499},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.11499},
  abstract = {This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy.},
  archiveprefix = {arxiv}
}

@misc{dongRAFTRewardRAnked2023,
  title = {{{RAFT}}: {{Reward rAnked FineTuning}} for {{Generative Foundation Model Alignment}}},
  shorttitle = {{{RAFT}}},
  author = {Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  year = {2023},
  month = dec,
  number = {arXiv:2304.06767},
  eprint = {2304.06767},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.06767},
  abstract = {Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models.},
  archiveprefix = {arxiv}
}

@misc{dongSelfcollaborationCodeGeneration2023,
  title = {Self-Collaboration {{Code Generation}} via {{ChatGPT}}},
  author = {Dong, Yihong and Jiang, Xue and Jin, Zhi and Li, Ge},
  year = {2023},
  month = may,
  number = {arXiv:2304.07590},
  eprint = {2304.07590},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.07590},
  abstract = {Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLMs act as distinct ``experts'', each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other's work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three ChatGPT roles (i.e., analyst, coder, and tester) responsible for software development's analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9\%-47.1\% Pass@1 compared to direct code generation, achieving state-of-the-art performance and even surpassing GPT-4. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex real-world tasks that are not readily solved by direct code generation, as evidenced in case study.},
  archiveprefix = {arxiv}
}

@misc{DPMDDPMScorebased,
  title = {{{DPM}}, {{DDPM}}, {{Score-based DDPM}}: }
}

@misc{DPODirectPreference,
  title = {{DPO: Direct Preference Optimization }},
  shorttitle = {{DPO}},
  journal = {},
  abstract = {1 RLHF:DPODPORLHFRLHF DPO{\dots}},
  howpublished = {https://zhuanlan.zhihu.com/p/642569664},
  langid = {chinese}
}

@misc{driessPaLMEEmbodiedMultimodal2023,
  title = {{{PaLM-E}}: {{An Embodied Multimodal Language Model}}},
  shorttitle = {{{PaLM-E}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03378},
  eprint = {2303.03378},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.03378},
  abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  archiveprefix = {arxiv}
}

@misc{duImprovingFactualityReasoning2023,
  title = {Improving {{Factuality}} and {{Reasoning}} in {{Language Models}} through {{Multiagent Debate}}},
  author = {Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor},
  year = {2023},
  month = may,
  number = {arXiv:2305.14325},
  eprint = {2305.14325},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14325},
  abstract = {Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.},
  archiveprefix = {arxiv}
}

@misc{EAGLE,
  title = {{{EAGLE}}},
  howpublished = {https://sites.google.com/view/eagle-llm}
}

@misc{eldanWhoHarryPotter2023,
  title = {Who's {{Harry Potter}}? {{Approximate Unlearning}} in {{LLMs}}},
  shorttitle = {Who's {{Harry Potter}}?},
  author = {Eldan, Ronen and Russinovich, Mark},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02238},
  eprint = {2310.02238},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.},
  archiveprefix = {arxiv}
}

@misc{FastapiXiLieTongBuHeYiBuXiangHuZhuanHuanChuLiShiJianJueJin,
  title = {Fastapi- - },
  howpublished = {https://juejin.cn/post/7091839981953482789}
}

@misc{FeatureOverviewReact,
  title = {Feature {{Overview}} {\textendash} {{React Flow}}},
  abstract = {A basic example showing the most used features of React Flow - controls, MiniMap, node types, edge types, edge labels, and custom styling.},
  howpublished = {https://reactflow.dev/examples}
}

@misc{fengDreaMovingHumanVideo2023,
  title = {{{DreaMoving}}: {{A Human Video Generation Framework}} Based on {{Diffusion Models}}},
  shorttitle = {{{DreaMoving}}},
  author = {Feng, Mengyang and Liu, Jinlin and Yu, Kai and Yao, Yuan and Hui, Zheng and Guo, Xiefan and Lin, Xianhui and Xue, Haolan and Shi, Chen and Li, Xiaowen and Li, Aojie and Kang, Xiaoyang and Lei, Biwen and Cui, Miaomiao and Ren, Peiran and Xie, Xuansong},
  year = {2023},
  month = dec,
  number = {arXiv:2312.05107},
  eprint = {2312.05107},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.05107},
  abstract = {In this paper, we present DreaMoving, a diffusion-based controllable video generation framework to produce high-quality customized human videos. Specifically, given target identity and posture sequences, DreaMoving can generate a video of the target identity moving or dancing anywhere driven by the posture sequences. To this end, we propose a Video ControlNet for motion-controlling and a Content Guider for identity preserving. The proposed model is easy to use and can be adapted to most stylized diffusion models to generate diverse results. The project page is available at https://dreamoving.github.io/dreamoving},
  archiveprefix = {arxiv}
}

@misc{fengRevealingMysteryChain2023,
  title = {Towards {{Revealing}} the {{Mystery}} behind {{Chain}} of {{Thought}}: {{A Theoretical Perspective}}},
  shorttitle = {Towards {{Revealing}} the {{Mystery}} behind {{Chain}} of {{Thought}}},
  author = {Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  year = {2023},
  month = jun,
  number = {arXiv:2305.15408},
  eprint = {2305.15408},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{FenXitransformerMoXingDeCanShuLiangJiSuanLiangZhongJianJiHuo,
  title = {{transformerKV cache}},
  journal = {},
  abstract = {1. OpenAIChatGPT(Large Language Model, LLM)``''GPT3GPT3{\dots}},
  howpublished = {https://zhuanlan.zhihu.com/p/624740065},
  langid = {chinese}
}

@misc{gaoASSISTGUITaskOrientedDesktop2023,
  title = {{{ASSISTGUI}}: {{Task-Oriented Desktop Graphical User Interface Automation}}},
  shorttitle = {{{ASSISTGUI}}},
  author = {Gao, Difei and Ji, Lei and Bai, Zechen and Ouyang, Mingyu and Li, Peiran and Mao, Dongxing and Wu, Qinchen and Zhang, Weichen and Wang, Peiyi and Guo, Xiangwu and Wang, Hengxu and Zhou, Luowei and Shou, Mike Zheng},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13108},
  eprint = {2312.13108},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Graphical User Interface (GUI) automation holds significant promise for assisting users with complex tasks, thereby boosting human productivity. Existing works leveraging Large Language Model (LLM) or LLM-based AI agents have shown capabilities in automating tasks on Android and Web platforms. However, these tasks are primarily aimed at simple device usage and entertainment operations. This paper presents a novel benchmark, AssistGUI, to evaluate whether models are capable of manipulating the mouse and keyboard on the Windows platform in response to user-requested tasks. We carefully collected a set of 100 tasks from nine widely-used software applications, such as, After Effects and MS Word, each accompanied by the necessary project files for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied Agent framework, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks. Our experimental results reveal that our GUI Parser and Reasoning mechanism outshine existing methods in performance. Nevertheless, the potential remains substantial, with the best model attaining only a 46\% success rate on our benchmark. We conclude with a thorough analysis of the current methods' limitations, setting the stage for future breakthroughs in this domain.},
  archiveprefix = {arxiv}
}

@misc{gaoConfuciusIterativeTool2023,
  title = {Confucius: {{Iterative Tool Learning}} from {{Introspection Feedback}} by {{Easy-to-Difficult Curriculum}}},
  shorttitle = {Confucius},
  author = {Gao, Shen and Shi, Zhengliang and Zhu, Minghang and Fang, Bowen and Xin, Xin and Ren, Pengjie and Chen, Zhumin and Ma, Jun and Ren, Zhaochun},
  year = {2023},
  month = dec,
  number = {arXiv:2308.14034},
  eprint = {2308.14034},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.14034},
  abstract = {Augmenting large language models (LLMs) with external tools has emerged as a promising approach to extending the capability of LLMs. Although some works employ open-source LLMs for the tool learning task, most of them are trained in a controlled environment in which LLMs only learn to execute the human-provided tools. However, selecting proper tools from the large toolset is also a crucial ability for the tool learning model to be applied in real-world applications. Existing methods usually directly employ self-instruction methods to train the model, which ignores differences in tool complexity. In this paper, we propose the Confucius, a novel tool learning framework to train LLM to use complicated tools in real-world scenarios, which contains two main phases: (1) We first propose a multi-stage learning method to teach the LLM to use various tools from an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative Self-instruct from Introspective Feedback (ISIF) to dynamically construct the dataset to improve the ability to use the complicated tool. Extensive experiments conducted on both controlled and real-world settings demonstrate the superiority of our tool learning framework in the real-world application scenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based baselines (e.g. GPT4Tools).},
  archiveprefix = {arxiv}
}

@misc{gaoCustomizingLanguageModel2024,
  title = {Customizing {{Language Model Responses}} with {{Contrastive In-Context Learning}}},
  author = {Gao, Xiang and Das, Kamalika},
  year = {2024},
  month = jan,
  number = {arXiv:2401.17390},
  eprint = {2401.17390},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real-world datasets, including StackExchange and Reddit, and found that it significantly improves performance compared to standard few-shot prompting},
  archiveprefix = {arxiv}
}

@misc{gaoEfficientToolUse2024,
  title = {Efficient {{Tool Use}} with {{Chain-of-Abstraction Reasoning}}},
  author = {Gao, Silin and {Dwivedi-Yu}, Jane and Yu, Ping and Tan, Xiaoqing Ellen and Pasunuru, Ramakanth and Golovneva, Olga and Sinha, Koustuv and Celikyilmaz, Asli and Bosselut, Antoine and Wang, Tianlu},
  year = {2024},
  month = jan,
  number = {arXiv:2401.17464},
  eprint = {2401.17464},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning. In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding and calling of external tools in parallel, which avoids the inference delay caused by waiting for tool responses. In mathematical reasoning and Wiki QA domains, we show that our method consistently outperforms previous chain-of-thought and tool-augmented baselines on both in-distribution and out-of-distribution test sets, with an average {\textasciitilde}6\% absolute QA accuracy improvement. LLM agents trained with our method also show more efficient tool use, with inference speed being on average {\textasciitilde}1.4x faster than baseline tool-augmented LLMs.},
  archiveprefix = {arxiv}
}

@misc{gaoGLLaVASolvingGeometric2023,
  title = {G-{{LLaVA}}: {{Solving Geometric Problem}} with {{Multi-Modal Large Language Model}}},
  shorttitle = {G-{{LLaVA}}},
  author = {Gao, Jiahui and Pi, Renjie and Zhang, Jipeng and Ye, Jiacheng and Zhong, Wanjun and Wang, Yufei and Hong, Lanqing and Han, Jianhua and Xu, Hang and Li, Zhenguo and Kong, Lingpeng},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11370},
  eprint = {2312.11370},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.11370},
  abstract = {Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.},
  archiveprefix = {arxiv}
}

@misc{gaoLLaMAAdapterV2ParameterEfficient2023,
  title = {{{LLaMA-Adapter V2}}: {{Parameter-Efficient Visual Instruction Model}}},
  shorttitle = {{{LLaMA-Adapter V2}}},
  author = {Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and Li, Hongsheng and Qiao, Yu},
  year = {2023},
  month = apr,
  number = {arXiv:2304.15010},
  eprint = {2304.15010},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMAAdapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMAAdapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instructionfollowing ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This strategy effectively alleviates the interference between the two tasks of image-text alignment and instruction following and achieves strong multi-modal reasoning with only a smallscale image-text and instruction dataset. During inference, we incorporate additional expert models (e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image understanding capability without incurring training costs. Compared to the original LLaMAAdapter, our LLaMA-Adapter V2 can perform open-ended multi-modal instructions by merely introducing 14M parameters over LLaMA. The newly designed framework also exhibits stronger language-only instruction-following capabilities and even excels in chat interactions. Our code and models are available at https://github.com/ ZrrSkywalker/LLaMA-Adapter.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{gaoRetrievalAugmentedGenerationLarge2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Guo, Qianyu and Wang, Meng and Wang, Haofen},
  year = {2024},
  month = jan,
  number = {arXiv:2312.10997},
  eprint = {2312.10997},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces the metrics and benchmarks for assessing RAG models, along with the most up-to-date evaluation framework. In conclusion, the paper delineates prospective avenues for research, including the identification of challenges, the expansion of multi-modalities, and the progression of the RAG infrastructure and its ecosystem.},
  archiveprefix = {arxiv}
}

@misc{geExtensiblePromptsLanguage2022,
  title = {Extensible {{Prompts}} for {{Language Models}}},
  author = {Ge, Tao and Hu, Jing and Dong, Li and Mao, Shaoguang and Xia, Yan and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2212.00616},
  eprint = {2212.00616},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.00616},
  abstract = {We propose eXtensible Prompt (X-Prompt) for prompting a large language model (LLM) beyond natural language (NL). X-Prompt instructs an LLM with not only NL but also an extensible vocabulary of imaginary words that are introduced to help represent what NL words hardly describe, allowing a prompt to be more descriptive. Like NL prompts, X-Prompt is out-of-distribution (OOD) robust, for which we propose context-guided learning with prompt augmentation to learn its imaginary words for general usability, enabling them to use in different prompt contexts for fine-grain specifications. The promising results of X-Prompt demonstrate its potential of approaching advanced interaction between humans and LLMs to bridge their communication gap.},
  archiveprefix = {arxiv}
}

@misc{geIncontextAutoencoderContext2023,
  title = {In-Context {{Autoencoder}} for {{Context Compression}} in a {{Large Language Model}}},
  author = {Ge, Tao and Hu, Jing and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.06945},
  eprint = {2307.06945},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.06945},
  abstract = {We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with \$4{\textbackslash}times\$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promising results demonstrate significant implications of the ICAE for its novel approach to the long context problem and its potential to reduce computation and memory overheads for LLM inference in practice, suggesting further research effort in context management for an LLM. Our code and data will be released shortly.},
  archiveprefix = {arxiv}
}

@misc{geshkovskiMathematicalPerspectiveTransformers2023,
  title = {A Mathematical Perspective on {{Transformers}}},
  author = {Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10794},
  eprint = {2312.10794},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.10794},
  abstract = {Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.},
  archiveprefix = {arxiv}
}

@misc{ghosalFlacunaUnleashingProblem2023,
  title = {Flacuna: {{Unleashing}} the {{Problem Solving Power}} of {{Vicuna}} Using {{FLAN Fine-Tuning}}},
  shorttitle = {Flacuna},
  author = {Ghosal, Deepanway and Chia, Yew Ken and Majumder, Navonil and Poria, Soujanya},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02053},
  eprint = {2307.02053},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recently, the release of INSTRUCTEVAL has provided valuable insights into the performance of large language models (LLMs) that utilize encoder-decoder or decoder-only architecture. Interestingly, despite being introduced four years ago, T5-based LLMs, such as FLAN-T5, continue to outperform the latest decoder-based LLMs, such as LLAMA and VICUNA, on tasks that require general problem-solving skills. This performance discrepancy can be attributed to three key factors: (1) Pre-training data, (2) Backbone architecture, and (3) Instruction dataset. In this technical report, our main focus is on investigating the impact of the third factor by leveraging VICUNA, a large language model based on LLAMA, which has undergone fine-tuning on ChatGPT conversations. To achieve this objective, we fine-tuned VICUNA using a customized instruction dataset collection called FLANMINI. This collection includes a subset of the large-scale instruction dataset known as FLAN, as well as various code-related datasets and conversational datasets derived from ChatGPT/GPT-4. This dataset comprises a large number of tasks that demand problem-solving skills. Our experimental findings strongly indicate that the enhanced problem-solving abilities of our model, FLACUNA, are obtained through fine-tuning VICUNA on the FLAN dataset, leading to significant improvements across numerous benchmark datasets in INSTRUCTEVAL. FLACUNA is publicly available at https://huggingface.co/declare-lab/flacuna-13b-v1.0.},
  archiveprefix = {arxiv}
}

@misc{girdharImageBindOneEmbedding2023,
  title = {{{ImageBind}}: {{One Embedding Space To Bind Them All}}},
  shorttitle = {{{ImageBind}}},
  author = {Girdhar, Rohit and {El-Nouby}, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  year = {2023},
  month = may,
  number = {arXiv:2305.05665},
  eprint = {2305.05665},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.05665},
  abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
  archiveprefix = {arxiv}
}

@misc{gouCRITICLargeLanguage2023,
  title = {{{CRITIC}}: {{Large Language Models Can Self-Correct}} with {{Tool-Interactive Critiquing}}},
  shorttitle = {{{CRITIC}}},
  author = {Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Yang, Yujiu and Duan, Nan and Chen, Weizhu},
  year = {2023},
  month = sep,
  number = {arXiv:2305.11738},
  eprint = {2305.11738},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.11738},
  abstract = {Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.},
  archiveprefix = {arxiv}
}

@misc{gouToRAToolIntegratedReasoning2023,
  title = {{{ToRA}}: {{A Tool-Integrated Reasoning Agent}} for {{Mathematical Problem Solving}}},
  shorttitle = {{{ToRA}}},
  author = {Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and {shen}, yelong and Yang, Yujiu and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17452},
  eprint = {2309.17452},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13\%-19\% absolute improvements on average. Notably, ToRA-7B reaches 44.6\% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22\% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50\% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.},
  archiveprefix = {arxiv}
}

@misc{guMambaLinearTimeSequence2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  year = {2023},
  month = dec,
  number = {arXiv:2312.00752},
  eprint = {2312.00752},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.00752},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  archiveprefix = {arxiv}
}

@misc{guMixofShowDecentralizedLowRank2023,
  title = {Mix-of-{{Show}}: {{Decentralized Low-Rank Adaptation}} for {{Multi-Concept Customization}} of {{Diffusion Models}}},
  shorttitle = {Mix-of-{{Show}}},
  author = {Gu, Yuchao and Wang, Xintao and Wu, Jay Zhangjie and Shi, Yujun and Chen, Yunpeng and Fan, Zihan and Xiao, Wuyou and Zhao, Rui and Chang, Shuning and Wu, Weijia and Ge, Yixiao and Shan, Ying and Shou, Mike Zheng},
  year = {2023},
  month = nov,
  number = {arXiv:2305.18292},
  eprint = {2305.18292},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18292},
  abstract = {Public large-scale text-to-image diffusion models, such as Stable Diffusion, have gained significant attention from the community. These models can be easily customized for new concepts using low-rank adaptations (LoRAs). However, the utilization of multiple concept LoRAs to jointly support multiple customized concepts presents a challenge. We refer to this scenario as decentralized multi-concept customization, which involves single-client concept tuning and center-node concept fusion. In this paper, we propose a new framework called Mix-of-Show that addresses the challenges of decentralized multi-concept customization, including concept conflicts resulting from existing single-client LoRA tuning and identity loss during model fusion. Mix-of-Show adopts an embedding-decomposed LoRA (ED-LoRA) for single-client tuning and gradient fusion for the center node to preserve the in-domain essence of single concepts and support theoretically limitless concept fusion. Additionally, we introduce regionally controllable sampling, which extends spatially controllable sampling (e.g., ControlNet and T2I-Adaptor) to address attribute binding and missing object problems in multi-concept sampling. Extensive experiments demonstrate that Mix-of-Show is capable of composing multiple customized concepts with high fidelity, including characters, objects, and scenes.},
  archiveprefix = {arxiv}
}

@misc{gunasekarTextbooksAreAll2023,
  title = {Textbooks {{Are All You Need}}},
  author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and {de Rosa}, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, S{\'e}bastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
  year = {2023},
  month = oct,
  number = {arXiv:2306.11644},
  eprint = {2306.11644},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
  archiveprefix = {arxiv}
}

@misc{guoAnimateDiffAnimateYour2023,
  title = {{{AnimateDiff}}: {{Animate Your Personalized Text-to-Image Diffusion Models}} without {{Specific Tuning}}},
  shorttitle = {{{AnimateDiff}}},
  author = {Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Wang, Yaohui and Qiao, Yu and Lin, Dahua and Dai, Bo},
  year = {2023},
  month = jul,
  number = {arXiv:2307.04725},
  eprint = {2307.04725},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.04725},
  abstract = {With the advance of text-to-image models (e.g., Stable Diffusion) and corresponding personalization techniques such as DreamBooth and LoRA, everyone can manifest their imagination into high-quality images at an affordable cost. Subsequently, there is a great demand for image animation techniques to further combine generated static images with motion dynamics. In this report, we propose a practical framework to animate most of the existing personalized text-to-image models once and for all, saving efforts in model-specific tuning. At the core of the proposed framework is to insert a newly initialized motion modeling module into the frozen text-to-image model and train it on video clips to distill reasonable motion priors. Once trained, by simply injecting this motion modeling module, all personalized versions derived from the same base T2I readily become text-driven models that produce diverse and personalized animated images. We conduct our evaluation on several public representative personalized text-to-image models across anime pictures and realistic photographs, and demonstrate that our proposed framework helps these models generate temporally smooth animation clips while preserving the domain and diversity of their outputs. Code and pre-trained weights will be publicly available at https://animatediff.github.io/ .},
  archiveprefix = {arxiv}
}

@misc{guptaPhotorealisticVideoGeneration2023,
  title = {Photorealistic {{Video Generation}} with {{Diffusion Models}}},
  author = {Gupta, Agrim and Yu, Lijun and Sohn, Kihyuk and Gu, Xiuye and Hahn, Meera and {Fei-Fei}, Li and Essa, Irfan and Jiang, Lu and Lezama, Jos{\'e}},
  year = {2023},
  month = dec,
  number = {arXiv:2312.06662},
  eprint = {2312.06662},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.06662},
  abstract = {We present W.A.L.T, a transformer-based approach for photorealistic video generation via diffusion modeling. Our approach has two key design decisions. First, we use a causal encoder to jointly compress images and videos within a unified latent space, enabling training and generation across modalities. Second, for memory and training efficiency, we use a window attention architecture tailored for joint spatial and spatiotemporal generative modeling. Taken together these design decisions enable us to achieve state-of-the-art performance on established video (UCF-101 and Kinetics-600) and image (ImageNet) generation benchmarks without using classifier free guidance. Finally, we also train a cascade of three models for the task of text-to-video generation consisting of a base latent video diffusion model, and two video super-resolution diffusion models to generate videos of \$512 {\textbackslash}times 896\$ resolution at \$8\$ frames per second.},
  archiveprefix = {arxiv}
}

@misc{guptaRAGVsFinetuning2024,
  title = {{{RAG}} vs {{Fine-tuning}}: {{Pipelines}}, {{Tradeoffs}}, and a {{Case Study}} on {{Agriculture}}},
  shorttitle = {{{RAG}} vs {{Fine-tuning}}},
  author = {Gupta, Aman and Shirgaonkar, Anup and Balaguer, Angels de Luis and Silva, Bruno and Holstein, Daniel and Li, Dawei and Marsman, Jennifer and Nunes, Leonardo O. and Rouzbahman, Mahsa and Sharp, Morris and Mecklenburg, Nick and Padilha, Rafael and Chandra, Ranveer and Cunha, Renato Luiz de Freitas and Filho, Roberto de M. Estev{\~a}o and Tsang, Ryan and Malvar, Sara and Sharma, Swati and Hendry, Todd and Aski, Vijay and Vijayendran, Vijetha and Benara, Vinamra},
  year = {2024},
  month = jan,
  number = {arXiv:2401.08406},
  eprint = {2401.08406},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.08406},
  abstract = {There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47\% to 72\%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.},
  archiveprefix = {arxiv}
}

@misc{HamelBlogOptimizing2023,
  title = {Hamel's {{Blog}} - {{Optimizing}} Latency},
  year = {2023},
  month = nov,
  journal = {Hamel's Blog},
  abstract = {An exploration of ways to optimize on latency.},
  howpublished = {https://hamel.dev/notes/llm/inference/03\_inference.html},
  langid = {english}
}

@misc{hanChartLlamaMultimodalLLM2023,
  title = {{{ChartLlama}}: {{A Multimodal LLM}} for {{Chart Understanding}} and {{Generation}}},
  shorttitle = {{{ChartLlama}}},
  author = {Han, Yucheng and Zhang, Chi and Chen, Xin and Yang, Xu and Wang, Zhibin and Yu, Gang and Fu, Bin and Zhang, Hanwang},
  year = {2023},
  month = nov,
  number = {arXiv:2311.16483},
  eprint = {2311.16483},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.16483},
  abstract = {Multi-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps are responsible for generating tabular data, creating chart figures, and designing instruction tuning data separately. Our method's flexibility enables us to generate diverse, high-quality instruction-tuning data consistently and efficiently while maintaining a low resource expenditure. Additionally, it allows us to incorporate a wider variety of chart and task types not yet featured in existing datasets. Next, we introduce ChartLlama, a multi-modal large language model that we've trained using our created dataset. ChartLlama outperforms all prior methods in ChartQA, Chart-to-text, and Chart-extraction evaluation benchmarks. Additionally, ChartLlama significantly improves upon the baseline in our specially compiled chart dataset, which includes new chart and task types. The results of ChartLlama confirm the value and huge potential of our proposed data generation method in enhancing chart comprehension.},
  archiveprefix = {arxiv}
}

@misc{HanShuDeCanShu,
  title = {{}},
  abstract = {PythonPython 3},
  howpublished = {https://www.liaoxuefeng.com/wiki/1016959663602400/1017261630425888},
  langid = {zh\_CN}
}

@misc{hanUncertaintyAwareLanguageAgent2024,
  title = {Towards {{Uncertainty-Aware Language Agent}}},
  author = {Han, Jiuzhou and Buntine, Wray and Shareghi, Ehsan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.14016},
  eprint = {2401.14016},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty.},
  archiveprefix = {arxiv}
}

@misc{haoChatLLMNetworkMore2023,
  title = {{{ChatLLM Network}}: {{More}} Brains, {{More}} Intelligence},
  shorttitle = {{{ChatLLM Network}}},
  author = {Hao, Rui and Hu, Linmei and Qi, Weijian and Wu, Qingliu and Zhang, Yirui and Nie, Liqiang},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12998},
  eprint = {2304.12998},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12998},
  abstract = {Dialogue-based language models mark a huge milestone in the field of artificial intelligence, by their impressive ability to interact with users, as well as a series of challenging tasks prompted by customized instructions. However, the prevalent large-scale dialogue-based language models like ChatGPT still have room for improvement, such as unstable responses to questions and the inability to think cooperatively like humans. Considering the ability of dialogue-based language models in conversation and their inherent randomness in thinking, we propose ChatLLM network that allows multiple dialogue-based language models to interact, provide feedback, and think together. We design the network of ChatLLMs based on ChatGPT. Specifically, individual instances of ChatGPT may possess distinct perspectives towards the same problem, and by consolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM network system can conduct decision-making more objectively and comprehensively. In addition, a language-based feedback mechanism comparable to backpropagation is devised to update the ChatGPTs within the network. Experiments on two datasets demonstrate that our network attains significant improvements in problem-solving, leading to observable progress amongst each member.},
  archiveprefix = {arxiv}
}

@misc{haoReasoningLanguageModel2023,
  title = {Reasoning with {{Language Model}} Is {{Planning}} with {{World Model}}},
  author = {Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  year = {2023},
  month = oct,
  number = {arXiv:2305.14992},
  eprint = {2305.14992},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14992},
  abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal \${\textbackslash}textit\{world model\}\$ to predict the world \${\textbackslash}textit\{state\}\$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, \${\textbackslash}underline\{R\}\$easoning vi\${\textbackslash}underline\{a\}\$ \${\textbackslash}underline\{P\}\$lanning \${\textbackslash}textbf\{(RAP)\}\$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration \${\textbackslash}textit\{vs.\}\$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33\% relative improvement in a plan generation setting.},
  archiveprefix = {arxiv}
}

@misc{haoStructuredPromptingScaling2022,
  title = {Structured {{Prompting}}: {{Scaling In-Context Learning}} to 1,000 {{Examples}}},
  shorttitle = {Structured {{Prompting}}},
  author = {Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06713},
  eprint = {2212.06713},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.06713},
  abstract = {Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Specifically, demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism. So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at https://aka.ms/structured-prompting.},
  archiveprefix = {arxiv}
}

@misc{haoToolkenGPTAugmentingFrozen2023,
  title = {{{ToolkenGPT}}: {{Augmenting Frozen Language Models}} with {{Massive Tools}} via {{Tool Embeddings}}},
  shorttitle = {{{ToolkenGPT}}},
  author = {Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting},
  year = {2023},
  month = oct,
  number = {arXiv:2305.11554},
  eprint = {2305.11554},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.11554},
  abstract = {Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, \${\textbackslash}textbf\{ToolkenGPT\}\$, which combines the benefits of both sides. Our approach represents each \${\textbackslash}underline\{tool\}\$ as a to\${\textbackslash}underline\{ken\}\$ (\${\textbackslash}textit\{toolken\}\$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.},
  archiveprefix = {arxiv}
}

@misc{heLatentVideoDiffusion2023,
  title = {Latent {{Video Diffusion Models}} for {{High-Fidelity Long Video Generation}}},
  author = {He, Yingqing and Yang, Tianyu and Zhang, Yong and Shan, Ying and Chen, Qifeng},
  year = {2023},
  month = mar,
  number = {arXiv:2211.13221},
  eprint = {2211.13221},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.13221},
  abstract = {AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models have shown remarkable results recently but require significant computational resources. To address this, we introduce lightweight video diffusion models by leveraging a low-dimensional 3D latent space, significantly outperforming previous pixel-space video diffusion models under a limited computational budget. In addition, we propose hierarchical diffusion in the latent space such that longer videos with more than one thousand frames can be produced. To further overcome the performance degradation issue for long video generation, we propose conditional latent perturbation and unconditional guidance that effectively mitigate the accumulated errors during the extension of video length. Extensive experiments on small domain datasets of different categories suggest that our framework generates more realistic and longer videos than previous strong baselines. We additionally provide an extension to large-scale text-to-video generation to demonstrate the superiority of our work. Our code and models will be made publicly available.},
  archiveprefix = {arxiv}
}

@misc{hendelInContextLearningCreates2023,
  title = {In-{{Context Learning Creates Task Vectors}}},
  author = {Hendel, Roee and Geva, Mor and Globerson, Amir},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15916},
  eprint = {2310.15916},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.15916},
  abstract = {In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the "standard" machine learning framework, where one uses a training set \$S\$ to find a best-fitting function \$f(x)\$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query \$x\$ and a single "task vector" calculated from the training set. Thus, ICL can be seen as compressing \$S\$ into a single task vector \${\textbackslash}boldsymbol\{{\textbackslash}theta\}(S)\$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.},
  archiveprefix = {arxiv}
}

@misc{hendrycksMeasuringMassiveMultitask2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2021},
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.03300},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arxiv}
}

@misc{hertzPrompttoPromptImageEditing2022,
  title = {Prompt-to-{{Prompt Image Editing}} with {{Cross Attention Control}}},
  author = {Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and {Cohen-Or}, Daniel},
  year = {2022},
  month = aug,
  abstract = {Recent large-scale text-driven synthesis models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Such text-based synthesis methods are particularly appealing to humans who are used to verbally describe their intent. Therefore, it is only natural to extend the text-driven image synthesis to text-driven image editing. Editing is challenging for these generative models, since an innate property of an editing technique is to preserve most of the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. To this end, we analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we present several applications which monitor the image synthesis by editing the textual prompt only. This includes localized editing by replacing a word, global editing by adding a specification, and even delicately controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts, demonstrating high-quality synthesis and fidelity to the edited prompts.},
  langid = {english}
}

@misc{heWebVoyagerBuildingEndtoEnd2024,
  title = {{{WebVoyager}}: {{Building}} an {{End-to-End Web Agent}} with {{Large Multimodal Models}}},
  shorttitle = {{{WebVoyager}}},
  author = {He, Hongliang and Yao, Wenlin and Ma, Kaixin and Yu, Wenhao and Dai, Yong and Zhang, Hongming and Lan, Zhenzhong and Yu, Dong},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13919},
  eprint = {2401.13919},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7\% task success rate, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager in practical applications. We found that our proposed automatic evaluation achieves 85.3\% agreement with human judgment, paving the way for further development of web agents in a real-world setting.},
  archiveprefix = {arxiv},
  langid = {english}
}

@article{hoIMAGENVIDEOHIGH,
  title = {{{IMAGEN VIDEO}}: {{HIGH DEFINITION VIDEO GENERATION WITH DIFFUSION MODELS}}},
  author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and Salimans, Tim},
  abstract = {We present Imagen Video, a text-conditional video generation system based on a cascade of video diffusion models. Given a text prompt, Imagen Video generates high definition videos using a base video generation model and a sequence of interleaved spatial and temporal video super-resolution models. We describe how we scale up the system as a high definition text-to-video model including design decisions such as the choice of fully-convolutional temporal and spatial superresolution models at certain resolutions, and the choice of the v-parameterization of diffusion models. In addition, we confirm and transfer findings from previous work on diffusion-based image generation to the video generation setting. Finally, we apply progressive distillation to our video models with classifier-free guidance for fast, high quality sampling. We find Imagen Video not only capable of generating videos of high fidelity, but also having a high degree of controllability and world knowledge, including the ability to generate diverse videos and text animations in various artistic styles and with 3D object understanding. See imagen.research.google/video for samples.},
  langid = {english}
}

@misc{hongCloserLookSelfVerification2023,
  title = {A {{Closer Look}} at the {{Self-Verification Abilities}} of {{Large Language Models}} in {{Logical Reasoning}}},
  author = {Hong, Ruixin and Zhang, Hongming and Pang, Xinyu and Yu, Dong and Zhang, Changshui},
  year = {2023},
  month = nov,
  number = {arXiv:2311.07954},
  eprint = {2311.07954},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.07954},
  abstract = {Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.},
  archiveprefix = {arxiv}
}

@misc{hongMetaGPTMetaProgramming2023,
  title = {{{MetaGPT}}: {{Meta Programming}} for {{Multi-Agent Collaborative Framework}}},
  shorttitle = {{{MetaGPT}}},
  author = {Hong, Sirui and Zheng, Xiawu and Chen, Jonathan and Cheng, Yuheng and Wang, Jinlin and Zhang, Ceyao and Wang, Zili and Yau, Steven Ka Shing and Lin, Zijuan and Zhou, Liyang and Ran, Chenyu and Xiao, Lingfeng and Wu, Chenglin},
  year = {2023},
  month = aug,
  number = {arXiv:2308.00352},
  eprint = {2308.00352},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.00352},
  abstract = {Recently, remarkable progress has been made in automated task-solving through the use of multi-agent driven by large language models (LLMs). However, existing LLM-based multi-agent works primarily focus on solving simple dialogue tasks, and complex tasks are rarely studied, mainly due to the LLM hallucination problem. This type of hallucination becomes cascading when naively chaining multiple intelligent agents, resulting in a failure to effectively address complex problems. Therefore, we introduce MetaGPT, an innovative framework that incorporates efficient human workflows as a meta programming approach into LLM-based multi-agent collaboration. Specifically, MetaGPT encodes Standardized Operating Procedures (SOPs) into prompts to enhance structured coordination. Subsequently, it mandates modular outputs, empowering agents with domain expertise comparable to human professionals, to validate outputs and minimize compounded errors. In this way, MetaGPT leverages the assembly line paradigm to assign diverse roles to various agents, thereby establishing a framework that can effectively and cohesively deconstruct complex multi-agent collaborative problems. Our experiments on collaborative software engineering benchmarks demonstrate that MetaGPT generates more coherent and correct solutions compared to existing chat-based multi-agent systems. This highlights the potential of integrating human domain knowledge into multi-agent systems, thereby creating new opportunities to tackle complex real-world challenges. The GitHub repository of this project is publicly available on:https://github.com/geekan/MetaGPT.},
  archiveprefix = {arxiv}
}

@misc{HowMakeLLMs2023,
  title = {How to Make {{LLMs}} Go Fast},
  year = {2023},
  month = dec,
  abstract = {Blog about linguistics, programming, and my projects},
  howpublished = {https://vgel.me/posts/faster-inference/},
  langid = {english}
}

@misc{hsiehToolDocumentationEnables2023,
  title = {Tool {{Documentation Enables Zero-Shot Tool-Usage}} with {{Large Language Models}}},
  author = {Hsieh, Cheng-Yu and Chen, Si-An and Li, Chun-Liang and Fujii, Yasuhisa and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and Pfister, Tomas},
  year = {2023},
  month = aug,
  number = {arXiv:2308.00675},
  eprint = {2308.00675},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.00675},
  abstract = {Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Second, on a newly collected realistic tool-use dataset with hundreds of available tool APIs, we show that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation. Third, we highlight the benefits of tool documentations by tackling image generation and video tracking using just-released unseen state-of-the-art models as tools. Finally, we highlight the possibility of using tool documentation to automatically enable new applications: by using nothing more than the documentation of GroundingDino, Stable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the just-released Grounded-SAM and Track Anything models.},
  archiveprefix = {arxiv}
}

@misc{huangCEvalMultiLevelMultiDiscipline2023,
  title = {C-{{Eval}}: {{A Multi-Level Multi-Discipline Chinese Evaluation Suite}} for {{Foundation Models}}},
  shorttitle = {C-{{Eval}}},
  author = {Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
  year = {2023},
  month = may,
  number = {arXiv:2305.08322},
  eprint = {2305.08322},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.08322},
  abstract = {New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60\%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.},
  archiveprefix = {arxiv}
}

@misc{huangInnerMonologueEmbodied2022,
  title = {Inner {{Monologue}}: {{Embodied Reasoning}} through {{Planning}} with {{Language Models}}},
  shorttitle = {Inner {{Monologue}}},
  author = {Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and Sermanet, Pierre and Brown, Noah and Jackson, Tomas and Luu, Linda and Levine, Sergey and Hausman, Karol and Ichter, Brian},
  year = {2022},
  month = jul,
  number = {arXiv:2207.05608},
  eprint = {2207.05608},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.05608},
  abstract = {Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
  archiveprefix = {arxiv}
}

@misc{huangLanguageModelsZeroShot2022,
  title = {Language {{Models}} as {{Zero-Shot Planners}}: {{Extracting Actionable Knowledge}} for {{Embodied Agents}}},
  shorttitle = {Language {{Models}} as {{Zero-Shot Planners}}},
  author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  year = {2022},
  month = mar,
  number = {arXiv:2201.07207},
  eprint = {2201.07207},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.07207},
  abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
  archiveprefix = {arxiv}
}

@misc{huangLargeLanguageModels2023,
  title = {Large {{Language Models Cannot Self-Correct Reasoning Yet}}},
  author = {Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01798},
  eprint = {2310.01798},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.01798},
  abstract = {Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance might even degrade post self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.},
  archiveprefix = {arxiv}
}

@misc{huangMetaToolBenchmarkLarge2023,
  title = {{{MetaTool Benchmark}} for {{Large Language Models}}: {{Deciding Whether}} to {{Use Tools}} and {{Which}} to {{Use}}},
  shorttitle = {{{MetaTool Benchmark}} for {{Large Language Models}}},
  author = {Huang, Yue and Shi, Jiawen and Li, Yuan and Fan, Chenrui and Wu, Siyuan and Zhang, Qihui and Liu, Yixin and Zhou, Pan and Wan, Yao and Gong, Neil Zhenqiang and Sun, Lichao},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03128},
  eprint = {2310.03128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.03128},
  abstract = {Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. Subsequently, we set the tasks for both tool usage awareness and tool selection. We define four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. We conduct experiments involving nine popular LLMs and find that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents. However, through the error analysis, we found there is still significant room for improvement. Finally, we conclude with insights for tool developers that follow ChatGPT to provide detailed descriptions that can enhance the tool selection performance of LLMs.},
  archiveprefix = {arxiv}
}

@misc{hubingerSleeperAgentsTraining2024,
  title = {Sleeper {{Agents}}: {{Training Deceptive LLMs}} That {{Persist Through Safety Training}}},
  shorttitle = {Sleeper {{Agents}}},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  howpublished = {https://arxiv.org/abs/2401.05566v2},
  langid = {english}
}

@misc{HuggingfaceDiffusersDiffusers,
  title = {Huggingface/Diffusers:  {{Diffusers}}: {{State-of-the-art}} Diffusion Models for Image and Audio Generation in {{PyTorch}}},
  shorttitle = {Huggingface/Diffusers},
  journal = {GitHub},
  abstract = { Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch - huggingface/diffusers:  Diffusers: State-of-the-art diffusion models for image and audio generation in Py...},
  howpublished = {https://github.com/huggingface/diffusers},
  langid = {english}
}

@misc{HuggingfacePeftPEFT,
  title = {Huggingface/Peft:  {{PEFT}}: {{State-of-the-art Parameter-Efficient Fine-Tuning}}.},
  howpublished = {https://github.com/huggingface/peft}
}

@misc{HuggingfaceTransformersTransformers,
  title = {Huggingface/Transformers:  {{Transformers}}: {{State-of-the-art Machine Learning}} for {{Pytorch}}, {{TensorFlow}}, and {{JAX}}.},
  shorttitle = {Huggingface/Transformers},
  journal = {GitHub},
  abstract = { Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX. - huggingface/transformers:  Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.},
  howpublished = {https://github.com/huggingface/transformers},
  langid = {english}
}

@misc{HuggingfaceTrlTrain,
  title = {Huggingface/Trl: {{Train}} Transformer Language Models with Reinforcement Learning.},
  howpublished = {https://github.com/huggingface/trl?tab=readme-ov-file}
}

@misc{huInstructImagenImageGeneration2024,
  title = {Instruct-{{Imagen}}: {{Image Generation}} with {{Multi-modal Instruction}}},
  shorttitle = {Instruct-{{Imagen}}},
  author = {Hu, Hexiang and Chan, Kelvin C. K. and Su, Yu-Chuan and Chen, Wenhu and Li, Yandong and Sohn, Kihyuk and Zhao, Yang and Ben, Xue and Gong, Boqing and Cohen, William and Chang, Ming-Wei and Jia, Xuhui},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01952},
  eprint = {2401.01952},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This paper presents instruct-imagen, a model that tackles heterogeneous image generation tasks and generalizes across unseen tasks. We introduce *multi-modal instruction* for image generation, a task representation articulating a range of generation intents with precision. It uses natural language to amalgamate disparate modalities (e.g., text, edge, style, subject, etc.), such that abundant generation intents can be standardized in a uniform format. We then build instruct-imagen by fine-tuning a pre-trained text-to-image diffusion model with a two-stage framework. First, we adapt the model using the retrieval-augmented training, to enhance model's capabilities to ground its generation on external multimodal context. Subsequently, we fine-tune the adapted model on diverse image generation tasks that requires vision-language understanding (e.g., subject-driven generation, etc.), each paired with a multi-modal instruction encapsulating the task's essence. Human evaluation on various image generation datasets reveals that instruct-imagen matches or surpasses prior task-specific models in-domain and demonstrates promising generalization to unseen and more complex tasks.},
  archiveprefix = {arxiv}
}

@misc{huLargeLanguageModels2023,
  title = {Do {{Large Language Models Know}} about {{Facts}}?},
  author = {Hu, Xuming and Chen, Junzhe and Li, Xiaochuan and Guo, Yufei and Wen, Lijie and Yu, Philip S. and Guo, Zhijiang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05177},
  eprint = {2310.05177},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.05177},
  abstract = {Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to comprehensively evaluate the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs are able to compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes will be publicly available.},
  archiveprefix = {arxiv}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv}
}

@misc{huThoughtCloningLearning2023,
  title = {Thought {{Cloning}}: {{Learning}} to {{Think}} While {{Acting}} by {{Imitating Human Thinking}}},
  shorttitle = {Thought {{Cloning}}},
  author = {Hu, Shengran and Clune, Jeff},
  year = {2023},
  month = may,
  number = {arXiv:2306.00323},
  eprint = {2306.00323},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Language is often considered a key aspect of human thinking, providing us with exceptional abilities to generalize, explore, plan, replan, and adapt to new situations. However, Reinforcement Learning (RL) agents are far from human-level performance in any of these abilities. We hypothesize one reason for such cognitive deficiencies is that they lack the benefits of thinking in language and that we can improve AI agents by training them to think like humans do. We introduce a novel Imitation Learning framework, Thought Cloning, where the idea is to not just clone the behaviors of human demonstrators, but also the thoughts humans have as they perform these behaviors. While we expect Thought Cloning to truly shine at scale on internet-sized datasets of humans thinking out loud while acting (e.g. online videos with transcripts), here we conduct experiments in a domain where the thinking and action data are synthetically generated. Results reveal that Thought Cloning learns much faster than Behavioral Cloning and its performance advantage grows the further out of distribution test tasks are, highlighting its ability to better handle novel situations. Thought Cloning also provides important benefits for AI Safety and Interpretability, and makes it easier to debug and improve AI. Because we can observe the agent's thoughts, we can (1) more easily diagnose why things are going wrong, making it easier to fix the problem, (2) steer the agent by correcting its thinking, or (3) prevent it from doing unsafe things it plans to do. Overall, by training agents how to think as well as behave, Thought Cloning creates safer, more powerful agents.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{huyenMultimodalityLargeMultimodal2023,
  title = {Multimodality and {{Large Multimodal Models}} ({{LMMs}})},
  author = {Huyen, Chip},
  year = {2023},
  month = oct,
  journal = {Chip Huyen},
  abstract = {For a long time, each ML model operated in one data mode {\textendash} text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).},
  howpublished = {https://huyenchip.com/2023/10/10/multimodal.html},
  langid = {english}
}

@misc{internlmteamInternLMMultilingualLanguage,
  title = {{{InternLM}}: {{A Multilingual Language Model}} with {{Progressively Enhanced Capabilities}}},
  author = {{InternLM Team}},
  langid = {english}
}

@inproceedings{jiaHeterogeneousGraphNeural2021,
  title = {Heterogeneous {{Graph Neural Networks}} for {{Concept Prerequisite Relation Learning}} in {{Educational Data}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Jia, Chenghao and Shen, Yongliang and Tang, Yechun and Sun, Lu and Lu, Weiming},
  year = {2021},
  pages = {2036--2047},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.164},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{jianBootstrappingVisionLanguageLearning2023,
  title = {Bootstrapping {{Vision-Language Learning}} with {{Decoupled Language Pre-training}}},
  author = {Jian, Yiren and Gao, Chongyang and Vosoughi, Soroush},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07063},
  eprint = {2307.07063},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.07063},
  abstract = {We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importantly, our framework is modality-agnostic and flexible in terms of architectural design, as validated by its successful application in a video learning task using varied base modules. The code is available at https://github.com/yiren-jian/BLIText},
  archiveprefix = {arxiv}
}

@misc{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06825},
  eprint = {2310.06825},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{jiangMixtralExperts2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04088},
  eprint = {2401.04088},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.04088},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{jiaoLearningPlanningbasedReasoning2024,
  title = {Learning {{Planning-based Reasoning}} by {{Trajectories Collection}} and {{Process Reward Synthesizing}}},
  author = {Jiao, Fangkai and Qin, Chengwei and Liu, Zhengyuan and Chen, Nancy F. and Joty, Shafiq},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00658},
  eprint = {2402.00658},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{jiaVisualPromptTuning2022,
  title = {Visual {{Prompt Tuning}}},
  author = {Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  year = {2022},
  month = jul,
  number = {arXiv:2203.12119},
  eprint = {2203.12119},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.12119},
  abstract = {The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1\% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost.},
  archiveprefix = {arxiv}
}

@misc{jiEfficientExactOptimization2024,
  title = {Towards {{Efficient}} and {{Exact Optimization}} of {{Language Model Alignment}}},
  author = {Ji, Haozhe and Lu, Cheng and Niu, Yilin and Ke, Pei and Wang, Hongning and Zhu, Jun and Tang, Jie and Huang, Minlie},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00856},
  eprint = {2402.00856},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution. In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms. We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data.},
  archiveprefix = {arxiv}
}

@misc{jinImpactReasoningStep2024,
  title = {The {{Impact}} of {{Reasoning Step Length}} on {{Large Language Models}}},
  author = {Jin, Mingyu and Yu, Qinkai and Shu, Dong and Zhao, Haiyan and Hua, Wenyue and Meng, Yanda and Zhang, Yongfeng and Du, Mengnan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04925},
  eprint = {2401.04925},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations, while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{juDirectInversionBoosting2023,
  title = {Direct {{Inversion}}: {{Boosting Diffusion-based Editing}} with 3 {{Lines}} of {{Code}}},
  shorttitle = {Direct {{Inversion}}},
  author = {Ju, Xuan and Zeng, Ailing and Bian, Yuxuan and Liu, Shaoteng and Xu, Qiang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01506},
  eprint = {2310.01506},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.01506},
  abstract = {Text-guided diffusion models have revolutionized image generation and editing, offering exceptional realism and diversity. Specifically, in the context of diffusion-based editing, where a source image is edited according to a target prompt, the process commences by acquiring a noisy latent vector corresponding to the source image via the diffusion model. This vector is subsequently fed into separate source and target diffusion branches for editing. The accuracy of this inversion process significantly impacts the final editing outcome, influencing both essential content preservation of the source image and edit fidelity according to the target prompt. Prior inversion techniques aimed at finding a unified solution in both the source and target diffusion branches. However, our theoretical and empirical analyses reveal that disentangling these branches leads to a distinct separation of responsibilities for preserving essential content and ensuring edit fidelity. Building on this insight, we introduce "Direct Inversion," a novel technique achieving optimal performance of both branches with just three lines of code. To assess image editing performance, we present PIE-Bench, an editing benchmark with 700 images showcasing diverse scenes and editing types, accompanied by versatile annotations and comprehensive evaluation metrics. Compared to state-of-the-art optimization-based inversion techniques, our solution not only yields superior performance across 8 editing methods but also achieves nearly an order of speed-up.},
  archiveprefix = {arxiv}
}

@misc{kadavathLanguageModelsMostly2022,
  title = {Language {{Models}} ({{Mostly}}) {{Know What They Know}}},
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and {Hatfield-Dodds}, Zac and DasSarma, Nova and {Tran-Johnson}, Eli and Johnston, Scott and {El-Showk}, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  year = {2022},
  month = nov,
  number = {arXiv:2207.05221},
  eprint = {2207.05221},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.05221},
  abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  archiveprefix = {arxiv}
}

@misc{kaddourChallengesApplicationsLarge2023,
  title = {Challenges and {{Applications}} of {{Large Language Models}}},
  author = {Kaddour, Jean and Harris, Joshua and Mozes, Maximilian and Bradley, Herbie and Raileanu, Roberta and McHardy, Robert},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10169},
  eprint = {2307.10169},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.10169},
  abstract = {Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.},
  archiveprefix = {arxiv}
}

@misc{kalliniMissionImpossibleLanguage2024,
  title = {Mission: {{Impossible Language Models}}},
  shorttitle = {Mission},
  author = {Kallini, Julie and Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle and Potts, Christopher},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06416},
  eprint = {2401.06416},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.06416},
  abstract = {Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.},
  archiveprefix = {arxiv}
}

@article{KDPGEnhancedMRCFramework,
  title = {{{KDPG-Enhanced MRC Framework}} for {{Scientific Entity Recognition}} in {{Survey Papers}}}
}

@article{kerbl3DGaussianSplatting2023,
  title = {{{3D Gaussian Splatting}} for {{Real-Time Radiance Field Rendering}}},
  author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkuehler, Thomas and Drettakis, George},
  year = {2023},
  month = aug,
  journal = {ACM Transactions on Graphics},
  volume = {42},
  number = {4},
  pages = {1--14},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3592433},
  abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ({$\geq$} 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
  langid = {english}
}

@misc{khattabDSPyCompilingDeclarative2023,
  title = {{{DSPy}}: {{Compiling Declarative Language Model Calls}} into {{Self-Improving Pipelines}}},
  shorttitle = {{{DSPy}}},
  author = {Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03714},
  eprint = {2310.03714},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25\% and 65\%, respectively) and pipelines with expert-created demonstrations (by up to 5-46\% and 16-40\%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy},
  archiveprefix = {arxiv}
}

@misc{kimLLMCompilerParallel2023,
  title = {An {{LLM Compiler}} for {{Parallel Function Calling}}},
  author = {Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael W. and Keutzer, Kurt and Gholami, Amir},
  year = {2023},
  month = dec,
  number = {arXiv:2312.04511},
  eprint = {2312.04511},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.04511},
  abstract = {Large Language Models (LLMs) have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute function calls, using user-provided functions to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has expanded LLMs' scope to include multi-function calling, where LLMs are equipped with a variety of functions and select the proper functions based on the context. Multi-function calling abilities of LLMs have catalyzed LLM-based software development, allowing them to tackle more complex problems. However, current methods for multi-function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multi-function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution strategies and dependencies; (ii) a Task Fetching Unit, dispatching function calling tasks; and (iii) an Executor, executing these tasks in parallel. LLMCompiler automatically computes an optimized orchestration for the function calls and can be used with open-source models such as LLaMA-2. We have benchmarked LLMCompiler on a range of tasks including cases with non-trivial inter-dependency between function calls, as well as cases that require dynamic replanning based on intermediate results. We observe consistent latency speedup of up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to {\textasciitilde}9\% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x latency gain over OpenAI's recent parallel function calling, while achieving similar accuracy.},
  archiveprefix = {arxiv}
}

@misc{kinnimentEvaluatingLanguageModelAgents2023,
  title = {Evaluating {{Language-Model Agents}} on {{Realistic Autonomous Tasks}}},
  author = {Kinniment, Megan and Sato, Lucas Jun Koba and Du, Haoxing and Goodrich, Brian and Hasin, Max and Chan, Lawrence and Miles, Luke Harold and Lin, Tao R. and Wijk, Hjalmar and Burget, Joel and Ho, Aaron and Barnes, Elizabeth and Christiano, Paul},
  year = {2023},
  month = dec,
  number = {arXiv:2312.11671},
  eprint = {2312.11671},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult. We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.},
  archiveprefix = {arxiv}
}

@misc{kondratyukVideoPoetLargeLanguage2023,
  title = {{{VideoPoet}}: {{A Large Language Model}} for {{Zero-Shot Video Generation}}},
  shorttitle = {{{VideoPoet}}},
  author = {Kondratyuk, Dan and Yu, Lijun and Gu, Xiuye and Lezama, Jos{\'e} and Huang, Jonathan and Hornung, Rachel and Adam, Hartwig and Akbari, Hassan and Alon, Yair and Birodkar, Vighnesh and Cheng, Yong and Chiu, Ming-Chang and Dillon, Josh and Essa, Irfan and Gupta, Agrim and Hahn, Meera and Hauth, Anja and Hendon, David and Martinez, Alonso and Minnen, David and Ross, David and Schindler, Grant and Sirotenko, Mikhail and Sohn, Kihyuk and Somandepalli, Krishna and Wang, Huisheng and Yan, Jimmy and Yang, Ming-Hsuan and Yang, Xuan and Seybold, Bryan and Jiang, Lu},
  year = {2023},
  month = dec,
  number = {arXiv:2312.14125},
  eprint = {2312.14125},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.14125},
  abstract = {We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model's state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet's ability to generate high-fidelity motions. Project page: http://sites.research.google/videopoet/},
  archiveprefix = {arxiv}
}

@misc{KuoSanMoXingSuanFaJiaSuZhiYiKuoSanMoXingZhengLiu,
  title = {{}},
  journal = {},
  abstract = { progressive distillationguided diffusion distillationstep distillationData-free DistillationLatent Consistency Models{\dots}},
  howpublished = {https://zhuanlan.zhihu.com/p/651180944},
  langid = {chinese}
}

@misc{KuoSanMoXingZhiDDIM,
  title = {{DDIM}},
  journal = {},
  abstract = {``What I cannot create, I do not understand.'' -- Richard Feynman  DDPMDDPM{\dots}},
  howpublished = {https://zhuanlan.zhihu.com/p/565698027},
  langid = {chinese}
}

@misc{KuoSanMoXingZhiDDPM,
  title = {{DDPM}},
  journal = {},
  abstract = {``What I cannot create, I do not understand.'' -- Richard Feynman  AIOpenAI2021DALLE{\dots}},
  howpublished = {https://zhuanlan.zhihu.com/p/563661713},
  langid = {chinese}
}

@misc{kwonEfficientMemoryManagement2023,
  title = {Efficient {{Memory Management}} for {{Large Language Model Serving}} with {{PagedAttention}}},
  author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph E. and Zhang, Hao and Stoica, Ion},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06180},
  eprint = {2309.06180},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.06180},
  abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4\${\textbackslash}times\$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm},
  archiveprefix = {arxiv}
}

@misc{lanCopyAllYou2023,
  title = {Copy {{Is All You Need}}},
  author = {Lan, Tian and Cai, Deng and Wang, Yan and Huang, Heyan and Mao, Xian-Ling},
  year = {2023},
  month = jul,
  number = {arXiv:2307.06962},
  eprint = {2307.06962},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.06962},
  abstract = {The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to domain-specific text collection without extra training. Finally, we observe that our approach attains additional performance gains by simply scaling up to larger text collections, again without further training.{\textbackslash}footnote\{Our source codes are publicly available at {\textbackslash}url\{https://github.com/gmftbyGMFTBY/Copyisallyouneed\}.\}},
  archiveprefix = {arxiv}
}

@misc{leeRLAIFScalingReinforcement2023,
  title = {{{RLAIF}}: {{Scaling Reinforcement Learning}} from {{Human Feedback}} with {{AI Feedback}}},
  shorttitle = {{{RLAIF}}},
  author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
  year = {2023},
  month = nov,
  number = {arXiv:2309.00267},
  eprint = {2309.00267},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.00267},
  abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences. However, gathering high-quality human preference labels can be a time-consuming and expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al., offers a promising alternative that leverages a powerful off-the-shelf LLM to generate preferences in lieu of human annotators. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, RLAIF achieves comparable or superior performance to RLHF, as rated by human evaluators. Furthermore, RLAIF demonstrates the ability to outperform a supervised fine-tuned baseline even when the LLM preference labeler is the same size as the policy. In another experiment, directly prompting the LLM for reward scores achieves superior performance to the canonical RLAIF setup, where LLM preference labels are first distilled into a reward model. Finally, we conduct extensive studies on techniques for generating aligned AI preferences. Our results suggest that RLAIF can achieve human-level performance, offering a potential solution to the scalability limitations of RLHF.},
  archiveprefix = {arxiv}
}

@misc{leeVolcanoMitigatingMultimodal2023,
  title = {Volcano: {{Mitigating Multimodal Hallucination}} through {{Self-Feedback Guided Revision}}},
  shorttitle = {Volcano},
  author = {Lee, Seongyun and Park, Sue Hyun and Jo, Yongrae and Seo, Minjoon},
  year = {2023},
  month = nov,
  journal = {arXiv.org},
  abstract = {Large multimodal models (LMMs) suffer from multimodal hallucination, where they provide incorrect responses misaligned with the given visual information. Recent works have conjectured that one of the reasons behind multimodal hallucination might be due to the vision encoder failing to ground on the image properly. To mitigate this issue, we propose a novel approach that leverages self-feedback as visual cues. Building on this approach, we introduce Volcano, a multimodal self-feedback guided revision model. Volcano generates natural language feedback to its initial response based on the provided visual information and utilizes this feedback to self-revise its initial response. Volcano effectively reduces multimodal hallucination and achieves state-of-the-art on MMHal-Bench, POPE, and GAVIE. It also improves on general multimodal abilities and outperforms previous models on MM-Vet and MMBench. Through a qualitative analysis, we show that Volcano's feedback is properly grounded on the image than the initial response. This indicates that Volcano can provide itself with richer visual information, helping alleviate multimodal hallucination. We publicly release Volcano models of 7B and 13B sizes along with the data and code at https://github.com/kaistAI/Volcano.},
  howpublished = {https://arxiv.org/abs/2311.07362v2},
  langid = {english}
}

@misc{leiInstructERCReformingEmotion2023,
  title = {{{InstructERC}}: {{Reforming Emotion Recognition}} in {{Conversation}} with a {{Retrieval Multi-task LLMs Framework}}},
  shorttitle = {{{InstructERC}}},
  author = {Lei, Shanglin and Dong, Guanting and Wang, Xiaoping and Wang, Keheng and Wang, Sirui},
  year = {2023},
  month = nov,
  number = {arXiv:2309.11911},
  eprint = {2309.11911},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.11911},
  abstract = {The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based plug-and-play plugin framework significantly outperforms all previous models and achieves comprehensive SOTA on three commonly used ERC datasets. Extensive analysis of parameter-efficient and data-scaling experiments provide empirical guidance for applying InstructERC in practical scenarios. Our code will be released after blind review.},
  archiveprefix = {arxiv}
}

@misc{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  author = {Lester, Brian and {Al-Rfou}, Rami and Constant, Noah},
  year = {2021},
  month = apr,
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  langid = {english}
}

@misc{liAgentAlignmentEvolving2024,
  title = {Agent {{Alignment}} in {{Evolving Social Norms}}},
  author = {Li, Shimin and Sun, Tianxiang and Qiu, Xipeng},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04620},
  eprint = {2401.04620},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent can align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{liangEncouragingDivergentThinking2023,
  title = {Encouraging {{Divergent Thinking}} in {{Large Language Models}} through {{Multi-Agent Debate}}},
  author = {Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming},
  year = {2023},
  month = may,
  number = {arXiv:2305.19118},
  eprint = {2305.19118},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{liaoMakePretrainedModel2023,
  title = {Make {{Pre-trained Model Reversible}}: {{From Parameter}} to {{Memory Efficient Fine-Tuning}}},
  shorttitle = {Make {{Pre-trained Model Reversible}}},
  author = {Liao, Baohao and Tan, Shaomu and Monz, Christof},
  year = {2023},
  month = oct,
  number = {arXiv:2306.00477},
  eprint = {2306.00477},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.00477},
  abstract = {Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT method. With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's starting point and making it reversible without additional pre-training. We evaluate MEFT on the GLUE benchmark and five question-answering tasks with various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to 84\% of full fine-tuning with a negligible amount of trainable parameters. Moreover, MEFT achieves the same score on GLUE and a comparable score on the question-answering tasks as full fine-tuning. A similar finding is also observed for the image classification task.},
  archiveprefix = {arxiv}
}

@misc{liAPIBankBenchmarkToolAugmented2023,
  title = {{{API-Bank}}: {{A Benchmark}} for {{Tool-Augmented LLMs}}},
  shorttitle = {{{API-Bank}}},
  author = {Li, Minghao and Song, Feifan and Yu, Bowen and Yu, Haiyang and Li, Zhoujun and Huang, Fei and Li, Yongbin},
  year = {2023},
  month = apr,
  number = {arXiv:2304.08244},
  eprint = {2304.08244},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.08244},
  abstract = {Recent research has shown that Large Language Models (LLMs) can utilize external tools to improve their contextual processing abilities, moving away from the pure language modeling paradigm and paving the way for Artificial General Intelligence. Despite this, there has been a lack of systematic evaluation to demonstrate the efficacy of LLMs using tools to respond to human instructions. This paper presents API-Bank, the first benchmark tailored for Tool-Augmented LLMs. API-Bank includes 53 commonly used API tools, a complete Tool-Augmented LLM workflow, and 264 annotated dialogues that encompass a total of 568 API calls. These resources have been designed to thoroughly evaluate LLMs' ability to plan step-by-step API calls, retrieve relevant APIs, and correctly execute API calls to meet human needs. The experimental results show that GPT-3.5 emerges the ability to use the tools relative to GPT3, while GPT-4 has stronger planning performance. Nevertheless, there remains considerable scope for further improvement when compared to human performance. Additionally, detailed error analysis and case studies demonstrate the feasibility of Tool-Augmented LLMs for daily use, as well as the primary challenges that future research needs to address.},
  archiveprefix = {arxiv}
}

@misc{liBLIP2BootstrappingLanguageImage2023,
  title = {{{BLIP-2}}: {{Bootstrapping Language-Image Pre-training}} with {{Frozen Image Encoders}} and {{Large Language Models}}},
  shorttitle = {{{BLIP-2}}},
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  year = {2023},
  month = jun,
  number = {arXiv:2301.12597},
  eprint = {2301.12597},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.12597},
  abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
  archiveprefix = {arxiv}
}

@misc{liCAMELCommunicativeAgents2023,
  title = {{{CAMEL}}: {{Communicative Agents}} for "{{Mind}}" {{Exploration}} of {{Large Scale Language Model Society}}},
  shorttitle = {{{CAMEL}}},
  author = {Li, Guohao and Hammoud, Hasan Abed Al Kader and Itani, Hani and Khizbullin, Dmitrii and Ghanem, Bernard},
  year = {2023},
  month = mar,
  number = {arXiv:2303.17760},
  eprint = {2303.17760},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.17760},
  abstract = {The rapid advancement of conversational and chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents and provide insight into their "cognitive" processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of chat agents, providing a valuable resource for investigating conversational language models. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond. The GitHub repository of this project is made publicly available on: https://github.com/lightaime/camel.},
  archiveprefix = {arxiv}
}

@misc{liChainCodeReasoning2023,
  title = {Chain of {{Code}}: {{Reasoning}} with a {{Language Model-Augmented Code Emulator}}},
  shorttitle = {Chain of {{Code}}},
  author = {Li, Chengshu and Liang, Jacky and Zeng, Andy and Chen, Xinyun and Hausman, Karol and Sadigh, Dorsa and Levine, Sergey and {Fei-Fei}, Li and Xia, Fei and Ichter, Brian},
  year = {2023},
  month = dec,
  number = {arXiv:2312.04474},
  eprint = {2312.04474},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.04474},
  abstract = {Code provides a general syntactic structure to build complex programs and perform precise computations when paired with a code interpreter - we hypothesize that language models (LMs) can leverage code-writing to improve Chain of Thought reasoning not only for logic and arithmetic tasks, but also for semantic ones (and in particular, those that are a mix of both). For example, consider prompting an LM to write code that counts the number of times it detects sarcasm in an essay: the LM may struggle to write an implementation for "detect\_sarcasm(string)" that can be executed by the interpreter (handling the edge cases would be insurmountable). However, LMs may still produce a valid solution if they not only write code, but also selectively "emulate" the interpreter by generating the expected output of "detect\_sarcasm(string)" and other lines of code that cannot be executed. In this work, we propose Chain of Code (CoC), a simple yet surprisingly effective extension that improves LM code-driven reasoning. The key idea is to encourage LMs to format semantic sub-tasks in a program as flexible pseudocode that the interpreter can explicitly catch undefined behaviors and hand off to simulate with an LM (as an "LMulator"). Experiments demonstrate that Chain of Code outperforms Chain of Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard, Chain of Code achieves 84\%, a gain of 12\% over Chain of Thought. CoC scales well with large and small models alike, and broadens the scope of reasoning questions that LMs can correctly answer by "thinking in code". Project webpage: https://chain-of-code.github.io.},
  archiveprefix = {arxiv}
}

@misc{liCodeIELargeCode2023,
  title = {{{CodeIE}}: {{Large Code Generation Models}} Are {{Better Few-Shot Information Extractors}}},
  shorttitle = {{{CodeIE}}},
  author = {Li, Peng and Sun, Tianxiang and Tang, Qiong and Yan, Hang and Wu, Yuanbin and Huang, Xuanjing and Qiu, Xipeng},
  year = {2023},
  month = may,
  number = {arXiv:2305.05711},
  eprint = {2305.05711},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.},
  archiveprefix = {arxiv}
}

@misc{liEmpoweringVisionLanguageModels2023,
  title = {Empowering {{Vision-Language Models}} to {{Follow Interleaved Vision-Language Instructions}}},
  author = {Li, Juncheng and Pan, Kaihang and Ge, Zhiqi and Gao, Minghe and Zhang, Hanwang and Ji, Wei and Zhang, Wenqiao and Chua, Tat-Seng and Tang, Siliang and Zhuang, Yueting},
  year = {2023},
  month = aug,
  number = {arXiv:2308.04152},
  eprint = {2308.04152},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Multimodal Large Language Models (MLLMs) have recently sparked significant interest, which demonstrates emergent capabilities to serve as a general-purpose model for various vision-language tasks. However, existing methods mainly focus on limited types of instructions with a single image as visual context, which hinders the widespread availability of MLLMs. In this paper, we introduce the I4 benchmark to comprehensively evaluate the instruction following ability on complicated interleaved vision-language instructions, which involve intricate image-text sequential context, covering a diverse range of scenarios (e.g., visually-rich webpages/textbooks, lecture slides, embodied dialogue). Systematic evaluation on our I4 benchmark reveals a common defect of existing methods: the Visual Prompt Generator (VPG) trained on image-captioning alignment objective tends to attend to common foreground information for captioning but struggles to extract specific information required by particular tasks. To address this issue, we propose a generic and lightweight controllable knowledge re-injection module, which utilizes the sophisticated reasoning ability of LLMs to control the VPG to conditionally extract instruction-specific visual information and re-inject it into the LLM. Further, we introduce an annotation-free cross-attention guided counterfactual image training strategy to methodically learn the proposed module by collaborating a cascade of foundation models. Enhanced by the proposed module and training strategy, we present Cheetor, a Transformer-based MLLM that can effectively handle a wide variety of interleaved vision-language instructions and achieves state-of-the-art zero-shot performance across all tasks of I4, without high-quality multimodal instruction tuning data. Cheetor also exhibits competitive performance compared with state-of-the-art instruction tuned models on MME benchmark.},
  archiveprefix = {arxiv}
}

@misc{liFasterDiffusionRethinking2023,
  title = {Faster {{Diffusion}}: {{Rethinking}} the {{Role}} of {{UNet Encoder}} in {{Diffusion Models}}},
  shorttitle = {Faster {{Diffusion}}},
  author = {Li, Senmao and Hu, Taihang and Khan, Fahad Shahbaz and Li, Linxuan and Yang, Shiqi and Wang, Yaxing and Cheng, Ming-Ming and Yang, Jian},
  year = {2023},
  month = dec,
  number = {arXiv:2312.09608},
  eprint = {2312.09608},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.09608},
  abstract = {One of the key components within diffusion models is the UNet for noise prediction. While several works have explored basic properties of the UNet decoder, its encoder largely remains unexplored. In this work, we conduct the first comprehensive study of the UNet encoder. We empirically analyze the encoder features and provide insights to important questions regarding their changes at the inference process. In particular, we find that encoder features change gently, whereas the decoder features exhibit substantial variations across different time-steps. This finding inspired us to omit the encoder at certain adjacent time-steps and reuse cyclically the encoder features in the previous time-steps for the decoder. Further based on this observation, we introduce a simple yet effective encoder propagation scheme to accelerate the diffusion sampling for a diverse set of tasks. By benefiting from our propagation scheme, we are able to perform in parallel the decoder at certain adjacent time-steps. Additionally, we introduce a prior noise injection method to improve the texture details in the generated image. Besides the standard text-to-image task, we also validate our approach on other tasks: text-to-video, personalized generation and reference-guided generation. Without utilizing any knowledge distillation technique, our approach accelerates both the Stable Diffusion (SD) and the DeepFloyd-IF models sampling by 41\${\textbackslash}\%\$ and 24\${\textbackslash}\%\$ respectively, while maintaining high-quality generation performance. Our code is available in {\textbackslash}href\{https://github.com/hutaiHang/Faster-Diffusion\}\{FasterDiffusion\}.},
  archiveprefix = {arxiv}
}

@misc{liLeveragingUnpairedData2023,
  title = {Leveraging {{Unpaired Data}} for {{Vision-Language Generative Models}} via {{Cycle Consistency}}},
  author = {Li, Tianhong and Bhardwaj, Sangnie and Tian, Yonglong and Zhang, Han and Barber, Jarred and Katabi, Dina and Lajoie, Guillaume and Chang, Huiwen and Krishnan, Dilip},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03734},
  eprint = {2310.03734},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.03734},
  abstract = {Current vision-language generative models rely on expansive corpora of paired image-text data to attain optimal performance and generalization capabilities. However, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text correlation, while human annotation is more accurate but requires significant manual effort and expense. We introduce \${\textbackslash}textbf\{ITIT\}\$ (\${\textbackslash}textbf\{I\}\$n\${\textbackslash}textbf\{T\}\$egrating \${\textbackslash}textbf\{I\}\$mage \${\textbackslash}textbf\{T\}\$ext): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data. ITIT is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework. During training, ITIT leverages a small set of paired image-text data to ensure its output matches the input reasonably well in both directions. Simultaneously, the model is also trained on much larger datasets containing only images or texts. This is achieved by enforcing cycle consistency between the original unpaired samples and the cycle-generated counterparts. For instance, it generates a caption for a given input image and then uses the caption to create an output image, and enforces similarity between the input and output images. Our experiments show that ITIT with unpaired datasets exhibits similar scaling behavior as using high-quality paired data. We demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3M) paired image-text data.},
  archiveprefix = {arxiv}
}

@inproceedings{liMakingLanguageModels2023,
  title = {Making {{Language Models Better Reasoners}} with {{Step-Aware Verifier}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  pages = {5315--5333},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.291},
  abstract = {Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9\% to 58.1\% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4\% to 83.2\%).}
}

@misc{linAgentSimsOpenSourceSandbox2023,
  title = {{{AgentSims}}: {{An Open-Source Sandbox}} for {{Large Language Model Evaluation}}},
  shorttitle = {{{AgentSims}}},
  author = {Lin, Jiaju and Zhao, Haoran and Zhang, Aochi and Wu, Yiting and Ping, Huqiuyue and Chen, Qin},
  year = {2023},
  month = aug,
  number = {arXiv:2308.04026},
  eprint = {2308.04026},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {With ChatGPT-like large language models (LLM) prevailing in the community, how to evaluate the ability of LLMs is an open question. Existing evaluation methods suffer from following shortcomings: (1) constrained evaluation abilities, (2) vulnerable benchmarks, (3) unobjective metrics. We suggest that task-based evaluation, where LLM agents complete tasks in a simulated environment, is a one-for-all solution to solve above problems. We present AgentSims, an easy-to-use infrastructure for researchers from all disciplines to test the specific capacities they are interested in. Researchers can build their evaluation tasks by adding agents and buildings on an interactive GUI or deploy and test new support mechanisms, i.e. memory, planning and tool-use systems, by a few lines of codes. Our demo is available at https://agentsims.com .},
  archiveprefix = {arxiv}
}

@misc{lingDeductiveVerificationChainofThought2023,
  title = {Deductive {{Verification}} of {{Chain-of-Thought Reasoning}}},
  author = {Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Huang, Zhiao and Lee, Mingu and Memisevic, Roland and Su, Hao},
  year = {2023},
  month = oct,
  number = {arXiv:2306.03872},
  eprint = {2306.03872},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform explicit and rigorous deductive reasoning, and also ensure the trustworthiness of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose Natural Program, a natural language-based deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a step-by-step manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lz1oceani/verify\_cot.},
  archiveprefix = {arxiv}
}

@misc{linSwiftSageGenerativeAgent2023,
  title = {{{SwiftSage}}: {{A Generative Agent}} with {{Fast}} and {{Slow Thinking}} for {{Complex Interactive Tasks}}},
  shorttitle = {{{SwiftSage}}},
  author = {Lin, Bill Yuchen and Fu, Yicheng and Yang, Karina and Ammanabrolu, Prithviraj and Brahman, Faeze and Huang, Shiyu and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
  year = {2023},
  month = may,
  number = {arXiv:2305.17390},
  eprint = {2305.17390},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex real-world tasks.},
  archiveprefix = {arxiv},
  langid = {english}
}

@inproceedings{liPrefixTuningOptimizingContinuous2021,
  title = {Prefix-{{Tuning}}: {{Optimizing Continuous Prompts}} for {{Generation}}},
  shorttitle = {Prefix-{{Tuning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Xiang Lisa and Liang, Percy},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  month = aug,
  pages = {4582--4597},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.353},
  abstract = {Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were ``virtual tokens''. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.}
}

@misc{liReMaxSimpleEffective2023,
  title = {{{ReMax}}: {{A Simple}}, {{Effective}}, and {{Efficient Reinforcement Learning Method}} for {{Aligning Large Language Models}}},
  shorttitle = {{{ReMax}}},
  author = {Li, Ziniu and Xu, Tian and Zhang, Yushun and Lin, Zhihang and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan},
  year = {2023},
  month = dec,
  number = {arXiv:2310.10505},
  eprint = {2310.10505},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.10505},
  abstract = {Alignment is crucial for training large language models. The predominant strategy is Reinforcement Learning from Human Feedback (RLHF), with Proximal Policy Optimization (PPO) as the de-facto algorithm. Yet, PPO is known to struggle with computational inefficiency, a challenge that this paper aims to address. We identify three important properties of RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on these properties, we develop ReMax, a new algorithm tailored for RLHF. The design of ReMax builds on the celebrated algorithm REINFORCE but is enhanced with a new variance-reduction technique. ReMax offers threefold advantages over PPO: first, it is simple to implement with just 6 lines of code. It further eliminates more than 4 hyper-parameters in PPO, which are laborious to tune. Second, ReMax reduces memory usage by about 50\%. To illustrate, PPO runs out of memory when fine-tuning a Llama2-7B model on A100-80GB GPUs, whereas ReMax can support the training. Even though memory-efficient techniques (e.g., ZeRO and offload) are employed for PPO to afford training, ReMax can utilize a larger batch size to increase throughput. Third, in terms of wall-clock time, PPO is about twice as slow as ReMax per iteration. Importantly, these improvements do not sacrifice task performance. We hypothesize that these advantages can be maintained in larger-scale models.},
  archiveprefix = {arxiv}
}

@misc{liSelfAlignmentInstructionBacktranslation2023,
  title = {Self-{{Alignment}} with {{Instruction Backtranslation}}},
  author = {Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
  year = {2023},
  month = aug,
  number = {arXiv:2308.06259},
  eprint = {2308.06259},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.},
  archiveprefix = {arxiv}
}

@misc{liSilkiePreferenceDistillation2023,
  title = {Silkie: {{Preference Distillation}} for {{Large Visual Language Models}}},
  shorttitle = {Silkie},
  author = {Li, Lei and Xie, Zhihui and Li, Mukai and Chen, Shunian and Wang, Peiyi and Chen, Liang and Yang, Yazheng and Wang, Benyou and Kong, Lingpeng},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10665},
  eprint = {2312.10665},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.10665},
  abstract = {This paper explores preference distillation for large vision language models (LVLMs), improving their ability to generate helpful and faithful responses anchoring the visual context. We first build a vision-language feedback (VLFeedback) dataset utilizing AI annotation. Specifically, responses are generated by models sampled from 12 LVLMs, conditioned on multi-modal instructions sourced from various datasets. We adopt GPT-4V to assess the generated outputs regarding helpfulness, visual faithfulness, and ethical considerations. Furthermore, the preference supervision is distilled into Qwen-VL-Chat through the direct preference optimization (DPO) method. The resulting model Silkie, achieves 6.9\% and 9.5\% relative improvement on the MME benchmark regarding the perception and cognition capabilities, respectively. Silkie also demonstrates reduced hallucination by setting a new state-of-the-art score of 3.02 on the MMHal-Bench benchmark. Further analysis shows that DPO with our VLFeedback dataset mainly boosts the fine-grained perception and complex cognition abilities of LVLMs, leading to more comprehensive improvements compared to human-annotated preference datasets.},
  archiveprefix = {arxiv}
}

@misc{liSuperfilteringWeaktoStrongData2024,
  title = {Superfiltering: {{Weak-to-Strong Data Filtering}} for {{Fast Instruction-Tuning}}},
  shorttitle = {Superfiltering},
  author = {Li, Ming and Zhang, Yong and He, Shwai and Li, Zhitao and Zhao, Hongyu and Wang, Jianzong and Cheng, Ning and Zhou, Tianyi},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00530},
  eprint = {2402.00530},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach.},
  archiveprefix = {arxiv}
}

@misc{liTheoryMindMultiAgent2023,
  title = {Theory of {{Mind}} for {{Multi-Agent Collaboration}} via {{Large Language Models}}},
  author = {Li, Huao and Chong, Yu Quan and Stepputtis, Simon and Campbell, Joseph and Hughes, Dana and Lewis, Michael and Sycara, Katia},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10701},
  eprint = {2310.10701},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.},
  archiveprefix = {arxiv}
}

@misc{liToolAugmentedRewardModeling2023,
  title = {Tool-{{Augmented Reward Modeling}}},
  author = {Li, Lei and Chai, Yekun and Wang, Shuohuan and Sun, Yu and Tian, Hao and Zhang, Ningyu and Wu, Hua},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01045},
  eprint = {2310.01045},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named {\textbackslash}name, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7\% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3\% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32\% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We anticipate that this publicly available dataset will facilitate and inspire further research advancements in the field.},
  archiveprefix = {arxiv}
}

@misc{liuAgentBenchEvaluatingLLMs2023,
  title = {{{AgentBench}}: {{Evaluating LLMs}} as {{Agents}}},
  shorttitle = {{{AgentBench}}},
  author = {Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and Zhang, Shudan and Deng, Xiang and Zeng, Aohan and Du, Zhengxiao and Zhang, Chenhui and Shen, Sheng and Zhang, Tianjun and Su, Yu and Sun, Huan and Huang, Minlie and Dong, Yuxiao and Tang, Jie},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03688},
  eprint = {2308.03688},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 25 LLMs (including APIs and open-sourced models) shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and open-sourced competitors. It also serves as a component of an ongoing project with wider coverage and deeper consideration towards systematic LLM evaluation. Datasets, environments, and an integrated evaluation package for AgentBench are released at https://github.com/THUDM/AgentBench},
  archiveprefix = {arxiv}
}

@misc{liuAPARLLMsCan2024,
  title = {{{APAR}}: {{LLMs Can Do Auto-Parallel Auto-Regressive Decoding}}},
  shorttitle = {{{APAR}}},
  author = {Liu, Mingdao and Zeng, Aohan and Wang, Bowen and Zhang, Peng and Tang, Jie and Dong, Yuxiao},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06761},
  eprint = {2401.06761},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70\% and a latency reduce of 20-35\% in high-throughput scenarios, compared to state-of-the-art serving frameworks.},
  archiveprefix = {arxiv}
}

@misc{liuBOLAABenchmarkingOrchestrating2023,
  title = {{{BOLAA}}: {{Benchmarking}} and {{Orchestrating LLM-augmented Autonomous Agents}}},
  shorttitle = {{{BOLAA}}},
  author = {Liu, Zhiwei and Yao, Weiran and Zhang, Jianguo and Xue, Le and Heinecke, Shelby and Murthy, Rithesh and Feng, Yihao and Chen, Zeyuan and Niebles, Juan Carlos and Arpit, Devansh and Xu, Ran and Mui, Phil and Wang, Huan and Xiong, Caiming and Savarese, Silvio},
  year = {2023},
  month = aug,
  number = {arXiv:2308.05960},
  eprint = {2308.05960},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, {\textbackslash}textit\{i.e.\} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at {\textbackslash}url\{https://github.com/salesforce/BOLAA\}.},
  archiveprefix = {arxiv}
}

@misc{liuChainHindsightAligns2023,
  title = {Chain of {{Hindsight Aligns Language Models}} with {{Feedback}}},
  author = {Liu, Hao and Sferrazza, Carmelo and Abbeel, Pieter},
  year = {2023},
  month = feb,
  number = {arXiv:2302.02676},
  eprint = {2302.02676},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.02676},
  abstract = {Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers from imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue benchmarks, with our approach markedly preferred in human evaluations.},
  archiveprefix = {arxiv}
}

@misc{liuControlLLMAugmentLanguage2023,
  title = {{{ControlLLM}}: {{Augment Language Models}} with {{Tools}} by {{Searching}} on {{Graphs}}},
  shorttitle = {{{ControlLLM}}},
  author = {Liu, Zhaoyang and Lai, Zeqiang and Gao, Zhangwei and Cui, Erfei and Zhu, Xizhou and Lu, Lewei and Chen, Qifeng and Qiao, Yu and Dai, Jifeng and Wang, Wenhai},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17796},
  eprint = {2310.17796},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.17796},
  abstract = {We present ControlLLM, a novel framework that enables large language models (LLMs) to utilize multi-modal tools for solving complex real-world tasks. Despite the remarkable performance of LLMs, they still struggle with tool invocation due to ambiguous user prompts, inaccurate tool selection and parameterization, and inefficient tool scheduling. To overcome these challenges, our framework comprises three key components: (1) a {\textbackslash}textit\{task decomposer\} that breaks down a complex task into clear subtasks with well-defined inputs and outputs; (2) a {\textbackslash}textit\{Thoughts-on-Graph (ToG) paradigm\} that searches the optimal solution path on a pre-built tool graph, which specifies the parameter and dependency relations among different tools; and (3) an {\textbackslash}textit\{execution engine with a rich toolbox\} that interprets the solution path and runs the tools efficiently on different computational devices. We evaluate our framework on diverse tasks involving image, audio, and video processing, demonstrating its superior accuracy, efficiency, and versatility compared to existing methods.},
  archiveprefix = {arxiv}
}

@misc{liuGPTUnderstandsToo2021,
  title = {{{GPT Understands}}, {{Too}}},
  author = {Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  year = {2021},
  month = mar,
  abstract = {Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.},
  langid = {english}
}

@misc{liuImprovedBaselinesVisual2023,
  title = {Improved {{Baselines}} with {{Visual Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03744},
  eprint = {2310.03744},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {\textasciitilde}1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
  archiveprefix = {arxiv}
}

@misc{liuLLaVAPlusLearningUse2023,
  title = {{{LLaVA-Plus}}: {{Learning}} to {{Use Tools}} for {{Creating Multimodal Agents}}},
  shorttitle = {{{LLaVA-Plus}}},
  author = {Liu, Shilong and Cheng, Hao and Liu, Haotian and Zhang, Hao and Li, Feng and Ren, Tianhe and Zou, Xueyan and Yang, Jianwei and Su, Hang and Zhu, Jun and Zhang, Lei and Gao, Jianfeng and Li, Chunyuan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05437},
  eprint = {2311.05437},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA-Plus outperforms LLaVA in existing capabilities and exhibits new ones. It is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.},
  archiveprefix = {arxiv}
}

@misc{liuLostMiddleHow2023,
  title = {Lost in the {{Middle}}: {{How Language Models Use Long Contexts}}},
  shorttitle = {Lost in the {{Middle}}},
  author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03172},
  eprint = {2307.03172},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.03172},
  abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
  archiveprefix = {arxiv}
}

@misc{liuMMBenchYourMultimodal2023,
  title = {{{MMBench}}: {{Is Your Multi-modal Model}} an {{All-around Player}}?},
  shorttitle = {{{MMBench}}},
  author = {Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and Chen, Kai and Lin, Dahua},
  year = {2023},
  month = jul,
  number = {arXiv:2307.06281},
  eprint = {2307.06281},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large vision-language models have recently achieved remarkable progress, exhibiting great perception and reasoning abilities concerning visual information. However, how to effectively evaluate these large vision-language models remains a major obstacle, hindering future model development. Traditional benchmarks like VQAv2 or COCO Caption provide quantitative performance measurements but suffer from a lack of fine-grained ability assessment and non-robust evaluation metrics. Recent subjective benchmarks, such as OwlEval, offer comprehensive evaluations of a model's abilities by incorporating human labor, but they are not scalable and display significant bias. In response to these challenges, we propose MMBench, a novel multi-modality benchmark. MMBench methodically develops a comprehensive evaluation pipeline, primarily comprised of two elements. The first element is a meticulously curated dataset that surpasses existing similar benchmarks in terms of the number and variety of evaluation questions and abilities. The second element introduces a novel CircularEval strategy and incorporates the use of ChatGPT. This implementation is designed to convert free-form predictions into pre-defined choices, thereby facilitating a more robust evaluation of the model's predictions. MMBench is a systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models. We hope MMBench will assist the research community in better evaluating their models and encourage future advancements in this domain. Project page: https://opencompass.org.cn/mmbench.},
  archiveprefix = {arxiv}
}

@misc{liUnderstandingInContextLearning2023,
  title = {Towards {{Understanding In-Context Learning}} with {{Contrastive Demonstrations}} and {{Saliency Maps}}},
  author = {Li, Zongxia and Xu, Paiheng and Liu, Fuxiao and Song, Hyemi},
  year = {2023},
  month = jul,
  number = {arXiv:2307.05052},
  eprint = {2307.05052},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.05052},
  abstract = {We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boosting ICL performance is task-dependent, with limited benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks. These insights are critical for understanding the functionality of LLMs and guiding the development of effective demonstrations, which is increasingly relevant in light of the growing use of LLMs in applications such as ChatGPT. Our research code is publicly available at https://github.com/paihengxu/XICL.},
  archiveprefix = {arxiv}
}

@misc{liuPTuningV2Prompt2021,
  title = {P-{{Tuning}} v2: {{Prompt Tuning Can Be Comparable}} to {{Fine-tuning Universally Across Scales}} and {{Tasks}}},
  shorttitle = {P-{{Tuning}} V2},
  author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  year = {2021},
  month = oct,
  abstract = {Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1\%-3\% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning {\textbackslash}cite\{li2021prefix,qin2021learning\} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.},
  langid = {english}
}

@misc{liuStatisticalRejectionSampling2023,
  title = {Statistical {{Rejection Sampling Improves Preference Optimization}}},
  author = {Liu, Tianqi and Zhao, Yao and Joshi, Rishabh and Khalman, Misha and Saleh, Mohammad and Liu, Peter J. and Liu, Jialu},
  year = {2023},
  month = sep,
  number = {arXiv:2309.06657},
  eprint = {2309.06657},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.06657},
  abstract = {Improving the alignment of language models with human preferences remains an active research challenge. Previous approaches have primarily utilized Reinforcement Learning from Human Feedback (RLHF) via online RL methods such as Proximal Policy Optimization (PPO). Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. DPO's lack of a reward model constrains its ability to sample preference pairs from the optimal policy, and SLiC is restricted to sampling preference pairs only from the SFT policy. To address these limitations, we introduce a novel approach called Statistical Rejection Sampling Optimization (RSO) that aims to source preference data from the target optimal policy using rejection sampling, enabling a more accurate estimation of the optimal policy. We also propose a unified framework that enhances the loss functions used in both SLiC and DPO from a preference modeling standpoint. Through extensive experiments across three diverse tasks, we demonstrate that RSO consistently outperforms both SLiC and DPO on evaluations from both Large Language Model (LLM) and human raters.},
  archiveprefix = {arxiv}
}

@misc{liuTuningLanguageModels2024,
  title = {Tuning {{Language Models}} by {{Proxy}}},
  author = {Liu, Alisa and Han, Xiaochuang and Wang, Yizhong and Tsvetkov, Yulia and Choi, Yejin and Smith, Noah A.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.08565},
  eprint = {2401.08565},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.08565},
  abstract = {Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88\% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model's factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.},
  archiveprefix = {arxiv}
}

@misc{liuVisualInstructionTuning2023,
  title = {Visual {{Instruction Tuning}}},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = {2023},
  month = apr,
  number = {arXiv:2304.08485},
  eprint = {2304.08485},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for generalpurpose visual and language understanding. Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instructionfollowing dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{longLargeLanguageModel2023,
  title = {Large {{Language Model Guided Tree-of-Thought}}},
  author = {Long, Jieyi},
  year = {2023},
  month = may,
  number = {arXiv:2305.08291},
  eprint = {2305.08291},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: {\textbackslash}url\{https://github.com/jieyilong/tree-of-thought-puzzle-solver\}.},
  archiveprefix = {arxiv}
}

@misc{longVideoDrafterContentConsistentMultiScene2024,
  title = {{{VideoDrafter}}: {{Content-Consistent Multi-Scene Video Generation}} with {{LLM}}},
  shorttitle = {{{VideoDrafter}}},
  author = {Long, Fuchen and Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  year = {2024},
  month = jan,
  number = {arXiv:2401.01256},
  eprint = {2401.01256},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The recent innovations and breakthroughs in diffusion models have significantly expanded the possibilities of generating high-quality videos for the given prompts. Most existing works tackle the single-scene scenario with only one video event occurring in a single background. Extending to generate multi-scene videos nevertheless is not trivial and necessitates to nicely manage the logic in between while preserving the consistent visual appearance of key content across video scenes. In this paper, we propose a novel framework, namely VideoDrafter, for content-consistent multi-scene video generation. Technically, VideoDrafter leverages Large Language Models (LLM) to convert the input prompt into comprehensive multi-scene script that benefits from the logical knowledge learnt by LLM. The script for each scene includes a prompt describing the event, the foreground/background entities, as well as camera movement. VideoDrafter identifies the common entities throughout the script and asks LLM to detail each entity. The resultant entity description is then fed into a text-to-image model to generate a reference image for each entity. Finally, VideoDrafter outputs a multi-scene video by generating each scene video via a diffusion process that takes the reference images, the descriptive prompt of the event and camera movement into account. The diffusion model incorporates the reference images as the condition and alignment to strengthen the content consistency of multi-scene videos. Extensive experiments demonstrate that VideoDrafter outperforms the SOTA video generation models in terms of visual quality, content consistency, and user preference.},
  archiveprefix = {arxiv}
}

@article{luBlendingAllYou,
  title = {Blending {{Is All You Need}}: {{Cheaper}}, {{Better Alternative}} to {{Trillion-Parameters LLM}}},
  author = {Lu, Xiaoding and Liu, Zongyi and Liusie, Adian and Raina, Vyas and Mudupalli, Vineet and Zhang, Yuwen and Beauchamp, William},
  langid = {english}
}

@misc{LucidrainsDenoisingdiffusionpytorchImplementation,
  title = {Lucidrains/Denoising-Diffusion-Pytorch: {{Implementation}} of {{Denoising Diffusion Probabilistic Model}} in {{Pytorch}}},
  shorttitle = {Lucidrains/Denoising-Diffusion-Pytorch},
  journal = {GitHub},
  abstract = {Implementation of Denoising Diffusion Probabilistic Model in Pytorch - lucidrains/denoising-diffusion-pytorch: Implementation of Denoising Diffusion Probabilistic Model in Pytorch},
  howpublished = {https://github.com/lucidrains/denoising-diffusion-pytorch},
  langid = {english}
}

@misc{luDPMSolverFastODE2022,
  title = {{{DPM-Solver}}: {{A Fast ODE Solver}} for {{Diffusion Probabilistic Model Sampling}} in {{Around}} 10 {{Steps}}},
  shorttitle = {{{DPM-Solver}}},
  author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  year = {2022},
  month = oct,
  number = {arXiv:2206.00927},
  eprint = {2206.00927},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.00927},
  abstract = {Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a \$4{\textbackslash}sim 16{\textbackslash}times\$ speedup compared with previous state-of-the-art training-free samplers on various datasets.},
  archiveprefix = {arxiv}
}

@misc{luoReasoningGraphsFaithful2023,
  title = {Reasoning on {{Graphs}}: {{Faithful}} and {{Interpretable Large Language Model Reasoning}}},
  shorttitle = {Reasoning on {{Graphs}}},
  author = {Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01061},
  eprint = {2310.01061},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.01061},
  abstract = {Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.},
  archiveprefix = {arxiv}
}

@misc{luoUnderstandingDiffusionModels2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11970},
  eprint = {2208.11970},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  archiveprefix = {arxiv}
}

@misc{luoWizardCoderEmpoweringCode2023,
  title = {{{WizardCoder}}: {{Empowering Code Large Language Models}} with {{Evol-Instruct}}},
  shorttitle = {{{WizardCoder}}},
  author = {Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  year = {2023},
  month = jun,
  number = {arXiv:2306.08568},
  eprint = {2306.08568},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.08568},
  abstract = {Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM},
  archiveprefix = {arxiv}
}

@misc{lyuGoalOrientedScriptConstruction2021,
  title = {Goal-{{Oriented Script Construction}}},
  author = {Lyu, Qing and Zhang, Li and {Callison-Burch}, Chris},
  year = {2021},
  month = aug,
  number = {arXiv:2107.13189},
  eprint = {2107.13189},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The knowledge of scripts, common chains of events in stereotypical scenarios, is a valuable asset for task-oriented natural language understanding systems. We propose the GoalOriented Script Construction task, where a model produces a sequence of steps to accomplish a given goal. We pilot our task on the first multilingual script learning dataset supporting 18 languages collected from wikiHow, a website containing half a million how-to articles. For baselines, we consider both a generationbased approach using a language model and a retrieval-based approach by first retrieving the relevant steps from a large candidate pool and then ordering them. We show that our task is practical, feasible but challenging for state-of-the-art Transformer models, and that our methods can be readily deployed for various other datasets and domains with decent zero-shot performance1.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{lyuMacawLLMMultiModalLanguage2023,
  title = {Macaw-{{LLM}}: {{Multi-Modal Language Modeling}} with {{Image}}, {{Audio}}, {{Video}}, and {{Text Integration}}},
  shorttitle = {Macaw-{{LLM}}},
  author = {Lyu, Chenyang and Wu, Minghao and Wang, Longyue and Huang, Xinting and Liu, Bingshuai and Du, Zefeng and Shi, Shuming and Tu, Zhaopeng},
  year = {2023},
  month = jun,
  number = {arXiv:2306.09093},
  eprint = {2306.09093},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.09093},
  abstract = {Although instruction-tuned large language models (LLMs) have exhibited remarkable capabilities across various NLP tasks, their effectiveness on other data modalities beyond text has not been fully studied. In this work, we propose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information. Macaw-LLM consists of three main components: a modality module for encoding multi-modal data, a cognitive module for harnessing pretrained LLMs, and an alignment module for harmonizing diverse representations. Our novel alignment module seamlessly bridges multi-modal features to textual features, simplifying the adaptation process from the modality modules to the cognitive module. In addition, we construct a large-scale multi-modal instruction dataset in terms of multi-turn dialogue, including 69K image instances and 50K video instances. We have made our data, code and model publicly available, which we hope can pave the way for future research in multi-modal LLMs and expand the capabilities of LLMs to handle diverse data modalities and address complex real-world scenarios.},
  archiveprefix = {arxiv}
}

@misc{maAdversarialSelfSupervisedDataFree2020,
  title = {Adversarial {{Self-Supervised Data-Free Distillation}} for {{Text Classification}}},
  author = {Ma, Xinyin and Shen, Yongliang and Fang, Gongfan and Chen, Chen and Jia, Chenghao and Lu, Weiming},
  year = {2020},
  month = oct,
  number = {arXiv:2010.04883},
  eprint = {2010.04883},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks. In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model. However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues. To tackle this problem, we propose a novel twostage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT). To avoid text generation in discrete space, we introduce a Plug \& Play Embedding Guessing method to craft pseudo embeddings from the teacher's hidden knowledge. Meanwhile, with a self-supervised module to quantify the student's ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner. To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks. We verify the effectiveness of our method on several text classification datasets.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@article{maasSHOW1ShowrunnerAgents,
  title = {{{SHOW-1}} and {{Showrunner Agents}} in {{Multi-Agent Simulations}}},
  author = {Maas, Philipp and Carey, Frank and Wheeler, Chris and Saatchi, Edward and Billington, Pete and Shamash, Jessica Yaffa},
  langid = {english}
}

@misc{madaanSelfRefineIterativeRefinement2023,
  title = {Self-{{Refine}}: {{Iterative Refinement}} with {{Self-Feedback}}},
  shorttitle = {Self-{{Refine}}},
  author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
  year = {2023},
  month = may,
  number = {arXiv:2303.17651},
  eprint = {2303.17651},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.17651},
  abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
  archiveprefix = {arxiv}
}

@misc{madaanTextPatternsEffective2022,
  title = {Text and {{Patterns}}: {{For Effective Chain}} of {{Thought}}, {{It Takes Two}} to {{Tango}}},
  shorttitle = {Text and {{Patterns}}},
  author = {Madaan, Aman and Yazdanbakhsh, Amir},
  year = {2022},
  month = oct,
  number = {arXiv:2209.07686},
  eprint = {2209.07686},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{maddelaTrainingModelsGenerate2023,
  title = {Training {{Models}} to {{Generate}}, {{Recognize}}, and {{Reframe Unhelpful Thoughts}}},
  author = {Maddela, Mounica and Ung, Megan and Xu, Jing and Madotto, Andrea and Foran, Heather and Boureau, Y.-Lan},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02768},
  eprint = {2307.02768},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.02768},
  abstract = {Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or minimal additional model training required.},
  archiveprefix = {arxiv}
}

@misc{maDeepCacheAcceleratingDiffusion2023,
  title = {{{DeepCache}}: {{Accelerating Diffusion Models}} for {{Free}}},
  shorttitle = {{{DeepCache}}},
  author = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.00858},
  eprint = {2312.00858},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Diffusion models have recently gained unprecedented attention in the field of image synthesis due to their remarkable generative capabilities. Notwithstanding their prowess, these models often incur substantial computational costs, primarily attributed to the sequential denoising process and cumbersome model size. Traditional methods for compressing diffusion models typically involve extensive retraining, presenting cost and feasibility challenges. In this paper, we introduce DeepCache, a novel training-free paradigm that accelerates diffusion models from the perspective of model architecture. DeepCache capitalizes on the inherent temporal redundancy observed in the sequential denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages, thereby curtailing redundant computations. Utilizing the property of the U-Net, we reuse the high-level features while updating the low-level features in a very cheap way. This innovative strategy, in turn, enables a speedup factor of 2.3\${\textbackslash}times\$ for Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1\${\textbackslash}times\$ for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments also demonstrate DeepCache's superiority over existing pruning and distillation methods that necessitate retraining and its compatibility with current sampling techniques. Furthermore, we find that under the same throughput, DeepCache effectively achieves comparable or even marginally improved results with DDIM or PLMS. The code is available at https://github.com/horseee/DeepCache},
  archiveprefix = {arxiv}
}

@misc{mainiRephrasingWebRecipe2024,
  title = {Rephrasing the {{Web}}: {{A Recipe}} for {{Compute}} and {{Data-Efficient Language Modeling}}},
  shorttitle = {Rephrasing the {{Web}}},
  author = {Maini, Pratyush and Seto, Skyler and Bai, He and Grangier, David and Zhang, Yizhe and Jaitly, Navdeep},
  year = {2024},
  month = jan,
  number = {arXiv:2401.16380},
  eprint = {2401.16380},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training (\${\textbackslash}textbf\{WRAP\}\$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as "like Wikipedia" or in "question-answer format" to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by \${\textbackslash}sim3x\$. At the same pre-training compute budget, it improves perplexity by more than 10\% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2\%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{malladiFineTuningLanguageModels2023,
  title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},
  author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},
  year = {2023},
  month = may,
  number = {arXiv:2305.17333},
  eprint = {2305.17333},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.},
  archiveprefix = {arxiv}
}

@misc{maNeighboringPerturbationsKnowledge2024,
  title = {Neighboring {{Perturbations}} of {{Knowledge Editing}} on {{Large Language Models}}},
  author = {Ma, Jun-Yu and Gu, Jia-Chen and Zhang, Ningyu and Ling, Zhen-Hua},
  year = {2024},
  month = jan,
  number = {arXiv:2401.17623},
  eprint = {2401.17623},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play framework termed Appending via Preservation and Prevention (APP) is proposed to mitigate the neighboring perturbation by maintaining the integrity of the answer list. Experiments demonstrate the effectiveness of APP coupling with four editing methods on three LLMs.},
  archiveprefix = {arxiv}
}

@misc{maPromptingDistillBoosting2022,
  title = {Prompting to {{Distill}}: {{Boosting Data-Free Knowledge Distillation}} via {{Reinforced Prompt}}},
  shorttitle = {Prompting to {{Distill}}},
  author = {Ma, Xinyin and Wang, Xinchao and Fang, Gongfan and Shen, Yongliang and Lu, Weiming},
  year = {2022},
  month = may,
  number = {arXiv:2205.07523},
  eprint = {2205.07523},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Data-free knowledge distillation (DFKD) conducts knowledge distillation via eliminating the dependence of original training data, and has recently achieved impressive results in accelerating pretrained language models. At the heart of DFKD is to reconstruct a synthetic dataset by inverting the parameters of the uncompressed model. Prior DFKD approaches, however, have largely relied on hand-crafted priors of the target data distribution for the reconstruction, which can be inevitably biased and often incompetent to capture the intrinsic distributions. To address this problem, we propose a prompt-based method, termed as PromptDFD, that allows us to take advantage of learned language priors, which effectively harmonizes the synthetic sentences to be semantically and grammatically correct. Specifically, PromptDFD leverages a pretrained generative model to provide language priors and introduces a reinforced topic prompter to control data synthesis, making the generated samples thematically relevant and semantically plausible, and thus friendly to downstream tasks. As shown in our experiments, the proposed method substantially improves the synthesis quality and achieves considerable improvements on distillation performance. In some cases, PromptDFD even gives rise to results on par with those from the data-driven knowledge distillation with access to the original training data.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{mialonGAIABenchmarkGeneral2023,
  title = {{{GAIA}}: A Benchmark for {{General AI Assistants}}},
  shorttitle = {{{GAIA}}},
  author = {Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  year = {2023},
  month = nov,
  number = {arXiv:2311.12983},
  eprint = {2311.12983},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.12983},
  abstract = {We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92{\textbackslash}\% vs. 15{\textbackslash}\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.},
  archiveprefix = {arxiv}
}

@misc{miaoEfficientGenerativeLarge2023,
  title = {Towards {{Efficient Generative Large Language Model Serving}}: {{A Survey}} from {{Algorithms}} to {{Systems}}},
  shorttitle = {Towards {{Efficient Generative Large Language Model Serving}}},
  author = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.15234},
  eprint = {2312.15234},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.15234},
  abstract = {In the rapidly evolving landscape of artificial intelligence (AI), generative large language models (LLMs) stand at the forefront, revolutionizing how we interact with our data. However, the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput. This survey addresses the imperative need for efficient LLM serving methodologies from a machine learning system (MLSys) research perspective, standing at the crux of advanced AI innovations and practical system optimizations. We provide in-depth analysis, covering a spectrum of solutions, ranging from cutting-edge algorithmic modifications to groundbreaking changes in system designs. The survey aims to provide a comprehensive understanding of the current state and future directions in efficient LLM serving, offering valuable insights for researchers and practitioners in overcoming the barriers of effective LLM deployment, thereby reshaping the future of AI.},
  archiveprefix = {arxiv}
}

@misc{MicrosoftDeepSpeedDeepSpeed,
  title = {Microsoft/{{DeepSpeed}}: {{DeepSpeed}} Is a Deep Learning Optimization Library That Makes Distributed Training and Inference Easy, Efficient, and Effective.},
  shorttitle = {Microsoft/{{DeepSpeed}}},
  journal = {GitHub},
  abstract = {DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. - microsoft/DeepSpeed: DeepSpeed is a deep learning optimization libr...},
  howpublished = {https://github.com/microsoft/DeepSpeed},
  langid = {english}
}

@misc{minRethinkingRoleDemonstrations2022,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = oct,
  number = {arXiv:2202.12837},
  eprint = {2202.12837},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LMs) are able to incontext learn{\textemdash}perform a new task via inference alone by conditioning on a few inputlabel pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required{\textemdash}randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{mirchandaniLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{General Pattern Machines}}},
  author = {Mirchandani, Suvir and Xia, Fei and Florence, Pete and Ichter, Brian and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy},
  year = {2023},
  month = jul,
  number = {arXiv:2307.04721},
  eprint = {2307.04721},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.04721},
  abstract = {We observe that pre-trained large language models (LLMs) are capable of autoregressively completing complex token sequences -- from arbitrary ones procedurally generated by probabilistic context-free grammars (PCFG), to more rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern completion proficiency can be partially retained even when the sequences are expressed using tokens randomly sampled from the vocabulary. These results suggest that without any additional training, LLMs can serve as general sequence modelers, driven by in-context learning. In this work, we investigate how these zero-shot capabilities may be applied to problems in robotics -- from extrapolating sequences of numbers that represent states over time to complete simple motions, to least-to-most prompting of reward-conditioned trajectories that can discover and represent closed-loop policies (e.g., a stabilizing controller for CartPole). While difficult to deploy today for real systems due to latency, context size limitations, and compute costs, the approach of using LLMs to drive low-level control may provide an exciting glimpse into how the patterns among words could be transferred to actions.},
  archiveprefix = {arxiv}
}

@misc{morrisLevelsAGIOperationalizing2023,
  title = {Levels of {{AGI}}: {{Operationalizing Progress}} on the {{Path}} to {{AGI}}},
  shorttitle = {Levels of {{AGI}}},
  author = {Morris, Meredith Ringel and {Sohl-dickstein}, Jascha and Fiedel, Noah and Warkentin, Tris and Dafoe, Allan and Faust, Aleksandra and Farabet, Clement and Legg, Shane},
  year = {2023},
  month = nov,
  number = {arXiv:2311.02462},
  eprint = {2311.02462},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We propose a framework for classifying the capabilities and behavior of Artificial General Intelligence (AGI) models and their precursors. This framework introduces levels of AGI performance, generality, and autonomy. It is our hope that this framework will be useful in an analogous way to the levels of autonomous driving, by providing a common language to compare models, assess risks, and measure progress along the path to AGI. To develop our framework, we analyze existing definitions of AGI, and distill six principles that a useful ontology for AGI should satisfy. These principles include focusing on capabilities rather than mechanisms; separately evaluating generality and performance; and defining stages along the path toward AGI, rather than focusing on the endpoint. With these principles in mind, we propose 'Levels of AGI' based on depth (performance) and breadth (generality) of capabilities, and reflect on how current systems fit into this ontology. We discuss the challenging requirements for future benchmarks that quantify the behavior and capabilities of AGI models against these levels. Finally, we discuss how these levels of AGI interact with deployment considerations such as autonomy and risk, and emphasize the importance of carefully selecting Human-AI Interaction paradigms for responsible and safe deployment of highly capable AI systems.},
  archiveprefix = {arxiv}
}

@misc{muCanLLMsFollow2023,
  title = {Can {{LLMs Follow Simple Rules}}?},
  author = {Mu, Norman and Chen, Sarah and Wang, Zifan and Chen, Sizhe and Karamardian, David and Aljeraisy, Lulwa and Hendrycks, Dan and Wagner, David},
  year = {2023},
  month = nov,
  number = {arXiv:2311.04235},
  eprint = {2311.04235},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.04235},
  abstract = {As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Evaluating how well LLMs follow developer-provided rules in the face of adversarial inputs typically requires manual review, which slows down monitoring and methods development. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 15 simple text scenarios in which the model is instructed to obey a set of rules in natural language while interacting with the human user. Each scenario has a concise evaluation program to determine whether the model has broken any rules in a conversation. Through manual exploration of model behavior in our scenarios, we identify 6 categories of attack strategies and collect two suites of test cases: one consisting of unique conversations from manual testing and one that systematically implements strategies from the 6 categories. Across various popular proprietary and open models such as GPT-4 and Llama 2, we find that all models are susceptible to a wide variety of adversarial hand-crafted user inputs, though GPT-4 is the best-performing model. Additionally, we evaluate open models under gradient-based attacks and find significant vulnerabilities. We propose RuLES as a challenging new setting for research into exploring and defending against both manual and automatic attacks on LLMs.},
  archiveprefix = {arxiv}
}

@misc{muennighoffOctoPackInstructionTuning2023,
  title = {{{OctoPack}}: {{Instruction Tuning Code Large Language Models}}},
  shorttitle = {{{OctoPack}}},
  author = {Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and {von Werra}, Leandro and Longpre, Shayne},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07124},
  eprint = {2308.07124},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.07124},
  abstract = {Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2\% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.},
  archiveprefix = {arxiv}
}

@misc{mukherjeeOrcaProgressiveLearning2023,
  title = {Orca: {{Progressive Learning}} from {{Complex Explanation Traces}} of {{GPT-4}}},
  shorttitle = {Orca},
  author = {Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
  year = {2023},
  month = jun,
  number = {arXiv:2306.02707},
  eprint = {2306.02707},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundation models (LFMs). A number of issues impact the quality of these models, ranging from limited imitation signals from shallow LFM outputs; small scale homogeneous training data; and most notably a lack of rigorous evaluation resulting in overestimating the small model's capability as they tend to learn to imitate the style, but not the reasoning process of LFMs. To address these challenges, we develop Orca (We are working with our legal team to publicly release a diff of the model weights in accordance with LLaMA's release policy to be published at https://aka.ms/orca-lm), a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4 including explanation traces; step-by-step thought processes; and other complex instructions, guided by teacher assistance from ChatGPT. To promote this progressive learning, we tap into large-scale and diverse imitation data with judicious sampling and selection. Orca surpasses conventional state-of-the-art instruction-tuned models such as Vicuna-13B by more than 100\% in complex zero-shot reasoning benchmarks like Big-Bench Hard (BBH) and 42\% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH benchmark and shows competitive performance (4 pts gap with optimized system message) in professional and academic examinations like the SAT, LSAT, GRE, and GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.},
  archiveprefix = {arxiv}
}

@misc{ningSkeletonofThoughtLargeLanguage2023,
  title = {Skeleton-of-{{Thought}}: {{Large Language Models Can Do Parallel Decoding}}},
  shorttitle = {Skeleton-of-{{Thought}}},
  author = {Ning, Xuefei and Lin, Zinan and Zhou, Zixuan and Yang, Huazhong and Wang, Yu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.15337},
  eprint = {2307.15337},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This work aims at decreasing the end-to-end generation latency of large language models (LLMs). One of the major causes of the high generation latency is the sequential decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated by the thinking and writing process of humans, we propose "Skeleton-of-Thought" (SoT), which guides LLMs to first generate the skeleton of the answer, and then conducts parallel API calls or batched decoding to complete the contents of each skeleton point in parallel. Not only does SoT provide considerable speed-up (up to 2.39x across 11 different LLMs), but it can also potentially improve the answer quality on several question categories in terms of diversity and relevance. SoT is an initial attempt at data-centric optimization for efficiency, and reveal the potential of pushing LLMs to think more like a human for answer quality.},
  archiveprefix = {arxiv}
}

@misc{NLPShiQiCong,
  title = {{NLP FlashAttention  PagedAttention,  Attention }},
  journal = {},
  abstract = {FlashAttention 1.1 GPU  FlashAttention  self-attention GPU  A100 (40GB HBM) {\dots}},
  howpublished = {https://zhuanlan.zhihu.com/p/638468472},
  langid = {chinese}
}

@article{NoNEFoundExplaining2023,
  title = {{{NoNE Found}}: {{Explaining}} the {{Output}} of {{Sequence-to-Sequence Models}} When {{No Named Entity}} Is {{Recognized}}},
  shorttitle = {{{NoNE Found}}},
  year = {2023},
  month = jun,
  abstract = {Sequence-to-sequence (seq2seq) models are known to be effective for named-entity recognition and researchers have recently started investigating how to explain the decisions of these models. However, until now, efforts have focused on explaining why a certain named entity has been recognized and have not considered the negative cases, i.e., true negative or false negative cases in which no named entity (NoNE) is recognized. In this paper, we introduce an approach to feature-relevance explainability for seq2seq models that leverages, a special class-of-input (COIN) token to capture whether or not a named entity was present in the input. We carry out experiments on a location detection task using a modified translation model (TANL). The experiments demonstrate that our NoNE approach is able to deliver important information about shortcomings of the seq2seq model and to uncover gaps in the formulation and application of the protocol used to annotate the data.},
  langid = {english}
}

@misc{nottinghamEmbodiedAgentsDream2023,
  title = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}: {{Embodied Decision Making}} Using {{Language Guided World Modelling}}},
  shorttitle = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}},
  author = {Nottingham, Kolby and Ammanabrolu, Prithviraj and Suhr, Alane and Choi, Yejin and Hajishirzi, Hannaneh and Singh, Sameer and Fox, Roy},
  year = {2023},
  month = apr,
  number = {arXiv:2301.12050},
  eprint = {2301.12050},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.12050},
  abstract = {Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.},
  archiveprefix = {arxiv}
}

@misc{olaussonDemystifyingGPTSelfRepair2023,
  title = {Demystifying {{GPT Self-Repair}} for {{Code Generation}}},
  author = {Olausson, Theo X. and Inala, Jeevana Priya and Wang, Chenglong and Gao, Jianfeng and {Solar-Lezama}, Armando},
  year = {2023},
  month = jun,
  number = {arXiv:2306.09896},
  eprint = {2306.09896},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.09896},
  abstract = {Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -- in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness of self-repair is only seen in GPT-4. We also observe that self-repair is bottlenecked by the feedback stage; using GPT-4 to give feedback on the programs generated by GPT-3.5 and using expert human programmers to give feedback on the programs generated by GPT-4, we unlock significant performance gains.},
  archiveprefix = {arxiv}
}

@misc{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02155},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arxiv}
}

@misc{panPreTrainingFineTuningGenerative2023,
  title = {Pre-{{Training}} and {{Fine-Tuning Generative Flow Networks}}},
  author = {Pan, Ling and Jain, Moksh and Madan, Kanika and Bengio, Yoshua},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03419},
  eprint = {2310.03419},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.03419},
  abstract = {Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that the pre-trained OC-GFN model can allow for a direct extraction of a policy capable of sampling from any new reward functions in downstream tasks. Nonetheless, adapting OC-GFN on a downstream task-specific reward involves an intractable marginalization over possible outcomes. We propose a novel way to approximate this marginalization by learning an amortized predictor enabling efficient fine-tuning. Extensive experimental results validate the efficacy of our approach, demonstrating the effectiveness of pre-training the OC-GFN, and its ability to swiftly adapt to downstream tasks and discover modes more efficiently. This work may serve as a foundation for further exploration of pre-training strategies in the context of GFlowNets.},
  archiveprefix = {arxiv}
}

@article{panUnifyingLargeLanguage2024,
  title = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}: {{A Roadmap}}},
  shorttitle = {Unifying {{Large Language Models}} and {{Knowledge Graphs}}},
  author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  year = {2024},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  eprint = {2306.08302},
  primaryclass = {cs},
  pages = {1--20},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2024.3352100},
  abstract = {Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
  archiveprefix = {arxiv}
}

@misc{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.03442},
  eprint = {2304.03442},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{patilGorillaLargeLanguage2023,
  title = {Gorilla: {{Large Language Model Connected}} with {{Massive APIs}}},
  shorttitle = {Gorilla},
  author = {Patil, Shishir G. and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E.},
  year = {2023},
  month = may,
  number = {arXiv:2305.15334},
  eprint = {2305.15334},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{paulREFINERReasoningFeedback2023,
  title = {{{REFINER}}: {{Reasoning Feedback}} on {{Intermediate Representations}}},
  shorttitle = {{{REFINER}}},
  author = {Paul, Debjit and Ismayilzada, Mete and Peyrard, Maxime and Borges, Beatriz and Bosselut, Antoine and West, Robert and Faltings, Boi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01904},
  eprint = {2304.01904},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{paulREFINERReasoningFeedback2023a,
  title = {{{REFINER}}: {{Reasoning Feedback}} on {{Intermediate Representations}}},
  shorttitle = {{{REFINER}}},
  author = {Paul, Debjit and Ismayilzada, Mete and Peyrard, Maxime and Borges, Beatriz and Bosselut, Antoine and West, Robert and Faltings, Boi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.01904},
  eprint = {2304.01904},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.01904},
  abstract = {Language models (LMs) have recently shown remarkable performance on reasoning tasks by explicitly generating intermediate inferences, e.g., chain-of-thought prompting. However, these intermediate inference steps may be inappropriate deductions from the initial context and lead to incorrect final predictions. Here we introduce REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning. Specifically, the critic provides structured feedback that the reasoning LM uses to iteratively improve its intermediate arguments. Empirical evaluations of REFINER on three diverse reasoning tasks show significant improvements over baseline LMs of comparable scale. Furthermore, when using GPT3.5 as the reasoner, the trained critic significantly improves reasoning without finetuning the reasoner. Finally, our critic model is trained without expensive human-in-the-loop data but can be substituted with humans at inference time.},
  archiveprefix = {arxiv}
}

@misc{pengYaRNEfficientContext2023,
  title = {{{YaRN}}: {{Efficient Context Window Extension}} of {{Large Language Models}}},
  shorttitle = {{{YaRN}}},
  author = {Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  year = {2023},
  month = aug,
  number = {arXiv:2309.00071},
  eprint = {2309.00071},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.00071},
  abstract = {Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn},
  archiveprefix = {arxiv}
}

@misc{prasadADaPTAsNeededDecomposition2023,
  title = {{{ADaPT}}: {{As-Needed Decomposition}} and {{Planning}} with {{Language Models}}},
  shorttitle = {{{ADaPT}}},
  author = {Prasad, Archiki and Koller, Alexander and Hartmann, Mareike and Clark, Peter and Sabharwal, Ashish and Bansal, Mohit and Khot, Tushar},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05772},
  eprint = {2311.05772},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) are increasingly being used for interactive decision-making tasks requiring planning and adapting to the environment. Recent works employ LLMs-as-agents in broadly two ways: iteratively determining the next action (iterative executors) or generating plans and executing sub-tasks using LLMs (plan-and-execute). However, these methods struggle with task complexity, as the inability to execute any sub-task may lead to task failure. To address these shortcomings, we introduce As-Needed Decomposition and Planning for complex Tasks (ADaPT), an approach that explicitly plans and decomposes complex sub-tasks as-needed, i.e., when the LLM is unable to execute them. ADaPT recursively decomposes sub-tasks to adapt to both task complexity and LLM capability. Our results demonstrate that ADaPT substantially outperforms established strong baselines, achieving success rates up to 28.3\% higher in ALFWorld, 27\% in WebShop, and 33\% in TextCraft -- a novel compositional dataset that we introduce. Through extensive analysis, we illustrate the importance of multilevel decomposition and establish that ADaPT dynamically adjusts to the capabilities of the executor LLM as well as to task complexity.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{qianCommunicativeAgentsSoftware2023,
  title = {Communicative {{Agents}} for {{Software Development}}},
  author = {Qian, Chen and Cong, Xin and Yang, Cheng and Chen, Weize and Su, Yusheng and Xu, Juyuan and Liu, Zhiyuan and Sun, Maosong},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07924},
  eprint = {2307.07924},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.07924},
  abstract = {Software engineering is a domain characterized by intricate decision-making processes, often relying on nuanced intuition and consultation. Recent advancements in deep learning have started to revolutionize software engineering practices through elaborate designs implemented at various stages of software development. In this paper, we present an innovative paradigm that leverages large language models (LLMs) throughout the entire software development process, streamlining and unifying key processes through natural language communication, thereby eliminating the need for specialized models at each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered software development company that mirrors the established waterfall model, meticulously dividing the development process into four distinct chronological stages: designing, coding, testing, and documenting. Each stage engages a team of agents, such as programmers, code reviewers, and test engineers, fostering collaborative dialogue and facilitating a seamless workflow. The chat chain acts as a facilitator, breaking down each stage into atomic subtasks. This enables dual roles, allowing for proposing and validating solutions through context-aware communication, leading to efficient resolution of specific subtasks. The instrumental analysis of ChatDev highlights its remarkable efficacy in software generation, enabling the completion of the entire software development process in under seven minutes at a cost of less than one dollar. It not only identifies and alleviates potential vulnerabilities but also rectifies potential hallucinations while maintaining commendable efficiency and cost-effectiveness. The potential of ChatDev unveils fresh possibilities for integrating LLMs into the realm of software development.},
  archiveprefix = {arxiv}
}

@misc{qianInvestigateConsolidateExploitGeneralStrategy2024,
  title = {Investigate-{{Consolidate-Exploit}}: {{A General Strategy}} for {{Inter-Task Agent Self-Evolution}}},
  shorttitle = {Investigate-{{Consolidate-Exploit}}},
  author = {Qian, Cheng and Liang, Shihao and Qin, Yujia and Ye, Yining and Cong, Xin and Lin, Yankai and Wu, Yesai and Liu, Zhiyuan and Sun, Maosong},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13996},
  eprint = {2401.13996},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80\% and significantly decreasing the demand for the model's capability. Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full autonomy.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{qianToolinkLinkingToolkit2023,
  title = {Toolink: {{Linking Toolkit Creation}} and {{Using}} through {{Chain-of-Solving}} on {{Open-Source Model}}},
  shorttitle = {Toolink},
  author = {Qian, Cheng and Xiong, Chenyan and Liu, Zhenghao and Liu, Zhiyuan},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05155},
  eprint = {2310.05155},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation on diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.},
  archiveprefix = {arxiv}
}

@misc{qiaoAUTOACTAutomaticAgent2024,
  title = {{{AUTOACT}}: {{Automatic Agent Learning}} from {{Scratch}} via {{Self-Planning}}},
  shorttitle = {{{AUTOACT}}},
  author = {Qiao, Shuofei and Zhang, Ningyu and Fang, Runnan and Luo, Yujie and Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Lv, Chengfei and Chen, Huajun},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  abstract = {Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent. Code will be available at https://github.com/zjunlp/AutoAct.},
  howpublished = {https://arxiv.org/abs/2401.05268v1},
  langid = {english}
}

@misc{qinDiffusionGPTLLMDrivenTexttoImage2024,
  title = {{{DiffusionGPT}}: {{LLM-Driven Text-to-Image Generation System}}},
  shorttitle = {{{DiffusionGPT}}},
  author = {Qin, Jie and Wu, Jie and Chen, Weifeng and Ren, Yuxi and Li, Huixia and Wu, Hefeng and Xiao, Xuefeng and Wang, Rui and Wen, Shilei},
  year = {2024},
  month = jan,
  number = {arXiv:2401.10061},
  eprint = {2401.10061},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.},
  archiveprefix = {arxiv}
}

@misc{qinOpenVoiceVersatileInstant2023,
  title = {{{OpenVoice}}: {{Versatile Instant Voice Cloning}}},
  shorttitle = {{{OpenVoice}}},
  author = {Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
  year = {2023},
  month = dec,
  journal = {arXiv.org},
  abstract = {We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice can clone voices into a new language without any massive-speaker training data for that language. OpenVoice is also computationally efficient, costing tens of times less than commercially available APIs that offer even inferior performance. To foster further research in the field, we have made the source code and trained model publicly accessible. We also provide qualitative results in our demo website. Prior to its public release, our internal version of OpenVoice was used tens of millions of times by users worldwide between May and October 2023, serving as the backend of MyShell.},
  howpublished = {https://arxiv.org/abs/2312.01479v5},
  langid = {english}
}

@misc{qinToolLearningFoundation2023,
  title = {Tool {{Learning}} with {{Foundation Models}}},
  author = {Qin, Yujia and Hu, Shengding and Lin, Yankai and Chen, Weize and Ding, Ning and Cui, Ganqu and Zeng, Zheni and Huang, Yufei and Xiao, Chaojun and Han, Chi and Fung, Yi Ren and Su, Yusheng and Wang, Huadong and Qian, Cheng and Tian, Runchu and Zhu, Kunlun and Liang, Shihao and Shen, Xingyu and Xu, Bokai and Zhang, Zhen and Ye, Yining and Li, Bowen and Tang, Ziwei and Yi, Jing and Zhu, Yuzhang and Dai, Zhenning and Yan, Lan and Cong, Xin and Lu, Yaxi and Zhao, Weilin and Huang, Yuxiang and Yan, Junxi and Han, Xu and Sun, Xian and Li, Dahai and Phang, Jason and Yang, Cheng and Wu, Tongshuang and Ji, Heng and Liu, Zhiyuan and Sun, Maosong},
  year = {2023},
  month = jun,
  number = {arXiv:2304.08354},
  eprint = {2304.08354},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.08354},
  abstract = {Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning. Overall, we hope this paper could inspire future research in integrating tools with foundation models.},
  archiveprefix = {arxiv}
}

@misc{Qin2023ToolLLM,
  title = {{{ToolLLM}}: {{Facilitating Large Language Models}} to {{Master}} 16000+ {{Real-world APIs}}},
  shorttitle = {{{ToolLLM}}},
  author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
  year = {2023},
  month = jul,
  number = {arXiv:2307.16789},
  eprint = {2307.16789},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Despite the advancements of open-source large language models (LLMs) and their variants, e.g., LLaMA and Vicuna, they remain significantly limited in performing higher-level tasks, such as following human instructions to use external tools (APIs). This is because current instruction tuning largely focuses on basic language tasks instead of the tool-use domain. This is in contrast to state-of-the-art (SOTA) LLMs, e.g., ChatGPT, which have demonstrated excellent tool-use capabilities but are unfortunately closed source. To facilitate tool-use capabilities within open-source LLMs, we introduce ToolLLM, a general tool-use framework of data construction, model training and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is created automatically using ChatGPT. Specifically, we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub, then prompt ChatGPT to generate diverse human instructions involving these APIs, covering both single-tool and multi-tool scenarios. Finally, we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To make the searching process more efficient, we develop a novel depth-first search-based decision tree (DFSDT), enabling LLMs to evaluate multiple reasoning traces and expand the search space. We show that DFSDT significantly enhances the planning and reasoning capabilities of LLMs. For efficient tool-use assessment, we develop an automatic evaluator: ToolEval. We fine-tune LLaMA on ToolBench and obtain ToolLLaMA. Our ToolEval reveals that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. To make the pipeline more practical, we devise a neural API retriever to recommend appropriate APIs for each instruction, negating the need for manual API selection.},
  archiveprefix = {arxiv}
}

@article{QueryParallelMachineReading2023,
  title = {A {{Query-Parallel Machine Reading Comprehension Framework}} for {{Low-resource NER}}},
  year = {2023},
  month = jun,
  abstract = {Named entity recognition (NER) is a fundamental task in natural language processing. Recently, NER has been formulated as a machine reading comprehension (MRC) task, in which manually-crafted queries are used to extract entities of different types. However, current MRC-based NER techniques are limited to extracting a single type of entities at a time and are largely geared towards resource-rich settings. This renders them inefficient during the inference phase, while also leaving their potential untapped for utilization in low-resource settings. We suggest a query-parallel MRC-based approach to address these issues, which is capable of extracting multiple entity types concurrently and is applicable to both resource-rich and resource-limited settings. Specifically, we propose a query-parallel encoder which uses a unidirectional attention mechanism to isolate the semantics of queries and model the query-context interaction with a unidirectional flow. This allows for easier generalization to new entity types or transfer to new domains. After obtaining the query and context representations through the encoder, they are fed into a query-conditioned biaffine predictor to predict entities simultaneously. The model is trained with parameter-efficient tuning technique, making it more data-efficiency. We conduct extensive experiments and demonstrate that our model perform competitively against strong baseline methods in resource-rich setting, and achieves state-of-the-art results in low-resource settings, including training-from-scratch, in-domain transfer and cross-domain transfer tasks.},
  langid = {english}
}

@misc{QuZaoKuoSanYinShiMoXingDenoisingDiffusion,
  title = {{{Denoising Diffusion Implicit Models}},{{DDIM}}}
}

@misc{rafailovDirectPreferenceOptimization2023,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  year = {2023},
  month = dec,
  number = {arXiv:2305.18290},
  eprint = {2305.18290},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  archiveprefix = {arxiv}
}

@misc{rameWARMBenefitsWeight2024,
  title = {{{WARM}}: {{On}} the {{Benefits}} of {{Weight Averaged Reward Models}}},
  shorttitle = {{{WARM}}},
  author = {Ram{\'e}, Alexandre and Vieillard, Nino and Hussenot, L{\'e}onard and Dadashi, Robert and Cideron, Geoffrey and Bachem, Olivier and Ferret, Johan},
  year = {2024},
  month = jan,
  number = {arXiv:2401.12187},
  eprint = {2401.12187},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.12187},
  abstract = {Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4\% win rate against a policy RL fine-tuned with a single RM.},
  archiveprefix = {arxiv}
}

@misc{ranaSayPlanGroundingLarge2023,
  title = {{{SayPlan}}: {{Grounding Large Language Models}} Using {{3D Scene Graphs}} for {{Scalable Task Planning}}},
  shorttitle = {{{SayPlan}}},
  author = {Rana, Krishan and Haviland, Jesse and Garg, Sourav and {Abou-Chakra}, Jad and Reid, Ian and Suenderhauf, Niko},
  year = {2023},
  month = jul,
  number = {arXiv:2307.06135},
  eprint = {2307.06135},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.06135},
  abstract = {Large language models (LLMs) have demonstrated impressive results in developing generalist planning agents for diverse tasks. However, grounding these plans in expansive, multi-floor, and multi-room environments presents a significant challenge for robotics. We introduce SayPlan, a scalable approach to LLM-based, large-scale task planning for robotics using 3D scene graph (3DSG) representations. To ensure the scalability of our approach, we: (1) exploit the hierarchical nature of 3DSGs to allow LLMs to conduct a semantic search for task-relevant subgraphs from a smaller, collapsed representation of the full graph; (2) reduce the planning horizon for the LLM by integrating a classical path planner and (3) introduce an iterative replanning pipeline that refines the initial plan using feedback from a scene graph simulator, correcting infeasible actions and avoiding planning failures. We evaluate our approach on two large-scale environments spanning up to 3 floors, 36 rooms and 140 objects, and show that our approach is capable of grounding large-scale, long-horizon task plans from abstract, and natural language instruction for a mobile manipulator robot to execute.},
  archiveprefix = {arxiv}
}

@misc{renRobotsThatAsk2023,
  title = {Robots {{That Ask For Help}}: {{Uncertainty Alignment}} for {{Large Language Model Planners}}},
  shorttitle = {Robots {{That Ask For Help}}},
  author = {Ren, Allen Z. and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and Xu, Zhenjia and Sadigh, Dorsa and Zeng, Andy and Majumdar, Anirudha},
  year = {2023},
  month = jul,
  number = {arXiv:2307.01928},
  eprint = {2307.01928},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.01928},
  abstract = {Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io},
  archiveprefix = {arxiv}
}

@article{RobustnessNamedEntityReplacements2023,
  title = {Robustness of {{Named-Entity Replacements}} for {{In-Context Learning}}},
  year = {2023},
  month = jun,
  abstract = {A key feature of modern large language models (LLMs) is their ability to perform in-context learning, a prompting technique where query-answer demonstrations are shown before the final query. This allows for generalization to novel distributions at inference time where the LLM can learn new rules without parameter updates. However, the choice of demonstrations and their relationship to a particular query can have a profound impact on model accuracy, raising concerns about the true in-context generalization capabilities (Zhao et al. 2021). In this work, we explore the robustness of the in-context learning paradigm by focusing on entities. In particular, we seek to understand the robustness of LLM in-context learning with respect to named entity replacements. We discover a significant variance in downstream performance based on the choice of the named-entities, across three popular reasoning tasks and two popular LLMs. Specifically, model accuracy on the test sets can fluctuate between -2.7 to +8.0 points depending on the choice of named entity replacements. Our analysis exposes the brittleness of LLM in-context learning with respect to named entities, and offers a simple mechanism to improve test performance by hyper-parameter tuning the named entities of a given dataset.},
  langid = {english}
}

@misc{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10752},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arxiv}
}

@article{roziereCodeLlamaOpen,
  title = {Code {{Llama}}: {{Open Foundation Models}} for {{Code}}},
  author = {Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Ellen, Xiaoqing and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and D{\'e}fossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
  langid = {english}
}

@misc{ruanIdentifyingRisksLM2023,
  title = {Identifying the {{Risks}} of {{LM Agents}} with an {{LM-Emulated Sandbox}}},
  author = {Ruan, Yangjun and Dong, Honghua and Wang, Andrew and Pitis, Silviu and Zhou, Yongchao and Ba, Jimmy and Dubois, Yann and Maddison, Chris J. and Hashimoto, Tatsunori},
  year = {2023},
  month = sep,
  number = {arXiv:2309.15817},
  eprint = {2309.15817},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.15817},
  abstract = {Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 68.8\% of failures identified with ToolEmu would be valid real-world agent failures. Using our curated initial benchmark consisting of 36 high-stakes tools and 144 test cases, we provide a quantitative risk analysis of current LM agents and identify numerous failures with potentially severe outcomes. Notably, even the safest LM agent exhibits such failures 23.9\% of the time according to our evaluator, underscoring the need to develop safer LM agents for real-world deployment.},
  archiveprefix = {arxiv}
}

@misc{ruanTPTUTaskPlanning2023,
  title = {{{TPTU}}: {{Task Planning}} and {{Tool Usage}} of {{Large Language Model-based AI Agents}}},
  shorttitle = {{{TPTU}}},
  author = {Ruan, Jingqing and Chen, Yihong and Zhang, Bin and Xu, Zhiwei and Bao, Tianpeng and Du, Guoqing and Shi, Shiwei and Mao, Hangyu and Zeng, Xingyu and Zhao, Rui},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03427},
  eprint = {2308.03427},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.03427},
  abstract = {With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.},
  archiveprefix = {arxiv}
}

@inproceedings{santilliAcceleratingTransformerInference2023,
  title = {Accelerating {{Transformer Inference}} for {{Translation}} via {{Parallel Decoding}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Santilli, Andrea and Severino, Silvio and Postolache, Emilian and Maiorca, Valentino and Mancusi, Michele and Marin, Riccardo and Rodol{\`a}, Emanuele},
  year = {2023},
  eprint = {2305.10427},
  primaryclass = {cs},
  pages = {12336--12355},
  doi = {10.18653/v1/2023.acl-long.689},
  abstract = {Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38\% w.r.t. the standard autoregressive decoding and nearly 2x when scaling the method on parallel resources. Finally, we introduce a decoding dependency graph visualizer (DDGviz) that let us see how the model has learned the conditional dependence between tokens and inspect the decoding procedure.},
  archiveprefix = {arxiv}
}

@misc{sarthiRAPTORRecursiveAbstractive2024,
  title = {{{RAPTOR}}: {{Recursive Abstractive Processing}} for {{Tree-Organized Retrieval}}},
  shorttitle = {{{RAPTOR}}},
  author = {Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.18059},
  eprint = {2401.18059},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.18059},
  abstract = {Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20\% in absolute accuracy.},
  archiveprefix = {arxiv}
}

@article{SequencetosectionNetworkDistant2023,
  title = {A {{Sequence-to-section Network}} for {{Distant Supervised Nested Named Entity Recognition}}},
  year = {2023},
  month = jun,
  abstract = {In this paper, we study the nested named entity recognition (NER) problem under distant supervision. With the incompleteness of distant annotation, the model training often suffer from a high false negative rate. This problem had been conquered by positive and unlabeled(PU) learning on flat NER. However,traditional PU learning considered this task as a sequence-to-sequence task, which cannot distinguish nested named entities from flat ones. To address these issues, we propose a sequence-to-section neural network for nested NER. Instead of judging whether a token belongs to an entity, we determine if a subsection of the text is an entity. Compared with traditional PU learning, our model introduces Ada-sampling in to update the original dictionary,which means our model can not only label entities but can enrich the dictionary of the field.},
  langid = {english}
}

@misc{shaikhSecondThoughtLet2023,
  title = {On {{Second Thought}}, {{Let}}'s {{Not Think Step}} by {{Step}}! {{Bias}} and {{Toxicity}} in {{Zero-Shot Reasoning}}},
  author = {Shaikh, Omar and Zhang, Hongxin and Held, William and Bernstein, Michael and Yang, Diyi},
  year = {2023},
  month = jun,
  number = {arXiv:2212.08061},
  eprint = {2212.08061},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.08061},
  abstract = {Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model's likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.},
  archiveprefix = {arxiv}
}

@misc{shanahanRolePlayLargeLanguage2023,
  title = {Role-{{Play}} with {{Large Language Models}}},
  author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  year = {2023},
  month = may,
  number = {arXiv:2305.16367},
  eprint = {2305.16367},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.16367},
  abstract = {As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.},
  archiveprefix = {arxiv}
}

@misc{shaoCharacterLLMTrainableAgent2023,
  title = {Character-{{LLM}}: {{A Trainable Agent}} for {{Role-Playing}}},
  shorttitle = {Character-{{LLM}}},
  author = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10158},
  eprint = {2310.10158},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.10158},
  abstract = {Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents {\textbackslash}textit\{memorize\} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.},
  archiveprefix = {arxiv}
}

@misc{sharmaTruthThereImproving2023,
  title = {The {{Truth}} Is in {{There}}: {{Improving Reasoning}} in {{Language Models}} with {{Layer-Selective Rank Reduction}}},
  shorttitle = {The {{Truth}} Is in {{There}}},
  author = {Sharma, Pratyusha and Ash, Jordan T. and Misra, Dipendra},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13558},
  eprint = {2312.13558},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.13558},
  abstract = {Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates.},
  archiveprefix = {arxiv}
}

@misc{sharmaUnderstandingSycophancyLanguage2023,
  title = {Towards {{Understanding Sycophancy}} in {{Language Models}}},
  author = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Cheng, Newton and Durmus, Esin and {Hatfield-Dodds}, Zac and Johnston, Scott R. and Kravec, Shauna and Maxwell, Timothy and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13548},
  eprint = {2310.13548},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgements favoring sycophantic responses.},
  archiveprefix = {arxiv}
}

@misc{sheaBuildingPersonaConsistent2023,
  title = {Building {{Persona Consistent Dialogue Agents}} with {{Offline Reinforcement Learning}}},
  author = {Shea, Ryan and Yu, Zhou},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10735},
  eprint = {2310.10735},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.10735},
  abstract = {Maintaining a consistent persona is a key quality for any open domain dialogue system. Current state-of-the-art systems do this by training agents with supervised learning or online reinforcement learning (RL). However, systems trained with supervised learning often lack consistency as they are never punished for uttering contradictions. Additional training with RL can alleviate some of these issues, however the training process is expensive. Instead, we propose an offline RL framework to improve the persona consistency of dialogue systems. Our framework allows us to combine the advantages of previous methods as we can inexpensively train our model on existing data as in supervised learning, while punishing and rewarding specific utterances as in RL. We also introduce a simple importance sampling method to reduce the variance of importance weights in offline RL training which we call Variance-Reducing MLE-Initialized (VaRMI) importance sampling. Our automatic and human evaluations show that our framework improves both the persona consistency and dialogue quality of a state-of-the-art social chatbot.},
  archiveprefix = {arxiv}
}

@misc{shenDiffusionNERBoundaryDiffusion2023,
  title = {{{DiffusionNER}}: {{Boundary Diffusion}} for {{Named Entity Recognition}}},
  shorttitle = {{{DiffusionNER}}},
  author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  year = {2023},
  month = may,
  number = {arXiv:2305.13298},
  eprint = {2305.13298},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this paper, we propose DIFFUSIONNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DIFFUSIONNER gradually adds noises to the gold entity boundaries by a fixed forward diffusion process and learns a reverse diffusion process to recover the entity boundaries. In inference, DIFFUSIONNER first randomly samples some noisy spans from a standard Gaussian distribution and then generates the named entities by denoising them with the learned reverse diffusion process. The proposed boundary-denoising diffusion process allows progressive refinement and dynamic sampling of entities, empowering DIFFUSIONNER with efficient and flexible entity generation capability. Experiments on multiple flat and nested NER datasets demonstrate that DIFFUSIONNER achieves comparable or even better performance than previous state-ofthe-art models1.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{shengSLoRAServingThousands2023,
  title = {S-{{LoRA}}: {{Serving Thousands}} of {{Concurrent LoRA Adapters}}},
  shorttitle = {S-{{LoRA}}},
  author = {Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2023},
  month = nov,
  number = {arXiv:2311.03285},
  eprint = {2311.03285},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services.},
  archiveprefix = {arxiv}
}

@misc{shenHuggingGPTSolvingAI2023,
  title = {{{HuggingGPT}}: {{Solving AI Tasks}} with {{ChatGPT}} and Its {{Friends}} in {{Hugging Face}}},
  shorttitle = {{{HuggingGPT}}},
  author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  year = {2023},
  month = may,
  number = {arXiv:2303.17580},
  eprint = {2303.17580},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards artificial general intelligence 2.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{shenLargeLanguageModel2023,
  title = {Large {{Language Model Alignment}}: {{A Survey}}},
  shorttitle = {Large {{Language Model Alignment}}},
  author = {Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  year = {2023},
  month = sep,
  number = {arXiv:2309.15025},
  eprint = {2309.15025},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.15025},
  abstract = {Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values. This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead. Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.},
  archiveprefix = {arxiv}
}

@misc{shenLocateLabelTwostage2021,
  title = {Locate and {{Label}}: {{A Two-stage Identifier}} for {{Nested Named Entity Recognition}}},
  shorttitle = {Locate and {{Label}}},
  author = {Shen, Yongliang and Ma, Xinyin and Tan, Zeqi and Zhang, Shuai and Wang, Wen and Lu, Weiming},
  year = {2021},
  month = jul,
  number = {arXiv:2105.06804},
  eprint = {2105.06804},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Named entity recognition (NER) is a wellstudied task in natural language processing. Traditional NER research only deals with flat entities and ignores nested entities. The span-based methods treat entity recognition as a span classification task. Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition. To tackle these issues, we propose a two-stage entity identifier. First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundaryadjusted span proposals with the corresponding categories. Our method effectively utilizes the boundary information of entities and partially matched spans during training. Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities. In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference. Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{shenParallelInstanceQuery2022,
  title = {Parallel {{Instance Query Network}} for {{Named Entity Recognition}}},
  author = {Shen, Yongliang and Wang, Xiaobin and Tan, Zeqi and Xu, Guangwei and Xie, Pengjun and Huang, Fei and Lu, Weiming and Zhuang, Yueting},
  year = {2022},
  month = mar,
  number = {arXiv:2203.10545},
  eprint = {2203.10545},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Named entity recognition (NER) is a fundamental task in natural language processing. Recent works treat named entity recognition as a reading comprehension task, constructing type-specific queries manually to extract entities. This paradigm suffers from three issues. First, type-specific queries can only extract one type of entities per inference, which is inefficient. Second, the extraction for different types of entities is isolated, ignoring the dependencies between them. Third, query construction relies on external knowledge and is difficult to apply to realistic scenarios with hundreds of entity types. To deal with them, we propose Parallel Instance Query Network (PIQN), which sets up global and learnable instance queries to extract entities from a sentence in a parallel manner. Each instance query predicts one entity, and by feeding all instance queries simultaneously, we can query all entities in parallel. Instead of being constructed from external knowledge, instance queries can learn their different query semantics during training. For training the model, we treat label assignment as a one-to-many Linear Assignment Problem (LAP) and dynamically assign gold entities to instance queries with minimal assignment cost. Experiments on both nested and flat NER datasets demonstrate that our proposed method outperforms previous state-ofthe-art models1.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{shenPromptNERPromptLocating2023,
  title = {{{PromptNER}}: {{Prompt Locating}} and {{Typing}} for {{Named Entity Recognition}}},
  shorttitle = {{{PromptNER}}},
  author = {Shen, Yongliang and Tan, Zeqi and Wu, Shuhui and Zhang, Wenqi and Zhang, Rongsheng and Xi, Yadong and Lu, Weiming and Zhuang, Yueting},
  year = {2023},
  month = may,
  number = {arXiv:2305.17104},
  eprint = {2305.17104},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource indomain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7\% on average1.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{shenTriggerSenseMemoryFlow2021,
  title = {A {{Trigger-Sense Memory Flow Framework}} for {{Joint Entity}} and {{Relation Extraction}}},
  author = {Shen, Yongliang and Ma, Xinyin and Tang, Yechun and Lu, Weiming},
  year = {2021},
  month = apr,
  number = {arXiv:2101.10213},
  eprint = {2101.10213},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Joint entity and relation extraction framework constructs a unified model to perform entity recognition and relation extraction simultaneously, which can exploit the dependency between the two tasks to mitigate the error propagation problem suffered by the pipeline model. Current efforts on joint entity and relation extraction focus on enhancing the interaction between entity recognition and relation extraction through parameter sharing, joint decoding, or other ad-hoc tricks (e.g., modeled as a semi-Markov decision process, cast as a multi-round reading comprehension task). However, there are still two issues on the table. First, the interaction utilized by most methods is still weak and uni-directional, which is unable to model the mutual dependency between the two tasks. Second, relation triggers are ignored by most methods, which can help explain why humans would extract a relation in the sentence. They're essential for relation extraction but overlooked. To this end, we present a Trigger-Sense Memory Flow Framework (TriMF) for joint entity and relation extraction. We build a memory module to remember category representations learned in entity recognition and relation extraction tasks. And based on it, we design a multi-level memory flow attention mechanism to enhance the bi-directional interaction between entity recognition and relation extraction. Moreover, without any human annotations, our model can enhance relation trigger information in a sentence through a trigger sensor module, which improves the model performance and makes model predictions with better interpretation. Experiment results show that our proposed framework achieves state-of-the-art results by improves the relation F1 to 52.44\% (+3.2\%) on SciERC, 66.49\% (+4.9\%) on ACE05, 72.35\% (+0.6\%) on CoNLL04 and 80.66\% (+2.3\%) on ADE.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{shiDragDiffusionHarnessingDiffusion2023,
  title = {{{DragDiffusion}}: {{Harnessing Diffusion Models}} for {{Interactive Point-based Image Editing}}},
  shorttitle = {{{DragDiffusion}}},
  author = {Shi, Yujun and Xue, Chuhui and Pan, Jiachun and Zhang, Wenqing and Tan, Vincent Y. F. and Bai, Song},
  year = {2023},
  month = jul,
  number = {arXiv:2306.14435},
  eprint = {2306.14435},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabling DragDiffusion to complete high-quality editing efficiently. Extensive experiments across a wide range of challenging cases (e.g., multi-objects, diverse object categories, various styles, etc.) demonstrate the versatility and generality of DragDiffusion. Code: https://github.com/Yujun-Shi/DragDiffusion.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{shiEHRAgentCodeEmpowers2024,
  title = {{{EHRAgent}}: {{Code Empowers Large Language Models}} for {{Complex Tabular Reasoning}} on {{Electronic Health Records}}},
  shorttitle = {{{EHRAgent}}},
  author = {Shi, Wenqi and Xu, Ran and Zhuang, Yuchen and Yu, Yue and Zhang, Jieyu and Wu, Hang and Zhu, Yuanda and Ho, Joyce and Yang, Carl and Wang, May D.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.07128},
  eprint = {2401.07128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent1, an LLM agent empowered with a code interface, to autonomously generate and execute code for complex clinical tasks within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on two real-world EHR datasets show that EHRAgent outperforms the strongest LLM agent baseline by 36.48\% and 12.41\%, respectively. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.},
  archiveprefix = {arxiv}
}

@misc{shinnReflexionLanguageAgents2023,
  title = {Reflexion: {{Language Agents}} with {{Verbal Reinforcement Learning}}},
  shorttitle = {Reflexion},
  author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = oct,
  number = {arXiv:2303.11366},
  eprint = {2303.11366},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.11366},
  abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
  archiveprefix = {arxiv}
}

@misc{ShowlabAwesomeVideoDiffusion2024,
  title = {Showlab/{{Awesome-Video-Diffusion}}},
  year = {2024},
  month = jan,
  abstract = {A curated list of recent diffusion models for video generation, editing, restoration, understanding, etc.},
  howpublished = {Show Lab}
}

@misc{singerMakeAVideoTexttoVideoGeneration2022,
  title = {Make-{{A-Video}}: {{Text-to-Video Generation}} without {{Text-Video Data}}},
  shorttitle = {Make-{{A-Video}}},
  author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14792},
  eprint = {2209.14792},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14792},
  abstract = {We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.},
  archiveprefix = {arxiv}
}

@misc{singhCodeFusionPretrainedDiffusion2023,
  title = {{{CodeFusion}}: {{A Pre-trained Diffusion Model}} for {{Code Generation}}},
  shorttitle = {{{CodeFusion}}},
  author = {Singh, Mukul and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Negreanu, Carina and Verbruggen, Gust},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17680},
  eprint = {2310.17680},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.},
  archiveprefix = {arxiv}
}

@misc{songConsistencyModels2023,
  title = {Consistency {{Models}}},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  year = {2023},
  month = may,
  number = {arXiv:2303.01469},
  eprint = {2303.01469},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.01469},
  abstract = {Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  archiveprefix = {arxiv}
}

@misc{songLLMPlannerFewShotGrounded2023,
  title = {{{LLM-Planner}}: {{Few-Shot Grounded Planning}} for {{Embodied Agents}} with {{Large Language Models}}},
  shorttitle = {{{LLM-Planner}}},
  author = {Song, Chan Hee and Wu, Jiaman and Washington, Clayton and Sadler, Brian M. and Chao, Wei-Lun and Su, Yu},
  year = {2023},
  month = mar,
  number = {arXiv:2212.04088},
  eprint = {2212.04088},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.04088},
  abstract = {This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5\% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. Website: https://dki-lab.github.io/LLM-Planner},
  archiveprefix = {arxiv}
}

@misc{songRestGPTConnectingLarge2023,
  title = {{{RestGPT}}: {{Connecting Large Language Models}} with {{Real-World Applications}} via {{RESTful APIs}}},
  shorttitle = {{{RestGPT}}},
  author = {Song, Yifan and Xiong, Weimin and Zhu, Dawei and Li, Cheng and Wang, Ke and Tian, Ye and Li, Sujian},
  year = {2023},
  month = jun,
  number = {arXiv:2306.06624},
  eprint = {2306.06624},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.06624},
  abstract = {Tool-augmented large language models (LLMs) have achieved remarkable progress in tackling a broad range of queries. However, existing work are still in the experimental stage and has limitations in extensibility and robustness, especially facing the real-world applications. In this paper, we consider a more realistic scenario, connecting LLMs with RESTful APIs, which use the commonly adopted REST software architectural style for web service development. To address the practical challenges of planning and API usage, we introduce RestGPT, which leverages LLMs to solve user requests by connecting with RESTful APIs. Specifically, we propose a coarse-to-fine online planning mechanism to enhance the ability of planning and API selection. For the complex scenario of calling RESTful APIs, we also specially designed an API executor to formulate parameters and parse API responses. Experiments show that RestGPT is able to achieve impressive results in complex tasks and has strong robustness, which paves a new way towards AGI.},
  archiveprefix = {arxiv}
}

@misc{spectorAcceleratingLLMInference2023,
  title = {Accelerating {{LLM Inference}} with {{Staged Speculative Decoding}}},
  author = {Spector, Benjamin and Re, Chris},
  year = {2023},
  month = aug,
  number = {arXiv:2308.04623},
  eprint = {2308.04623},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.04623},
  abstract = {Recent advances with large language models (LLM) illustrate their diverse capabilities. We propose a novel algorithm, staged speculative decoding, to accelerate LLM inference in small-batch, on-device scenarios. We address the low arithmetic intensity of small-batch inference by improving upon previous work in speculative decoding. First, we restructure the speculative batch as a tree, which reduces generation costs and increases the expected tokens per batch. Second, we add a second stage of speculative decoding. Taken together, we reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L model while perfectly preserving output quality.},
  archiveprefix = {arxiv}
}

@misc{staticYueDuLiShiJiLu,
  title = {},
  shorttitle = {Chartero},
  author = {{static}, volatile},
  abstract = {Chartero }
}

@misc{stiennonLearningSummarizeHuman2022,
  title = {Learning to Summarize from Human Feedback},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
  year = {2022},
  month = feb,
  number = {arXiv:2009.01325},
  eprint = {2009.01325},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
  archiveprefix = {arxiv}
}

@misc{sumersCognitiveArchitecturesLanguage2023,
  title = {Cognitive {{Architectures}} for {{Language Agents}}},
  author = {Sumers, Theodore and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
  year = {2023},
  month = sep,
  number = {arXiv:2309.02427},
  eprint = {2309.02427},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.02427},
  abstract = {Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning. However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent. To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents. We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework. Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future.},
  archiveprefix = {arxiv}
}

@misc{sunAdaPlannerAdaptivePlanning2023,
  title = {{{AdaPlanner}}: {{Adaptive Planning}} from {{Feedback}} with {{Language Models}}},
  shorttitle = {{{AdaPlanner}}},
  author = {Sun, Haotian and Zhuang, Yuchen and Kong, Lingkai and Dai, Bo and Zhang, Chao},
  year = {2023},
  month = may,
  number = {arXiv:2305.16653},
  eprint = {2305.16653},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decisionmaking performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73\% and 4.11\% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available on https://github.com/haotiansun14/AdaPlanner.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{sunAligningLargeMultimodal2023,
  title = {Aligning {{Large Multimodal Models}} with {{Factually Augmented RLHF}}},
  author = {Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and Keutzer, Kurt and Darrell, Trevor},
  year = {2023},
  month = sep,
  number = {arXiv:2309.14525},
  eprint = {2309.14525},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in "hallucination", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94\% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87\% level), and an improvement by 60\% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.},
  archiveprefix = {arxiv}
}

@misc{sunGenerativePretrainingMultimodality2023,
  title = {Generative {{Pretraining}} in {{Multimodality}}},
  author = {Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
  year = {2023},
  month = jul,
  number = {arXiv:2307.05222},
  eprint = {2307.05222},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.05222},
  abstract = {We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models. Extended capabilities such as multimodal assistants via instruction tuning are also demonstrated with impressive performance.},
  archiveprefix = {arxiv}
}

@inproceedings{sunMinimallySupervisedRelationInduction2022,
  title = {Minimally-{{Supervised Relation Induction}} from {{Pre-trained Language Model}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Sun, Lu and Shen, Yongliang and Lu, Weiming},
  year = {2022},
  pages = {1776--1786},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.findings-naacl.135},
  abstract = {Relation Induction is a very practical task in Natural Language Processing (NLP) area. In practical application scenarios, people want to induce more entity pairs having the same relation from only a few seed entity pairs. Thus, instead of the laborious supervised setting, in this paper, we focus on the minimally-supervised setting where only a couple of seed entity pairs per relation are provided. Although the conventional relation induction methods have made some success, their performance depends heavily on the quality of word embeddings. The great success of Pre-trained Language Models, such as BERT, changes the NLP area a lot, and they are proven to be able to better capture relation knowledge. In this paper, we propose a novel method to induce relation with BERT under the minimally-supervised setting. Specifically, we firstly extract proper templates from the corpus by using the mask-prediction task in BERT to build pseudo-sentences as the context of entity pairs. Then we use BERT attention weights to better represent the pseudosentences. In addition, We also use the Integrated Gradient of entity pairs to iteratively select better templates further. Finally, with the high-quality pseudo-sentences, we can train a better classifier for relation induction. Experiments on Google Analogy Test Sets (GATS), Bigger Analogy Test Set (BATS) and DiffVec demonstrate that our proposed method achieves state-of-the-art performance.},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{sunRetentiveNetworkSuccessor2023,
  title = {Retentive {{Network}}: {{A Successor}} to {{Transformer}} for {{Large Language Models}}},
  shorttitle = {Retentive {{Network}}},
  author = {Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu},
  year = {2023},
  month = aug,
  number = {arXiv:2307.08621},
  eprint = {2307.08621},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost \$O(1)\$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.},
  archiveprefix = {arxiv}
}

@misc{sunSALMONSelfAlignmentPrincipleFollowing2023,
  title = {{{SALMON}}: {{Self-Alignment}} with {{Principle-Following Reward Models}}},
  shorttitle = {{{SALMON}}},
  author = {Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05910},
  eprint = {2310.05910},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.05910},
  abstract = {Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.},
  archiveprefix = {arxiv}
}

@misc{sunSimpleEffectivePruning2023,
  title = {A {{Simple}} and {{Effective Pruning Approach}} for {{Large Language Models}}},
  author = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J. Zico},
  year = {2023},
  month = oct,
  number = {arXiv:2306.11695},
  eprint = {2306.11695},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.11695},
  abstract = {As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda.},
  archiveprefix = {arxiv}
}

@misc{sunThinkonGraphDeepResponsible2023,
  title = {Think-on-{{Graph}}: {{Deep}} and {{Responsible Reasoning}} of {{Large Language Model}} on {{Knowledge Graph}}},
  shorttitle = {Think-on-{{Graph}}},
  author = {Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Ni, Lionel M. and Shum, Heung-Yeung and Guo, Jian},
  year = {2023},
  month = nov,
  number = {arXiv:2307.07697},
  eprint = {2307.07697},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.07697},
  abstract = {Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``\${\textbackslash}hbox\{LLM\}{\textbackslash}otimes{\textbackslash}hbox\{KG\}\$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.},
  archiveprefix = {arxiv}
}

@misc{suzgunMetaPromptingEnhancingLanguage2024,
  title = {Meta-{{Prompting}}: {{Enhancing Language Models}} with {{Task-Agnostic Scaffolding}}},
  shorttitle = {Meta-{{Prompting}}},
  author = {Suzgun, Mirac and Kalai, Adam Tauman},
  year = {2024},
  month = jan,
  number = {arXiv:2401.12954},
  eprint = {2401.12954},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct "expert" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1\%, expert (dynamic) prompting by 17.3\%, and multipersona prompting by 15.2\%.},
  archiveprefix = {arxiv}
}

@misc{tangMultihopReadingComprehension2020,
  title = {Multi-Hop {{Reading Comprehension}} across {{Documents}} with {{Path-based Graph Convolutional Network}}},
  author = {Tang, Zeyun and Shen, Yongliang and Ma, Xinyin and Xu, Wei and Yu, Jiale and Lu, Weiming},
  year = {2020},
  month = jun,
  number = {arXiv:2006.06478},
  eprint = {2006.06478},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Multi-hop reading comprehension across multiple documents attracts much attention recently. In this paper, we propose a novel approach to tackle this multi-hop reading comprehension problem. Inspired by human reasoning processing, we construct a path-based reasoning graph from supporting documents. This graph can combine both the idea of the graph-based and path-based approaches, so it is better for multi-hop reasoning. Meanwhile, we propose Gated-RGCN to accumulate evidence on the path-based reasoning graph, which contains a new question-aware gating mechanism to regulate the usefulness of information propagating across documents and add question information during reasoning. We evaluate our approach on WikiHop dataset, and our approach achieves state-of-the-art accuracy against previously published approaches. Especially, our ensemble model surpasses human performance by 4.2\%.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{tangToolAlpacaGeneralizedTool2023,
  title = {{{ToolAlpaca}}: {{Generalized Tool Learning}} for {{Language Models}} with 3000 {{Simulated Cases}}},
  shorttitle = {{{ToolAlpaca}}},
  author = {Tang, Qiaoyu and Deng, Ziliang and Lin, Hongyu and Han, Xianpei and Liang, Qiao and Sun, Le},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05301},
  eprint = {2306.05301},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Enabling large language models to effectively utilize real-world tools is crucial for achieving embodied intelligence. Existing approaches to tool learning have primarily relied on either extremely large language models, such as GPT4, to attain generalized tool-use abilities in a zero-shot manner, or have utilized supervised learning to train limited types of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without specific tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first collects a comprehensive dataset by building a multi-agent simulation environment, which contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5. This validation supports the notion that learning generalized tool-use abilities is feasible for compact language models.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{tanQuerybasedInstanceDiscrimination2022,
  title = {Query-Based {{Instance Discrimination Network}} for {{Relational Triple Extraction}}},
  author = {Tan, Zeqi and Shen, Yongliang and Hu, Xuming and Zhang, Wenqi and Cheng, Xiaoxia and Lu, Weiming and Zhuang, Yueting},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01797},
  eprint = {2211.01797},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Joint entity and relation extraction has been a core task in the field of information extraction. Recent approaches usually consider the extraction of relational triples from a stereoscopic perspective, either learning a relation-specific tagger or separate classifiers for each relation type. However, they still suffer from error propagation, relation redundancy and lack of high-level connections between triples. To address these issues, we propose a novel querybased approach to construct instance-level representations for relational triples. By metricbased comparison between query embeddings and token embeddings, we can extract all types of triples in one step, thus eliminating the error propagation problem. In addition, we learn the instance-level representation of relational triples via contrastive learning. In this way, relational triples can not only enclose rich classlevel semantics but also access to high-order global connections. Experimental results show that our proposed method achieves the state of the art on five widely used benchmarks.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{tanSequencetoSetNetworkNested2021,
  title = {A {{Sequence-to-Set Network}} for {{Nested Named Entity Recognition}}},
  author = {Tan, Zeqi and Shen, Yongliang and Zhang, Shuai and Lu, Weiming and Zhuang, Yueting},
  year = {2021},
  month = jun,
  number = {arXiv:2105.08901},
  eprint = {2105.08901},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Named entity recognition (NER) is a widely studied task in natural language processing. Recently, a growing number of studies have focused on the nested NER. The span-based methods, considering the entity recognition as a span classification task, can deal with nested entities naturally. But they suffer from the huge search space and the lack of interactions between entities. To address these issues, we propose a novel sequence-to-set neural network for nested NER. Instead of specifying candidate spans in advance, we provide a fixed set of learnable vectors to learn the patterns of the valuable spans. We utilize a non-autoregressive decoder to predict the final set of entities in one pass, in which we are able to capture dependencies between entities. Compared with the sequence-tosequence method, our model is more suitable for such unordered recognition task as it is insensitive to the label order. In addition, we utilize the loss function based on bipartite matching to compute the overall training loss. Experimental results show that our proposed model achieves state-ofthe-art on three nested NER corpora: ACE 2004, ACE 2005 and KBP 2017. The code is available at https://github.com/zqtan1024/sequence-to-set.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{Tara_Backup,
  title = {Tara\_{{Backup}}}
}

@misc{thejaBoostingRAGPicking2023,
  title = {Boosting {{RAG}}: {{Picking}} the {{Best Embedding}} \& {{Reranker}} Models},
  shorttitle = {Boosting {{RAG}}},
  author = {Theja, Ravi},
  year = {2023},
  month = nov,
  journal = {Medium},
  abstract = {Evaluate embeddings and rerankers on your dataset to find the best retrieval mix for RAG pipeline.},
  howpublished = {https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83},
  langid = {english}
}

@techreport{TianDongTaiChangJingShenJingWangLuoBiaoShiYuJianMoFangFa,
  title = {},
  author = {, }
}

@misc{TransformersDeepSpeedIntegration,
  title = {Transformers {{DeepSpeed Integration}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/transformers/installation}
}

@article{trinhSolvingOlympiadGeometry2024,
  title = {Solving Olympiad Geometry without Human Demonstrations},
  author = {Trinh, Trieu H. and Wu, Yuhuai and Le, Quoc V. and He, He and Luong, Thang},
  year = {2024},
  month = jan,
  journal = {Nature},
  volume = {625},
  number = {7995},
  pages = {476--482},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06747-5},
  abstract = {Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1{\textendash}4, owing to their reputed difficulty among the world's best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a~translated IMO theorem in 2004.},
  copyright = {2024 The Author(s)},
  langid = {english}
}

@misc{valmeekamPlanBenchExtensibleBenchmark2023,
  title = {{{PlanBench}}: {{An Extensible Benchmark}} for {{Evaluating Large Language Models}} on {{Planning}} and {{Reasoning}} about {{Change}}},
  shorttitle = {{{PlanBench}}},
  author = {Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  year = {2023},
  month = nov,
  number = {arXiv:2206.10498},
  eprint = {2206.10498},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
  archiveprefix = {arxiv}
}

@misc{valmeekamPlanningAbilitiesLarge2023,
  title = {On the {{Planning Abilities}} of {{Large Language Models}} : {{A Critical Investigation}}},
  shorttitle = {On the {{Planning Abilities}} of {{Large Language Models}}},
  author = {Valmeekam, Karthik and Marquez, Matthew and Sreedharan, Sarath and Kambhampati, Subbarao},
  year = {2023},
  month = nov,
  number = {arXiv:2305.15771},
  eprint = {2305.15771},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs in LLM-Modulo settings where they act as a source of heuristic guidance for external planners and verifiers. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of {\textasciitilde}12\% across the domains. However, the results in the LLM-Modulo setting show more promise. In the LLM-Modulo setting, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.},
  archiveprefix = {arxiv}
}

@misc{viswanathanPrompt2ModelGeneratingDeployable2023,
  title = {{{Prompt2Model}}: {{Generating Deployable Models}} from {{Natural Language Instructions}}},
  shorttitle = {{{Prompt2Model}}},
  author = {Viswanathan, Vijay and Zhao, Chenyang and Bertsch, Amanda and Wu, Tongshuang and Neubig, Graham},
  year = {2023},
  month = aug,
  number = {arXiv:2308.12261},
  eprint = {2308.12261},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.12261},
  abstract = {Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20\% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.},
  archiveprefix = {arxiv}
}

@misc{voronovMindYourFormat2024,
  title = {Mind {{Your Format}}: {{Towards Consistent Evaluation}} of {{In-Context Learning Improvements}}},
  shorttitle = {Mind {{Your Format}}},
  author = {Voronov, Anton and Wolf, Lena and Ryabinin, Max},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06766},
  eprint = {2401.06766},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.06766},
  abstract = {Large language models demonstrate a remarkable capability for learning to solve new tasks from a few examples. The prompt template, or the way the input examples are formatted to obtain the prompt, is an important yet often overlooked aspect of in-context learning. In this work, we conduct a comprehensive study of the template format's influence on the in-context learning performance. We evaluate the impact of the prompt template across models (from 770M to 70B parameters) and 4 standard classification datasets. We show that a poor choice of the template can reduce the performance of the strongest models and inference methods to a random guess level. More importantly, the best templates do not transfer between different setups and even between models of the same family. Our findings show that the currently prevalent approach to evaluation, which ignores template selection, may give misleading results due to different templates in different works. As a first step towards mitigating this issue, we propose Template Ensembles that aggregate model predictions across several templates. This simple test-time augmentation boosts average performance while being robust to the choice of random set of templates.},
  archiveprefix = {arxiv}
}

@misc{vuFreshLLMsRefreshingLarge2023,
  title = {{{FreshLLMs}}: {{Refreshing Large Language Models}} with {{Search Engine Augmentation}}},
  shorttitle = {{{FreshLLMs}}},
  author = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and Luong, Thang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03214},
  eprint = {2310.03214},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.03214},
  abstract = {Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.},
  archiveprefix = {arxiv}
}

@misc{wanEfficientLargeLanguage2024,
  title = {Efficient {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Efficient {{Large Language Models}}},
  author = {Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and Chowdhury, Mosharaf and Zhang, Mi},
  year = {2024},
  month = jan,
  number = {arXiv:2312.03863},
  eprint = {2312.03863},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges.In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field.},
  archiveprefix = {arxiv}
}

@misc{wangAugmentingLanguageModels2023,
  title = {Augmenting {{Language Models}} with {{Long-Term Memory}}},
  author = {Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  year = {2023},
  month = jun,
  number = {arXiv:2306.07174},
  eprint = {2306.07174},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LONGMEM), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LONGMEM can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LONGMEM can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong longcontext models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{wangAutoStoryGeneratingDiverse2023,
  title = {{{AutoStory}}: {{Generating Diverse Storytelling Images}} with {{Minimal Human Effort}}},
  shorttitle = {{{AutoStory}}},
  author = {Wang, Wen and Zhao, Canyu and Chen, Hao and Chen, Zhekai and Zheng, Kecheng and Shen, Chunhua},
  year = {2023},
  month = nov,
  number = {arXiv:2311.11243},
  eprint = {2311.11243},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Story visualization aims to generate a series of images that match the story described in texts, and it requires the generated images to satisfy high quality, alignment with the text description, and consistency in character identities. Given the complexity of story visualization, existing methods drastically simplify the problem by considering only a few specific characters and scenarios, or requiring the users to provide per-image control conditions such as sketches. However, these simplifications render these methods incompetent for real applications. To this end, we propose an automated story visualization system that can effectively generate diverse, high-quality, and consistent sets of story images, with minimal human interactions. Specifically, we utilize the comprehension and planning capabilities of large language models for layout planning, and then leverage large-scale text-to-image models to generate sophisticated story images based on the layout. We empirically find that sparse control conditions, such as bounding boxes, are suitable for layout planning, while dense control conditions, e.g., sketches and keypoints, are suitable for generating high-quality image content. To obtain the best of both worlds, we devise a dense condition generation module to transform simple bounding box layouts into sketch or keypoint control conditions for final image generation, which not only improves the image quality but also allows easy and intuitive user interactions. In addition, we propose a simple yet effective method to generate multi-view consistent character images, eliminating the reliance on human labor to collect or draw character images.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{wangDAMONLPSemEval2022Task2022,
  title = {{{DAMO-NLP}} at {{SemEval-2022 Task}} 11: {{A Knowledge-based System}} for {{Multilingual Named Entity Recognition}}},
  shorttitle = {{{DAMO-NLP}} at {{SemEval-2022 Task}} 11},
  author = {Wang, Xinyu and Shen, Yongliang and Cai, Jiong and Wang, Tao and Wang, Xiaobin and Xie, Pengjun and Huang, Fei and Lu, Weiming and Zhuang, Yueting and Tu, Kewei and Lu, Wei and Jiang, Yong},
  year = {2022},
  month = jun,
  number = {arXiv:2203.00545},
  eprint = {2203.00545},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The MultiCoNER shared task aims at detecting semantically ambiguous and complex named entities in short and low-context settings for multiple languages. The lack of contexts makes the recognition of ambiguous named entities challenging. To alleviate this issue, our team DAMO-NLP proposes a knowledge-based system, where we build a multilingual knowledge base based on Wikipedia to provide related context information to the named entity recognition (NER) model. Given an input sentence, our system effectively retrieves related contexts from the knowledge base. The original input sentences are then augmented with such context information, allowing significantly better contextualized token representations to be captured. Our system wins 10 out of 13 tracks in the MultiCoNER shared task.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{wangDataEfficientDetectionTransformers2022,
  title = {Towards {{Data-Efficient Detection Transformers}}},
  author = {Wang, Wen and Zhang, Jing and Cao, Yang and Shen, Yongliang and Tao, Dacheng},
  year = {2022},
  month = aug,
  number = {arXiv:2203.09507},
  eprint = {2203.09507},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Detection transformers have achieved competitive performance on the sample-rich COCO dataset. However, we show most of them suffer from significant performance drops on small-size datasets, like Cityscapes. In other words, the detection transformers are generally data-hungry. To tackle this problem, we empirically analyze the factors that affect data efficiency, through a step-by-step transition from a dataefficient RCNN variant to the representative DETR. The empirical results suggest that sparse feature sampling from local image areas holds the key. Based on this observation, we alleviate the data-hungry issue of existing detection transformers by simply alternating how key and value sequences are constructed in the cross-attention layer, with minimum modifications to the original models. Besides, we introduce a simple yet effective label augmentation method to provide richer supervision and improve data efficiency. Experiments show that our method can be readily applied to different detection transformers and improve their performance on both small-size and sample-rich datasets. Code will be made publicly available at https://github.com/encounter1997/DE-DETRs.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{wangEnableLanguageModels2023,
  title = {Enable {{Language Models}} to {{Implicitly Learn Self-Improvement From Data}}},
  author = {Wang, Ziqi and Hou, Le and Lu, Tianjian and Wu, Yuexin and Li, Yunxuan and Yu, Hongkun and Ji, Heng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.00898},
  eprint = {2310.00898},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.00898},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.},
  archiveprefix = {arxiv}
}

@misc{wangGenDeFLearningGenerative2023,
  title = {{{GenDeF}}: {{Learning Generative Deformation Field}} for {{Video Generation}}},
  shorttitle = {{{GenDeF}}},
  author = {Wang, Wen and Zheng, Kecheng and Wang, Qiuyu and Chen, Hao and Shi, Zifan and Yang, Ceyuan and Shen, Yujun and Shen, Chunhua},
  year = {2023},
  month = dec,
  number = {arXiv:2312.04561},
  eprint = {2312.04561},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.04561},
  abstract = {We offer a new perspective on approaching the task of video generation. Instead of directly synthesizing a sequence of frames, we propose to render a video by warping one static image with a generative deformation field (GenDeF). Such a pipeline enjoys three appealing advantages. First, we can sufficiently reuse a well-trained image generator to synthesize the static image (also called canonical image), alleviating the difficulty in producing a video and thereby resulting in better visual quality. Second, we can easily convert a deformation field to optical flows, making it possible to apply explicit structural regularizations for motion modeling, leading to temporally consistent results. Third, the disentanglement between content and motion allows users to process a synthesized video through processing its corresponding static image without any tuning, facilitating many applications like video editing, keypoint tracking, and video segmentation. Both qualitative and quantitative results on three common video generation benchmarks demonstrate the superiority of our GenDeF method.},
  archiveprefix = {arxiv}
}

@misc{wangJARVIS1OpenWorldMultitask2023,
  title = {{{JARVIS-1}}: {{Open-World Multi-task Agents}} with {{Memory-Augmented Multimodal Language Models}}},
  shorttitle = {{{JARVIS-1}}},
  author = {Wang, Zihao and Cai, Shaofei and Liu, Anji and Jin, Yonggang and Hou, Jinbing and Zhang, Bowei and Lin, Haowei and He, Zhaofeng and Zheng, Zilong and Yang, Yaodong and Ma, Xiaojian and Liang, Yitao},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05997},
  eprint = {2311.05997},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. In our experiments, JARVIS-1 exhibits nearly perfect performances across over 200 varying tasks from the Minecraft Universe Benchmark, ranging from entry to intermediate levels. JARVIS-1 has achieved a completion rate of 12.5\% in the long-horizon diamond pickaxe task. This represents a significant increase up to 5 times compared to previous records. Furthermore, we show that JARVIS-1 is able to \${\textbackslash}textit\{self-improve\}\$ following a life-long learning paradigm thanks to multimodal memory, sparking a more general intelligence and improved autonomy. The project page is available at https://craftjarvis-jarvis1.github.io.},
  archiveprefix = {arxiv}
}

@misc{wangLearningRetrieveInContext2023,
  title = {Learning to {{Retrieve In-Context Examples}} for {{Large Language Models}}},
  author = {Wang, Liang and Yang, Nan and Wei, Furu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07164},
  eprint = {2307.07164},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.07164},
  abstract = {Large language models (LLMs) have demonstrated their ability to learn in-context, allowing them to perform various tasks based on a few input-output examples. However, the effectiveness of in-context learning is heavily reliant on the quality of the selected examples. In this paper, we propose a novel framework to iteratively train dense retrievers that can identify high-quality in-context examples for LLMs. Our framework initially trains a reward model based on LLM feedback to evaluate the quality of candidate examples, followed by knowledge distillation to train a bi-encoder based dense retriever. Our experiments on a suite of 30 tasks demonstrate that our framework significantly enhances in-context learning performance. Furthermore, we show the generalization ability of our framework to unseen tasks during training. An in-depth analysis reveals that our model improves performance by retrieving examples with similar patterns, and the gains are consistent across LLMs of varying sizes.},
  archiveprefix = {arxiv}
}

@misc{wangMathCoderSeamlessCode2023,
  title = {{{MathCoder}}: {{Seamless Code Integration}} in {{LLMs}} for {{Enhanced Mathematical Reasoning}}},
  shorttitle = {{{MathCoder}}},
  author = {Wang, Ke and Ren, Houxing and Zhou, Aojun and Lu, Zimu and Luo, Sichun and Shi, Weikang and Zhang, Renrui and Song, Linqi and Zhan, Mingjie and Li, Hongsheng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03731},
  eprint = {2310.03731},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2\%) and GSM8K (83.9\%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.},
  archiveprefix = {arxiv}
}

@misc{wangMathShepherdLabelFreeStepbyStep2023,
  title = {Math-{{Shepherd}}: {{A Label-Free Step-by-Step Verifier}} for {{LLMs}} in {{Mathematical Reasoning}}},
  shorttitle = {Math-{{Shepherd}}},
  author = {Wang, Peiyi and Li, Lei and Shao, Zhihong and Xu, R. X. and Dai, Damai and Li, Yifei and Chen, Deli and Wu, Y. and Sui, Zhifang},
  year = {2023},
  month = dec,
  number = {arXiv:2312.08935},
  eprint = {2312.08935},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.08935},
  abstract = {Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks. However, even the most advanced open-source LLMs, such as the LLaMA family models, still face challenges when it comes to accurately solving complex multi-step mathematical problems. In this paper, we present an innovative process-oriented math verifier called {\textbackslash}textbf\{Math-Shepherd\}, which assigns a reward score to each step of the LLM's outputs on math problems. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. With the guidance of Math-Shepherd, a series of open-source LLMs demonstrate exceptional performance. Among them, DeepSeek 67B {\textbackslash}citep\{DeepSeek-llm\} stands out by achieving accuracy rates of 93.3{\textbackslash}\% on the GSM8K dataset and 48.1{\textbackslash}\% on the MATH dataset, without external enhancement such as tool usage. Our Math-Shepherd also outperforms the self-consistency method and other existing verification models. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.},
  archiveprefix = {arxiv}
}

@misc{wangMobileAgentAutonomousMultiModal2024,
  title = {Mobile-{{Agent}}: {{Autonomous Multi-Modal Mobile Device Agent}} with {{Visual Perception}}},
  shorttitle = {Mobile-{{Agent}}},
  author = {Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
  year = {2024},
  month = jan,
  number = {arXiv:2401.16158},
  eprint = {2401.16158},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.},
  archiveprefix = {arxiv}
}

@misc{wangModaVerseEfficientlyTransforming2024,
  title = {{{ModaVerse}}: {{Efficiently Transforming Modalities}} with {{LLMs}}},
  shorttitle = {{{ModaVerse}}},
  author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06395},
  eprint = {2401.06395},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Humans possess the capability to comprehend diverse modalities and seamlessly transfer information between them. In this work, we introduce ModaVerse, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio. Predominant MLLM frameworks have largely relied on the alignment of latent spaces of textual and non-textual features. This alignment process, which synchronizes a language model trained on textual data with encoders and decoders trained on multi-modal data, often necessitates extensive training of several projection layers in multiple stages. Inspired by LLM-as-agent methodologies, we propose a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language. It aligns the LLM's output with the input of generative models, avoiding the complexities associated with latent feature alignments, and simplifying the multiple training stages of existing MLLMs into a single, efficient process. This conceptual advancement leads to significant reductions in both data and computational costs. By conducting experiments on several benchmarks, we demonstrate that our approach attains comparable performance with the state of the art while achieving considerable efficiencies in data usage and training duration.},
  archiveprefix = {arxiv}
}

@misc{wangOpenChatAdvancingOpensource2023,
  title = {{{OpenChat}}: {{Advancing Open-source Language Models}} with {{Mixed-Quality Data}}},
  shorttitle = {{{OpenChat}}},
  author = {Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  year = {2023},
  month = sep,
  number = {arXiv:2309.11235},
  eprint = {2309.11235},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. In this study, we present a novel framework, named OpenChat, to advance open-source language models with mixed-quality data. Specifically, we consider the general SFT training data, consisting of a small amount of expert data mixed with a large proportion of sub-optimal data, without any preference labels. We propose the C(onditioned)-RLFT, which regards different data sources as coarse-grained reward labels and learns a class-conditioned policy to leverage complementary data quality information. Interestingly, the optimal policy in C-RLFT can be easily solved through single-stage, RL-free supervised learning, which is lightweight and avoids costly human preference labeling. Through extensive experiments on three standard benchmarks, our openchat-13b fine-tuned with C-RLFT achieves the highest average performance among all 13b open-source language models. Moreover, we use AGIEval to validate the model generalization performance, in which only openchat-13b surpasses the base model. Finally, we conduct a series of analyses to shed light on the effectiveness and robustness of OpenChat. Our code, data, and models are publicly available at https://github.com/imoneoi/openchat.},
  archiveprefix = {arxiv}
}

@misc{wangOrthogonalSubspaceLearning2023,
  title = {Orthogonal {{Subspace Learning}} for {{Language Model Continual Learning}}},
  author = {Wang, Xiao and Chen, Tianze and Ge, Qiming and Xia, Han and Bao, Rong and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  year = {2023},
  month = oct,
  number = {arXiv:2310.14152},
  eprint = {2310.14152},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.14152},
  abstract = {Benefiting from massive corpora and advanced hardware, large language models (LLMs) exhibit remarkable capabilities in language understanding and generation. However, their performance degrades in scenarios where multiple tasks are encountered sequentially, also known as catastrophic forgetting. In this paper, we propose orthogonal low-rank adaptation (O-LoRA), a simple and efficient approach for continual learning in language models, effectively mitigating catastrophic forgetting while learning new tasks. Specifically, O-LoRA learns tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Our method induces only marginal additional parameter costs and requires no user data storage for replay. Experimental results on continual learning benchmarks show that our method outperforms state-of-the-art methods. Furthermore, compared to previous approaches, our method excels in preserving the generalization ability of LLMs on unseen tasks.},
  archiveprefix = {arxiv}
}

@misc{wangPlanandSolvePromptingImproving2023,
  title = {Plan-and-{{Solve Prompting}}: {{Improving Zero-Shot Chain-of-Thought Reasoning}} by {{Large Language Models}}},
  shorttitle = {Plan-and-{{Solve Prompting}}},
  author = {Wang, Lei and Xu, Wanyu and Lan, Yihuai and Hu, Zhiqiang and Lan, Yunshi and Lee, Roy Ka-Wei and Lim, Ee-Peng},
  year = {2023},
  month = may,
  number = {arXiv:2305.04091},
  eprint = {2305.04091},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zeroshot-CoT concatenates the target problem statement with ``Let's think step by step'' as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Planand-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGIEdgerunners/Plan-and-Solve-Prompting.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{wangPromptAgentStrategicPlanning2023,
  title = {{{PromptAgent}}: {{Strategic Planning}} with {{Language Models Enables Expert-level Prompt Optimization}}},
  shorttitle = {{{PromptAgent}}},
  author = {Wang, Xinyuan and Li, Chenxi and Wang, Zhen and Bai, Fan and Luo, Haotian and Zhang, Jiayou and Jojic, Nebojsa and Xing, Eric P. and Hu, Zhiting},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16427},
  eprint = {2310.16427},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.16427},
  abstract = {Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.},
  archiveprefix = {arxiv}
}

@misc{wangSciBenchEvaluatingCollegeLevel2023,
  title = {{{SciBench}}: {{Evaluating College-Level Scientific Problem-Solving Abilities}} of {{Large Language Models}}},
  shorttitle = {{{SciBench}}},
  author = {Wang, Xiaoxuan and Hu, Ziniu and Lu, Pan and Zhu, Yanqiao and Zhang, Jieyu and Subramaniam, Satyen and Loomba, Arjun R. and Zhang, Shichang and Sun, Yizhou and Wang, Wei},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10635},
  eprint = {2307.10635},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.10635},
  abstract = {Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of delivering satisfactory performance, with an overall score of merely 35.80\%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms others and some strategies that demonstrate improvements in certain problem-solving skills result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.},
  archiveprefix = {arxiv}
}

@misc{wangSCOTTSelfConsistentChainofThought2023,
  title = {{{SCOTT}}: {{Self-Consistent Chain-of-Thought Distillation}}},
  shorttitle = {{{SCOTT}}},
  author = {Wang, Peifeng and Wang, Zhengyang and Li, Zheng and Gao, Yifan and Yin, Bing and Ren, Xiang},
  year = {2023},
  month = jul,
  number = {arXiv:2305.01879},
  eprint = {2305.01879},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.01879},
  abstract = {Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that, while yielding comparable end-task performance, our method can generate CoT rationales that are more faithful than baselines do. Further analysis suggests that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.},
  archiveprefix = {arxiv}
}

@misc{wangSecretsRLHFLarge2024,
  title = {Secrets of {{RLHF}} in {{Large Language Models Part II}}: {{Reward Modeling}}},
  shorttitle = {Secrets of {{RLHF}} in {{Large Language Models Part II}}},
  author = {Wang, Binghai and Zheng, Rui and Chen, Lu and Liu, Yan and Dou, Shihan and Huang, Caishuang and Shen, Wei and Jin, Senjie and Zhou, Enyu and Shi, Chenyu and Gao, Songyang and Xu, Nuo and Zhou, Yuhao and Fan, Xiaoran and Xi, Zhiheng and Zhao, Jun and Wang, Xiao and Ji, Tao and Yan, Hang and Shen, Lixing and Chen, Zhan and Gui, Tao and Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing and Wu, Zuxuan and Jiang, Yu-Gang},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  abstract = {Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training. In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental results confirm that data with varying preference strengths have different impacts on reward model performance. We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data. (2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization.},
  howpublished = {https://arxiv.org/abs/2401.06080v2},
  langid = {english}
}

@misc{wangSelfConsistencyImprovesChain2023,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2023},
  month = mar,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  archiveprefix = {arxiv}
}

@misc{wangSelfInstructAligningLanguage2023,
  title = {Self-{{Instruct}}: {{Aligning Language Models}} with {{Self-Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year = {2023},
  month = may,
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
  archiveprefix = {arxiv}
}

@misc{wangSurveyLargeLanguage2023,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  year = {2023},
  month = aug,
  number = {arXiv:2308.11432},
  eprint = {2308.11432},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
  archiveprefix = {arxiv}
}

@misc{wangTransformingCombiningRewards2024,
  title = {Transforming and {{Combining Rewards}} for {{Aligning Large Language Models}}},
  author = {Wang, Zihao and Nagpal, Chirag and Berant, Jonathan and Eisenstein, Jacob and D'Amour, Alex and Koyejo, Sanmi and Veitch, Victor},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00742},
  eprint = {2402.00742},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.},
  archiveprefix = {arxiv}
}

@misc{wangUnleashingCognitiveSynergy2023,
  title = {Unleashing {{Cognitive Synergy}} in {{Large Language Models}}: {{A Task-Solving Agent}} through {{Multi-Persona Self-Collaboration}}},
  shorttitle = {Unleashing {{Cognitive Synergy}} in {{Large Language Models}}},
  author = {Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng},
  year = {2023},
  month = jul,
  number = {arXiv:2307.05300},
  eprint = {2307.05300},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.05300},
  abstract = {Human intelligence thrives on the concept of cognitive synergy, where collaboration and information integration among different cognitive processes yield superior outcomes compared to individual cognitive processes in isolation. Although Large Language Models (LLMs) have demonstrated promising performance as general task-solving agents, they still struggle with tasks that require intensive domain knowledge and complex reasoning. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist refers to an intelligent agent that collaborates with multiple minds, combining their individual strengths and knowledge, to enhance problem-solving and overall performance in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have discovered that assigning multiple, fine-grained personas in LLMs elicits better problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP effectively elicits internal knowledge acquisition abilities, reduces hallucination, and maintains strong reasoning capabilities. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.},
  archiveprefix = {arxiv}
}

@misc{wangVoyagerOpenEndedEmbodied2023,
  title = {Voyager: {{An Open-Ended Embodied Agent}} with {{Large Language Models}}},
  shorttitle = {Voyager},
  author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  year = {2023},
  month = may,
  number = {arXiv:2305.16291},
  eprint = {2305.16291},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce VOYAGER, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. VOYAGER consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. VOYAGER interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by VOYAGER are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, VOYAGER shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3{\texttimes} more unique items, travels 2.3{\texttimes} longer distances, and unlocks key tech tree milestones up to 15.3{\texttimes} faster than prior SOTA. VOYAGER is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{wanKnowledgeFusionLarge2024,
  title = {Knowledge {{Fusion}} of {{Large Language Models}}},
  author = {Wan, Fanqi and Huang, Xinting and Cai, Deng and Quan, Xiaojun and Bi, Wei and Shi, Shuming},
  year = {2024},
  month = jan,
  number = {arXiv:2401.10491},
  eprint = {2401.10491},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of LLMs can improve the performance of the target model across a range of capabilities such as reasoning, commonsense, and code generation. Our code, model weights, and data are public at {\textbackslash}url\{https://github.com/fanqiwan/FuseLLM\}.},
  archiveprefix = {arxiv}
}

@misc{weiFinetunedLanguageModels2021,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2021},
  month = sep,
  number = {arXiv:2109.01652},
  eprint = {2109.01652},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  archiveprefix = {arxiv}
}

@misc{weiMultiPartyChatConversational2023,
  title = {Multi-{{Party Chat}}: {{Conversational Agents}} in {{Group Settings}} with {{Humans}} and {{Models}}},
  shorttitle = {Multi-{{Party Chat}}},
  author = {Wei, Jimmy and Shuster, Kurt and Szlam, Arthur and Weston, Jason and Urbanek, Jack and Komeili, Mojtaba},
  year = {2023},
  month = jun,
  number = {arXiv:2304.13835},
  eprint = {2304.13835},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.13835},
  abstract = {Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.},
  archiveprefix = {arxiv}
}

@misc{welleckGeneratingSequencesLearning2022,
  title = {Generating {{Sequences}} by {{Learning}} to {{Self-Correct}}},
  author = {Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
  year = {2022},
  month = oct,
  number = {arXiv:2211.00053},
  eprint = {2211.00053},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.00053},
  abstract = {Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.},
  archiveprefix = {arxiv}
}

@misc{wenEmpoweringLLMUse2023,
  title = {Empowering {{LLM}} to Use {{Smartphone}} for {{Intelligent Task Automation}}},
  author = {Wen, Hao and Li, Yuanchun and Liu, Guohong and Zhao, Shanhui and Yu, Tao and Li, Toby Jia-Jun and Jiang, Shiqi and Liu, Yunhao and Zhang, Yaqin and Liu, Yunxin},
  year = {2023},
  month = aug,
  number = {arXiv:2308.15272},
  eprint = {2308.15272},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or end-users. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system that can handle arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9\%, and complete tasks with a success rate of 71.3\%, outperforming the GPT-4-powered baselines by 36.4\% and 39.7\%. The demo, benchmark suites, and source code of AutoDroid will be released at https://autodroid-sys.github.io/.},
  archiveprefix = {arxiv}
}

@inproceedings{wengLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Better Reasoners}} with {{Self-Verification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Weng, Yixuan and Zhu, Minjun and Xia, Fei and Li, Bin and He, Shizhu and Liu, Shengping and Sun, Bin and Liu, Kang and Zhao, Jun},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {2550--2575},
  publisher = {{Association for Computational Linguistics}},
  address = {{Singapore}},
  doi = {10.18653/v1/2023.findings-emnlp.167},
  abstract = {Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.}
}

@misc{wengLLMPoweredAutonomous2023,
  title = {{{LLM Powered Autonomous Agents}}},
  author = {Weng, Lilian},
  year = {2023},
  month = jun,
  abstract = {Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components:},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2023-06-23-agent/},
  langid = {english}
}

@misc{wengPromptEngineering2023,
  title = {Prompt {{Engineering}}},
  author = {Weng, Lilian},
  year = {2023},
  month = mar,
  abstract = {Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics. This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/},
  langid = {english}
}

@misc{wengWhatAreDiffusion2021,
  title = {What Are {{Diffusion Models}}?},
  author = {Weng, Lilian},
  year = {2021},
  month = jul,
  abstract = {[Updated on 2021-09-19: Highly recommend this blog post on score-based generative modeling by Yang Song (author of several key papers in the references)]. [Updated on 2022-08-27: Added classifier-free guidance, GLIDE, unCLIP and Imagen. [Updated on 2022-08-31: Added latent diffusion model. So far, I've written about three types of generative models, GAN, VAE, and Flow-based models. They have shown great success in generating high-quality samples, but each has some limitations of its own.},
  chapter = {posts},
  howpublished = {https://lilianweng.github.io/posts/2021-07-11-diffusion-models/},
  langid = {english}
}

@techreport{WuChengShiZhiHuiAnJianDeXiangGuanJiChuLiLunHeShiJueFenXiJiShu,
  title = {},
  author = {, }
}

@techreport{WuDongTaiChangJingDuoLiDuJiaoHuShenJingWangLuoBiaoShiYuJianMoFangFa,
  title = {},
  author = {, }
}

@misc{wuGuidedVisualSearch2023,
  title = {V*: {{Guided Visual Search}} as a {{Core Mechanism}} in {{Multimodal LLMs}}},
  shorttitle = {V*},
  author = {Wu, Penghao and Xie, Saining},
  year = {2023},
  month = dec,
  number = {arXiv:2312.14135},
  eprint = {2312.14135},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {When we look around and perform complex tasks, how we see and selectively process what we see is crucial. However, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images. To address this, we introduce V*, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying. When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise targeting of specific visual elements. This integration results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL). We further create V*Bench, a benchmark specifically designed to evaluate MLLMs in their ability to process high-resolution images and focus on visual details. Our study highlights the necessity of incorporating visual search capabilities into multimodal systems. The code is available https://github.com/penghao-wu/vstar.},
  archiveprefix = {arxiv}
}

@techreport{WuJiYuXiShuBiaoDaDeTuXiangYuYiLiJieJiZhiYanJiu,
  title = {},
  author = {, }
}

@misc{wuMultimodalDatasetDistillation2023,
  title = {Multimodal {{Dataset Distillation}} for {{Image-Text Retrieval}}},
  author = {Wu, Xindi and Deng, Zhiwei and Russakovsky, Olga},
  year = {2023},
  month = aug,
  number = {arXiv:2308.07545},
  eprint = {2308.07545},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.07545},
  abstract = {Dataset distillation methods offer the promise of reducing a large-scale dataset down to a significantly smaller set of (potentially synthetic) training examples, which preserve sufficient information for training a new model from scratch. So far dataset distillation methods have been developed for image classification. However, with the rise in capabilities of vision-language models, and especially given the scale of datasets necessary to train these models, the time is ripe to expand dataset distillation methods beyond image classification. In this work, we take the first steps towards this goal by expanding on the idea of trajectory matching to create a distillation method for vision-language datasets. The key challenge is that vision-language datasets do not have a set of discrete classes. To overcome this, our proposed multimodal dataset distillation method jointly distill the images and their corresponding language descriptions in a contrastive formulation. Since there are no existing baselines, we compare our approach to three coreset selection methods (strategic subsampling of the training dataset), which we adapt to the vision-language setting. We demonstrate significant improvements on the challenging Flickr30K and COCO retrieval benchmark: the best coreset selection method which selects 1000 image-text pairs for training is able to achieve only 5.6\% image-to-text retrieval accuracy (recall@1); in contrast, our dataset distillation approach almost doubles that with just 100 (an order of magnitude fewer) training pairs.},
  archiveprefix = {arxiv}
}

@misc{wuNExTGPTAnytoAnyMultimodal2023,
  title = {{{NExT-GPT}}: {{Any-to-Any Multimodal LLM}}},
  shorttitle = {{{NExT-GPT}}},
  author = {Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  year = {2023},
  month = sep,
  number = {arXiv:2309.05519},
  eprint = {2309.05519},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1\%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/},
  archiveprefix = {arxiv}
}

@misc{wuPlanEliminateTrack2023,
  title = {Plan, {{Eliminate}}, and {{Track}} -- {{Language Models}} Are {{Good Teachers}} for {{Embodied Agents}}},
  author = {Wu, Yue and Min, So Yeon and Bisk, Yonatan and Salakhutdinov, Ruslan and Azaria, Amos and Li, Yuanzhi and Mitchell, Tom and Prabhumoye, Shrimai},
  year = {2023},
  month = may,
  number = {arXiv:2305.02412},
  eprint = {2305.02412},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.02412},
  abstract = {Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15\% improvement over SOTA for generalization to human goal specifications.},
  archiveprefix = {arxiv}
}

@misc{wuProposeandRefineTwoStageSet2022,
  title = {Propose-and-{{Refine}}: {{A Two-Stage Set Prediction Network}} for {{Nested Named Entity Recognition}}},
  shorttitle = {Propose-and-{{Refine}}},
  author = {Wu, Shuhui and Shen, Yongliang and Tan, Zeqi and Lu, Weiming},
  year = {2022},
  month = sep,
  number = {arXiv:2204.12732},
  eprint = {2204.12732},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Nested named entity recognition (nested NER) is a fundamental task in natural language processing. Various span-based methods have been proposed to detect nested entities with span representations. However, span-based methods do not consider the relationship between a span and other entities or phrases, which is helpful in the NER task. Besides, span-based methods have trouble predicting long entities due to limited span enumeration length. To mitigate these issues, we present the Propose-andRefine Network (PnRNet), a two-stage set prediction network for nested NER. In the propose stage, we use a span-based predictor to generate some coarse entity predictions as entity proposals. In the refine stage, proposals interact with each other, and richer contextual information is incorporated into the proposal representations. The refined proposal representations are used to re-predict entity boundaries and classes. In this way, errors in coarse proposals can be eliminated, and the boundary prediction is no longer constrained by the span enumeration length limitation. Additionally, we build multi-scale sentence representations, which better model the hierarchical structure of sentences and provide richer contextual information than tokenlevel representations. Experiments show that PnRNet achieves state-of-the-art performance on four nested NER datasets and one flat NER dataset.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{wuSmartPlayBenchmarkLLMs2023,
  title = {{{SmartPlay}} : {{A Benchmark}} for {{LLMs}} as {{Intelligent Agents}}},
  shorttitle = {{{SmartPlay}}},
  author = {Wu, Yue and Tang, Xuan and Mitchell, Tom M. and Li, Yuanzhi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01557},
  eprint = {2310.01557},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/microsoft/SmartPlay},
  archiveprefix = {arxiv}
}

@misc{wuTextitGuidedVisual2023,
  title = {\${\textbackslash}textit\{\vphantom\}{{V}}\vphantom\{\}\^*\$: {{Guided Visual Search}} as a {{Core Mechanism}} in {{Multimodal LLMs}}},
  shorttitle = {\${\textbackslash}textit\{\vphantom\}{{V}}\vphantom\{\}\^*\$},
  author = {Wu, Penghao and Xie, Saining},
  year = {2023},
  month = dec,
  number = {arXiv:2312.14135},
  eprint = {2312.14135},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.14135},
  abstract = {When we look around and perform complex tasks, how we see and selectively process what we see is crucial. However, the lack of this visual search mechanism in current multimodal LLMs (MLLMs) hinders their ability to focus on important visual details, especially when handling high-resolution and visually crowded images. To address this, we introduce \${\textbackslash}textit\{V\}\^*\$, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying. When combined with an MLLM, this mechanism enhances collaborative reasoning, contextual understanding, and precise targeting of specific visual elements. This integration results in a new MLLM meta-architecture, named \${\textbackslash}textbf\{S\}\$how, s\${\textbackslash}textbf\{EA\}\$rch, and Tel\${\textbackslash}textbf\{L\}\$ (SEAL). We further create \${\textbackslash}textit\{V\}\^*\$Bench, a benchmark specifically designed to evaluate MLLMs in their ability to process high-resolution images and focus on visual details. Our study highlights the necessity of incorporating visual search capabilities into multimodal systems. The code is available https://github.com/penghao-wu/vstar.},
  archiveprefix = {arxiv}
}

@techreport{WuZiRanKeXueJiJinXieZuoZhiDao,
  title = {},
  author = {, }
}

@misc{xiaChainLoRAEfficient2024,
  title = {Chain of {{LoRA}}: {{Efficient Fine-tuning}} of {{Language Models}} via {{Residual Learning}}},
  shorttitle = {Chain of {{LoRA}}},
  author = {Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  year = {2024},
  month = jan,
  abstract = {Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks. We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as empirical results to validate the effectiveness of our algorithm. Across various models (OPT and llama-2) and seven benchmarking tasks, we demonstrate that COLA can consistently outperform LoRA without additional computational or memory costs.},
  langid = {english}
}

@misc{xiangLanguageModelsMeet2023,
  title = {Language {{Models Meet World Models}}: {{Embodied Experiences Enhance Language Models}}},
  shorttitle = {Language {{Models Meet World Models}}},
  author = {Xiang, Jiannan and Tao, Tianhua and Gu, Yi and Shu, Tianmin and Wang, Zirui and Yang, Zichao and Hu, Zhiting},
  year = {2023},
  month = oct,
  number = {arXiv:2305.10626},
  eprint = {2305.10626},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28\% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).},
  archiveprefix = {arxiv}
}

@misc{xiaoEfficientStreamingLanguage2023,
  title = {Efficient {{Streaming Language Models}} with {{Attention Sinks}}},
  author = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  year = {2023},
  month = sep,
  number = {arXiv:2309.17453},
  eprint = {2309.17453},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.17453},
  abstract = {Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.},
  archiveprefix = {arxiv}
}

@misc{xiaShearedLLaMAAccelerating2023,
  title = {Sheared {{LLaMA}}: {{Accelerating Language Model Pre-training}} via {{Structured Pruning}}},
  shorttitle = {Sheared {{LLaMA}}},
  author = {Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06694},
  eprint = {2310.06694},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.06694},
  abstract = {The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.},
  archiveprefix = {arxiv}
}

@misc{xiaUnlockingEfficiencyLarge2024,
  title = {Unlocking {{Efficiency}} in {{Large Language Model Inference}}: {{A Comprehensive Survey}} of {{Speculative Decoding}}},
  shorttitle = {Unlocking {{Efficiency}} in {{Large Language Model Inference}}},
  author = {Xia, Heming and Yang, Zhe and Dong, Qingxiu and Wang, Peiyi and Li, Yongqi and Ge, Tao and Liu, Tianyu and Li, Wenjie and Sui, Zhifang},
  year = {2024},
  month = jan,
  number = {arXiv:2401.07851},
  eprint = {2401.07851},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.07851},
  abstract = {To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first efficiently drafts several future tokens and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, including current leading techniques, the challenges faced, and potential future directions in this field. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.},
  archiveprefix = {arxiv}
}

@misc{xieDataSelectionLanguage2023,
  title = {Data {{Selection}} for {{Language Models}} via {{Importance Resampling}}},
  author = {Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy},
  year = {2023},
  month = nov,
  number = {arXiv:2302.03169},
  eprint = {2302.03169},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.03169},
  abstract = {Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given unlabeled target samples. Due to the scale and dimensionality of the raw text data, existing methods use simple heuristics or require human experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. We instantiate the DSIR framework with hashed n-gram features for efficiency, enabling the selection of 100M documents from the full Pile dataset in 4.5 hours. To measure whether hashed n-gram features preserve the aspects of the data that are relevant to the target, we define KL reduction, a data metric that measures the proximity between the selected pretraining data and the target on some feature space. Across 8 data selection methods (including expert selection), KL reduction on hashed n-gram features highly correlates with average downstream accuracy (r=0.82). When selecting data for continued pretraining on a specific domain, DSIR performs comparably to expert curation across 8 target distributions. When pretraining general-domain models (target is Wikipedia and books), DSIR improves over random selection and heuristic filtering baselines by 2-2.5\% on the GLUE benchmark. Code is available at https://github.com/p-lambda/dsir.},
  archiveprefix = {arxiv}
}

@misc{xieSelfEvaluationGuidedBeam2023,
  title = {Self-{{Evaluation Guided Beam Search}} for {{Reasoning}}},
  author = {Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, Xu and Kan, Min-Yen and He, Junxian and Xie, Qizhe},
  year = {2023},
  month = oct,
  number = {arXiv:2305.00633},
  eprint = {2305.00633},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.00633},
  abstract = {Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by \$6.34{\textbackslash}\%\$, \$9.56{\textbackslash}\%\$, and \$5.46{\textbackslash}\%\$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.},
  archiveprefix = {arxiv}
}

@misc{xieSelfEvaluationGuidedBeam2023a,
  title = {Self-{{Evaluation Guided Beam Search}} for {{Reasoning}}},
  author = {Xie, Yuxi and Kawaguchi, Kenji and Zhao, Yiran and Zhao, Xu and Kan, Min-Yen and He, Junxian and Xie, Qizhe},
  year = {2023},
  month = oct,
  number = {arXiv:2305.00633},
  eprint = {2305.00633},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.00633},
  abstract = {Breaking down a problem into intermediate steps has demonstrated impressive performance in Large Language Model (LLM) reasoning. However, the growth of the reasoning chain introduces uncertainty and error accumulation, making it challenging to elicit accurate final results. To tackle this challenge of uncertainty in multi-step reasoning, we introduce a stepwise self-evaluation mechanism to guide and calibrate the reasoning process of LLMs. We propose a decoding algorithm integrating the self-evaluation guidance via stochastic beam search. The self-evaluation guidance serves as a better-calibrated automatic criterion, facilitating an efficient search in the reasoning space and resulting in superior prediction quality. Stochastic beam search balances exploitation and exploration of the search space with temperature-controlled randomness. Our approach surpasses the corresponding Codex-backboned baselines in few-shot accuracy by \$6.34{\textbackslash}\%\$, \$9.56{\textbackslash}\%\$, and \$5.46{\textbackslash}\%\$ on the GSM8K, AQuA, and StrategyQA benchmarks, respectively. Experiment results with Llama-2 on arithmetic reasoning demonstrate the efficiency of our method in outperforming the baseline methods with comparable computational budgets. Further analysis in multi-step reasoning finds our self-evaluation guidance pinpoints logic failures and leads to higher consistency and robustness. Our code is publicly available at https://guideddecoding.github.io/.},
  archiveprefix = {arxiv}
}

@misc{xingSurveyVideoDiffusion2023,
  title = {A {{Survey}} on {{Video Diffusion Models}}},
  author = {Xing, Zhen and Feng, Qijun and Chen, Haoran and Dai, Qi and Hu, Han and Xu, Hang and Wu, Zuxuan and Jiang, Yu-Gang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.10647},
  eprint = {2310.10647},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The recent wave of AI-generated content (AIGC) has witnessed substantial success in computer vision, with the diffusion model playing a crucial role in this achievement. Due to their impressive generative capabilities, diffusion models are gradually superseding methods based on GANs and auto-regressive Transformers, demonstrating exceptional performance not only in image generation and editing, but also in the realm of video-related research. However, existing surveys mainly focus on diffusion models in the context of image generation, with few up-to-date reviews on their application in the video domain. To address this gap, this paper presents a comprehensive review of video diffusion models in the AIGC era. Specifically, we begin with a concise introduction to the fundamentals and evolution of diffusion models. Subsequently, we present an overview of research on diffusion models in the video domain, categorizing the work into three key areas: video generation, video editing, and other video understanding tasks. We conduct a thorough review of the literature in these three key areas, including further categorization and practical contributions in the field. Finally, we discuss the challenges faced by research in this domain and outline potential future developmental trends. A comprehensive list of video diffusion models studied in this survey is available at https://github.com/ChenHsing/Awesome-Video-Diffusion-Models.},
  archiveprefix = {arxiv}
}

@misc{xiongEffectiveLongContextScaling2023,
  title = {Effective {{Long-Context Scaling}} of {{Foundation Models}}},
  author = {Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and Khabsa, Madian and Fang, Han and Mehdad, Yashar and Narang, Sharan and Malik, Kshitiz and Fan, Angela and Bhosale, Shruti and Edunov, Sergey and Lewis, Mike and Wang, Sinong and Ma, Hao},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16039},
  eprint = {2309.16039},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.16039},
  abstract = {We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.},
  archiveprefix = {arxiv}
}

@misc{xiRisePotentialLarge2023,
  title = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}: {{A Survey}}},
  shorttitle = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}},
  author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Liu, Qin and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huan, Xuanjing and Gui, Tao},
  year = {2023},
  month = sep,
  number = {arXiv:2309.07864},
  eprint = {2309.07864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent AI agents since the mid-20th century. However, these efforts have mainly focused on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a sufficiently general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile and remarkable capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many research efforts have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for AI agents. Building upon this, we present a conceptual framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored to suit different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge when they form societies, and the insights they offer for human society. Finally, we discuss a range of key topics and open problems within the field.},
  archiveprefix = {arxiv}
}

@misc{xuLemurHarmonizingNatural2023,
  title = {Lemur: {{Harmonizing Natural Language}} and {{Code}} for {{Language Agents}}},
  shorttitle = {Lemur},
  author = {Xu, Yiheng and Su, Hongjin and Xing, Chen and Mi, Boyu and Liu, Qian and Shi, Weijia and Hui, Binyuan and Zhou, Fan and Liu, Yitao and Xie, Tianbao and Cheng, Zhoujun and Zhao, Siheng and Kong, Lingpeng and Wang, Bailin and Xiong, Caiming and Yu, Tao},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06830},
  eprint = {2310.06830},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. https://github.com/OpenLemur/Lemur},
  archiveprefix = {arxiv}
}

@misc{xuToolManipulationCapability2023,
  title = {On the {{Tool Manipulation Capability}} of {{Open-source Large Language Models}}},
  author = {Xu, Qiantong and Hong, Fenglu and Li, Bo and Hu, Changran and Chen, Zhengyu and Zhang, Jian},
  year = {2023},
  month = may,
  number = {arXiv:2305.16504},
  eprint = {2305.16504},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.16504},
  abstract = {Recent studies on software tool manipulation with large language models (LLMs) mostly rely on closed model APIs. The industrial adoption of these models is substantially constrained due to the security and robustness risks in exposing information to closed LLM API services. In this paper, we ask can we enhance open-source LLMs to be competitive to leading closed LLM APIs in tool manipulation, with practical amount of human supervision. By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures. These insights motivate us to revisit classical methods in LLM literature, and demonstrate that we can adapt them as model alignment with programmatic data generation, system prompts and in-context demonstration retrievers to enhance open-source LLMs for tool manipulation. To evaluate these techniques, we create the ToolBench, a tool manipulation benchmark consisting of diverse software tools for real-world tasks. We demonstrate that our techniques can boost leading open-source LLMs by up to 90\% success rate, showing capabilities competitive to OpenAI GPT-4 in 4 out of 8 ToolBench tasks. We show that such enhancement typically requires about one developer day to curate data for each tool, rendering a recipe with practical amount of human supervision.},
  archiveprefix = {arxiv}
}

@misc{xuWizardLMEmpoweringLarge2023,
  title = {{{WizardLM}}: {{Empowering Large Language Models}} to {{Follow Complex Instructions}}},
  shorttitle = {{{WizardLM}}},
  author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  year = {2023},
  month = jun,
  number = {arXiv:2304.12244},
  eprint = {2304.12244},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12244},
  abstract = {Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90{\textbackslash}\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs. Our code and data are public at https://github.com/nlpxucan/WizardLM},
  archiveprefix = {arxiv}
}

@misc{yanCorrectiveRetrievalAugmented2024,
  title = {Corrective {{Retrieval Augmented Generation}}},
  author = {Yan, Shi-Qi and Gu, Jia-Chen and Zhu, Yun and Ling, Zhen-Hua},
  year = {2024},
  month = jan,
  number = {arXiv:2401.15884},
  eprint = {2401.15884},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{yangAppAgentMultimodalAgents2023,
  title = {{{AppAgent}}: {{Multimodal Agents}} as {{Smartphone Users}}},
  shorttitle = {{{AppAgent}}},
  author = {Yang, Zhao and Liu, Jiaxuan and Han, Yucheng and Chen, Xin and Huang, Zebiao and Fu, Bin and Yu, Gang},
  year = {2023},
  month = dec,
  number = {arXiv:2312.13771},
  eprint = {2312.13771},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent's functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent's proficiency in handling a diverse array of high-level tasks.},
  archiveprefix = {arxiv}
}

@misc{yangDawnLMMsPreliminary2023,
  title = {The {{Dawn}} of {{LMMs}}: {{Preliminary Explorations}} with {{GPT-4V}}(Ision)},
  shorttitle = {The {{Dawn}} of {{LMMs}}},
  author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  year = {2023},
  month = oct,
  number = {arXiv:2309.17421},
  eprint = {2309.17421},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.17421},
  abstract = {Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf},
  archiveprefix = {arxiv}
}

@misc{yangGatedLinearAttention2023,
  title = {Gated {{Linear Attention Transformers}} with {{Hardware-Efficient Training}}},
  author = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  year = {2023},
  month = dec,
  number = {arXiv:2312.06635},
  eprint = {2312.06635},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear (with respect to output length) inference complexity. Recent works such as RetNet (Sun et al., 2023) and TransNormerLLM (Qin et al., 2023a) observe that adding a global decay term to the additive RNN update rule greatly improves performance, sometimes outperforming standard Transformers with softmax attention when trained at scale. In this work we show that adding a data-dependent gating mechanism further improves performance. We derive a parallel form of this gated linear attention layer that enables efficient training. However, a straightforward, numerically stable implementation of this parallel form requires generalized matrix multiplications in log-space for numerical stability, and thus cannot take advantage of tensor cores on modern GPUs which are optimized for standard matrix multiplications. We develop a hardware-efficient version of the parallel form that can still make use of tensor cores through block-parallel computations over sequence chunks. Experiments on moderate-scale language modeling (340M-parameter models trained on 15B tokens, 1.3B-parameter models trained on 100B tokens) show that gated linear attention (GLA) Transformers perform competitively against a strong LLaMA-architecture Transformer baseline (Touvron et al., 2023) as well as Mamba (Gu \& Dao, 2023), a recently introduced state-space model with a data-dependent state transition mechanism. For training speed, our Triton-based implementation performs comparably to CUDA-optimized FlashAttention-2 (Dao, 2023) under the regular 2048 training length setting, while outperforming FlashAttention-2 when training on longer sequences beyond 4096.},
  archiveprefix = {arxiv}
}

@misc{yangGPT4ToolsTeachingLarge2023,
  title = {{{GPT4Tools}}: {{Teaching Large Language Model}} to {{Use Tools}} via {{Self-instruction}}},
  shorttitle = {{{GPT4Tools}}},
  author = {Yang, Rui and Song, Lin and Li, Yanwei and Zhao, Sijie and Ge, Yixiao and Li, Xiu and Shan, Ying},
  year = {2023},
  month = may,
  number = {arXiv:2305.18752},
  eprint = {2305.18752},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18752},
  abstract = {This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have shown great potential for tool usage through sophisticated prompt engineering. Nevertheless, these models typically rely on prohibitive computational costs and publicly inaccessible data. To address these challenges, we propose the GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools. It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts. By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation. Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways. Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools. The code and demo are available at https://github.com/StevenGrove/GPT4Tools.},
  archiveprefix = {arxiv}
}

@misc{yangIfLLMWizard2024,
  title = {If {{LLM Is}} the {{Wizard}}, {{Then Code Is}} the {{Wand}}: {{A Survey}} on {{How Code Empowers Large Language Models}} to {{Serve}} as {{Intelligent Agents}}},
  shorttitle = {If {{LLM Is}} the {{Wizard}}, {{Then Code Is}} the {{Wand}}},
  author = {Yang, Ke and Liu, Jiateng and Wu, John and Yang, Chaoqi and Fung, Yi R. and Li, Sha and Huang, Zixuan and Cao, Xu and Wang, Xingyao and Wang, Yiquan and Ji, Heng and Zhai, Chengxiang},
  year = {2024},
  month = jan,
  number = {arXiv:2401.00812},
  eprint = {2401.00812},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs' training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.},
  archiveprefix = {arxiv}
}

@misc{yangInterCodeStandardizingBenchmarking2023,
  title = {{{InterCode}}: {{Standardizing}} and {{Benchmarking Interactive Coding}} with {{Execution Feedback}}},
  shorttitle = {{{InterCode}}},
  author = {Yang, John and Prabhakar, Akshara and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = jun,
  number = {arXiv:2306.14898},
  eprint = {2306.14898},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.14898},
  abstract = {Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create two interactive code environments with Bash and SQL as action spaces, leveraging data from the static Spider and NL2Bash datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan \& Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to incorporate new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. Project site with code and data: https://intercode-benchmark.github.io},
  archiveprefix = {arxiv}
}

@misc{yangMultiCandidateSpeculativeDecoding2024,
  title = {Multi-{{Candidate Speculative Decoding}}},
  author = {Yang, Sen and Huang, Shujian and Dai, Xinyu and Chen, Jiajun},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06706},
  eprint = {2401.06706},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.},
  archiveprefix = {arxiv}
}

@misc{yangSetofMarkPromptingUnleashes2023,
  title = {Set-of-{{Mark Prompting Unleashes Extraordinary Visual Grounding}} in {{GPT-4V}}},
  author = {Yang, Jianwei and Zhang, Hao and Li, Feng and Zou, Xueyan and Li, Chunyuan and Gao, Jianfeng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11441},
  eprint = {2310.11441},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting.},
  archiveprefix = {arxiv}
}

@misc{yanLARPLanguageAgentRole2023,
  title = {{{LARP}}: {{Language-Agent Role Play}} for {{Open-World Games}}},
  shorttitle = {{{LARP}}},
  author = {Yan, Ming and Li, Ruihao and Zhang, Hao and Wang, Hao and Yang, Zhilan and Yan, Ji},
  year = {2023},
  month = dec,
  number = {arXiv:2312.17653},
  eprint = {2312.17653},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Language agents have shown impressive problem-solving skills within defined settings and brief timelines. Yet, with the ever-evolving complexities of open-world simulations, there's a pressing need for agents that can flexibly adapt to complex environments and consistently maintain a long-term memory to ensure coherent actions. To bridge the gap between language agents and open-world games, we introduce Language Agent for Role-Playing (LARP), which includes a cognitive architecture that encompasses memory processing and a decision-making assistant, an environment interaction module with a feedback-driven learnable action space, and a postprocessing method that promotes the alignment of various personalities. The LARP framework refines interactions between users and agents, predefined with unique backgrounds and personalities, ultimately enhancing the gaming experience in open-world contexts. Furthermore, it highlights the diverse uses of language models in a range of areas such as entertainment, education, and various simulation scenarios. The project page is released at https://miao-ai-lab.github.io/LARP/.},
  archiveprefix = {arxiv}
}

@misc{yaoEditingLargeLanguage2023,
  title = {Editing {{Large Language Models}}: {{Problems}}, {{Methods}}, and {{Opportunities}}},
  shorttitle = {Editing {{Large Language Models}}},
  author = {Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  year = {2023},
  month = oct,
  number = {arXiv:2305.13172},
  eprint = {2305.13172},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.13172},
  abstract = {Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at https://github.com/zjunlp/EasyEdit.},
  archiveprefix = {arxiv}
}

@misc{yaoInstructionsIntrinsicHuman2023,
  title = {From {{Instructions}} to {{Intrinsic Human Values}} -- {{A Survey}} of {{Alignment Goals}} for {{Big Models}}},
  author = {Yao, Jing and Yi, Xiaoyuan and Wang, Xiting and Wang, Jindong and Xie, Xing},
  year = {2023},
  month = sep,
  number = {arXiv:2308.12014},
  eprint = {2308.12014},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.12014},
  abstract = {Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.},
  archiveprefix = {arxiv}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = {2023},
  month = may,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, ``Tree of Thoughts'' (ToT), which generalizes over the popular ``Chain of Thought'' approach to prompting language models, and enables exploration over coherent units of text (``thoughts'') that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{yasunagaLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Analogical Reasoners}}},
  author = {Yasunaga, Michihiro and Chen, Xinyun and Li, Yujia and Pasupat, Panupong and Leskovec, Jure and Liang, Percy and Chi, Ed H. and Zhou, Denny},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01714},
  eprint = {2310.01714},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.01714},
  abstract = {Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, Analogical Prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.},
  archiveprefix = {arxiv}
}

@misc{yeGeneFaceGeneralizedStable2023,
  title = {{{GeneFace}}++: {{Generalized}} and {{Stable Real-Time Audio-Driven 3D Talking Face Generation}}},
  shorttitle = {{{GeneFace}}++},
  author = {Ye, Zhenhui and He, Jinzheng and Jiang, Ziyue and Huang, Rongjie and Huang, Jiawei and Liu, Jinglin and Ren, Yi and Yin, Xiang and Ma, Zejun and Zhao, Zhou},
  year = {2023},
  month = may,
  number = {arXiv:2305.00787},
  eprint = {2305.00787},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Generating talking person portraits with arbitrary speech audio is a crucial problem in the field of digital human and metaverse. A modern talking face generation method is expected to achieve the goals of generalized audio-lip synchronization, good video quality, and high system efficiency. Recently, neural radiance field (NeRF) has become a popular rendering technique in this field since it could achieve high-fidelity and 3D-consistent talking face generation with a few-minute-long training video. However, there still exist several challenges for NeRF-based methods: 1) as for the lip synchronization, it is hard to generate a long facial motion sequence of high temporal consistency and audio-lip accuracy; 2) as for the video quality, due to the limited data used to train the renderer, it is vulnerable to out-of-domain input condition and produce bad rendering results occasionally; 3) as for the system efficiency, the slow training and inference speed of the vanilla NeRF severely obstruct its usage in real-world applications. In this paper, we propose GeneFace++ to handle these challenges by 1) utilizing the pitch contour as an auxiliary feature and introducing a temporal loss in the facial motion prediction process; 2) proposing a landmark locally linear embedding method to regulate the outliers in the predicted motion sequence to avoid robustness issues; 3) designing a computationally efficient NeRF-based motion-to-video renderer to achieves fast training and real-time inference. With these settings, GeneFace++ becomes the first NeRF-based method that achieves stable and real-time talking face generation with generalized audio-lip synchronization. Extensive experiments show that our method outperforms state-of-the-art baselines in terms of subjective and objective evaluation. Video samples are available at https://genefaceplusplus.github.io .},
  archiveprefix = {arxiv}
}

@misc{yeIPAdapterTextCompatible2023,
  title = {{{IP-Adapter}}: {{Text Compatible Image Prompt Adapter}} for {{Text-to-Image Diffusion Models}}},
  shorttitle = {{{IP-Adapter}}},
  author = {Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei},
  year = {2023},
  month = aug,
  number = {arXiv:2308.06721},
  eprint = {2308.06721},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.06721},
  abstract = {Recent years have witnessed the strong power of large text-to-image diffusion models for the impressive generative capability to create high-fidelity images. However, it is very tricky to generate desired images using only text prompt as it often involves complex prompt engineering. An alternative to text prompt is image prompt, as the saying goes: "an image is worth a thousand words". Although existing methods of direct fine-tuning from pretrained models are effective, they require large computing resources and are not compatible with other base models, text prompt, and structural controls. In this paper, we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models. The key design of our IP-Adapter is decoupled cross-attention mechanism that separates cross-attention layers for text features and image features. Despite the simplicity of our method, an IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fully fine-tuned image prompt model. As we freeze the pretrained diffusion model, the proposed IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. With the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation. The project page is available at {\textbackslash}url\{https://ip-adapter.github.io\}.},
  archiveprefix = {arxiv}
}

@misc{yeMPLUGOwlModularizationEmpowers2023,
  title = {{{mPLUG-Owl}}: {{Modularization Empowers Large Language Models}} with {{Multimodality}}},
  shorttitle = {{{mPLUG-Owl}}},
  author = {Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and Li, Chenliang and Xu, Yuanhong and Chen, Hehong and Tian, Junfeng and Qi, Qian and Zhang, Ji and Huang, Fei},
  year = {2023},
  month = apr,
  number = {arXiv:2304.14178},
  eprint = {2304.14178},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.14178},
  abstract = {Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.},
  archiveprefix = {arxiv}
}

@misc{yeProAgentRoboticProcess2023,
  title = {{{ProAgent}}: {{From Robotic Process Automation}} to {{Agentic Process Automation}}},
  shorttitle = {{{ProAgent}}},
  author = {Ye, Yining and Cong, Xin and Tian, Shizuo and Cao, Jiannan and Wang, Hao and Qin, Yujia and Lu, Yaxi and Yu, Heyang and Wang, Huadong and Lin, Yankai and Liu, Zhiyuan and Sun, Maosong},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10751},
  eprint = {2311.10751},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.10751},
  abstract = {From ancient water wheels to robotic process automation (RPA), automation technology has evolved throughout history to liberate human beings from arduous tasks. Yet, RPA struggles with tasks needing human-like intelligence, especially in elaborate design of workflow construction and dynamic decision-making in workflow execution. As Large Language Models (LLMs) have emerged human-like intelligence, this paper introduces Agentic Process Automation (APA), a groundbreaking automation paradigm using LLM-based agents for advanced automation by offloading the human labor to agents associated with construction and execution. We then instantiate ProAgent, an LLM-based agent designed to craft workflows from human instructions and make intricate decisions by coordinating specialized agents. Empirical experiments are conducted to detail its construction and execution procedure of workflow, showcasing the feasibility of APA, unveiling the possibility of a new paradigm of automation driven by agents. Our code is public at https://github.com/OpenBMB/ProAgent.},
  archiveprefix = {arxiv}
}

@misc{yeToolEyesFineGrainedEvaluation2024,
  title = {{{ToolEyes}}: {{Fine-Grained Evaluation}} for {{Tool Learning Capabilities}} of {{Large Language Models}} in {{Real-world Scenarios}}},
  shorttitle = {{{ToolEyes}}},
  author = {Ye, Junjie and Li, Guanyu and Gao, Songyang and Huang, Caishuang and Wu, Yilong and Li, Sixian and Fan, Xiaoran and Dou, Shihan and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  year = {2024},
  month = jan,
  number = {arXiv:2401.00741},
  eprint = {2401.00741},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Existing evaluations of tool learning primarily focus on validating the alignment of selected tools for large language models (LLMs) with expected outcomes. However, these approaches rely on a limited set of scenarios where answers can be pre-determined, diverging from genuine needs. Furthermore, a sole emphasis on outcomes disregards the intricate capabilities essential for LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a fine-grained system tailored for the evaluation of the LLMs' tool learning capabilities in authentic scenarios. The system meticulously examines seven real-world scenarios, analyzing five dimensions crucial to LLMs in tool learning: format alignment, intent comprehension, behavior planning, tool selection, and answer organization. Additionally, ToolEyes incorporates a tool library boasting approximately 600 tools, serving as an intermediary between LLMs and the physical world. Evaluations involving ten LLMs across three categories reveal a preference for specific scenarios and limited cognitive abilities in tool learning. Intriguingly, expanding the model size even exacerbates the hindrance to tool learning. These findings offer instructive insights aimed at advancing the field of tool learning. The data is available att https://github.com/Junjie-Ye/ToolEyes.git.},
  archiveprefix = {arxiv}
}

@misc{yinLumosLearningAgents2023,
  title = {Lumos: {{Learning Agents}} with {{Unified Data}}, {{Modular Design}}, and {{Open-Source LLMs}}},
  shorttitle = {Lumos},
  author = {Yin, Da and Brahman, Faeze and Ravichander, Abhilasha and Chandu, Khyathi and Chang, Kai-Wei and Choi, Yejin and Lin, Bill Yuchen},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05657},
  eprint = {2311.05657},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce Lumos, a novel framework for training language agents that employs a unified data format and a modular architecture based on open-source large language models (LLMs). Lumos consists of three distinct modules: planning, grounding, and execution. The planning module breaks down a task into a series of high-level, tool-agnostic subgoals, which are then made specific by the grounding module through a set of low-level actions. These actions are subsequently executed by the execution module, utilizing a range of off-the-shelf tools and APIs. In order to train these modules effectively, high-quality annotations of subgoals and actions were collected and are made available for fine-tuning open-source LLMs for various tasks such as complex question answering, web tasks, and math problems. Leveraging this unified data and modular design, Lumos not only achieves comparable or superior performance to current, state-of-the-art agents, but also exhibits several key advantages: (1) Lumos surpasses GPT-4/3.5-based agents in complex question answering and web tasks, while equalling the performance of significantly larger LLM agents on math tasks; (2) Lumos outperforms open-source agents created through conventional training methods and those using chain-of-thoughts training; and (3) Lumos is capable of effectively generalizing to unseen interactive tasks, outperforming larger LLM-based agents and even exceeding performance of specialized agents.},
  archiveprefix = {arxiv}
}

@misc{yinWoodpeckerHallucinationCorrection2023,
  title = {Woodpecker: {{Hallucination Correction}} for {{Multimodal Large Language Models}}},
  shorttitle = {Woodpecker},
  author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Xu, Tong and Wang, Hao and Sui, Dianbo and Shen, Yunhang and Li, Ke and Sun, Xing and Chen, Enhong},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16045},
  eprint = {2310.16045},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.16045},
  abstract = {Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66\%/24.33\% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.},
  archiveprefix = {arxiv}
}

@misc{yooImprovingVisualPrompt2023,
  title = {Improving {{Visual Prompt Tuning}} for {{Self-supervised Vision Transformers}}},
  author = {Yoo, Seungryong and Kim, Eunji and Jung, Dahuin and Lee, Jungbeom and Yoon, Sungroh},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05067},
  eprint = {2306.05067},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Visual Prompt Tuning (VPT) is an effective tuning method for adapting pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra learnable tokens, known as prompts, which steer the frozen pretrained ViTs. Although VPT has demonstrated its applicability with supervised vision transformers, it often underperforms with self-supervised ones. Through empirical observations, we deduce that the effectiveness of VPT hinges largely on the ViT blocks with which the prompt tokens interact. Specifically, VPT shows improved performance on image classification tasks for MAE and MoCo v3 when the prompt tokens are inserted into later blocks rather than the first block. These observations suggest that there exists an optimal location of blocks for the insertion of prompt tokens. Unfortunately, identifying the optimal blocks for prompts within each self-supervised ViT for diverse future scenarios is a costly process. To mitigate this problem, we propose a simple yet effective method that learns a gate for each ViT block to adjust its intervention into the prompt tokens. With our method, prompt tokens are selectively influenced by blocks that require steering for task adaptation. Our method outperforms VPT variants in FGVC and VTAB image classification and ADE20K semantic segmentation. The code is available at https://github.com/ryongithub/GatedPromptTuning.},
  archiveprefix = {arxiv}
}

@misc{yuanCausalityawareConceptExtraction2023,
  title = {Causality-Aware {{Concept Extraction}} Based on {{Knowledge-guided Prompting}}},
  author = {Yuan, Siyu and Yang, Deqing and Liu, Jinxi and Tian, Shuyu and Liang, Jiaqing and Xiao, Yanghua and Xie, Rui},
  year = {2023},
  month = jun,
  number = {arXiv:2305.01876},
  eprint = {2305.01876},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt can effectively alleviate concept bias and improve the performance of PLM-based CE models.The code has been released on https://github.com/siyuyuan/KPCE.},
  archiveprefix = {arxiv}
}

@misc{yuanDistillingScriptKnowledge2023,
  title = {Distilling {{Script Knowledge}} from {{Large Language Models}} for {{Constrained Language Planning}}},
  author = {Yuan, Siyu and Chen, Jiangjie and Fu, Ziquan and Ge, Xuyang and Shah, Soham and Jankowski, Charles Robert and Xiao, Yanghua and Yang, Deqing},
  year = {2023},
  month = may,
  number = {arXiv:2305.05252},
  eprint = {2305.05252},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{yuanEASYTOOLEnhancingLLMbased2024,
  title = {{{EASYTOOL}}: {{Enhancing LLM-based Agents}} with {{Concise Tool Instruction}}},
  shorttitle = {{{EASYTOOL}}},
  author = {Yuan, Siyu and Song, Kaitao and Chen, Jiangjie and Tan, Xu and Shen, Yongliang and Kan, Ren and Li, Dongsheng and Yang, Deqing},
  year = {2024},
  month = jan,
  number = {arXiv:2401.06201},
  eprint = {2401.06201},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at {\textbackslash}url\{https://github.com/microsoft/JARVIS/\} in the future.},
  archiveprefix = {arxiv}
}

@misc{yuanPlan4MCSkillReinforcement2023,
  title = {{{Plan4MC}}: {{Skill Reinforcement Learning}} and {{Planning}} for {{Open-World Minecraft Tasks}}},
  shorttitle = {{{Plan4MC}}},
  author = {Yuan, Haoqi and Zhang, Chi and Wang, Hongcheng and Xie, Feiyang and Cai, Penglin and Dong, Hao and Lu, Zongqing},
  year = {2023},
  month = mar,
  number = {arXiv:2303.16563},
  eprint = {2303.16563},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.16563},
  abstract = {We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin. The project's website and code can be found at https://sites.google.com/view/plan4mc.},
  archiveprefix = {arxiv}
}

@misc{yuanRRHFRankResponses2023,
  title = {{{RRHF}}: {{Rank Responses}} to {{Align Language Models}} with {{Human Feedback}} without Tears},
  shorttitle = {{{RRHF}}},
  author = {Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  year = {2023},
  month = oct,
  number = {arXiv:2304.05302},
  eprint = {2304.05302},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.05302},
  abstract = {Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling. Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-n learner. Codes available at https://github.com/GanjinZero/RRHF.},
  archiveprefix = {arxiv}
}

@misc{yuanTaskLAMAProbingComplex2023,
  title = {{{TaskLAMA}}: {{Probing}} the {{Complex Task Understanding}} of {{Language Models}}},
  shorttitle = {{{TaskLAMA}}},
  author = {Yuan, Quan and Kazemi, Mehran and Xu, Xin and Noble, Isaac and Imbrasaite, Vaiva and Ramachandran, Deepak},
  year = {2023},
  month = aug,
  number = {arXiv:2308.15299},
  eprint = {2308.15299},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.15299},
  abstract = {Structured Complex Task Decomposition (SCTD) is the problem of breaking down a complex real-world task (such as planning a wedding) into a directed acyclic graph over individual steps that contribute to achieving the task, with edges specifying temporal dependencies between them. SCTD is an important component of assistive planning tools, and a challenge for commonsense reasoning systems. We probe how accurately SCTD can be done with the knowledge extracted from Large Language Models (LLMs). We introduce a high-quality human-annotated dataset for this problem and novel metrics to fairly assess performance of LLMs against several baselines. Our experiments reveal that LLMs are able to decompose complex tasks into individual steps effectively, with a relative improvement of 15\% to 280\% over the best baseline. We also propose a number of approaches to further improve their performance, with a relative improvement of 7\% to 37\% over the base model. However, we find that LLMs still struggle to predict pairwise temporal dependencies, which reveals a gap in their understanding of complex tasks.},
  archiveprefix = {arxiv}
}

@misc{yuanTinyGPTVEfficientMultimodal2023,
  title = {{{TinyGPT-V}}: {{Efficient Multimodal Large Language Model}} via {{Small Backbones}}},
  shorttitle = {{{TinyGPT-V}}},
  author = {Yuan, Zhengqing and Li, Zhaoxu and Sun, Lichao},
  year = {2023},
  month = dec,
  number = {arXiv:2312.16862},
  eprint = {2312.16862},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.16862},
  abstract = {In the era of advanced multimodel learning, multimodal large language models (MLLMs) such as GPT-4V have made remarkable strides towards bridging language and visual elements. However, the closed-source nature and considerable computational demand present notable challenges for universal usage and modifications. This is where open-source MLLMs like LLaVA and MiniGPT-4 come in, presenting groundbreaking achievements across tasks. Despite these accomplishments, computational efficiency remains an unresolved issue, as these models, like LLaVA-v1.5-13B, require substantial resources. Addressing these issues, we introduce TinyGPT-V, a new-wave model marrying impressive performance with commonplace computational capacity. It stands out by requiring merely a 24G GPU for training and an 8G GPU or CPU for inference. Built upon Phi-2, TinyGPT-V couples an effective language backbone with pre-trained vision modules from BLIP-2 or CLIP. TinyGPT-V's 2.8B parameters can undergo a unique quantisation process, suitable for local deployment and inference tasks on 8G various devices. Our work fosters further developments for designing cost-effective, efficient, and high-performing MLLMs, expanding their applicability in a broad array of real-world scenarios. Furthermore this paper proposed a new paradigm of Multimodal Large Language Model via small backbones. Our code and training weights are placed at: https://github.com/DLYuanGod/TinyGPT-V and https://huggingface.co/Tyrannosaurus/TinyGPT-V respectively.},
  archiveprefix = {arxiv}
}

@misc{yueLargeLanguageModel2023,
  title = {Large {{Language Model Cascades}} with {{Mixture}} of {{Thoughts Representations}} for {{Cost-efficient Reasoning}}},
  author = {Yue, Murong and Zhao, Jie and Zhang, Min and Du, Liang and Yao, Ziyu},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03094},
  eprint = {2310.03094},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.03094},
  abstract = {Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40\% of its cost.},
  archiveprefix = {arxiv}
}

@misc{yueMAmmoTHBuildingMath2023,
  title = {{{MAmmoTH}}: {{Building Math Generalist Models}} through {{Hybrid Instruction Tuning}}},
  shorttitle = {{{MAmmoTH}}},
  author = {Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  year = {2023},
  month = oct,
  number = {arXiv:2309.05653},
  eprint = {2309.05653},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.05653},
  abstract = {We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16\% and 32\%. Remarkably, our MAmmoTH-7B model reaches 33\% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23\%, and the MAmmoTH-34B model achieves 44\% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.},
  archiveprefix = {arxiv}
}

@misc{yuLanguageModelsAre2023,
  title = {Language {{Models}} Are {{Super Mario}}: {{Absorbing Abilities}} from {{Homologous Models}} as a {{Free Lunch}}},
  shorttitle = {Language {{Models}} Are {{Super Mario}}},
  author = {Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  year = {2023},
  month = nov,
  number = {arXiv:2311.03099},
  eprint = {2311.03099},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.03099},
  abstract = {In this paper, we uncover that Language Models (LMs), either encoder- or decoder-based, can obtain new capabilities by assimilating the parameters of homologous models without retraining or GPUs. Typically, new abilities of LMs can be imparted by Supervised Fine-Tuning (SFT), reflected in the disparity between fine-tuned and pre-trained parameters (i.e., delta parameters). We initially observe that by introducing a novel operation called DARE (Drop And REscale), most delta parameters can be directly set to zeros without affecting the capabilities of SFT LMs and larger models can tolerate a higher proportion of discarded parameters. Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE and subsequently merge them into a single model by parameter averaging. We conduct experiments on eight datasets from the GLUE benchmark with BERT and RoBERTa. We also merge WizardLM, WizardMath, and Code Alpaca based on Llama 2. Experimental results show that: (1) The delta parameter value ranges for SFT models are typically small, often within 0.005, and DARE can eliminate 99\% of them effortlessly. However, once the models are continuously pre-trained, the value ranges can grow to around 0.03, making DARE impractical. We have also tried to remove fine-tuned instead of delta parameters and find that a 10\% reduction can lead to drastically decreased performance (even to 0). This highlights that SFT merely stimulates the abilities via delta parameters rather than injecting new abilities into LMs; (2) DARE can merge multiple task-specific LMs into one LM with diverse abilities. For instance, the merger of WizardLM and WizardMath improves the GSM8K zero-shot accuracy of WizardLM from 2.2 to 66.3, retaining its instruction-following ability while surpassing WizardMath's original 64.2 performance. Codes are available at https://github.com/yule-BUAA/MergeLM.},
  archiveprefix = {arxiv}
}

@misc{yuTeachingLanguageModels2023,
  title = {Teaching {{Language Models}} to {{Self-Improve}} through {{Interactive Demonstrations}}},
  author = {Yu, Xiao and Peng, Baolin and Galley, Michel and Gao, Jianfeng and Yu, Zhou},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13522},
  eprint = {2310.13522},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more cost-effective and faster ones. To reduce this gap, we introduce TriPosT, a training algorithm that endows smaller models with such self-improvement ability, and show that our approach can improve a LLaMA-7b's performance on math and reasoning tasks by up to 7.13\%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on its own generations. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its own mistakes is crucial for small models to improve their performance.},
  archiveprefix = {arxiv}
}

@misc{zengAgentTuningEnablingGeneralized2023,
  title = {{{AgentTuning}}: {{Enabling Generalized Agent Abilities}} for {{LLMs}}},
  shorttitle = {{{AgentTuning}}},
  author = {Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
  year = {2023},
  month = oct,
  number = {arXiv:2310.12823},
  eprint = {2310.12823},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.12823},
  abstract = {Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs' agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning , serving open and powerful alternatives to commercial LLMs for agent tasks.},
  archiveprefix = {arxiv}
}

@misc{zengEvaluatingLargeLanguage2023,
  title = {Evaluating {{Large Language Models}} at {{Evaluating Instruction Following}}},
  author = {Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07641},
  eprint = {2310.07641},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07641},
  abstract = {As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these "LLM evaluators", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.},
  archiveprefix = {arxiv}
}

@misc{zhang_2HeavyHitterOracle2023,
  title = {H\$\_2\${{O}}: {{Heavy-Hitter Oracle}} for {{Efficient Generative Inference}} of {{Large Language Models}}},
  shorttitle = {H\$\_2\${{O}}},
  author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and Wang, Zhangyang and Chen, Beidi},
  year = {2023},
  month = jul,
  number = {arXiv:2306.14048},
  eprint = {2306.14048},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.14048},
  abstract = {Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H\$\_2\$). Through a comprehensive investigation, we find that (i) the emergence of H\$\_2\$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H\$\_2\$O), a KV cache eviction policy that dynamically retains a balance of recent and H\$\_2\$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H\$\_2\$O with 20\% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29\${\textbackslash}times\$, 29\${\textbackslash}times\$, and 3\${\textbackslash}times\$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9\${\textbackslash}times\$. The code is available at https://github.com/FMInference/H2O.},
  archiveprefix = {arxiv}
}

@misc{zhangAddingConditionalControl2023,
  title = {Adding {{Conditional Control}} to {{Text-to-Image Diffusion Models}}},
  author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  year = {2023},
  month = nov,
  number = {arXiv:2302.05543},
  eprint = {2302.05543},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.05543},
  abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({$<$}50k) and large ({$>$}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
  archiveprefix = {arxiv}
}

@misc{zhangAutomaticChainThought2022,
  title = {Automatic {{Chain}} of {{Thought Prompting}} in {{Large Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03493},
  eprint = {2210.03493},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{zhangBuildingCooperativeEmbodied2023,
  title = {Building {{Cooperative Embodied Agents Modularly}} with {{Large Language Models}}},
  author = {Zhang, Hongxin and Du, Weihua and Shan, Jiaming and Zhou, Qinhong and Du, Yilun and Tenenbaum, Joshua B. and Shu, Tianmin and Gan, Chuang},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02485},
  eprint = {2307.02485},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.02485},
  abstract = {Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.},
  archiveprefix = {arxiv}
}

@misc{zhangDataCopilotBridgingBillions2023,
  title = {Data-{{Copilot}}: {{Bridging Billions}} of {{Data}} and {{Humans}} with {{Autonomous Workflow}}},
  shorttitle = {Data-{{Copilot}}},
  author = {Zhang, Wenqi and Shen, Yongliang and Lu, Weiming and Zhuang, Yueting},
  year = {2023},
  month = jun,
  number = {arXiv:2306.07209},
  eprint = {2306.07209},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Various industries such as finance, meteorology, and energy generate vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it necessitates labor-intensive efforts and a high level of expertise for these data-related tasks. Considering that large language models (LLMs) have showcased promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while displaying and interacting in a human-friendly manner. Based on this belief, we propose DataCopilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end. Acting like an experienced expert, Data-Copilot autonomously transforms raw data into visualization results that best match the user's intent. Specifically, Data-Copilot autonomously designs versatile interfaces (tools) for data management, processing, prediction, and visualization. In real-time response, it automatically deploys a concise workflow by invoking corresponding interfaces step by step for the user's request. The interface design and deployment processes are fully controlled by Data-Copilot itself, without human assistance. Besides, we create a Data-Copilot demo that links abundant data from different domains (stock, fund, company, economics, and live news) and accurately respond to diverse requests, serving as a reliable AI assistant. Our project and demo are available at https://github.com/zwq2018/Data-Copilot.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@inproceedings{zhangDeBiasGenerativeExtraction2022,
  title = {De-{{Bias}} for {{Generative Extraction}} in {{Unified NER Task}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhang, Shuai and Shen, Yongliang and Tan, Zeqi and Wu, Yiquan and Lu, Weiming},
  year = {2022},
  pages = {808--818},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.59},
  abstract = {Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence. Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER. Among the existing approaches, only the generative model can be uniformly adapted to these three subtasks. However, when the generative model is applied to NER, its optimization objective is not consistent with the task, which makes the model vulnerable to the incorrect biases. In this paper, we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders: pre-context confounder and entityorder confounder. Furthermore, we design Intra- and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment. Experiments show that our method can improve the performance of the generative NER model in various datasets.},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{zhangEvaluatingPerformanceLarge2023,
  title = {Evaluating the {{Performance}} of {{Large Language Models}} on {{GAOKAO Benchmark}}},
  author = {Zhang, Xiaotian and Li, Chunyang and Zong, Yi and Ying, Zhengyu and He, Liang and Qiu, Xipeng},
  year = {2023},
  month = may,
  number = {arXiv:2305.12474},
  eprint = {2305.12474},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.12474},
  abstract = {Large language models have demonstrated remarkable performance across various natural language processing tasks; however, their efficacy in more challenging and domain-specific tasks remains less explored. This paper introduces the GAOKAO-Benchmark (GAOKAO-Bench), an intuitive benchmark that employs questions from the Chinese Gaokao examination as test samples for evaluating large language models.In order to align the evaluation results with humans as much as possible, we designed a method based on zero-shot prompts to analyze the accuracy and scoring rate of the model by dividing the questions into subjective and objective types. We evaluated the ChatGPT model on GAOKAO-Benchmark performance.Our findings reveal that the ChatGPT model excels in tackling objective questions, while also shedding light on its shortcomings and areas for improvement. To further scrutinize the model's responses, we incorporate human evaluations.In conclusion, this research contributes a robust evaluation benchmark for future large-scale language models and offers valuable insights into the limitations of such models.},
  archiveprefix = {arxiv}
}

@misc{zhangGPT4RoIInstructionTuning2023,
  title = {{{GPT4RoI}}: {{Instruction Tuning Large Language Model}} on {{Region-of-Interest}}},
  shorttitle = {{{GPT4RoI}}},
  author = {Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03601},
  eprint = {2307.03601},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Instruction tuning large language model (LLM) on image-text pairs has achieved unprecedented vision-language multimodal abilities. However, their vision-language alignments are only built on image-level, the lack of region-level alignment limits their advancements to fine-grained multimodal understanding. In this paper, we propose instruction tuning on region-of-interest. The key design is to reformulate the bounding box as the format of spatial instruction. The interleaved sequences of visual features extracted by the spatial instruction and the language embedding are input to LLM, and trained on the transformed region-text data in instruction tuning format. Our region-level vision-language model, termed as GPT4RoI, brings brand new conversational and interactive experience beyond image-level understanding. (1) Controllability: Users can interact with our model by both language and spatial instructions to flexibly adjust the detail level of the question. (2) Capacities: Our model supports not only single-region spatial instruction but also multi-region. This unlocks more region-level multimodal capacities such as detailed region caption and complex region reasoning. (3) Composition: Any off-the-shelf object detector can be a spatial instruction provider so as to mine informative object attributes from our model, like color, shape, material, action, relation to other objects, etc. The code, data, and demo can be found at https://github.com/jshilong/GPT4RoI.},
  archiveprefix = {arxiv},
  langid = {english}
}

@article{zhangLANGUAGEGUIDEDWORLDMODELS,
  title = {{{LANGUAGE-GUIDED WORLD MODELS A MODEL-BASED APPROACH TO AI CONTROL}}},
  author = {Zhang, Alex and Nguyen, Khanh and Tuyls, Jens and Lin, Albert and Narasimhan, Karthik},
  abstract = {Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new language descriptions and environment dynamics. Our experiments reveal that the current state-of-the-art Transformer architecture performs poorly on this benchmark, motivating us to design a more robust architecture. To showcase the practicality of our proposed LWMs, we simulate a scenario where these models augment the interpretability and safety of an agent by enabling it to generate and discuss plans with a human before execution. By effectively incorporating language feedback on the plan, the models boost the agent performance in the real environment by up to three times without collecting any interactive experiences in this environment.},
  langid = {english}
}

@misc{zhangLargeLanguageModels2023,
  title = {Large {{Language Models Are Semi-Parametric Reinforcement Learning Agents}}},
  author = {Zhang, Danyang and Chen, Lu and Zhang, Situo and Xu, Hongshen and Zhao, Zihan and Yu, Kai},
  year = {2023},
  month = oct,
  number = {arXiv:2306.07929},
  eprint = {2306.07929},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.07929},
  abstract = {Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4\% and 2\% for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER.},
  archiveprefix = {arxiv}
}

@misc{zhangLLaMAAdapterEfficientFinetuning2023,
  title = {{{LLaMA-Adapter}}: {{Efficient Fine-tuning}} of {{Language Models}} with {{Zero-init Attention}}},
  shorttitle = {{{LLaMA-Adapter}}},
  author = {Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  year = {2023},
  month = jun,
  number = {arXiv:2303.16199},
  eprint = {2303.16199},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.16199},
  abstract = {We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.},
  archiveprefix = {arxiv}
}

@misc{zhangMetaTransformerUnifiedFramework2023,
  title = {Meta-{{Transformer}}: {{A Unified Framework}} for {{Multimodal Learning}}},
  shorttitle = {Meta-{{Transformer}}},
  author = {Zhang, Yiyuan and Gong, Kaixiong and Zhang, Kaipeng and Li, Hongsheng and Qiao, Yu and Ouyang, Wanli and Yue, Xiangyu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10802},
  eprint = {2307.10802},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities (\${\textbackslash}textit\{e.g.\}\$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a \${\textbackslash}textbf\{frozen\}\$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer},
  archiveprefix = {arxiv}
}

@misc{zhangMMLLMsRecentAdvances2024,
  title = {{{MM-LLMs}}: {{Recent Advances}} in {{MultiModal Large Language Models}}},
  shorttitle = {{{MM-LLMs}}},
  author = {Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13601},
  eprint = {2401.13601},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.13601},
  abstract = {In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of \$26\$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this survey contributes to the ongoing advancement of the MM-LLMs domain.},
  archiveprefix = {arxiv}
}

@misc{zhangMultiViewReasoningConsistent2022,
  title = {Multi-{{View Reasoning}}: {{Consistent Contrastive Learning}} for {{Math Word Problem}}},
  shorttitle = {Multi-{{View Reasoning}}},
  author = {Zhang, Wenqi and Shen, Yongliang and Ma, Yanna and Cheng, Xiaoxia and Tan, Zeqi and Nong, Qingpeng and Lu, Weiming},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11694},
  eprint = {2210.11694},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and diverse equations. However, human solving naturally involves two consistent reasoning views: top-down and bottom-up, just as math equations also can be expressed in multiple equivalent forms: preorder and post-order. We propose a multi-view consistent contrastive learning for a more complete semantics-to-equation mapping. The entire process is decoupled into two independent but consistent views: top-down decomposition and bottom-up construction, and the two reasoning views are aligned in multi-granularity for consistency, enhancing global generation and precise reasoning. Experiments on multiple datasets across two languages show our approach significantly outperforms the existing baselines, especially on complex problems 1. We also show after consistent alignment, multiview can absorb the merits of both views and generate more diverse results consistent with the mathematical laws.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{zhangSelfContrastBetterReflection2024,
  title = {Self-{{Contrast}}: {{Better Reflection Through Inconsistent Solving Perspectives}}},
  shorttitle = {Self-{{Contrast}}},
  author = {Zhang, Wenqi and Shen, Yongliang and Wu, Linjuan and Peng, Qiuying and Wang, Jun and Zhuang, Yueting and Lu, Weiming},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02009},
  eprint = {2401.02009},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the selfevaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{zhangSoaring4K400K2024,
  title = {Soaring from {{4K}} to {{400K}}: {{Extending LLM}}'s {{Context}} with {{Activation Beacon}}},
  shorttitle = {Soaring from {{4K}} to {{400K}}},
  author = {Zhang, Peitian and Liu, Zheng and Xiao, Shitao and Shao, Ninglu and Ye, Qiwei and Dou, Zhicheng},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03462},
  eprint = {2401.03462},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {The utilization of long contexts poses a big challenge for large language models due to their limited context window length. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose Activation Beacon, which condenses LLM's raw activations into more compact forms such that it can perceive a much longer context with a limited context window. Activation Beacon is introduced as a plug-and-play module for the LLM. It fully preserves the LLM's original capability on short contexts while extending the new capability on processing longer contexts. Besides, it works with short sliding windows to process the long context, which achieves a competitive memory and time efficiency in both training and inference. Activation Beacon is learned by the auto-regression task conditioned on a mixture of beacons with diversified condensing ratios. Thanks to such a treatment, it can be efficiently trained purely with short-sequence data in just 10K steps, which consumes less than 9 hours on a single 8xA800 GPU machine. The experimental studies show that Activation Beacon is able to extend Llama-2-7B's context length by \${\textbackslash}times100\$ times (from 4K to 400K), meanwhile achieving a superior result on both long-context generation and understanding tasks. Our model and code will be available at the BGE repository.},
  archiveprefix = {arxiv}
}

@misc{zhangTinyLlamaOpenSourceSmall2024,
  title = {{{TinyLlama}}: {{An Open-Source Small Language Model}}},
  shorttitle = {{{TinyLlama}}},
  author = {Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02385},
  eprint = {2401.02385},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2401.02385},
  abstract = {We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.},
  archiveprefix = {arxiv}
}

@misc{zhaoLargeLanguageModels2023,
  title = {Large {{Language Models}} as {{Commonsense Knowledge}} for {{Large-Scale Task Planning}}},
  author = {Zhao, Zirui and Lee, Wee Sun and Hsu, David},
  year = {2023},
  month = oct,
  number = {arXiv:2305.14078},
  eprint = {2305.14078},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14078},
  abstract = {Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a policy and shows surprisingly interesting results. This paper shows that LLMs provide a commonsense model of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin, for complex, novel tasks. Further experiments and analyses on multiple tasks -- multiplication, multi-hop travel planning, object rearrangement -- suggest minimum description length (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy.},
  archiveprefix = {arxiv}
}

@misc{zhaoSLiCHFSequenceLikelihood2023,
  title = {{{SLiC-HF}}: {{Sequence Likelihood Calibration}} with {{Human Feedback}}},
  shorttitle = {{{SLiC-HF}}},
  author = {Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J.},
  year = {2023},
  month = may,
  number = {arXiv:2305.10425},
  eprint = {2305.10425},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.},
  archiveprefix = {arxiv}
}

@misc{zhaoSurveyLargeLanguage2023,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = jun,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence by machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling effect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities (e.g., incontext learning) that are not present in small-scale language models (e.g., BERT). To discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size (e.g., containing tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides an up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.},
  archiveprefix = {arxiv},
  langid = {english}
}

@misc{zhaTableGPTUnifyingTables2023,
  title = {{{TableGPT}}: {{Towards Unifying Tables}}, {{Nature Language}} and {{Commands}} into {{One GPT}}},
  shorttitle = {{{TableGPT}}},
  author = {Zha, Liangyu and Zhou, Junlin and Li, Liyao and Wang, Rui and Huang, Qingyi and Yang, Saisai and Yuan, Jing and Su, Changbao and Li, Xiang and Su, Aofeng and Zhang, Tao and Zhou, Chen and Shou, Kaizhe and Wang, Miao and Zhu, Wufang and Lu, Guoshan and Ye, Chao and Ye, Yali and Ye, Wentao and Zhang, Yiming and Deng, Xinglong and Xu, Jie and Wang, Haobo and Chen, Gang and Zhao, Junbo},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08674},
  eprint = {2307.08674},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.08674},
  abstract = {Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate. The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality. In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands. It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data. At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information. By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework's adaptability to specific use cases.},
  archiveprefix = {arxiv}
}

@inproceedings{zhengJudgingLLMasaJudgeMTBench2023,
  title = {Judging {{LLM-as-a-Judge}} with {{MT-Bench}} and {{Chatbot Arena}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2023},
  month = nov,
  abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\textbackslash}\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
  langid = {english}
}

@misc{zhengMiniGPT5InterleavedVisionandLanguage2023,
  title = {{{MiniGPT-5}}: {{Interleaved Vision-and-Language Generation}} via {{Generative Vokens}}},
  shorttitle = {{{MiniGPT-5}}},
  author = {Zheng, Kaizhi and He, Xuehai and Wang, Xin Eric},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02239},
  eprint = {2310.02239},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large Language Models (LLMs) have garnered significant attention for their advancements in natural language processing, demonstrating unparalleled prowess in text comprehension and generation. Yet, the simultaneous generation of images with coherent textual narratives remains an evolving frontier. In response, we introduce an innovative interleaved vision-and-language generation technique anchored by the concept of "generative vokens," acting as the bridge for harmonized image-text outputs. Our approach is characterized by a distinctive two-staged training strategy focusing on description-free multimodal generation, where the training requires no comprehensive descriptions of images. To bolster model integrity, classifier-free guidance is incorporated, enhancing the effectiveness of vokens on image generation. Our model, MiniGPT-5, exhibits substantial improvement over the baseline Divter model on the MMDialog dataset and consistently delivers superior or comparable multimodal outputs in human evaluations on the VIST dataset, highlighting its efficacy across diverse benchmarks.},
  archiveprefix = {arxiv}
}

@misc{zhengSecretsRLHFLarge2023,
  title = {Secrets of {{RLHF}} in {{Large Language Models Part I}}: {{PPO}}},
  shorttitle = {Secrets of {{RLHF}} in {{Large Language Models Part I}}},
  author = {Zheng, Rui and Dou, Shihan and Gao, Songyang and Shen, Wei and Wang, Binghai and Liu, Yan and Jin, Senjie and Liu, Qin and Xiong, Limao and Chen, Lu and Xi, Zhiheng and Zhou, Yuhao and Xu, Nuo and Lai, Wenbin and Zhu, Minghao and Weng, Rongxiang and Cheng, Wensen and Chang, Cheng and Yin, Zhangyue and Hua, Yuan and Huang, Haoran and Sun, Tianxiang and Yan, Hang and Gui, Tao and Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing},
  year = {2023},
  month = jul,
  number = {arXiv:2307.04964},
  eprint = {2307.04964},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.04964},
  abstract = {Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include {\textbackslash}textbf\{reward models\} to measure human preferences, {\textbackslash}textbf\{Proximal Policy Optimization\} (PPO) to optimize policy model outputs, and {\textbackslash}textbf\{process supervision\} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes},
  archiveprefix = {arxiv}
}

@misc{zhengTakeStepBack2023,
  title = {Take a {{Step Back}}: {{Evoking Reasoning}} via {{Abstraction}} in {{Large Language Models}}},
  shorttitle = {Take a {{Step Back}}},
  author = {Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06117},
  eprint = {2310.06117},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide the reasoning steps, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L models and observe substantial performance gains on a wide range of challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU Physics and Chemistry by 7\% and 11\%, TimeQA by 27\%, and MuSiQue by 7\%.},
  archiveprefix = {arxiv}
}

@misc{zhongAGIEvalHumanCentricBenchmark2023,
  title = {{{AGIEval}}: {{A Human-Centric Benchmark}} for {{Evaluating Foundation Models}}},
  shorttitle = {{{AGIEval}}},
  author = {Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  year = {2023},
  month = apr,
  number = {arXiv:2304.06364},
  eprint = {2304.06364},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95\% accuracy rate on the SAT Math test and a 92.5\% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/microsoft/AGIEval.},
  archiveprefix = {arxiv}
}

@misc{zhouContextfaithfulPromptingLarge2023,
  title = {Context-Faithful {{Prompting}} for {{Large Language Models}}},
  author = {Zhou, Wenxuan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  year = {2023},
  month = mar,
  number = {arXiv:2303.11315},
  eprint = {2303.11315},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.11315},
  abstract = {Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts.},
  archiveprefix = {arxiv}
}

@misc{zhouHowFaRAre2023,
  title = {How {{FaR Are Large Language Models From Agents}} with {{Theory-of-Mind}}?},
  author = {Zhou, Pei and Madaan, Aman and Potharaju, Srividya Pranavi and Gupta, Aditya and McKee, Kevin R. and Holtzman, Ari and Pujara, Jay and Ren, Xiang and Mishra, Swaroop and Nematzadeh, Aida and Upadhyay, Shyam and Faruqui, Manaal},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03051},
  eprint = {2310.03051},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {"Thinking is for Doing." Humans can infer other people's mental states from observations--an ability called Theory-of-Mind (ToM)--and subsequently act pragmatically on those inferences. Existing question answering benchmarks such as ToMi ask models questions to make inferences about beliefs of characters in a story, but do not test whether models can then use these inferences to guide their actions. We propose a new evaluation paradigm for large language models (LLMs): Thinking for Doing (T4D), which requires models to connect inferences about others' mental states to actions in social scenarios. Experiments on T4D demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking characters' beliefs in stories, but they struggle to translate this capability into strategic action. Our analysis reveals the core challenge for LLMs lies in identifying the implicit inferences about mental states without being explicitly asked about as in ToMi, that lead to choosing the correct action in T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee and Reflect (FaR), which provides a reasoning structure that encourages LLMs to anticipate future challenges and reason about potential actions. FaR boosts GPT-4's performance from 50\% to 71\% on T4D, outperforming other prompting methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to diverse out-of-distribution story structures and scenarios that also require ToM inferences to choose an action, consistently outperforming other methods including few-shot in-context learning.},
  archiveprefix = {arxiv}
}

@misc{zhouLanguageAgentTree2023,
  title = {Language {{Agent Tree Search Unifies Reasoning Acting}} and {{Planning}} in {{Language Models}}},
  author = {Zhou, Andy and Yan, Kai and {Shlapentokh-Rothman}, Michal and Wang, Haohan and Wang, Yu-Xiong},
  year = {2023},
  month = dec,
  number = {arXiv:2310.04406},
  eprint = {2310.04406},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.04406},
  abstract = {While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.},
  archiveprefix = {arxiv}
}

@misc{zhouLeasttoMostPromptingEnables2023,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  year = {2023},
  month = apr,
  number = {arXiv:2205.10625},
  eprint = {2205.10625},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10625},
  abstract = {Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  archiveprefix = {arxiv}
}

@misc{zhouLIMALessMore2023,
  title = {{{LIMA}}: {{Less Is More}} for {{Alignment}}},
  shorttitle = {{{LIMA}}},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  year = {2023},
  month = may,
  number = {arXiv:2305.11206},
  eprint = {2305.11206},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.11206},
  abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  archiveprefix = {arxiv}
}

@techreport{ZhuangKeFanHuaDeLingYuZhiShiXueXiYuJiSuanYinQingKeJiChuangXin2030XinYiDaiRenGongZhiNengChongDaXiangMu,
  title = {{-2030-``''}},
  author = {, },
  langid = {chinese}
}

@misc{ZhuangShiQi,
  title = {{}},
  abstract = {PythonPython 3},
  howpublished = {https://www.liaoxuefeng.com/wiki/1016959663602400/1017451662295584},
  langid = {zh\_CN}
}

@misc{zhuangToolChainEfficientAction2023,
  title = {{{ToolChain}}*: {{Efficient Action Space Navigation}} in {{Large Language Models}} with {{A}}* {{Search}}},
  shorttitle = {{{ToolChain}}*},
  author = {Zhuang, Yuchen and Chen, Xiang and Yu, Tong and Mitra, Saayan and Bursztyn, Victor and Rossi, Ryan A. and Sarkhel, Somdeb and Zhang, Chao},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13227},
  eprint = {2310.13227},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.13227},
  abstract = {Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A* search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution. Extensive experiments on multiple tool-use and reasoning tasks demonstrate that ToolChain* efficiently balances exploration and exploitation within an expansive action space. It outperforms state-of-the-art baselines on planning and reasoning tasks by 3.1\% and 3.5\% on average while requiring 7.35x and 2.31x less time, respectively.},
  archiveprefix = {arxiv}
}

@misc{zhuangToolQADatasetLLM2023,
  title = {{{ToolQA}}: {{A Dataset}} for {{LLM Question Answering}} with {{External Tools}}},
  shorttitle = {{{ToolQA}}},
  author = {Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  year = {2023},
  month = jun,
  number = {arXiv:2306.13304},
  eprint = {2306.13304},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.13304},
  abstract = {Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available to the broader scientific community on GitHub.},
  archiveprefix = {arxiv}
}

@misc{zhuangVloggerMakeYour2024,
  title = {Vlogger: {{Make Your Dream A Vlog}}},
  shorttitle = {Vlogger},
  author = {Zhuang, Shaobin and Li, Kunchang and Chen, Xinyuan and Wang, Yaohui and Liu, Ziwei and Qiao, Yu and Wang, Yali},
  year = {2024},
  month = jan,
  journal = {arXiv.org},
  abstract = {In this work, we present Vlogger, a generic AI system for generating a minute-level video blog (i.e., vlog) of user descriptions. Different from short videos with a few seconds, vlog often contains a complex storyline with diversified scenes, which is challenging for most existing video generation approaches. To break through this bottleneck, our Vlogger smartly leverages Large Language Model (LLM) as Director and decomposes a long video generation task of vlog into four key stages, where we invoke various foundation models to play the critical roles of vlog professionals, including (1) Script, (2) Actor, (3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings, our Vlogger can generate vlogs through explainable cooperation of top-down planning and bottom-up shooting. Moreover, we introduce a novel video diffusion model, ShowMaker, which serves as a videographer in our Vlogger for generating the video snippet of each shooting scene. By incorporating Script and Actor attentively as textual and visual prompts, it can effectively enhance spatial-temporal coherence in the snippet. Besides, we design a concise mixed training paradigm for ShowMaker, boosting its capacity for both T2V generation and prediction. Finally, the extensive experiments show that our method achieves state-of-the-art performance on zero-shot T2V generation and prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs from open-world descriptions, without loss of video coherence on script and actor. The code and model is all available at https://github.com/zhuangshaobin/Vlogger.},
  howpublished = {https://arxiv.org/abs/2401.09414v1},
  langid = {english}
}

@misc{zhuDeductiveBeamSearch2024,
  title = {Deductive {{Beam Search}}: {{Decoding Deducible Rationale}} for {{Chain-of-Thought Reasoning}}},
  shorttitle = {Deductive {{Beam Search}}},
  author = {Zhu, Tinghui and Zhang, Kai and Xie, Jian and Su, Yu},
  year = {2024},
  month = jan,
  number = {arXiv:2401.17686},
  eprint = {2401.17686},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves DBS's capability of detecting diverse and subtle reasoning errors and robustness on different model scales.},
  archiveprefix = {arxiv}
}

@misc{zhuExtrapolatingLargeLanguage2023,
  title = {Extrapolating {{Large Language Models}} to {{Non-English}} by {{Aligning Languages}}},
  author = {Zhu, Wenhao and Lv, Yunzhe and Dong, Qingxiu and Yuan, Fei and Xu, Jingjing and Huang, Shujian and Kong, Lingpeng and Chen, Jiajun and Li, Lei},
  year = {2023},
  month = aug,
  number = {arXiv:2308.04948},
  eprint = {2308.04948},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Due to the unbalanced training data distribution, the language ability of large language models (LLMs) is often biased towards English. In this paper, we propose to empower pre-trained LLMs on non-English languages by building semantic alignment across languages. We perform instruction-tuning on LLaMA with both translation task data and cross-lingual general task data to obtain cross-lingual models (x-LLaMA). Experiment results on cross-lingual benchmark XQUAD and MLQA show that x-LLaMA models outperform the English instruction-tuned counterpart (Alpaca) by 42.50\% on average on six non-English languages. Further experiments on Chinese benchmark C-Eval show that x-LLaMA achieves significant improvement on Chinese humanities tasks, outperforming Alpaca by 8.2\%. We also discover that incorporating non-English text on the target side of translation data is particularly effective for boosting non-English ability. Besides, we find that semantic alignment within LLM can be further strengthened as translation task data scales up and we present the formulation of the underlying scaling law. Evaluation results on translation dataset Flores-101 show that {\textbackslash}method outperforms previous LLaMA-based models in all evaluated directions. Code and data will be available at: https://github.com/OwenNJU/x-LLM.},
  archiveprefix = {arxiv}
}

@misc{zhuGhostMinecraftGenerally2023,
  title = {Ghost in the {{Minecraft}}: {{Generally Capable Agents}} for {{Open-World Environments}} via {{Large Language Models}} with {{Text-based Knowledge}} and {{Memory}}},
  shorttitle = {Ghost in the {{Minecraft}}},
  author = {Zhu, Xizhou and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Su, Weijie and Yang, Chenyu and Huang, Gao and Li, Bin and Lu, Lewei and Wang, Xiaogang and Qiao, Yu and Zhang, Zhaoxiang and Dai, Jifeng},
  year = {2023},
  month = jun,
  number = {arXiv:2305.17144},
  eprint = {2305.17144},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.17144},
  abstract = {The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular "ObtainDiamond" task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the "ObtainDiamond" task stands at around 20\%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5\% in success rate on the "ObtainDiamond" task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM.},
  archiveprefix = {arxiv}
}

@misc{zhuLargeLanguageModels2023,
  title = {Large {{Language Models}} Can {{Learn Rules}}},
  author = {Zhu, Zhaocheng and Xue, Yuan and Chen, Xinyun and Zhou, Denny and Tang, Jian and Schuurmans, Dale and Dai, Hanjun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07064},
  eprint = {2310.07064},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07064},
  abstract = {When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an absolute gain of 11-27\% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.},
  archiveprefix = {arxiv}
}

@misc{zhuMiniGPT4EnhancingVisionLanguage2023,
  title = {{{MiniGPT-4}}: {{Enhancing Vision-Language Understanding}} with {{Advanced Large Language Models}}},
  shorttitle = {{{MiniGPT-4}}},
  author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  year = {2023},
  month = oct,
  number = {arXiv:2304.10592},
  eprint = {2304.10592},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.10592},
  abstract = {The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/.},
  archiveprefix = {arxiv}
}

@misc{zhuMolecularSubstructureAwareNetwork2022,
  title = {Molecular {{Substructure-Aware Network}} for {{Drug-Drug Interaction Prediction}}},
  author = {Zhu, Xinyu and Shen, Yongliang and Lu, Weiming},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11267},
  eprint = {2208.11267},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Concomitant administration of drugs can cause drug-drug interactions (DDIs). Some drug combinations are beneficial, but other ones may cause negative effects which are previously unrecorded. Previous works on DDI prediction usually rely on hand-engineered domain knowledge, which is laborious to obtain. In this work, we propose a novel model, Molecular Substructure-Aware Network (MSAN), to effectively predict potential DDIs from molecular structures of drug pairs. We adopt a Transformer-like substructure extraction module to acquire a fixed number of representative vectors that are associated with various substructure patterns of the drug molecule. Then, interaction strength between the two drugs' substructures will be captured by a similarity-based interaction module. We also perform a substructure dropping augmentation before graph encoding to alleviate overfitting. Experimental results from a real-world dataset reveal that our proposed model achieves the state-of-the-art performance. We also show that the predictions of our model are highly interpretable through a case study.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  langid = {english}
}

@misc{zhuSolvingMathWord2023,
  title = {Solving {{Math Word Problems}} via {{Cooperative Reasoning}} Induced {{Language Models}}},
  author = {Zhu, Xinyu and Wang, Junjie and Zhang, Lin and Zhang, Yuxiang and Huang, Yongfeng and Gan, Ruyi and Zhang, Jiaxing and Yang, Yujiu},
  year = {2023},
  month = may,
  number = {arXiv:2210.16257},
  eprint = {2210.16257},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6\% increase over best baselines. Our codes are available at https://github.com/TianHongZXY/CoRe},
  archiveprefix = {arxiv}
}

@misc{zhuVisionMambaEfficient2024,
  title = {Vision {{Mamba}}: {{Efficient Visual Representation Learning}} with {{Bidirectional State Space Model}}},
  shorttitle = {Vision {{Mamba}}},
  author = {Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  year = {2024},
  month = jan,
  number = {arXiv:2401.09417},
  eprint = {2401.09417},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., Mamba, have shown great potential for long sequence modeling. Building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance of visual representation learning on self-attention is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation \& memory efficiency. For example, Vim is 2.8\${\textbackslash}times\$ faster than DeiT and saves 86.8\% GPU memory when performing batch inference to extract features on images with a resolution of 1248\${\textbackslash}times\$1248. The results demonstrate that Vim is capable of overcoming the computation \& memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to become the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.},
  archiveprefix = {arxiv},
  langid = {english}
}
