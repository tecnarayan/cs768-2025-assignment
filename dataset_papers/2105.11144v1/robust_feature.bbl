\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Athalye, A., Carlini, N., and Wagner, D.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Ben-Tal et~al.(2013)Ben-Tal, Den~Hertog, De~Waegenaere, Melenberg, and
  Rennen]{ben2013robust}
Ben-Tal, A., Den~Hertog, D., De~Waegenaere, A., Melenberg, B., and Rennen, G.
\newblock Robust solutions of optimization problems affected by uncertain
  probabilities.
\newblock \emph{Management Science}, 59\penalty0 (2):\penalty0 341--357, 2013.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock Preprint, 2020.

\bibitem[Bubeck(2014)]{bubeck2014convex}
Bubeck, S.
\newblock Convex optimization: Algorithms and complexity.
\newblock Preprint arXiv:1405.4980, 2014.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and
  Specia]{cer2017semeval}
Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L.
\newblock Semeval-2017 task 1: semantic textual similarity multilingual and
  crosslingual focused evaluation.
\newblock In \emph{International Workshop on Semantic Evaluation}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Conference of the North American Chapter of the Association
  for Computational Linguistics}, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Duchi(2016)]{duchi2016lecture}
Duchi, J.
\newblock Lecture notes for statistics 311/electrical engineering 377, 2016.
\newblock \url{http://web.stanford.edu/class/stats311/lecture-notes.pdf}.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2015explaning}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2016.

\bibitem[Hendrycks \& Dietterich(2018)Hendrycks and
  Dietterich]{hendrycks2018benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and Mazeika]{hendrycks2019using}
Hendrycks, D., Lee, K., and Mazeika, M.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Hendrycks et~al.(2020{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath,
  Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2020many}
Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,
  R., Zhu, T., Parajuli, S., Guo, M., et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock Preprint arXiv:2006.16241, 2020{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2020{\natexlab{b}})Hendrycks, Liu, Wallace, Dziedzic,
  Krishnan, and Song]{hendrycks2020pretrained}
Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D.
\newblock Pretrained transformers improve out-of-distribution robustness.
\newblock In \emph{Annual Conference of the Association for Computational
  Linguistics}, 2020{\natexlab{b}}.

\bibitem[Kannan et~al.(2018)Kannan, Kurakin, and
  Goodfellow]{kannan2018adversarial}
Kannan, H., Kurakin, A., and Goodfellow, I.
\newblock Adversarial logit pairing.
\newblock Preprint arXiv:1803.06373, 2018.

\bibitem[Kornblith et~al.(2019)Kornblith, Shlens, and Le]{kornblith2019better}
Kornblith, S., Shlens, J., and Le, Q.~V.
\newblock Do better imagenet models transfer better?
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2019.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee \& Raginsky(2018)Lee and Raginsky]{lee2018minimax}
Lee, J. and Raginsky, M.
\newblock Minimax statistical learning with wasserstein distances.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Li et~al.(2020)Li, Fang, Xu, and Zhao]{li2019inductive}
Li, Y., Fang, E.~X., Xu, H., and Zhao, T.
\newblock Inductive bias of gradient descent based adversarial training on
  separable data.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{liu2020toward}
Liu, C., Zhu, L., and Belkin, M.
\newblock Toward a theory of optimization for over-parameterized systems of
  non-linear equations: the lessons of deep learning.
\newblock Preprint arXiv:2003.00307, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock Preprint arXiv:1907.11692, 2019.

\bibitem[Lohn(2020)]{lohn2020estimating}
Lohn, A.~J.
\newblock Estimating the brittleness of ai: safety integrity levels and the
  need for testing out-of-distribution performance.
\newblock Preprint arXiv:2009.00802, 2020.

\bibitem[Loshchilov \& Hutter(2018)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lyu \& Li(2019)Lyu and Li]{lyu2019gradient}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas2011learning}
Maas, A., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Annual Conference of the Association for Computational
  Linguistics}, 2011.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Namkoong(2019)]{namkoong2019reliable}
Namkoong, H.
\newblock \emph{Reliable machine learning via distributional robustness}.
\newblock PhD thesis, Stanford University, 2019.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{nouiehed2019solving}
Nouiehed, M., Sanjabi, M., Huang, T., Lee, J.~D., and Razaviyayn, M.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision, 2021.
\newblock \url{https://openai.com/blog/clip/}.

\bibitem[Raghunathan et~al.(2019)Raghunathan, Xie, Yang, Duchi, and
  Liang]{raghunathan2019adversarial}
Raghunathan, A., Xie, S.~M., Yang, F., Duchi, J.~C., and Liang, P.
\newblock Adversarial training can hurt generalization.
\newblock Preprint arXiv:1906.06032, 2019.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{rakhlin2012making}
Rakhlin, A., Shamir, O., and Sridharan, K.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{International Coference on Machine Learning}, 2012.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Salman et~al.(2020{\natexlab{a}})Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., and Madry, A.
\newblock Do adversarially robust imagenet models transfer better?
\newblock Preprint arXiv:2007.08489, 2020{\natexlab{a}}.

\bibitem[Salman et~al.(2020{\natexlab{b}})Salman, Ilyas, Engstrom, Vemprala,
  Madry, and Kapoor]{salman2020unadversarial}
Salman, H., Ilyas, A., Engstrom, L., Vemprala, S., Madry, A., and Kapoor, A.
\newblock Unadversarial examples: designing objects for robust vision.
\newblock Preprint arXiv:2012.12235, 2020{\natexlab{b}}.

\bibitem[Schneider et~al.(2020)Schneider, Rusak, Eck, Bringmann, Brendel, and
  Bethge]{schneider2020improving}
Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., and Bethge, M.
\newblock Improving robustness against common corruptions by covariate shift
  adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Shapiro(2017)]{shapiro2017distributionally}
Shapiro, A.
\newblock Distributionally robust stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2258--2275, 2017.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{sinha2018certifying}
Sinha, A., Namkoong, H., and Duchi, J.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and
  Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2013.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock Preprint arXiv:1312.6199, 2013.

\bibitem[Tu et~al.(2020)Tu, Lalwani, Gella, and He]{tu2020empirical}
Tu, L., Lalwani, G., Gella, S., and He, H.
\newblock An empirical study on robustness to spurious correlations using
  pre-trained language models.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 621--633, 2020.

\bibitem[Utrera et~al.(2021)Utrera, Kravitz, Erichson, Khanna, and
  Mahoney]{utrera2020adversarially}
Utrera, F., Kravitz, E., Erichson, N.~B., Khanna, R., and Mahoney, M.~W.
\newblock Adversarially-trained deep nets transfer better.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[van~der Vaart \& Wellner(2000)van~der Vaart and Wellner]{van2000weak}
van~der Vaart, A.~W. and Wellner, J.~A.
\newblock \emph{Weak convergence and empirical processes}.
\newblock Springer series in statistics. Springer, 2000.

\bibitem[Vershynin(2018)]{vershynin2018}
Vershynin, R.
\newblock \emph{High-dimensional probability: an introduction with applications
  in data science}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2018.

\bibitem[Villani(2008)]{villani2008optimal}
Villani, C.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Volpi et~al.(2018)Volpi, Namkoong, Sener, Duchi, Murino, and
  Savarese]{volpi2018generalizing}
Volpi, R., Namkoong, H., Sener, O., Duchi, J.~C., Murino, V., and Savarese, S.
\newblock Generalizing to unseen domains via adversarial data augmentation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Wainwright(2019)]{wainwright2019}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: a non-asymptotic viewpoint}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2019.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2018broad}
Williams, A., Nangia, N., and Bowman, S.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{Conference of the North American Chapter of the Association
  for Computational Linguistics}, 2018.

\bibitem[Wolf et~al.(2020)Wolf, Chaumond, Debut, Sanh, Delangue, Moi, Cistac,
  Funtowicz, Davison, Shleifer, et~al.]{wolf2020transformers}
Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi, A., Cistac, P.,
  Funtowicz, M., Davison, J., Shleifer, S., et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2020.

\bibitem[Xie et~al.(2017)Xie, Liang, and Song]{xie2017diversity}
Xie, B., Liang, Y., and Song, L.
\newblock Diversity leads to generalization in neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2017.

\bibitem[Xie et~al.(2020)Xie, Tan, Gong, Wang, Yuille, and
  Le]{xie2020adversarial}
Xie, C., Tan, M., Gong, B., Wang, J., Yuille, A.~L., and Le, Q.~V.
\newblock Adversarial examples improve image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2020.

\bibitem[Xu \& Mannor(2012)Xu and Mannor]{xu2012robustness}
Xu, H. and Mannor, S.
\newblock Robustness and generalization.
\newblock \emph{Machine learning}, 86\penalty0 (3):\penalty0 391--423, 2012.

\bibitem[Yin et~al.(2019)Yin, Kannan, and Bartlett]{yin2019rademacher}
Yin, D., Kannan, R., and Bartlett, P.
\newblock Rademacher complexity for adversarially robust generalization.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Zhu et~al.(2019)Zhu, Cheng, Gan, Sun, Goldstein, and
  Liu]{zhu2019freelb}
Zhu, C., Cheng, Y., Gan, Z., Sun, S., Goldstein, T., and Liu, J.
\newblock Freelb: Enhanced adversarial training for natural language
  understanding.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}
