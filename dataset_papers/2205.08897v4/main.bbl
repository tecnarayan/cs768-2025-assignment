\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Box \& Jenkins(1968)Box and Jenkins]{box_arima2}
Box, G. E.~P. and Jenkins, G.~M.
\newblock Some recent advances in forecasting and control.
\newblock \emph{Journal of the Royal Statistical Society. Series C (Applied
  Statistics)}, 17\penalty0 (2):\penalty0 91--109, 1968.

\bibitem[Box \& Pierce(1970)Box and Pierce]{box_distribution_1970}
Box, G. E.~P. and Pierce, D.~A.
\newblock Distribution of residual autocorrelations in
  autoregressive-integrated moving average time series models.
\newblock volume~65, pp.\  1509--1526. Taylor \& Francis, 1970.

\bibitem[Challu et~al.(2022)Challu, Olivares, Oreshkin, Garza, Mergenthaler,
  and Dubrawski]{challu2022n}
Challu, C., Olivares, K.~G., Oreshkin, B.~N., Garza, F., Mergenthaler, M., and
  Dubrawski, A.
\newblock N-hits: Neural hierarchical interpolation for time series
  forecasting.
\newblock \emph{arXiv preprint arXiv:2201.12886}, 2022.

\bibitem[Chung et~al.(2014)Chung, G{\"{u}}l{\c{c}}ehre, Cho, and
  Bengio]{GRU_cho_et_al_2014}
Chung, J., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Cho, K., and Bengio, Y.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{CoRR}, abs/1412.3555, 2014.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{Bert/NAACL/Jacob}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies (NAACL-HLT), Minneapolis, MN, USA, June 2-7, 2019}, pp.\
  4171--4186, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{Transformers-for-image-at-scale/iclr/DosovitskiyB0WZ21}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}, 2021.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{Hippo}
Gu, A., Dao, T., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1474--1487, 2020.

\bibitem[Gu et~al.(2021{\natexlab{a}})Gu, Goel, and R{\'e}]{S4}
Gu, A., Goel, K., and R{\'e}, C.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock \emph{arXiv preprint arXiv:2111.00396}, 2021{\natexlab{a}}.

\bibitem[Gu et~al.(2021{\natexlab{b}})Gu, Johnson, Goel, Saab, Dao, Rudra, and
  R{\'e}]{LSSL}
Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., and R{\'e}, C.
\newblock Combining recurrent, convolutional, and continuous-time models with
  linear state space layers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Gupta et~al.(2021)Gupta, Xiao, and
  Bogdan]{Multiwavelet-based-Operator-Learning}
Gupta, G., Xiao, X., and Bogdan, P.
\newblock Multiwavelet-based operator learning for differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS),
  2021, December 6-14, 2021, virtual}, pp.\  24048--24062, 2021.

\bibitem[Hochreiter \& Schmidhuber(1997{\natexlab{a}})Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780,
  1997{\natexlab{a}}.

\bibitem[Hochreiter \& Schmidhuber(1997{\natexlab{b}})Hochreiter and
  Schmidhuber]{hochreiter_long_1997_lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock Long {Short}-{Term} {Memory}.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780,
  November 1997{\natexlab{b}}.
\newblock ISSN 0899-7667, 1530-888X.

\bibitem[Kim et~al.(2021)Kim, Kim, Tae, Park, Choi, and Choo]{reversible}
Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J.
\newblock Reversible instance normalization for accurate time-series
  forecasting against distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kingma \& Ba(2017)Kingma and Ba]{kingma_adam:_2017}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} {Method} for {Stochastic} {Optimization}.
\newblock \emph{arXiv:1412.6980 [cs]}, January 2017.
\newblock arXiv: 1412.6980.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and
  Levskaya]{DBLP:conf/iclr/KitaevKL20-reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Lai et~al.(2018)Lai, Chang, Yang, and
  Liu]{lai2018modeling-exchange-dataset}
Lai, G., Chang, W.-C., Yang, Y., and Liu, H.
\newblock Modeling long-and short-term temporal patterns with deep neural
  networks.
\newblock In \emph{The 41st International ACM SIGIR Conference on Research \&
  Development in Information Retrieval}, pp.\  95--104, 2018.

\bibitem[Lee{-}Thorp et~al.(2021)Lee{-}Thorp, Ainslie, Eckstein, and
  Onta{\~{n}}{\'{o}}n]{FNet}
Lee{-}Thorp, J., Ainslie, J., Eckstein, I., and Onta{\~{n}}{\'{o}}n, S.
\newblock {FNet}: Mixing tokens with fourier transforms.
\newblock \emph{CoRR}, abs/2105.03824, 2021.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and
  Yan]{Log-transformer-shiyang-2019}
Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X., and Yan, X.
\newblock Enhancing the locality and breaking the memory bottleneck of
  transformer on time series forecasting.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Li et~al.(2020)Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya,
  Stuart, and Anandkumar]{Fourier-Neural-Operator}
Li, Z., Kovachki, N.~B., Azizzadenesheli, K., Liu, B., Bhattacharya, K.,
  Stuart, A.~M., and Anandkumar, A.
\newblock Fourier neural operator for parametric partial differential
  equations.
\newblock \emph{CoRR}, abs/2010.08895, 2020.

\bibitem[Liu et~al.(2022)Liu, Yu, Liao, Li, Lin, Liu, and
  Dustdar]{liu2022pyraformer}
Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A.~X., and Dustdar, S.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time
  series modeling and forecasting.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Makridakis et~al.(1982)Makridakis, Andersen, Carbone, Fildes, Hibon,
  Lewandowski, Newton, Parzen, and Winkler]{seasonal-naive}
Makridakis, S., Andersen, A., Carbone, R., Fildes, R., Hibon, M., Lewandowski,
  R., Newton, J., Parzen, E., and Winkler, R.
\newblock The accuracy of extrapolation (time series) methods: Results of a
  forecasting competition.
\newblock \emph{Journal of Forecasting}, 1\penalty0 (2):\penalty0 111--153,
  1982.
\newblock \doi{https://doi.org/10.1002/for.3980010202}.
\newblock URL
  \url{https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980010202}.

\bibitem[Oreshkin et~al.(2019)Oreshkin, Carpov, Chapados, and Bengio]{nbeats}
Oreshkin, B.~N., Carpov, D., Chapados, N., and Bengio, Y.
\newblock {N-BEATS}: Neural basis expansion analysis for interpretable time
  series forecasting.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{DBLP:icml/On-the-difficult-gradient-vanishing-explode}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning, {ICML} 2013, Atlanta, GA, USA, 16-21 June 2013}, volume~28, pp.\
  1310--1318, 2013.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and
  Chintala]{NEURIPS2019_9015_pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035. 2019.

\bibitem[Qin et~al.(2017)Qin, Song, Chen, Cheng, Jiang, and
  Cottrell]{dual-state-attention-rnn-qin}
Qin, Y., Song, D., Chen, H., Cheng, W., Jiang, G., and Cottrell, G.~W.
\newblock A dual-stage attention-based recurrent neural network for time series
  prediction.
\newblock In \emph{Proceedings of the Twenty-Sixth International Joint
  Conference on Artificial Intelligence (IJCAI), Melbourne, Australia, August
  19-25, 2017}, pp.\  2627--2633. ijcai.org, 2017.

\bibitem[Rangapuram et~al.(2018)Rangapuram, Seeger, Gasthaus, Stella, Wang, and
  Januschowski]{deep-state-space-models-for-time-series-forecasting}
Rangapuram, S.~S., Seeger, M.~W., Gasthaus, J., Stella, L., Wang, Y., and
  Januschowski, T.
\newblock Deep state space models for time series forecasting.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Rao et~al.(2021)Rao, Zhao, Zhu, Lu, and
  Zhou]{DBLP:Global-filter-FNO-in-cv}
Rao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J.
\newblock Global filter networks for image classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Salinas et~al.(2020)Salinas, Flunkert, Gasthaus, and
  Januschowski]{DBLP:journals/corr/FlunkertSG17-deepAR}
Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T.
\newblock {DeepAR}: Probabilistic forecasting with autoregressive recurrent
  networks.
\newblock \emph{International Journal of Forecasting}, 36\penalty0
  (3):\penalty0 1181--1191, 2020.

\bibitem[Sen et~al.(2019)Sen, Yu, and
  Dhillon]{Think_globally_act_locally_tcn_time_series_2019}
Sen, R., Yu, H., and Dhillon, I.~S.
\newblock Think globally, act locally: {A} deep neural network approach to
  high-dimensional time series forecasting.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS),
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  4838--4847, 2019.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{LRA}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{attention_is_all_you_need}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voelker et~al.(2019)Voelker, Kaji\'{c}, and Eliasmith]{LMU}
Voelker, A., Kaji\'{c}, I., and Eliasmith, C.
\newblock Legendre memory units: Continuous-time representation in recurrent
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Wang et~al.(2018)Wang, Wang, Li, and Wu]{wang2018multilevel}
Wang, J., Wang, Z., Li, J., and Wu, J.
\newblock Multilevel wavelet decomposition network for interpretable time
  series analysis.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  2437--2446, 2018.

\bibitem[Wen et~al.(2022)Wen, Zhou, Zhang, Chen, Ma, Yan, and
  Sun]{wen2022transformers}
Wen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J., and Sun, L.
\newblock Transformers in time series: A survey.
\newblock \emph{arXiv preprint arXiv:2202.07125}, 2022.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{Autoformer}
Wu, H., Xu, J., Wang, J., and Long, M.
\newblock Autoformer: Decomposition transformers with auto-correlation for
  long-term series forecasting.
\newblock In \emph{Proceedings of the Advances in Neural Information Processing
  Systems (NeurIPS)}, pp.\  101--112, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Lin, Song, and
  Dhillon]{FourierRecurrentUnits}
Zhang, J., Lin, Y., Song, Z., and Dhillon, I.~S.
\newblock Learning long term dependencies via fourier recurrent units.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML), Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15,
  2018}, volume~80, pp.\  5810--5818, 2018.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and
  Zhang]{haoyietal-informer-2021}
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In \emph{The Thirty-Fifth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2021, Virtual Conference}, volume~35, pp.\
  11106--11115, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Ma, Wen, Wang, Sun, and Jin]{FedFormer}
Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R.
\newblock {FEDformer}: Frequency enhanced decomposed transformer for long-term
  series forecasting.
\newblock In \emph{39th International Conference on Machine Learning (ICML)},
  2022.

\end{thebibliography}
