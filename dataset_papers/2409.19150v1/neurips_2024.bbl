\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anselmi et~al.(2015)Anselmi, Rosasco, Tan, and Poggio]{Memo035}
Anselmi, F., Rosasco, L., Tan, C., and Poggio, T.
\newblock Deep convolutional networks are hierarchical kernel machines.
\newblock \emph{Center for Brains, Minds and Machines (CBMM) Memo No. 035}, 2015.

\bibitem[Arora \& Barak(2009)Arora and Barak]{arora2009computational}
Arora, S. and Barak, B.
\newblock \emph{Computational complexity: a modern approach}.
\newblock Cambridge University Press, 2009.

\bibitem[Aytekin(2022)]{aytekin2022neural}
Aytekin, C.
\newblock Neural networks are decision trees, 2022.

\bibitem[Bird et~al.(2009)Bird, Klein, and Loper]{bird2009natural}
Bird, S., Klein, E., and Loper, E.
\newblock \emph{Natural language processing with Python: analyzing text with the natural language toolkit}.
\newblock " O'Reilly Media, Inc.", 2009.

\bibitem[Blum(1992)]{blum1992rank}
Blum, A.
\newblock Rank-r decision trees are a subclass of r-decision lists.
\newblock \emph{Information Processing Letters}, 42\penalty0 (4):\penalty0 183--185, 1992.

\bibitem[Breiman(2001)]{breiman2001random}
Breiman, L.
\newblock Random forests.
\newblock \emph{Machine learning}, 45:\penalty0 5--32, 2001.

\bibitem[Breiman et~al.(1984)Breiman, Friedman, Olshen, and Stone]{CART}
Breiman, L., Friedman, J., Olshen, R., and Stone, C.
\newblock \emph{Classification and Regression Trees}.
\newblock Wadsworth Publishing, 1984.

\bibitem[Brutzkus et~al.(2020)Brutzkus, Daniely, and Malach]{brutzkus2020id3}
Brutzkus, A., Daniely, A., and Malach, E.
\newblock Id3 learns juntas for smoothed product distributions.
\newblock In \emph{Conference on Learning Theory}, pp.\  902--915. PMLR, 2020.

\bibitem[Bshouty \& Burroughs(2003)Bshouty and Burroughs]{bshouty2003proper}
Bshouty, N.~H. and Burroughs, L.
\newblock On the proper learning of axis-parallel concepts.
\newblock \emph{The Journal of Machine Learning Research}, 4:\penalty0 157--176, 2003.

\bibitem[Chen \& Guestrin(2016)Chen and Guestrin]{xgboost}
Chen, T. and Guestrin, C.
\newblock {XGBoost}: A scalable tree boosting system.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, KDD '16, pp.\  785--794, New York, NY, USA, 2016. ACM.
\newblock ISBN 978-1-4503-4232-2.
\newblock \doi{10.1145/2939672.2939785}.
\newblock URL \url{http://doi.acm.org/10.1145/2939672.2939785}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai, T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[De et~al.(2024)De, Smith, Fernando, Botev, Cristian-Muraru, Gu, Haroun, Berrada, Chen, Srinivasan, Desjardins, Doucet, Budden, Teh, Pascanu, Freitas, and Gulcehre]{de2024griffin}
De, S., Smith, S.~L., Fernando, A., Botev, A., Cristian-Muraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y., Srinivasan, S., Desjardins, G., Doucet, A., Budden, D., Teh, Y.~W., Pascanu, R., Freitas, N.~D., and Gulcehre, C.
\newblock Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.

\bibitem[Ehrenfeucht \& Haussler(1989)Ehrenfeucht and Haussler]{ehrenfeucht1989learning}
Ehrenfeucht, A. and Haussler, D.
\newblock Learning decision trees from random examples.
\newblock \emph{Information and Computation}, 82\penalty0 (3):\penalty0 231--246, 1989.

\bibitem[Eldan \& Li(2023)Eldan and Li]{eldan2023tinystories}
Eldan, R. and Li, Y.
\newblock Tinystories: How small can language models be and still speak coherent english?, 2023.

\bibitem[Filimonov(2011)]{filimonov2011decision}
Filimonov, D.
\newblock \emph{Decision tree-based syntactic language modeling}.
\newblock University of Maryland, College Park, 2011.

\bibitem[Friedman(2002)]{friedman2002stochastic}
Friedman, J.~H.
\newblock Stochastic gradient boosting.
\newblock \emph{Computational statistics \& data analysis}, 38\penalty0 (4):\penalty0 367--378, 2002.

\bibitem[Grinsztajn et~al.(2022)Grinsztajn, Oyallon, and Varoquaux]{grinsztajn2022tree}
Grinsztajn, L., Oyallon, E., and Varoquaux, G.
\newblock Why do tree-based models still outperform deep learning on typical tabular data?
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 507--520, 2022.

\bibitem[Gu \& Dao(2023)Gu and Dao]{gu2023mamba}
Gu, A. and Dao, T.
\newblock Mamba: Linear-time sequence modeling with selective state spaces, 2023.

\bibitem[Hakkinen \& Tian(2001)Hakkinen and Tian]{hakkinen2001n}
Hakkinen, J. and Tian, J.
\newblock N-gram and decision tree based language identification for written words.
\newblock In \emph{IEEE Workshop on Automatic Speech Recognition and Understanding, 2001. ASRU'01.}, pp.\  335--338. IEEE, 2001.

\bibitem[Heeman(1999)]{heeman1999pos}
Heeman, P.~A.
\newblock Pos tags and decision trees for language modeling.
\newblock In \emph{1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora}, 1999.

\bibitem[Hopcroft et~al.(2001)Hopcroft, Motwani, and Ullman]{hopcroft2001introduction}
Hopcroft, J.~E., Motwani, R., and Ullman, J.~D.
\newblock Introduction to automata theory, languages, and computation.
\newblock \emph{Acm Sigact News}, 32\penalty0 (1):\penalty0 60--65, 2001.

\bibitem[Kearns \& Mansour(1996)Kearns and Mansour]{kearns1996boosting}
Kearns, M. and Mansour, Y.
\newblock On the boosting ability of top-down decision tree learning algorithms.
\newblock In \emph{Proceedings of the twenty-eighth annual ACM symposium on Theory of computing}, pp.\  459--468, 1996.

\bibitem[Lewis(2000)]{lewis2000introduction}
Lewis, R.~J.
\newblock An introduction to classification and regression tree (cart) analysis.
\newblock In \emph{Annual meeting of the society for academic emergency medicine in San Francisco, California}, volume~14. Citeseer, 2000.

\bibitem[Ma et~al.(2023)Ma, Zhou, Kong, He, Gui, Neubig, May, and Zettlemoyer]{ma2023mega}
Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and Zettlemoyer, L.
\newblock Mega: Moving average equipped gated attention, 2023.

\bibitem[Magerman(1995)]{magerman1995statistical}
Magerman, D.~M.
\newblock Statistical decision-tree models for parsing.
\newblock \emph{arXiv preprint cmp-lg/9504030}, 1995.

\bibitem[Malach(2023)]{malach2023auto}
Malach, E.
\newblock Auto-regressive next-token predictors are universal learners.
\newblock \emph{arXiv preprint arXiv:2309.06979}, 2023.

\bibitem[Meek et~al.(2002)Meek, Chickering, and Heckerman]{meek2002autoregressive}
Meek, C., Chickering, D.~M., and Heckerman, D.
\newblock Autoregressive tree models for time-series analysis.
\newblock In \emph{Proceedings of the 2002 SIAM International Conference on Data Mining}, pp.\  229--244. SIAM, 2002.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and Dean]{mikolov2013efficient}
Mikolov, T., Chen, K., Corrado, G., and Dean, J.
\newblock Efficient estimation of word representations in vector space, 2013.

\bibitem[Nallapati \& Allan(2002)Nallapati and Allan]{nallapati2002capturing}
Nallapati, R. and Allan, J.
\newblock Capturing term dependencies using a sentence tree based language model.
\newblock In \emph{Proceedings of CIKM}, volume~2, pp.\  383--390. Citeseer, 2002.

\bibitem[Navada et~al.(2011)Navada, Ansari, Patil, and Sonkamble]{navada2011overview}
Navada, A., Ansari, A.~N., Patil, S., and Sonkamble, B.~A.
\newblock Overview of use of decision tree algorithms in machine learning.
\newblock In \emph{2011 IEEE control and system graduate research colloquium}, pp.\  37--42. IEEE, 2011.

\bibitem[OpenAI et~al.(2023)OpenAI, :, Achiam, and Steven~Adler]{openai2023gpt4}
OpenAI, :, Achiam, J., and Steven~Adler, e.~a.
\newblock Gpt-4 technical report, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R.
\newblock Training language models to follow instructions with human feedback, 2022.

\bibitem[Poggio(2022)]{Memo138}
Poggio, T.
\newblock Compositional sparsity: a framework for ml.
\newblock \emph{Center for Brains, Minds and Machines (CBMM) Memo No. 138}, 2022.

\bibitem[Potamianos \& Jelinek(1998)Potamianos and Jelinek]{potamianos1998study}
Potamianos, G. and Jelinek, F.
\newblock A study of n-gram and decision tree letter language modeling methods.
\newblock \emph{Speech Communication}, 24\penalty0 (3):\penalty0 171--192, 1998.

\bibitem[Quinlan(1986)]{quinlan1986induction}
Quinlan, J.~R.
\newblock Induction of decision trees.
\newblock \emph{Machine learning}, 1:\penalty0 81--106, 1986.

\bibitem[Rivest(1987)]{rivest1987learning}
Rivest, R.~L.
\newblock Learning decision lists.
\newblock \emph{Machine learning}, 2:\penalty0 229--246, 1987.

\bibitem[Shwartz-Ziv \& Armon(2022)Shwartz-Ziv and Armon]{shwartz2022tabular}
Shwartz-Ziv, R. and Armon, A.
\newblock Tabular data: Deep learning is not all you need.
\newblock \emph{Information Fusion}, 81:\penalty0 84--90, 2022.

\bibitem[Srivastava et~al.(2023)Srivastava, Rastogi, and Abhishek~Rao]{srivastava2023imitation}
Srivastava, A., Rastogi, A., and Abhishek~Rao, e.~a.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2023.

\bibitem[Sun et~al.(2023)Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{sun2023retentive}
Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F.
\newblock Retentive network: A successor to transformer for large language models, 2023.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Schärli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, and Wei]{suzgun2022challenging}
Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H.~W., Chowdhery, A., Le, Q.~V., Chi, E.~H., Zhou, D., and Wei, J.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.

\bibitem[van~der Maaten(2013)]{vandermaaten2013barneshutsne}
van~der Maaten, L.
\newblock Barnes-hut-sne, 2013.

\bibitem[Vaswani et~al.(2023)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2023attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need, 2023.

\bibitem[Wong et~al.(2023)Wong, Grand, Lew, Goodman, Mansinghka, Andreas, and Tenenbaum]{wong2023word}
Wong, L., Grand, G., Lew, A.~K., Goodman, N.~D., Mansinghka, V.~K., Andreas, J., and Tenenbaum, J.~B.
\newblock From word models to world models: Translating from natural language to the probabilistic language of thought, 2023.

\bibitem[Yarotsky(2016)]{DBLP:journals/corr/Yarotsky16}
Yarotsky, D.
\newblock Error bounds for approximations with deep relu networks.
\newblock \emph{CoRR}, abs/1610.01145, 2016.
\newblock URL \url{http://arxiv.org/abs/1610.01145}.

\end{thebibliography}
