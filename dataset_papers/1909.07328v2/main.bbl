% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{xaisurvey}{article}{}
      \name{author}{2}{}{%
        {{hash=8a9ff04737c73b2b3a33aa683bf3fbce}{%
           family={Adadi},
           familyi={A\bibinitperiod},
           given={Amina},
           giveni={A\bibinitperiod}}}%
        {{hash=d9bcbdaf716d702fac9dc8cc5800ef30}{%
           family={Berrada},
           familyi={B\bibinitperiod},
           given={Mohammed},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Institute of Electrical}%
        {Electronics Engineers ({IEEE})}%
      }
      \strng{namehash}{020b8c17b7a31bebb298656b5ca70bc3}
      \strng{fullhash}{020b8c17b7a31bebb298656b5ca70bc3}
      \strng{bibnamehash}{020b8c17b7a31bebb298656b5ca70bc3}
      \strng{authorbibnamehash}{020b8c17b7a31bebb298656b5ca70bc3}
      \strng{authornamehash}{020b8c17b7a31bebb298656b5ca70bc3}
      \strng{authorfullhash}{020b8c17b7a31bebb298656b5ca70bc3}
      \field{sortinit}{A}
      \field{sortinithash}{a3dcedd53b04d1adfd5ac303ecd5e6fa}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{{IEEE} Access}
      \field{title}{Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence ({XAI})}
      \field{volume}{6}
      \field{year}{2018}
      \field{pages}{52138\bibrangedash 52160}
      \range{pages}{23}
      \verb{doi}
      \verb 10.1109/access.2018.2870052
      \endverb
    \endentry
    \entry{sanitychecksforsaliencymaps}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=13c8415286093e947788ee180780fd7a}{%
           family={Adebayo},
           familyi={A\bibinitperiod},
           given={Julius},
           giveni={J\bibinitperiod}}}%
        {{hash=4f550339f0337905aa634f39e1ba4833}{%
           family={Gilmer},
           familyi={G\bibinitperiod},
           given={Justin},
           giveni={J\bibinitperiod}}}%
        {{hash=5cb8bde1cea7b52d8f0be2812f15dee9}{%
           family={Muelly},
           familyi={M\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=e771760b6d0d33f8b4dfd907d8d57ac2}{%
           family={Hardt},
           familyi={H\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod}}}%
        {{hash=a11a6718c039f3e3f3f31167e5094c10}{%
           family={Kim},
           familyi={K\bibinitperiod},
           given={Been},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{eb99efa7fab5a2d6b81f774831a06990}
      \strng{fullhash}{512d3819219d4fad97c50708eff1b5bd}
      \strng{bibnamehash}{eb99efa7fab5a2d6b81f774831a06990}
      \strng{authorbibnamehash}{eb99efa7fab5a2d6b81f774831a06990}
      \strng{authornamehash}{eb99efa7fab5a2d6b81f774831a06990}
      \strng{authorfullhash}{512d3819219d4fad97c50708eff1b5bd}
      \field{sortinit}{A}
      \field{sortinithash}{a3dcedd53b04d1adfd5ac303ecd5e6fa}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Sanity checks for saliency maps}
      \field{year}{2018}
      \field{pages}{9505\bibrangedash 9515}
      \range{pages}{11}
    \endentry
    \entry{invariantriskmin}{article}{}
      \name{author}{4}{}{%
        {{hash=ab4a5e058dea2d852ff02aa4e10045c3}{%
           family={Arjovsky},
           familyi={A\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=ac762f2592005edf6bad58f566967c6c}{%
           family={Bottou},
           familyi={B\bibinitperiod},
           given={L{é}on},
           giveni={L\bibinitperiod}}}%
        {{hash=94f7047ed067ed3ca0ce018501dd9db9}{%
           family={Gulrajani},
           familyi={G\bibinitperiod},
           given={Ishaan},
           giveni={I\bibinitperiod}}}%
        {{hash=47d681c0cbae75196012a538c46ccc06}{%
           family={Lopez-Paz},
           familyi={L\bibinithyphendelim P\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{c0253375e3f94eb82dddef2732bbe5a4}
      \strng{fullhash}{fff4e7042f937a836fba8845cc73b792}
      \strng{bibnamehash}{c0253375e3f94eb82dddef2732bbe5a4}
      \strng{authorbibnamehash}{c0253375e3f94eb82dddef2732bbe5a4}
      \strng{authornamehash}{c0253375e3f94eb82dddef2732bbe5a4}
      \strng{authorfullhash}{fff4e7042f937a836fba8845cc73b792}
      \field{sortinit}{A}
      \field{sortinithash}{a3dcedd53b04d1adfd5ac303ecd5e6fa}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{arXiv preprint arXiv:1907.02893}
      \field{title}{Invariant risk minimization}
      \field{year}{2019}
    \endentry
    \entry{layerwiserelevance}{article}{}
      \name{author}{6}{}{%
        {{hash=8295a9e73c42cad599c45a2cec0c6d27}{%
           family={Bach},
           familyi={B\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=d122a2a87d21da4007f460564975e967}{%
           family={Binder},
           familyi={B\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=e0889b25b37f68a2d73747dc5dfcb8a0}{%
           family={Montavon},
           familyi={M\bibinitperiod},
           given={Gr{é}goire},
           giveni={G\bibinitperiod}}}%
        {{hash=cbac16af17fd4a388467bf85b707de24}{%
           family={Klauschen},
           familyi={K\bibinitperiod},
           given={Frederick},
           giveni={F\bibinitperiod}}}%
        {{hash=9f1b6144a45b1967e989e74552e37ada}{%
           family={Müller},
           familyi={M\bibinitperiod},
           given={Klaus-Robert},
           giveni={K\bibinithyphendelim R\bibinitperiod}}}%
        {{hash=7f49b6382ab4ca917a2aa2249f875f79}{%
           family={Samek},
           familyi={S\bibinitperiod},
           given={Wojciech},
           giveni={W\bibinitperiod}}}%
      }
      \name{editor}{1}{}{%
        {{hash=a05eb926566f3e584fb96eea9d8e78be}{%
           family={Suarez},
           familyi={S\bibinitperiod},
           given={Oscar\bibnamedelima Deniz},
           giveni={O\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Public Library of Science ({PLoS})}%
      }
      \strng{namehash}{e54f9799fd08ceb06062346ab39063d7}
      \strng{fullhash}{5ee3866a5cd207d0c7f8d8e4b0ef8904}
      \strng{bibnamehash}{e54f9799fd08ceb06062346ab39063d7}
      \strng{authorbibnamehash}{e54f9799fd08ceb06062346ab39063d7}
      \strng{authornamehash}{e54f9799fd08ceb06062346ab39063d7}
      \strng{authorfullhash}{5ee3866a5cd207d0c7f8d8e4b0ef8904}
      \strng{editorbibnamehash}{a05eb926566f3e584fb96eea9d8e78be}
      \strng{editornamehash}{a05eb926566f3e584fb96eea9d8e78be}
      \strng{editorfullhash}{a05eb926566f3e584fb96eea9d8e78be}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{{PLOS} {ONE}}
      \field{month}{7}
      \field{number}{7}
      \field{title}{On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation}
      \field{volume}{10}
      \field{year}{2015}
      \field{pages}{e0130140}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1371/journal.pone.0130140
      \endverb
    \endentry
    \entry{bahdanauatt}{article}{}
      \name{author}{3}{}{%
        {{hash=6d80adec79a13a33e73215c5f46f1605}{%
           family={Bahdanau},
           familyi={B\bibinitperiod},
           given={Dzmitry},
           giveni={D\bibinitperiod}}}%
        {{hash=3da7501a79d9346572c7fd6e41b615df}{%
           family={Cho},
           familyi={C\bibinitperiod},
           given={Kyunghyun},
           giveni={K\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{fullhash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{bibnamehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{authorbibnamehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{authornamehash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \strng{authorfullhash}{ccf5ebef61998aaab5ec6eace8f4564d}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.}
      \field{day}{1}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ICLR}
      \field{month}{9}
      \field{title}{Neural Machine Translation by Jointly Learning to Align and Translate}
      \field{year}{2014}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 1409.0473
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1409.0473v7:PDF
      \endverb
      \keyw{cs.CL,cs.LG,cs.NE,stat.ML}
    \endentry
    \entry{neurosymsurvey}{article}{}
      \name{author}{14}{}{%
        {{hash=0610dca062acc8ef3c8d15ffcfdc835d}{%
           family={Besold},
           familyi={B\bibinitperiod},
           given={Tarek\bibnamedelima R.},
           giveni={T\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=70658466246873155f5b9b3a737cdd4e}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur},
           giveni={A\bibinitperiod},
           prefix={d'Avila},
           prefixi={d\bibinitperiod}}}%
        {{hash=ad716c4f6aa5883cc0f54907883ad01d}{%
           family={Bader},
           familyi={B\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
        {{hash=906dbd4c811afa1403515435beaefda9}{%
           family={Bowman},
           familyi={B\bibinitperiod},
           given={Howard},
           giveni={H\bibinitperiod}}}%
        {{hash=926269a097c38456f5b32d7efb1d1a80}{%
           family={Domingos},
           familyi={D\bibinitperiod},
           given={Pedro},
           giveni={P\bibinitperiod}}}%
        {{hash=6c91a433a7cc7ab507b9bd279fa64cac}{%
           family={Hitzler},
           familyi={H\bibinitperiod},
           given={Pascal},
           giveni={P\bibinitperiod}}}%
        {{hash=2f20d4f12c2ef5b4bca1d79e816b5428}{%
           family={Kuehnberger},
           familyi={K\bibinitperiod},
           given={Kai-Uwe},
           giveni={K\bibinithyphendelim U\bibinitperiod}}}%
        {{hash=8a6bd56db2a68b7d2f595538052b85e9}{%
           family={Lamb},
           familyi={L\bibinitperiod},
           given={Luis\bibnamedelima C.},
           giveni={L\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=0b754fc5deaf0a8cd3e41d53d76ec051}{%
           family={Lowd},
           familyi={L\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=60e2d6c3d23058b1644cb54b92ef14f1}{%
           family={Lima},
           familyi={L\bibinitperiod},
           given={Priscila\bibnamedelimb Machado\bibnamedelima Vieira},
           giveni={P\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim V\bibinitperiod}}}%
        {{hash=dbe6416f5849a48535bf7520c02044aa}{%
           family={Penning},
           familyi={P\bibinitperiod},
           given={Leo},
           giveni={L\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=97e3d7cfaabb3f1ce935aec0d9e3d0b9}{%
           family={Pinkas},
           familyi={P\bibinitperiod},
           given={Gadi},
           giveni={G\bibinitperiod}}}%
        {{hash=3e2b0bc20e35ee839c381d54ed997131}{%
           family={Poon},
           familyi={P\bibinitperiod},
           given={Hoifung},
           giveni={H\bibinitperiod}}}%
        {{hash=9affecbfe51d0fd75ae0999379cdb855}{%
           family={Zaverucha},
           familyi={Z\bibinitperiod},
           given={Gerson},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{4cdaa667128f738fb7b20888a1bf1e01}
      \strng{fullhash}{2b6844d7468590dbaeab0ddfcb1c15ac}
      \strng{bibnamehash}{4cdaa667128f738fb7b20888a1bf1e01}
      \strng{authorbibnamehash}{4cdaa667128f738fb7b20888a1bf1e01}
      \strng{authornamehash}{4cdaa667128f738fb7b20888a1bf1e01}
      \strng{authorfullhash}{2b6844d7468590dbaeab0ddfcb1c15ac}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.}
      \field{day}{10}
      \field{eprintclass}{cs.AI}
      \field{eprinttype}{arXiv}
      \field{month}{11}
      \field{title}{Neural-Symbolic Learning and Reasoning: A Survey and Interpretation}
      \field{year}{2017}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 1711.03902
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1711.03902v1:PDF
      \endverb
      \keyw{cs.AI}
    \endentry
    \entry{neurosymbolic}{book}{}
      \name{author}{3}{}{%
        {{hash=cd2832a90f1088c4c5a477c890f2fd75}{%
           family={Broda},
           familyi={B\bibinitperiod},
           given={Krysia\bibnamedelima B.},
           giveni={K\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=8db3413ad93e844a891a80f281f4ebdb}{%
           family={Garcez},
           familyi={G\bibinitperiod},
           given={Artur\bibnamedelimb S.\bibnamedelimi D'Avila},
           giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=e4c171212e77fadaec078dad6dc4d173}{%
           family={Gabbay},
           familyi={G\bibinitperiod},
           given={Dov\bibnamedelima M.},
           giveni={D\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer London}%
      }
      \strng{namehash}{5bbb9f4775f99030bc33bf27f8b94f70}
      \strng{fullhash}{5bbb9f4775f99030bc33bf27f8b94f70}
      \strng{bibnamehash}{5bbb9f4775f99030bc33bf27f8b94f70}
      \strng{authorbibnamehash}{5bbb9f4775f99030bc33bf27f8b94f70}
      \strng{authornamehash}{5bbb9f4775f99030bc33bf27f8b94f70}
      \strng{authorfullhash}{5bbb9f4775f99030bc33bf27f8b94f70}
      \field{sortinit}{B}
      \field{sortinithash}{8de16967003c7207dae369d874f1456e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{6}
      \field{isbn}{1852335122}
      \field{month}{8}
      \field{pagetotal}{288}
      \field{title}{Neural-Symbolic Learning Systems}
      \field{year}{2002}
      \field{dateera}{ce}
    \endentry
    \entry{gradcamplus}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=5e375073aa77690342f9b9919270f6df}{%
           family={Chattopadhay},
           familyi={C\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod}}}%
        {{hash=8cb5c014ba91724d7680d6817f6ad2ae}{%
           family={Sarkar},
           familyi={S\bibinitperiod},
           given={Anirban},
           giveni={A\bibinitperiod}}}%
        {{hash=d74a9767168e3a4c2917ff3328b654f2}{%
           family={Howlader},
           familyi={H\bibinitperiod},
           given={Prantik},
           giveni={P\bibinitperiod}}}%
        {{hash=1a6d0bd847a9168d28d9e43f792ba079}{%
           family={Balasubramanian},
           familyi={B\bibinitperiod},
           given={Vineeth\bibnamedelima N.},
           giveni={V\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {IEEE}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{372e4d29342a23917d836131c42dae84}
      \strng{fullhash}{357557e525610f2ce787d571ab357532}
      \strng{bibnamehash}{372e4d29342a23917d836131c42dae84}
      \strng{authorbibnamehash}{372e4d29342a23917d836131c42dae84}
      \strng{authornamehash}{372e4d29342a23917d836131c42dae84}
      \strng{authorfullhash}{357557e525610f2ce787d571ab357532}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{2018 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})}
      \field{month}{3}
      \field{title}{Grad-{CAM}++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks}
      \field{year}{2018}
      \field{pages}{839\bibrangedash 847}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/wacv.2018.00097
      \endverb
    \endentry
    \entry{attsurvey}{article}{}
      \name{author}{4}{}{%
        {{hash=359aa2e87e18291e695db7d1fa8f456f}{%
           family={Chaudhari},
           familyi={C\bibinitperiod},
           given={Sneha},
           giveni={S\bibinitperiod}}}%
        {{hash=f3904f2106445ed889ef7400cbcc04b0}{%
           family={Polatkan},
           familyi={P\bibinitperiod},
           given={Gungor},
           giveni={G\bibinitperiod}}}%
        {{hash=d600f5ebab485732b865f68c4edd1e10}{%
           family={Ramanath},
           familyi={R\bibinitperiod},
           given={Rohan},
           giveni={R\bibinitperiod}}}%
        {{hash=2129efcb0df555b548f8871aa184cbbc}{%
           family={Mithal},
           familyi={M\bibinitperiod},
           given={Varun},
           giveni={V\bibinitperiod}}}%
      }
      \strng{namehash}{b82e3f18fa08e55d645899324af52c40}
      \strng{fullhash}{152c57d6d6304fc17f19932f34eb2072}
      \strng{bibnamehash}{b82e3f18fa08e55d645899324af52c40}
      \strng{authorbibnamehash}{b82e3f18fa08e55d645899324af52c40}
      \strng{authornamehash}{b82e3f18fa08e55d645899324af52c40}
      \strng{authorfullhash}{152c57d6d6304fc17f19932f34eb2072}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy which groups existing techniques into coherent categories. We review the different neural architectures in which attention has been incorporated, and also show how attention improves interpretability of neural models. Finally, we discuss some applications in which modeling attention has a significant impact. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.}
      \field{day}{5}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{IJCAI}
      \field{month}{4}
      \field{title}{An Attentive Survey of Attention Models}
      \field{year}{2019}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 1904.02874
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1904.02874v1:PDF
      \endverb
      \keyw{cs.LG,stat.ML}
    \endentry
    \entry{gru}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=3da7501a79d9346572c7fd6e41b615df}{%
           family={Cho},
           familyi={C\bibinitperiod},
           given={Kyunghyun},
           giveni={K\bibinitperiod}}}%
        {{hash=d27fe3f9898ba01eeca28e3bd205f9ea}{%
           family={Merrienboer},
           familyi={M\bibinitperiod},
           given={Bart},
           giveni={B\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=6d80adec79a13a33e73215c5f46f1605}{%
           family={Bahdanau},
           familyi={B\bibinitperiod},
           given={Dzmitry},
           giveni={D\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{9b1920db887fe0d658725c1e606b61f4}
      \strng{fullhash}{2b512caaa9756b9174a0d4bede4107b7}
      \strng{bibnamehash}{9b1920db887fe0d658725c1e606b61f4}
      \strng{authorbibnamehash}{9b1920db887fe0d658725c1e606b61f4}
      \strng{authornamehash}{9b1920db887fe0d658725c1e606b61f4}
      \strng{authorfullhash}{2b512caaa9756b9174a0d4bede4107b7}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation}
      \field{title}{On the Properties of Neural Machine Translation: Encoder–Decoder Approaches}
      \field{year}{2014}
      \verb{doi}
      \verb 10.3115/v1/w14-4012
      \endverb
    \endentry
    \entry{deeplogic}{article}{}
      \name{author}{2}{}{%
        {{hash=fcdeda7ffba0b75f814828161a41afb8}{%
           family={Cingillioglu},
           familyi={C\bibinitperiod},
           given={Nuri},
           giveni={N\bibinitperiod}}}%
        {{hash=9aaf1fdc987fd59f123077bfb569fc7f}{%
           family={Russo},
           familyi={R\bibinitperiod},
           given={Alessandra},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{8e79726a2fad7bf6bbea1e33ab9d3e05}
      \strng{fullhash}{8e79726a2fad7bf6bbea1e33ab9d3e05}
      \strng{bibnamehash}{8e79726a2fad7bf6bbea1e33ab9d3e05}
      \strng{authorbibnamehash}{8e79726a2fad7bf6bbea1e33ab9d3e05}
      \strng{authornamehash}{8e79726a2fad7bf6bbea1e33ab9d3e05}
      \strng{authorfullhash}{8e79726a2fad7bf6bbea1e33ab9d3e05}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Combining machine learning with logic-based expert systems in order to get the best of both worlds are becoming increasingly popular. However, to what extent machine learning can already learn to reason over rule-based knowledge is still an open problem. In this paper, we explore how symbolic logic, defined as logic programs at a character level, is learned to be represented in a high-dimensional vector space using RNN-based iterative neural networks to perform reasoning. We create a new dataset that defines 12 classes of logic programs exemplifying increased level of complexity of logical reasoning and train the networks in an end-to-end fashion to learn whether a logic program entails a given query. We analyse how learning the inference algorithm gives rise to representations of atoms, literals and rules within logic programs and evaluate against increasing lengths of predicate and constant symbols as well as increasing steps of multi-hop reasoning.}
      \field{eprintclass}{cs.NE}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{AAAI-MAKE}
      \field{title}{DeepLogic: Towards End-to-End Differentiable Logical Reasoning}
      \field{year}{2019}
      \verb{eprint}
      \verb 1805.07433
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1805.07433v3:PDF
      \endverb
      \keyw{cs.NE,cs.AI,cs.LO}
    \endentry
    \entry{nbf}{incollection}{}
      \name{author}{1}{}{%
        {{hash=56c1cb987df81f882e06f838bfbf71f7}{%
           family={Clark},
           familyi={C\bibinitperiod},
           given={Keith\bibnamedelima L.},
           giveni={K\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer {US}}%
      }
      \strng{namehash}{56c1cb987df81f882e06f838bfbf71f7}
      \strng{fullhash}{56c1cb987df81f882e06f838bfbf71f7}
      \strng{bibnamehash}{56c1cb987df81f882e06f838bfbf71f7}
      \strng{authorbibnamehash}{56c1cb987df81f882e06f838bfbf71f7}
      \strng{authornamehash}{56c1cb987df81f882e06f838bfbf71f7}
      \strng{authorfullhash}{56c1cb987df81f882e06f838bfbf71f7}
      \field{sortinit}{C}
      \field{sortinithash}{4c244ceae61406cdc0cc2ce1cb1ff703}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Logic and Data Bases}
      \field{title}{Negation as Failure}
      \field{year}{1978}
      \field{pages}{293\bibrangedash 322}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1007/978-1-4684-3384-5_11
      \endverb
    \endentry
    \entry{evans2018learning}{article}{}
      \name{author}{2}{}{%
        {{hash=173072a12c87679d6e0aeed7dbfb2abc}{%
           family={Evans},
           familyi={E\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=e03325308d4d207f32caeecad362f184}{%
           family={Grefenstette},
           familyi={G\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {{AI} Access Foundation}%
      }
      \strng{namehash}{6163901be53ea7496c5acc754150e480}
      \strng{fullhash}{6163901be53ea7496c5acc754150e480}
      \strng{bibnamehash}{6163901be53ea7496c5acc754150e480}
      \strng{authorbibnamehash}{6163901be53ea7496c5acc754150e480}
      \strng{authornamehash}{6163901be53ea7496c5acc754150e480}
      \strng{authorfullhash}{6163901be53ea7496c5acc754150e480}
      \field{sortinit}{E}
      \field{sortinithash}{c554bd1a0b76ea92b9f105fe36d9c7b0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.}
      \field{eprintclass}{cs.NE}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{Journal of Artificial Intelligence Research}
      \field{title}{Learning Explanatory Rules from Noisy Data}
      \field{volume}{61}
      \field{year}{2018}
      \field{pages}{1\bibrangedash 64}
      \range{pages}{64}
      \verb{doi}
      \verb 10.1613/jair.5714
      \endverb
      \verb{eprint}
      \verb 1711.04574
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1711.04574v2:PDF
      \endverb
      \keyw{cs.NE,math.LO}
    \endentry
    \entry{senseandreference}{article}{}
      \name{author}{1}{}{%
        {{hash=1c468d5fce77082669983ce899030b96}{%
           family={Frege},
           familyi={F\bibinitperiod},
           given={Gottlob},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {JSTOR}%
      }
      \strng{namehash}{1c468d5fce77082669983ce899030b96}
      \strng{fullhash}{1c468d5fce77082669983ce899030b96}
      \strng{bibnamehash}{1c468d5fce77082669983ce899030b96}
      \strng{authorbibnamehash}{1c468d5fce77082669983ce899030b96}
      \strng{authornamehash}{1c468d5fce77082669983ce899030b96}
      \strng{authorfullhash}{1c468d5fce77082669983ce899030b96}
      \field{sortinit}{F}
      \field{sortinithash}{fb0c0faa89eb6abae8213bf60e6799ea}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{The Philosophical Review}
      \field{month}{5}
      \field{number}{3}
      \field{title}{Sense and Reference}
      \field{volume}{57}
      \field{year}{1948}
      \field{pages}{209}
      \range{pages}{1}
      \verb{doi}
      \verb 10.2307/2181485
      \endverb
    \endentry
    \entry{sticksword}{book}{}
      \name{author}{4}{}{%
        {{hash=abf7f5fbb1cc91b14ae6f76f782c9f58}{%
           family={Frost},
           familyi={F\bibinitperiod},
           given={Joe\bibnamedelima L.},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=fd1a605e1fd96b668bd02e1e7cce388d}{%
           family={Brown},
           familyi={B\bibinitperiod},
           given={Pei-San},
           giveni={P\bibinithyphendelim S\bibinitperiod}}}%
        {{hash=9d9e5a8137712a8107380fe2cb86d3e1}{%
           family={Sutterby},
           familyi={S\bibinitperiod},
           given={John\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=f68851a4f2d5f6403fe391b8de8dfa72}{%
           family={Thornton},
           familyi={T\bibinitperiod},
           given={Candra\bibnamedelima D.},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Association for Childhood Education International}%
      }
      \strng{namehash}{d877159a734868faa1e45ae192f2fcd3}
      \strng{fullhash}{9b1d884bb10c43921f39d5382b1f4328}
      \strng{bibnamehash}{d877159a734868faa1e45ae192f2fcd3}
      \strng{authorbibnamehash}{d877159a734868faa1e45ae192f2fcd3}
      \strng{authornamehash}{d877159a734868faa1e45ae192f2fcd3}
      \strng{authorfullhash}{9b1d884bb10c43921f39d5382b1f4328}
      \field{sortinit}{F}
      \field{sortinithash}{fb0c0faa89eb6abae8213bf60e6799ea}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{0871731649}
      \field{title}{The Developmental Benefits Of Playgrounds}
      \field{year}{2004}
    \endentry
    \entry{neuralturing}{article}{}
      \name{author}{3}{}{%
        {{hash=dfca94b0427da7f9088af596e23b46c0}{%
           family={Graves},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=f96594cc1d20df4a987bf8b9770d1ef0}{%
           family={Wayne},
           familyi={W\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=970650e9394fccd4144d4b829505d2b3}{%
           family={Danihelka},
           familyi={D\bibinitperiod},
           given={Ivo},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{fullhash}{a6d94af175112562d0f33e48f97adfff}
      \strng{bibnamehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{authorbibnamehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{authornamehash}{a6d94af175112562d0f33e48f97adfff}
      \strng{authorfullhash}{a6d94af175112562d0f33e48f97adfff}
      \field{sortinit}{G}
      \field{sortinithash}{62eb2aa29549e4fdbd3cb154ec5711cb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.}
      \field{day}{20}
      \field{eprintclass}{cs.NE}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{arXiv:1410.5401}
      \field{month}{10}
      \field{title}{Neural Turing Machines}
      \field{year}{2014}
      \field{dateera}{ce}
      \verb{eprint}
      \verb http://arxiv.org/abs/1410.5401v2
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1410.5401v2:PDF
      \endverb
      \keyw{cs.NE}
    \endentry
    \entry{dnc}{article}{}
      \name{author}{20}{}{%
        {{hash=dfca94b0427da7f9088af596e23b46c0}{%
           family={Graves},
           familyi={G\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=f96594cc1d20df4a987bf8b9770d1ef0}{%
           family={Wayne},
           familyi={W\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=745ec7c2fc050c47e2b7e40a3ea7bd3d}{%
           family={Reynolds},
           familyi={R\bibinitperiod},
           given={Malcolm},
           giveni={M\bibinitperiod}}}%
        {{hash=b63b9b2d91f6ba1b1739563edc0432ab}{%
           family={Harley},
           familyi={H\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=970650e9394fccd4144d4b829505d2b3}{%
           family={Danihelka},
           familyi={D\bibinitperiod},
           given={Ivo},
           giveni={I\bibinitperiod}}}%
        {{hash=2b5c158555f2549cca6faa3ccffcab6e}{%
           family={Grabska-Barwi{ń}ska},
           familyi={G\bibinithyphendelim B\bibinitperiod},
           given={Agnieszka},
           giveni={A\bibinitperiod}}}%
        {{hash=5790ba56913305cdb71e1347d821eff5}{%
           family={Colmenarejo},
           familyi={C\bibinitperiod},
           given={Sergio\bibnamedelima G{ó}mez},
           giveni={S\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=e03325308d4d207f32caeecad362f184}{%
           family={Grefenstette},
           familyi={G\bibinitperiod},
           given={Edward},
           giveni={E\bibinitperiod}}}%
        {{hash=44574cf2790d8c3428206a582c947779}{%
           family={Ramalho},
           familyi={R\bibinitperiod},
           given={Tiago},
           giveni={T\bibinitperiod}}}%
        {{hash=333bf32d39a38f126d82c78d782e93ba}{%
           family={Agapiou},
           familyi={A\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=db6afdf70ea61bf8ed973a75b90ff818}{%
           family={Badia},
           familyi={B\bibinitperiod},
           given={Adri{à}\bibnamedelima Puigdom{è}nech},
           giveni={A\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=b5f8311a9762696646ffd301a8da6e11}{%
           family={Hermann},
           familyi={H\bibinitperiod},
           given={Karl\bibnamedelima Moritz},
           giveni={K\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=4e065eb03ba0a9a33876236e75fdcce0}{%
           family={Zwols},
           familyi={Z\bibinitperiod},
           given={Yori},
           giveni={Y\bibinitperiod}}}%
        {{hash=46cc21fb5dec973c05ceb0f321e02ca0}{%
           family={Ostrovski},
           familyi={O\bibinitperiod},
           given={Georg},
           giveni={G\bibinitperiod}}}%
        {{hash=fa2243eed057c0f80c42aff81aa286f2}{%
           family={Cain},
           familyi={C\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=59998a15386f62e4d2776176ab58d49c}{%
           family={King},
           familyi={K\bibinitperiod},
           given={Helen},
           giveni={H\bibinitperiod}}}%
        {{hash=0e19fe61e6ad22ca347bffe3e453c372}{%
           family={Summerfield},
           familyi={S\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=7f78b89d658b3089596ea20b3dc21304}{%
           family={Blunsom},
           familyi={B\bibinitperiod},
           given={Phil},
           giveni={P\bibinitperiod}}}%
        {{hash=481308b301166b521c74fde6566e97e6}{%
           family={Kavukcuoglu},
           familyi={K\bibinitperiod},
           given={Koray},
           giveni={K\bibinitperiod}}}%
        {{hash=b160026950ebb1e2286dfb40c15482f5}{%
           family={Hassabis},
           familyi={H\bibinitperiod},
           given={Demis},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Nature}%
      }
      \strng{namehash}{8b91c8aa3714ffeda8908af3996aa567}
      \strng{fullhash}{8205fd3154b1a2a06df641b2c7ef95f5}
      \strng{bibnamehash}{8b91c8aa3714ffeda8908af3996aa567}
      \strng{authorbibnamehash}{8b91c8aa3714ffeda8908af3996aa567}
      \strng{authornamehash}{8b91c8aa3714ffeda8908af3996aa567}
      \strng{authorfullhash}{8205fd3154b1a2a06df641b2c7ef95f5}
      \field{sortinit}{G}
      \field{sortinithash}{62eb2aa29549e4fdbd3cb154ec5711cb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Nature}
      \field{month}{10}
      \field{number}{7626}
      \field{title}{Hybrid computing using a neural network with dynamic external memory}
      \field{volume}{538}
      \field{year}{2016}
      \field{pages}{471\bibrangedash 476}
      \range{pages}{6}
      \verb{doi}
      \verb 10.1038/nature20101
      \endverb
    \endentry
    \entry{invariantcausalpred}{article}{}
      \name{author}{3}{}{%
        {{hash=76d93908fa9bdcbddb2817a8a317a3da}{%
           family={Heinze-Deml},
           familyi={H\bibinithyphendelim D\bibinitperiod},
           given={Christina},
           giveni={C\bibinitperiod}}}%
        {{hash=b50a0fee091540dedaefacc234101037}{%
           family={Peters},
           familyi={P\bibinitperiod},
           given={Jonas},
           giveni={J\bibinitperiod}}}%
        {{hash=ab9469dddb5d18ec039f709e2d7c4e00}{%
           family={Meinshausen},
           familyi={M\bibinitperiod},
           given={Nicolai},
           giveni={N\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {De Gruyter}%
      }
      \strng{namehash}{07a27cac5e69e38b849a9e7ca84794a5}
      \strng{fullhash}{07a27cac5e69e38b849a9e7ca84794a5}
      \strng{bibnamehash}{07a27cac5e69e38b849a9e7ca84794a5}
      \strng{authorbibnamehash}{07a27cac5e69e38b849a9e7ca84794a5}
      \strng{authornamehash}{07a27cac5e69e38b849a9e7ca84794a5}
      \strng{authorfullhash}{07a27cac5e69e38b849a9e7ca84794a5}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of Causal Inference}
      \field{number}{2}
      \field{title}{Invariant causal prediction for nonlinear models}
      \field{volume}{6}
      \field{year}{2018}
    \endentry
    \entry{entnet}{article}{}
      \name{author}{5}{}{%
        {{hash=401562d96db136e78effff95552f584b}{%
           family={Henaff},
           familyi={H\bibinitperiod},
           given={Mikael},
           giveni={M\bibinitperiod}}}%
        {{hash=417a25f3511d7c21e76d4a15e67dd679}{%
           family={Weston},
           familyi={W\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=b31b3f03b1e1ee0eec6ff58f9a9df960}{%
           family={Szlam},
           familyi={S\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=f1733e10bf044cbd7be63e10e5689d78}{%
           family={Bordes},
           familyi={B\bibinitperiod},
           given={Antoine},
           giveni={A\bibinitperiod}}}%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{528f7432e133d1051cae290966217251}
      \strng{fullhash}{1308886ae793751c03f12c6f00d2d509}
      \strng{bibnamehash}{528f7432e133d1051cae290966217251}
      \strng{authorbibnamehash}{528f7432e133d1051cae290966217251}
      \strng{authornamehash}{528f7432e133d1051cae290966217251}
      \strng{authorfullhash}{1308886ae793751c03f12c6f00d2d509}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ICLR 2017}
      \field{title}{Tracking the World State with Recurrent Entity Networks}
      \field{year}{2017}
      \verb{eprint}
      \verb 1612.03969
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1612.03969v3:PDF
      \endverb
      \keyw{cs.CL}
    \endentry
    \entry{lstm}{article}{}
      \name{author}{2}{}{%
        {{hash=41b31e29fb2bdbf9f5c9c1b0d5b3e815}{%
           family={Hochreiter},
           familyi={H\bibinitperiod},
           given={Sepp},
           giveni={S\bibinitperiod}}}%
        {{hash=288bdbcfe1b91ad7484d7a24f74f99ed}{%
           family={Schmidhuber},
           familyi={S\bibinitperiod},
           given={Jürgen},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {{MIT} Press - Journals}%
      }
      \strng{namehash}{4c2e1e2e1ac91e1df9d4f7b85ebe39b4}
      \strng{fullhash}{4c2e1e2e1ac91e1df9d4f7b85ebe39b4}
      \strng{bibnamehash}{4c2e1e2e1ac91e1df9d4f7b85ebe39b4}
      \strng{authorbibnamehash}{4c2e1e2e1ac91e1df9d4f7b85ebe39b4}
      \strng{authornamehash}{4c2e1e2e1ac91e1df9d4f7b85ebe39b4}
      \strng{authorfullhash}{4c2e1e2e1ac91e1df9d4f7b85ebe39b4}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Neural Computation}
      \field{month}{11}
      \field{number}{8}
      \field{title}{Long Short-Term Memory}
      \field{volume}{9}
      \field{year}{1997}
      \field{pages}{1735\bibrangedash 1780}
      \range{pages}{46}
      \verb{doi}
      \verb 10.1162/neco.1997.9.8.1735
      \endverb
    \endentry
    \entry{statisticscausalinference}{article}{}
      \name{author}{1}{}{%
        {{hash=f77d8823946c674172702e2a9422f960}{%
           family={Holland},
           familyi={H\bibinitperiod},
           given={Paul\bibnamedelima W.},
           giveni={P\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Informa {UK} Limited}%
      }
      \strng{namehash}{f77d8823946c674172702e2a9422f960}
      \strng{fullhash}{f77d8823946c674172702e2a9422f960}
      \strng{bibnamehash}{f77d8823946c674172702e2a9422f960}
      \strng{authorbibnamehash}{f77d8823946c674172702e2a9422f960}
      \strng{authornamehash}{f77d8823946c674172702e2a9422f960}
      \strng{authorfullhash}{f77d8823946c674172702e2a9422f960}
      \field{sortinit}{H}
      \field{sortinithash}{6db6145dae8dc9e1271a8d556090b50a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of the American Statistical Association}
      \field{month}{12}
      \field{number}{396}
      \field{title}{Statistics and Causal Inference}
      \field{volume}{81}
      \field{year}{1986}
      \field{pages}{945\bibrangedash 960}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1080/01621459.1986.10478354
      \endverb
    \endentry
    \entry{adam}{article}{}
      \name{author}{2}{}{%
        {{hash=b6fbd171848aad4edf3925543f1f1522}{%
           family={Kingma},
           familyi={K\bibinitperiod},
           given={Diederik\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=8aa66e8231cc2fdbe67aa4f18ca970c6}{%
           family={Ba},
           familyi={B\bibinitperiod},
           given={Jimmy},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{fullhash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{bibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorbibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authornamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorfullhash}{a09df9f123146b8e2c7f1134c9496932}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ICLR}
      \field{title}{Adam: A Method for Stochastic Optimization}
      \field{year}{2015}
      \verb{eprint}
      \verb 1412.6980
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1412.6980v9:PDF
      \endverb
      \keyw{cs.LG}
    \endentry
    \entry{dmn}{inproceedings}{}
      \name{author}{9}{}{%
        {{hash=fdac29683eb8897db4a029583528dcac}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Ankit},
           giveni={A\bibinitperiod}}}%
        {{hash=ebcbf9bb0eadff8335a54c0f8b36b213}{%
           family={Irsoy},
           familyi={I\bibinitperiod},
           given={Ozan},
           giveni={O\bibinitperiod}}}%
        {{hash=9c74e14b54e2e2cfc30a15201e08cc1c}{%
           family={Ondruska},
           familyi={O\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
        {{hash=48ea6f65be60970c81190df53e86239e}{%
           family={Iyyer},
           familyi={I\bibinitperiod},
           given={Mohit},
           giveni={M\bibinitperiod}}}%
        {{hash=b75383e6b48c8360c7a60031424c85cf}{%
           family={Bradbury},
           familyi={B\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=94f7047ed067ed3ca0ce018501dd9db9}{%
           family={Gulrajani},
           familyi={G\bibinitperiod},
           given={Ishaan},
           giveni={I\bibinitperiod}}}%
        {{hash=b9e3c1ea1561174dcce1d2598b711a6d}{%
           family={Zhong},
           familyi={Z\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod}}}%
        {{hash=464e3fe7b58cca6f2b90e468168bcf21}{%
           family={Paulus},
           familyi={P\bibinitperiod},
           given={Romain},
           giveni={R\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{60556418857862dfd48ed1a29845436c}
      \strng{fullhash}{698948b0b56774b93d800752efe5b989}
      \strng{bibnamehash}{60556418857862dfd48ed1a29845436c}
      \strng{authorbibnamehash}{60556418857862dfd48ed1a29845436c}
      \strng{authornamehash}{60556418857862dfd48ed1a29845436c}
      \strng{authorfullhash}{698948b0b56774b93d800752efe5b989}
      \field{sortinit}{K}
      \field{sortinithash}{d3edc18d54b9438a72c24c925bfb38f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.}
      \field{booktitle}{ICML}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{title}{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}
      \field{year}{2016}
      \field{pages}{1378\bibrangedash 1387}
      \range{pages}{10}
      \verb{eprint}
      \verb 1506.07285
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1506.07285v5:PDF
      \endverb
      \keyw{cs.CL,cs.LG,cs.NE}
    \endentry
    \entry{cogmodelsymbolic}{article}{}
      \name{author}{1}{}{%
        {{hash=39722b0f7cf3e89609bff256eaec8e94}{%
           family={Lewis},
           familyi={L\bibinitperiod},
           given={Richard\bibnamedelima L.},
           giveni={R\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{39722b0f7cf3e89609bff256eaec8e94}
      \strng{fullhash}{39722b0f7cf3e89609bff256eaec8e94}
      \strng{bibnamehash}{39722b0f7cf3e89609bff256eaec8e94}
      \strng{authorbibnamehash}{39722b0f7cf3e89609bff256eaec8e94}
      \strng{authornamehash}{39722b0f7cf3e89609bff256eaec8e94}
      \strng{authorfullhash}{39722b0f7cf3e89609bff256eaec8e94}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{The MIT encyclopedia of the cognitive sciences}
      \field{title}{Cognitive modeling, symbolic}
      \field{year}{1999}
      \field{pages}{525\bibrangedash 527}
      \range{pages}{3}
    \endentry
    \entry{interpretability}{article}{}
      \name{author}{1}{}{%
        {{hash=599ae457cf41d4ce64e09433edbc964b}{%
           family={Lipton},
           familyi={L\bibinitperiod},
           given={Zachary\bibnamedelima C.},
           giveni={Z\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery ({ACM})}%
      }
      \strng{namehash}{599ae457cf41d4ce64e09433edbc964b}
      \strng{fullhash}{599ae457cf41d4ce64e09433edbc964b}
      \strng{bibnamehash}{599ae457cf41d4ce64e09433edbc964b}
      \strng{authorbibnamehash}{599ae457cf41d4ce64e09433edbc964b}
      \strng{authornamehash}{599ae457cf41d4ce64e09433edbc964b}
      \strng{authorfullhash}{599ae457cf41d4ce64e09433edbc964b}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{Communications of the {ACM}}
      \field{number}{10}
      \field{title}{The Mythos of Model Interpretability}
      \field{volume}{61}
      \field{year}{2018}
      \field{pages}{36\bibrangedash 43}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1145/3233231
      \endverb
      \verb{eprint}
      \verb 1606.03490
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1606.03490v3:PDF
      \endverb
      \keyw{cs.LG,cs.AI,cs.CV,cs.NE,stat.ML}
    \endentry
    \entry{gn2n}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=0138deaf332692ced30d823b9cebc488}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Fei},
           giveni={F\bibinitperiod}}}%
        {{hash=8feeff76343793ae5e3c4d7c2d39d3d8}{%
           family={Perez},
           familyi={P\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{dd5bd99a536532550f0dd6d2cc06beb4}
      \strng{fullhash}{dd5bd99a536532550f0dd6d2cc06beb4}
      \strng{bibnamehash}{dd5bd99a536532550f0dd6d2cc06beb4}
      \strng{authorbibnamehash}{dd5bd99a536532550f0dd6d2cc06beb4}
      \strng{authornamehash}{dd5bd99a536532550f0dd6d2cc06beb4}
      \strng{authorfullhash}{dd5bd99a536532550f0dd6d2cc06beb4}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.}
      \field{booktitle}{Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{title}{Gated End-to-End Memory Networks}
      \field{year}{2017}
      \field{pages}{1\bibrangedash 10}
      \range{pages}{10}
      \verb{doi}
      \verb 10.18653/v1/e17-1001
      \endverb
      \verb{eprint}
      \verb 1610.04211
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1610.04211v2:PDF
      \endverb
      \keyw{cs.CL,stat.ML}
    \endentry
    \entry{nmtatt}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=2d7f3674afa2647fbcb67bd41aaae2be}{%
           family={Luong},
           familyi={L\bibinitperiod},
           given={Thang},
           giveni={T\bibinitperiod}}}%
        {{hash=c7225c8adf8195ef056d44ae3f589af2}{%
           family={Pham},
           familyi={P\bibinitperiod},
           given={Hieu},
           giveni={H\bibinitperiod}}}%
        {{hash=2214edb8305f7ccd7cdc310b3a8ae1b4}{%
           family={Manning},
           familyi={M\bibinitperiod},
           given={Christopher\bibnamedelima D.},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{df0182e306f4cc3f7de3ae2568de3dd2}
      \strng{fullhash}{df0182e306f4cc3f7de3ae2568de3dd2}
      \strng{bibnamehash}{df0182e306f4cc3f7de3ae2568de3dd2}
      \strng{authorbibnamehash}{df0182e306f4cc3f7de3ae2568de3dd2}
      \strng{authornamehash}{df0182e306f4cc3f7de3ae2568de3dd2}
      \strng{authorfullhash}{df0182e306f4cc3f7de3ae2568de3dd2}
      \field{sortinit}{L}
      \field{sortinithash}{dad3efd0836470093a7b4a7bb756eb8c}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.}
      \field{booktitle}{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}
      \field{day}{17}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{month}{8}
      \field{title}{Effective Approaches to Attention-based Neural Machine Translation}
      \field{year}{2015}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.18653/v1/d15-1166
      \endverb
      \verb{eprint}
      \verb 1508.04025
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1508.04025v5:PDF
      \endverb
      \keyw{cs.CL}
    \endentry
    \entry{xaisocialsci}{article}{}
      \name{author}{1}{}{%
        {{hash=c41411769fd972add78f7652d57a6afc}{%
           family={Miller},
           familyi={M\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier {BV}}%
      }
      \strng{namehash}{c41411769fd972add78f7652d57a6afc}
      \strng{fullhash}{c41411769fd972add78f7652d57a6afc}
      \strng{bibnamehash}{c41411769fd972add78f7652d57a6afc}
      \strng{authorbibnamehash}{c41411769fd972add78f7652d57a6afc}
      \strng{authornamehash}{c41411769fd972add78f7652d57a6afc}
      \strng{authorfullhash}{c41411769fd972add78f7652d57a6afc}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
      \field{eprintclass}{cs.AI}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{Artificial Intelligence}
      \field{title}{Explanation in Artificial Intelligence: Insights from the Social Sciences}
      \field{volume}{267}
      \field{year}{2019}
      \field{pages}{1\bibrangedash 38}
      \range{pages}{38}
      \verb{doi}
      \verb 10.1016/j.artint.2018.07.007
      \endverb
      \verb{eprint}
      \verb 1706.07269
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1706.07269v3:PDF
      \endverb
      \keyw{cs.AI}
    \endentry
    \entry{languageofthought}{book}{}
      \name{author}{2}{}{%
        {{hash=5440f7bb633e3bf144c6296ff6a33da6}{%
           family={Morton},
           familyi={M\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
        {{hash=99a8aced15864fc92f53f1bb39897f06}{%
           family={Fodor},
           familyi={F\bibinitperiod},
           given={Jerry\bibnamedelima A.},
           giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Philosophy Documentation Center}%
      }
      \strng{namehash}{9d164818af4a1e41b9a330adc80d75ef}
      \strng{fullhash}{9d164818af4a1e41b9a330adc80d75ef}
      \strng{bibnamehash}{9d164818af4a1e41b9a330adc80d75ef}
      \strng{authorbibnamehash}{9d164818af4a1e41b9a330adc80d75ef}
      \strng{authornamehash}{9d164818af4a1e41b9a330adc80d75ef}
      \strng{authorfullhash}{9d164818af4a1e41b9a330adc80d75ef}
      \field{sortinit}{M}
      \field{sortinithash}{2e5c2f51f7fa2d957f3206819bf86dc3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{The Journal of Philosophy}
      \field{month}{3}
      \field{title}{The Language of Thought.}
      \field{volume}{75}
      \field{year}{1978}
      \field{pages}{161}
      \range{pages}{1}
      \verb{doi}
      \verb 10.2307/2025426
      \endverb
    \endentry
    \entry{pearlcausationcounterfactual}{article}{}
      \name{author}{1}{}{%
        {{hash=809f695b398afbb54b544c49e8d1bbbb}{%
           family={Pearl},
           familyi={P\bibinitperiod},
           given={Judea},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{fullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{bibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorbibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authornamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorfullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \field{extraname}{1}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Synthese}
      \field{number}{1-2}
      \field{title}{Probabilities of causation: three counterfactual interpretations and their identification}
      \field{volume}{121}
      \field{year}{1999}
      \field{pages}{93\bibrangedash 149}
      \range{pages}{57}
    \endentry
    \entry{pearlcausalinference}{article}{}
      \name{author}{1}{}{%
        {{hash=809f695b398afbb54b544c49e8d1bbbb}{%
           family={Pearl},
           familyi={P\bibinitperiod},
           given={Judea},
           giveni={J\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Technical Report, Communications of Association for Computing Machinery}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery ({ACM})}%
      }
      \strng{namehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{fullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{bibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorbibnamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authornamehash}{809f695b398afbb54b544c49e8d1bbbb}
      \strng{authorfullhash}{809f695b398afbb54b544c49e8d1bbbb}
      \field{extraname}{2}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Communications of the {ACM}}
      \field{month}{2}
      \field{number}{3}
      \field{title}{The Seven Tools of Causal Inference with Reflections on Machine Learning}
      \field{volume}{62}
      \field{year}{2019}
      \field{pages}{54\bibrangedash 60}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1145/3241036
      \endverb
    \endentry
    \entry{piagetsymthought}{book}{}
      \name{author}{1}{}{%
        {{hash=76cd658e2f3ecbeedfaceb2d2a1de596}{%
           family={Piaget},
           familyi={P\bibinitperiod},
           given={Jean},
           giveni={J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Routledge}%
      }
      \strng{namehash}{76cd658e2f3ecbeedfaceb2d2a1de596}
      \strng{fullhash}{76cd658e2f3ecbeedfaceb2d2a1de596}
      \strng{bibnamehash}{76cd658e2f3ecbeedfaceb2d2a1de596}
      \strng{authorbibnamehash}{76cd658e2f3ecbeedfaceb2d2a1de596}
      \strng{authornamehash}{76cd658e2f3ecbeedfaceb2d2a1de596}
      \strng{authorfullhash}{76cd658e2f3ecbeedfaceb2d2a1de596}
      \field{sortinit}{P}
      \field{sortinithash}{bb5b15f2db90f7aef79bb9e83defefcb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{17}
      \field{isbn}{0415254019}
      \field{month}{5}
      \field{pagetotal}{216}
      \field{title}{The Psychology of Intelligence}
      \field{year}{2001}
      \field{dateera}{ce}
    \endentry
    \entry{leastcommongeneraliser}{article}{}
      \name{author}{1}{}{%
        {{hash=34c2c5f9ff95cb3442400d374c40483b}{%
           family={Reynolds},
           familyi={R\bibinitperiod},
           given={John\bibnamedelima C.},
           giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Edinburgh University Press}%
      }
      \strng{namehash}{34c2c5f9ff95cb3442400d374c40483b}
      \strng{fullhash}{34c2c5f9ff95cb3442400d374c40483b}
      \strng{bibnamehash}{34c2c5f9ff95cb3442400d374c40483b}
      \strng{authorbibnamehash}{34c2c5f9ff95cb3442400d374c40483b}
      \strng{authornamehash}{34c2c5f9ff95cb3442400d374c40483b}
      \strng{authorfullhash}{34c2c5f9ff95cb3442400d374c40483b}
      \field{sortinit}{R}
      \field{sortinithash}{b9c68a358aea118dfa887b6e902414a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Machine intelligence}
      \field{title}{Transformational systems and algebraic structure of atomic formulas}
      \field{volume}{5}
      \field{year}{1970}
      \field{pages}{135\bibrangedash 151}
      \range{pages}{17}
    \endentry
    \entry{timntp}{article}{}
      \name{author}{2}{}{%
        {{hash=b3ff4029884ead9928954ea33714192c}{%
           family={Rocktäschel},
           familyi={R\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=6cd024b217f3f2f8126e253b8c84d24d}{%
           family={Riedel},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{dc0eaa012862d6e86f3158d9cb10e35c}
      \strng{fullhash}{dc0eaa012862d6e86f3158d9cb10e35c}
      \strng{bibnamehash}{dc0eaa012862d6e86f3158d9cb10e35c}
      \strng{authorbibnamehash}{dc0eaa012862d6e86f3158d9cb10e35c}
      \strng{authornamehash}{dc0eaa012862d6e86f3158d9cb10e35c}
      \strng{authorfullhash}{dc0eaa012862d6e86f3158d9cb10e35c}
      \field{sortinit}{R}
      \field{sortinithash}{b9c68a358aea118dfa887b6e902414a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.}
      \field{booktitle}{NIPS}
      \field{day}{31}
      \field{eprintclass}{cs.NE}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{NIPS}
      \field{month}{5}
      \field{title}{End-to-End Differentiable Proving}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{3791\bibrangedash 3803}
      \range{pages}{13}
      \verb{eprint}
      \verb 1705.11040
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1705.11040v2:PDF
      \endverb
      \keyw{cs.NE,cs.AI,cs.LG,cs.LO}
    \endentry
    \entry{counterfactualthinking}{article}{}
      \name{author}{1}{}{%
        {{hash=65f4dff341bc1087652068a29be3a3a1}{%
           family={Roese},
           familyi={R\bibinitperiod},
           given={Neal\bibnamedelima J.},
           giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {American Psychological Association ({APA})}%
      }
      \strng{namehash}{65f4dff341bc1087652068a29be3a3a1}
      \strng{fullhash}{65f4dff341bc1087652068a29be3a3a1}
      \strng{bibnamehash}{65f4dff341bc1087652068a29be3a3a1}
      \strng{authorbibnamehash}{65f4dff341bc1087652068a29be3a3a1}
      \strng{authornamehash}{65f4dff341bc1087652068a29be3a3a1}
      \strng{authorfullhash}{65f4dff341bc1087652068a29be3a3a1}
      \field{sortinit}{R}
      \field{sortinithash}{b9c68a358aea118dfa887b6e902414a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Psychological Bulletin}
      \field{number}{1}
      \field{title}{Counterfactual thinking.}
      \field{volume}{121}
      \field{year}{1997}
      \field{pages}{133\bibrangedash 148}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1037/0033-2909.121.1.133
      \endverb
    \endentry
    \entry{russell2016artificial}{book}{}
      \name{author}{2}{}{%
        {{hash=143fa183327d9fcd9de18eec99d6ca97}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Stuart},
           giveni={S\bibinitperiod}}}%
        {{hash=5de798d5fa3c0236c0478134cd23f52a}{%
           family={Norvig},
           familyi={N\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Addison Wesley}%
      }
      \strng{namehash}{b280605b721b4ebcba5395298499f924}
      \strng{fullhash}{b280605b721b4ebcba5395298499f924}
      \strng{bibnamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authorbibnamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authornamehash}{b280605b721b4ebcba5395298499f924}
      \strng{authorfullhash}{b280605b721b4ebcba5395298499f924}
      \field{sortinit}{R}
      \field{sortinithash}{b9c68a358aea118dfa887b6e902414a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{28}
      \field{isbn}{1292153962}
      \field{month}{11}
      \field{title}{Artificial Intelligence: A Modern Approach, Global Edition}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{urlraw}
      \verb https://www.ebook.de/de/product/25939961/stuart_russell_peter_norvig_artificial_intelligence_a_modern_approach_global_edition.html
      \endverb
      \verb{url}
      \verb https://www.ebook.de/de/product/25939961/stuart_russell_peter_norvig_artificial_intelligence_a_modern_approach_global_edition.html
      \endverb
    \endentry
    \entry{chineseroom}{article}{}
      \name{author}{1}{}{%
        {{hash=932676a29fb955b21e5599a8c1c5e6ca}{%
           family={Searle},
           familyi={S\bibinitperiod},
           given={John\bibnamedelima R.},
           giveni={J\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{932676a29fb955b21e5599a8c1c5e6ca}
      \strng{fullhash}{932676a29fb955b21e5599a8c1c5e6ca}
      \strng{bibnamehash}{932676a29fb955b21e5599a8c1c5e6ca}
      \strng{authorbibnamehash}{932676a29fb955b21e5599a8c1c5e6ca}
      \strng{authornamehash}{932676a29fb955b21e5599a8c1c5e6ca}
      \strng{authorfullhash}{932676a29fb955b21e5599a8c1c5e6ca}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Behavioral and brain sciences}
      \field{number}{3}
      \field{title}{Minds, brains, and programs}
      \field{volume}{3}
      \field{year}{1980}
      \field{pages}{417\bibrangedash 424}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1016/b978-1-4832-1446-7.50007-8
      \endverb
    \endentry
    \entry{gradcam}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=2bb65ae1ae492d8a5f0410b495e608cf}{%
           family={Selvaraju},
           familyi={S\bibinitperiod},
           given={Ramprasaath\bibnamedelima R.},
           giveni={R\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=615d55ccf6f32ec74ad65001e3a4ebbb}{%
           family={Cogswell},
           familyi={C\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
        {{hash=95fdc805087b8b7c1bb60f09d4ee2641}{%
           family={Das},
           familyi={D\bibinitperiod},
           given={Abhishek},
           giveni={A\bibinitperiod}}}%
        {{hash=485265d172ca1779fa9691a5b6091cc8}{%
           family={Vedantam},
           familyi={V\bibinitperiod},
           given={Ramakrishna},
           giveni={R\bibinitperiod}}}%
        {{hash=d0ab7a4601951a0c59352f1dcb531afb}{%
           family={Parikh},
           familyi={P\bibinitperiod},
           given={Devi},
           giveni={D\bibinitperiod}}}%
        {{hash=d0a408119272ca1bd49b625e3b927d05}{%
           family={Batra},
           familyi={B\bibinitperiod},
           given={Dhruv},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{44b36a9d8aed667d91631e9717a56509}
      \strng{fullhash}{aa15df90de6a99f6dc73b935fda62089}
      \strng{bibnamehash}{44b36a9d8aed667d91631e9717a56509}
      \strng{authorbibnamehash}{44b36a9d8aed667d91631e9717a56509}
      \strng{authornamehash}{44b36a9d8aed667d91631e9717a56509}
      \strng{authorfullhash}{aa15df90de6a99f6dc73b935fda62089}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{2017 {IEEE} International Conference on Computer Vision ({ICCV})}
      \field{month}{10}
      \field{title}{Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization}
      \field{year}{2017}
      \field{pages}{618\bibrangedash 626}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/iccv.2017.74
      \endverb
    \endentry
    \entry{qrn}{article}{}
      \name{author}{4}{}{%
        {{hash=c6a70b90cc23e43aa6914a271ac0004d}{%
           family={Seo},
           familyi={S\bibinitperiod},
           given={Minjoon},
           giveni={M\bibinitperiod}}}%
        {{hash=28789ec78f6ba72f59baf40dfb377588}{%
           family={Min},
           familyi={M\bibinitperiod},
           given={Sewon},
           giveni={S\bibinitperiod}}}%
        {{hash=396c6ddedb6f986906fc3e4994d19974}{%
           family={Farhadi},
           familyi={F\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
        {{hash=e9dafefb58a6a6e1e496fdf8928b3eeb}{%
           family={Hajishirzi},
           familyi={H\bibinitperiod},
           given={Hannaneh},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{914d0909687202a05edc4c33359dc32c}
      \strng{fullhash}{eb31072afef20199053992bed9ce2fd4}
      \strng{bibnamehash}{914d0909687202a05edc4c33359dc32c}
      \strng{authorbibnamehash}{914d0909687202a05edc4c33359dc32c}
      \strng{authornamehash}{914d0909687202a05edc4c33359dc32c}
      \strng{authorfullhash}{eb31072afef20199053992bed9ce2fd4}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ICLR}
      \field{title}{Query-Reduction Networks for Question Answering}
      \field{year}{2017}
      \verb{eprint}
      \verb 1606.04582
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1606.04582v6:PDF
      \endverb
      \keyw{cs.CL,cs.NE}
    \endentry
    \entry{lcgfromfacts}{book}{}
      \name{author}{1}{}{%
        {{hash=2c4fa58cdb2e4b928b69f41b192b2535}{%
           family={Shapiro},
           familyi={S\bibinitperiod},
           given={Ehud\bibnamedelima Y.},
           giveni={E\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Yale University, Department of Computer Science}%
      }
      \strng{namehash}{2c4fa58cdb2e4b928b69f41b192b2535}
      \strng{fullhash}{2c4fa58cdb2e4b928b69f41b192b2535}
      \strng{bibnamehash}{2c4fa58cdb2e4b928b69f41b192b2535}
      \strng{authorbibnamehash}{2c4fa58cdb2e4b928b69f41b192b2535}
      \strng{authornamehash}{2c4fa58cdb2e4b928b69f41b192b2535}
      \strng{authorfullhash}{2c4fa58cdb2e4b928b69f41b192b2535}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Inductive inference of theories from facts}
      \field{year}{1981}
    \endentry
    \entry{sochersentiment}{inproceedings}{}
      \name{author}{7}{}{%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=d353b3b0ddb25592692eb6cb7c08cc1d}{%
           family={Perelygin},
           familyi={P\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=d5565a209f6dff10d31d090cf8d77853}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Jean},
           giveni={J\bibinitperiod}}}%
        {{hash=8e5c02ba2cc4095aa510449351c450ee}{%
           family={Chuang},
           familyi={C\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=2214edb8305f7ccd7cdc310b3a8ae1b4}{%
           family={Manning},
           familyi={M\bibinitperiod},
           given={Christopher\bibnamedelima D},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=49e889356ff39df159461bc2895c7e16}{%
           family={Ng},
           familyi={N\bibinitperiod},
           given={Andrew\bibnamedelima Y},
           giveni={A\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=699d626ef41a1a827d623529c1a45e5a}{%
           family={Potts},
           familyi={P\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{e7c60479e674fc5df9f60993d72cca39}
      \strng{fullhash}{bf7636803a50b22d7abc949eb9fbd699}
      \strng{bibnamehash}{e7c60479e674fc5df9f60993d72cca39}
      \strng{authorbibnamehash}{e7c60479e674fc5df9f60993d72cca39}
      \strng{authornamehash}{e7c60479e674fc5df9f60993d72cca39}
      \strng{authorfullhash}{bf7636803a50b22d7abc949eb9fbd699}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of the 2013 conference on empirical methods in natural language processing}
      \field{title}{Recursive deep models for semantic compositionality over a sentiment treebank}
      \field{year}{2013}
      \field{pages}{1631\bibrangedash 1642}
      \range{pages}{12}
    \endentry
    \entry{liftedneuralnetworks}{article}{}
      \name{author}{4}{}{%
        {{hash=6ae4bd00647c70297fb91cf5763bce2d}{%
           family={Sourek},
           familyi={S\bibinitperiod},
           given={Gustav},
           giveni={G\bibinitperiod}}}%
        {{hash=5c4a0cba6aaf4a029804abd79d7b6127}{%
           family={Aschenbrenner},
           familyi={A\bibinitperiod},
           given={Vojtech},
           giveni={V\bibinitperiod}}}%
        {{hash=7ca371de6e830c6a0233904c0f89738a}{%
           family={Zelezny},
           familyi={Z\bibinitperiod},
           given={Filip},
           giveni={F\bibinitperiod}}}%
        {{hash=18a7d7a7766a37c7915fddb7758c172b}{%
           family={Kuzelka},
           familyi={K\bibinitperiod},
           given={Ondrej},
           giveni={O\bibinitperiod}}}%
      }
      \strng{namehash}{9a654795c70c1d2e0a8e356d775003a8}
      \strng{fullhash}{27651580a6e231ea42998481098309e5}
      \strng{bibnamehash}{9a654795c70c1d2e0a8e356d775003a8}
      \strng{authorbibnamehash}{9a654795c70c1d2e0a8e356d775003a8}
      \strng{authornamehash}{9a654795c70c1d2e0a8e356d775003a8}
      \strng{authorfullhash}{27651580a6e231ea42998481098309e5}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.}
      \field{day}{20}
      \field{eprintclass}{cs.AI}
      \field{eprinttype}{arXiv}
      \field{month}{8}
      \field{title}{Lifted Relational Neural Networks}
      \field{year}{2015}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 1508.05128
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1508.05128v2:PDF
      \endverb
      \keyw{cs.AI,cs.LG,cs.NE}
    \endentry
    \entry{numberbatch}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=4d757257b940c16e71cac823afe9b2e3}{%
           family={Speer},
           familyi={S\bibinitperiod},
           given={Robyn},
           giveni={R\bibinitperiod}}}%
        {{hash=2b3c1fd1a7f952fbf0ec71e46d60d749}{%
           family={Chin},
           familyi={C\bibinitperiod},
           given={Joshua},
           giveni={J\bibinitperiod}}}%
        {{hash=17886ebb8b9a8fb21c809ebeda86995c}{%
           family={Havasi},
           familyi={H\bibinitperiod},
           given={Catherine},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{c42dca8bccfa26ad07dd2768ca2916d6}
      \strng{fullhash}{c42dca8bccfa26ad07dd2768ca2916d6}
      \strng{bibnamehash}{c42dca8bccfa26ad07dd2768ca2916d6}
      \strng{authorbibnamehash}{c42dca8bccfa26ad07dd2768ca2916d6}
      \strng{authornamehash}{c42dca8bccfa26ad07dd2768ca2916d6}
      \strng{authorfullhash}{c42dca8bccfa26ad07dd2768ca2916d6}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{AAAI 31 (2017) 4444-4451}
      \field{title}{{ConceptNet} 5.5: An Open Multilingual Graph of General Knowledge}
      \field{year}{2017}
      \field{pages}{4444\bibrangedash 4451}
      \range{pages}{8}
      \verb{eprint}
      \verb 1612.03975
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1612.03975v2:PDF
      \endverb
      \keyw{cs.CL,I.2.7}
    \endentry
    \entry{memn2n}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=72c54cfe76db45d84a3413df9e903ff4}{%
           family={Sukhbaatar},
           familyi={S\bibinitperiod},
           given={Sainbayar},
           giveni={S\bibinitperiod}}}%
        {{hash=b31b3f03b1e1ee0eec6ff58f9a9df960}{%
           family={Szlam},
           familyi={S\bibinitperiod},
           given={Arthur},
           giveni={A\bibinitperiod}}}%
        {{hash=417a25f3511d7c21e76d4a15e67dd679}{%
           family={Weston},
           familyi={W\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=a6784304d1cc890b2cb6c6c7f2f3fd76}{%
           family={Fergus},
           familyi={F\bibinitperiod},
           given={Rob},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{fullhash}{c1747fc37d20f3babae706cfe6829ee1}
      \strng{bibnamehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{authorbibnamehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{authornamehash}{140eda2c6b05153d13195f7f9b9bcfed}
      \strng{authorfullhash}{c1747fc37d20f3babae706cfe6829ee1}
      \field{sortinit}{S}
      \field{sortinithash}{c319cff79d99c853d775f88277d4e45f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.}
      \field{booktitle}{NIPS}
      \field{day}{31}
      \field{eprintclass}{cs.NE}
      \field{eprinttype}{arXiv}
      \field{month}{3}
      \field{title}{End-To-End Memory Networks}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{pages}{2440\bibrangedash 2448}
      \range{pages}{9}
      \verb{eprint}
      \verb 1503.08895
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1503.08895v5:PDF
      \endverb
      \keyw{cs.NE,cs.CL}
    \endentry
    \entry{chainer}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=00aa5ab0f3de8d9e1d3f6f0ebb11f16d}{%
           family={Tokui},
           familyi={T\bibinitperiod},
           given={Seiya},
           giveni={S\bibinitperiod}}}%
        {{hash=9bdf179d7f4efcc82ff3d771aa9e1003}{%
           family={Oono},
           familyi={O\bibinitperiod},
           given={Kenta},
           giveni={K\bibinitperiod}}}%
        {{hash=fffd3afc634974630f83f8b19b02a14e}{%
           family={Hido},
           familyi={H\bibinitperiod},
           given={Shohei},
           giveni={S\bibinitperiod}}}%
        {{hash=49a54d314e1ac4274a9574ef1ccd0e32}{%
           family={Clayton},
           familyi={C\bibinitperiod},
           given={Justin},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{eeef67582aa20c6fca3f21376c56aea5}
      \strng{fullhash}{7d03de91bb355f1ff464a733e42562c9}
      \strng{bibnamehash}{eeef67582aa20c6fca3f21376c56aea5}
      \strng{authorbibnamehash}{eeef67582aa20c6fca3f21376c56aea5}
      \strng{authornamehash}{eeef67582aa20c6fca3f21376c56aea5}
      \strng{authorfullhash}{7d03de91bb355f1ff464a733e42562c9}
      \field{sortinit}{T}
      \field{sortinithash}{51f9faf24c60c62ca764a77f78cf5666}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)}
      \field{title}{Chainer: a Next-Generation Open Source Framework for Deep Learning}
      \field{year}{2015}
      \verb{urlraw}
      \verb http://learningsys.org/papers/LearningSys_2015_paper_33.pdf
      \endverb
      \verb{url}
      \verb http://learningsys.org/papers/LearningSys_2015_paper_33.pdf
      \endverb
    \endentry
    \entry{coevoflangandbrain}{book}{}
      \name{author}{2}{}{%
        {{hash=6c566acbec13a42c586f8a8aa28caa39}{%
           family={Unger},
           familyi={U\bibinitperiod},
           given={J.\bibnamedelimi Marshall},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=1c7f1f30538deb25d24534e7ac9ea234}{%
           family={Deacon},
           familyi={D\bibinitperiod},
           given={Terrence\bibnamedelima W.},
           giveni={T\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Wiley}%
      }
      \strng{namehash}{fbbb44ef63d92f5d235f34ebb5dab1fe}
      \strng{fullhash}{fbbb44ef63d92f5d235f34ebb5dab1fe}
      \strng{bibnamehash}{fbbb44ef63d92f5d235f34ebb5dab1fe}
      \strng{authorbibnamehash}{fbbb44ef63d92f5d235f34ebb5dab1fe}
      \strng{authornamehash}{fbbb44ef63d92f5d235f34ebb5dab1fe}
      \strng{authorfullhash}{fbbb44ef63d92f5d235f34ebb5dab1fe}
      \field{sortinit}{U}
      \field{sortinithash}{77a6935510e008adcf5b555e7b4f0711}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{The Modern Language Journal}
      \field{title}{The Symbolic Species: The Co-Evolution of Language and the Brain}
      \field{volume}{82}
      \field{year}{1998}
      \field{pages}{437}
      \range{pages}{1}
      \verb{doi}
      \verb 10.2307/329984
      \endverb
    \endentry
    \entry{memnn}{article}{}
      \name{author}{3}{}{%
        {{hash=417a25f3511d7c21e76d4a15e67dd679}{%
           family={Weston},
           familyi={W\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=83e9081b3be58a20d597b22b70648e30}{%
           family={Chopra},
           familyi={C\bibinitperiod},
           given={Sumit},
           giveni={S\bibinitperiod}}}%
        {{hash=f1733e10bf044cbd7be63e10e5689d78}{%
           family={Bordes},
           familyi={B\bibinitperiod},
           given={Antoine},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{16241d235ff39923179fc96c9fe2e8ee}
      \strng{fullhash}{16241d235ff39923179fc96c9fe2e8ee}
      \strng{bibnamehash}{16241d235ff39923179fc96c9fe2e8ee}
      \strng{authorbibnamehash}{16241d235ff39923179fc96c9fe2e8ee}
      \strng{authornamehash}{16241d235ff39923179fc96c9fe2e8ee}
      \strng{authorfullhash}{16241d235ff39923179fc96c9fe2e8ee}
      \field{sortinit}{W}
      \field{sortinithash}{1af34bd8c148ffb32de1494636b49713}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.}
      \field{eprintclass}{cs.AI}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ICLR}
      \field{title}{Memory Networks}
      \field{year}{2015}
      \verb{eprint}
      \verb 1410.3916
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1410.3916v11:PDF
      \endverb
      \keyw{cs.AI,cs.CL,stat.ML}
    \endentry
    \entry{babi}{article}{}
      \name{author}{7}{}{%
        {{hash=417a25f3511d7c21e76d4a15e67dd679}{%
           family={Weston},
           familyi={W\bibinitperiod},
           given={Jason},
           giveni={J\bibinitperiod}}}%
        {{hash=f1733e10bf044cbd7be63e10e5689d78}{%
           family={Bordes},
           familyi={B\bibinitperiod},
           given={Antoine},
           giveni={A\bibinitperiod}}}%
        {{hash=83e9081b3be58a20d597b22b70648e30}{%
           family={Chopra},
           familyi={C\bibinitperiod},
           given={Sumit},
           giveni={S\bibinitperiod}}}%
        {{hash=f242a0a9b3b93ccf3879646ed9ab662f}{%
           family={Rush},
           familyi={R\bibinitperiod},
           given={Alexander\bibnamedelima M.},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=f0f9993391f26a73b94828bfc6fb784f}{%
           family={Merriënboer},
           familyi={M\bibinitperiod},
           given={Bart},
           giveni={B\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=977d047821122d1c2e7aa855c30c8cf2}{%
           family={Joulin},
           familyi={J\bibinitperiod},
           given={Armand},
           giveni={A\bibinitperiod}}}%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{7772e4fb5bc277a5e9983ae2ef9e3801}
      \strng{fullhash}{aee7dd01fc047042bb18dc0be2fa9512}
      \strng{bibnamehash}{7772e4fb5bc277a5e9983ae2ef9e3801}
      \strng{authorbibnamehash}{7772e4fb5bc277a5e9983ae2ef9e3801}
      \strng{authornamehash}{7772e4fb5bc277a5e9983ae2ef9e3801}
      \strng{authorfullhash}{aee7dd01fc047042bb18dc0be2fa9512}
      \field{sortinit}{W}
      \field{sortinithash}{1af34bd8c148ffb32de1494636b49713}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.}
      \field{eprintclass}{cs.AI}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ICLR}
      \field{title}{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}
      \field{year}{2016}
      \verb{eprint}
      \verb 1502.05698
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1502.05698v10:PDF
      \endverb
      \keyw{cs.AI,cs.CL,stat.ML}
    \endentry
    \entry{dmnvisual}{article}{}
      \name{author}{3}{}{%
        {{hash=1fa4dfdd5ecf049d39575fe7ba1ec9f3}{%
           family={Xiong},
           familyi={X\bibinitperiod},
           given={Caiming},
           giveni={C\bibinitperiod}}}%
        {{hash=2bcb1b2c74f1dd53259ace692fc64b56}{%
           family={Merity},
           familyi={M\bibinitperiod},
           given={Stephen},
           giveni={S\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{b288bc2addace34c65233650c5d08215}
      \strng{fullhash}{b288bc2addace34c65233650c5d08215}
      \strng{bibnamehash}{b288bc2addace34c65233650c5d08215}
      \strng{authorbibnamehash}{b288bc2addace34c65233650c5d08215}
      \strng{authornamehash}{b288bc2addace34c65233650c5d08215}
      \strng{authorfullhash}{b288bc2addace34c65233650c5d08215}
      \field{sortinit}{X}
      \field{sortinithash}{e90038f30fa4b9ce59606fc8347e3cc7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \babi-10k text question-answering dataset without supporting fact supervision.}
      \field{booktitle}{ICML}
      \field{day}{4}
      \field{eprintclass}{cs.NE}
      \field{eprinttype}{arXiv}
      \field{journaltitle}{ICML}
      \field{month}{3}
      \field{title}{Dynamic Memory Networks for Visual and Textual Question Answering}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{pages}{2397\bibrangedash 2406}
      \range{pages}{10}
      \verb{eprint}
      \verb 1603.01417
      \endverb
      \verb{file}
      \verb :http\://arxiv.org/pdf/1603.01417v1:PDF
      \endverb
      \keyw{cs.NE,cs.CL,cs.CV}
    \endentry
  \enddatalist
\endrefsection
\endinput

