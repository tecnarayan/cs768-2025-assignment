\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[wu2(2018)]{wu2018error}
Error compensated quantized sgd and its applications to large-scale distributed
  optimization.
\newblock \emph{arXiv preprint arXiv:1806.08054}, 2018.

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pages 265--283, 2016.

\bibitem[Agarwal and Duchi(2011)]{agarwal2011distributed}
A.~Agarwal and J.~C. Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  873--881, 2011.

\bibitem[Alistarh et~al.(2018)Alistarh, Allen-Zhu, and
  Li]{Alistarh2018ByzantineSG}
D.~Alistarh, Z.~Allen-Zhu, and J.~Li.
\newblock Byzantine stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1803.08917}, 2018.

\bibitem[Blanchard et~al.(2017)Blanchard, Guerraoui, Stainer,
  et~al.]{blanchard2017machine}
P.~Blanchard, R.~Guerraoui, J.~Stainer, et~al.
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  119--129, 2017.

\bibitem[Blot et~al.(2016)Blot, Picard, Cord, and Thome]{blot2016gossip}
M.~Blot, D.~Picard, M.~Cord, and N.~Thome.
\newblock Gossip training for deep learning.
\newblock \emph{arXiv preprint arXiv:1611.09726}, 2016.

\bibitem[Boyd et~al.(2006)Boyd, Ghosh, Prabhakar, and Shah]{boyd2006randomized}
S.~Boyd, A.~Ghosh, B.~Prabhakar, and D.~Shah.
\newblock Randomized gossip algorithms.
\newblock \emph{IEEE transactions on information theory}, 52\penalty0
  (6):\penalty0 2508--2530, 2006.

\bibitem[Colin et~al.(2016)Colin, Bellet, Salmon, and
  Cl{\'e}men{\c{c}}on]{colin2016gossip}
I.~Colin, A.~Bellet, J.~Salmon, and S.~Cl{\'e}men{\c{c}}on.
\newblock Gossip dual averaging for decentralized optimization of pairwise
  functions.
\newblock \emph{arXiv preprint arXiv:1606.02421}, 2016.

\bibitem[Daily et~al.(2018)Daily, Vishnu, Siegel, Warfel, and
  Amatya]{daily2018gossipgrad}
J.~Daily, A.~Vishnu, C.~Siegel, T.~Warfel, and V.~Amatya.
\newblock Gossipgrad: Scalable deep learning using gossip communication based
  asynchronous gradient descent.
\newblock \emph{arXiv preprint arXiv:1803.05880}, 2018.

\bibitem[Drumond et~al.(2018)Drumond, LIN, Jaggi, and Falsafi]{NIPS2018_7327}
M.~Drumond, T.~LIN, M.~Jaggi, and B.~Falsafi.
\newblock Training dnns with hybrid block floating point.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 451--461. Curran Associates, Inc., 2018.

\bibitem[Goldstein et~al.(2016)Goldstein, Taylor, Barabin, and
  Sayre]{goldstein2016unwrapping}
T.~Goldstein, G.~Taylor, K.~Barabin, and K.~Sayre.
\newblock Unwrapping admm: efficient distributed computing via transpose
  reduction.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1151--1158,
  2016.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch sgd: training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Hajinezhad et~al.(2016)Hajinezhad, Hong, Zhao, and
  Wang]{hajinezhad2016nestt}
D.~Hajinezhad, M.~Hong, T.~Zhao, and Z.~Wang.
\newblock Nestt: A nonconvex primal-dual splitting method for distributed and
  stochastic optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3215--3223, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2018)He, Bian, and Jaggi]{He:2018aa}
L.~He, A.~Bian, and M.~Jaggi.
\newblock Cola: Decentralized linear learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4541--4551, 2018.

\bibitem[Jiang et~al.(2017)Jiang, Cui, Zhang, and
  Yu]{Jiang:2017:HDP:3035918.3035933}
J.~Jiang, B.~Cui, C.~Zhang, and L.~Yu.
\newblock Heterogeneity-aware distributed parameter servers.
\newblock In \emph{Proceedings of the 2017 ACM International Conference on
  Management of Data}, SIGMOD '17, pages 463--478, New York, NY, USA, 2017.
  ACM.
\newblock ISBN 978-1-4503-4197-4.
\newblock \doi{10.1145/3035918.3035933}.
\newblock URL \url{http://doi.acm.org/10.1145/3035918.3035933}.

\bibitem[Jin et~al.(2016)Jin, Yuan, Iandola, and Keutzer]{jin2016scale}
P.~H. Jin, Q.~Yuan, F.~Iandola, and K.~Keutzer.
\newblock How to scale distributed deep learning?
\newblock \emph{arXiv preprint arXiv:1611.04581}, 2016.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lan et~al.(2017)Lan, Lee, and Zhou]{lan2017communication}
G.~Lan, S.~Lee, and Y.~Zhou.
\newblock Communication-efficient algorithms for decentralized and stochastic
  optimization.
\newblock \emph{arXiv preprint arXiv:1701.03961}, 2017.

\bibitem[Leblond et~al.(2016)Leblond, Pedregosa, and
  Lacoste-Julien]{leblond2016asaga}
R.~Leblond, F.~Pedregosa, and S.~Lacoste-Julien.
\newblock Asaga: asynchronous parallel saga.
\newblock \emph{arXiv preprint arXiv:1606.04809}, 2016.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
M.~Li, D.~G. Andersen, J.~W. Park, A.~J. Smola, A.~Ahmed, V.~Josifovski,
  J.~Long, E.~J. Shekita, and B.-Y. Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{OSDI}, volume~14, pages 583--598, 2014.

\bibitem[Li and Zhang(2010)]{li2010consensus}
T.~Li and J.-F. Zhang.
\newblock Consensus conditions of multi-agent systems with time-varying
  topologies and stochastic communication noises.
\newblock \emph{IEEE Transactions on Automatic Control}, 55\penalty0
  (9):\penalty0 2043--2057, 2010.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
X.~Lian, Y.~Huang, Y.~Li, and J.~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2737--2745, 2015.

\bibitem[Lian et~al.(2017{\natexlab{a}})Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
X.~Lian, C.~Zhang, H.~Zhang, C.-J. Hsieh, W.~Zhang, and J.~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5336--5346, 2017{\natexlab{a}}.

\bibitem[Lian et~al.(2017{\natexlab{b}})Lian, Zhang, Zhang, and
  Liu]{lian2017asynchronous}
X.~Lian, W.~Zhang, C.~Zhang, and J.~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1710.06952}, 2017{\natexlab{b}}.

\bibitem[Lin et~al.(2018)Lin, Stich, and Jaggi]{Lin:2018aa}
T.~Lin, S.~U. Stich, and M.~Jaggi.
\newblock Don't use large mini-batches, use local sgd.
\newblock \emph{arXiv preprint arXiv:1808.07217}, 2018.

\bibitem[Lobel and Ozdaglar(2011)]{lobel2011distributed}
I.~Lobel and A.~Ozdaglar.
\newblock Distributed subgradient methods for convex optimization over random
  networks.
\newblock \emph{IEEE Transactions on Automatic Control}, 56\penalty0
  (6):\penalty0 1291, 2011.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson,
  et~al.]{mcmahan2016communication}
H.~B. McMahan, E.~Moore, D.~Ramage, S.~Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Nedi{\'c} and Olshevsky(2015)]{nedic2015distributed}
A.~Nedi{\'c} and A.~Olshevsky.
\newblock Distributed optimization over time-varying directed graphs.
\newblock \emph{IEEE Transactions on Automatic Control}, 60\penalty0
  (3):\penalty0 601--615, 2015.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
A.~Nedic, A.~Olshevsky, and W.~Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
B.~Recht, C.~Re, S.~Wright, and F.~Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  693--701, 2011.

\bibitem[Renggli et~al.(2018)Renggli, Alistarh, and
  Hoefler]{renggli2018sparcml}
C.~Renggli, D.~Alistarh, and T.~Hoefler.
\newblock Sparcml: High-performance sparse communication for machine learning.
\newblock \emph{arXiv preprint arXiv:1802.08021}, 2018.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
K.~Scaman, F.~Bach, S.~Bubeck, Y.~T. Lee, and L.~Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock \emph{arXiv preprint arXiv:1702.08704}, 2017.

\bibitem[Seide and Agarwal(2016)]{seide2016cntk}
F.~Seide and A.~Agarwal.
\newblock Cntk: Microsoft's open-source deep-learning toolkit.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 2135--2135. ACM, 2016.

\bibitem[Shen et~al.(2018{\natexlab{a}})Shen, Mokhtari, Zhou, Zhao, and
  Qian]{pmlr-v80-shen18a}
Z.~Shen, A.~Mokhtari, T.~Zhou, P.~Zhao, and H.~Qian.
\newblock Towards more efficient stochastic decentralized learning: Faster
  convergence and sparse communication.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 4624--4633, Stockholmsm{\"a}ssan,
  Stockholm Sweden, 10--15 Jul 2018{\natexlab{a}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/shen18a.html}.

\bibitem[Shen et~al.(2018{\natexlab{b}})Shen, Mokhtari, Zhou, Zhao, and
  Qian]{shen2018towards}
Z.~Shen, A.~Mokhtari, T.~Zhou, P.~Zhao, and H.~Qian.
\newblock Towards more efficient stochastic decentralized learning: Faster
  convergence and sparse communication.
\newblock \emph{arXiv preprint arXiv:1805.09969}, 2018{\natexlab{b}}.

\bibitem[Sirb and Ye(2016)]{sirb2016consensus}
B.~Sirb and X.~Ye.
\newblock Consensus optimization with delayed and stochastic gradients on
  decentralized networks.
\newblock In \emph{Big Data (Big Data), 2016 IEEE International Conference on},
  pages 76--85. IEEE, 2016.

\bibitem[Sra et~al.(2015)Sra, Yu, Li, and Smola]{sra2015adadelay}
S.~Sra, A.~W. Yu, M.~Li, and A.~J. Smola.
\newblock Adadelay: Delay adaptive distributed stochastic convex optimization.
\newblock \emph{arXiv preprint arXiv:1508.05003}, 2015.

\bibitem[Stich(2018)]{local_sgd}
S.~U. Stich.
\newblock Local sgd converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{Stich:2018aa}
S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4452--4463, 2018.

\bibitem[Tang et~al.(2018{\natexlab{a}})Tang, Gan, Zhang, Zhang, and
  Liu]{Tang:2018aa}
H.~Tang, S.~Gan, C.~Zhang, T.~Zhang, and J.~Liu.
\newblock Communication compression for decentralized training.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7663--7673, 2018{\natexlab{a}}.

\bibitem[Tang et~al.(2018{\natexlab{b}})Tang, Lian, Yan, Zhang, and
  Liu]{tang2018d}
H.~Tang, X.~Lian, M.~Yan, C.~Zhang, and J.~Liu.
\newblock D2: Decentralized training over decentralized data.
\newblock \emph{arXiv preprint arXiv:1803.07068}, 2018{\natexlab{b}}.

\bibitem[Thakur et~al.(2005)Thakur, Rabenseifner, and
  Gropp]{Thakur:2005:OCC:2747766.2747771}
R.~Thakur, R.~Rabenseifner, and W.~Gropp.
\newblock Optimization of collective communication operations in mpich.
\newblock \emph{Int. J. High Perform. Comput. Appl.}, 19\penalty0 (1):\penalty0
  49--66, Feb. 2005.
\newblock ISSN 1094-3420.
\newblock \doi{10.1177/1094342005051521}.
\newblock URL \url{http://dx.doi.org/10.1177/1094342005051521}.

\bibitem[Wang et~al.(2016)Wang, Kolar, Srebro, and Zhang]{wang2016efficient}
J.~Wang, M.~Kolar, N.~Srebro, and T.~Zhang.
\newblock Efficient distributed learning with sparsity.
\newblock \emph{arXiv preprint arXiv:1605.07991}, 2016.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Xu et~al.(2017)Xu, Taylor, Li, Figueiredo, Yuan, and
  Goldstein]{xu2017adaptive}
Z.~Xu, G.~Taylor, H.~Li, M.~Figueiredo, X.~Yuan, and T.~Goldstein.
\newblock Adaptive consensus admm for distributed optimization.
\newblock \emph{arXiv preprint arXiv:1706.02869}, 2017.

\bibitem[Yin et~al.(2018)Yin, Chen, Ramchandran, and
  Bartlett]{Yin2018ByzantineRobustDL}
D.~Yin, Y.~Chen, K.~Ramchandran, and P.~Bartlett.
\newblock Byzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock \emph{arXiv preprint arXiv:1803.01498}, 2018.

\bibitem[Zhang et~al.(2013)Zhang, Zhang, You, Zheng, and
  Xu]{zhang2013asynchronous}
S.~Zhang, C.~Zhang, Z.~You, R.~Zheng, and B.~Xu.
\newblock Asynchronous stochastic gradient descent for dnn training.
\newblock In \emph{Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
  International Conference on}, pages 6660--6663. IEEE, 2013.

\bibitem[Zhang et~al.(2015)Zhang, Choromanska, and LeCun]{zhang2015deep}
S.~Zhang, A.~E. Choromanska, and Y.~LeCun.
\newblock Deep learning with elastic averaging sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  685--693, 2015.

\bibitem[Zhou et~al.(2018)Zhou, Mertikopoulos, Bambos, Glynn, Ye, Li, and
  Fei-Fei]{zhou2018distributed}
Z.~Zhou, P.~Mertikopoulos, N.~Bambos, P.~Glynn, Y.~Ye, L.-J. Li, and
  L.~Fei-Fei.
\newblock Distributed asynchronous optimization with unbounded delays: How slow
  can you go?
\newblock In \emph{International Conference on Machine Learning}, pages
  5965--5974, 2018.

\end{thebibliography}
