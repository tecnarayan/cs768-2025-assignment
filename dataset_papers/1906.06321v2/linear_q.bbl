\begin{thebibliography}{10}

\bibitem{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2312--2320, 2011.

\bibitem{agrawal2017posterior}
Shipra Agrawal and Randy Jia.
\newblock Posterior sampling for reinforcement learning: worst-case regret
  bounds.
\newblock In {\em NIPS}, 2017.

\bibitem{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em arXiv preprint arXiv:1904.11955}, 2019.

\bibitem{audibert2010best}
Jean-Yves Audibert and S{\'e}bastien Bubeck.
\newblock Best arm identification in multi-armed bandits.
\newblock In {\em COLT-23th Conference on learning theory-2010}, pages 13--p,
  2010.

\bibitem{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.05449}, 2017.

\bibitem{azizzadenesheli2018efficient}
K.~{Azizzadenesheli}, E.~{Brunskill}, and A.~{Anandkumar}.
\newblock Efficient exploration through bayesian deep {Q}-networks.
\newblock In {\em 2018 Information Theory and Applications Workshop (ITA)},
  pages 1--9, Feb 2018.

\bibitem{bertsekas1996neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock {\em Neuro-dynamic programming}, volume~5.
\newblock Athena Scientific Belmont, MA, 1996.

\bibitem{dani2008stochastic}
Varsha Dani, Thomas~P Hayes, and Sham~M Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In {\em Conference on Learning Theory}, 2008.

\bibitem{dann2018polynomial}
Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford,
  and Robert~E Schapire.
\newblock On polynomial time {PAC} reinforcement learning with rich
  observations.
\newblock {\em arXiv preprint arXiv:1803.00606}, 2018.

\bibitem{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, pages 5717--5727, USA, 2017. Curran
  Associates Inc.

\bibitem{du2019provably}
Simon~S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav
  Dud{\'\i}k, and John Langford.
\newblock Provably efficient {RL} with rich observations via latent state
  decoding.
\newblock {\em arXiv preprint arXiv:1901.09018}, 2019.

\bibitem{everitt2002cambridge}
Brian Everitt.
\newblock {\em The Cambridge dictionary of statistics}.
\newblock Cambridge University Press, Cambridge, UK; New York, 2002.

\bibitem{fortunato2018noisy}
Meire Fortunato, Mohammad~Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo
  Hessel, Ian Osband, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis,
  Olivier Pietquin, Charles Blundell, and Shane Legg.
\newblock Noisy networks for exploration.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{hsu2012random}
Daniel Hsu, Sham~M Kakade, and Tong Zhang.
\newblock Random design analysis of ridge regression.
\newblock In {\em Conference on learning theory}, pages 9--1, 2012.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1563--1600, 2010.

\bibitem{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  {PAC}-learnable.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1704--1713. JMLR. org, 2017.

\bibitem{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem{kakade2018variance}
Sham Kakade, Mengdi Wang, and Lin Yang.
\newblock Variance reduction methods for sublinear reinforcement learning.
\newblock 02 2018.

\bibitem{kakade2003sample}
Sham~Machandranath Kakade et~al.
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Mach. Learn.}, 49(2-3):209--232, November 2002.

\bibitem{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock {PAC} reinforcement learning with rich observations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1840--1848, 2016.

\bibitem{li2011knows}
Lihong Li, Michael~L Littman, Thomas~J Walsh, and Alexander~L Strehl.
\newblock Knows what it knows: a framework for self-aware learning.
\newblock {\em Machine learning}, 82(3):399--443, 2011.

\bibitem{lipton2018bbq}
Zachary~Chase Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal Ahmed, and
  Li~Deng.
\newblock {BBQ}-networks: Efficient exploration in deep reinforcement learning
  for task-oriented dialogue systems.
\newblock In {\em AAAI}, 2018.

\bibitem{melo2007q}
Francisco~S Melo and M~Isabel Ribeiro.
\newblock Q-learning with linear function approximation.
\newblock In {\em International Conference on Computational Learning Theory},
  pages 308--322. Springer, 2007.

\bibitem{osband2016generalization}
Ian Osband, Benjamin Van~Roy, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock In {\em Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48}, ICML'16, pages
  2377--2386. JMLR.org, 2016.

\bibitem{pazis2013pac}
Jason Pazis and Ronald Parr.
\newblock {PAC} optimal exploration in continuous space markov decision
  processes.
\newblock In {\em Proceedings of the Twenty-Seventh AAAI Conference on
  Artificial Intelligence}, AAAI'13, pages 774--781. AAAI Press, 2013.

\bibitem{samuel1959some}
A.~L. {Samuel}.
\newblock Some studies in machine learning using the game of checkers.
\newblock {\em IBM Journal of Research and Development}, 3(3):210--229, July
  1959.

\bibitem{simchowitz2019non}
Max Simchowitz and Kevin Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular {MDPs}.
\newblock 05 2019.

\bibitem{song2019efficient}
Zhao Song and Wen Sun.
\newblock Efficient model-free reinforcement learning in metric spaces.
\newblock {\em arXiv preprint arXiv:1905.00475}, 2019.

\bibitem{strehl2006pac}
Alexander~L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael~L
  Littman.
\newblock {PAC} model-free reinforcement learning.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 881--888. ACM, 2006.

\bibitem{sun2018model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based reinforcement learning in contextual decision processes.
\newblock {\em arXiv preprint arXiv:1811.08540}, 2018.

\bibitem{sutton1999open}
Richard~S. Sutton.
\newblock Open theoretical questions in reinforcement learning.
\newblock In {\em EuroCOLT}, 1999.

\bibitem{tropp2015introduction}
Joel~A Tropp et~al.
\newblock An introduction to matrix concentration inequalities.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(1-2):1--230, 2015.

\bibitem{vu2013fantope}
Vincent~Q Vu, Juhee Cho, Jing Lei, and Karl Rohe.
\newblock Fantope projection and selection: A near-optimal convex relaxation of
  sparse {PCA}.
\newblock In {\em Advances in neural information processing systems}, pages
  2670--2678, 2013.

\bibitem{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292, 1992.

\bibitem{wen2013efficient}
Zheng Wen and Benjamin Van~Roy.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3021--3029, 2013.

\bibitem{yang2019learning}
Lin~F Yang, Chengzhuo Ni, and Mengdi Wang.
\newblock Learning to control in metric space with optimal regret.
\newblock {\em arXiv preprint arXiv:1905.01576}, 2019.

\bibitem{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock {\em arXiv preprint arXiv:1901.00210}, 2019.

\bibitem{zou2019finite}
Shaofeng Zou, Tengyu Xu, and Yingbin Liang.
\newblock Finite-sample analysis for {SARSA} and {Q}-learning with linear
  function approximation.
\newblock {\em arXiv preprint arXiv:1902.02234}, 2019.

\end{thebibliography}
