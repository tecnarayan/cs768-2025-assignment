\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock {Attention Is All You Need}.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock {Language Models are Few-Shot Learners}.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber,
  Dohan, Lewkowycz, Bosma, Luan, Sutton, and Odena]{Nye2021ShowYW}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, Charles Sutton, and Augustus Odena.
\newblock {Show Your Work: Scratchpads for Intermediate Computation with
  Language Models}.
\newblock \emph{arXiv}, abs/2112.00114, 2021.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{Wei2022ChainOT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock {Chain-of-Thought Prompting Elicits Reasoning in Large Language
  Models}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, and Sutton]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell~I. Nye, Maarten Bosma, Henryk
  Michalewski, David Dohan, Ellen Jiang, Carrie~J. Cai, Michael Terry, Quoc~V.
  Le, and Charles Sutton.
\newblock {Program Synthesis with Large Language Models}.
\newblock \emph{arXiv}, abs/2108.07732, 2021.

\bibitem[Press et~al.(2022{\natexlab{a}})Press, Zhang, Min, Schmidt, Smith, and
  Lewis]{Press2022MeasuringAN}
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah~A. Smith, and Mike
  Lewis.
\newblock {Measuring and Narrowing the Compositionality Gap in Language
  Models}.
\newblock \emph{arXiv:2210.03350}, abs/2210.03350, 2022{\natexlab{a}}.

\bibitem[Creswell et~al.(2023)Creswell, Shanahan, and
  Higgins]{creswell2022selection}
Antonia Creswell, Murray Shanahan, and Irina Higgins.
\newblock {Selection-Inference: Exploiting Large Language Models for
  Interpretable Logical Reasoning}.
\newblock In \emph{ICLR}, 2023.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, and Goodman]{Zelikman2022STaRBR}
E.~Zelikman, Yuhuai Wu, and Noah~D. Goodman.
\newblock {STaR: Bootstrapping Reasoning With Reasoning}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Huang et~al.(2022)Huang, Xia, Xiao, Chan, Liang, Florence, Zeng,
  Tompson, Mordatch, Chebotar, Sermanet, Brown, Jackson, Luu, Levine, Hausman,
  and Ichter]{huang2022inner}
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy
  Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah
  Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian
  Ichter.
\newblock {Inner Monologue: Embodied Reasoning through Planning with Language
  Models}.
\newblock In \emph{CoRL}, 2022.

\bibitem[Fan et~al.(2020)Fan, Lavril, Grave, Joulin, and
  Sukhbaatar]{fan2020addressing}
Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar
  Sukhbaatar.
\newblock {Addressing Some Limitations of Transformers with Feedback Memory}.
\newblock \emph{arXiv}, 2020.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock {Long Short-Term Memory}.
\newblock \emph{Neural computation}, 1997.

\bibitem[Ju et~al.(2022)Ju, Roller, Sukhbaatar, and Weston]{Ju2021StaircaseAF}
Da~Ju, Stephen Roller, Sainbayar Sukhbaatar, and Jason Weston.
\newblock {Staircase Attention for Recurrent Processing of Sequences}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Hutchins et~al.(2022)Hutchins, Schlag, Wu, Dyer, and
  Neyshabur]{Hutchins2022BlockRecurrentT}
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock {Block-Recurrent Transformers}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[van~den Broek et~al.(2009)van~den Broek, White, Kendeou, and
  Carlson]{van2009reading}
Paul van~den Broek, Mary~Jane White, Panayiota Kendeou, and Sarah Carlson.
\newblock Reading between the lines.
\newblock \emph{Developmental and individual differences in cognitive processes
  in reading comprehension. In. K. Wagner, C. Schatschneider, \& C.
  Plythian-Sence (Eds.), Beyond decoding: The behavioral and biological
  foundations of reading comprehension}, pages 107--123, 2009.

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh,
  Slone, Gur-Ari, Dyer, and Neyshabur]{cem2022exploring}
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay
  Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.
\newblock {Exploring Length Generalization in Large Language Models}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Toshniwal et~al.(2022)Toshniwal, Wiseman, Livescu, and
  Gimpel]{toshniwal2022chess}
Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel.
\newblock {Chess as a Testbed for Language Model State Tracking}.
\newblock In \emph{AAAI}, 2022.

\bibitem[Roy and Roth(2016)]{roy2016solving}
Subhro Roy and Dan Roth.
\newblock Solving general arithmetic word problems.
\newblock \emph{arXiv preprint arXiv:1608.01413}, 2016.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Hilton, Nakano, Hesse,
  and Schulman]{Cobbe2021TrainingVT}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{ArXiv}, abs/2110.14168, 2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock {Language Models are Unsupervised Multitask Learners}.
\newblock In \emph{OpenAI blog}, 2019.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock {HuggingFace's Transformers: State-of-the-art Natural Language
  Processing}.
\newblock In \emph{EMNLP: System Demonstrations}, 2020.

\bibitem[Weston et~al.(2016)Weston, Bordes, Chopra, and
  Mikolov]{Weston2015TowardsAQ}
Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov.
\newblock {Towards AI-Complete Question Answering: A Set of Prerequisite Toy
  Tasks}.
\newblock In \emph{ICLR}, 2016.

\bibitem[Wang and Komatsuzaki(2021)]{wang2021gpt}
Ben Wang and Aran Komatsuzaki.
\newblock Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Bottou(2014)]{bottou2014machine}
L{\'e}on Bottou.
\newblock From machine learning to machine reasoning: An essay.
\newblock \emph{Machine learning}, 94:\penalty0 133--149, 2014.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Szlam, Weston, and
  Fergus]{sukhbaatar2015end}
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.
\newblock {End-to-End Memory Networks}.
\newblock \emph{NeurIPS}, 2015.

\bibitem[Henaff et~al.(2016)Henaff, Weston, Szlam, Bordes, and
  LeCun]{henaff2016tracking}
Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun.
\newblock Tracking the world state with recurrent entity networks.
\newblock \emph{arXiv preprint arXiv:1612.03969}, 2016.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{Dehghani2018UniversalT}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock {Universal Transformers}.
\newblock In \emph{ICLR}, 2019.

\bibitem[Camburu et~al.(2018)Camburu, Rockt\"{a}schel, Lukasiewicz, and
  Blunsom]{camburu18explanation}
Oana-Maria Camburu, Tim Rockt\"{a}schel, Thomas Lukasiewicz, and Phil Blunsom.
\newblock e-{SNLI}: {Natural Language Inference with Natural Language
  Explanations}.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Ling et~al.(2017)Ling, Yogatama, Dyer, and Blunsom]{ling2017program}
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
\newblock Program induction by rationale generation: Learning to solve and
  explain algebraic word problems.
\newblock \emph{arXiv preprint arXiv:1705.04146}, 2017.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima22large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock {Large Language Models are Zero-Shot Reasoners}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Trivedi et~al.(2022)Trivedi, Balasubramanian, Khot, and
  Sabharwal]{Trivedi2022InterleavingRW}
H.~Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal.
\newblock {Interleaving Retrieval with Chain-of-Thought Reasoning for
  Knowledge-Intensive Multi-Step Questions}.
\newblock \emph{arXiv}, abs/2212.10509, 2022.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn,
  Gopalakrishnan, Hausman, Herzog, et~al.]{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
  David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
  et~al.
\newblock {Do As I Can, Not As I Say: Grounding Language in Robotic
  Affordances}.
\newblock \emph{arXiv:2204.01691}, 2022.

\bibitem[Schick et~al.(2023{\natexlab{a}})Schick, Dwivedi-Yu, Dess{\`\i},
  Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom]{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria
  Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{arXiv preprint arXiv:2302.04761}, 2023{\natexlab{a}}.

\bibitem[Press et~al.(2022{\natexlab{b}})Press, Smith, and
  Lewis]{press2022train}
Ofir Press, Noah Smith, and Mike Lewis.
\newblock {Train Short, Test Long: Attention with Linear Biases Enables Input
  Length Extrapolation}.
\newblock In \emph{ICLR}, 2022{\natexlab{b}}.

\bibitem[Neishi and Yoshinaga(2019)]{neishi-yoshinaga-2019-relation}
Masato Neishi and Naoki Yoshinaga.
\newblock {On the Relation between Position Information and Sentence Length in
  Neural Machine Translation}.
\newblock In \emph{CoNLL}, 2019.

\bibitem[Kiyono et~al.(2021)Kiyono, Kobayashi, Suzuki, and
  Inui]{kiyono-etal-2021-shape}
Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui.
\newblock {SHAPE}: {S}hifted {A}bsolute {P}osition {E}mbedding for
  {T}ransformers.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Hupkes et~al.(2020)Hupkes, Dankers, Mul, and Bruni]{ijcai2020p708}
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni.
\newblock Compositionality decomposed: How do neural networks generalise?
\newblock In \emph{IJCAI}, 2020.

\bibitem[Sinha et~al.(2022)Sinha, Kazemnejad, Reddy, Pineau, Hupkes, and
  Williams]{sinha2022curious}
Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke
  Hupkes, and Adina Williams.
\newblock {The Curious Case of Absolute Position Embeddings}.
\newblock \emph{Findings of EMNLP}, 2022.

\bibitem[Graves(2016)]{graves2016adaptive}
Alex Graves.
\newblock {Adaptive Computation Time for Recurrent Neural Networks}.
\newblock \emph{arXiv:1603.08983}, 2016.

\bibitem[Bolukbasi et~al.(2017)Bolukbasi, Wang, Dekel, and
  Saligrama]{bolukbasi2017adaptive}
Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama.
\newblock {Adaptive Neural Networks for Efficient Inference}.
\newblock In \emph{ICML}, 2017.

\bibitem[Banino et~al.(2021)Banino, Balaguer, and
  Blundell]{banino2021pondernet}
Andrea Banino, Jan Balaguer, and Charles Blundell.
\newblock {PonderNet: Learning to Ponder}.
\newblock In \emph{ICML Workshop on Automated Machine Learning}, 2021.

\bibitem[Schick et~al.(2023{\natexlab{b}})Schick, Dwivedi-Yu, Jiang, Petroni,
  Lewis, Izacard, You, Nalmpantis, Grave, and Riedel]{schick2022peer}
Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis,
  Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, and
  Sebastian Riedel.
\newblock {PEER: A Collaborative Language Model}.
\newblock In \emph{ICLR}, 2023{\natexlab{b}}.

\bibitem[Gu et~al.(2019)Gu, Wang, and Zhao]{gu2019levenshtein}
Jiatao Gu, Changhan Wang, and Junbo Zhao.
\newblock {Levenshtein Transformer}.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Stern et~al.(2019)Stern, Chan, Kiros, and
  Uszkoreit]{stern2019insertion}
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit.
\newblock {Insertion Transformer: Flexible Sequence Generation via Insertion
  Operations}.
\newblock In \emph{ICML}, 2019.

\bibitem[Elgohary et~al.(2019)Elgohary, Peskov, and
  Boyd-Graber]{elgohary2019can}
Ahmed Elgohary, Denis Peskov, and Jordan Boyd-Graber.
\newblock {Can You Unpack That? Learning to Rewrite Questions-in-Context}.
\newblock In \emph{EMNLP-IJCNLP}, 2019.

\bibitem[Kim et~al.(2022)Kim, Kim, Yoo, and Kang]{kim2022towards}
Gangwoo Kim, Sungdong Kim, Kang~Min Yoo, and Jaewoo Kang.
\newblock {Generating Information-Seeking Conversations from Unlabeled
  Documents}.
\newblock In \emph{EMNLP}, 2022.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Murtadha, Wen, and Liu]{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu.
\newblock {Roformer: Enhanced transformer with rotary position embedding}.
\newblock \emph{arXiv:2104.09864}, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock {PaLM: Scaling Language Modeling with Pathways}.
\newblock \emph{arXiv:2204.02311}, 2022.

\end{thebibliography}
