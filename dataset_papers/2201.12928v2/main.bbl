\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antoniou et~al.(2018)Antoniou, Edwards, and
  Storkey]{antoniou2018train}
Antoniou, A., Edwards, H., and Storkey, A.
\newblock How to train your maml.
\newblock \emph{arXiv preprint arXiv:1810.09502}, 2018.

\bibitem[Bach(2011)]{bach2011learning}
Bach, F.
\newblock Learning with submodular functions: A convex optimization
  perspective.
\newblock \emph{arXiv preprint arXiv:1111.6453}, 2011.

\bibitem[Bach(2019)]{bach2019submodular}
Bach, F.
\newblock Submodular functions: from discrete to continuous domains.
\newblock \emph{Mathematical Programming}, 175\penalty0 (1):\penalty0 419--459,
  2019.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and
  Raffel, C.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Bertinetto et~al.(2018)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto2018meta}
Bertinetto, L., Henriques, J.~F., Torr, P., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chapelle et~al.(2009)Chapelle, Scholkopf, and Zien]{chapelle2009semi}
Chapelle, O., Scholkopf, B., and Zien, A.
\newblock Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book
  reviews].
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (3):\penalty0 542--542, 2009.

\bibitem[Chen et~al.(2019)Chen, Liu, Kira, Wang, and Huang]{chen2019closer}
Chen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C.~F., and Huang, J.-B.
\newblock A closer look at few-shot classification.
\newblock \emph{arXiv preprint arXiv:1904.04232}, 2019.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020convergence}
Fallah, A., Mokhtari, A., and Ozdaglar, A.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1082--1092. PMLR, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1126--1135. JMLR. org, 2017.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{finn2018probabilistic}
Finn, C., Xu, K., and Levine, S.
\newblock Probabilistic model-agnostic meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.02817}, 2018.

\bibitem[Fujishige(2005)]{fujishige2005submodular}
Fujishige, S.
\newblock \emph{Submodular functions and optimization}.
\newblock Elsevier, 2005.

\bibitem[Grant et~al.(2018)Grant, Finn, Levine, Darrell, and
  Griffiths]{grant2018recasting}
Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock \emph{arXiv preprint arXiv:1801.08930}, 2018.

\bibitem[Huang et~al.(2021{\natexlab{a}})Huang, Zhang, Zhang, Wu, and
  Xu]{huang2021ptn}
Huang, H., Zhang, J., Zhang, J., Wu, Q., and Xu, C.
\newblock Ptn: A poisson transfer network for semi-supervised few-shot
  learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  1602--1609, 2021{\natexlab{a}}.

\bibitem[Huang et~al.(2021{\natexlab{b}})Huang, Geng, Jiang, Deng, and
  Xu]{Huang_2021_ICCV}
Huang, K., Geng, J., Jiang, W., Deng, X., and Xu, Z.
\newblock Pseudo-loss confidence metric for semi-supervised few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pp.\  8671--8680, October 2021{\natexlab{b}}.

\bibitem[Iyer \& Bilmes(2019)Iyer and Bilmes]{iyer2019memoization}
Iyer, R. and Bilmes, J.
\newblock A memoization framework for scaling submodular optimization to large
  scale problems.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  2340--2349. PMLR, 2019.

\bibitem[Iyer et~al.(2021)Iyer, Khargoankar, Bilmes, and Asanani]{smi}
Iyer, R., Khargoankar, N., Bilmes, J., and Asanani, H.
\newblock Submodular combinatorial information measures with applications in
  machine learning.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  722--754. PMLR, 2021.

\bibitem[Kaushal et~al.(2019{\natexlab{a}})Kaushal, Iyer, Kothawade, Mahadev,
  Doctor, and Ramakrishnan]{kaushal2019learning}
Kaushal, V., Iyer, R., Kothawade, S., Mahadev, R., Doctor, K., and
  Ramakrishnan, G.
\newblock Learning from less data: A unified data subset selection and active
  learning framework for computer vision.
\newblock In \emph{2019 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  1289--1299. IEEE, 2019{\natexlab{a}}.

\bibitem[Kaushal et~al.(2019{\natexlab{b}})Kaushal, Subramanian, Kothawade,
  Iyer, and Ramakrishnan]{kaushal2019framework}
Kaushal, V., Subramanian, S., Kothawade, S., Iyer, R., and Ramakrishnan, G.
\newblock A framework towards domain specific video summarization.
\newblock In \emph{2019 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  666--675. IEEE, 2019{\natexlab{b}}.

\bibitem[Kaushal et~al.(2020)Kaushal, Kothawade, Iyer, and
  Ramakrishnan]{kaushal2020realistic}
Kaushal, V., Kothawade, S., Iyer, R., and Ramakrishnan, G.
\newblock Realistic video summarization through visiocity: A new benchmark and
  evaluation framework.
\newblock In \emph{Proceedings of the 2nd International Workshop on AI for
  Smart TV Content Production, Access and Delivery}, pp.\  37--44, 2020.

\bibitem[Killamsetty et~al.(2020)Killamsetty, Li, Zhao, Iyer, and
  Chen]{killamsetty2020nested}
Killamsetty, K., Li, C., Zhao, C., Iyer, R., and Chen, F.
\newblock A nested bi-level optimization framework for robust few shot
  learning.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2011, 2020.

\bibitem[Killamsetty et~al.(2021{\natexlab{a}})Killamsetty, Sivasubramanian,
  Ramakrishnan, De, and Iyer]{killamsetty2021grad}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., De, A., and Iyer, R.
\newblock Grad-match: A gradient matching based data subset selection for
  efficient learning.
\newblock \emph{In ICML}, 2021{\natexlab{a}}.

\bibitem[Killamsetty et~al.(2021{\natexlab{b}})Killamsetty, Sivasubramanian,
  Ramakrishnan, and Iyer]{killamsetty2020glister}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., and Iyer, R.
\newblock Glister: Generalization based data subset selection for efficient and
  robust learning.
\newblock \emph{In AAAI}, 2021{\natexlab{b}}.

\bibitem[Kothawade et~al.(2020)Kothawade, Girdhar, Lavania, and
  Iyer]{kothawade2020deep}
Kothawade, S., Girdhar, J., Lavania, C., and Iyer, R.
\newblock Deep submodular networks for extractive data summarization.
\newblock \emph{arXiv preprint arXiv:2010.08593}, 2020.

\bibitem[Kothawade et~al.(2021{\natexlab{a}})Kothawade, Beck, Killamsetty, and
  Iyer]{kothawade2021similar}
Kothawade, S., Beck, N., Killamsetty, K., and Iyer, R.
\newblock Similar: Submodular information measures based active learning in
  realistic scenarios.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Kothawade et~al.(2021{\natexlab{b}})Kothawade, Ghosh, Shekhar, Xiang,
  and Iyer]{kothawade2021talisman}
Kothawade, S., Ghosh, S., Shekhar, S., Xiang, Y., and Iyer, R.
\newblock Talisman: Targeted active learning for object detection with rare
  classes and slices using submodular mutual information.
\newblock \emph{arXiv preprint arXiv:2112.00166}, 2021{\natexlab{b}}.

\bibitem[Kothawade et~al.(2021{\natexlab{c}})Kothawade, Kaushal, Ramakrishnan,
  Bilmes, and Iyer]{kothawade2021prism}
Kothawade, S., Kaushal, V., Ramakrishnan, G., Bilmes, J., and Iyer, R.
\newblock Prism: A rich class of parameterized submodular information measures
  for guided subset selection.
\newblock \emph{arXiv preprint arXiv:2103.00128}, 2021{\natexlab{c}}.

\bibitem[Laine \& Aila(2016)Laine and Aila]{laine2016temporal}
Laine, S. and Aila, T.
\newblock Temporal ensembling for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem[Lazarou et~al.(2021)Lazarou, Stathaki, and
  Avrithis]{lazarou2021iterative}
Lazarou, M., Stathaki, T., and Avrithis, Y.
\newblock Iterative label cleaning for transductive and semi-supervised
  few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  8751--8760, 2021.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
Lee, D.-H. et~al.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML},
  volume~3, pp.\  896, 2013.

\bibitem[Li et~al.(2019)Li, Sun, Liu, Zhou, Zheng, Chua, and
  Schiele]{li2019learning}
Li, X., Sun, Q., Liu, Y., Zhou, Q., Zheng, S., Chua, T.-S., and Schiele, B.
\newblock Learning to self-train for semi-supervised few-shot classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 10276--10286, 2019.

\bibitem[Liu et~al.(2017)Liu, Iyer, Kirchhoff, and Bilmes]{liu2017svitchboard}
Liu, Y., Iyer, R., Kirchhoff, K., and Bilmes, J.
\newblock Svitchboard-ii and fisver-i: Crafting high quality and low complexity
  conversational english speech corpora using submodular function optimization.
\newblock \emph{Computer Speech \& Language}, 42:\penalty0 122--142, 2017.

\bibitem[Liu et~al.(2019)Liu, Lee, Park, Kim, Yang, Hwang, and
  Yang]{liu2018learning}
Liu, Y., Lee, J., Park, M., Kim, S., Yang, E., Hwang, S.~J., and Yang, Y.
\newblock Learning to propagate labels: Transductive propagation network for
  few-shot learning.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Mirzasoleiman et~al.(2015)Mirzasoleiman, Badanidiyuru, Karbasi,
  Vondr{\'a}k, and Krause]{mirzasoleiman2015lazier}
Mirzasoleiman, B., Badanidiyuru, A., Karbasi, A., Vondr{\'a}k, J., and Krause,
  A.
\newblock Lazier than lazy greedy.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~29, 2015.

\bibitem[Mishra et~al.(2017)Mishra, Rohaninejad, Chen, and
  Abbeel]{mishra2017simple}
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.
\newblock A simple neural attentive meta-learner.
\newblock \emph{arXiv preprint arXiv:1707.03141}, 2017.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Miyato, T., Maeda, S.-i., Koyama, M., and Ishii, S.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Munkhdalai \& Yu(2017)Munkhdalai and Yu]{munkhdalai2017meta}
Munkhdalai, T. and Yu, H.
\newblock Meta networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2554--2563. PMLR, 2017.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and
  Fisher]{nemhauser1978analysis}
Nemhauser, G.~L., Wolsey, L.~A., and Fisher, M.~L.
\newblock An analysis of approximations for maximizing submodular set
  functions—i.
\newblock \emph{Mathematical programming}, 14\penalty0 (1):\penalty0 265--294,
  1978.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{oliver2018realistic}
Oliver, A., Odena, A., Raffel, C., Cubuk, E.~D., and Goodfellow, I.~J.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock \emph{arXiv preprint arXiv:1804.09170}, 2018.

\bibitem[Oreshkin et~al.(2018)Oreshkin, Rodriguez, and
  Lacoste]{oreshkin2018tadam}
Oreshkin, B.~N., Rodriguez, P., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock \emph{arXiv preprint arXiv:1805.10123}, 2018.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{ravi2016optimization}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Ren et~al.(2018)Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,
  Larochelle, and Zemel]{ren2018meta}
Ren, M., Triantafillou, E., Ravi, S., Snell, J., Swersky, K., Tenenbaum, J.~B.,
  Larochelle, H., and Zemel, R.~S.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Rusu et~al.(2018)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{rusu2018meta}
Rusu, A.~A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S.,
  and Hadsell, R.
\newblock Meta-learning with latent embedding optimization.
\newblock \emph{arXiv preprint arXiv:1807.05960}, 2018.

\bibitem[Sajjadi et~al.(2016)Sajjadi, Javanmardi, and
  Tasdizen]{sajjadi2016regularization}
Sajjadi, M., Javanmardi, M., and Tasdizen, T.
\newblock Regularization with stochastic transformations and perturbations for
  deep semi-supervised learning.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 1163--1171, 2016.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro2016meta}
Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1842--1850. PMLR, 2016.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Snell, J., Swersky, K., and Zemel, R.~S.
\newblock Prototypical networks for few-shot learning.
\newblock \emph{arXiv preprint arXiv:1703.05175}, 2017.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Li, Zhang, Carlini, Cubuk, Kurakin,
  Zhang, and Raffel]{sohn2020fixmatch}
Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E.~D.,
  Kurakin, A., Zhang, H., and Raffel, C.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock \emph{arXiv preprint arXiv:2001.07685}, 2020.

\bibitem[Sun et~al.(2019)Sun, Liu, Chua, and Schiele]{sun2019meta}
Sun, Q., Liu, Y., Chua, T.-S., and Schiele, B.
\newblock Meta-transfer learning for few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  403--412, 2019.

\bibitem[Tarvainen \& Valpola(2017)Tarvainen and Valpola]{tarvainen2017mean}
Tarvainen, A. and Valpola, H.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock \emph{arXiv preprint arXiv:1703.01780}, 2017.

\bibitem[Tian et~al.(2020)Tian, Wang, Krishnan, Tenenbaum, and
  Isola]{tian2020rethinking}
Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J.~B., and Isola, P.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock In \emph{European Conference on Computer Vision}, pp.\  266--282.
  Springer, 2020.

\bibitem[Tohidi et~al.(2020)Tohidi, Amiri, Coutino, Gesbert, Leus, and
  Karbasi]{tohidi2020submodularity}
Tohidi, E., Amiri, R., Coutino, M., Gesbert, D., Leus, G., and Karbasi, A.
\newblock Submodularity in action: From machine learning to signal processing
  applications.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (5):\penalty0
  120--133, 2020.

\bibitem[Tschiatschek et~al.(2014)Tschiatschek, Iyer, Wei, and
  Bilmes]{tschiatschek2014learning}
Tschiatschek, S., Iyer, R.~K., Wei, H., and Bilmes, J.~A.
\newblock Learning mixtures of submodular functions for image collection
  summarization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1413--1421, 2014.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et~al.
\newblock Matching networks for one shot learning.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 3630--3638, 2016.

\bibitem[Wang et~al.(2021)Wang, Pontil, and Ciliberto]{wang2021role}
Wang, R., Pontil, M., and Ciliberto, C.
\newblock The role of global labels in few-shot classification and how to infer
  them.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 27160--27170, 2021.

\bibitem[Wang et~al.(2020)Wang, Xu, Liu, Zhang, and Fu]{Wang_2020_CVPR}
Wang, Y., Xu, C., Liu, C., Zhang, L., and Fu, Y.
\newblock Instance credibility inference for few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Xie et~al.(2019)Xie, Dai, Hovy, Luong, and Le]{xie2019unsupervised}
Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q.~V.
\newblock Unsupervised data augmentation for consistency training.
\newblock \emph{arXiv preprint arXiv:1904.12848}, 2019.

\bibitem[Yu et~al.(2020)Yu, Chen, Cheng, and Luo]{yu2020transmatch}
Yu, Z., Chen, L., Cheng, Z., and Luo, J.
\newblock Transmatch: A transfer-learning scheme for semi-supervised few-shot
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12856--12864, 2020.

\bibitem[Zhao et~al.(2022)Zhao, Mi, Wu, Jiang, Khan, and
  Chen]{zhao2022adaptive}
Zhao, C., Mi, F., Wu, X., Jiang, K., Khan, L., and Chen, F.
\newblock Adaptive fairness-aware online meta-learning for changing
  environments.
\newblock \emph{arXiv preprint arXiv:2205.11264}, 2022.

\end{thebibliography}
