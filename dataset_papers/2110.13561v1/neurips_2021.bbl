\begin{thebibliography}{10}

\bibitem{Power}
Individual household electric power consumption data set.
\newblock
  \url{https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption}.
\newblock Accessed: 2021-05-25.

\bibitem{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622. PMLR, 2015.

\bibitem{brahim2004gaussian}
Sofiane Brahim-Belhouari and Amine Bermak.
\newblock Gaussian process for nonstationary time series prediction.
\newblock {\em Computational Statistics \& Data Analysis}, 47(4):705--712,
  2004.

\bibitem{chen2019closer}
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang~Frank Wang, and Jia-Bin
  Huang.
\newblock A closer look at few-shot classification.
\newblock {\em arXiv preprint arXiv:1904.04232}, 2019.

\bibitem{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}, 2016.

\bibitem{du2021metakernel}
Yingjun Du, Haoliang Sun, Xiantong Zhen, Jun Xu, Yilong Yin, Ling Shao, and
  Cees G.~M. Snoek.
\newblock Metakernel: Learning variational random features with limited labels,
  2021.

\bibitem{dutordoir2018gaussian}
Vincent Dutordoir, Hugh Salimbeni, Marc Deisenroth, and James Hensman.
\newblock Gaussian process conditional density estimation, 2018.

\bibitem{eeg_dataset}
SM~Fernandez-Fraga, MA~Aceves-Fernandez, JC~Pedraza-Ortega, and
  JM~Ramos-Arreguin.
\newblock Screen task experiments for eeg signals based on ssvep brain computer
  interface.
\newblock {\em International Journal of Advanced Research}, 6(2):1718--1732,
  2018.
\newblock Accessed: 2021-05-25.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1126--1135. PMLR, 2017.

\bibitem{finn2018probabilistic}
Chelsea Finn, Kelvin Xu, and Sergey Levine.
\newblock Probabilistic model-agnostic meta-learning.
\newblock {\em arXiv preprint arXiv:1806.02817}, 2018.

\bibitem{fortuin2019meta}
Vincent Fortuin, Heiko Strathmann, and Gunnar R{\"a}tsch.
\newblock Meta-learning mean functions for gaussian processes.
\newblock {\em arXiv preprint arXiv:1901.08098}, 2019.

\bibitem{garnelo2018neural}
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo~J Rezende,
  SM~Eslami, and Yee~Whye Teh.
\newblock Neural processes.
\newblock {\em arXiv preprint arXiv:1807.01622}, 2018.

\bibitem{gong1996investigation}
Shaogang Gong, Stephen McKenna, and John~J Collins.
\newblock An investigation into face pose distributions.
\newblock In {\em Proceedings of the Second International Conference on
  Automatic Face and Gesture Recognition}, pages 265--270. IEEE, 1996.

\bibitem{gordon2018meta}
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and
  Richard~E Turner.
\newblock Meta-learning probabilistic inference for prediction.
\newblock {\em arXiv preprint arXiv:1805.09921}, 2018.

\bibitem{grant2018recasting}
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock {\em arXiv preprint arXiv:1801.08930}, 2018.

\bibitem{grathwohl2018ffjord}
Will Grathwohl, Ricky~TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David
  Duvenaud.
\newblock Ffjord: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock {\em arXiv preprint arXiv:1810.01367}, 2018.

\bibitem{harrison2018meta}
James Harrison, Apoorva Sharma, and Marco Pavone.
\newblock Meta-learning priors for efficient online bayesian regression.
\newblock In {\em International Workshop on the Algorithmic Foundations of
  Robotics}, pages 318--337. Springer, 2018.

\bibitem{jerfel2018reconciling}
Ghassen Jerfel, Erin Grant, Thomas~L Griffiths, and Katherine Heller.
\newblock Reconciling meta-learning and continual learning with online mixtures
  of tasks.
\newblock {\em arXiv preprint arXiv:1812.06080}, 2018.

\bibitem{kingma2018glow}
Diederik~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock {\em arXiv preprint arXiv:1807.03039}, 2018.

\bibitem{krogh1992simple}
Anders Krogh and John~A Hertz.
\newblock A simple weight decay can improve generalization.
\newblock In {\em Advances in neural information processing systems}, pages
  950--957, 1992.

\bibitem{NIPS2006_f42c7f9c}
Neil Lawrence, Guido Sanguinetti, and Magnus Rattray.
\newblock Modelling transcriptional regulation using gaussian processes.
\newblock In B.~Sch\"{o}lkopf, J.~Platt, and T.~Hoffman, editors, {\em Advances
  in Neural Information Processing Systems}, volume~19. MIT Press, 2007.

\bibitem{lazaro2012bayesian}
Miguel L{\'a}zaro-Gredilla.
\newblock Bayesian warped gaussian processes.
\newblock {\em Advances in Neural Information Processing Systems},
  25:1619--1627, 2012.

\bibitem{NIPS2012_d840cc5d}
Miguel L\'{a}zaro-Gredilla.
\newblock Bayesian warped gaussian processes.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, {\em Advances in Neural Information Processing Systems}, volume~25.
  Curran Associates, Inc., 2012.

\bibitem{li2020international}
Yan Li, Ethan~X Fang, Huan Xu, and Tuo Zhao.
\newblock International conference on learning representations 2020.
\newblock In {\em International Conference on Learning Representations 2020},
  2020.

\bibitem{li2019kernel}
Zhu Li, Adrian Perez-Suay, Gustau Camps-Valls, and Dino Sejdinovic.
\newblock Kernel dependence regularizers and gaussian processes with
  applications to algorithmic fairness, 2019.

\bibitem{maronas2021transforming}
Juan Maro{\~n}as, Oliver Hamelijnck, Jeremias Knoblauch, and Theodoros
  Damoulas.
\newblock Transforming gaussian processes with normalizing flows.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1081--1089. PMLR, 2021.

\bibitem{najibi2021enhanced}
Fatemeh Najibi, Dimitra Apostolopoulou, and Eduardo Alonso.
\newblock Enhanced performance gaussian process regression for probabilistic
  short-term solar output forecast.
\newblock {\em International Journal of Electrical Power \& Energy Systems},
  130:106916, 2021.

\bibitem{pan2009survey}
Sinno~Jialin Pan and Qiang Yang.
\newblock A survey on transfer learning.
\newblock {\em IEEE Transactions on knowledge and data engineering},
  22(10):1345--1359, 2009.

\bibitem{patacchiola2020bayesian}
Massimiliano Patacchiola, Jack Turner, Elliot~J Crowley, Michael O'Boyle, and
  Amos~J Storkey.
\newblock Bayesian meta-learning for the few-shot setting via deep kernels.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{nasdaq_dataset}
Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison
  Cottrell.
\newblock A dual-stage attention-based recurrent neural network for time series
  prediction, 2017.
\newblock Accessed: 2021-05-25.

\bibitem{quinonero2005unifying}
Joaquin Quinonero-Candela and Carl~Edward Rasmussen.
\newblock A unifying view of sparse approximate gaussian process regression.
\newblock {\em The Journal of Machine Learning Research}, 6:1939--1959, 2005.

\bibitem{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock {\em arXiv preprint arXiv:1909.04630}, 2019.

\bibitem{rasmussen2003gaussian}
Carl~Edward Rasmussen.
\newblock Gaussian processes in machine learning.
\newblock In {\em Summer school on machine learning}, pages 63--71. Springer,
  2003.

\bibitem{rasmussen2006gaussian}
Carl~Edward Rasmussen and C~Williams.
\newblock Gaussian processes for machine learning the mit press.
\newblock {\em Cambridge, MA}, 2006.

\bibitem{GP_rasmussen}
Carl~Edward Rasmussen and Christopher K.~I. Williams.
\newblock {\em Gaussian Processes for Machine Learning (Adaptive Computation
  and Machine Learning)}.
\newblock The MIT Press, 2005.

\bibitem{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages
  1530--1538. PMLR, 2015.

\bibitem{rothfuss2021pacoh}
Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause.
\newblock Pacoh: Bayes-optimal meta-learning with pac-guarantees.
\newblock In {\em International Conference on Machine Learning}, pages
  9116--9126. PMLR, 2021.

\bibitem{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard~S Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock {\em arXiv preprint arXiv:1703.05175}, 2017.

\bibitem{snell2020bayesian}
Jake Snell and Richard Zemel.
\newblock Bayesian few-shot classification with one-vs-each
  p$\backslash$'olya-gamma augmented gaussian processes.
\newblock {\em arXiv preprint arXiv:2007.10417}, 2020.

\bibitem{NIPS2003_6b5754d7}
Edward Snelson, Zoubin Ghahramani, and Carl Rasmussen.
\newblock Warped gaussian processes.
\newblock In S.~Thrun, L.~Saul, and B.~Sch\"{o}lkopf, editors, {\em Advances in
  Neural Information Processing Systems}, volume~16. MIT Press, 2004.

\bibitem{snelson2004warped}
Edward Snelson, Carl~Edward Rasmussen, and Zoubin Ghahramani.
\newblock Warped gaussian processes.
\newblock {\em Advances in neural information processing systems}, 16:337--344,
  2004.

\bibitem{sung2018learning}
Flood Sung, Yongxin Yang, Li~Zhang, Tao Xiang, Philip~HS Torr, and Timothy~M
  Hospedales.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1199--1208, 2018.

\bibitem{titsias2020information}
Michalis~K Titsias, Sotirios Nikoloutsopoulos, and Alexandre Galashov.
\newblock Information theoretic meta learning with gaussian processes.
\newblock {\em arXiv preprint arXiv:2009.03228}, 2020.

\bibitem{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem{tossou2019adaptive}
Prudencio Tossou, Basile Dura, Francois Laviolette, Mario Marchand, and
  Alexandre Lacoste.
\newblock Adaptive deep kernel learning.
\newblock {\em arXiv preprint arXiv:1905.12131}, 2019.

\bibitem{trapp2020deep}
Martin Trapp, Robert Peharz, Franz Pernkopf, and Carl~Edward Rasmussen.
\newblock Deep structured mixtures of gaussian processes.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2251--2261. PMLR, 2020.

\bibitem{venkitaraman2020task}
Arun Venkitaraman, Anders Hansson, and Bo~Wahlberg.
\newblock Task-similarity aware meta-learning through nonparametric kernel
  regression.
\newblock {\em arXiv preprint arXiv:2006.07212}, 2020.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan
  Wierstra.
\newblock Matching networks for one shot learning.
\newblock {\em arXiv preprint arXiv:1606.04080}, 2016.

\bibitem{wang2020generalizing}
Yaqing Wang, Quanming Yao, James~T Kwok, and Lionel~M Ni.
\newblock Generalizing from a few examples: A survey on few-shot learning.
\newblock {\em ACM Computing Surveys (CSUR)}, 53(3):1--34, 2020.

\bibitem{wilson2013gaussian}
Andrew Wilson and Ryan Adams.
\newblock Gaussian process kernels for pattern discovery and extrapolation.
\newblock In {\em International conference on machine learning}, pages
  1067--1075. PMLR, 2013.

\bibitem{xiang2014beyond}
Yu~Xiang, Roozbeh Mottaghi, and Silvio Savarese.
\newblock Beyond pascal: A benchmark for 3d object detection in the wild.
\newblock In {\em IEEE winter conference on applications of computer vision},
  pages 75--82. IEEE, 2014.

\bibitem{xu2020metafun}
Jin Xu, Jean-Francois Ton, Hyunjik Kim, Adam Kosiorek, and Yee~Whye Teh.
\newblock Metafun: Meta-learning with iterative functional updates.
\newblock In {\em International Conference on Machine Learning}, pages
  10617--10627. PMLR, 2020.

\bibitem{yeung2009learning}
Dit-Yan Yeung and Yu~Zhang.
\newblock Learning inverse dynamics by gaussian process begrression under the
  multi-task learning framework.
\newblock In {\em The Path to Autonomous Robots}, pages 1--12. Springer, 2009.

\bibitem{yin2019meta}
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn.
\newblock Meta-learning without memorization.
\newblock {\em arXiv preprint arXiv:1912.03820}, 2019.

\bibitem{yoon2018bayesian}
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin
  Ahn.
\newblock Bayesian model-agnostic meta-learning.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 7343--7353, 2018.

\bibitem{zikeba2020regflow}
Maciej Zi{\k{e}}ba, Marcin Przewi{\k{e}}{\'z}likowski, Marek {\'S}mieja, Jacek
  Tabor, Tomasz Trzcinski, and Przemys{\l}aw Spurek.
\newblock Regflow: Probabilistic flow-based regression for future prediction.
\newblock {\em arXiv preprint arXiv:2011.14620}, 2020.

\end{thebibliography}
