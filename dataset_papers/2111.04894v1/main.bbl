\begin{thebibliography}{}

\bibitem[Abbasi-yadkori et~al., 2011]{NIPS2011_e1d5be1c}
Abbasi-yadkori, Y., P\'{a}l, D., and Szepesv\'{a}ri, C. (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}.

\bibitem[Achiam et~al., 2017]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017).
\newblock Constrained policy optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Alshiekh et~al., 2018]{alshiekh2018safe}
Alshiekh, M., Bloem, R., Ehlers, R., K{\"o}nighofer, B., Niekum, S., and Topcu,
  U. (2018).
\newblock Safe reinforcement learning via shielding.
\newblock In {\em AAAI Conference on Artificial Intelligence (AAAI)}.

\bibitem[Altman, 1999]{altman1999constrained}
Altman, E. (1999).
\newblock {\em Constrained Markov decision processes}, volume~7.
\newblock CRC Press.

\bibitem[Amani et~al., 2019]{amani2019linear}
Amani, S., Alizadeh, M., and Thrampoulidis, C. (2019).
\newblock Linear stochastic bandits under safety constraints.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}.

\bibitem[Berkenkamp et~al., 2017]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A. (2017).
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}.

\bibitem[Bhatnagar and Lakshmanan, 2012]{bhatnagar2012online}
Bhatnagar, S. and Lakshmanan, K. (2012).
\newblock An online actor--critic algorithm with function approximation for
  constrained markov decision processes.
\newblock {\em Journal of Optimization Theory and Applications},
  153(3):688--708.

\bibitem[Biyik et~al., 2019]{biyik2019efficient}
Biyik, E., Margoliash, J., Alimo, S.~R., and Sadigh, D. (2019).
\newblock Efficient and safe exploration in deterministic markov decision
  processes with unknown transition models.
\newblock In {\em American Control Conference (ACC)}. IEEE.

\bibitem[Borkar, 2005]{borkar2005actor}
Borkar, V.~S. (2005).
\newblock An actor-critic algorithm for constrained markov decision processes.
\newblock {\em Systems \& control letters}, 54(3):207--213.

\bibitem[Bradtke and Barto, 1996]{bradtke1996linear}
Bradtke, S.~J. and Barto, A.~G. (1996).
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock {\em Machine learning}, 22(1):33--57.

\bibitem[Cheng et~al., 2019]{cheng2019end}
Cheng, R., Orosz, G., Murray, R.~M., and Burdick, J.~W. (2019).
\newblock End-to-end safe reinforcement learning through barrier functions for
  safety-critical continuous control tasks.
\newblock In {\em AAAI Conference on Artificial Intelligence (AAAI)}.

\bibitem[Chevalier-Boisvert et~al., 2018]{gym_minigrid}
Chevalier-Boisvert, M., Willems, L., and Pal, S. (2018).
\newblock Minimalistic gridworld environment for {OpenAI} gym.
\newblock \url{https://github.com/maximecb/gym-minigrid}.

\bibitem[Chow et~al., 2017]{chow2017risk}
Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. (2017).
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock {\em Journal of Machine Learning Research (JMLR)}, 18(1):6070--6120.

\bibitem[Ding et~al., 2021]{ding2021provably}
Ding, D., Wei, X., Yang, Z., Wang, Z., and Jovanovic, M. (2021).
\newblock Provably efficient safe exploration via primal-dual policy
  optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTAT)}.

\bibitem[Filippi et~al., 2010]{NIPS2010_4166}
Filippi, S., Cappe, O., Garivier, A., and Szepesv\'{a}ri, C. (2010).
\newblock Parametric bandits: The generalized linear case.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}.

\bibitem[Fulton and Platzer, 2018]{fulton2018safe}
Fulton, N. and Platzer, A. (2018).
\newblock Safe reinforcement learning via formal methods: Toward safe control
  through proof and learning.
\newblock In {\em AAAI Conference on Artificial Intelligence (AAAI)}.

\bibitem[Garc{\i}a and Fern{\'a}ndez, 2015]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F. (2015).
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em Journal of Machine Learning Research (JMLR)}, 16(1):1437--1480.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory (COLT)}.

\bibitem[Li et~al., 2010]{li2010contextual}
Li, L., Chu, W., Langford, J., and Schapire, R.~E. (2010).
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In {\em International Conference on World Wide Web (WWW)}.

\bibitem[Li et~al., 2017]{li2017provably}
Li, L., Lu, Y., and Zhou, D. (2017).
\newblock Provably optimal algorithms for generalized linear contextual
  bandits.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Liu et~al., 2020]{liu2020ipo}
Liu, Y., Ding, J., and Liu, X. (2020).
\newblock Ipo: Interior-point policy optimization under constraints.
\newblock In {\em AAAI Conference on Artificial Intelligence (AAAI)}.

\bibitem[Osband et~al., 2016]{osband2016generalization}
Osband, I., Van~Roy, B., and Wen, Z. (2016).
\newblock Generalization and exploration via randomized value functions.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Paternain et~al., 2019]{paternain2019safe}
Paternain, S., Calvo-Fullana, M., Chamon, L.~F., and Ribeiro, A. (2019).
\newblock Safe policies for reinforcement learning via primal-dual methods.
\newblock {\em arXiv preprint arXiv:1911.09101}.

\bibitem[Rasmussen, 2004]{rasmussengaussian}
Rasmussen, C.~E. (2004).
\newblock Gaussian processes in machine learning.
\newblock In {\em Advanced Lectures on Machine Learning}, pages 63--71.
  Springer.

\bibitem[Ray et~al., 2019]{Ray2019}
Ray, A., Achiam, J., and Amodei, D. (2019).
\newblock {\em Benchmarking safe exploration in deep reinforcement learning}.
\newblock OpenAI.

\bibitem[Satija et~al., 2020]{satija2020constrained}
Satija, H., Amortila, P., and Pineau, J. (2020).
\newblock Constrained markov decision processes via backward value functions.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Scherrer et~al., 2012]{scherrer2012approximate}
Scherrer, B., Ghavamzadeh, M., Gabillon, V., and Geist, M. (2012).
\newblock Approximate modified policy iteration.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Schulman et~al., 2015]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015).
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Strehl and Littman, 2005]{strehl2005theoretical}
Strehl, A.~L. and Littman, M.~L. (2005).
\newblock A theoretical analysis of model-based interval estimation.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Sui et~al., 2015]{sui2015safe}
Sui, Y., Gotovos, A., Burdick, J.~W., and Krause, A. (2015).
\newblock Safe exploration for optimization with {Gaussian} processes.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Sutton, 1988]{sutton1988learning}
Sutton, R.~S. (1988).
\newblock Learning to predict by the methods of temporal differences.
\newblock {\em Machine learning}, 3(1):9--44.

\bibitem[Tessler et~al., 2018]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S. (2018).
\newblock Reward constrained policy optimization.
\newblock {\em arXiv preprint arXiv:1805.11074}.

\bibitem[Turchetta et~al., 2016]{turchetta2016safe}
Turchetta, M., Berkenkamp, F., and Krause, A. (2016).
\newblock Safe exploration in finite {Markov} decision processes with
  {Gaussian} processes.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}.

\bibitem[Turchetta et~al., 2019]{turchetta19goose}
Turchetta, M., Berkenkamp, F., and Krause, A. (2019).
\newblock Safe exploration for interactive machine learning.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}.

\bibitem[Wachi and Sui, 2020]{wachi_sui_snomdp_icml2020}
Wachi, A. and Sui, Y. (2020).
\newblock Safe reinforcement learning in constrained {M}arkov decision
  processes.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[Wachi et~al., 2018]{wachi2018safe}
Wachi, A., Sui, Y., Yue, Y., and Ono, M. (2018).
\newblock Safe exploration and optimization of constrained {MDP}s using
  {Gaussian} processes.
\newblock In {\em {AAAI} Conference on Artificial Intelligence ({AAAI})}.

\bibitem[Wang et~al., 2019]{wang2019optimism}
Wang, Y., Wang, R., Du, S.~S., and Krishnamurthy, A. (2019).
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock In {\em International Conference on Learning Representation (ICLR)}.

\bibitem[Yang and Wang, 2020]{pmlr-v119-yang20h}
Yang, L. and Wang, M. (2020).
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\end{thebibliography}
