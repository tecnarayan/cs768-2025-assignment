\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WZW{\etalchar{+}}21}

\bibitem[ALL17]{ao2017fast}
Shuang Ao, Xiang Li, and Charles Ling.
\newblock Fast generalized distillation for semi-supervised domain adaptation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[BSR{\etalchar{+}}05]{burges2005learning}
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole
  Hamilton, and Greg Hullender.
\newblock Learning to rank using gradient descent.
\newblock In {\em Proceedings of the 22nd international conference on Machine
  learning}, pages 89--96, 2005.

\bibitem[CC11]{chapelle2011yahoo}
Olivier Chapelle and Yi~Chang.
\newblock Yahoo! learning to rank challenge overview.
\newblock In {\em Proceedings of the learning to rank challenge}, pages 1--24.
  PMLR, 2011.

\bibitem[CJFY17]{chen2017training}
Yunpeng Chen, Xiaojie Jin, Jiashi Feng, and Shuicheng Yan.
\newblock Training group orthogonal neural networks with privileged
  information.
\newblock {\em arXiv preprint arXiv:1701.06772}, 2017.

\bibitem[CJKB22]{collier2022transfer}
Mark Collier, Rodolphe Jenatton, Efi Kokiopoulou, and Jesse Berent.
\newblock Transfer and marginalize: Explaining away label noise with privileged
  information.
\newblock {\em arXiv preprint arXiv:2202.09244}, 2022.

\bibitem[CM18]{celik2018extending}
Z~Berkay Celik and Patrick McDaniel.
\newblock Extending detection with privileged information via generalized
  distillation.
\newblock In {\em 2018 IEEE Security and Privacy Workshops (SPW)}, pages
  83--88. IEEE, 2018.

\bibitem[DLN{\etalchar{+}}16]{dato2016fast}
Domenico Dato, Claudio Lucchese, Franco~Maria Nardini, Salvatore Orlando,
  Raffaele Perego, Nicola Tonellotto, and Rossano Venturini.
\newblock Fast ranking with additive ensembles of oblivious and non-oblivious
  regression trees.
\newblock {\em ACM Transactions on Information Systems (TOIS)}, 35(2):1--31,
  2016.

\bibitem[FA12]{feyereisl2012privileged}
Jan Feyereisl and Uwe Aickelin.
\newblock Privileged information for data clustering.
\newblock {\em Information Sciences}, 194:4--23, 2012.

\bibitem[FKSH14]{feyereisl2014object}
Jan Feyereisl, Suha Kwak, Jeany Son, and Bohyung Han.
\newblock Object localization based on structural svm using privileged
  information.
\newblock {\em Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[FLT{\etalchar{+}}18]{furlanello2018born}
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima
  Anandkumar.
\newblock Born again neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1607--1616. PMLR, 2018.

\bibitem[FSK{\etalchar{+}}17]{fukuda2017efficient}
Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata, Samuel Thomas, Jia Cui, and
  Bhuvana Ramabhadran.
\newblock Efficient knowledge distillation from an ensemble of teachers.
\newblock In {\em Interspeech}, pages 3697--3701, 2017.

\bibitem[FTRS13]{fouad2013incorporating}
Shereen Fouad, Peter Tino, Somak Raychaudhury, and Petra Schneider.
\newblock Incorporating privileged information through metric learning.
\newblock {\em IEEE transactions on neural networks and learning systems},
  24(7):1086--1098, 2013.

\bibitem[GCA{\etalchar{+}}19]{gao2019privileged}
Zhifan Gao, Jonathan Chung, Mohamed Abdelrazek, Stephanie Leung, William~Kongto
  Hau, Zhanchao Xian, Heye Zhang, and Shuo Li.
\newblock Privileged modality distillation for vessel border detection in
  intracoronary imaging.
\newblock {\em IEEE transactions on medical imaging}, 39(5):1524--1534, 2019.

\bibitem[GCFY18]{gong2018teaching}
Chen Gong, Xiaojun Chang, Meng Fang, and Jian Yang.
\newblock Teaching semi-supervised classifier via generalized distillation.
\newblock In {\em IJCAI}, pages 2156--2162, 2018.

\bibitem[GMM19]{garcia2019learning}
Nuno~C Garcia, Pietro Morerio, and Vittorio Murino.
\newblock Learning with privileged information via adversarial discriminative
  modality distillation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  42(10):2581--2593, 2019.

\bibitem[GYMT21]{gou2021knowledge}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819,
  2021.

\bibitem[HAS{\etalchar{+}}20]{hofstatter2020improving}
Sebastian Hofst{\"a}tter, Sophia Althammer, Michael Schr{\"o}der, Mete Sertkan,
  and Allan Hanbury.
\newblock Improving efficient neural ranking models with cross-architecture
  knowledge distillation.
\newblock {\em arXiv preprint arXiv:2010.02666}, 2020.

\bibitem[HVD{\etalchar{+}}15]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem[LDX{\etalchar{+}}20]{li2020robust}
Xue Li, Bo~Du, Chang Xu, Yipeng Zhang, Lefei Zhang, and Dacheng Tao.
\newblock Robust learning with imperfect privileged information.
\newblock {\em Artificial Intelligence}, 282:103246, 2020.

\bibitem[LHS14]{lapin2014learning}
Maksim Lapin, Matthias Hein, and Bernt Schiele.
\newblock Learning using privileged information: Svm+ and weighted svm.
\newblock {\em Neural Networks}, 53:95--108, 2014.

\bibitem[LLKH20]{lee2020learning}
Wonkyung Lee, Junghyup Lee, Dohyung Kim, and Bumsub Ham.
\newblock Learning with privileged information for efficient image
  super-resolution.
\newblock In {\em European Conference on Computer Vision}, pages 465--482.
  Springer, 2020.

\bibitem[LPSBV16]{LopSchBotVap16}
D.~Lopez-Paz, B.~Sch{\"o}lkopf, L.~Bottou, and V.~Vapnik.
\newblock Unifying distillation and privileged information.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  November 2016.

\bibitem[MM16]{markov2016robust}
Konstantin Markov and Tomoko Matsui.
\newblock Robust speech recognition using generalized distillation framework.
\newblock In {\em Interspeech}, pages 2364--2368, 2016.

\bibitem[PIVV10]{pechyony2010smo}
Dmitry Pechyony, Rauf Izmailov, Akshay Vashist, and Vladimir Vapnik.
\newblock Smo-style algorithms for learning using privileged information.
\newblock In {\em Dmin}, pages 235--241. Citeseer, 2010.

\bibitem[PPA18]{polino2018model}
Antonio Polino, Razvan Pascanu, and Dan Alistarh.
\newblock Model compression via distillation and quantization.
\newblock {\em arXiv preprint arXiv:1802.05668}, 2018.

\bibitem[PV10]{pechyony2010theory}
Dmitry Pechyony and Vladimir Vapnik.
\newblock On the theory of learnining with privileged information.
\newblock {\em Advances in neural information processing systems}, 23, 2010.

\bibitem[QL13]{Qin2013IntroducingL4}
Tao Qin and Tie-Yan Liu.
\newblock Introducing letor 4.0 datasets.
\newblock {\em ArXiv}, abs/1306.2597, 2013.

\bibitem[QYT{\etalchar{+}}21]{qin2021born}
Zhen Qin, Le~Yan, Yi~Tay, Honglei Zhuang, Xuanhui Wang, Michael Bendersky, and
  Marc Najork.
\newblock Born again neural rankers.
\newblock {\em arXiv preprint arXiv:2109.15285}, 2021.

\bibitem[QYZ{\etalchar{+}}21]{qin2021are}
Zhen Qin, Le~Yan, Honglei Zhuang, Yi~Tay, Rama~Kumar Pasumarthi, Xuanhui Wang,
  Mike Bendersky, and Marc Najork.
\newblock Are neural rankers still outperformed by gradient boosted decision
  trees?
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[RPM{\etalchar{+}}21]{reddi2021rankdistil}
Sashank Reddi, Rama~Kumar Pasumarthi, Aditya Menon, Ankit~Singh Rawat, Felix
  Yu, Seungyeon Kim, Andreas Veit, and Sanjiv Kumar.
\newblock Rankdistil: Knowledge distillation for ranking.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2368--2376. PMLR, 2021.

\bibitem[SER14]{SERRATORO201440}
Exploring some practical issues of svm+: Is really privileged information that
  helps?
\newblock {\em Pattern Recognition Letters}, 42:40--46, 2014.

\bibitem[SQL13]{sharmanska2013learning}
Viktoriia Sharmanska, Novi Quadrianto, and Christoph~H Lampert.
\newblock Learning to rank using privileged information.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 825--832, 2013.

\bibitem[TW18]{tang2018ranking}
Jiaxi Tang and Ke~Wang.
\newblock Ranking distillation: Learning compact ranking models with high
  performance for recommender system.
\newblock In {\em Proceedings of the 24th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 2289--2298, 2018.

\bibitem[VI15]{vapnik2015learning}
Vladimir Vapnik and Rauf Izmailov.
\newblock Learning using privileged information: Similarity control and
  knowledge transfer.
\newblock 16(1), 2015.

\bibitem[VV09]{VAPNIK2009544}
Vladimir Vapnik and Akshay Vashist.
\newblock A new learning paradigm: Learning using privileged information.
\newblock {\em Neural Networks}, 22(5):544--557, 2009.
\newblock Advances in Neural Networks Research: IJCNN2009.

\bibitem[WZW{\etalchar{+}}21]{wang2021privileged}
Shuai Wang, Kun Zhang, Le~Wu, Haiping Ma, Richang Hong, and Meng Wang.
\newblock Privileged graph distillation for cold start recommendation.
\newblock In {\em Proceedings of the 44th International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, pages 1187--1196, 2021.

\bibitem[XLG{\etalchar{+}}20]{xu2020privileged}
Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei
  Sun, Jian Wu, Hanxiao Sun, and Wenwu Ou.
\newblock Privileged features distillation at taobao recommendations.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 2590--2598, 2020.

\bibitem[Yu20]{yu2020pt}
Hai-Tao Yu.
\newblock Pt-ranking: A benchmarking platform for neural learning-to-rank.
\newblock {\em arXiv preprint arXiv:2008.13368}, 2020.

\bibitem[ZWBN20]{zhuang2020feature}
Honglei Zhuang, Xuanhui Wang, Michael Bendersky, and Marc Najork.
\newblock Feature transformation for neural ranking models.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, pages 1649--1652, 2020.

\bibitem[ZXHL18]{zhang2018deep}
Ying Zhang, Tao Xiang, Timothy~M Hospedales, and Huchuan Lu.
\newblock Deep mutual learning.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4320--4328, 2018.

\end{thebibliography}
