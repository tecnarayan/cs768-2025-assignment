\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bibas et~al.(2019{\natexlab{a}})Bibas, Fogel, and
  Feder]{Bibas2019ARegression}
K.~Bibas, Y.~Fogel, and M.~Feder.
\newblock {A New Look at an Old Problem: A Universal Learning Approach to
  Linear Regression}.
\newblock \emph{IEEE International Symposium on Information Theory -
  Proceedings}, 2019-July:\penalty0 2304--2308, 5 2019{\natexlab{a}}.
\newblock \doi{10.1109/ISIT.2019.8849398}.
\newblock URL \url{http://arxiv.org/abs/1905.04708
  http://dx.doi.org/10.1109/ISIT.2019.8849398}.

\bibitem[Bibas et~al.(2019{\natexlab{b}})Bibas, Fogel, and
  Feder]{Bibas2019DeepNetworks}
K.~Bibas, Y.~Fogel, and M.~Feder.
\newblock {Deep pnml: Predictive normalized maximum likelihood for deep neural
  networks}.
\newblock \emph{arXiv preprint arXiv:1904.12286}, 2019{\natexlab{b}}.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{Blundell2015WeightNetworks}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock {Weight Uncertainty in Neural Networks}.
\newblock \emph{32nd International Conference on Machine Learning, ICML 2015},
  2:\penalty0 1613--1622, 5 2015.

\bibitem[Cook and Weisberg(1982)]{cook1982residuals}
R.~D. Cook and S.~Weisberg.
\newblock \emph{Residuals and influence in regression}.
\newblock New York: Chapman and Hall, 1982.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{Dusenberry2020EfficientFactors}
M.~W. Dusenberry, G.~Jerfel, Y.~Wen, Y.-a. Ma, J.~Snoek, K.~Heller,
  B.~Lakshminarayanan, and D.~Tran.
\newblock {Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors}.
\newblock \emph{arXiv preprint arXiv:2005.07186}, 2020.

\bibitem[Fogel and Feder(2018{\natexlab{a}})]{Fogel2018UniversalData}
Y.~Fogel and M.~Feder.
\newblock {Universal Supervised Learning for Individual Data}.
\newblock 12 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1812.09520}.

\bibitem[Fogel and Feder(2018{\natexlab{b}})]{Fogel2018UniversalLog-Loss}
Y.~Fogel and M.~Feder.
\newblock {Universal batch learning with log-loss}.
\newblock In \emph{2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 21--25, 2018{\natexlab{b}}.

\bibitem[Gal and Ghahramani(2015)]{Gal2015DropoutLearning}
Y.~Gal and Z.~Ghahramani.
\newblock {Dropout as a Bayesian Approximation: Representing Model Uncertainty
  in Deep Learning}.
\newblock \emph{33rd International Conference on Machine Learning, ICML 2016},
  3:\penalty0 1651--1660, 6 2015.

\bibitem[Giordano et~al.(2019)Giordano, Stephenson, Liu, Jordan, and
  Broderick]{giordano2019swiss}
R.~Giordano, W.~Stephenson, R.~Liu, M.~Jordan, and T.~Broderick.
\newblock A swiss army infinitesimal jackknife.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1139--1147, 2019.

\bibitem[Grunwald(2004)]{Grunwald2004APrinciple}
P.~Grunwald.
\newblock {A tutorial introduction to the minimum description length
  principle}.
\newblock 6 2004.
\newblock URL \url{http://arxiv.org/abs/math/0406077}.

\bibitem[Gr{\"{u}}nwald et~al.(2017)Gr{\"{u}}nwald, Van~Ommen, and
  {others}]{Grunwald2017InconsistencyIt}
P.~Gr{\"{u}}nwald, T.~Van~Ommen, and {others}.
\newblock {Inconsistency of Bayesian inference for misspecified linear models,
  and a proposal for repairing it}.
\newblock \emph{Bayesian Analysis}, 12\penalty0 (4):\penalty0 1069--1103, 2017.

\bibitem[Gr{\"{u}}nwald(2007)]{Grunwald2007TheLearning}
P.~D. Gr{\"{u}}nwald.
\newblock \emph{{The Minimum Description Length Principle (Adaptive Computation
  and Machine Learning)}}.
\newblock The MIT Press, 2007.
\newblock ISBN 0262072815.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{Guo2017OnNetworks}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
\newblock {On calibration of modern neural networks}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1321--1330, 2017.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
D.~Hendrycks and T.~Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Hinton and van Camp(1993)]{Hinton1993KeepingWeights}
G.~E. Hinton and D.~van Camp.
\newblock {Keeping neural networks simple by minimizing the description length
  of the weights}.
\newblock In \emph{Proceedings of the Sixth Annual Conference on Computational
  Learning Theory}, pages 5--13, 1993.

\bibitem[Hochreiter and Schmidhuber(1997)]{Hochreiter1997FlatMinima}
S.~Hochreiter and J.~Schmidhuber.
\newblock {Flat minima}.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{Hoffman2013StochasticInference}
M.~D. Hoffman, D.~M. Blei, C.~Wang, and J.~Paisley.
\newblock {Stochastic variational inference}.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{Izmailov2018AveragingGeneralization}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~P. Vetrov, and A.~G. Wilson.
\newblock {Averaging Weights Leads to Wider Optima and Better Generalization}.
\newblock In \emph{UAI}, 2018.

\bibitem[Kakade et~al.(2006)Kakade, Seeger, and
  Foster]{Kakade2006Worst-caseModels}
S.~M. Kakade, M.~W. Seeger, and D.~P. Foster.
\newblock {Worst-case bounds for Gaussian process models}.
\newblock In \emph{Advances in neural information processing systems}, pages
  619--626, 2006.

\bibitem[Koh and Liang(2017)]{pmlr-v70-koh17a}
P.~W. Koh and P.~Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock 2017.

\bibitem[Krizhevsky(2012)]{Krizhevsky2012LearningImages}
A.~Krizhevsky.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock \emph{University of Toronto}, 6 2012.

\bibitem[Lakshminarayanan et~al.(2016)Lakshminarayanan, Pritzel, and
  Blundell]{Lakshminarayanan2016SimpleEnsembles}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock {Simple and Scalable Predictive Uncertainty Estimation using Deep
  Ensembles}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{Maddox2019ALearning}
W.~J. Maddox, P.~Izmailov, T.~Garipov, D.~P. Vetrov, and A.~G. Wilson.
\newblock {A simple baseline for bayesian uncertainty in deep learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
J.~Martens and R.~Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pages
  2408--2417, 2015.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{Naeini2015ObtainingBinning}
M.~P. Naeini, G.~Cooper, and M.~Hauskrecht.
\newblock {Obtaining well calibrated probabilities using bayesian binning}.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019trust}
Y.~Ovadia, E.~Fertig, J.~Ren, Z.~Nado, D.~Sculley, S.~Nowozin, J.~V. Dillon,
  B.~Lakshminarayanan, and J.~Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift, 2019.

\bibitem[Rissanen(1989)]{Rissanen1989StochasticTheory}
J.~Rissanen.
\newblock \emph{{Stochastic Complexity in Statistical Inquiry Theory}}.
\newblock World Scientific Publishing Co., Inc., USA, 1989.
\newblock ISBN 9971508591.

\bibitem[Rissanen and Roos(2007)]{Rissanen2007ConditionalModels}
J.~Rissanen and T.~Roos.
\newblock {Conditional NML universal models}.
\newblock In \emph{2007 Information Theory and Applications Workshop}, pages
  337--341, 2007.

\bibitem[Rissanen(1996)]{Rissanen1996FisherComplexity}
J.~J. Rissanen.
\newblock {Fisher information and stochastic complexity}.
\newblock \emph{IEEE Transactions on Information Theory}, 42\penalty0
  (1):\penalty0 40--47, 1996.
\newblock ISSN 00189448.
\newblock \doi{10.1109/18.481776}.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018scalable}
H.~Ritter, A.~Botev, and D.~Barber.
\newblock A scalable laplace approximation for neural networks.
\newblock In \emph{6th International Conference on Learning Representations,
  ICLR 2018-Conference Track Proceedings}, volume~6. International Conference
  on Representation Learning, 2018.

\bibitem[Roos et~al.(2008)Roos, Silander, Kontkanen, and
  Myllymaki]{Roos2008BayesianModels}
T.~Roos, T.~Silander, P.~Kontkanen, and P.~Myllymaki.
\newblock {Bayesian network structure learning using factorized NML universal
  models}.
\newblock In \emph{2008 Information Theory and Applications Workshop}, pages
  272--276, 2008.

\bibitem[Shtarkov(1987)]{Shtarkov1987UniversalMessages}
Y.~Shtarkov.
\newblock {Universal sequential coding of single messages}.
\newblock \emph{Problems of Information Transmission}, 23\penalty0
  (3):\penalty0 186, 1987.

\bibitem[Simonyan and Zisserman(2014)]{Simonyan2014VeryRecognition}
K.~Simonyan and A.~Zisserman.
\newblock {Very deep convolutional networks for large-scale image recognition}.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{Srivastava2014Dropout:Overfitting}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock {Dropout: A Simple Way to Prevent Neural Networks from Overfitting}.
\newblock \emph{Journal of Machine Learning Research}, 2014.

\bibitem[Sun et~al.(2020)Sun, Wang, Zhuang, Miller, Hardt, and Efros]{sun19ttt}
Y.~Sun, X.~Wang, L.~Zhuang, J.~Miller, M.~Hardt, and A.~A. Efros.
\newblock Test-time training with self-supervision for generalization under
  distribution shifts.
\newblock In \emph{ICML}, 2020.

\bibitem[Vovk(1990)]{Vovk1990AggregatingStrategies}
V.~G. Vovk.
\newblock {Aggregating Strategies}.
\newblock In \emph{Proceedings of the Third Annual Workshop on Computational
  Learning Theory}, 1990.

\end{thebibliography}
