\begin{thebibliography}{10}

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem{hui2020evaluation}
Like Hui and Mikhail Belkin.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock {\em arXiv preprint arXiv:2006.07322}, 2020.

\bibitem{janocha2016loss}
Katarzyna Janocha and Wojciech~Marian Czarnecki.
\newblock On loss functions for deep neural networks in classification.
\newblock {\em Schedae Informaticae}, 25:49--59, 2016.

\bibitem{papyan2020prevalence}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(40):24652--24663, 2020.

\bibitem{papyan2020traces}
Vardan Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock {\em Journal of Machine Learning Research}, 21(252):1--64, 2020.

\bibitem{han2021neural}
XY~Han, Vardan Papyan, and David~L Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the
  central path.
\newblock {\em arXiv preprint arXiv:2106.02073}, 2021.

\bibitem{lu2020neural}
Jianfeng Lu and Stefan Steinerberger.
\newblock Neural collapse with cross-entropy loss.
\newblock {\em arXiv preprint arXiv:2012.08465}, 2020.

\bibitem{weinan2020emergence}
E~Weinan and Stephan Wojtowytsch.
\newblock On the emergence of tetrahedral symmetry in the final and penultimate
  layers of neural network classifiers.
\newblock {\em arXiv preprint arXiv:2012.05420}, 2020.

\bibitem{mixon2020neural}
Dustin~G Mixon, Hans Parshall, and Jianzong Pi.
\newblock Neural collapse with unconstrained features.
\newblock {\em arXiv preprint arXiv:2011.11619}, 2020.

\bibitem{graf2021dissecting}
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt.
\newblock Dissecting supervised constrastive learning.
\newblock In {\em International Conference on Machine Learning}, pages
  3821--3830. PMLR, 2021.

\bibitem{fang2021layer}
Cong Fang, Hangfeng He, Qi~Long, and Weijie~J Su.
\newblock Layer-peeled model: Toward understanding well-trained deep neural
  networks.
\newblock {\em arXiv e-prints}, pages arXiv--2101, 2021.

\bibitem{ji2021unconstrained}
Wenlong Ji, Yiping Lu, Yiliang Zhang, Zhun Deng, and Weijie~J Su.
\newblock An unconstrained layer-peeled perspective on neural collapse.
\newblock {\em arXiv preprint arXiv:2110.02796}, 2021.

\bibitem{tirer2022extended}
Tom Tirer and Joan Bruna.
\newblock Extended unconstrained features model for exploring deep neural
  collapse.
\newblock {\em arXiv preprint arXiv:2202.08087}, 2022.

\bibitem{ergen2021revealing}
Tolga Ergen and Mert Pilanci.
\newblock Revealing the structure of deep neural networks via convex duality.
\newblock In {\em International Conference on Machine Learning}, pages
  3004--3014. PMLR, 2021.

\bibitem{zhu2021geometric}
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and
  Qing Qu.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{rangamani2022neural}
Akshay Rangamani and Andrzej Banburski-Fahey.
\newblock Neural collapse in deep homogeneous classifiers and the role of
  weight decay.
\newblock In {\em ICASSP 2022-2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 4243--4247. IEEE, 2022.

\bibitem{poggio2020explicit}
Tomaso Poggio and Qianli Liao.
\newblock Explicit regularization and implicit bias in deep network classifiers
  trained with the square loss.
\newblock {\em arXiv preprint arXiv:2101.00072}, 2020.

\bibitem{poggio2020implicit}
Tomaso Poggio and Qianli Liao.
\newblock Implicit dynamic regularization in deep networks.
\newblock Technical report, Center for Brains, Minds and Machines (CBMM), 2020.

\bibitem{rangamani2021dynamics}
Akshay Rangamani, Mengjia Xu, Andrzej Banburski, Qianli Liao, and Tomaso
  Poggio.
\newblock Dynamics and neural collapse in deep classifiers trained with the
  square loss.
\newblock {\em Technical report, Center for Brains, Minds and Machines (CBMM)},
  2021.

\bibitem{rangamani2022deep}
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt.
\newblock Neural collapse in deep homogeneous claaifiers and the role of weight
  decay.
\newblock In {\em International Conference on Machine Learning}, pages
  3821--3830. PMLR, 2021.

\bibitem{zhou2022optimization}
Jinxin Zhou, Xiao Li, Tianyu Ding, Chong You, Qing Qu, and Zhihui Zhu.
\newblock On the optimization landscape of neural collapse under mse loss:
  Global optimality with unconstrained features.
\newblock {\em arXiv preprint arXiv:2203.01238}, 2022.

\bibitem{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points---online stochastic gradient for tensor
  decomposition.
\newblock In {\em Proceedings of The 28th Conference on Learning Theory}, pages
  797--842, 2015.

\bibitem{sun2015nonconvex}
Ju~Sun, Qing Qu, and John Wright.
\newblock When are nonconvex problems not scary?
\newblock {\em arXiv preprint arXiv:1510.06096}, 2015.

\bibitem{zhang2020symmetry}
Yuqian Zhang, Qing Qu, and John Wright.
\newblock From symmetry to geometry: Tractable nonconvex problems.
\newblock {\em arXiv preprint arXiv:2007.06753}, 2020.

\bibitem{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock {\em Communications of the ACM}, 64(3):107--115, 2021.

\bibitem{hui2022limitations}
Like Hui, Mikhail Belkin, and Preetum Nakkiran.
\newblock Limitations of neural collapse for understanding generalization in
  deep learning.
\newblock {\em arXiv preprint arXiv:2202.08384}, 2022.

\bibitem{mukhoti2020calibrating}
Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr,
  and Puneet Dokania.
\newblock Calibrating deep neural networks using focal loss.
\newblock {\em Advances in Neural Information Processing Systems},
  33:15288--15299, 2020.

\bibitem{smith2022cyclical}
Leslie~N Smith.
\newblock Cyclical focal loss.
\newblock {\em arXiv preprint arXiv:2202.08978}, 2022.

\bibitem{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey~E Hinton.
\newblock When does label smoothing help?
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{chen2020investigation}
Blair Chen, Liu Ziyin, Zihao Wang, and Paul~Pu Liang.
\newblock An investigation of how label smoothing affects generalization.
\newblock {\em arXiv preprint arXiv:2010.12648}, 2020.

\bibitem{cybenko1989approximation}
G~Cybenko.
\newblock Approximation by superposition of sigmoidal functions.
\newblock {\em Mathematics of Control, Signals and Systems}, 2(4):303--314,
  1989.

\bibitem{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural networks}, 4(2):251--257, 1991.

\bibitem{lu2017expressive}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The expressive power of neural networks: a view from the width.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6232--6240, 2017.

\bibitem{shaham2018provable}
Uri Shaham, Alexander Cloninger, and Ronald~R Coifman.
\newblock Provable approximation properties for deep neural networks.
\newblock {\em Applied and Computational Harmonic Analysis}, 44(3):537--557,
  2018.

\bibitem{lee2016gradient}
Jason~D Lee, Max Simchowitz, Michael~I Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In {\em Conference on learning theory}, pages 1246--1257. PMLR, 2016.

\bibitem{haeffele2015global}
Benjamin~D Haeffele and Ren{\'e} Vidal.
\newblock Global optimality in tensor factorization, deep learning, and beyond.
\newblock {\em arXiv preprint arXiv:1506.07540}, 2015.

\bibitem{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock {\em arXiv preprint arXiv:1605.07272}, 2016.

\bibitem{bhojanapalli2016global}
Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In {\em Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 3880--3888, 2016.

\bibitem{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock In {\em International Conference on Machine Learning}, pages
  1233--1242. PMLR, 2017.

\bibitem{li2019non}
Qiuwei Li, Zhihui Zhu, and Gongguo Tang.
\newblock The non-convex geometry of low-rank matrix optimization.
\newblock {\em Information and Inference: A Journal of the IMA}, 8(1):51--96,
  2019.

\bibitem{li2019symmetry}
Xingguo Li, Junwei Lu, Raman Arora, Jarvis Haupt, Han Liu, Zhaoran Wang, and
  Tuo Zhao.
\newblock Symmetry, saddle points, and global optimization landscape of
  nonconvex matrix factorization.
\newblock {\em IEEE Transactions on Information Theory}, 65(6):3489--3514,
  2019.

\bibitem{chi2019nonconvex}
Yuejie Chi, Yue~M Lu, and Yuxin Chen.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock {\em IEEE Transactions on Signal Processing}, 67(20):5239--5269,
  2019.

\bibitem{burer2003nonlinear}
Samuel Burer and Renato~DC Monteiro.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock {\em Mathematical Programming}, 95(2):329--357, 2003.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{wen2016discriminative}
Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu~Qiao.
\newblock A discriminative feature learning approach for deep face recognition.
\newblock In {\em European conference on computer vision}, pages 499--515.
  Springer, 2016.

\bibitem{liu2016large}
Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang.
\newblock Large-margin softmax loss for convolutional neural networks.
\newblock In {\em ICML}, volume~2, page~7, 2016.

\bibitem{liu2017sphereface}
Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le~Song.
\newblock Sphereface: Deep hypersphere embedding for face recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 212--220, 2017.

\bibitem{wang2018cosface}
Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng
  Li, and Wei Liu.
\newblock Cosface: Large margin cosine loss for deep face recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5265--5274, 2018.

\bibitem{deng2019arcface}
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
\newblock Arcface: Additive angular margin loss for deep face recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4690--4699, 2019.

\bibitem{zhang2018generalized}
Zhilu Zhang and Mert Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{wang2019symmetric}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 322--330, 2019.

\bibitem{wei2021optimizing}
Jiaheng Wei and Yang Liu.
\newblock When optimizing $f$-divergence is robust with label noise.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{menon2021long}
Aditya~Krishna Menon, Sadeep Jayasumana, Ankit~Singh Rawat, Himanshu Jain,
  Andreas Veit, and Sanjiv Kumar.
\newblock Long-tail learning via logit adjustment.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{jitkrittum2022elm}
Wittawat Jitkrittum, Aditya~Krishna Menon, Ankit~Singh Rawat, and Sanjiv Kumar.
\newblock Elm: Embedding and logit margins for long-tail learning.
\newblock {\em arXiv preprint arXiv:2204.13208}, 2022.

\bibitem{chechik2010large}
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio.
\newblock Large scale online learning of image similarity through ranking.
\newblock {\em Journal of Machine Learning Research}, 11(3), 2010.

\bibitem{sohn2016improved}
Kihyuk Sohn.
\newblock Improved deep metric learning with multi-class n-pair loss objective.
\newblock In {\em Advances in neural information processing systems}, pages
  1857--1865, 2016.

\bibitem{levi2020reducing}
Elad Levi, Tete Xiao, Xiaolong Wang, and Trevor Darrell.
\newblock Reducing class collapse in metric learning with easy positive
  sampling.
\newblock {\em arXiv preprint arXiv:2006.05162}, 2020.

\bibitem{zhang2017learning}
Xu~Zhang, Felix~X Yu, Sanjiv Kumar, and Shih-Fu Chang.
\newblock Learning spread-out local feature descriptors.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 4595--4603, 2017.

\bibitem{qian2019softtriple}
Qi~Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin.
\newblock Softtriple loss: Deep metric learning without triplet sampling.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6450--6458, 2019.

\bibitem{yu2020learning}
Yaodong Yu, Kwan Ho~Ryan Chan, Chong You, Chaobing Song, and Yi~Ma.
\newblock Learning diverse and discriminative representations via the principle
  of maximal coding rate reduction.
\newblock {\em arXiv preprint arXiv:2006.08558}, 2020.

\bibitem{kornblith2021better}
Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi.
\newblock Why do better loss functions lead to less transferable features?
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{cuppen1980divide}
Jan~JM Cuppen.
\newblock A divide and conquer method for the symmetric tridiagonal
  eigenproblem.
\newblock {\em Numerische Mathematik}, 36(2):177--195, 1980.

\bibitem{stor2015forward}
N~Jakov{\v{c}}evi{\'c} Stor, I~Slapni{\v{c}}ar, and Jesse~L Barlow.
\newblock Forward stable eigenvalue decomposition of rank-one modifications of
  diagonal matrices.
\newblock {\em Linear Algebra and its Applications}, 487:301--315, 2015.

\end{thebibliography}
