\begin{thebibliography}{10}

\bibitem{aflalo2020knapsack}
Yonathan Aflalo, Asaf Noy, Ming Lin, Itamar Friedman, and Lihi Zelnik.
\newblock Knapsack pruning with inner distillation.
\newblock {\em arXiv preprint arXiv:2002.08258}, 2020.

\bibitem{alvarez2016learning}
Jose~M Alvarez and Mathieu Salzmann.
\newblock Learning the number of neurons in deep networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2270--2278, 2016.

\bibitem{andonov2000unbounded}
Rumen Andonov, Vincent Poirriez, and Sanjay Rajopadhye.
\newblock Unbounded knapsack problem: Dynamic programming revisited.
\newblock {\em European Journal of Operational Research}, 123(2):394--407,
  2000.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{cai2020zeroq}
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael~W Mahoney, and Kurt
  Keutzer.
\newblock Zeroq: A novel zero shot quantization framework.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13169--13178, 2020.

\bibitem{chen2018constraint}
Changan Chen, Frederick Tung, Naveen Vedula, and Greg Mori.
\newblock Constraint-aware deep neural network compression.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 400--415, 2018.

\bibitem{chetlur2014cudnn}
Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John
  Tran, Bryan Catanzaro, and Evan Shelhamer.
\newblock cudnn: Efficient primitives for deep learning.
\newblock {\em arXiv preprint arXiv:1410.0759}, 2014.

\bibitem{chin2020towards}
Ting-Wu Chin, Ruizhou Ding, Cha Zhang, and Diana Marculescu.
\newblock Towards efficient model compression via learned global ranking.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1518--1528, 2020.

\bibitem{chamnet}
Xiaoliang Dai, Peizhao Zhang, Bichen Wu, Hongxu Yin, Fei Sun, Yanghan Wang,
  Marat Dukhan, Yunqing Hu, Yiming Wu, Yangqing Jia, P~Vajda, M~Uyttendaele,
  and Niraj~K Jha.
\newblock {ChamNet}: {Towards} efficient network design through platform-aware
  model adaptation.
\newblock In {\em CVPR}, 2019.

\bibitem{de2020force}
Pau de~Jorge, Amartya Sanyal, Harkirat~S Behl, Philip~HS Torr, Gregory Rogez,
  and Puneet~K Dokania.
\newblock Progressive skeletonization: Trimming more fat from a network at
  initialization.
\newblock {\em arXiv preprint arXiv:2006.09081}, 2020.

\bibitem{ding2019centripetal}
Xiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong Han.
\newblock Centripetal sgd for pruning very deep convolutional networks with
  complicated structure.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4943--4953, 2019.

\bibitem{dong2018dpp}
Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun.
\newblock Dpp-net: Device-aware progressive search for pareto-optimal neural
  architectures.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 517--531, 2018.

\bibitem{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em International journal of computer vision}, 88(2):303--338, 2010.

\bibitem{frankle2018the}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv preprint arXiv:1902.09574}, 2019.

\bibitem{gao2019vacl}
Susan Gao, Xin Liu, Lung-Sheng Chien, William Zhang, and Jose~M Alvarez.
\newblock Vacl: Variance-aware cross-layer regularization for pruning deep
  residual networks.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision Workshops}, pages 0--0, 2019.

\bibitem{gao2018dynamic}
Xitong Gao, Yiren Zhao, {\L}ukasz Dudziak, Robert Mullins, and Cheng-zhong Xu.
\newblock Dynamic channel pruning: Feature boosting and suppression.
\newblock {\em arXiv preprint arXiv:1810.05331}, 2018.

\bibitem{guo2021gdp}
Yi~Guo, Huan Yuan, Jianchao Tan, Zhangyang Wang, Sen Yang, and Ji~Liu.
\newblock Gdp: Stabilized neural network pruning via gates with differentiable
  polarization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5239--5250, 2021.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition. corr abs/1512.03385
  (2015), 2015.

\bibitem{he2018amc}
Yihui He, Ji~Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 784--800, 2018.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{hu2016network}
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Network trimming: A data-driven neuron pruning approach towards
  efficient deep architectures.
\newblock {\em arXiv preprint arXiv:1607.03250}, 2016.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{huang2017speed}
Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara,
  Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama,
  et~al.
\newblock Speed/accuracy trade-offs for modern convolutional object detectors.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7310--7311, 2017.

\bibitem{jeong2022optimal}
Yeonwoo Jeong, Deokjae Lee, Gaon An, Changyong Son, and Hyun~Oh Song.
\newblock Optimal channel selection with discrete qcqp.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 7915--7941. PMLR, 2022.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
\newblock {SNIP}: Single-shot network pruning based on connection sensitivity.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{leroux2018iamnn}
Sam Leroux, Pavlo Molchanov, Pieter Simoens, Bart Dhoedt, Thomas Breuel, and
  Jan Kautz.
\newblock Iamnn: Iterative and adaptive mobile neural network for efficient
  image classification.
\newblock {\em arXiv preprint arXiv:1804.10123}, 2018.

\bibitem{li2020eagleeye}
Bailin Li, Bowen Wu, Jiang Su, and Guangrun Wang.
\newblock Eagleeye: Fast sub-net evaluation for efficient neural network
  pruning.
\newblock In {\em European Conference on Computer Vision}, pages 639--654.
  Springer, 2020.

\bibitem{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 2980--2988, 2017.

\bibitem{liu2016ssd}
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
  Cheng-Yang Fu, and Alexander~C Berg.
\newblock Ssd: Single shot multibox detector.
\newblock In {\em European conference on computer vision}, pages 21--37.
  Springer, 2016.

\bibitem{liu2019metapruning}
Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng,
  and Jian Sun.
\newblock Metapruning: Meta learning for automatic neural network channel
  pruning.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3296--3305, 2019.

\bibitem{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{luo2020neural}
Jian-Hao Luo and Jianxin Wu.
\newblock Neural network pruning with residual-connections and limited-data.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1458--1467, 2020.

\bibitem{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 5058--5066, 2017.

\bibitem{lym2019prunetrain}
Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and
  Mattan Erez.
\newblock Prunetrain: fast neural network training by dynamic sparse model
  reconfiguration.
\newblock In {\em Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--13, 2019.

\bibitem{ma2018shufflenet}
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
\newblock Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 116--131, 2018.

\bibitem{martello1999dynamic}
Silvano Martello, David Pisinger, and Paolo Toth.
\newblock Dynamic programming and strong bounds for the 0-1 knapsack problem.
\newblock {\em Management science}, 45(3):414--424, 1999.

\bibitem{martello1990knapsack}
Silvano Martello and Paolo Toth.
\newblock {\em Knapsack problems: algorithms and computer implementations}.
\newblock John Wiley \& Sons, Inc., 1990.

\bibitem{molchanov2019importance}
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz.
\newblock Importance estimation for neural network pruning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11264--11272, 2019.

\bibitem{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock {\em arXiv preprint arXiv:1611.06440}, 2016.

\bibitem{mullapudi2019online}
Ravi~Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, and Kayvon
  Fatahalian.
\newblock Online model distillation for efficient video inference.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3573--3582, 2019.

\bibitem{nvidiarecipe}
Nvidia.
\newblock Convolutional networks for image classification in pytorch.
\newblock
  \url{https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets#convolutional-networks-for-image-classification-in-pytorch},
  2020.

\bibitem{paszke2017pytorch}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In {\em NIPS-W}, 2017.

\bibitem{radosavovic2020designing}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr
  Doll{\'a}r.
\newblock Designing network design spaces.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10428--10436, 2020.

\bibitem{radu2019performance}
Valentin Radu, Kuba Kaszyk, Yuan Wen, Jack Turner, Jos{\'e} Cano, Elliot~J
  Crowley, Bj{\"o}rn Franke, Amos Storkey, and Michael O'Boyle.
\newblock Performance aware convolutional neural network channel pruning for
  embedded gpus.
\newblock In {\em 2019 IEEE International Symposium on Workload
  Characterization (IISWC)}, pages 24--34. IEEE, 2019.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in neural information processing systems}, 28:91--99,
  2015.

\bibitem{imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em Int. J. Comput. Vision}, 115(3):211–252, December 2015.

\bibitem{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem{shen2022prune}
Maying Shen, Pavlo Molchanov, Hongxu Yin, and Jose~M Alvarez.
\newblock When to prune? a policy towards early structural pruning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12247--12256, 2022.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{sui2021chip}
Yang Sui, Miao Yin, Yi~Xie, Huy Phan, Saman Aliari~Zonouz, and Bo~Yuan.
\newblock Chip: Channel independence-based pruning for compact neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  34:24604--24616, 2021.

\bibitem{tan2019mnasnet}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2820--2828, 2019.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{tiwari2021chipnet}
Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, and Deepak Gupta.
\newblock Chipnet: Budget-aware pruning with heaviside continuous
  approximations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{vahdat2020unas}
Arash Vahdat, Arun Mallya, Ming-Yu Liu, and Jan Kautz.
\newblock Unas: Differentiable architecture search meets reinforcement
  learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11266--11275, 2020.

\bibitem{Wang2020grasp}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wang2021neural}
Huan Wang, Can Qin, Yulun Zhang, and Yun Fu.
\newblock Neural pruning via growing regularization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{wang2020apq}
Tianzhe Wang, Kuan Wang, Han Cai, Ji~Lin, Zhijian Liu, Hanrui Wang, Yujun Lin,
  and Song Han.
\newblock Apq: Joint search for network architecture, pruning and quantization
  policy.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2078--2087, 2020.

\bibitem{wu2019fbnet}
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu,
  Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer.
\newblock Fbnet: Hardware-aware efficient convnet design via differentiable
  neural architecture search.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10734--10742, 2019.

\bibitem{wu2020constraint}
Yu-Cheng Wu, Chih-Ting Liu, Bo-Ying Chen, and Shao-Yi Chien.
\newblock Constraint-aware importance estimation for global filter pruning
  under multiple resource constraints.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 686--687, 2020.

\bibitem{xia2021fully}
Wenhan Xia, Hongxu Yin, Xiaoliang Dai, and Niraj~K Jha.
\newblock Fully dynamic inference with deep neural networks.
\newblock {\em IEEE Transactions on Emerging Topics in Computing}, 2021.

\bibitem{yang2018netadapt}
Tien-Ju Yang, Andrew Howard, Bo~Chen, Xiao Zhang, Alec Go, Mark Sandler,
  Vivienne Sze, and Hartwig Adam.
\newblock Netadapt: Platform-aware neural network adaptation for mobile
  applications.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 285--300, 2018.

\bibitem{yin2020dreaming}
Hongxu Yin, Pavlo Molchanov, Jose~M Alvarez, Zhizhong Li, Arun Mallya, Derek
  Hoiem, Niraj~K Jha, and Jan Kautz.
\newblock Dreaming to distill: Data-free knowledge transfer via
  {D}eep{I}nversion.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8715--8724, 2020.

\bibitem{yin2022vit}
Hongxu Yin, Arash Vahdat, Jose~M Alvarez, Arun Mallya, Jan Kautz, and Pavlo
  Molchanov.
\newblock A-{ViT}: {A}daptive tokens for efficient vision transformer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10809--10818, 2022.

\bibitem{you2019gate}
Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping Wang.
\newblock Gate decorator: Global filter pruning method for accelerating deep
  convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1909.08174}, 2019.

\bibitem{yu2019autoslim}
Jiahui Yu and Thomas Huang.
\newblock Autoslim: Towards one-shot architecture search for channel numbers.
\newblock {\em arXiv preprint arXiv:1903.11728}, 2019.

\bibitem{zhu2016trained}
Chenzhuo Zhu, Song Han, Huizi Mao, and William~J Dally.
\newblock Trained ternary quantization.
\newblock {\em arXiv preprint arXiv:1612.01064}, 2016.

\end{thebibliography}
