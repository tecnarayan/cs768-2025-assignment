
@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{goujaud2023provable,
  title={Provable non-accelerations of the heavy-ball method},
  author={Goujaud, Baptiste and Taylor, Adrien and Dieuleveut, Aymeric},
  journal={arXiv preprint arXiv:2307.11291},
  year={2023}
}

@inproceedings{wang2022provable,
  title={Provable acceleration of heavy ball beyond quadratics for a class of polyak-lojasiewicz functions when the non-convexity is averaged-out},
  author={Wang, Jun-Kun and Lin, Chi-Heng and Wibisono, Andre and Hu, Bin},
  booktitle={International Conference on Machine Learning},
  pages={22839--22864},
  year={2022},
  organization={PMLR}
}
@ARTICLE{XCHZ20,
  title={Analytical convergence regions of accelerated gradient descent in nonconvex optimization under Regularity Condition},
  author={Huaqing Xiong and Yuejie Chi and Bin Hu and Wei Zhang},
  journal={Automatica},
  year={2020}
}


@ARTICLE{HU20,
  title={ECE 598: (Lecture 2) {U}nifying the Analysis in Control and Optimization via Semidefinite Programs},
  author={Bin Hu},
  url={https://binhu7.github.io/courses/ECE598/Fall2020/files/LectureNote2_SDPLTI.pdf},
  year={2020}
}


@ARTICLE{wang2018acceleration,
  title={Acceleration through optimistic no-regret dynamics},
  author={Wang, Jun-Kun and Abernethy, Jacob D},
  journal={NeurIPS},
  year={2018}
}


@ARTICLE{wang2018acceleration,
  title={Acceleration through optimistic no-regret dynamics},
  author={Wang, Jun-Kun and Abernethy, Jacob D},
  journal={NeurIPS},
  year={2018}
}


@ARTICLE{JKKNS18,
  title={Accelerating Stochastic Gradient Descent For Least Squares Regression},
  author={Prateek Jain and Sham M. Kakade and Rahul Kidambi and Praneeth Netrapalli and Aaron Sidford},
  journal   = {COLT},
  year      = {2018}
}


@ARTICLE{DL18,
  title={Error bounds, quadratic growth, and linear convergence of proximal methods},
  author={Dmitriy Drusvyatskiy and Adrian S. Lewis},
  journal   = {Mathematics of Operations Research },
  year      = {2018}
}

@ARTICLE{OBP15,
  title={{iPiasco}: Inertial Proximal Algorithm for strongly convex Optimization},
  author={Peter Ochs and Thomas Brox and Thomas Pock},
  journal   = {Journal of Mathematical Imaging and Vision},
  year      = {2015}
}


@ARTICLE{LT93,
  title={Error bounds and convergence analysis of feasible descent methods: A general approach},
  author={Zhi-Quan Luo and Paul Tseng},
  journal   = {Annals of Operations Research},
  year      = {1993}
}


@ARTICLE{LPPSJR19,
  title={First-order Methods Almost Always Avoid Saddle Points},
  author={Jason D. Lee and Ioannis Panageas and Georgios Piliouras and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  journal   = {Mathematical Programming},
  year      = {2019}
}

@ARTICLE{OS19,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Samet Oymak and Mahdi Soltanolkotabi},
  journal   = {ICML},
  year      = {2019}
}


@ARTICLE{BDL07,
  title={The Lojasiewicz Inequality for Nonsmooth Subanalytic Functions with Applications to Subgradient Dynamical Systems},
  author={Jerome Bolte and Aris Daniilidis and Adrian Lewis},
  journal   = {SIAM Journal on Optimization},
  year      = {2007}
}

@ARTICLE{ABRS10,
  title={Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequality},
  author={Hedy Attouch and Jerome Bolte and Patrick Redont and Antoine Soubeyran},
  journal   = {Mathematics of operations research},
  year      = {2010}
}


@ARTICLE{WAL21,
  title={No-Regret Dynamics in the Fenchel Game: A Unified Framework for Algorithmic Convex Optimization},
  author={Jun-Kun Wang and Jacob Abernethy and Kfir Y. Levy},
  journal   = {arXiv:2111.11309},
  year      = {2021}
}


@ARTICLE{AGV21,
  title={Convergence rates for the Heavy-Ball continuous dynamics for non-convex optimization, under Polyak-Łojasiewicz conditioning},
  author={Vassilis Apidopoulos and Nicolo Ginatta and Silvia Villa},
  journal   = {arXiv:2107.10123},
  year      = {2021}
}


@ARTICLE{OV00,
  title={Generalization of an Inequality by Talagrand and Links with the Logarithmic Sobolev Inequality},
  author={Felix Otto and Cedric Villani},
  journal   = {Journal of Functional Analysis},
  year      = {2000}
}

@ARTICLE{MCCFBJ19,
  title={Is There an Analog of Nesterov Acceleration for MCMC?},
  author={Yi-An Ma and Niladri S. Chatterji and Xiang Cheng and Nicolas Flammarion and Peter L. Bartlett and Michael I. Jordan},
  journal   = {arXiv:1902.00996},
  year      = {2019}
}





@ARTICLE{ACGS21,
  title={Averaging on the Bures-Wasserstein manifold: dimension-free convergence of gradient descent},
  author={Jason Altschuler and Sinho Chewi and Patrik Gerber and Austin Stromme},
  journal   = {NeurIPS},
  year      = {2021}
}

@ARTICLE{MSS21,
  title={Non-asymptotic convergence bounds for Wasserstein approximation using point clouds},
  author={Quentin Merigot and Filippo Santambrogio and Clement Sarrazin},
  journal   = {NeurIPS},
  year      = {2021}
}

@ARTICLE{LZB21,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  journal   = {},
  year      = {2021}
}

@ARTICLE{C21,
  title={Sparse Optimization on Measures with Over-parameterized Gradient Descent},
  author={Lenaic Chizat},
  journal   = {Mathematical Programming},
  year      = {2021}
}


@ARTICLE{PP21,
  title={Dynamics of Stochastic Momentum Methods on Large-scale, Quadratic Models},
  author={Courtney Paquette and Elliot Paquette},
  journal   = {NeurIPS},
  year      = {2021}
}


@ARTICLE{WLA21,
  title={A Modular Analysis of Provable Acceleration via Polyak's Momentum: Training a Wide ReLU Network and a Deep Linear Network},
  author={Jun-Kun Wang and Chi-Heng Lin and Jacob Abernethy},
  journal   = {ICML},
  year      = {2021}
}


@ARTICLE{CM21,
  title={High-probability bounds for Non-Convex Stochastic Optimization with Heavy Tails},
  author={Ashok Cutkosky and Harsh Mehta},
  journal   = {NeurIPS},
  year      = {2021}
}


@ARTICLE{KS21,
  title={Continuous Time Analysis of Momentum Methods},
  author={Nikola B. Kovachki and Andrew M. Stuart},
  journal   = {JMLR},
  year      = {2021}
}


@ARTICLE{LWA21,
  title={What Happens after SGD Reaches Zero Loss? --A Mathematical Framework},
  author={Zhiyuan Li and Tianhao Wang and Sanjeev Arora},
  journal   = {arXiv:2110.06914},
  year      = {2021}
}


@ARTICLE{KNS17,
  title={Linear Convergence under the Polyak-Łojasiewicz Inequality},
  author={Hamed Karimi and Julie Nutini and Mark Schmidt},
  year      = {2017}
}



@ARTICLE{KNS16,
  title={Linear Convergence of Gradient and Proximal-Gradient Methods under the Polyak-Lojasiewicz Condition},
  author={Hamed Karimi and Julie Nutini and Mark Schmidt},
  journal   = {ECML},
  year      = {2016}
}



@ARTICLE{LZB21,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  journal   = {arXiv:2003.00307},
  year      = {2021}
}



@ARTICLE{OS19,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Samet Oymak and Mahdi Soltanolkotabi},
  journal   = {ICML},
  year      = {2019}
}


@ARTICLE{FGM18,
  title={Condition Number Analysis of Logistic Regression, and its Implications for Standard First-Order Solution Methods},
  author={Robert M. Freund and Paul Grigas and Rahul Mazumder},
  journal   = {arXiv:1810.08727},
  year      = {2018}
}


@ARTICLE{B10,
  title={Self-concordant analysis for logistic regression},
  author={Francis Bach},
  journal   = {Electron. J. Statistics},
  year      = {2010}
}



@ARTICLE{GFJ15,
  title={Global convergence of the Heavy-ball method for convex optimization},
  author={Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal   = {ECC},
  year      = {2015}
}


@ARTICLE{B07,
  title={Perturbation Bounds for Matrix Eigenvalues},
  author={Rajendra Bhatia},
  journal   = {SIAM},
  year      = {2007}
}



@ARTICLE{L20,
  title={Solving Large Scale Cubic Regularization by a Generalized Eigenvalue Problem},
  author={Felix Lieder},
  journal   = {SIAM Journal on Optimization},
  year      = {2020}
}


@ARTICLE{G19,
  title={Negative momentum for improved game dynamics},
  author={Gidel, Gauthier and Hemmat, Reyhane Askari and Pezeshki, Mohammad and Le Priol, R{\'e}mi and Huang, Gabriel and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal   = {AISTATS},
  year      = {2019}
}


@ARTICLE{CW06,
  title={Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees},
  author={Yudong Chen and Martin J. Wainwright},
  journal   = {COLT},
  year      = {2016}
} 


@ARTICLE{BKS06,
  title={Dropping Convexity for Faster Semi-definite Optimization},
  author={Srinadh Bhojanapalli and Anastasios Kyrillidisy and Sujay Sanghaviy},
  journal   = {COLT},
  year      = {2016}
} 


@ARTICLE{ZKHC21,
  title={On the computational and statistical complexity of over-parameterized matrix sensing},
  author={Jiacheng Zhuo and Jeongyeol Kwon and Nhat Ho and Constantine Caramanis},
  journal   = {arXiv:2102.02756},
  year      = {2021}
} 


@ARTICLE{KDP18,
  title={Non-monotone behavior of the heavy ball method},
  author={Anastasiya Kulakova and Marina Danilova and Boris Polyak},
  journal   = {arXiv:1811.00658},
  year      = {2018}
} 

@ARTICLE{RBU20,
  title={Linear Convergence and Implicit Regularization of Generalized Mirror Descent with Time-Dependent Mirrors},
  author={Adityanarayanan Radhakrishnan and Mikhail Belkin and Caroline Uhler},
  journal   = {arXiv:2009.08574},
  year      = {2020}
} 


@ARTICLE{JST21,
  title={Fast margin maximization via dual acceleration},
  author={Ziwei Ji and Nati Srebro and Matus Telgarsky},
  journal   = {ICML},
  year      = {2021}
} 


@ARTICLE{MCCFBJ21,
  title={Is there an analog of Nesterov acceleration for MCMC?},
  author={Yi-An Ma and Niladri S. Chatterji and Xiang Cheng and Nicolas Flammarion and Peter L. Bartlett and Michael I. Jordan},
  journal   = {Bernoulli},
  year      = {2021}
} 


@ARTICLE{BZL21,
  title={Local and Global Linear Convergence of General Low-rank Matrix Recovery Problems},
  author={Yingjie Bi and Haixiang Zhang and Javad Lavaei},
  journal   = {arXiv},
  year      = {2021}
} 


@ARTICLE{WR21,
  title={Implicit Regularization in Matrix Sensing via Mirror Descent},
  author={Fan Wu and Patrick Rebeschini},
  journal   = {arXiv},
  year      = {2021}
} 

@ARTICLE{ADR20,
  title={Convergence rates of the Heavy-Ball method with Lojasiewicz property},
  author={Jean-François Aujol and Charles Dossal and Aude Rondepierre},
  journal   = {hal-02928958},
  year      = {2020}
} 



@ARTICLE{WLA21,
  title={A Modular Analysis of Provable Acceleration via Polyak's momentum: Training a Wide ReLU Network and a Deep Linear Network},
  author={Jun-Kun Wang and Chi-Heng Lin and Jacob Abernethy},
  journal   = {ICML},
  year      = {2021}
} 


@ARTICLE{LZB20b,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  journal   = {arXiv:2003.00307},
  year      = {2020}
} 


@ARTICLE{WR20,
  title={A Continuous-Time Mirror Descent Approach to Sparse Phase Retrieval},
  author={Fan Wu and Patrick Rebeschini},
  journal   = {NeurIPS},
  year      = {2020}
}



@ARTICLE{LY17,
  title={Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
  author={Yuanzhi Li and Yang Yuan},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{SHL18,
  title={Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced},
  author={Simon S. Du and Wei Hu and Jason D. Lee},
  journal   = {NeurIPS},
  year      = {2018}
} 


@ARTICLE{YZQM20,
  title={Robust Recovery via Implicit Bias of Discrepant Learning Rates for Double Over-parameterization},
  author={Chong You and Zhihui Zhu and Qing Qu and Yi Ma},
  journal   = {arXiv:2006.08857},
  year      = {2020}
} 



@ARTICLE{ALH19,
  title={Stochastic Mirror Descent on Overparameterized Nonlinear Models: Convergence, Implicit Regularization, and Generalization},
  author={Navid Azizan and Sahin Lale and Babak Hassibi},
  journal   = {arXiv:1906.03830},
  year      = {2019}
} 




@ARTICLE{ADT20,
  title={The Implicit Regularization of Stochastic Gradient Flow for Least Squares},
  author={Alnur Ali and Edgar Dobriban and Ryan J. Tibshiran},
  journal   = {ICML},
  year      = {2020}
} 


@ARTICLE{FYY20,
  title={Understanding Implicit Regularization in Over-Parameterized Nonlinear Statistical Model},
  author={Jianqing Fan and Zhuoran Yang and Mengxin Yu},
  journal   = {arXiv:2007.08322},
  year      = {2020}
} 


@ARTICLE{MMRV20,
  title={Implicit regularization for convex regularizers},
  author={Cesare Molinari and Mathurin Massias and Lorenzo Rosasco and Silvia Villa},
  journal   = {arXiv:2006.09859},
  year      = {2020}
} 



@ARTICLE{VKR19,
  title={Implicit Regularization for Optimal Sparse Recovery},
  author={Tomas Va{\v s}kevi{\v c}ius and Varun Kanade and Patrick Rebeschini},
  journal   = {NeurIPS},
  year      = {2019}
}M

@ARTICLE{HWLM20,
  title={Shape Matters: Understanding the Implicit Bias of the Noise Covariance},
  author={Jeff Z. HaoChen and Colin Wei and Jason D. Lee and Tengyu Ma},
  journal   = {arXiv:2006.08680},
  year      = {2020}
} 


@ARTICLE{CB20,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Lenaic Chizat and Francis Bach},
  journal   = {COLT},
  year      = {2020}
} 


@ARTICLE{GSD20,
  title={The implicit bias of depth: How incremental learning drives generalization},
  author={Daniel Gissin and Shai Shalev-Shwartz and Amit Daniely},
  journal   = {ICLR},
  year      = {2020}
} 

@ARTICLE{S20,
  title={Gradient Methods Never Overfit On Separable Data},
  author={Ohad Shamir},
  journal   = {arXiv:2007.00028},
  year      = {2020}
} 

@ARTICLE{JDET20,
  title={Gradient descent follows the regularization path for general losses},
  author={Ziwei Ji and Miroslav Dud{\'}k and Robert E. Schapire and Matus Telgarsky},
  journal   = {COLT},
  year      = {2020}
} 


@ARTICLE{GLSS18,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Suriya Gunasekar and Jason D. Lee and Daniel Soudry and Nathan Srebro},
  journal   = {ICML},
  year      = {2018}
} 

@ARTICLE{JT19,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ziwei Ji and Matus Telgarsky},
  journal   = {ICLR},
  year      = {2019}
} 


@ARTICLE{JT19,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ziwei Ji and Matus Telgarsky},
  journal   = {ICLR},
  year      = {2019}
} 


@ARTICLE{LL20,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Kaifeng Lyu and Jian Li},
  journal   = {ICLR},
  year      = {2020}
} 


@ARTICLE{SHNGS18,
  title={Implicit regularization in deep learning may not be explainable by
norms},
  author={Noam Razin and Nadav Cohen},
  journal   = {arXiv:2005.06398},
  year      = {2020}
} 


@ARTICLE{SHNGS18,
  title={The implicit bias of gradient descent on separable data},
  author={Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
  journal   = {JMLR},
  year      = {2018}
} 

@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {arXiv:2007.06738},
  year      = {2020}
} 


@ARTICLE{T13,
  title={Margins, shrinkage and boosting},
  author={Matus Telgarsky},
  journal   = {ICML},
  year      = {2013}
} 


@ARTICLE{WGSMGLSS20,
  title={Kernel and rich regimes in overparametrized models},
  author={Blake Woodworth and Suriya Gunasekar and Pedro Savarese and Edward Moroshko and Itay Golan and Jason Lee and Daniel Soudry and Nathan Srebro},
  journal   = {COLT},
  year      = {2020}
} 


@ARTICLE{BGLHL20,
  title={Implicit Regularization in Deep Learning: A View from Function Space},
  author={Aristide Baratin and Thomas George and C{\'e}sar Laurent and R Devon Hjelm and Guillaume Lajoie and Pascal Vincent and Simon Lacoste-Julien},
  journal   = {arXiv:2008.00938},
  year      = {2020}
} 


@ARTICLE{NTSS17,
  title={Geometry of Optimization and Implicit Regularization in Deep Learning},
  author={Behnam Neyshabur and Ryota Tomioka and Ruslan Salakhutdinov and Nathan Srebro},
  journal   = {arXiv:1705.03071},
  year      = {2017}
} 

@ARTICLE{NH17,
  title={The loss surface of deep and wide neural networks},
  author={Quynh Nguyen and Matthias Hein},
  journal   = {ICML},
  year      = {2017}
} 


@ARTICLE{NH17,
  title={The loss surface of deep and wide neural networks},
  author={Quynh Nguyen and Matthias Hein},
  journal   = {ICML},
  year      = {2017}
}

@ARTICLE{VBB19,
  title={Spurious valleys in one-hidden-layer neural network optimization landscapes},
  author={Luca Venturi and Afonso S Bandeira and Joan Bruna},
  journal   = {JMLR},
  year      = {2019}
}


@ARTICLE{MVZ20,
  title={Optimization and Generalization of Shallow Neural Networks with
Quadratic Activation Functions},
  author={Stefano Sarao Mannelia and Eric Vanden-Eijnden and Lenka Zdeborov{\'a}},
  journal   = {arXiv:2006.15459},
  year      = {2020}
}


@ARTICLE{MBCKUZ20,
  title={Complex Dynamics in Simple Neural Networks: Understanding Gradient Flow in Phase Retrieval},
  author={Stefano Sarao Mannellia and Giulio Birolib and Chiara Cammarotac and  Florent Krzakalab and Pierfrancesco Urbania and Lenka Zdeborov{\'a}},
  journal   = {arXiv:2006.06997},
  year      = {2020}
}


@ARTICLE{FCG20,
  title={Agnostic Learning of a Single Neuron with Gradient Descent},
  author={Spencer Frei and Yuan Cao and Quanquan Gu},
  journal   = {arXiv:2005.14426},
  year      = {2020}
}


@ARTICLE{GWBNS17,
  title={Implicit Regularization in Matrix Factorization},
  author={Suriya Gunasekar and Blake Woodworth and Srinadh Bhojanapalli and Behnam Neyshabur and Nathan Srebro},
  journal   = {NIPS},
  year      = {2017}
}

@ARTICLE{GBL19,
  title={Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks},
  author={Gauthier Gidel and Francis Bach and Simon Lacoste-Julien},
  journal   = {NeurIPS},
  year      = {2019}
}




@ARTICLE{SY19,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Lili Su and Pengkun Yang},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{T20,
  title={Student Specialization in Deep Rectified Networks With Finite Width and Input Dimension},
  author={Yuandong Tian},
  journal   = {ICML},
  year      = {2020}
}


@ARTICLE{AS20,
  title={The Numerics of Phase Retrieval},
  author={Albert Fannjiang and Thomas Strohmer},
  journal   = {Acta Numerica},
  year      = {2020}
}


@ARTICLE{JT20,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ziwei Ji and Matus Telgarsky},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{ZCZG19,
  title={Stochastic gradient descent optimizes overparameterized deep relu networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal   = {Machine Learning, Springer},
  year      = {2019}
}


@ARTICLE{DZPS19,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{ZL19_icml,
  title={A convergence theory for deep learning via overparameterization},
  author={Zeyuan Allen-Zhu and Yuanzhi Li and and Zhao Song},
  journal   = {ICML},
  year      = {2019}
}

@ARTICLE{ZCZG19},
  title={Stochastic gradient descent optimizes overparameterized deep relu networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal   = {Machine Learning, Springer},
  year      = {2019}
}


@ARTICLE{HHS18,
  title={Fix your classifier: the marginal value of training the last weight layer},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {ICLR},
  year      = {2018}
}


@ARTICLE{KLD19,
  title={No Spurious Local Minima in Deep Quadratic Networks},
  author={Abbas Kazemipour and Brett Larsen and Shaul Druckmann},
  journal   = {arXiv:2001.00098},
  year      = {2019}
}


@ARTICLE{BG19,
  title={Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem},
  author={Alon Brutzkus and Amir Globerson},
  journal   = {ICML},
  year      = {2019}
}


@ARTICLE{GASKZ19,
  title={Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
  author={Sebastian Goldt and Madhu Advani and Andrew M. Saxe and Florent Krzakala and Lenka Zdeborova},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{GMMM19,
  title={Limitations of Lazy Training of Two-layers Neural Networks},
  author={Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{BHK18,
  title={Foundations of Data Science},
  author={Avrim Blum and John Hopcroft and Ravindran Kannan},
  journal   = {Neural computation},
  year      = {2018}
}


@ARTICLE{BHL19,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
  author={Peter L Bartlett and David P Helmbold and Philip M Long},
  journal   = {Neural computation},
  year      = {2019}
}

@ARTICLE{MS18,
  title={Notes on first-order methods for minimizing smooth functions},
  author={Michael Saunders},
  journal   = {Lecture note},
  year      = {2018}
}

@ARTICLE{S14,
  title={Algorithms and Theory for Clustering and Nonconvex Quadratic Programming},
  author={Mahdi Soltanolkotabi},
  journal   = {Stanford University Ph. D. Dissertation},
  year      = {2014}
}



@ARTICLE{D20,
  title={Memorizing Gaussians with no over-parameterizaion via gradient decent on neural networks},
  author={Amit Daniely},
  journal   = {arXiv:1909.11837},
  year      = {2020}
}


@ARTICLE{GWZ19,
  title={Mildly overparametrized neural nets can memorize training data efficiently},
  author={Rong Ge and Runzhe Wang and Haoyu Zhao},
  journal   = {arXiv:1909.11837},
  year      = {2019}
}

@ARTICLE{GKZ19,
  title={Stationary Points of Shallow Neural Networks with Quadratic Activation Function},
  author={David Gamarnik and Eren C. Kızıldag and Ilias Zadik},
  journal   = {arXiv:1912.01599},
  year      = {2019}
}

@ARTICLE{SJL18,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Mahdi Soltanolkotabi and Adel Javanmard and Jason D. Lee},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2018}
}


@ARTICLE{DL18,
  title={On the Power of Over-parametrization in Neural Networks with Quadratic Activation},
  author={Simon Du and Jason Lee},
  journal   = {ICML},
  year      = {2018}
}

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 

 
@ARTICLE{BG17,
  title={Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs},
  author={Alon Brutzkus and Amir Globerson},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{S17,
  title={Learning Relus via Gradient Descent},
  author={Mahdi Soltanolkotabi},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{KSA19,
  title={Fitting relus via sgd and quantized sgd},
  author={Seyed Mohammadreza Mousavi Kalan and Mahdi Soltanolkotabi and A. Salman Avestimehr},
  journal   = {ISIT},
  year      = {2019}
}

@ARTICLE{MBM17,
  title={The landscape of empirical risk for non-convex losses},
  author={Song Mei and Yu Bai and Andrea Montanari},
  journal   = {PNAS},
  year      = {2018}
}

@ARTICLE{T17,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Yuandong Tian},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{GKM19,
  title={Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals},
  author={Surbhi Goel and Adam Klivans and Raghu Meka},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{GKM18,
  title={Learning One Convolutional Layer with Overlapping Patches},
  author={Surbhi Goel and Adam Klivans and Raghu Meka},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{GKLW16,
  title={Learning Two-layer Neural Networks with Symmetric Inputs},
  author={Rong Ge and Rohith Kuditipudi and Zhize Li and Xiang Wang},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{JSA16,
  title={Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods},
  author={Majid Janzamin and Hanie Sedghi and Anima Anandkumar},
  journal   = {arXiv:1506.08473},
  year      = {2016}
}

@ARTICLE{KKSK11,
  title={Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression?},
  author={Sham M. Kakade and Varun Kanade and Ohad Shamir and Adam Kalai},
  journal   = {NeurIPS},
  year      = {2011}
}


@ARTICLE{DLT18,
  title={When is a Convolutional Filter Easy To Learn?},
  author={Simon S. Du and Jason D. Lee and Yuandong Tian},
  journal   = {ICLR},
  year      = {2018}
}

@ARTICLE{GKKT17,
  title={Reliably Learning the ReLU in Polynomial Time},
  author={Surbhi Goel and Varun Kanade and Adam Klivans and Justin Thaler},
  journal   = {COLT},
  year      = {2017}
}

@ARTICLE{YS20,
  title={Learning a Single Neuron with Gradient Methods},
  author={Gilad Yehudai and Ohad Shamir},
  journal   = {arXiv:2001.05205},
  year      = {2020}
}




@ARTICLE{EGKZ20,
  title={Neural Networks and Polynomial Regression. Demystifying the Overparametrization Phenomena},
  author={Matt Emschwiller and David Gamarnik and Eren C. Kızıldag and Ilias Zadik},
  journal   = {arXiv:2003.10523},
  year      = {2020}
}

@ARTICLE{ACHL19,
  title={Implicit Regularization in Deep Matrix Factorization},
  author={Sanjeev Arora and Nadav Cohen and Wei Hu and Yuping Luo},
  journal   = {NerurIPS},
  year      = {2019}
}

@ARTICLE{KB15,
  title={Adam: A method for stochastic optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal   = {ICLR},
  year      = {2015}
}


@ARTICLE{LSZG19,
  title={Aggregated Momentum: Stability Through Passive Damping},
  author={James Lucas and Shengyang Sun and Richard Zemel and Roger Grosse},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{ACH18,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Sanjeev Arora and Nadav Cohen and Elad Hazan},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{LSS14,
  title={On the computational efficiency of training neural networks},
  author={Roi Livni and Shai Shalev-Shwartz and Ohad Shamir},
  journal   = {NeurIPS},
  year      = {2014}
}


@ARTICLE{JA20a,
  title={Solving Phase Retrieval Faster with Momentum},
  author={Jun-Kun Wang and Jacob Abernethy},
  journal   = {},
  year      = {2020}
}


@ARTICLE{LMCC19,
  title={Nonconvex Matrix Factorization from Rank-One Measurements},
  author={Yuanxin Li and Cong Ma and Yuxin Chen and Yuejie Chi},
  journal   = {AISTATS},
  year      = {2019}
}

@ARTICLE{AFWZ17,
  title={Entrywise Eigenvector Analysis of Random Matrices with Low Expected Rank},
  author={Emmanuel Abbe and Jianqing Fan and Kaizheng Wang and Yiqiao Zhong},
  journal   = {arXiv:1709.09565},
  year      = {2017}
}


@ARTICLE{GLZX19,
  title={Understanding the Role of Momentum in Stochastic Gradient Methods},
  author={Igor Gitman and Hunter Lang and Pengchuan Zhang and Lin Xiao},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NIPS},
  year      = {2017}
}

@ARTICLE{CJY18,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Yuansi Chen and Chi Jin and Bin Yu},
  journal   = {arXiv:1804.01619},
  year      = {2018}
}


@ARTICLE{LR18,
  title={Accelerated Gossip via Stochastic Heavy Ball Method},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {Allerton},
  year      = {2018}
}


@ARTICLE{KH1918,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  journal   = {ICLR},
  year      = {2019}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NIPS},
  year = {2017}
}

@article{BCMN14,
 author = {Afonso S. Bandeira and Jameson Cahill and Dustin G. Mixon and Aaron A. Nelson},
 title = {Saving phase: Injectivity and stability for phase retrieval},
 journal = {Applied and Computational Harmonic Analysis},
 year = {2014}
} 


@article{WCA20,
 author = {Jun-Kun Wang and Chi-Heng Lin and Jacob Abernethy},
 title = {Escaping Saddle Points Faster with Stochastic Momentum},
 journal = {ICLR},
 year = {2020}
} 

@article{WSW16,
 author = {Chris D. White and Sujay Sanghavi and Rachel Ward},
 title = {The local convexity of solving systems of quadratic equations},
 journal = {Results in Mathematics},
 year = {2016}
} 


@article{ZL15,
 author = {Qinqing Zheng and John Lafferty},
 title = {A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements},
 journal = {NIPS},
 year = {2015}
} 

@article{TBSSR16,
 author = {Stephen Tu and Ross Boczar and Max Simchowitz and Mahdi Soltanolkotabi and Benjamin Recht},
 title = {Low-rank Solutions of Linear Matrix Equations via Procrustes Flow},
 journal = {ICML},
 year = {2016}
} 

@article{DDP18,
 author = {Damek Davis and Dmitriy Drusvyatskiy and Courtney Paquette},
 title = {The nonsmooth landscape of phase retrieval},
 journal = {IMA Journal on Numerical Analysis},
 year = {2018}
} 

@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometric Analysis of Phase Retrieval},
 journal = {IEEE ISIT},
 year = {2016}
} 

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 


@article{LGL15,
 author = {Gen Li and Yuantao Gu and Yue M. Lu},
 title = {Phase retrieval using iterative projections: Dynamics in the large systems limit},
 journal = {IEEE Allerton},
 year = {2015}
} 

@article{MXM18,
 author = {Junjie Ma and Ji Xu and Arian Maleki},
 title = {Optimization-based AMP for Phase Retrieval: The Impact of Initialization and l2-regularization},
 journal = {IEEE Transactions on Information Theory},
 year = {2018}
} 


@article{Z19,
 author = {Teng Zhang},
 title = {Phase retrieval using alternating minimization in a batch setting},
 journal = {Applied and Computational Harmonic Analysis},
 year = {2019}
} 

@article{YYFZWN18,
 author = {Zhuoran Yang and Lin Yang and Ethan Fang and Tuo Zhao and Zhaoran Wang and Matey Neykov},
 title = {Misspecified Nonconvex Statitical Optimization for Sparse Phase Retrival},
 journal = {Mathematical Programming},
 year = {2018}
} 


@article{ZCL17,
 author = {Huishuai Zhang and Yuejie Chi and Yingbin Liang},
 title = {Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow},
 journal = {ICML},
 year = {2017}
} 


@article{QZEW17,
 author = {Qing Qu and Yuqian Zhang and Yonina C. Eldar and John Wright},
 title = {Convolutional Phase Retrieval via Gradient Descent},
 journal = {NIPS},
 year = {2017}
} 


@article{ZWGC18,
 author = {Liang Zhang and Gang Wang and Georgios B. Giannakis and Jie Che},
 title = {Compressive Phase Retrieval via Reweighted Amplitude Flow},
 journal = {IEEE Transactions on Signal Processing},
 year = {2018}
} 

@article{CLW19,
 author = {Jian-Feng Cai and Haixia Liu and Yang Wan},
 title = {Fast Rank-One Alternating Minimization Algorithm for Phase Retrieval},
 journal = {Journal of Scientific Computing},
 year = {2019}
} 

@article{TV19,
 author = {Yan Shuo Tan and Roman Vershynin},
 title = {Online Stochastic Gradient Descent with Arbitrary Initialization Solves Non-smooth, Non-convex Phase Retrieval},
 journal = {arXiv:1910.12837},
 year = {2019}
} 


@article{TV18,
 author = {Yan Shuo Tan and Roman Vershynin},
 title = {Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees},
 journal = {Information and Inference },
 year = {2018}
} 


@article{BEB17,
 author = {Tamir Bendory and Yonina C. Eldar and Nicolas Boumal},
 title = {Non-Convex Phase Retrieval from STFT Measurements},
 journal = {IEEE Transactions on Signal Processing},
 year = {2017}
} 


@article{W15,
 author = {Ke Wei},
 title = {Solving systems of phaseless equations via Kaczmarz methods: A proof of concept study},
 journal = {Inverse Problems},
 year = {2015}
} 

@ARTICLE{LRP16,
  title={Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints},
  author={Laurent Lessard and Benjamin Recht and Andrew Packard},
  journal   = {SIAM Journal on Optimization},
  year      = {2016}
}


@article{CFL15,
 author = {Pengwen Chen and Albert Fannjiang and Gi-Ren Liu},
 title = {Phase Retrieval with One or Two Diffraction Patterns by Alternating Projection with Null Initialization},
 journal = {Journal of Fourier Analysis and Applications},
 year = {2015}
} 


@article{GX17,
 author = {Bing Gao and Zhiqiang Xu},
 title = {Phaseless recovery using the Gauss-Newton method},
 journal = {IEEE Transactions on Signal Processing},
 year = {2017}
} 


@article{DR18,
 author = {John Duchi and Feng Ruan},
 title = {Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval},
 journal = {Information and Inference},
 year = {2018}
} 


@article{CL16,
 author = {Yuejie Chi and Yue M. Lu},
 title = {Kaczmarz method for solving quadratic equations},
 journal = {IEEE Signal Processing Letters},
 year = {2016}
} 

@article{WGSC17,
 author = {Gang Wang and Georgios B. Giannakis and Yousef Saad and Yonina C. Eldar},
 title = {Solving Most Systems of Random Quadratic Equations},
 journal = {NIPS},
 year = {2017}
} 


@article{ZZLC17,
 author = {Huishuai Zhang and Yi Zhou and Yingbin Liang and Yuejie Chi},
 title = {A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms},
 journal = {JMLR},
 year = {2017}
} 


@article{WGE17,
 author = {Gang Wang and Georgios B. Giannakis and Yonina C. Eldar},
 title = {Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow},
 journal = {IEEE Transactions on Information Theory},
 year = {2017}
} 


@article{M18,
 author = {Mahdi Soltanolkotabi},
 title = {Structured signal recovery from quadratic measurements: Breaking sample complexity barriers via nonconvex optimization},
 journal = {IEEE Transactions on Information Theory},
 year = {2018}
} 


@article{CLM16,
 author = {T. Tony Cai and Xiaodong Li and Zongming Ma},
 title = {Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger flow},
 journal = {The Annals of Statistics},
 year = {2016}
} 


@article{NJS13,
 author = {Praneeth Netrapalli and Prateek Jain and Sujay Sanghavi},
 title = {Phase Retrieval using Alternating Minimization},
 journal = {NIPS},
 year = {2013}
} 

@article{CC17,
 author = {Yuxin Chen and Emmanuel J. Cand{\'e}s},
 title = {Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems},
 journal = {Communications on Pure and Applied Mathematics},
 year = {2017}
} 


@article{CLS15,
 author = {Emmanuel J. Cand{\'e}s and Xiaodong Li and Mahdi Soltanolkotabi},
 title = {Phase Retrieval via Wirtinger Flow: Theory and Algorithms},
 journal = {IEEE Transactions on Information Theory},
 year = {2015}
} 


@article{DH14,
 author = {Laurent Demanet and Paul Hand},
 title = {Stable optimizationless recovery from phaseless linear measurements},
 journal = {Journal of Fourier Analysis and Applications},
 year = {2014}
} 

@article{CL14,
 author = {Emmanuel J. Cand{\'e}s and Xiaodong Li},
 title = {Solving Quadratic Equations via PhaseLift when There Are About As Many Equations As Unknowns},
 journal = {Foundations of Computational Mathematics},
 year = {2014}
} 


@article{SECCMS15,
 author = {Yoav Shechtman and Yonina C. Eldar and Oren Cohen and Henry Nicholas Chapman 
 and Jianwei Miao and Mordechai Segev},
 title = {Phase retrieval with application to optical imaging: a contemporary overview},
 journal = {IEEE signal processing magazine},
 year = {2015}
} 


@article{CSV13,
 author = {Emmanuel J. Cand{\'e}s and Thomas Strohmer and Vladislav Voroninski},
 title = {PhaseLift: Exact and Stable Signal Recovery from Magnitude Measurements via Convex Programming},
 journal = {Communications on Pure and Applied Mathematics},
 year = {2013}
} 


@article{CESV12,
 author = {Emmanuel J. Cand{\'e}s and Yonina Eldar and Thomas Strohmer and Vlad Voroninski},
 title = {Phase retrieval via matrix completion},
 journal = {SIAM Journal on Imaging Sciences},
 year = {2013}
} 

@article{CCFMY18,
 author = {Yuxin Chen and Yuejie Chi and Jianqing Fan and Cong Ma and Yuling Yan},
 title = {Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval},
 journal = {Mathematical Programming},
 year = {2019}
} 


@article{CD19,
 author = {Yair Carmon and John Duchi},
 title = {Gradient Descent Finds the Cubic-Regularized Nonconvex Newton Step},
 journal = {SIAM Journal on Optimization},
 year = {2019}
} 



@article{GH15,
 author = {Dan Garber and Elad Hazan},
 title = {Fast and Simple PCA via Convex Optimization},
 journal = {arXiv:1509.05647},
 year = {2015}
} 

@article{GHM15,
 author = {Dan Garber and Elad Hazan and Tengyu Ma},
 title = {Online Learning of Eigenvectors},
 journal = {ICML},
 year = {2015}
} 

@article{ZSJBD17,
 author = {Kai Zhong and Zhao Song and Prateek Jain and Peter L. Bartlett and Inderjit S. Dhillon},
 title = {Recovery Guarantees for One-hidden-layer Neural Networks},
 journal = {ICML},
 year = {2017}
} 


@article{XHDMR18,
 author = {Peng Xu and Bryan He and Christopher De Sa and Ioannis Mitliagkas and Christopher Re},
 title = {Accelerated Stochastic Power Iteration},
 journal = {AISTATS},
 year = {2018}
} 


@article{S16,
 author = {Ohad Shamir},
 title = {Convergence of Stochastic Gradient Descent for PCA},
 journal = {ICML},
 year = {2016}
} 

@article{JJKNS16,
 author = {Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli and Aaron Sidford},
 title = {Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm},
 journal = {COLT},
 year = {2016}
} 


@article{MWCC17,
 author = {Cong Ma and Kaizheng Wang and Yuejie Chi and Yuxin Chen},
 title = {Implicit Regularization in Nonconvex Statistical Estimation:
Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind Deconvolution},
 journal = {Foundations of Computational Mathematics},
 year = {2017}
} 


@article{CGZ19,
 author = {Bugra Can and Mert G{\"u}rb{\"u}zbalaban and Lingjiong Zhu},
 title = {Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances},
 journal = {ICML},
 year = {2019}
} 



@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 


@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 

@article{SQW15,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {When Are Nonconvex Problems Not Scary?},
 journal = {NIPS Workshop on Non-convex Optimization for Machine Learning: Theory and Practice},
 year = {2015}
} 


@article{BHK15,
 author = {Avrim Blum and John Hopcroft and Ravindran Kannan},
 title = {Foundations of Data Science},
 year = {2015}
} 


@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NIPS},
  year      = {2017}
}


@article{goh2017why,
  author = {Gabriel Goh},
  title = {Why Momentum Really Works},
  journal = {Distill},
  year = {2017}
}

@article{G17,
  author = {Xavier Gastaldi},
  title = {Shake-Shake regularization},
  journal = {arXiv:1705.07485},
  year = {2017}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NIPS},
  year = {2017}
}

@article{CZMVL18,
  author = {Ekin D Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V Le.},
  title = {Autoaugment: Learning augmentation policies from data.},
  journal = {arXiv:1805.09501},
  year = {2018}
}



@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}

@ARTICLE{P64,
  title={Some methods of speeding up the convergence of iteration methods},
  author={B.T. Polyak},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  year      = {1964}
}

@ARTICLE{GPS16,
  title={Stochastic Heavy Ball},
  author={S{\'e}bastien Gadat and Fabien Panloup and Sofiane Saadane},
  journal   = {arXiv:1609.04228},
  year      = {2016}
}

@ARTICLE{GFJ15,
  title={Global convergence of the Heavy-ball method for convex optimization},
  author={Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal   = {ECC},
  year      = {2015}
}



@ARTICLE{SYLHGJ19,
  title={Non-ergodic Convergence Analysis of Heavy-Ball Algorithms},
  author={Tao Sun and Penghang Yin and Dongsheng Li and Chun Huang and Lei Guan and Hao Jiang},
  journal   = {AAAI},
  year      = {2019}
}

@ARTICLE{OCBP14,
  title={ipiano: Inertial proximal algorithm for nonconvex optimization},
  author={Peter Ochs and Yunjin Chen and Thomas Brox and Thomas Pock},
  journal   = {SIAM Journal of Imaging Sciences},
  year      = {2014}
}



@ARTICLE{GLZ16,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Saeed Ghadimi and Guanghui Lan and Hongchao Zhang},
  journal   = {Mathematical Programming},
  year      = {2016}
}


@ARTICLE{LJCJ17,
  title={Nonconvex finite-sum optimization via scsg methods},
  author={Lihua Lei and Cheng Ju and Jianbo Chen and Michael I. Jordan},
  journal   = {NIPS},
  year      = {2017}
}


@ARTICLE{YXG18,
  title={Stochastic nested variance reduced gradient descent for nonconvex optimization},
  author={Yaodong Yu and Pan Xu and Quanquan Gu},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{CHMAL15,
  title={The Loss Surfaces of Multilayer Networks},
  author={Anna Choromanska and Mikael Henaff and Michael Mathieu and G{\'e}rard Ben Arous and Yann LeCun},
  journal   = {AISTAT},
  year      = {2015}
}

@ARTICLE{N13,
  title={Introductory lectures on convex optimization: a basic course},
  author={Yurii Nesterov},
  journal   = {Springer},
  year      = {2013}
}


@ARTICLE{NKJK18,
  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},
  author={Rahul Kidambi and Praneeth Netrapalli and Prateek Jain and Sham M. Kakade},
  journal   = {ICLR},
  year      = {2018}
}


@ARTICLE{JNGKJ19,
  title={Stochastic Gradient Descent Escapes Saddle Points Efficiently},
  author={Chi Jin and Praneeth Netrapalli and Rong Ge and Sham M. Kakade and Michael I. Jordan},
  journal   = {arXiv:1902.04811},
  year      = {2019}
}


@ARTICLE{G11,
  title={Recovering low-rank matrices from few coefficients in any basis},
  author={David Gross},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2011}
}

@ARTICLE{SMDH13,
  title={On the importance of initialization and momentum in deep learning},
  author={Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  journal   = {ICML},
  year      = {2013}
}


@ARTICLE{WKXS18,
  title={Identifying Generalization Properties in Neural Networks},
  author={Huan Wang and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  journal   = {arXiv:1809.07402},
  year      = {2018}
}

@ARTICLE{GS18,
  title={Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization},
  author={Alon Gonen and Shai Shalev-Shwartz},
  journal   = {JMLR},
  year      = {2018}
}


@ARTICLE{SSSS09,
  title={Learnability and Stability in the General Learning Setting.},
  author={Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  journal   = {COLT},
  year      = {2009}
}


@ARTICLE{SSSS10,
  title={Learnability, Stability and Uniform Convergence},
  author={Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  journal   = {JMLR},
  year      = {2010}
}


@ARTICLE{BL02,
  title={Stability and Generalization},
  author={Olivier Bousquet and Andre Elisseeff},
  journal   = {JMLR},
  year      = {2002}
}


@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}


@ARTICLE{FV18,
  title={Generalization Bounds for Uniformly Stable Algorithms},
  author={Vitaly Feldman and Jan Vondrak},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{KL18,
  title={Data-Dependent Stability of Stochastic Gradient Descent},
  author={Ilja Kuzborskij, Christoph H. Lampert},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{MWZZ18,
  title={Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints},
  author={Wenlong Mou and Liwei Wang and Xiyu Zhai and Kai Zheng},
  journal   = {COLT},
  year      = {2018}
}

@ARTICLE{CP17,
  title={Stability and Generalization of Learning Algorithms that Converge to Global Optima},
  author={Zachary Charles and Dimitris Papailiopoulos},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{GS17,
  title={Fast Rates for Empirical Risk Minimization of Strict Saddle Problems},
  author={Alon Gonen and Shai Shalev-Shwartz},
  journal   = {COLT},
  year      = {2017}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{HRS16,
  title={Train Faster, Generalize Better: Stability of Stochastic Gradient Descent},
  author={Moritz Hardt and Benjamin Recht and Yoram Singer},
  journal   = {ICML},
  year      = {2016}
}




@ARTICLE{MJ20,
  title={Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization},
  author={Vien V. Mai and Mikael Johansson},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{LL20,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Kaifeng Lyu and Jian Li},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{CCGZ20,
  title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Network},
  author={Zixiang Chen and Yuan Cao and Quanquan Gu and Tong Zhang},
  journal   = {NeurIPS},
  year      = {2020}
}

@ARTICLE{PE20,
  title={Neural Networks are Convex Regularizers: Exact Polynomial-time Convex
Optimization Formulations for Two-layer Networks},
  author={Mert Pilanci and Tolga Ergen},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{CHS20,
  title={Label-Aware Neural Tangent Kernel: Toward Better Generalization and Local Elasticity},
  author={Shuxiao Chen and Hangfeng He and Weijie J. Su},
  journal   = {NeurIPS},
  year      = {2020}
}


@ARTICLE{HL17,
  title={Dissipativity Theory for Nesterov’s Accelerated Method},
  author={Bin Hu and Laurent Lessard},
  journal   = {ICML},
  year      = {2017}
}

@ARTICLE{H20,
  title={Unifying the Analysis in Control and Optimization via Semidefinite Programs},
  author={Bin Hu},
  journal   = {Lecture Note},
  year      = {2020}
}


@ARTICLE{FSRV20,
  title={Conformal symplectic and relativistic optimization},
  author={Guilherme Franca and Jeremias Sulam and Daniel P Robinson and Rene Vidal},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  year      = {2020}
}

@ARTICLE{DJ19,
  title={Generalized Momentum-Based Methods: A Hamiltonian Perspective},
  author={Jelena Diakonikolas and Michael I. Jordan},
  journal   = {arXiv:1906.00436},
  year      = {2019}
}

@ARTICLE{WJR21,
  title={A Lyapunov analysis of momentum methods in optimization},
  author={Ashia C. Wilson and Michael Jordan and Benjamin Recht},
  journal   = {JMLR},
  year      = {2021}
}

@ARTICLE{CDO18,
  title={On Acceleration with Noise-Corrupted Gradients},
  author={Michael B. Cohen and Jelena Diakonikolas and Lorenzo Orecchia},
  journal   = {ICML},
  year      = {2018}
}

@ARTICLE{MJ20,
  title={Optimization with momentum: Dynamical, controltheoretic,
and symplectic perspectives},
  author={Michael Muehlebach and Michael I Jordan},
  journal   = {arXiv:2002.12493},
  year      = {2020}
}

@article{wibisono2016variational,
  title={A variational perspective on accelerated methods in optimization},
  author={Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={47},
  pages={E7351--E7358},
  year={2016},
  publisher={National Acad Sciences}
}


@ARTICLE{LB18,
  title={Parametrized Accelerated Methods Free of Condition Number},
  author={Chaoyue Liu and Mikhail Belkin},
  journal   = {arXiv:1802.10235},
  year      = {2018}
}


@ARTICLE{DKB18,
  title={Non-monotone Behavior of the Heavy Ball Method},
  author={Marina Danilova and Anastasiya Kulakova and Boris Polyak},
  journal   = {arXiv:1811.00658},
  year      = {2018}
}


@ARTICLE{F18,
  title={Matrix Norms and Spectral Radii},
  author={Simon Foucart},
  journal   = {Online lecture note},
  year      = {2018}
}


@ARTICLE{wiki20,
  title={Spectral radius},
  author={Wikipedia},
  url    = "https://en.wikipedia.org/wiki/Spectral_radius",
  year      = {2020}
}
    
@ARTICLE{SDJS18,
  title={Understanding the Acceleration Phenomenon via High-Resolution Differential Equations},
  author={Bin Shi and Simon S. Du and Michael I. Jordan and Weijie J. Su},
  journal   = {Mathematical Programming},
  year      = {2021}
}


@ARTICLE{G41,
  title={Normierte ringe},
  author={I. Gelfand},
  journal   = {Mat. Sbornik},
  year      = {1941}
}

@ARTICLE{M19,
  title={Accelerated Methods - Polyak’s Momentum (Heavy Ball Method)},
  author={Ioannis Mitliagkas},
  journal   = {Online Lecture Note},
  year      = {2019}
}


@ARTICLE{SHL18,
  title={Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced},
  author={Simon S. Du and Wei Hu and Jason D. Lee},
  journal   = {NeurIPS},
  year      = {2018}
} 

@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2020}
} 

@ARTICLE{GWBNS17,
  title={Implicit Regularization in Matrix Factorization},
  author={Suriya Gunasekar and Blake Woodworth and Srinadh Bhojanapalli and Behnam Neyshabur and Nathan Srebro},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{JT19,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ziwei Ji and Matus Telgarsky},
  journal   = {ICLR},
  year      = {2019}
} 

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 

@ARTICLE{RC20,
  title={Implicit regularization in deep learning may not be explainable by
norms},
  author={Noam Razin and Nadav Cohen},
  journal   = {NeurIPS2020},
  year      = {2020}
} 

@ARTICLE{ACHL19,
  title={Implicit Regularization in Deep Matrix Factorization},
  author={Sanjeev Arora and Nadav Cohen and Wei Hu and Yuping Luo},
  journal   = {NerurIPS},
  year      = {2019}
}


@ARTICLE{GBL19,
  title={Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks},
  author={Gauthier Gidel and Francis Bach and Simon Lacoste-Julien},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{BHL18,
  title={Exponential convergence time of gradient descent for one-dimensional deep linear neural networks},
  author={Ohad Shamir},
  journal   = {COLT},
  year      = {2019}
} 

@ARTICLE{WWM19,
  title={Global Convergence of Gradient Descent for Deep Linear Residual Networks},
  author={Lei Wu and Qingcan Wang and Chao Ma},
  journal   = {NeurIPS},
  year      = {2019}
} 

@ARTICLE{SMG14,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Andrew M Saxe and James L McClelland and Surya Ganguli},
  journal   = {ICLR},
  year      = {2014}
} 


@ARTICLE{K16,
  title={Deep learning without poor local minima},
  author={Kenji Kawaguchi},
  journal   = {NeurIPS},
  year      = {2016}
} 

@ARTICLE{HM16,
  title={Identity matters in deep learning},
  author={Moritz Hardt and Tengyu Ma},
  journal   = {ICLR},
  year      = {2016}
} 

@ARTICLE{LK17,
  title={Depth creates no bad local minima},
  author={Haihao Lu and Kenji Kawaguchi},
  journal   = {arXiv:1702.08580},
  year      = {2017}
} 

@ARTICLE{YSJ17,
  title={Global optimality conditions for deep neural networks},
  author={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},
  journal   = {ICLR},
  year      = {2018}
} 

@ARTICLE{ZL18,
  title={Critical points of linear neural networks: Analytical forms and landscape},
  author={Yi Zhou and Yingbin Liang},
  journal   = {ICLR},
  year      = {2018}
} 

@ARTICLE{LvB18,
  title={Deep linear networks with arbitrary loss: All local minima
are global},
  author={Thomas Laurent and James von Brecht},
  journal   = {ICML},
  year      = {2018}
} 


@ARTICLE{BHL18,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
  author={Peter L. Bartlett and David P. Helmbold and Philip M. Long},
  journal   = {ICML},
  year      = {2018}
} 


@ARTICLE{DH19,
  title={Width Provably Matters in Optimization for Deep Linear Neural Networks},
  author={Simon S. Du and Wei Hu},
  journal   = {ICML},
  year      = {2019}
} 

@ARTICLE{HXP20,
  title={Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks},
  author={Wei Hu and Lechao Xiao and Jeffrey Pennington},
  journal   = {ICLR},
  year      = {2020}
} 


@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {arXiv:2007.06738},
  year      = {2020}
} 



@ARTICLE{ADR20,
  title={Convergence rates of the Heavy-Ball method with Lojasiewicz property},
  author={Jean-Francois Aujol and Charles Dossal and Aude Rondepierre},
  journal   = {hal-02928958},
  year      = {2020}
} 

@ARTICLE{P63,
  title={Gradient methods for minimizing functionals},
  author={Boris Polyak},
  journal   = {Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki},
  year      = {1963}
} 


@ARTICLE{LZB20b,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  journal   = {arXiv:2003.00307},
  year      = {2020}
} 

@ARTICLE{SYS20,
  title={The Effects of Mild Over-parameterization on the Optimization
Landscape of Shallow ReLU Neural Networks},
  author={Itay Safran and Gilad Yehudai and Ohad Shamir},
  journal   = {arXiv:2006.01005},
  year      = {2020}
} 


@ARTICLE{LZB20a,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Chaoyue Liu and Libin Zhu and Mikhail Belkin},
  journal   = {arXiv:2010.01092},
  year      = {2020}
} 


@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {arXiv:2007.06738},
  year      = {2020}
} 



@ARTICLE{MGWLSS20,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
  journal   = {arXiv:2007.06738},
  year      = {2020}
} 


@ARTICLE{BM19,
  title={On the Inductive Bias of Neural Tangent Kernels},
  author={Alberto Bietti and Julien Mairal},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{AMMC20,
  title={A new regret analysis for Adam-type algorithms},
  author={Ahmet Alacaoglu and Yura Malitsky and Panayotis Mertikopoulos and Volkan Cevher},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{NB15,
  title={From Averaging to Acceleration, There is Only a
Step-size},
  author={Nicolas Flammarion and Francis Bach},
  journal   = {COLT},
  year      = {2015}
}


@ARTICLE{MPTDD18,
  title={Hamiltonian Descent Methods},
  author={Chris J. Maddison and Daniel Paulin and Yee Whye Teh and Brendan O’Donoghue and Arnaud Doucet},
  journal   = {arXiv:1809.05042},
  year      = {2018}
}

@ARTICLE{DJ19,
  title={Generalized Momentum-Based Methods: A Hamiltonian Perspective},
  author={Jelena Diakonikolas and Michael I. Jordan},
  journal   = {arXiv:1906.00436},
  year      = {2019}
}


@ARTICLE{HWTZ20,
  title={Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? — A Neural Tangent Kernel Perspective},
  author={Kaixuan Huang and Yuqing Wang and Molei Tao and Tuo Zhao},
  journal   = {arXiv:2002.06262},
  year      = {2020}
}


@ARTICLE{LSSWY20,
  title={Generalized Leverage Score Sampling for Neural Networks},
  author={Jason D. Lee and Ruoqi Shen and Zhao Song and Mengdi Wang and Zheng Yu},
  journal   = {arXiv:2009.09829},
  year      = {2020}
}

@ARTICLE{ACH18,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Sanjeev Arora and Nadav Cohen and Elad Hazan},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{Y19,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Greg Yang},
  journal   = {arXiv:1902.04760},
  year      = {2019}
}


@ARTICLE{OS19,
  title={Towards moderate overparameterization: global convergence
guarantees for training shallow neural networks},
  author={Samet Oymak and Mahdi Soltanolkotabi},
  journal   = {arXiv:1902.04674},
  year      = {2019}
}


@ARTICLE{SY19,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Lili Su and Pengkun Yang},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{SY19,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Perspective},
  author={Lili Su and Pengkun Yang},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{CK20,
  title={Understanding Accelerated Stochastic Gradient Descent via the Growth Condition},
  author={You-Lin Chen and Mladen Kolar},
  journal   = {arXiv:2006.06782},
  year      = {2020}
}

@ARTICLE{SGD20,
  title={On the convergence of the Stochastic Heavy Ball Method},
  author={Othmane Sebbouh and Robert M. Gower and Aaron Defazio},
  journal   = {arXiv:2006.07867},
  year      = {2020}
}

@ARTICLE{KCH20,
  title={Global Convergence of Second-order Dynamics in Two-layer Neural Networks},
  author={Walid Krichene and Kenneth F. Caluyay and Abhishek Halder},
  journal   = {arXiv:2006.07867},
  year      = {2020}
}


@ARTICLE{HSS20,
  title={Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond},
  author={Oliver Hinder and Aaron Sidford and Nimit Sohoni},
  journal   = {COLT},
  year      = {2020}
}

@ARTICLE{Getal20,
  title={Understanding Accelerated Stochastic Gradient Descent via the Growth Condition},
  author={Eduard Gorbunov and Adel Bibi and Ozan Sener and El Houcine Bergou and Peter Richt{\'a}rik},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{Getal20,
  title={A Stochastic Derivative Free Optimization Method with Momentum},
  author={Eduard Gorbunov and Adel Bibi and Ozan Sener and El Houcine Bergou and Peter Richt{\'a}rik},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{HXAP20,
  title={The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks},
  author={Wei Hu and Lechao Xiao and Ben Adlam and Jeffrey Pennington},
  journal   = {NeurIPS},
  year      = {2020}
}


@ARTICLE{NB19,
  title={SGD on Neural Networks Learns Functions of Increasing Complexity},
  author={Preetum Nakkiran and Gal Kaplun and Dimitris Kalimeris and Tristan Yang and Benjamin L. Edelman and Fred Zhang and Boaz Barak},
  journal   = {NeurIPS},
  year      = {2019}
}



@ARTICLE{FDZ19,
  title={Over parameterized two-level neural networks can learn near optimal feature representations},
  author={Cong Fang and Hanze Dong and Tong Zhang},
  journal   = {arXiv:1910.11508},
  year      = {2019}
}


@ARTICLE{BPSW20,
  title={Training (Overparametrized) Neural Networks in Near-Linear Time},
  author={Jan van den Brand and Binghui Peng and Zhao Song and Omri Weinstein},
  journal   = {arXiv:2006.11648},
  year      = {2020}
}


@ARTICLE{CO19,
  title={Momentum-Based Variance Reduction in Non-Convex SGD},
  author={Ashok Cutkosky and Francesco Orabona},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{LGY20,
  title={An Improved Analysis of Stochastic Gradient Descent with Momentum},
  author={Yanli Liu and Yuan Gao and Wotao Yin},
  journal   = {NeurIPS},
  year      = {2020}
}


@ARTICLE{LB20,
  title={Accelerating SGD with momentum for over-parameterized learning},
  author={Chaoyue Liu and Mikhail Belkin},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{LY17,
  title={Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
  author={Yuanzhi Li and Yang Yuan},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{GBL19,
  title={Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks},
  author={Gauthier Gidel and Francis Bach and Simon Lacoste-Julien},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{GLSS18,
  title={Implicit Bias of Gradient Descent on Linear Convolutional Networks},
  author={Suriya Gunasekar and Jason D. Lee and Daniel Soudry and Nathan Srebro},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{GLSS18,
  title={Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced},
  author={Simon S. Du and Wei Hu and Jason D. Lee},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{WLLM19,
  title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  author={Colin Wei and Jason D. Lee and Qiang Liu and Tengyu Ma},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{ACGH19,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{ACGH19,
  title={Convergence of Adversarial Training in Overparametrized Networks},
  author={Ruiqi Gao and Tianle Cai and Haochuan Li and Liwei Wang and Cho-Jui Hsieh and Jason D. Lee},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{WDW19,
  title={Global convergence of adaptive gradient methods for an
over-parameterized neural network},
  author={Xiaoxia Wu and Simon S Du and Rachel Ward},
  journal   = {arXiv:1902.07111},
  year      = {2019}
}


@ARTICLE{WDS19,
  title={Learning distributions generated by one-layer relu networks},
  author={Shanshan Wu and Alexandros G Dimakis and Sujay Sanghavi},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{ACGH19,
  title={SGD Learns One-Layer Networks in WGANs},
  author={Qi Lei and Jason D. Lee and Alex Dimakis and Costis Daskalakis},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{Cetal19,
  title={A gram-gauss-newton method learning overparameterized deep neural networks for regression problems},
  author={Tianle Cai and Ruiqi Gao and Jikai Hou and Siyu Chen and Dong Wang and Di He and Zhihua Zhang and Liwei Wang},
  journal   = {arXiv.org:1905.11675},
  year      = {2019}
}

@ARTICLE{GML18,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Rong Ge and Tengyu Ma and Jason D. Lee},
  journal   = {ICLR},
  year      = {2018}
}

@ARTICLE{OS19,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Samet Oymak and Mahdi Soltanolkotabi},
  journal   = {ICML},
  year      = {2019}
}

@ARTICLE{ZLG20,
  title={On the global convergence of training deep linear resnets},
  author={Difan Zou and Philip M. Long and Quanquan Gu},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{DGM20,
  title={Optimization Theory for ReLU Neural Networks Trained with Normalization Layers},
  author={Yonatan Dukler and Quanquan Gu and Guido Montufar},
  journal   = {ICML},
  year      = {2020}
}


@ARTICLE{PSG2020,
  title={Effect of Activation Functions on the Training of Overparametrized Neural Nets},
  author={Abhishek Panigrahi and Abhishek Shetty and Navin Goyal},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{P87,
  title={Introduction to optimization},
  author={Boris T. Polyak},
  journal   = {Optimization Software},
  year      = {1987}
}

@ARTICLE{Dan17,
  title={SGD Learns the Conjugate Kernel Class of the Network},
  author={Amit Daniely},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{JN15,
  title={The hierarchy of local minimums in polynomial optimization.},
  author={Jiawang Nie},
  journal   = {Mathematical programming},
  year      = {2015}
}

@ARTICLE{MK87,
  title={Some np-complete problems in quadratic and nonlinear programming},
  author={Katta G Murty and Santosh N Kabadi},
  journal   = {Mathematical programming},
  year      = {1987}
}

@ARTICLE{N00,
  title={Squared functional systems and optimization problems},
  author={Yurii Nesterov},
  journal   = {High performance optimization, Springer},
  year      = {2000}
}

@ARTICLE{AG16,
  title={Efficient approaches for escaping higher order saddle points in non-convex optimization},
  author={Anima Anandkumar and Rong Ge},
  journal   = {COLT},
  year      = {2016}
}

@ARTICLE{LR18,
  title={Accelerated Gossip via Stochastic Heavy Ball Method},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {Allerton},
  year      = {2018}
}

@article{CGZ19,
 author = {Bugra Can and Mert G{\"u}rb{\"u}zbalaban and Lingjiong Zhu},
 title = {Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances},
 journal = {ICML},
 year = {2019}
} 



@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 


@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 

@article{SQW15,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {When Are Nonconvex Problems Not Scary?},
 journal = {NeurIPS Workshop on Non-convex Optimization for Machine Learning: Theory and Practice},
 year = {2015}
} 


@article{SECCMS15,
 author = {Yoav Shechtman and Yonina C. Eldar and Oren Cohen and Henry Nicholas Chapman 
 and Jianwei Miao and Mordechai Segev},
 title = {Phase retrieval with application to optical imaging: a contemporary overview.},
 journal = {IEEE signal processing magazine},
 year = {2015}
} 

@article{CESV12,
 author = {Emmanuel J. Cand{\'e}s and Yonina Eldar and Thomas Strohmer and Vlad Voroninski},
 title = {Phase retrieval via matrix completion.},
 journal = {SIAM Journal on Imaging Sciences},
 year = {2013}
} 

@article{CCFMY18,
 author = {Yuxin Chen and Yuejie Chi and Jianqing Fan and Cong Ma and Yuling Yan},
 title = {Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval},
 journal = {Mathematical Programming},
 year = {2018}
} 


@article{BHK15,
 author = {Avrim Blum and John Hopcroft and Ravindran Kannan},
 title = {Foundations of Data Science},
 year = {2015}
} 


@article{Curtis17,
 author = {Curtis, Frank E. and Robinson, Daniel P. and Samadi, Mohammadreza},
 title = {A Trust Region Algorithm with a Worst-case Iteration Complexity of $O(\epsilon^{-3/2})$ for Nonconvex Optimization},
 journal = {Mathematical Programming},
 year = {2017},
} 



@ARTICLE{KSH12,
  title={ImageNet Classification with Deep Convolutional Neural Networks},
  author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  journal={NeurIPS},
  year={2012}
}


@ARTICLE{Rnet16,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}


@article{silver2017,
  author = {David Silver and Julian Schrittwieser and Karen Simonyan and et al.},
  title = {Mastering the game of Go without human knowledge},
  journal = {Nature},
  year = {2017}
}

@ARTICLE{attention17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               et al.},
  title     = {Attention is All you Need},
  journal = {{NeurIPS}},
  year      = {2017}
}


@ARTICLE{Baidu16,
  title =    {Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin},
  author =   {Dario Amodei and Sundaram Ananthanarayanan and Rishita Anubhai and et al.},
  journal = {ICML},
  year =   {2016}
}



@ARTICLE{KB15,
  title = {Adam: A Method for Stochastic Optimization},
  author    = {Diederik P. Kingma and Jimmy Ba},
  journal = {ICLR},
  year = {2015}
}

@ARTICLE{RKK18,
  title={On the Convergence of Adam and Beyond },
  author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  journal={ICLR},
  year={2018}
}

@ARTICLE{KH1918,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  journal   = {ICLR},
  year      = {2019}
}

@article{goh2017why,
  author = {Gabriel Goh},
  title = {Why Momentum Really Works},
  journal = {Distill},
  year = {2017}
}

@article{G17,
  author = {Xavier Gastaldi},
  title = {Shake-Shake regularization},
  journal = {arXiv:1705.07485},
  year = {2017}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NeurIPS},
  year = {2017}
}

@article{CZMVL18,
  author = {Ekin D Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V Le.},
  title = {Autoaugment: Learning augmentation policies from data.},
  journal = {arXiv:1805.09501},
  year = {2018}
}



@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}

@ARTICLE{P64,
  title={Some methods of speeding up the convergence of iteration methods},
  author={B.T. Polyak},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  year      = {1964}
}

@ARTICLE{GPS16,
  title={Stochastic Heavy Ball},
  author={S{\'e}bastien Gadat and Fabien Panloup and Sofiane Saadane},
  journal   = {arXiv:1609.04228},
  year      = {2016}
}

@ARTICLE{GFJ15,
  title={Global convergence of the Heavy-ball method for convex optimization},
  author={Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal   = {ECC},
  year      = {2015}
}


@ARTICLE{LR17,
  title={Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {Computational Optimization and Applications},
  year      = {2020}
}

@ARTICLE{SYLHGJ19,
  title={Non-ergodic Convergence Analysis of Heavy-Ball Algorithms},
  author={Tao Sun and Penghang Yin and Dongsheng Li and Chun Huang and Lei Guan and Hao Jiang},
  journal   = {AAAI},
  year      = {2019}
}

@ARTICLE{OCBP14,
  title={ipiano: Inertial proximal algorithm for nonconvex optimization},
  author={Peter Ochs and Yunjin Chen and Thomas Brox and Thomas Pock},
  journal   = {SIAM Journal of Imaging Sciences},
  year      = {2014}
}



@ARTICLE{GLZ16,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Saeed Ghadimi and Guanghui Lan and Hongchao Zhang},
  journal   = {Mathematical Programming},
  year      = {2016}
}


@ARTICLE{LJCJ17,
  title={Nonconvex finite-sum optimization via scsg methods},
  author={Lihua Lei and Cheng Ju and Jianbo Chen and Michael I. Jordan},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{YXG18,
  title={Stochastic nested variance reduced gradient descent for nonconvex optimization},
  author={Yaodong Yu and Pan Xu and Quanquan Gu},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{CHMAL15,
  title={The Loss Surfaces of Multilayer Networks},
  author={Anna Choromanska and Mikael Henaff and Michael Mathieu and G{\'e}rard Ben Arous and Yann LeCun},
  journal   = {AISTAT},
  year      = {2015}
}



@ARTICLE{dauphin14,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Yann Dauphin and Razvan Pascanu and Caglar Gulcehre and Kyunghyun Cho and Surya Ganguli and Yoshua Bengio},
  journal   = {NeurIPS},
  year      = {2014}
}


@ARTICLE{TSJRJ18,
  title={Stochastic cubic regularization for fast nonconvex optimization},
  author={Nilesh Tripuraneni and Mitchell Stern and Chi Jin and Jeffrey Regier and Michael I Jordan},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{FLLZ18,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Cong Fang and Chris Junchi Li and Zhouchen Lin and Tong Zhang.},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{XRY18,
  title={First-order stochastic algorithms for escaping from saddle points in almost linear time},
  author={Yi Xu and Jing Rong and Tianbao Yang},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{RZSPBSS18,
  title={A generic approach for escaping saddle points},
  author={Sashank Reddi and Manzil Zaheer and Suvrit Sra and Barnabas Poczos and Francis Bach and Ruslan Salakhutdinov and Alex Smola},
  journal   = {AISTATS},
  year      = {2018}
}


@ARTICLE{MOJ16,
  title={Escaping saddle points in constrained optimization},
  author={Aryan Mokhtari and Asuman Ozdaglar and Ali Jadbabaie},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{KL16,
  title={The power of normalization: Faster evasion of saddle points},
  author={Kfir Y. Levy},
  journal   = {arXiv:1611.04831},
  year      = {2016}
}


@ARTICLE{AL18,
  title={Neon2: Finding local minima via first-order oracles},
  author={Zeyuan Allen-Zhu and Yuanzhi Li},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{AABHM17,
  title={Finding approximate local minima faster than gradient descent},
  author={Naman Agarwal and Zeyuan Allen-Zhu and Brian Bullins and Elad Hazan and Tengyu Ma},
  journal   = {STOC},
  year      = {2017}
}


@ARTICLE{SRRKKS19,
  title={Escaping Saddle Points with Adaptive Gradient Methods},
  author={Matthew Staib and Sashank J. Reddi and Satyen Kale and Sanjiv Kumar and Suvrit Sra},
  journal   = {ICML},
  year      = {2019}
}

@ARTICLE{RSS12,
  title={Making Stochastic Gradient Descent Optimal for Strongly Convex Problems},
  author={Alexander Rakhlin and Ohad Shamir and Karthik Sridharan},
  journal   = {ICML},
  year      = {2012}
}


@ARTICLE{N13,
  title={Introductory lectures on convex optimization: a basic course},
  author={Yurii Nesterov},
  journal   = {Springer},
  year      = {2013}
}


@ARTICLE{NKJK18,
  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},
  author={Rahul Kidambi and Praneeth Netrapalli and Prateek Jain and Sham M. Kakade},
  journal   = {ICLR},
  year      = {2018}
}


@ARTICLE{JNGKJ19,
  title={Stochastic Gradient Descent Escapes Saddle Points Efficiently},
  author={Chi Jin and Praneeth Netrapalli and Rong Ge and Sham M. Kakade and Michael I. Jordan},
  journal   = {arXiv:1902.04811},
  year      = {2019}
}


@ARTICLE{G11,
  title={Recovering low-rank matrices from few coefficients in any basis},
  author={David Gross},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2011}
}


@ARTICLE{KL17,
  title={Sub-sampled Cubic Regularization for Non-convex Optimization},
  author={Jonas Moritz Kohler and Aurelien Lucchi},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{LPPSJR19,
  title={First-order methods almost always avoid strict saddle-points},
  author={Jason D. Lee and Ioannis Panageas and Georgios Piliouras and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  journal   = {Mathematical Programming, Series B},
  year      = {2019}
}


@ARTICLE{GHJY15,
  title={Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition},
  author={Rong Ge and Furong Huang and Chi Jin and Yang Yuan},
  journal   = {COLT},
  year      = {2015}
}



@ARTICLE{DJLJPS18,
  title={Gradient Descent Can Take Exponential Time to Escape Saddle Points},
  author={Simon S. Du and Chi Jin and Jason D. Lee and Michael I. Jordan and Barnabas Poczos and Aarti Singh},
  journal   = {NeurIPS},
  year      = {2017}
}



@ARTICLE{CNJ18,
  title={Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent},
  author={Chi Jin and Praneeth Netrapalli and Michael I. Jordan},
  journal   = {COLT},
  year      = {2018}
}


@ARTICLE{TSJRJ18,
  title={Stochastic Cubic Regularization for Fast Nonconvex Optimization},
  author={Nilesh Tripuraneni and Mitchell Stern and Chi Jin and Jeffrey Regier and Michael I. Jordan},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{JGNKJ17,
  title={How to Escape Saddle Points Efficiently},
  author={Chi Jin and Rong Ge and Praneeth Netrapalli and Sham M. Kakade and Michael I. Jordan},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{FLZCOLT19,
  title={Sharp Analysis for Nonconvex SGD Escaping from Saddle Points},
  author={Cong Fang and Zhouchen Lin and Tong Zhang},
  journal   = {COLT},
  year      = {2019}
}

@ARTICLE{DKLH18,
  title={Escaping Saddles with Stochastic Gradients},
  author={Hadi Daneshmand and Jonas Kohler and Aurelien Lucchi and Thomas Hofmann},
  journal   = {ICML},
  year      = {2018}
}

@ARTICLE{CDHS18,
  title={Accelerated Methods for NonConvex Optimization},
  author={Yair Carmon and John Duchi and Oliver Hinder and Aaron Sidford},
  journal   = {SIAM Journal of Optimization},
  year      = {2018}
}


@ARTICLE{SMDH13,
  title={On the importance of initialization and momentum in deep learning},
  author={Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  journal   = {ICML},
  year      = {2013}
}


@ARTICLE{JKBFBS19,
  title={On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length},
  author={Stanislaw Jastrzebski and Zachary Kenton and Nicolas Ballas and Asja Fischer and Yoshua Bengio and Amost Storkey},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{WKXS18,
  title={Identifying Generalization Properties in Neural Networks},
  author={Huan Wang and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  journal   = {arXiv:1809.07402},
  year      = {2018}
}


@ARTICLE{TSS19,
  title={Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis},
  author={Yusuke Tsuzuku and Issei Sato and Masashi Sugiyama},
  journal   = {arXiv:1901.04653},
  year      = {2019}
}


@ARTICLE{entropysgd17,
  title={Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  author={Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
  journal   = {ICLR},
  year      = {2017}
}


@ARTICLE{KMNST17,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
  journal   = {ICLR},
  year      = {2017}
}



@ARTICLE{DPBB17,
  title={Sharp Minima Can Generalize For Deep Nets},
  author={Laurent Dinh and Razvan Pascanu and Samy Bengio and Yoshua Bengio},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{ZBHRV17,
  title={Understanding deep learning requires rethinking generalization},
  author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  journal   = {ICLR},
  year      = {2017}
}

@ARTICLE{LXLS19,
  title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
  author={Liangchen Luo and Yuanhao Xiong and Yan Liu and Xu Sun},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{CWLK19,
  title={Decaying momentum helps neural network training},
  author={John Chen and Cameron Wolfe and Zhao Li and Anastasios Kyrillidis},
  journal   = {arXiv:1910.04952},
  year      = {2019}
}

@ARTICLE{LCZZ18,
  title={Towards Understanding Nonconvex Stochastic Optimization with Momentum using Diffusion Approximations},
  author={Tianyi Liu and Zhehui Chen and Enlu Zhou and Tuo Zhao},
  journal   = {arXiv:1802.05155},
  year      = {2018}
}


@ARTICLE{SP20,
  title={Universal Average-Case Optimality of Polyak Momentum},
  author={Damien Scieur and Fabian Pedregosa},
  journal   = {ICML},
  year      = {2020}
}

@ARTICLE{JT20,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ziwei Ji and Matus Telgarsky},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{LL18,
  title={Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data},
  author={Yuanzhi Li and Yingyu Liang},
  journal   = {NeurIPS},
  year      = {2018}
}



@ARTICLE{DZPS19,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{DLLWZ16,
  title={Gradient descent finds global minima of deep neural networks},
  author={Simon S Du and Jason D Lee and Haochuan Li and Liwei Wang and Xiyu Zhai},
  journal   = {ICML},
  year      = {2019}
}



@ARTICLE{ZL19_icml,
  title={A convergence theory for deep learning via overparameterization},
  author={Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
  journal   = {ICML},
  year      = {2019}
}

@ARTICLE{ZY19,
  title={Quadratic suffices for over-parametrization via matrix chernoff bound},
  author={Zhao Song and Xin Yang},
  journal   = {arXiv:1906.03593},
  year      = {2019}
}

@ARTICLE{ZCZG19,
  title={Stochastic gradient descent optimizes overparameterized deep relu networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal   = {Machine Learning, Springer},
  year      = {2019}
}


@ARTICLE{ADHLSW19_nips,
  title={On exact computation with an infinitely wide neural net},
  author={Sanjeev Arora and Simon S Du and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{ADHLSW19_icml,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Sanjeev Arora and Simon S Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{JGH18,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Arthur Jacot and Franck Gabriel and Clement Hongler},
  journal   = {NeurIPS},
  year      = {2018}
}

@ARTICLE{LXSBSP19,
  title={Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
  author={Jaehoon Lee and Lechao Xiao and Samuel S. Schoenholz and Yasaman Bahri and Jascha Sohl-Dickstein and Jeffrey Pennington},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{COB19,
  title={On lazy training in differentiable programming},
  author={Lenaic Chizat and Edouard Oyallon and Francis Bach},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{AS20,
  title={The Numerics of Phase Retrieval},
  author={Albert Fannjiang and Thomas Strohmer},
  journal   = {Acta Numerica},
  year      = {2020}
}


@ARTICLE{GMMM19,
  title={Limitations of Lazy Training of Two-layers Neural Networks},
  author={Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{BHK18,
  title={Foundations of Data Science},
  author={Avrim Blum and John Hopcroft and Ravindran Kannan},
  journal   = {Neural computation},
  year      = {2018}
}


@ARTICLE{BHL19,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
  author={Peter L Bartlett and David P Helmbold and Philip M Long},
  journal   = {Neural computation},
  year      = {2019}
}

@ARTICLE{MS18,
  title={Notes on first-order methods for minimizing smooth functions},
  author={Michael Saunders},
  journal   = {Lecture note},
  year      = {2018}
}

@ARTICLE{A18,
  title={Heavy ball method on convex quadratic problem},
  author={Andersen Ang},
  journal   = {Lecture note},
  year      = {2018}
}

@ARTICLE{R18,
  title={Lyapunov analysis and the Heavy Ball Method},
  author={Benjamin Recht},
  journal   = {Lecture note},
  year      = {2018}
}


@ARTICLE{S14,
  title={Algorithms and Theory for Clustering and Nonconvex Quadratic Programming},
  author={Mahdi Soltanolkotabi},
  journal   = {Stanford University Ph. D. Dissertation},
  year      = {2014}
}



@ARTICLE{D20,
  title={Memorizing Gaussians with no over-parameterizaion via gradient decent on neural networks},
  author={Amit Daniely},
  journal   = {arXiv:1909.11837},
  year      = {2020}
}


@ARTICLE{GWZ19,
  title={Mildly overparametrized neural nets can memorize training data efficiently},
  author={Rong Ge and Runzhe Wang and Haoyu Zhao},
  journal   = {arXiv:1909.11837},
  year      = {2019}
}

@ARTICLE{GKZ19,
  title={Stationary Points of Shallow Neural Networks with Quadratic Activation Function},
  author={David Gamarnik and Eren C. Kızıldag and Ilias Zadik},
  journal   = {arXiv:1912.01599},
  year      = {2019}
}

@ARTICLE{SJL18,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Mahdi Soltanolkotabi and Adel Javanmard and Jason D. Lee},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2018}
}


@ARTICLE{DL18,
  title={On the Power of Over-parametrization in Neural Networks with Quadratic Activation},
  author={Simon Du and Jason Lee},
  journal   = {ICML},
  year      = {2018}
}

@ARTICLE{LMCC19,
  title={Nonconvex Matrix Factorization from Rank-One Measurements},
  author={Yuanxin Li and Cong Ma and Yuxin Chen and Yuejie Chi},
  journal   = {AISTATS},
  year      = {2019}
}

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 

 
@ARTICLE{BG17,
  title={Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs},
  author={Alon Brutzkus and Amir Globerson},
  journal   = {ICML},
  year      = {2017}
}

@ARTICLE{BL20,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Yu Bai and Jason D. Lee},
  journal   = {ICLR},
  year      = {2020}
}

@ARTICLE{ZG19,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Difan Zou and Quanquan Gu},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{BSMM19,
  title={Linearized two-layers neural networks in high dimension},
  author={Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and and Andrea Montanari},
  journal   = {arXiv:1904.12191},
  year      = {2019}
}

@ARTICLE{LMZ20,
  title={Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK},
  author={Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
  journal   = {COLT},
  year      = {2020}
}


@ARTICLE{ZMG19,
  title={Fast convergence of natural gradient descent for over-parameterized neural networks},
  author={Guodong Zhang and James Martens and Roger B Grosse},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{HN20,
  title={Finite Depth and Width Corrections to the Neural Tangent Kernel},
  author={Boris Hanin and Mihai Nica},
  journal   = {ICLR},
  year      = {2020}
}


@ARTICLE{S17,
  title={Learning Relus via Gradient Descent},
  author={Mahdi Soltanolkotabi},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{KSA19,
  title={Fitting relus via sgd and quantized sgd},
  author={Seyed Mohammadreza Mousavi Kalan and Mahdi Soltanolkotabi and A. Salman Avestimehr},
  journal   = {ISIT},
  year      = {2019}
}

@ARTICLE{MBM17,
  title={The landscape of empirical risk for non-convex losses},
  author={Song Mei and Yu Bai and Andrea Montanari},
  journal   = {PNAS},
  year      = {2018}
}

@ARTICLE{T17,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Yuandong Tian},
  journal   = {ICML},
  year      = {2017}
}


@ARTICLE{GKM19,
  title={Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian Marginals},
  author={Surbhi Goel and Adam Klivans and Raghu Meka},
  journal   = {NeurIPS},
  year      = {2019}
}


@ARTICLE{GKM18,
  title={Learning One Convolutional Layer with Overlapping Patches},
  author={Surbhi Goel and Adam Klivans and Raghu Meka},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{GKLW16,
  title={Learning Two-layer Neural Networks with Symmetric Inputs},
  author={Rong Ge and Rohith Kuditipudi and Zhize Li and Xiang Wang},
  journal   = {ICLR},
  year      = {2019}
}

@ARTICLE{JSA16,
  title={Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods},
  author={Majid Janzamin and Hanie Sedghi and Anima Anandkumar},
  journal   = {arXiv:1506.08473},
  year      = {2016}
}

@ARTICLE{KKSK11,
  title={Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression?},
  author={Sham M. Kakade and Varun Kanade and Ohad Shamir and Adam Kalai},
  journal   = {NeurIPS},
  year      = {2011}
}


@ARTICLE{DLT18,
  title={When is a Convolutional Filter Easy To Learn?},
  author={Simon S. Du and Jason D. Lee and Yuandong Tian},
  journal   = {ICLR},
  year      = {2018}
}

@ARTICLE{GKKT17,
  title={Reliably Learning the ReLU in Polynomial Time},
  author={Surbhi Goel and Varun Kanade and Adam Klivans and Justin Thaler},
  journal   = {COLT},
  year      = {2017}
}

@ARTICLE{YS20,
  title={Learning a Single Neuron with Gradient Methods},
  author={Gilad Yehudai and Ohad Shamir},
  journal   = {COLT},
  year      = {2020}
}

@ARTICLE{KS17,
  title={Improving Generalization Performance by Switching from Adam to SGD},
  author={Nitish Shirish Keskar, Richard Socher},
  journal   = {arXiv:1712.07628},
  year      = {2017}
}


@ARTICLE{LMCC19,
  title={Nonconvex Matrix Factorization from Rank-One Measurements},
  author={Yuanxin Li and Cong Ma and Yuxin Chen and Yuejie Chi},
  journal   = {AISTATS},
  year      = {2019}
}

@ARTICLE{AFWZ17,
  title={Entrywise Eigenvector Analysis of Random Matrices with Low Expected Rank},
  author={Emmanuel Abbe and Jianqing Fan and Kaizheng Wang and Yiqiao Zhong},
  journal   = {arXiv:1709.09565},
  year      = {2017}
}


@ARTICLE{GLZX19,
  title={Understanding the Role of Momentum in Stochastic Gradient Methods},
  author={Igor Gitman and Hunter Lang and Pengchuan Zhang and Lin Xiao},
  journal   = {NeurIPS},
  year      = {2019}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}

@ARTICLE{CJY18,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Yuansi Chen and Chi Jin and Bin Yu},
  journal   = {arXiv:1804.01619},
  year      = {2018}
}


@ARTICLE{LR18,
  title={Accelerated Gossip via Stochastic Heavy Ball Method},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {Allerton},
  year      = {2018}
}


@ARTICLE{KH1918,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  journal   = {ICLR},
  year      = {2019}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NeurIPS},
  year = {2017}
}

@article{BCMN14,
 author = {Afonso S. Bandeira and Jameson Cahill and Dustin G. Mixon and Aaron A. Nelson},
 title = {Saving phase: Injectivity and stability for phase retrieval},
 journal = {Applied and Computational Harmonic Analysis},
 year = {2014}
} 


@article{WCA20,
 author = {Jun-Kun Wang and Chi-Heng Lin and Jacob Abernethy},
 title = {Escaping Saddle Points Faster with Stochastic Momentum},
 journal = {ICLR},
 year = {2020}
} 

@article{WSW16,
 author = {Chris D. White and Sujay Sanghavi and Rachel Ward},
 title = {The local convexity of solving systems of quadratic equations},
 journal = {Results in Mathematics},
 year = {2016}
} 


@article{ZL15,
 author = {Qinqing Zheng and John Lafferty},
 title = {A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements},
 journal = {NeurIPS},
 year = {2015}
} 

@article{TBSSR16,
 author = {Stephen Tu and Ross Boczar and Max Simchowitz and Mahdi Soltanolkotabi and Benjamin Recht},
 title = {Low-rank Solutions of Linear Matrix Equations via Procrustes Flow},
 journal = {ICML},
 year = {2016}
} 

@article{DDP18,
 author = {Damek Davis and Dmitriy Drusvyatskiy and Courtney Paquette},
 title = {The nonsmooth landscape of phase retrieval},
 journal = {IMA Journal on Numerical Analysis},
 year = {2018}
} 

@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometric Analysis of Phase Retrieval},
 journal = {IEEE ISIT},
 year = {2016}
} 

@article{LMZ18,
 author = {Yuanzhi Li and Tengyu Ma and Hongyang Zhang},
 title = {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
 journal = {COLT},
 year = {2018}
} 


@article{LGL15,
 author = {Gen Li and Yuantao Gu and Yue M. Lu},
 title = {Phase retrieval using iterative projections: Dynamics in the large systems limit},
 journal = {IEEE Allerton},
 year = {2015}
} 

@article{MXM18,
 author = {Junjie Ma and Ji Xu and Arian Maleki},
 title = {Optimization-based AMP for Phase Retrieval: The Impact of Initialization and l2-regularization},
 journal = {IEEE Transactions on Information Theory},
 year = {2018}
} 


@article{Z19,
 author = {Teng Zhang},
 title = {Phase retrieval using alternating minimization in a batch setting},
 journal = {Applied and Computational Harmonic Analysis},
 year = {2019}
} 

@article{YYFZWN18,
 author = {Zhuoran Yang and Lin Yang and Ethan Fang and Tuo Zhao and Zhaoran Wang and Matey Neykov},
 title = {Misspecified Nonconvex Statitical Optimization for Sparse Phase Retrival},
 journal = {Mathematical Programming},
 year = {2018}
} 


@article{ZCL17,
 author = {Huishuai Zhang and Yuejie Chi and Yingbin Liang},
 title = {Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow},
 journal = {ICML},
 year = {2017}
} 


@article{QZEW17,
 author = {Qing Qu and Yuqian Zhang and Yonina C. Eldar and John Wright},
 title = {Convolutional Phase Retrieval via Gradient Descent},
 journal = {NeurIPS},
 year = {2017}
} 


@article{ZWGC18,
 author = {Liang Zhang and Gang Wang and Georgios B. Giannakis and Jie Che},
 title = {Compressive Phase Retrieval via Reweighted Amplitude Flow},
 journal = {IEEE Transactions on Signal Processing},
 year = {2018}
} 

@article{CLW19,
 author = {Jian-Feng Cai and Haixia Liu and Yang Wan},
 title = {Fast Rank-One Alternating Minimization Algorithm for Phase Retrieval},
 journal = {Journal of Scientific Computing},
 year = {2019}
} 

@article{TV19,
 author = {Yan Shuo Tan and Roman Vershynin},
 title = {Online Stochastic Gradient Descent with Arbitrary Initialization Solves Non-smooth, Non-convex Phase Retrieval},
 journal = {arXiv:1910.12837},
 year = {2019}
} 


@article{TV18,
 author = {Yan Shuo Tan and Roman Vershynin},
 title = {Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees},
 journal = {Information and Inference },
 year = {2018}
} 


@article{BEB17,
 author = {Tamir Bendory and Yonina C. Eldar and Nicolas Boumal},
 title = {Non-Convex Phase Retrieval from STFT Measurements},
 journal = {IEEE Transactions on Signal Processing},
 year = {2017}
} 


@article{W15,
 author = {Ke Wei},
 title = {Solving systems of phaseless equations via Kaczmarz methods: A proof of concept study},
 journal = {Inverse Problems},
 year = {2015}
} 

@ARTICLE{LRP16,
  title={Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints},
  author={Laurent Lessard and Benjamin Recht and Andrew Packard},
  journal   = {SIAM Journal on Optimization},
  year      = {2016}
}


@article{CFL15,
 author = {Pengwen Chen and Albert Fannjiang and Gi-Ren Liu},
 title = {Phase Retrieval with One or Two Diffraction Patterns by Alternating Projection with Null Initialization},
 journal = {Journal of Fourier Analysis and Applications},
 year = {2015}
} 


@article{GX17,
 author = {Bing Gao and Zhiqiang Xu},
 title = {Phaseless recovery using the Gauss-Newton method},
 journal = {IEEE Transactions on Signal Processing},
 year = {2017}
} 


@article{DR18,
 author = {John Duchi and Feng Ruan},
 title = {Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval},
 journal = {Information and Inference},
 year = {2018}
} 


@article{CL16,
 author = {Yuejie Chi and Yue M. Lu},
 title = {Kaczmarz method for solving quadratic equations},
 journal = {IEEE Signal Processing Letters},
 year = {2016}
} 

@article{WGSC17,
 author = {Gang Wang and Georgios B. Giannakis and Yousef Saad and Yonina C. Eldar},
 title = {Solving Most Systems of Random Quadratic Equations},
 journal = {NeurIPS},
 year = {2017}
} 


@article{ZZLC17,
 author = {Huishuai Zhang and Yi Zhou and Yingbin Liang and Yuejie Chi},
 title = {A Nonconvex Approach for Phase Retrieval: Reshaped Wirtinger Flow and Incremental Algorithms},
 journal = {JMLR},
 year = {2017}
} 


@article{WGE17,
 author = {Gang Wang and Georgios B. Giannakis and Yonina C. Eldar},
 title = {Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow},
 journal = {IEEE Transactions on Information Theory},
 year = {2017}
} 


@article{M18,
 author = {Mahdi Soltanolkotabi},
 title = {Structured signal recovery from quadratic measurements: Breaking sample complexity barriers via nonconvex optimization},
 journal = {IEEE Transactions on Information Theory},
 year = {2018}
} 


@article{CLM16,
 author = {T. Tony Cai and Xiaodong Li and Zongming Ma},
 title = {Optimal rates of convergence for noisy sparse phase retrieval via thresholded wirtinger flow},
 journal = {The Annals of Statistics},
 year = {2016}
} 


@article{NJS13,
 author = {Praneeth Netrapalli and Prateek Jain and Sujay Sanghavi},
 title = {Phase Retrieval using Alternating Minimization},
 journal = {NeurIPS},
 year = {2013}
} 

@article{CC17,
 author = {Yuxin Chen and Emmanuel J. Cand{\'e}s},
 title = {Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems},
 journal = {Communications on Pure and Applied Mathematics},
 year = {2017}
} 


@article{CLS15,
 author = {Emmanuel J. Cand{\'e}s and Xiaodong Li and Mahdi Soltanolkotabi},
 title = {Phase Retrieval via Wirtinger Flow: Theory and Algorithms},
 journal = {IEEE Transactions on Information Theory},
 year = {2015}
} 


@article{DH14,
 author = {Laurent Demanet and Paul Hand},
 title = {Stable optimizationless recovery from phaseless linear measurements},
 journal = {Journal of Fourier Analysis and Applications},
 year = {2014}
} 

@article{CL14,
 author = {Emmanuel J. Cand{\'e}s and Xiaodong Li},
 title = {Solving Quadratic Equations via PhaseLift when There Are About As Many Equations As Unknowns},
 journal = {Foundations of Computational Mathematics},
 year = {2014}
} 


@article{SECCMS15,
 author = {Yoav Shechtman and Yonina C. Eldar and Oren Cohen and Henry Nicholas Chapman 
 and Jianwei Miao and Mordechai Segev},
 title = {Phase retrieval with application to optical imaging: a contemporary overview},
 journal = {IEEE signal processing magazine},
 year = {2015}
} 


@article{CSV13,
 author = {Emmanuel J. Cand{\'e}s and Thomas Strohmer and Vladislav Voroninski},
 title = {PhaseLift: Exact and Stable Signal Recovery from Magnitude Measurements via Convex Programming},
 journal = {Communications on Pure and Applied Mathematics},
 year = {2013}
} 


@article{CESV12,
 author = {Emmanuel J. Cand{\'e}s and Yonina Eldar and Thomas Strohmer and Vlad Voroninski},
 title = {Phase retrieval via matrix completion},
 journal = {SIAM Journal on Imaging Sciences},
 year = {2013}
} 

@article{CCFMY18,
 author = {Yuxin Chen and Yuejie Chi and Jianqing Fan and Cong Ma and Yuling Yan},
 title = {Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval},
 journal = {Mathematical Programming},
 year = {2019}
} 


@article{CD19,
 author = {Yair Carmon and John Duchi},
 title = {Gradient Descent Finds the Cubic-Regularized Nonconvex Newton Step},
 journal = {SIAM Journal on Optimization},
 year = {2019}
} 



@article{GH15,
 author = {Dan Garber and Elad Hazan},
 title = {Fast and Simple PCA via Convex Optimization},
 journal = {arXiv:1509.05647},
 year = {2015}
} 

@article{GHM15,
 author = {Dan Garber and Elad Hazan and Tengyu Ma},
 title = {Online Learning of Eigenvectors},
 journal = {ICML},
 year = {2015}
} 

@article{ZSJBD17,
 author = {Kai Zhong and Zhao Song and Prateek Jain and Peter L. Bartlett and Inderjit S. Dhillon},
 title = {Recovery Guarantees for One-hidden-layer Neural Networks},
 journal = {ICML},
 year = {2017}
} 


@article{XHDMR18,
 author = {Peng Xu and Bryan He and Christopher De Sa and Ioannis Mitliagkas and Christopher Re},
 title = {Accelerated Stochastic Power Iteration},
 journal = {AISTATS},
 year = {2018}
} 


@article{S16,
 author = {Ohad Shamir},
 title = {Convergence of Stochastic Gradient Descent for PCA},
 journal = {ICML},
 year = {2016}
} 

@article{JJKNS16,
 author = {Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli and Aaron Sidford},
 title = {Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample Guarantees for Oja's Algorithm},
 journal = {COLT},
 year = {2016}
} 


@article{MWCC17,
 author = {Cong Ma and Kaizheng Wang and Yuejie Chi and Yuxin Chen},
 title = {Implicit Regularization in Nonconvex Statistical Estimation:
Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind Deconvolution},
 journal = {Foundations of Computational Mathematics},
 year = {2017}
} 


@article{CGZ19,
 author = {Bugra Can and Mert G{\"u}rb{\"u}zbalaban and Lingjiong Zhu},
 title = {Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances},
 journal = {ICML},
 year = {2019}
} 



@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 


@article{SQW16,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {A Geometrical Analysis of Phase Retrieval},
 journal = {International Symposium on Information Theory},
 year = {2016}
} 

@article{SQW15,
 author = {Ju Sun and Qing Qu and John Wright},
 title = {When Are Nonconvex Problems Not Scary?},
 journal = {NeurIPS Workshop on Non-convex Optimization for Machine Learning: Theory and Practice},
 year = {2015}
} 


@article{BHK15,
 author = {Avrim Blum and John Hopcroft and Ravindran Kannan},
 title = {Foundations of Data Science},
 year = {2015}
} 


@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}


@article{goh2017why,
  author = {Gabriel Goh},
  title = {Why Momentum Really Works},
  journal = {Distill},
  year = {2017}
}

@article{G17,
  author = {Xavier Gastaldi},
  title = {Shake-Shake regularization},
  journal = {arXiv:1705.07485},
  year = {2017}
}

@article{WRSSR17,
  author = {Ashia C Wilson and Rebecca Roelofs and Mitchell Stern and Nathan Srebro and and Benjamin Recht.},
  title = {The marginal value of adaptive gradient methods in machine learning},
  journal = {NeurIPS},
  year = {2017}
}

@article{CZMVL18,
  author = {Ekin D Cubuk and Barret Zoph and Dandelion Mane and Vijay Vasudevan and Quoc V Le.},
  title = {Autoaugment: Learning augmentation policies from data.},
  journal = {arXiv:1805.09501},
  year = {2018}
}



@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}

@ARTICLE{P64,
  title={Some methods of speeding up the convergence of iteration methods},
  author={B.T. Polyak},
  journal   = {USSR Computational Mathematics and Mathematical Physics},
  year      = {1964}
}

@ARTICLE{GPS16,
  title={Stochastic Heavy Ball},
  author={S{\'e}bastien Gadat and Fabien Panloup and Sofiane Saadane},
  journal   = {arXiv:1609.04228},
  year      = {2016}
}

@ARTICLE{GFJ15,
  title={Global convergence of the Heavy-ball method for convex optimization},
  author={Euhanna Ghadimi and Hamid Reza Feyzmahdavian and Mikael Johansson},
  journal   = {ECC},
  year      = {2015}
}


@ARTICLE{LR17,
  title={Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods},
  author={Nicolas Loizou and Peter Richt{\'a}rik},
  journal   = {arXiv:1712.09677},
  year      = {2017}
}

@ARTICLE{SYLHGJ19,
  title={Non-ergodic Convergence Analysis of Heavy-Ball Algorithms},
  author={Tao Sun and Penghang Yin and Dongsheng Li and Chun Huang and Lei Guan and Hao Jiang},
  journal   = {AAAI},
  year      = {2019}
}

@ARTICLE{OCBP14,
  title={ipiano: Inertial proximal algorithm for nonconvex optimization},
  author={Peter Ochs and Yunjin Chen and Thomas Brox and Thomas Pock},
  journal   = {SIAM Journal of Imaging Sciences},
  year      = {2014}
}



@ARTICLE{GLZ16,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Saeed Ghadimi and Guanghui Lan and Hongchao Zhang},
  journal   = {Mathematical Programming},
  year      = {2016}
}


@ARTICLE{LJCJ17,
  title={Nonconvex finite-sum optimization via scsg methods},
  author={Lihua Lei and Cheng Ju and Jianbo Chen and Michael I. Jordan},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{YXG18,
  title={Stochastic nested variance reduced gradient descent for nonconvex optimization},
  author={Yaodong Yu and Pan Xu and Quanquan Gu},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{CHMAL15,
  title={The Loss Surfaces of Multilayer Networks},
  author={Anna Choromanska and Mikael Henaff and Michael Mathieu and G{\'e}rard Ben Arous and Yann LeCun},
  journal   = {AISTAT},
  year      = {2015}
}

@ARTICLE{N13,
  title={Introductory lectures on convex optimization: a basic course},
  author={Yurii Nesterov},
  journal   = {Springer},
  year      = {2013}
}


@ARTICLE{NKJK18,
  title={On the insufficiency of existing momentum schemes for Stochastic Optimization},
  author={Rahul Kidambi and Praneeth Netrapalli and Prateek Jain and Sham M. Kakade},
  journal   = {ICLR},
  year      = {2018}
}


@ARTICLE{JNGKJ19,
  title={Stochastic Gradient Descent Escapes Saddle Points Efficiently},
  author={Chi Jin and Praneeth Netrapalli and Rong Ge and Sham M. Kakade and Michael I. Jordan},
  journal   = {arXiv:1902.04811},
  year      = {2019}
}


@ARTICLE{G11,
  title={Recovering low-rank matrices from few coefficients in any basis},
  author={David Gross},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2011}
}

@ARTICLE{SMDH13,
  title={On the importance of initialization and momentum in deep learning},
  author={Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton},
  journal   = {ICML},
  year      = {2013}
}


@ARTICLE{WKXS18,
  title={Identifying Generalization Properties in Neural Networks},
  author={Huan Wang and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
  journal   = {arXiv:1809.07402},
  year      = {2018}
}

@ARTICLE{GS18,
  title={Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization},
  author={Alon Gonen and Shai Shalev-Shwartz},
  journal   = {JMLR},
  year      = {2018}
}


@ARTICLE{SSSS09,
  title={Learnability and Stability in the General Learning Setting.},
  author={Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  journal   = {COLT},
  year      = {2009}
}


@ARTICLE{SSSS10,
  title={Learnability, Stability and Uniform Convergence},
  author={Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
  journal   = {JMLR},
  year      = {2010}
}


@ARTICLE{BL02,
  title={Stability and Generalization},
  author={Olivier Bousquet and Andre Elisseeff},
  journal   = {JMLR},
  year      = {2002}
}


@ARTICLE{YLL18,
  title={Unified Convergence Analysis of Stochastic Momentum Methods for Convex and Non-convex Optimization},
  author={Tianbao Yang and Qihang Lin and Zhe Li},
  journal   = {IJCAI},
  year      = {2018}
}


@ARTICLE{FV18,
  title={Generalization Bounds for Uniformly Stable Algorithms},
  author={Vitaly Feldman and Jan Vondrak},
  journal   = {NeurIPS},
  year      = {2018}
}


@ARTICLE{KL18,
  title={Data-Dependent Stability of Stochastic Gradient Descent},
  author={Ilja Kuzborskij, Christoph H. Lampert},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{MWZZ18,
  title={Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints},
  author={Wenlong Mou and Liwei Wang and Xiyu Zhai and Kai Zheng},
  journal   = {COLT},
  year      = {2018}
}

@ARTICLE{CP17,
  title={Stability and Generalization of Learning Algorithms that Converge to Global Optima},
  author={Zachary Charles and Dimitris Papailiopoulos},
  journal   = {ICML},
  year      = {2018}
}


@ARTICLE{GS17,
  title={Fast Rates for Empirical Risk Minimization of Strict Saddle Problems},
  author={Alon Gonen and Shai Shalev-Shwartz},
  journal   = {COLT},
  year      = {2017}
}

@ARTICLE{HHS17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Elad Hoffer and Itay Hubara and Daniel Soudry},
  journal   = {NeurIPS},
  year      = {2017}
}


@ARTICLE{HRS16,
  title={Train Faster, Generalize Better: Stability of Stochastic Gradient Descent},
  author={Moritz Hardt and Benjamin Recht and Yoram Singer},
  journal   = {ICML},
  year      = {2016}
}

