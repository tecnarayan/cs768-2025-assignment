\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adamczak(2015)]{adamczak15}
R.~Adamczak.
\newblock {A note on the Hanson-Wright inequality for random vectors with
  dependencies}.
\newblock \emph{Electronic Communications in Probability}, 20\penalty0
  (none):\penalty0 1 -- 13, 2015.
\newblock \doi{10.1214/ECP.v20-3829}.
\newblock URL \url{https://doi.org/10.1214/ECP.v20-3829}.

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Z.~Allen-Zhu.
\newblock \href{https://jmlr.org/papers/volume18/16-410/16-410.pdf}{Katyusha:
  The first direct acceleration of stochastic gradient methods}.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8194--8244, 2017.

\bibitem[Asmussen(2003)]{asmussen03}
S.~Asmussen.
\newblock \emph{\href{https://link.springer.com/book/10.1007/b97236}{Applied
  probability and queues}}, volume~51 of \emph{Applications of Mathematics (New
  York)}.
\newblock Springer-Verlag, New York, second edition, 2003.
\newblock Stochastic Modelling and Applied Probability.

\bibitem[Bardenet and M.(2015)]{bardenet15}
R.~Bardenet and Odalric-Ambrym M.
\newblock {Concentration inequalities for sampling without replacement}.
\newblock \emph{Bernoulli}, 21\penalty0 (3):\penalty0 1361 -- 1385, 2015.
\newblock \doi{10.3150/14-BEJ605}.
\newblock URL \url{https://doi.org/10.3150/14-BEJ605}.

\bibitem[De et~al.(2017)De, Yadav, Jacobs, and Goldstein]{de17}
S.~De, A.~Yadav, D.~Jacobs, and T.~Goldstein.
\newblock \href{http://proceedings.mlr.press/v54/de17a/de17a.pdf}{Big Batch
  SGD: Automated Inference using Adaptive Batch Sizes}.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Flammarion and Bach(2015)]{flammarion15}
N.~Flammarion and F.~Bach.
\newblock From averaging to acceleration, there is only a step-size.
\newblock In Peter Grünwald, Elad Hazan, and Satyen Kale, editors,
  \emph{Proceedings of The 28th Conference on Learning Theory}, volume~40 of
  \emph{Proceedings of Machine Learning Research}, pages 658--695, Paris,
  France, 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v40/Flammarion15.html}.

\bibitem[Gadat et~al.(2018)Gadat, Panloup, and Saadane]{gadat18}
S.~Gadat, F.~Panloup, and S.~Saadane.
\newblock {Stochastic heavy ball}.
\newblock \emph{Electronic Journal of Statistics}, 12\penalty0 (1):\penalty0
  461 -- 529, 2018.
\newblock \doi{10.1214/18-EJS1395}.
\newblock URL \url{https://doi.org/10.1214/18-EJS1395}.

\bibitem[Goyal et~al.(2017)Goyal, Dollár, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal17}
P.~Goyal, P.~Dollár, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock \href{https://arxiv.org/abs/1706.02677}{Accurate, Large Minibatch
  SGD: Training ImageNet in 1 Hour}, 2017.

\bibitem[Jain et~al.(2018)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{jain2018accelerating}
P.~Jain, S.~Kakade, R.~Kidambi, P.~Netrapalli, and A.~Sidford.
\newblock
  \href{http://proceedings.mlr.press/v75/jain18a/jain18a.pdf}{Accelerating
  Stochastic Gradient Descent for Least Squares Regression}.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory
  (COLT)}, volume~75 of \emph{Proceedings of Machine Learning Research}, pages
  545--604. PMLR, 2018.

\bibitem[Kidambi et~al.(2018)Kidambi, Netrapalli, Jain, and
  Kakade]{kidambi2018}
R.~Kidambi, P.~Netrapalli, P.~Jain, and S.~Kakade.
\newblock On the insufficiency of existing momentum schemes for stochastic
  optimization.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pages 1--9, 2018.
\newblock \doi{10.1109/ITA.2018.8503173}.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Y.~LeCun, C.~Cortes, and C.~Burges.
\newblock "mnist" handwritten digit database, 2010.
\newblock URL \url{http://yann. lecun. com/exdb/mnist}.

\bibitem[{Liu} and {Belkin}(2020)]{Liu2020accelerating}
C.~{Liu} and M.~{Belkin}.
\newblock \href{https://arxiv.org/pdf/1810.13395.pdf}{Accelerating SGD with
  momentum for over-parameterized learning}.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Loizou and Richtarik(2020)]{loizou20}
N.~Loizou and P.~Richtarik.
\newblock Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods.
\newblock \emph{Comput Optim Appl}, 77:\penalty0 653--710, 2020.
\newblock URL \url{https://doi.org/10.1007/s10589-020-00220-z}.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma2018}
S.~Ma, R.~Bassily, and M.~Belkin.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3325--3334. PMLR, 2018.

\bibitem[Marchenko and Pastur(1967)]{marchenko1967}
V.~Marchenko and L.~Pastur.
\newblock Distribution of eigenvalues for some sets of random matrices.
\newblock \emph{Mathematics of the USSR-Sbornik}, 1967.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish18}
S.~McCandlish, J.~Kaplan, D.~Amodei, and OpenAI~Dota Team.
\newblock \href{https://arxiv.org/abs/1812.06162}{An Empirical Model of
  Large-Batch Training}, 2018.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Y.~Nesterov.
\newblock \emph{\href{http://dx.doi.org/10.1007/978-1-4419-8853-9}{Introductory
  lectures on convex optimization}}.
\newblock Springer, 2004.

\bibitem[Orvieto et~al.(2020)Orvieto, Kohler, and Lucchi]{orvieto20}
A.~Orvieto, J.~Kohler, and A.~Lucchi.
\newblock The role of memory in stochastic optimization.
\newblock In Ryan~P. Adams and Vibhav Gogate, editors, \emph{Proceedings of The
  35th Uncertainty in Artificial Intelligence Conference}, volume 115 of
  \emph{Proceedings of Machine Learning Research}, pages 356--366. PMLR, 2020.
\newblock URL \url{https://proceedings.mlr.press/v115/orvieto20a.html}.

\bibitem[Paquette and Paquette(2021)]{paquette2021dynamics}
C.~Paquette and E.~Paquette.
\newblock
  \href{https://papers.nips.cc/paper/2021/hash/4cf0ed8641cfcbbf46784e620a0316fb-Abstract.html}{Dynamics
  of Stochastic Momentum Methods on Large-scale, Quadratic Models}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~34, 2021.

\bibitem[Paquette et~al.(2021)Paquette, Lee, Pedregosa, and
  Paquette]{paquette21a}
C.~Paquette, K.~Lee, F.~Pedregosa, and E.~Paquette.
\newblock Sgd in the large: Average-case analysis, asymptotics, and stepsize
  criticality.
\newblock In \emph{Proceedings of Thirty Fourth Conference on Learning Theory
  (COLT)}, volume 134 of \emph{Proceedings of Machine Learning Research}, pages
  3548--3626. PMLR, 2021.
\newblock URL \url{https://proceedings.mlr.press/v134/paquette21a.html}.

\bibitem[Pedregosa(2021)]{pedregosa2021residual}
F.~Pedregosa.
\newblock A hitchhiker's guide to momentum, 2021.
\newblock URL \url{http://fa.bianp.net/blog/2021/hitchhiker/}.

\bibitem[Polyak(1964)]{Polyak1962Some}
B.T. Polyak.
\newblock \href{https://doi.org/10.1016/0041-5553(64)90137-5}{Some methods of
  speeding up the convergence of iteration methods}.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 04,
  1964.

\bibitem[Sebbouh et~al.(2021)Sebbouh, Gower, and Defazio]{sebbouh21}
O.~Sebbouh, R.~Gower, and A.~Defazio.
\newblock Almost sure convergence rates for stochastic gradient descent and
  stochastic heavy ball.
\newblock In \emph{Proceedings of Thirty Fourth Conference on Learning Theory
  (COLT)}, volume 134 of \emph{Proceedings of Machine Learning Research}, pages
  3935--3971. PMLR, 2021.
\newblock URL \url{https://proceedings.mlr.press/v134/sebbouh21a.html}.

\bibitem[Smith et~al.(2018)Smith, Kindermans, Ying, and Le]{smith18}
S.L. Smith, P.-J. Kindermans, C.~Ying, and Q.~V. Le.
\newblock \href{https://openreview.net/forum?id=B1Yy1BxCZ}{Don't Decay the
  Learning Rate, Increase the Batch Size}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever13}
I.~Sutskever, J.~Martens, G.~Dahl, and G.~Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In Sanjoy Dasgupta and David McAllester, editors, \emph{Proceedings
  of the 30th International Conference on Machine Learning (ICML)}, volume~28
  of \emph{Proceedings of Machine Learning Research}, pages 1139--1147,
  Atlanta, Georgia, USA, 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/sutskever13.html}.

\bibitem[Vershynin(2018)]{vershynin18}
R.~Vershynin.
\newblock \emph{High-Dimensional Probability: An Introduction with Applications
  in Data Science}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2018.
\newblock \doi{10.1017/9781108231596}.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue,
  and Grosse]{zhang19}
G.~Zhang, L.~Li, Z.~Nado, J.~Martens, S.~Sachdeva, G.~Dahl, C.~Shallue, and
  R.~Grosse.
\newblock Which algorithmic choices matter at which batch sizes? insights from
  a noisy quadratic model.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~32, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/e0eacd983971634327ae1819ea8b6214-Paper.pdf}.

\end{thebibliography}
