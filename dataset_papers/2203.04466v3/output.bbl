\begin{thebibliography}{109}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aarts \& Lenstra(1996)Aarts and Lenstra]{ls1996book}
Aarts, E. and Lenstra, J.
\newblock \emph{Local Search in Combinatorial Optimization}.
\newblock Wiley, 1996.

\bibitem[Aghasi et~al.(2017)Aghasi, Abdi, Nguyen, and
  Romberg]{aghasi2017nettrim}
Aghasi, A., Abdi, A., Nguyen, N., and Romberg, J.
\newblock {Net-Trim}: Convex pruning of deep neural networks with performance
  guarantee.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Applegate et~al.(2006)Applegate, Bibxy, Chv\'atal, and
  Cook]{tsp2006book}
Applegate, D., Bibxy, R., Chv\'atal, V., and Cook, W.
\newblock \emph{The Traveling Salesman Problem: A Computational Study}.
\newblock Princeton University Press, 2006.

\bibitem[Arora et~al.(2018{\natexlab{a}})Arora, Basu, Mianjy, and
  Mukherjee]{arora2018understanding}
Arora, R., Basu, A., Mianjy, P., and Mukherjee, A.
\newblock Understanding deep neural networks with rectified linear units.
\newblock In \emph{ICLR}, 2018{\natexlab{a}}.

\bibitem[Arora et~al.(2018{\natexlab{b}})Arora, Ge, Neyshabur, and
  Zhang]{arora2018bounds}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Baykal et~al.(2019)Baykal, Liebenwein, Gilitschenski, Feldman, and
  Rus]{baykal2019coresets}
Baykal, C., Liebenwein, L., Gilitschenski, I., Feldman, D., and Rus, D.
\newblock Data-dependent coresets for compressing neural networks with
  applications to generalization bounds.
\newblock In \emph{ICLR}, 2019.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019nas}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine learning practice and the bias-variance
  trade-off.
\newblock \emph{PNAS}, 116\penalty0 (32):\penalty0 15849--15854, 2019.

\bibitem[Benders(1962)]{benders1962}
Benders, J.
\newblock Partitioning procedures for solving mixed-variables programming
  problems.
\newblock \emph{Numerische Mathematik}, 4:\penalty0 238--252, 1962.

\bibitem[Bixby(2012)]{bixby2012history}
Bixby, R.
\newblock A brief history of linear and mixed-integer programming computation.
\newblock In \emph{Documenta Mathematica, Extra Volume: Optimization Stories},
  pp.\  107--121, 2012.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020survey}
Blalock, D., Ortiz, J., Frankle, J., and Guttag, J.
\newblock What is the state of neural network pruning?
\newblock In \emph{MLSys}, 2020.

\bibitem[Chen et~al.(2021)Chen, Ji, Ding, Fang, Wang, Zhu, Liang, Shi, Yi, and
  Tu]{chen2021once}
Chen, T., Ji, B., Ding, T., Fang, B., Wang, G., Zhu, Z., Liang, L., Shi, Y.,
  Yi, S., and Tu, X.
\newblock Only train once: A one-shot neural network training and pruning
  framework.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Chijiwa et~al.(2021)Chijiwa, Yamaguchi, Ida, Umakoshi, and
  Inoue]{daiki2021randomization}
Chijiwa, D., Yamaguchi, S., Ida, Y., Umakoshi, K., and Inoue, T.
\newblock Pruning randomly initialized neural networks with iterative
  randomization.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Conforti et~al.(2014)Conforti, Cornu\'ejols, and
  Zambelli]{ccz2014book}
Conforti, M., Cornu\'ejols, G., and Zambelli, G.
\newblock \emph{Integer Programming}.
\newblock Springer, 2014.

\bibitem[Croes(1958)]{croes1958inversion}
Croes, G.
\newblock A method for solving traveling-salesman problems.
\newblock \emph{Operations Research}, 6:\penalty0 791--812, 1958.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, Ranzato, and
  Freitas]{denil2013parameters}
Denil, M., Shakibi, B., Dinh, L., Ranzato, M., and Freitas, N.
\newblock Predicting parameters in deep learning.
\newblock In \emph{NeurIPS}, 2013.

\bibitem[Denton et~al.(2014)Denton, Zaremba, Bruna, LeCun, and
  Fergus]{denton2014linear}
Denton, E., Zaremba, W., Bruna, J., LeCun, Y., and Fergus, R.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017layerwise}
Dong, X., Chen, S., and Pan, S.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Ebrahimi \& Klabjan(2021)Ebrahimi and Klabjan]{ebrahimi2021kfac}
Ebrahimi, A. and Klabjan, D.
\newblock Neuron-based pruning of deep neural networks with better
  generalization using kronecker factored curvature approximation, 2021.

\bibitem[ElAraby et~al.(2020)ElAraby, Wolf, and
  Carvalho]{elaraby2020importance}
ElAraby, M., Wolf, G., and Carvalho, M.
\newblock Identifying efficient sub-networks using mixed integer programming.
\newblock In \emph{OPT Workshop}, 2020.

\bibitem[Elesedy et~al.(2020)Elesedy, Kanade, and Teh]{elesedy2020lth}
Elesedy, B., Kanade, V., and Teh, Y.~W.
\newblock Lottery tickets in linear models: An analysis of iterative magnitude
  pruning, 2020.

\bibitem[Feo \& Resende(1989)Feo and Resende]{feo1989setcovering}
Feo, T.~A. and Resende, M. G.~C.
\newblock A probabilistic heuristic for a computationally difficult set
  covering problem.
\newblock \emph{Operations Research Letters}, 8\penalty0 (2):\penalty0 67--71,
  1989.

\bibitem[Feo \& Resende(1995)Feo and Resende]{feo1995grasp}
Feo, T.~A. and Resende, M. G.~C.
\newblock Greedy randomized adaptive search procedures.
\newblock \emph{Journal of Global Optimization}, 6\penalty0 (2):\penalty0
  109--133, 1995.

\bibitem[Flood(1956)]{flood1956tsp}
Flood, M.
\newblock The traveling-salesman problem.
\newblock \emph{Operations Research}, 4:\penalty0 61--75, 1956.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2019lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{ICLR}, 2019.

\bibitem[Frankle et~al.(2021)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2021initialization}
Frankle, J., Dziugaite, G., Roy, D., and Carbin, M.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock In \emph{ICLR}, 2021.

\bibitem[Ganev \& Walters(2022)Ganev and Walters]{iclr2022compression}
Ganev, I. and Walters, R.
\newblock Model compression via symmetries of the parameter space.
\newblock 2022.
\newblock URL \url{https://openreview.net/forum?id=8MN_GH4Ckp4}.

\bibitem[Good et~al.(2022)Good, Lin, Sieg, Ferguson, Yu, Zhe, Wieczorek, and
  Serra]{good2022recall}
Good, A., Lin, J., Sieg, H., Ferguson, M., Yu, X., Zhe, S., Wieczorek, J., and
  Serra, T.
\newblock Recall distortion in neural network pruning and the undecayed pruning
  algorithm.
\newblock 2022.

\bibitem[Gordon et~al.(2020)Gordon, Duh, and Andrews]{gordon2020bert}
Gordon, M., Duh, K., and Andrews, N.
\newblock Compressing {BERT}: Studying the effects of weight pruning on
  transfer learning.
\newblock In \emph{Rep4NLP Workshop}, 2020.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015connections}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{NeurIPS}, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2016deepcompression}
Han, S., Mao, H., and Dally, W.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and {Huffman} coding.
\newblock In \emph{ICLR}, 2016.

\bibitem[Hanin \& Rolnick(2019{\natexlab{a}})Hanin and
  Rolnick]{hanin2019complexity}
Hanin, B. and Rolnick, D.
\newblock Complexity of linear regions in deep networks.
\newblock In \emph{ICML}, 2019{\natexlab{a}}.

\bibitem[Hanin \& Rolnick(2019{\natexlab{b}})Hanin and Rolnick]{hanin2019deep}
Hanin, B. and Rolnick, D.
\newblock Deep relu networks have surprisingly few activation patterns.
\newblock In \emph{NeurIPS}, 2019{\natexlab{b}}.

\bibitem[Hanson \& Pratt(1988)Hanson and Pratt]{hanson1988minimal}
Hanson, S. and Pratt, L.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock In \emph{NeurIPS}, 1988.

\bibitem[Hart \& Shogan(1987)Hart and Shogan]{hart1987semigreedy}
Hart, J.~P. and Shogan, A.~W.
\newblock Semi-greedy heuristics: An empirical study.
\newblock \emph{Operations Research Letters}, 6\penalty0 (3):\penalty0
  107--114, 1987.

\bibitem[Hassibi \& Stork(1992)Hassibi and Stork]{hassibi1992surgeon}
Hassibi, B. and Stork, D.
\newblock Second order derivatives for network pruning: {Optimal Brain
  Surgeon}.
\newblock In \emph{NeurIPS}, 1992.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{hassibi1993surgeon}
Hassibi, B., Stork, D., and Wolff, G.
\newblock Optimal brain surgeon and general network pruning.
\newblock In \emph{IEEE International Conference on Neural Networks}, 1993.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017cnn}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{ICCV}, 2017.

\bibitem[Hooker \& Ottosson(2003)Hooker and Ottosson]{hooker2003lbbd}
Hooker, J. and Ottosson, G.
\newblock Logic-based {Benders} decomposition.
\newblock \emph{Mathematical Programming}, 96:\penalty0 33--60, 2003.

\bibitem[Hooker et~al.(2019)Hooker, Courville, Clark, Dauphin, and
  Frome]{hooker2019forget}
Hooker, S., Courville, A., Clark, G., Dauphin, Y., and Frome, A.
\newblock What do compressed deep neural networks forget?
\newblock \emph{arXiv}, 1911.05248, 2019.

\bibitem[Hooker et~al.(2020)Hooker, Moorosi, Clark, Bengio, and
  Denton]{hooker2020bias}
Hooker, S., Moorosi, N., Clark, G., Bengio, S., and Denton, E.
\newblock Characterising bias in compressed models.
\newblock \emph{arXiv}, 2010.03058, 2020.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{mobilenets}
Howard, A., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock {MobileNets}: Efficient convolutional neural networks for mobile
  vision applications.
\newblock 2017.

\bibitem[Jaderberg et~al.(2014)Jaderberg, Vedaldi, and
  Zisserman]{jaderberg2014lowrank}
Jaderberg, M., Vedaldi, A., and Zisserman, A.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock In \emph{BMVC}, 2014.

\bibitem[Janowsky(1989)]{janowsky1989prunning}
Janowsky, S.
\newblock Pruning versus clipping in neural networks.
\newblock \emph{Physical Review A}, 1989.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Krizhevsky, A. et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lebedev \& Lempitsky(2016)Lebedev and
  Lempitsky]{lebedev2016braindamage}
Lebedev, V. and Lempitsky, V.
\newblock Fast {ConvNets} using group-wise brain damage.
\newblock In \emph{CVPR}, 2016.

\bibitem[Lebedev et~al.(2015)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lebedev2015decomposition}
Lebedev, V., Ganin, Y., Rakhuba, M., Oseledets, I., and Lempitsky, V.
\newblock Speeding-up convolutional neural networks using fine-tuned
  {CP}-decomposition.
\newblock In \emph{ICLR}, 2015.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{lecun1989damage}
LeCun, Y., Denker, J., and Solla, S.
\newblock Optimal brain damage.
\newblock In \emph{NeurIPS}, 1989.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998mnist}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 1998.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2019pretraining}
Lee, N., Ajanthan, T., and Torr, P.
\newblock {SNIP}: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{ICLR}, 2019.

\bibitem[Lee et~al.(2020)Lee, Ajanthan, Gould, and Torr]{lee2020pretraining}
Lee, N., Ajanthan, T., Gould, S., and Torr, P.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock In \emph{ICLR}, 2020.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{li2017cnn}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{ICLR}, 2017.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{li2018landscape}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Li et~al.(2020)Li, Sun, Su, Suzuki, and Huang]{li2020understanding}
Li, J., Sun, Y., Su, J., Suzuki, T., and Huang, F.
\newblock Understanding generalization in deep learning via tensor methods,
  2020.

\bibitem[Liebenwein et~al.(2020)Liebenwein, Baykal, Lang, Feldman, and
  Rus]{liebenwein2020provable}
Liebenwein, L., Baykal, C., Lang, H., Feldman, D., and Rus, D.
\newblock Provable filter pruning for efficient neural networks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Liebenwein et~al.(2021)Liebenwein, Baykal, Carter, Gifford, and
  Rus]{liebenwein2021lost}
Liebenwein, L., Baykal, C., Carter, B., Gifford, D., and Rus, D.
\newblock Lost in pruning: The effects of pruning neural networks beyond test
  accuracy.
\newblock In \emph{MLSys}, 2021.

\bibitem[Lin(1965)]{lin1965tsp}
Lin, S.
\newblock Computer solutions of the traveling salesman problem.
\newblock \emph{The Bell System Technical Journal}, 44:\penalty0 2245--2269,
  1965.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Zhang, Kuang, Zhou, Xue, Wang,
  Chen, Yang, Liao, and Zhang]{liu2021group}
Liu, L., Zhang, S., Kuang, Z., Zhou, A., Xue, J.-H., Wang, X., Chen, Y., Yang,
  W., Liao, Q., and Zhang, W.
\newblock Group fisher pruning for practical network compression.
\newblock In \emph{ICML}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Chen, Chen, Atashgahi, Yin, Kou,
  Shen, Pechenizkiy, Wang, and Mocanu]{liu2021sparse}
Liu, S., Chen, T., Chen, X., Atashgahi, Z., Yin, L., Kou, H., Shen, L.,
  Pechenizkiy, M., Wang, Z., and Mocanu, D.~C.
\newblock Sparse training via boosting pruning plasticity with
  neuroregeneration.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{liu2019structure}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{ICLR}, 2019.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017thinet}
Luo, J., Wu, J., and Lin, W.
\newblock {ThiNet}: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{ICCV}, 2017.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Shwartz, and
  Shamir]{malach2020lth}
Malach, E., Yehudai, G., Shalev-Shwartz, S., and Shamir, O.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock In \emph{ICML}, 2020.

\bibitem[Mariet \& Sra(2016)Mariet and Sra]{mariet2016diversity}
Mariet, Z. and Sra, S.
\newblock Diversity networks: Neural network compression using determinantal
  point processes.
\newblock In \emph{ICLR}, 2016.

\bibitem[Molchanov et~al.(2017)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2017taylor}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In \emph{ICLR}, 2017.

\bibitem[Mont\'{u}far(2017)]{montufar2017notes}
Mont\'{u}far, G.
\newblock Notes on the number of linear regions of deep neural networks.
\newblock In \emph{SampTA}, 2017.

\bibitem[Mont\'{u}far et~al.(2014)Mont\'{u}far, Pascanu, Cho, and
  Bengio]{montufar2014on}
Mont\'{u}far, G., Pascanu, R., Cho, K., and Bengio, Y.
\newblock On the number of linear regions of deep neural networks.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Mont\'{u}far et~al.(2021)Mont\'{u}far, Ren, and
  Zhang]{montufar2021maxout}
Mont\'{u}far, G., Ren, Y., and Zhang, L.
\newblock Sharp bounds for the number of regions of maxout networks and
  vertices of {Minkowski} sums, 2021.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989relevance}
Mozer, M. and Smolensky, P.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1989.

\bibitem[Newcomer(1996)]{flounder1996}
Newcomer, J.~M.
\newblock Optimization: Your worst enemy.
\newblock \url{http://flounder.com/optimization.htm}, 1996.
\newblock Accessed: 2022-01-28.

\bibitem[Nguyen et~al.(2021)Nguyen, Novak, Xiao, and Lee]{nguyen2021dataset}
Nguyen, T., Novak, R., Xiao, L., and Lee, J.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5186--5198, 2021.

\bibitem[Orseau et~al.(2020)Orseau, Hutter, and Rivasplata]{orseau2020lth}
Orseau, L., Hutter, M., and Rivasplata, O.
\newblock Logarithmic pruning is all you need.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Padberg(1989)]{padberg1989}
Padberg, M.
\newblock The boolean quadric polytope: Some characteristics, facets and
  relatives.
\newblock \emph{Mathematical Programming}, 45:\penalty0 139--172, 1989.

\bibitem[Paganini(2020)]{paganini2020responsibly}
Paganini, M.
\newblock Prune responsibly.
\newblock \emph{arXiv}, 2009.09936, 2020.

\bibitem[Pascanu et~al.(2014)Pascanu, Mont\'{u}far, and Bengio]{pascanu2013on}
Pascanu, R., Mont\'{u}far, G., and Bengio, Y.
\newblock On the number of response regions of deep feedforward networks with
  piecewise linear activations.
\newblock In \emph{ICLR}, 2014.

\bibitem[Pensia et~al.(2020)Pensia, Rajput, Nagle, Vishwakarma, and
  Papailiopoulos]{pensia2020lth}
Pensia, A., Rajput, S., Nagle, A., Vishwakarma, H., and Papailiopoulos, D.
\newblock Optimal lottery tickets via {SubsetSum}: Logarithmic
  over-parameterization is sufficient.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Qian \& Klabjan(2021)Qian and Klabjan]{qian2021lth}
Qian, X. and Klabjan, D.
\newblock A probabilistic approach to neural network pruning.
\newblock In \emph{ICML}, 2021.

\bibitem[Ramalingam et~al.(2021)Ramalingam, Glasner, Patel, Vemulapalli,
  Jayasumana, and Kumar]{ramalingam2021less}
Ramalingam, S., Glasner, D., Patel, K., Vemulapalli, R., Jayasumana, S., and
  Kumar, S.
\newblock Less data is more: Selecting informative and diverse subsets with
  balancing constraints.
\newblock \emph{CoRR}, abs/2104.12835, 2021.

\bibitem[Ramanujan et~al.(2020)Ramanujan, Wortsman, Kembhavi, Farhadi, and
  Rastegari]{ramanujan2020}
Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., and Rastegari, M.
\newblock What's hidden in a randomly weighted neural network?
\newblock In \emph{CVPR}, 2020.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{renda2020retraining}
Renda, A., Frankle, J., and Carbin, M.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Roh et~al.(2021)Roh, Lee, Whang, and Suh]{roh2021sample}
Roh, Y., Lee, K., Whang, S., and Suh, C.
\newblock Sample selection for fair and robust training.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 815--827, 2021.

\bibitem[Serra \& Ramalingam(2020)Serra and Ramalingam]{serra2020empirical}
Serra, T. and Ramalingam, S.
\newblock Empirical bounds on linear regions of deep rectifier networks.
\newblock In \emph{AAAI}, 2020.

\bibitem[Serra et~al.(2018)Serra, Tjandraatmadja, and
  Ramalingam]{serra2018bounding}
Serra, T., Tjandraatmadja, C., and Ramalingam, S.
\newblock Bounding and counting linear regions of deep neural networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Serra et~al.(2020)Serra, Kumar, and Ramalingam]{serra2020lossless}
Serra, T., Kumar, A., and Ramalingam, S.
\newblock Lossless compression of deep neural networks.
\newblock In \emph{CPAIOR}, 2020.

\bibitem[Serra et~al.(2021)Serra, Yu, Kumar, and
  Ramalingam]{serra2021compression}
Serra, T., Yu, X., Kumar, A., and Ramalingam, S.
\newblock Scaling up exact neural network compression by {ReLU} stability.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Singh \& Alistarh(2020)Singh and Alistarh]{singh2020woodfisher}
Singh, S.~P. and Alistarh, D.
\newblock {WoodFisher}: Efficient second-order approximation for neural network
  compression.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Sourek \& Zelezny(2021)Sourek and Zelezny]{sourek2021lossless}
Sourek, G. and Zelezny, F.
\newblock Lossless compression of structured convolutional models via lifting.
\newblock In \emph{ICLR}, 2021.

\bibitem[Srinivas \& Babu(2015)Srinivas and Babu]{srinivas2015datafree}
Srinivas, S. and Babu, R.~V.
\newblock Data-free parameter pruning for deep neural networks.
\newblock In \emph{BMVC}, 2015.

\bibitem[Su et~al.(2018)Su, Li, Bhattacharjee, and Huang]{su2018tensorial}
Su, J., Li, J., Bhattacharjee, B., and Huang, F.
\newblock Tensorial neural networks: Generalization of neural networks and
  application to model compression, 2018.

\bibitem[Suau et~al.(2020)Suau, Zappella, and Apostoloff]{suau2020distillation}
Suau, X., Zappella, L., and Apostoloff, N.
\newblock Filter distillation for network compression.
\newblock In \emph{WACV}, 2020.

\bibitem[Sun et~al.(2020)Sun, Li, Liang, Ding, and Srikant]{ruoyu2020landscape}
Sun, R., Li, D., Liang, S., Ding, T., and Srikant, R.
\newblock The global landscape of neural networks: An overview.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (5):\penalty0
  95--108, 2020.

\bibitem[Suzuki et~al.(2020{\natexlab{a}})Suzuki, Abe, Murata, Horiuchi, Ito,
  Wachi, Hirai, Yukishima, and Nishimura]{suzuki2020spectral}
Suzuki, T., Abe, H., Murata, T., Horiuchi, S., Ito, K., Wachi, T., Hirai, S.,
  Yukishima, M., and Nishimura, T.
\newblock Spectral-pruning: Compressing deep neural network via spectral
  analysis.
\newblock In \emph{IJCAI}, 2020{\natexlab{a}}.

\bibitem[Suzuki et~al.(2020{\natexlab{b}})Suzuki, Abe, and
  Nishimura]{suzuki2020bounds}
Suzuki, T., Abe, H., and Nishimura, T.
\newblock Compression based bound for non-compressed network: Unified
  generalization error analysis of large compressible deep neural network.
\newblock In \emph{ICLR}, 2020{\natexlab{b}}.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020synapticflow}
Tanaka, H., Kunin, D., Yamins, D., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Telgarsky(2015)]{Telgarsky2015}
Telgarsky, M.
\newblock Representation benefits of deep feedforward networks.
\newblock 2015.

\bibitem[Tseran \& Mont\'{u}far(2021)Tseran and
  Mont\'{u}far]{tseran2021expected}
Tseran, H. and Mont\'{u}far, G.
\newblock On the expected complexity of maxout networks.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Verma \& Pesquet(2021)Verma and Pesquet]{verma2021subdifferential}
Verma, S. and Pesquet, J.-C.
\newblock Sparsifying networks via subdifferential inclusion.
\newblock In \emph{ICML}, 2021.

\bibitem[Wang et~al.(2019)Wang, Grosse, Fidler, and Zhang]{wang2019eigendamage}
Wang, C., Grosse, R., Fidler, S., and Zhang, G.
\newblock {EigenDamage}: Structured pruning in the {Kronecker}-factored
  eigenbasis.
\newblock In \emph{ICML}, 2019.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{wang2020tickets}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{ICLR}, 2020.

\bibitem[Wang et~al.(2018)Wang, Sun, Eriksson, Wang, and
  Aggarwal]{wang2018wide}
Wang, W., Sun, Y., Eriksson, B., Wang, W., and Aggarwal, V.
\newblock Wide compression: Tensor ring nets, 2018.

\bibitem[Wu \& Wang(2021)Wu and Wang]{wu2021adversarial}
Wu, D. and Wang, Y.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Xing et~al.(2020)Xing, Sha, Hong, Shang, and Liu]{xing2020lossless}
Xing, X., Sha, L., Hong, P., Shang, Z., and Liu, J.
\newblock Probabilistic connection importance inference and lossless
  compression of deep neural networks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Xiong et~al.(2020)Xiong, Huang, Yu, Liu, Zhu, and Shao]{xiong2020cnn}
Xiong, H., Huang, L., Yu, M., Liu, L., Zhu, F., and Shao, L.
\newblock On the number of linear regions of convolutional neural networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Ye et~al.(2020)Ye, Gong, Nie, Zhou, Klivans, and Liu]{ye2020forward}
Ye, M., Gong, C., Nie, L., Zhou, D., Klivans, A., and Liu, Q.
\newblock Good subnetworks provably exist: Pruning via greedy forward
  selection.
\newblock In \emph{ICML}, 2020.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{yu2018importance}
Yu, R., Li, A., Chen, C., Lai, J., Morariu, V., Han, X., Gao, M., Lin, C., and
  Davis, L.
\newblock {NISP}: Pruning networks using neuron importance score propagation.
\newblock In \emph{CVPR}, 2018.

\bibitem[Zeng \& Urtasun(2018)Zeng and Urtasun]{zeng2018mlprune}
Zeng, W. and Urtasun, R.
\newblock {MLPrune}: Multi-layer pruning for automated neural network
  compression.
\newblock 2018.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017rethinking}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Chen, Chen, and Wang]{zhang2021lessdata}
Zhang, Z., Chen, X., Chen, T., and Wang, Z.
\newblock Efficient lottery ticket finding: Less data is more.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{zhou2019lth}
Zhou, H., Lan, J., Liu, R., and Yosinski, J.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In \emph{NeurIPS}, 2019.

\end{thebibliography}
