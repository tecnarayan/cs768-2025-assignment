\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjevani and Shamir(2015)]{ArjevaniShamir2015NIPS}
Yossi Arjevani and Ohad Shamir.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 28}, pages
  1756--1764, 2015.

\bibitem[Bauschke et~al.(2017)Bauschke, Bolte, and
  Teboulle]{bauschke2017descent}
Heinz~H. Bauschke, J{\'e}r{\^o}me Bolte, and Marc Teboulle.
\newblock A descent lemma beyond {L}ipschitz gradient continuity: first-order
  methods revisited and applications.
\newblock \emph{Mathematics of Operations Research}, 42\penalty0 (2):\penalty0
  330--348, 2017.

\bibitem[Beck(2017)]{Beck17book}
Amir Beck.
\newblock \emph{First-Order Methods in Optimization}.
\newblock MOS-SIAM Series on Optimization. SIAM, 2017.

\bibitem[Beck and Teboulle(2009)]{BeckTeboulle09fista}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM Journal on Imaging Sciences}, 2\penalty0 (1):\penalty0
  183--202, 2009.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Pascal Massart.
\newblock \emph{Concentration Inequalities: A Nonasymptotic Theory of
  Independence}.
\newblock Oxford University Press, 2013.

\bibitem[Boyd et~al.(2010)Boyd, Parikh, Chu, Peleato, and Eckstein]{Boyd10ADMM}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends in Machine Learning}, 3\penalty0
  (1):\penalty0 1--122, 2010.

\bibitem[Chen and Teboulle(1993)]{ChenTeboulle93}
Gong Chen and Marc Teboulle.
\newblock Convergence analysis of a proximal-like minimization algorithm using
  {Bregman} functions.
\newblock \emph{SIAM Journal on Optimization}, 3\penalty0 (3):\penalty0
  538--543, August 1993.

\bibitem[Dragomir et~al.(2019)Dragomir, Taylor, d'Aspremont, and
  Bolte]{dragomir2019optimal}
Radu-Alexandru Dragomir, Adrien Taylor, Alexandre d'Aspremont, and
  J{\'e}r{\^o}me Bolte.
\newblock Optimal complexity and certification of {B}regman first-order
  methods.
\newblock \emph{arXiv preprint arXiv:1911.08510}, 2019.

\bibitem[Hanzely et~al.(2018)Hanzely, Richtarik, and Xiao]{Hanzely2018abpg}
Filip Hanzely, Peter Richtarik, and Lin Xiao.
\newblock Accelerated {B}regman proximal gradient methods for relatively smooth
  convex optimization.
\newblock arXiv:1808.03045, 2018.

\bibitem[Hoeffding(1963)]{hoeffding1963probability}
Wassily Hoeffding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock \emph{Journal of the American Statistical Association}, 58\penalty0
  (301):\penalty0 13--30, 1963.

\bibitem[Jaggi et~al.(2014)Jaggi, Smith, Takac, Terhorst, Krishnan, Hofmann,
  and Jordan]{CoCoA2014NIPS}
Martin Jaggi, Virginia Smith, Martin Takac, Jonathan Terhorst, Sanjay Krishnan,
  Thomas Hofmann, and Michael~I. Jordan.
\newblock Communication-efficient distributed dual coordinate ascent.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, pages
  3068--3076, 2014.

\bibitem[Johnson and Zhang(2013)]{JohnsonZhang13}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages
  315--323, 2013.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Rose, and Li]{lewis2004rcv1}
David~D Lewis, Yiming Yang, Tony~G Rose, and Fan Li.
\newblock {RCV1}: A new benchmark collection for text categorization research.
\newblock \emph{Journal of machine learning research}, 5\penalty0
  (Apr):\penalty0 361--397, 2004.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3384--3392, 2015.

\bibitem[Lin and Xiao(2015)]{LinXiao2015homotopy}
Qihang Lin and Lin Xiao.
\newblock An adaptive accelerated proximal gradient method and its homotopy
  continuation for sparse optimization.
\newblock \emph{Computational Optimization and Applications}, 60\penalty0
  (3):\penalty0 633--674, Apr 2015.

\bibitem[Lu et~al.(2018)Lu, Freund, and Nesterov]{lu2018relatively}
Haihao Lu, Robert~M Freund, and Yurii Nesterov.
\newblock Relatively smooth convex optimization by first-order methods, and
  applications.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  333--354, 2018.

\bibitem[Ma et~al.(2015)Ma, Smith, Jaggi, Jordan, Richt\'{a}rik, and
  Tak\'{a}\v{c}]{CoCoA2015ICML}
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael~I. Jordan, Peter
  Richt\'{a}rik, and Martin Tak\'{a}\v{c}.
\newblock Adding vs. averaging in distributed primal-dual optimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pages 1973--1982, 2015.

\bibitem[Ma et~al.(2017)Ma, Smith, Jaggi, Jordan, Richt\'{a}rik, and
  Tak\'{a}\v{c}]{CoCoA2017arbitrary}
Chenxin Ma, Virginia Smith, Martin Jaggi, Michael~I. Jordan, Peter
  Richt\'{a}rik, and Martin Tak\'{a}\v{c}.
\newblock Distributed optimization with arbitrary local solvers.
\newblock \emph{Optimization Methods and Software}, 32\penalty0 (4):\penalty0
  813--848, 2017.

\bibitem[Mahajan et~al.(2018)Mahajan, Agrawal, Keerthi, Sellamanickam, and
  Bottou]{MahajanKeerthi17}
Dhruv Mahajan, Nikunj Agrawal, S.~Sathiya Keerthi, Sundararajan Sellamanickam,
  and Leon Bottou.
\newblock An efficient distributed learning algorithm based on effective local
  functional approximations.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (74):\penalty0 1--37, 2018.

\bibitem[Nesterov(2004)]{Nesterov04book}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Kluwer, Boston, 2004.

\bibitem[Nesterov(2013)]{Nesterov13composite}
Yurii Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming, Ser.\ B}, 140:\penalty0 125--161,
  2013.

\bibitem[Nesterov and Stich(2017)]{nesterov2017efficiency}
Yurii Nesterov and Sebastian~U Stich.
\newblock Efficiency of the accelerated coordinate descent method on structured
  optimization problems.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (1):\penalty0
  110--123, 2017.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Sashank~J. Reddi, Jakub Kone{\v{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock {AIDE}: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Rockafellar(1970)]{Rockafellar70book}
R.~T. Rockafellar.
\newblock \emph{Convex Analysis}.
\newblock Princeton University Press, 1970.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{ScamanBach2017}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 3027--3036, 2017.

\bibitem[Shalev-Shwartz(2016)]{shalev2016sdca}
Shai Shalev-Shwartz.
\newblock Sdca without duality, regularization, and individual convexity.
\newblock In \emph{International Conference on Machine Learning}, pages
  747--754, 2016.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  {N}ewton-type method.
\newblock In \emph{International Conference on Machine Learning}, pages
  1000--1008, 2014.

\bibitem[Tropp(2015)]{tropp2015introduction}
Joel~A. Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{Foundations and Trends in Machine Learning}, 8\penalty0
  (1-2):\penalty0 1--230, 2015.

\bibitem[Vershynin(2019)]{vershynin2019high}
Roman Vershynin.
\newblock \emph{High-Dimensional Probability, An Introduction with Applications
  in Data Science}.
\newblock Cambridge University Press, 2019.

\bibitem[Wang and Zhang(2019)]{wang2019utilizing}
Jialei Wang and Tong Zhang.
\newblock Utilizing second order information in minibatch stochastic variance
  reduced proximal iterations.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (42):\penalty0 1--56, 2019.

\bibitem[Wang et~al.(2018)Wang, Roosta-Khorasani, Xu, and
  Mahoney]{wang2018giant}
Shusen Wang, Farbod Roosta-Khorasani, Peng Xu, and Michael~W Mahoney.
\newblock {GIANT}: Globally improved approximate {N}ewton method for
  distributed optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2332--2342, 2018.

\bibitem[Xiao et~al.(2019)Xiao, Yu, Lin, and Chen]{dscovr2019}
Lin Xiao, Adams~Wei Yu, Qihang Lin, and Weizhu Chen.
\newblock {DSCOVR}: Randomized primal-dual block coordinate algorithms for
  asynchronous distributed optimization.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (43):\penalty0 1--58, 2019.

\bibitem[Yu et~al.(2010)Yu, Lo, Hsieh, Lou, McKenzie, Chou, Chung, Ho, Chang,
  Wei, et~al.]{yu2010feature}
Hsiang-Fu Yu, Hung-Yi Lo, Hsun-Ping Hsieh, Jing-Kai Lou, Todd~G McKenzie,
  Jung-Wei Chou, Po-Han Chung, Chia-Hua Ho, Chun-Fu Chang, Yin-Hsuan Wei,
  et~al.
\newblock Feature engineering and classifier ensemble for {KDD} cup 2010.
\newblock In \emph{KDD Cup}, 2010.

\bibitem[Yuan and Li(2019)]{yuan2019convergence}
Xiao-Tong Yuan and Ping Li.
\newblock On convergence of distributed approximate {N}ewton methods:
  Globalization, sharper bounds and beyond.
\newblock \emph{arXiv preprint arXiv:1908.02246}, 2019.

\bibitem[Zhang and Xiao(2015)]{zhang2015disco}
Yuchen Zhang and Lin Xiao.
\newblock {DiSCO}: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International Conference on Machine Learning}, pages
  362--370, 2015.

\bibitem[Zhang and Xiao(2018)]{ZhangXiao2018DiSCO}
Yuchen Zhang and Lin Xiao.
\newblock Communication-efficient distributed optimization of self-concordant
  empirical loss.
\newblock In \emph{Large-Scale and Distributed Optimization}, number 2227 in
  Lecture Notes in Mathematics, chapter~11, pages 289--341. Springer, 2018.

\end{thebibliography}
