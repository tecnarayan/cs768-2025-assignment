\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille \& Soatto(2018{\natexlab{a}})Achille and
  Soatto]{achille2018emergence}
Achille, A. and Soatto, S.
\newblock Emergence of invariance and disentanglement in deep representations.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1947--1980, 2018{\natexlab{a}}.

\bibitem[Achille \& Soatto(2018{\natexlab{b}})Achille and
  Soatto]{achille2018information}
Achille, A. and Soatto, S.
\newblock Information dropout: Learning optimal representations through noisy
  computation.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12):\penalty0 2897--2905, 2018{\natexlab{b}}.

\bibitem[Arora et~al.(2016)Arora, Basu, Mianjy, and
  Mukherjee]{arora2016understanding}
Arora, R., Basu, A., Mianjy, P., and Mukherjee, A.
\newblock Understanding deep neural networks with rectified linear units.
\newblock \emph{arXiv preprint arXiv:1611.01491}, 2016.

\bibitem[Blum \& Rivest(1989)Blum and Rivest]{blum1989training}
Blum, A. and Rivest, R.~L.
\newblock Training a 3-node neural network is np-complete.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  494--501, 1989.

\bibitem[Boob et~al.(2018)Boob, Dey, and Lan]{boob2018complexity}
Boob, D., Dey, S.~S., and Lan, G.
\newblock Complexity of training relu neural network.
\newblock \emph{arXiv preprint arXiv:1809.10787}, 2018.

\bibitem[Burgess et~al.(2017)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins, and Lerchner]{burgess2018understanding}
Burgess, C.~P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G.,
  and Lerchner, A.
\newblock Understanding disentangling in $\beta$-vae.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2017.

\bibitem[Chen et~al.(2018)Chen, Song, Wainwright, and Jordan]{chen2018learning}
Chen, J., Song, L., Wainwright, M., and Jordan, M.
\newblock Learning to explain: An information-theoretic perspective on model
  interpretation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  882--891, 2018.

\bibitem[Cortes et~al.(2017)Cortes, Gonzalvo, Kuznetsov, Mohri, and
  Yang]{cortes2017adanet}
Cortes, C., Gonzalvo, X., Kuznetsov, V., Mohri, M., and Yang, S.
\newblock Adanet: Adaptive structural learning of artificial neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  874--883. JMLR. org, 2017.

\bibitem[Dosovitskiy \& Brox(2016)Dosovitskiy and
  Brox]{dosovitskiy2016inverting}
Dosovitskiy, A. and Brox, T.
\newblock Inverting visual representations with convolutional networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4829--4837, 2016.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du2019implicit}
Du, Y. and Mordatch, I.
\newblock Implicit generation and modeling with energy based models.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pp.\  3608--3618. Curran Associates, Inc., 2019.

\bibitem[Everingham et~al.(2015)Everingham, Eslami, Van~Gool, Williams, Winn,
  and Zisserman]{everingham2015pascal}
Everingham, M., Eslami, S.~A., Van~Gool, L., Williams, C.~K., Winn, J., and
  Zisserman, A.
\newblock The pascal visual object classes challenge: A retrospective.
\newblock \emph{International journal of computer vision}, 111\penalty0
  (1):\penalty0 98--136, 2015.

\bibitem[Fong \& Vedaldi(2017)Fong and Vedaldi]{fong2017interpretable}
Fong, R.~C. and Vedaldi, A.
\newblock Interpretable explanations of black boxes by meaningful perturbation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  3429--3437, 2017.

\bibitem[Fort et~al.(2019)Fort, Nowak, and Narayanan]{fort2019stiffness}
Fort, S., Nowak, P.~K., and Narayanan, S.
\newblock Stiffness: A new perspective on generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:1901.09491}, 2019.

\bibitem[Gao et~al.(2018)Gao, Lu, Zhou, Zhu, and Nian~Wu]{gao2018learning}
Gao, R., Lu, Y., Zhou, J., Zhu, S.-C., and Nian~Wu, Y.
\newblock Learning generative convnets via multi-grid modeling and sampling.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  9155--9164, 2018.

\bibitem[Goldfeld et~al.(2019)Goldfeld, Van Den~Berg, Greenewald, Melnyk,
  Nguyen, Kingsbury, and Polyanskiy]{goldfeld2019estimating}
Goldfeld, Z., Van Den~Berg, E., Greenewald, K., Melnyk, I., Nguyen, N.,
  Kingsbury, B., and Polyanskiy, Y.
\newblock Estimating information flow in deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2299--2308, 2019.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pp.\  630--645.
  Springer, 2016{\natexlab{b}}.

\bibitem[Heckel \& Yilmaz(2020)Heckel and Yilmaz]{heckel2020early}
Heckel, R. and Yilmaz, F.~F.
\newblock Early stopping in deep networks: Double descent and how to eliminate
  it.
\newblock \emph{arXiv preprint arXiv:2007.10099}, 2020.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2017beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock \emph{ICLR}, 2\penalty0 (5):\penalty0 6, 2017.

\bibitem[Jastrzebski et~al.(2017)Jastrzebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{jastrzkebski2017three}
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y.,
  and Storkey, A.~J.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Kalimeris et~al.(2019)Kalimeris, Kaplun, Nakkiran, Edelman, Yang,
  Barak, and Zhang]{NIPS2019_8609}
Kalimeris, D., Kaplun, G., Nakkiran, P., Edelman, B., Yang, T., Barak, B., and
  Zhang, H.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  3496--3506. Curran Associates, Inc., 2019.

\bibitem[Kindermans et~al.(2017)Kindermans, Sch{\"u}tt, Alber, M{\"u}ller,
  Erhan, Kim, and D{\"a}hne]{kindermans2017learning}
Kindermans, P.-J., Sch{\"u}tt, K.~T., Alber, M., M{\"u}ller, K.-R., Erhan, D.,
  Kim, B., and D{\"a}hne, S.
\newblock Learning how to explain neural networks: Patternnet and
  patternattribution.
\newblock \emph{arXiv preprint arXiv:1705.05598}, 2017.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kolchinsky \& Tracey(2017)Kolchinsky and
  Tracey]{kolchinsky2017estimating}
Kolchinsky, A. and Tracey, B.~D.
\newblock Estimating mixture entropy with pairwise distances.
\newblock \emph{Entropy}, 19\penalty0 (7):\penalty0 361, 2017.

\bibitem[Kolchinsky et~al.(2019)Kolchinsky, Tracey, and
  Wolpert]{kolchinsky2019nonlinear}
Kolchinsky, A., Tracey, B.~D., and Wolpert, D.~H.
\newblock Nonlinear information bottleneck.
\newblock \emph{Entropy}, 21\penalty0 (12):\penalty0 1181, 2019.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{kornblith2019similarity}
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G.
\newblock Similarity of neural network representations revisited.
\newblock \emph{arXiv preprint arXiv:1905.00414}, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Le \& Yang(2015)Le and Yang]{tiny_imagenet}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7, 2015.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting structured data}, 1\penalty0 (0), 2006.

\bibitem[Liang et~al.(2019)Liang, Li, Li, and Zhang]{liang2019knowledge}
Liang, R., Li, T., Li, L., and Zhang, Q.
\newblock Knowledge consistency between neural networks and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liang et~al.(2017)Liang, Poggio, Rakhlin, and Stokes]{liang2017fisher}
Liang, T., Poggio, T., Rakhlin, A., and Stokes, J.
\newblock Fisher-rao metric, geometry, and complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.01530}, 2017.

\bibitem[Liu et~al.(2021)Liu, Wang, Ren, Wang, Yin, and Zhang]{liu2021trap}
Liu, D., Wang, S., Ren, J., Wang, K., Yin, S., and Zhang, Q.
\newblock Trap of feature diversity in the learning of mlps.
\newblock \emph{arXiv preprint arXiv:2112.00980}, 2021.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Liu, Z., Luo, P., Wang, X., and Tang, X.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  3730--3738, 2015.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
Livni, R., Shalev-Shwartz, S., and Shamir, O.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  855--863, 2014.

\bibitem[Lundberg \& Lee(2017)Lundberg and Lee]{lundberg2017unified}
Lundberg, S.~M. and Lee, S.-I.
\newblock A unified approach to interpreting model predictions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4765--4774, 2017.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Mahendran \& Vedaldi(2015)Mahendran and
  Vedaldi]{mahendran2015understanding}
Mahendran, A. and Vedaldi, A.
\newblock Understanding deep image representations by inverting them.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  5188--5196, 2015.

\bibitem[Manurangsi \& Reichman(2018)Manurangsi and
  Reichman]{manurangsi2018computational}
Manurangsi, P. and Reichman, D.
\newblock The computational complexity of training relu (s).
\newblock \emph{arXiv preprint arXiv:1810.04207}, 2018.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2019deep}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{arXiv preprint arXiv:1912.02292}, 2019.

\bibitem[Noshad et~al.(2019)Noshad, Zeng, and Hero]{noshad2019scalable}
Noshad, M., Zeng, Y., and Hero, A.~O.
\newblock Scalable mutual information estimation using dependence graphs.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  2962--2966. IEEE, 2019.

\bibitem[Pang et~al.(2020)Pang, Han, Nijkamp, Zhu, and Wu]{pang2020learning}
Pang, B., Han, T., Nijkamp, E., Zhu, S.-C., and Wu, Y.~N.
\newblock Learning latent space energy-based prior model.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Pascanu et~al.(2013)Pascanu, Gulcehre, Cho, and
  Bengio]{pascanu2013construct}
Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y.
\newblock How to construct deep recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6026}, 2013.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32, pp.\  8026--8037. Curran
  Associates, Inc., 2019.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Dickstein]{raghu2017expressive}
Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Dickstein, J.~S.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2847--2854. JMLR. org, 2017.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and
  Le]{ramachandran2017searching}
Ramachandran, P., Zoph, B., and Le, Q.~V.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941}, 2017.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
Ribeiro, M.~T., Singh, S., and Guestrin, C.
\newblock "why should {I} trust you?": Explaining the predictions of any
  classifier.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pp.\  1135--1144. ACM, 2016.

\bibitem[Saxe et~al.(2019)Saxe, Bansal, Dapello, Advani, Kolchinsky, Tracey,
  and Cox]{saxe2019information}
Saxe, A.~M., Bansal, Y., Dapello, J., Advani, M., Kolchinsky, A., Tracey,
  B.~D., and Cox, D.~D.
\newblock On the information bottleneck theory of deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124020, 2019.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{selvaraju2017grad}
Selvaraju, R.~R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra,
  D.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  618--626, 2017.

\bibitem[Shwartz-Ziv \& Tishby(2017)Shwartz-Ziv and Tishby]{shwartz2017opening}
Shwartz-Ziv, R. and Tishby, N.
\newblock Opening the black box of deep neural networks via information.
\newblock \emph{arXiv preprint arXiv:1703.00810}, 2017.

\bibitem[Simonyan et~al.(2017)Simonyan, Vedaldi, and
  Zisserman]{simonyan2017deep}
Simonyan, K., Vedaldi, A., and Zisserman, A.
\newblock Deep inside convolutional networks: visualising image classification
  models and saliency maps.
\newblock \emph{arXiv preprint arXiv:1312.6034}, 2017.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
Tishby, N., Pereira, F.~C., and Bialek, W.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem[Ver~Steeg \& Galstyan(2015)Ver~Steeg and Galstyan]{ver2015maximally}
Ver~Steeg, G. and Galstyan, A.
\newblock Maximally informative hierarchical representations of
  high-dimensional data.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1004--1012,
  2015.

\bibitem[Wang et~al.(2020)Wang, Ren, Lin, Zhu, Wang, and
  Zhang]{wang2020unified}
Wang, X., Ren, J., Lin, S., Zhu, X., Wang, Y., and Zhang, Q.
\newblock A unified approach to interpreting and boosting adversarial
  transferability.
\newblock \emph{arXiv preprint arXiv:2010.04055}, 2020.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Yi, Su, Gao, Hsieh, and
  Daniel]{weng2018evaluating}
Weng, T.-W., Zhang, H., Chen, P.-Y., Yi, J., Su, D., Gao, Y., Hsieh, C.-J., and
  Daniel, L.
\newblock Evaluating the robustness of neural networks: An extreme value theory
  approach.
\newblock \emph{arXiv preprint arXiv:1801.10578}, 2018.

\bibitem[Wolchover(2017)]{wolchover2017new}
Wolchover, N.
\newblock New theory cracks open the black box of deep learning.
\newblock \emph{In {Quanta Magazine}}, 2017.

\bibitem[Xu \& Raginsky(2017)Xu and Raginsky]{xu2017information}
Xu, A. and Raginsky, M.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2524--2533, 2017.

\bibitem[Xu(2018)]{xu2018understanding}
Xu, Z.~J.
\newblock Understanding training and generalization in deep learning by fourier
  analysis.
\newblock \emph{arXiv preprint arXiv:1808.04295}, 2018.

\bibitem[Yosinski et~al.(2015)Yosinski, Clune, Nguyen, Fuchs, and
  Lipson]{yosinski2015understanding}
Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., and Lipson, H.
\newblock Understanding neural networks through deep visualization.
\newblock \emph{arXiv preprint arXiv:1506.06579}, 2015.

\bibitem[Zeiler \& Fergus(2014)Zeiler and Fergus]{zeiler2014visualizing}
Zeiler, M.~D. and Fergus, R.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European conference on computer vision}, pp.\  818--833.
  Springer, 2014.

\bibitem[Zhang et~al.(2016)Zhang, Wu, Che, Lin, Memisevic, Salakhutdinov, and
  Bengio]{zhang2016architectural}
Zhang, S., Wu, Y., Che, T., Lin, Z., Memisevic, R., Salakhutdinov, R.~R., and
  Bengio, Y.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1822--1830, 2016.

\bibitem[Zhou et~al.(2015)Zhou, Khosla, Lapedriza, Oliva, and
  Torralba]{zhou2014object}
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A.
\newblock Object detectors emerge in deep scene cnns.
\newblock \emph{In {ICLR}}, 2015.

\bibitem[Zhou et~al.(2016)Zhou, Khosla, Lapedriza, Oliva, and
  Torralba]{zhou2016learning}
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A.
\newblock Learning deep features for discriminative localization.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2921--2929, 2016.

\end{thebibliography}
