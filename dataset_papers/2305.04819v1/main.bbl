\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
A.~Agarwal, S.~M. Kakade, J.~D. Lee, and G.~Mahajan.
\newblock Optimality and approximation with policy gradient methods in {M}arkov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 64--66. PMLR, 2020.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
A.~Antos, C.~Szepesv{\'a}ri, and R.~Munos.
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Branavan et~al.(2009)Branavan, Chen, Zettlemoyer, and
  Barzilay]{branavan2009reinforcement}
S.~R.~K. Branavan, H.~Chen, L.~S. Zettlemoyer, and R.~Barzilay.
\newblock Reinforcement learning for mapping instructions to actions.
\newblock In \emph{Proceedings of the Joint Conference of the 47th Annual
  Meeting of the ACL and the 4th International Joint Conference on Natural
  Language Processing of the AFNLP: Volume 1 - Volume 1}, ACL '09, page
  82â€“90, USA, 2009. Association for Computational Linguistics.

\bibitem[Brown and Sandholm(2018)]{brown2018superhuman}
N.~Brown and T.~Sandholm.
\newblock Superhuman {AI} for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock \emph{Science}, 359\penalty0 (6374):\penalty0 418--424, 2018.

\bibitem[Cen et~al.(2021)Cen, Wei, and Chi]{cen2021fast}
S.~Cen, Y.~Wei, and Y.~Chi.
\newblock Fast policy extragradient methods for competitive games with entropy
  regularization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  27952--27964. Curran Associates, Inc., 2021.

\bibitem[Cen et~al.(2022)Cen, Chi, Du, and Xiao]{cen2022faster}
S.~Cen, Y.~Chi, S.~S. Du, and L.~Xiao.
\newblock Faster last-iterate convergence of policy optimization in zero-sum
  {M}arkov games.
\newblock \emph{arXiv preprint arXiv:2210.01050}, 2022.

\bibitem[Chen and Jiang(2019)]{chen2019information}
J.~Chen and N.~Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1042--1051. PMLR, 2019.

\bibitem[Daskalakis et~al.(2020)Daskalakis, Foster, and
  Golowich]{daskalakis2020independent}
C.~Daskalakis, D.~J. Foster, and N.~Golowich.
\newblock Independent policy gradient methods for competitive reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 5527--5540, 2020.

\bibitem[de~Witt et~al.(2020)de~Witt, Gupta, Makoviichuk, Makoviychuk, Torr,
  Sun, and Whiteson]{de2020independent}
C.~S. de~Witt, T.~Gupta, D.~Makoviichuk, V.~Makoviychuk, P.~H. Torr, M.~Sun,
  and S.~Whiteson.
\newblock Is independent learning all you need in the {S}tar{C}raft multi-agent
  challenge?
\newblock \emph{arXiv preprint arXiv:2011.09533}, 2020.

\bibitem[Ding et~al.(2022)Ding, Wei, Zhang, and Jovanovic]{ding2022independent}
D.~Ding, C.-Y. Wei, K.~Zhang, and M.~Jovanovic.
\newblock Independent policy gradient for large-scale {M}arkov potential games:
  Sharper rates, function approximation, and game-agnostic convergence.
\newblock In \emph{International Conference on Machine Learning}, pages
  5166--5220. PMLR, 2022.

\bibitem[Du et~al.(2019)Du, Kakade, Wang, and Yang]{du2019good}
S.~S. Du, S.~M. Kakade, R.~Wang, and L.~F. Yang.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1910.03016}, 2019.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmarking}
Y.~Duan, X.~Chen, R.~Houthooft, J.~Schulman, and P.~Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International conference on machine learning}, pages
  1329--1338. PMLR, 2016.

\bibitem[Fan et~al.(2020)Fan, Wang, Xie, and Yang]{fan2020theoretical}
J.~Fan, Z.~Wang, Y.~Xie, and Z.~Yang.
\newblock A theoretical analysis of deep {Q}-learning.
\newblock In \emph{Learning for Dynamics and Control}, pages 486--489. PMLR,
  2020.

\bibitem[Farahmand et~al.(2010)Farahmand, Szepesv{\'a}ri, and
  Munos]{farahmand2010error}
A.-m. Farahmand, C.~Szepesv{\'a}ri, and R.~Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock \emph{Advances in Neural Information Processing Systems}, 23, 2010.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and
  Whiteson]{foerster2018counterfactual}
J.~Foerster, G.~Farquhar, T.~Afouras, N.~Nardelli, and S.~Whiteson.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem[Fox et~al.(2022)Fox, Mcaleer, Overman, and
  Panageas]{fox2022independent}
R.~Fox, S.~M. Mcaleer, W.~Overman, and I.~Panageas.
\newblock Independent natural policy gradient always converges in markov
  potential games.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4414--4425. PMLR, 2022.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
S.~Fujimoto, H.~Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pages
  1587--1596. PMLR, 2018.

\bibitem[Gimpel and Smith(2010)]{gimpel2010softmax}
K.~Gimpel and N.~A. Smith.
\newblock Softmax-margin {CRF}s: {T}raining log-linear models with cost
  functions.
\newblock In \emph{Human Language Technologies: The 2010 Annual Conference of
  the North American Chapter of the Association for Computational Linguistics},
  pages 733--736, 2010.

\bibitem[Guo et~al.(2016)Guo, Singh, Lewis, and Lee]{guo2016deep}
X.~Guo, S.~Singh, R.~Lewis, and H.~Lee.
\newblock Deep learning for reward design to improve {M}onte {C}arlo tree
  search in {A}tari games.
\newblock \emph{arXiv preprint arXiv:1604.07095}, 2016.

\bibitem[Heess et~al.(2013)Heess, Silver, and Teh]{heess2013actor}
N.~Heess, D.~Silver, and Y.~W. Teh.
\newblock Actor-critic reinforcement learning with energy-based policies.
\newblock In \emph{European Workshop on Reinforcement Learning}, pages 45--58.
  PMLR, 2013.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
C.~Jin, R.~Ge, P.~Netrapalli, S.~M. Kakade, and M.~I. Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{International conference on machine learning}, pages
  1724--1732. PMLR, 2017.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Y.~Jin, Z.~Yang, and Z.~Wang.
\newblock Is pessimism provably efficient for offline {RL}?
\newblock In \emph{International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
S.~Kakade and J.~Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kakade(2001)]{kakade2001natural}
S.~M. Kakade.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems}, 14, 2001.

\bibitem[Kakade(2003)]{kakade2003sample}
S.~M. Kakade.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock University of London, University College London (United Kingdom),
  2003.

\bibitem[Kearns and Singh(2002)]{kearns2002near}
M.~Kearns and S.~Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 209--232, 2002.

\bibitem[Kraemer and Banerjee(2016)]{kraemer2016multi}
L.~Kraemer and B.~Banerjee.
\newblock Multi-agent reinforcement learning as a rehearsal for decentralized
  planning.
\newblock \emph{Neurocomputing}, 190:\penalty0 82--94, 2016.

\bibitem[Kuba et~al.(2022)Kuba, Chen, Wen, Wen, Sun, Wang, and
  Yang]{kuba2022trust}
J.~G. Kuba, R.~Chen, M.~Wen, Y.~Wen, F.~Sun, J.~Wang, and Y.~Yang.
\newblock Trust region policy optimisation in multi-agent reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=EcGGFkNTxdJ}.

\bibitem[Laskin et~al.(2020)Laskin, Lee, Stooke, Pinto, Abbeel, and
  Srinivas]{laskin2020reinforcement}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 19884--19895, 2020.

\bibitem[Lee et~al.(2020)Lee, Fischer, Liu, Guo, Lee, Canny, and
  Guadarrama]{lee2020predictive}
K.-H. Lee, I.~Fischer, A.~Liu, Y.~Guo, H.~Lee, J.~Canny, and S.~Guadarrama.
\newblock Predictive information accelerates learning in {RL}.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11890--11901, 2020.

\bibitem[Leonardos et~al.(2022)Leonardos, Overman, Panageas, and
  Piliouras]{leonardos2022global}
S.~Leonardos, W.~Overman, I.~Panageas, and G.~Piliouras.
\newblock Global convergence of multi-agent policy gradient in markov potential
  games.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=gfwON7rAm4}.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li2020breaking}
G.~Li, Y.~Wei, Y.~Chi, Y.~Gu, and Y.~Chen.
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 12861--12872, 2020.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Littman(1994)]{littman1994markov}
M.~L. Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings 1994}, pages 157--163.
  Elsevier, 1994.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
B.~Liu, Q.~Cai, Z.~Yang, and Z.~Wang.
\newblock Neural trust region/proximal policy optimization attains globally
  optimal policy.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Y.~Liu, A.~Swaminathan, A.~Agarwal, and E.~Brunskill.
\newblock Provably good batch off-policy reinforcement learning without great
  exploration.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1264--1274, 2020.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Pieter~Abbeel, and
  Mordatch]{lowe2017multi}
R.~Lowe, Y.~I. Wu, A.~Tamar, J.~Harb, O.~Pieter~Abbeel, and I.~Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Moskovitz et~al.(2021)Moskovitz, Parker-Holder, Pacchiano, Arbel, and
  Jordan]{moskovitz2021tactical}
T.~Moskovitz, J.~Parker-Holder, A.~Pacchiano, M.~Arbel, and M.~Jordan.
\newblock Tactical optimism and pessimism for deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12849--12863, 2021.

\bibitem[Munos(2003)]{munos2003}
R.~Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{International Conference on Machine Learning}, page
  560â€“567, 2003.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R.~Munos and C.~Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: {A} basic
  course}, volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Papoudakis et~al.(2021)Papoudakis, Christianos, Sch{\"a}fer, and
  Albrecht]{papoudakis1benchmarking}
G.~Papoudakis, F.~Christianos, L.~Sch{\"a}fer, and S.~V. Albrecht.
\newblock Benchmarking multi-agent deep reinforcement learning algorithms in
  cooperative tasks.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021.

\bibitem[Perolat et~al.(2015)Perolat, Scherrer, Piot, and
  Pietquin]{perolat2015approximate}
J.~Perolat, B.~Scherrer, B.~Piot, and O.~Pietquin.
\newblock Approximate dynamic programming for two-player zero-sum {M}arkov
  games.
\newblock In \emph{International Conference on Machine Learning}, pages
  1321--1329, 2015.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
P.~Rashidinejad, B.~Zhu, C.~Ma, J.~Jiao, and S.~Russell.
\newblock Bridging offline reinforcement learning and imitation learning: {A}
  tale of pessimism.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11702--11716, 2021.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: {F}rom theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shani et~al.(2020)Shani, Efroni, and Mannor]{shani2020adaptive}
L.~Shani, Y.~Efroni, and S.~Mannor.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized {MDP}s.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 5668--5675, 2020.

\bibitem[Shapley(1953)]{shapley1953stochastic}
L.~S. Shapley.
\newblock Stochastic games.
\newblock \emph{Proceedings of the national academy of sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.

\bibitem[Sidford et~al.(2018)Sidford, Wang, Wu, Yang, and Ye]{sidford2018near}
A.~Sidford, M.~Wang, X.~Wu, L.~F. Yang, and Y.~Ye.
\newblock Near-optimal time and sample complexities for solving discounted
  markov decision process with a generative model.
\newblock \emph{arXiv preprint arXiv:1806.01492}, 2018.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of {G}o without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Thrun and Schwartz(1993)]{thrun1993issues}
S.~Thrun and A.~Schwartz.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In \emph{Proceedings of the Fourth Connectionist Models Summer
  School}, volume 255, page 263. Hillsdale, NJ, 1993.

\bibitem[Tian et~al.(2019)Tian, Ma, Gong, Sengupta, Chen, Pinkerton, and
  Zitnick]{tian2019elf}
Y.~Tian, J.~Ma, Q.~Gong, S.~Sengupta, Z.~Chen, J.~Pinkerton, and L.~Zitnick.
\newblock Elf opengo: An analysis and open reimplementation of {A}lpha{Z}ero.
\newblock In \emph{International Conference on Machine Learning}, pages
  6244--6253. PMLR, 2019.

\bibitem[Uehara and Sun(2021)]{uehara2021pessimistic}
M.~Uehara and W.~Sun.
\newblock Pessimistic model-based offline {RL}: {P}ac bounds and posterior
  sampling under partial coverage.
\newblock \emph{arXiv e-prints}, pages arXiv--2107, 2021.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in {S}tar{C}raft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wen et~al.(2019)Wen, Yang, Luo, Wang, and Pan]{wen2019probabilistic}
Y.~Wen, Y.~Yang, R.~Luo, J.~Wang, and W.~Pan.
\newblock Probabilistic recursive reasoning for multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1901.09207}, 2019.

\bibitem[Xie and Jiang(2020)]{xie2020q}
T.~Xie and N.~Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: {A}
  theoretical comparison.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  550--559. PMLR, 2020.

\bibitem[Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
T.~Xie, C.-A. Cheng, N.~Jiang, P.~Mineiro, and A.~Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 6683--6694, 2021.

\bibitem[Yang et~al.(2018)Yang, Luo, Li, Zhou, Zhang, and Wang]{yang2018mean}
Y.~Yang, R.~Luo, M.~Li, M.~Zhou, W.~Zhang, and J.~Wang.
\newblock Mean field multi-agent reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  5571--5580. PMLR, 2018.

\bibitem[Yu et~al.(2021)Yu, Velu, Vinitsky, Wang, Bayen, and
  Wu]{yu2021surprising}
C.~Yu, A.~Velu, E.~Vinitsky, Y.~Wang, A.~Bayen, and Y.~Wu.
\newblock The surprising effectiveness of {PPO} in cooperative, multi-agent
  games.
\newblock \emph{arXiv preprint arXiv:2103.01955}, 2021.

\bibitem[Zhan et~al.(2022)Zhan, Huang, Huang, Jiang, and Lee]{zhan2022offline}
W.~Zhan, B.~Huang, A.~Huang, N.~Jiang, and J.~Lee.
\newblock Offline reinforcement learning with realizability and single-policy
  concentrability.
\newblock In \emph{Conference on Learning Theory}, pages 2730--2775. PMLR,
  2022.

\bibitem[Zhang et~al.(2020)Zhang, Chen, Huang, Li, Yang, Zhang, and
  Wang]{zhang2020bi}
H.~Zhang, W.~Chen, Z.~Huang, M.~Li, Y.~Yang, W.~Zhang, and J.~Wang.
\newblock Bi-level actor-critic for multi-agent coordination.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 7325--7332, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Yang, and Ba{\c{s}}ar]{zhang2021multi}
K.~Zhang, Z.~Yang, and T.~Ba{\c{s}}ar.
\newblock Multi-agent reinforcement learning: {A} selective overview of
  theories and algorithms.
\newblock \emph{Handbook of Reinforcement Learning and Control}, pages
  321--384, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Tian, Lee, and Du]{zhao2022provably}
Y.~Zhao, Y.~Tian, J.~Lee, and S.~Du.
\newblock Provably efficient policy optimization for two-player zero-sum
  {M}arkov games.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2736--2761. PMLR, 2022.

\end{thebibliography}
