\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{almazrouei2023falcon}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al.
\newblock The falcon series of open language models.
\newblock {\em arXiv preprint arXiv:2311.16867}, 2023.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In {\em International Conference on Machine Learning}, pages 2397--2430. PMLR, 2023.

\bibitem{Bisk2020}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In {\em Thirty-Fourth AAAI Conference on Artificial Intelligence}, 2020.

\bibitem{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock {\em arXiv preprint arXiv:2204.06745}, 2022.

\bibitem{bloom1970space}
Burton~H Bloom.
\newblock Space/time trade-offs in hash coding with allowable errors.
\newblock {\em Communications of the ACM}, 13(7):422--426, 1970.

\bibitem{bommasani2023foundation}
Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang, and Percy Liang.
\newblock The foundation model transparency index.
\newblock {\em arXiv preprint arXiv:2310.12941}, 2023.

\bibitem{bommasani2023holistic}
Rishi Bommasani, Percy Liang, and Tony Lee.
\newblock Holistic evaluation of language models.
\newblock {\em Annals of the New York Academy of Sciences}, 1525(1):140--146, 2023.

\bibitem{broder1997resemblance}
Andrei~Z Broder.
\newblock On the resemblance and containment of documents.
\newblock In {\em Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)}, pages 21--29. IEEE, 1997.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em Journal of Machine Learning Research}, 24(240):1--113, 2023.

\bibitem{Clark2018ThinkYH}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock {\em ArXiv}, abs/1803.05457, 2018.

\bibitem{diederik2014adam}
P~Kingma Diederik.
\newblock Adam: A method for stochastic optimization.
\newblock 2014.

\bibitem{dodge2021documenting}
Jesse Dodge, Maarten Sap, Ana Marasovi{\'c}, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner.
\newblock Documenting large webtext corpora: A case study on the colossal clean crawled corpus.
\newblock {\em arXiv preprint arXiv:2104.08758}, 2021.

\bibitem{du2022glam}
Nan Du, Yanping Huang, Andrew~M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In {\em International Conference on Machine Learning}, pages 5547--5569. PMLR, 2022.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 07 2024.

\bibitem{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock {\em arXiv preprint arXiv:2402.00838}, 2024.

\bibitem{hendryckstest2021}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock {\em Proceedings of the International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock {\em arXiv preprint arXiv:1904.09751}, 2019.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{jin2019pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu.
\newblock Pubmedqa: A dataset for biomedical research question answering.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2567--2577, 2019.

\bibitem{joulin2017bag}
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov.
\newblock Bag of tricks for efficient text classification.
\newblock In {\em Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers}, pages 427--431. Association for Computational Linguistics, April 2017.

\bibitem{Kocetkov2022TheStack}
Denis Kocetkov, Raymond Li, Loubna Ben~Allal, Jia Li, Chenghao Mou, Carlos Muñoz~Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de~Vries.
\newblock The stack: 3 tb of permissively licensed source code.
\newblock {\em Preprint}, 2022.

\bibitem{laurenccon2022bigscience}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova~del Moral, Teven Le~Scao, Leandro Von~Werra, Chenghao Mou, Eduardo Gonz{\'a}lez~Ponferrada, Huu Nguyen, et~al.
\newblock The bigscience roots corpus: A 1.6 tb composite multilingual dataset.
\newblock {\em Advances in Neural Information Processing Systems}, 35:31809--31826, 2022.

\bibitem{le2023bloom}
Teven Le~Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock 2023.

\bibitem{lee2021deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock {\em arXiv preprint arXiv:2107.06499}, 2021.

\bibitem{lewkowycz2022solving}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:3843--3857, 2022.

\bibitem{li2024datacomp}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et~al.
\newblock Datacomp-lm: In search of the next generation of training sets for language models.
\newblock {\em arXiv preprint arXiv:2406.11794}, 2024.

\bibitem{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock {\em arXiv preprint arXiv:2305.06161}, 2023.

\bibitem{li2023towards}
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.
\newblock Towards general text embeddings with multi-stage contrastive learning.
\newblock {\em arXiv preprint arXiv:2308.03281}, 2023.

\bibitem{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods, 2021.

\bibitem{longpre2023pretrainer}
Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et~al.
\newblock A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, \& toxicity.
\newblock {\em arXiv preprint arXiv:2305.13169}, 2023.

\bibitem{magnusson2023paloma}
Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya~Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan~Pete Walsh, Yanai Elazar, Kyle Lo, et~al.
\newblock Paloma: A benchmark for evaluating language model fit.
\newblock {\em arXiv preprint arXiv:2312.10523}, 2023.

\bibitem{mehta2024openelm}
Sachin Mehta, Mohammad~Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et~al.
\newblock Openelm: An efficient language model family with open-source training and inference framework.
\newblock {\em arXiv preprint arXiv:2404.14619}, 2024.

\bibitem{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et~al.
\newblock Mixed precision training.
\newblock {\em arXiv preprint arXiv:1710.03740}, 2017.

\bibitem{OpenBookQA2018}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In {\em EMNLP}, 2018.

\bibitem{naturalinstructions}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing instructions.
\newblock In {\em ACL}, 2022.

\bibitem{nie2019adversarial}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.
\newblock Adversarial nli: A new benchmark for natural language understanding.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}. Association for Computational Linguistics, 2020.

\bibitem{Nomic2024}
Nomic.
\newblock Structure unstructured data: Interact, discover insights and build with unstructured text, image and audio data., 2024.
\newblock Accessed: 2024-06-12.

\bibitem{paperno2016lambada}
Denis Paperno, Germ\'{a}n Kruszewski, Angeliki Lazaridou, Ngoc~Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1525--1534, Berlin, Germany, August 2016. Association for Computational Linguistics.

\bibitem{penedo2024fineweb}
Guilherme Penedo, Hynek Kydl{\'\i}{\v{c}}ek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von~Werra, Thomas Wolf, et~al.
\newblock The fineweb datasets: Decanting the web for the finest text data at scale.
\newblock {\em arXiv preprint arXiv:2406.17557}, 2024.

\bibitem{penedo2024refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D Manning.
\newblock Coqa: A conversational question answering challenge.
\newblock {\em Transactions of the Association for Computational Linguistics}, 7:249--266, 2019.

\bibitem{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: an adversarial winograd schema challenge at scale.
\newblock {\em Commun. ACM}, 64(9):99–106, aug 2021.

\bibitem{sanh2021multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, Manan Dey, M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization, 2021.

\bibitem{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock {\em arXiv preprint arXiv:1904.09728}, 2019.

\bibitem{shen2023slimpajama}
Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing.
\newblock Slimpajama-dc: Understanding data combinations for llm training.
\newblock {\em arXiv preprint arXiv:2309.10818}, 2023.

\bibitem{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al.
\newblock Dolma: An open corpus of three trillion tokens for language model pretraining research.
\newblock {\em arXiv preprint arXiv:2402.00159}, 2024.

\bibitem{MosaicML2023Introducing}
MosaicML~NLP Team.
\newblock Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.
\newblock Accessed: 2023-05-05.

\bibitem{SnowflakeArctic2023}
Snowflake AI~Research Team.
\newblock Snowflake arctic: The best llm for enterprise ai — efficiently intelligent, truly open, 2023.
\newblock Accessed: 2024-05-27.

\bibitem{tirumala2024d4}
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos.
\newblock D4: Improving llm pretraining via document de-duplication and diversification.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{tokpanov2024zyda}
Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim, James Whittington, and Quentin Anthony.
\newblock Zyda: A 1.3t dataset for open language modeling, 2024.

\bibitem{touvron2023allama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023bllama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{SciQ}
Johannes Welbl, Nelson~F Liu, and Matt Gardner.
\newblock Crowdsourcing multiple choice science questions.
\newblock {\em arXiv preprint arXiv:1707.06209}, 2017.

\bibitem{wenzek2019ccnet}
Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm{\'a}n, Armand Joulin, and Edouard Grave.
\newblock Ccnet: Extracting high quality monolingual datasets from web crawl data.
\newblock {\em arXiv preprint arXiv:1911.00359}, 2019.

\bibitem{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang.
\newblock Data selection for language models via importance resampling.
\newblock {\em Advances in Neural Information Processing Systems}, 36:34201--34227, 2023.

\bibitem{xue-etal-2021-mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel.
\newblock m{T}5: A massively multilingual pre-trained text-to-text transformer.
\newblock In {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 483--498, Online, June 2021. Association for Computational Linguistics.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem{zhang2024map}
Ge~Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou~Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et~al.
\newblock Map-neo: Highly capable and transparent bilingual large language model series.
\newblock {\em arXiv preprint arXiv:2405.19327}, 2024.

\bibitem{zhao2023pytorch}
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et~al.
\newblock Pytorch fsdp: experiences on scaling fully sharded data parallel.
\newblock {\em arXiv preprint arXiv:2304.11277}, 2023.

\end{thebibliography}
