\begin{thebibliography}{10}

\bibitem{dym_2}
T.~Amir, S.~J. Gortler, I.~Avni, R.~Ravina, and N.~Dym.
\newblock Neural injective functions for multisets, measures and graphs via a finite witness theorem.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{baek2021accurate}
J.~Baek, M.~Kang, and S.~J. Hwang.
\newblock Accurate learning of graph representations with graph multiset pooling.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{bevilacqua2022equivariant}
B.~Bevilacqua, F.~Frasca, D.~Lim, B.~Srinivasan, C.~Cai, G.~Balamurugan, M.~M. Bronstein, and H.~Maron.
\newblock Equivariant subgraph aggregation networks.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{gnns_in_sciences_example_3}
J.~Bradshaw, M.~J. Kusner, B.~Paige, M.~H.~S. Segler, and J.~M. Hern{\'{a}}ndez{-}Lobato.
\newblock A generative model for electron paths.
\newblock In {\em 7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem{bresson2017residual}
X.~Bresson and T.~Laurent.
\newblock Residual gated graph convnets.
\newblock {\em arXiv preprint arXiv:1711.07553}, 2017.

\bibitem{brody2022how}
S.~Brody, U.~Alon, and E.~Yahav.
\newblock How attentive are graph attention networks?
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{buterez2022graph}
D.~Buterez, J.~P. Janet, S.~J. Kiddle, D.~Oglic, and P.~Li{\`o}.
\newblock Graph neural networks with adaptive readouts.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{cahill2024bilipschitz}
J.~Cahill, J.~W. Iverson, and D.~G. Mixon.
\newblock Towards a bilipschitz invariant theory, 2024.

\bibitem{chiang2019cluster}
W.-L. Chiang, X.~Liu, S.~Si, Y.~Li, S.~Bengio, and C.-J. Hsieh.
\newblock Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks.
\newblock In {\em Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \& data mining}, pages 257--266, 2019.

\bibitem{regularizing}
E.~Cohen-Karlik, A.~B. David, and A.~Globerson.
\newblock Regularizing towards permutation invariance in recurrent models.
\newblock In {\em Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc.

\bibitem{corso2020principal}
G.~Corso, L.~Cavalleri, D.~Beaini, P.~Li{\`o}, and P.~Veli{\v{c}}kovi{\'c}.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock {\em Advances in Neural Information Processing Systems}, 33:13260--13271, 2020.

\bibitem{pna}
G.~Corso, L.~Cavalleri, D.~Beaini, P.~Li\`{o}, and P.~Veli\v{c}kovi\'{c}.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{chebyshev}
M.~Defferrard, X.~Bresson, and P.~Vandergheynst.
\newblock Convolutional neural networks on graphs with fast localized spectral filtering.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~29. Curran Associates, Inc., 2016.

\bibitem{gnns_in_sciences_example_1}
D.~Duvenaud, D.~Maclaurin, J.~Aguilera{-}Iparraguirre, R.~G{\'{o}}mez{-}Bombarelli, T.~Hirzel, A.~Aspuru{-}Guzik, and R.~P. Adams.
\newblock Convolutional networks on graphs for learning molecular fingerprints.
\newblock {\em CoRR}, abs/1509.09292, 2015.

\bibitem{dwivedi2023benchmarking}
V.~P. Dwivedi, C.~K. Joshi, A.~T. Luu, T.~Laurent, Y.~Bengio, and X.~Bresson.
\newblock Benchmarking graph neural networks.
\newblock {\em Journal of Machine Learning Research}, 24(43):1--48, 2023.

\bibitem{dwivedi2022long}
V.~P. Dwivedi, L.~Ramp{\'a}{\v{s}}ek, M.~Galkin, A.~Parviz, G.~Wolf, A.~T. Luu, and D.~Beaini.
\newblock Long range graph benchmark.
\newblock {\em Advances in Neural Information Processing Systems}, 35:22326--22340, 2022.

\bibitem{dym_1}
N.~Dym and S.~J. Gortler.
\newblock Low-dimensional invariant embeddings for universal geometric learning.
\newblock {\em Foundations of Computational Mathematics}, pages 1--41, 2024.

\bibitem{social_networks_example_1}
W.~Fan, Y.~Ma, Q.~Li, J.~Wang, G.~Cai, J.~Tang, and D.~Yin.
\newblock A graph neural network framework for social recommendations.
\newblock {\em {IEEE} Trans. Knowl. Data Eng.}, 34(5):2033--2047, 2022.

\bibitem{gomez2018automatic}
R.~G{\'o}mez-Bombarelli, J.~N. Wei, D.~Duvenaud, J.~M. Hern{\'a}ndez-Lobato, B.~S{\'a}nchez-Lengeling, D.~Sheberla, J.~Aguilera-Iparraguirre, T.~D. Hirzel, R.~P. Adams, and A.~Aspuru-Guzik.
\newblock Automatic chemical design using a data-driven continuous representation of molecules.
\newblock {\em ACS central science}, 4(2):268--276, 2018.

\bibitem{lstm}
W.~L. Hamilton, R.~Ying, and J.~Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock In {\em Proceedings of the 31st International Conference on Neural Information Processing Systems}, NIPS'17, page 1025â€“1035, Red Hook, NY, USA, 2017. Curran Associates Inc.

\bibitem{hu2020open}
W.~Hu, M.~Fey, M.~Zitnik, Y.~Dong, H.~Ren, B.~Liu, M.~Catasta, and J.~Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock {\em Advances in neural information processing systems}, 33:22118--22133, 2020.

\bibitem{zinc}
J.~Irwin, T.~Sterling, M.~Mysinger, E.~Bolstad, and R.~Coleman.
\newblock Zinc: A free tool to discover chemistry for biology.
\newblock {\em Journal of chemical information and modeling}, 52, 05 2012.

\bibitem{gnns_in_sciences_example_2}
W.~Jin, K.~Yang, R.~Barzilay, and T.~S. Jaakkola.
\newblock Learning multimodal graph-to-graph translation for molecule optimization.
\newblock In {\em 7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem{gcn}
T.~N. Kipf and M.~Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{kortvelesy2023generalised}
R.~Kortvelesy, S.~Morad, and A.~Prorok.
\newblock Generalised f-mean aggregation for graph neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 36:34439--34450, 2023.

\bibitem{gnns_for_cv_example_1}
E.~Kosman and D.~D. Castro.
\newblock Graphvid: It only takes a few nodes to understand a video.
\newblock In S.~Avidan, G.~J. Brostow, M.~Ciss{\'{e}}, G.~M. Farinella, and T.~Hassner, editors, {\em Computer Vision - {ECCV} 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part {XXXV}}, volume 13695 of {\em Lecture Notes in Computer Science}, pages 195--212. Springer, 2022.

\bibitem{li2020deepergcn}
G.~Li, C.~Xiong, A.~Thabet, and B.~Ghanem.
\newblock Deepergcn: All you need to train deeper gcns.
\newblock {\em arXiv preprint arXiv:2006.07739}, 2020.

\bibitem{gnns_for_nlp_example_2}
X.~Liu, X.~You, X.~Zhang, J.~Wu, and P.~Lv.
\newblock Tensor graph convolutional networks for text classification.
\newblock In {\em The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pages 8409--8416. {AAAI} Press, 2020.

\bibitem{locatello2020object}
F.~Locatello, D.~Weissenborn, T.~Unterthiner, A.~Mahendran, G.~Heigold, J.~Uszkoreit, A.~Dosovitskiy, and T.~Kipf.
\newblock Object-centric learning with slot attention.
\newblock {\em Advances in Neural Information Processing Systems}, 33:11525--11538, 2020.

\bibitem{fishnets}
T.~L. Makinen, J.~Alsing, and B.~D. Wandelt.
\newblock Fishnets: Information-optimal, scalable aggregation for sets and graphs, 2023.

\bibitem{PPGN}
H.~Maron, H.~Ben{-}Hamu, H.~Serviansky, and Y.~Lipman.
\newblock Provably powerful graph networks.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pages 2153--2164, 2019.

\bibitem{morris2020tudataset}
C.~Morris, N.~M. Kriege, F.~Bause, K.~Kersting, P.~Mutzel, and M.~Neumann.
\newblock Tudataset: A collection of benchmark datasets for learning with graphs.
\newblock {\em arXiv preprint arXiv:2007.08663}, 2020.

\bibitem{gnns_for_cv_example_2}
M.~Narasimhan, S.~Lazebnik, and A.~G. Schwing.
\newblock Out of the box: Reasoning with graph convolution nets for factual visual question answering.
\newblock {\em CoRR}, abs/1811.00538, 2018.

\bibitem{ong2022learnable}
E.~Ong and P.~Veli{\v{c}}kovi{\'c}.
\newblock Learnable commutative monoids for graph neural networks.
\newblock In {\em The First Learning on Graphs Conference}, 2022.

\bibitem{polynomials}
O.~Puny, D.~Lim, B.~T. Kiani, H.~Maron, and Y.~Lipman.
\newblock Equivariant polynomials for graph neural networks.
\newblock In A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and J.~Scarlett, editors, {\em International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 28191--28222. {PMLR}, 2023.

\bibitem{gps}
L.~Ramp{\'{a}}sek, M.~Galkin, V.~P. Dwivedi, A.~T. Luu, G.~Wolf, and D.~Beaini.
\newblock Recipe for a general, powerful, scalable graph transformer.
\newblock {\em CoRR}, abs/2205.12454, 2022.

\bibitem{rampasek2022GPS}
L.~Ramp\'{a}\v{s}ek, M.~Galkin, V.~P. Dwivedi, A.~T. Luu, G.~Wolf, and D.~Beaini.
\newblock {Recipe for a General, Powerful, Scalable Graph Transformer}.
\newblock {\em Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem{vpa}
L.~Schneckenreiter, R.~Freinschlag, F.~Sestak, J.~Brandstetter, G.~Klambauer, and A.~Mayr.
\newblock {GNN}-{VPA}: A variance-preserving aggregation strategy for graph neural networks.
\newblock In {\em The Second Tiny Papers Track at ICLR 2024}, 2024.

\bibitem{schneckenreiter2024gnn}
L.~Schneckenreiter, R.~Freinschlag, F.~Sestak, J.~Brandstetter, G.~Klambauer, and A.~Mayr.
\newblock Gnn-vpa: A variance-preserving aggregation strategy for graph neural networks.
\newblock {\em arXiv preprint arXiv:2403.04747}, 2024.

\bibitem{tailor2022adaptive}
S.~A. Tailor, F.~Opolka, P.~Lio, and N.~D. Lane.
\newblock Adaptive filters for low-latency and memory-efficient graph neural networks.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{tailor2022egc}
S.~A. Tailor, F.~Opolka, P.~Lio, and N.~D. Lane.
\newblock Do we need anistropic graph neural networks?
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{velivckovic2017graph}
P.~Veli{\v{c}}kovi{\'c}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Lio, and Y.~Bengio.
\newblock Graph attention networks.
\newblock {\em arXiv preprint arXiv:1710.10903}, 2017.

\bibitem{velickovic2018graph}
P.~Veli{\v{c}}kovi{\'{c}}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Li{\`{o}}, and Y.~Bengio.
\newblock {Graph Attention Networks}.
\newblock {\em International Conference on Learning Representations}, 2018.
\newblock accepted as poster.

\bibitem{JacobiConv}
X.~Wang and M.~Zhang.
\newblock How powerful are spectral graph neural networks.
\newblock {\em ICML}, 2022.

\bibitem{gnns_for_nlp_example_1}
Z.~Wang, Q.~Lv, X.~Lan, and Y.~Zhang.
\newblock Cross-lingual knowledge graph alignment via graph convolutional networks.
\newblock In E.~Riloff, D.~Chiang, J.~Hockenmaier, and J.~Tsujii, editors, {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 349--357, Brussels, Belgium, Oct.-Nov. 2018. Association for Computational Linguistics.

\bibitem{sgc}
F.~Wu, A.~Souza, T.~Zhang, C.~Fifty, T.~Yu, and K.~Weinberger.
\newblock Simplifying graph convolutional networks.
\newblock In {\em Proceedings of the 36th International Conference on Machine Learning}, pages 6861--6871. PMLR, 2019.

\bibitem{xu2018powerful}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
\newblock How powerful are graph neural networks?
\newblock {\em arXiv preprint arXiv:1810.00826}, 2018.

\bibitem{xu2018how}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
\newblock How powerful are graph neural networks?
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{graph_transformers_basic}
S.~Yun, M.~Jeong, R.~Kim, J.~Kang, and H.~J. Kim.
\newblock Graph transformer networks.
\newblock In H.~M. Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'Alch{\'{e}}{-}Buc, E.~B. Fox, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada}, pages 11960--11970, 2019.

\bibitem{deep-sets}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~Poczos, R.~R. Salakhutdinov, and A.~J. Smola.
\newblock Deep sets.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\end{thebibliography}
