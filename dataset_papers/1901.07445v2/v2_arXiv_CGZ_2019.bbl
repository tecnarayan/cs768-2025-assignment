\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{AWBR09}

\bibitem[AFGO18]{StrConvex}
N.~S. Aybat, A.~Fallah, M.~G\"{u}rb\"{u}zbalaban, and A.~Ozdaglar.
\newblock Robust accelerated gradient methods for smooth strongly convex
  functions.
\newblock {\em arXiv preprint arXiv:1805.10579}, 2018.

\bibitem[AFGO19]{aybat2019universally}
Necdet~Serhat Aybat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar.
\newblock A universally optimal multistage accelerated stochastic gradient
  method.
\newblock {\em arXiv preprint arXiv:1901.08022}, 2019.

\bibitem[AWBR09]{agarwal-minmax}
Alekh Agarwal, Martin~J Wainwright, Peter~L. Bartlett, and Pradeep~K.
  Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock In Y.~Bengio, D.~Schuurmans, J.~D. Lafferty, C.~K.~I. Williams, and
  A.~Culotta, editors, {\em Advances in Neural Information Processing Systems
  22}, pages 1--9. Curran Associates, Inc., 2009.

\bibitem[Bec17]{FirstOrderMethods}
A.~Beck.
\newblock {\em First-Order Methods in Optimization}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  2017.

\bibitem[BST14]{bassily2014private}
Raef Bassily, Adam Smith, and Abhradeep Thakurta.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In {\em Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual
  Symposium on}, pages 464--473. IEEE, 2014.

\bibitem[{Bub}14]{Bubeck2014}
S.~{Bubeck}.
\newblock {Theory of Convex Optimization for Machine Learning}.
\newblock {\em arXiv preprint arXiv:1405.4980}, May 2014.

\bibitem[BWBZ13]{birand2013measurements}
Berk Birand, Howard Wang, Keren Bergman, and Gil Zussman.
\newblock Measurements-based power control-a cross-layered framework.
\newblock In {\em National Fiber Optic Engineers Conference}, pages JTh2A--66.
  Optical Society of America, 2013.

\bibitem[CDLZ16]{min-max}
Sabyasachi Chatterjee, John~C. Duchi, John Lafferty, and Yuancheng Zhu.
\newblock Local minimax complexity of stochastic convex optimization.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  3423--3431. Curran Associates, Inc., 2016.

\bibitem[CDO18]{cohen18}
Michael~B. {Cohen}, Jelena {Diakonikolas}, and Lorenzo {Orecchia}.
\newblock {On Acceleration with Noise-Corrupted Gradients}.
\newblock {\em arXiv e-prints}, page arXiv:1805.12591, May 2018.

\bibitem[{\c{C}}{\i}n11]{ccinlar2011probability}
Erhan {\c{C}}{\i}nlar.
\newblock {\em Probability and Stochastics}, volume 261.
\newblock Springer Science \& Business Media, New York, 2011.

\bibitem[CW05]{combettes2005signal}
Patrick~L Combettes and Val{\'e}rie~R Wajs.
\newblock Signal recovery by proximal forward-backward splitting.
\newblock {\em Multiscale Modeling \& Simulation}, 4(4):1168--1200, 2005.

\bibitem[d'A08]{daspremont}
A.~d'Aspremont.
\newblock Smooth optimization with approximate gradient.
\newblock {\em SIAM Journal on Optimization}, 19(3):1171--1183, 2008.

\bibitem[DDB17]{bachGD}
Aymeric Dieuleveut, Alain Durmus, and Francis Bach.
\newblock Bridging the gap between constant step size stochastic gradient
  descent and {M}arkov chains.
\newblock {\em arXiv preprint arXiv:1707.06386}, 2017.

\bibitem[DFB17]{dieuleveut2017harder}
Aymeric Dieuleveut, Nicolas Flammarion, and Francis Bach.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock {\em The Journal of Machine Learning Research}, 18(1):3520--3570,
  2017.

\bibitem[DGN13]{devolder2013intermediate}
O.~Devolder, F.~Glineur, and Y.~Nesterov.
\newblock Intermediate gradient methods for smooth convex problems with inexact
  oracle.
\newblock Technical report, Universit{\'e} catholique de Louvain, Center for
  Operations Research and Econometrics (CORE), 2013.

\bibitem[DGN14]{devolder2014first}
O.~Devolder, F.~Glineur, and Y.~Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock {\em Mathematical Programming}, 146(1-2):37--75, 2014.

\bibitem[FB15]{flammarion2015averaging}
N.~Flammarion and F.~Bach.
\newblock From averaging to acceleration, there is only a step-size.
\newblock In {\em Conference on Learning Theory}, pages 658--695, 2015.

\bibitem[Fl{\aa}04]{flaam2004optimization}
Sjur~Didrik Fl{\aa}m.
\newblock Optimization under uncertainty using momentum.
\newblock In {\em Dynamic Stochastic Optimization}, pages 249--256. Springer,
  2004.

\bibitem[FRMP17]{fazlyab2017dynamical}
Mahyar Fazlyab, Alejandro Ribeiro, Manfred Morari, and Victor~M Preciado.
\newblock A dynamical systems perspective to convergence rate analysis of
  proximal algorithms.
\newblock In {\em Communication, Control, and Computing (Allerton), 2017 55th
  Annual Allerton Conference on}, pages 354--360. IEEE, 2017.

\bibitem[GFJ14]{ghadimiHB}
Euhanna {Ghadimi}, Hamid~Reza {Feyzmahdavian}, and Mikael {Johansson}.
\newblock {Global convergence of the Heavy-ball method for convex
  optimization}.
\newblock {\em arXiv e-prints}, page arXiv:1412.7457, December 2014.

\bibitem[GGZ18a]{GGZarxiv2}
Xuefeng {Gao}, Mert {Gurbuzbalaban}, and Lingjiong {Zhu}.
\newblock {Breaking Reversibility Accelerates Langevin Dynamics for Global
  Non-Convex Optimization}.
\newblock {\em arXiv preprint arXiv:1812.07725}, December 2018.

\bibitem[GGZ18b]{GGZarxiv1}
Xuefeng {Gao}, Mert {G{\"u}rb{\"u}zbalaban}, and Lingjiong {Zhu}.
\newblock {Global Convergence of Stochastic Gradient Hamiltonian Monte Carlo
  for Non-Convex Stochastic Optimization: Non-Asymptotic Performance Bounds and
  Momentum-Based Acceleration}.
\newblock {\em arXiv preprint arXiv:1809.04618}, September 2018.

\bibitem[GHJY15]{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points--online stochastic gradient for tensor
  decomposition.
\newblock In {\em Conference on Learning Theory}, pages 797--842, 2015.

\bibitem[GL12]{ghadimi2012optimal-1}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock {\em SIAM Journal on Optimization}, 22(4):1469--1492, 2012.

\bibitem[GL13]{ghadimi-lan-shrinking}
S.~Ghadimi and G.~Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization, ii: Shrinking procedures and optimal
  algorithms.
\newblock {\em SIAM Journal on Optimization}, 23(4):2061--2089, 2013.

\bibitem[GPS18]{gadat2018stochastic}
S{\'e}bastien Gadat, Fabien Panloup, and Sofiane Saadane.
\newblock Stochastic heavy ball.
\newblock {\em Electronic Journal of Statistics}, 12(1):461--529, 2018.

\bibitem[GVL96]{golub1996matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock {\em Matrix computations}.
\newblock Johns Hopkins University Press, Baltimore, 3rd edition, 1996.

\bibitem[Har56]{Harris}
T.~E. Harris.
\newblock The existence of stationary measures for certain {M}arkov processes.
\newblock In {\em Proceedings of the Third Berkeley Symposium on Mathematical
  Statistics and Probability, 1954-1955, vol. II}, pages 113--124, Berkeley and
  Los Angeles, 1956.

\bibitem[Har14]{Hardt-blog}
M.~Hardt.
\newblock Robustness versus acceleration., August 2014.

\bibitem[HL17]{hu2017dissipativity}
B.~Hu and L.~Lessard.
\newblock Dissipativity theory for {N}esterov's accelerated method.
\newblock {\em arXiv preprint arXiv:1706.04381}, 2017.

\bibitem[HM11]{hairer}
M.~Hairer and J.~C. Mattingly.
\newblock Yet another look at {H}arris' ergodic theorem for {M}arkov chains.
\newblock In {\em Seminar on Stochastic Analysis, Random Fields and
  Applications VI}, pages 109--118, Basel, 2011.

\bibitem[HPK09]{hu2009accelerated}
Chonghai Hu, Weike Pan, and James~T Kwok.
\newblock Accelerated gradient methods for stochastic optimization and online
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  781--789, 2009.

\bibitem[JKK{\etalchar{+}}17]{jain2017accelerating}
Prateek Jain, Sham~M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron
  Sidford.
\newblock Accelerating stochastic gradient descent.
\newblock {\em arXiv preprint arXiv:1704.08227}, 2017.

\bibitem[KV17]{vavasis}
Sahar {Karimi} and Stephen {Vavasis}.
\newblock {A single potential governing convergence of conjugate gradient,
  accelerated gradient and geometric descent}.
\newblock {\em arXiv e-prints}, page arXiv:1712.09498, December 2017.

\bibitem[Lan12]{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133:365--397, 2012.

\bibitem[LR17]{loizou2017momentum}
Nicolas Loizou and Peter Richt{\'a}rik.
\newblock Momentum and stochastic momentum for stochastic gradient, {N}ewton,
  proximal point and subspace descent methods.
\newblock {\em arXiv preprint arXiv:1712.09677}, 2017.

\bibitem[LR18]{loizou2018accelerated}
Nicolas Loizou and Peter Richt{\'a}rik.
\newblock Accelerated gossip via stochastic heavy ball method.
\newblock {\em arXiv preprint arXiv:1809.08657}, 2018.

\bibitem[LRP16]{lessard2016analysis}
Laurent Lessard, Benjamin Recht, and Andrew Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock {\em SIAM Journal on Optimization}, 26(1):57--95, 2016.

\bibitem[MT93]{Meyn1993}
S.~P. Meyn and R.~L. Tweedie.
\newblock {\em Markov Chains and Stochastic Stability}.
\newblock Communications and Control Engineering Series. Springer-Verlag,
  London, 1993.

\bibitem[MT94]{Meyn1994}
S.~P. Meyn and R.~L. Tweedie.
\newblock Computable bounds for geometric convergence rates of {M}arkov chains.
\newblock {\em Annals of Applied Probability}, 4(4):981--1011, 1994.

\bibitem[Nes04]{nesterov2004introductory}
Yurii Nesterov.
\newblock {\em Introductory Lectures on Convex Optimization. Applied
  Optimization, Vol. 87}.
\newblock Kluwer Academic Publishers, Boston, 2004.

\bibitem[Nit14]{nitanda2014stochastic}
Atsushi Nitanda.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1574--1582, 2014.

\bibitem[NVL{\etalchar{+}}15]{GradImprovLearning}
Arvind Neelakantan, Luke Vilnis, Quoc~V. Le, Ilya Sutskever, Lukasz Kaiser,
  Karol Kurach, and James Martens.
\newblock Adding gradient noise improves learning for very deep networks.
\newblock {\em CoRR}, abs/1511.06807, 2015.

\bibitem[OC15]{o2015adaptive}
Brendan O'Donoghue and Emmanuel Candes.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock {\em Foundations of Computational Mathematics}, 15(3):715--732, 2015.

\bibitem[PB{\etalchar{+}}14]{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends{\textregistered} in Optimization},
  1(3):127--239, 2014.

\bibitem[Pol64]{Polyak64heavyball}
B.T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics}, 4(5):1
  -- 17, 1964.

\bibitem[Pol87]{polyak1987introduction}
Boris~T. Polyak.
\newblock {\em Introduction to optimization}.
\newblock Translations series in mathematics and engineering. Optimization
  Software, 1987.

\bibitem[Rec12]{Recht}
Benjamin Recht.
\newblock Lyapunov analysis and the heavy ball method.
\newblock {\em Online lecture notes}, 2012.

\bibitem[RR11]{raginsky2011information}
Maxim Raginsky and Alexander Rakhlin.
\newblock Information-based complexity, feedback and dynamics in convex
  programming.
\newblock {\em IEEE Transactions on Information Theory}, 57(10):7036--7056,
  2011.

\bibitem[RRT17]{raginsky2017non}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock Non-convex learning via stochastic gradient {{L}}angevin dynamics: a
  nonasymptotic analysis.
\newblock {\em arXiv preprint arXiv:1702.03849}, 2017.

\bibitem[SBC14]{su2014differential}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling {N}esterov{'}s accelerated
  gradient method: Theory and insights.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2510--2518, 2014.

\bibitem[SMDH13]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1139--1147, 2013.

\bibitem[SSG19]{simsekli2019tail}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1901.06053}, 2019.

\bibitem[Vap13]{vapnik2013nature}
Vladimir Vapnik.
\newblock {\em The Nature of Statistical Learning Theory}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Var09]{varga2009matrix}
Richard~S Varga.
\newblock {\em Matrix Iterative Analysis}, volume~27.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Vil09]{villani2008optimal}
C{\'e}dric Villani.
\newblock {\em Optimal Transport: Old and New}.
\newblock Springer, Berlin, 2009.

\bibitem[Wil92]{williams2by2}
Kenneth~S. Williams.
\newblock The $n$th power of a $2\times 2$ matrix.
\newblock {\em Mathematics Magazine}, 65(5):336--336, 1992.

\bibitem[WRJ16]{wilson2016lyapunov}
A.C. Wilson, B.~Recht, and M.I. Jordan.
\newblock A {L}yapunov analysis of momentum methods in optimization.
\newblock {\em arXiv preprint arXiv:1611.02635}, 2016.

\bibitem[Xia10]{xiao2010dual}
Lin Xiao.
\newblock Dual averaging methods for regularized stochastic learning and online
  optimization.
\newblock {\em Journal of Machine Learning Research}, 11(Oct):2543--2596, 2010.

\bibitem[YLL16]{yang2016unified}
Tianbao Yang, Qihang Lin, and Zhe Li.
\newblock Unified convergence analysis of stochastic momentum methods for
  convex and non-convex optimization.
\newblock {\em arXiv preprint arXiv:1604.03257}, 2016.

\end{thebibliography}
