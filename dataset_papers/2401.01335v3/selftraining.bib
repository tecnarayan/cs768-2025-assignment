@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@inproceedings{kenton2019bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of naacL-HLT},
  volume={1},
  pages={2},
  year={2019}
}

@article{saunders2022self,
  title={Self-critiquing models for assisting human evaluators},
  author={Saunders, William and Yeh, Catherine and Wu, Jeff and Bills, Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan},
  journal={arXiv preprint arXiv:2206.05802},
  year={2022}
}


@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@inproceedings{victor2022multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Victor, Sanh and Albert, Webson and Colin, Raffel and Stephen, Bach and Lintang, Sutawika and Zaid, Alyafeai and Antoine, Chaffin and Arnaud, Stiegler and Arun, Raja and Manan, Dey and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2104.08773},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{vershynin,
  author    = {Roman Vershynin},
  title     = {Introduction to the non-asymptotic analysis of random matrices},
  journal   = {arXiv preprint},
  volume    = {arXiv:1011.3027},
  year      = {2010}
}

@article{samuel2000some,
  title={Some studies in machine learning using the game of checkers},
  author={Samuel, Arthur L},
  journal={IBM Journal of research and development},
  volume={44},
  number={1.2},
  pages={206--226},
  year={2000},
  publisher={IBM}
}

@article{silver2017masteringchess,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}


@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{tesauro1995temporal,
  title={Temporal difference learning and TD-Gammon},
  author={Tesauro, Gerald and others},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995}
}
@article{rizve2021defense,
  title={In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning},
  author={Rizve, Mamshad Nayeem and Duarte, Kevin and Rawat, Yogesh S and Shah, Mubarak},
  journal={arXiv preprint arXiv:2101.06329},
  year={2021}
}

@book{shalevschwartz,
 author = {Shalev-Shwartz, Shai and Ben-David, Shai},
 title = {Understanding Machine Learning: From Theory to Algorithms},
 year = {2014},
 isbn = {1107057132, 9781107057135},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 


@inproceedings{pham2021meta,
  title={Meta pseudo labels},
  author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11557--11568},
  year={2021}
}


@inproceedings{yehudai20,
  author    = {Gilad Yehudai and
               Ohad Shamir},
  title     = {Learning a Single Neuron with Gradient Methods},
  booktitle   = {Conference on Learning Theory (COLT)},
  year      = {2020}
}
@inproceedings{vempala,
  author    = {Santosh S. Vempala and
               John Wilmes},
  title     = {Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial
               Convergence and {SQ} Lower Bounds},
  booktitle = {Conference on Learning Theory (COLT)},
  year      = {2019},
  timestamp = {Thu, 24 Oct 2019 15:31:49 +0200},
  biburl    = {https://dblp.org/rec/conf/colt/VempalaW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@book{wainwright,
  title={High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
  author={Wainwright, M.J.},
  isbn={9781108498029},
  lccn={2018043475},
  series={Cambridge Series in Statistical and Probabilistic Mathematics},
  url={https://books.google.com/books?id=8C8nuQEACAAJ},
  year={2019},
  publisher={Cambridge University Press}
}



@inproceedings{jitelgarsky20.polylog,
  author    = {Ziwei Ji and
               Matus Telgarsky},
  title     = {Polylogarithmic width suffices for gradient descent to achieve arbitrarily
               small test error with shallow ReLU networks},
  year = {2020},
  booktitle = {International Conference on Learning Representations (ICLR)},
}




@inproceedings{beygelzimer,
  author    = {Alina Beygelzimer and
               John Langford and
               Lihong Li and
               Lev Reyzin and
               Robert E. Schapire},
  title     = {Contextual Bandit Algorithms with Supervised Learning Guarantees},
  booktitle = {Conference on Artificial
               Intelligence and Statistics (AISTATS)},
  year      = {2011},
  timestamp = {Wed, 29 May 2019 08:41:47 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/BeygelzimerLLRS11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{zou2019gradient,
author={Zou, Difan
and Cao, Yuan
and Zhou, Dongruo
and Gu, Quanquan},
title={Gradient descent optimizes over-parameterized deep {ReLU} networks},
journal={Machine Learning},
year={2019}
}



@inproceedings{zou2019improved,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}





@article{chen.polylog,
  author    = {Zixiang Chen and
               Yuan Cao and
               Difan Zou and
               Quanquan Gu},
  title     = {How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  journal   = {arXiv},
  volume    = {abs/1911.12360},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.12360},
  archivePrefix = {arXiv},
  eprint    = {1911.12360},
  timestamp = {Tue, 03 Dec 2019 20:41:07 +0100},
  biburl    = {https://dblp.org/rec/journals/arXiv/abs-1911-12360.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{allenzhu.kernel,
  author    = {Zeyuan Allen{-}Zhu and
               Yuanzhi Li},
  title     = {What Can ResNet Learn Efficiently, Going Beyond Kernels?},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019},
  timestamp = {Fri, 06 Mar 2020 16:59:29 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/Allen-ZhuL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{allenzhu.3layer,
title = {Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers},
author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
booktitle = {Advances in Neural Information Processing Systems 32},
year = {2019},
}

@inproceedings{du2017,
  author    = {Simon S. Du and
               Jason D. Lee and
               Yuandong Tian},
  title     = {When is a Convolutional Filter Easy to Learn?},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2018},
  timestamp = {Thu, 25 Jul 2019 14:25:58 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DuLT18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{kakade2011,
  author    = {Sham M. Kakade and
               Adam Kalai and
               Varun Kanade and
               Ohad Shamir},
  title     = {Efficient Learning of Generalized Linear and Single Index Models with
               Isotonic Regression},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2011},
  timestamp = {Fri, 06 Mar 2020 16:57:02 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/KakadeKKS11.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pinelis1986,
author = {Pinelis, I. F. and Sakhanenko, A. I.},
title = {Remarks on Inequalities for Large Deviation Probabilities},
journal = {Theory of Probability \& Its Applications},
volume = {30},
number = {1},
pages = {143-148},
year = {1986}
}

@inproceedings{foster2018,
  author    = {Dylan J. Foster and
               Ayush Sekhari and
               Karthik Sridharan},
  title     = {Uniform Convergence of Gradients for Non-Convex Learning and Optimization},
  booktitle = {Advances in Neural Information Processing Systems },
  year      = {2018},
  timestamp = {Fri, 06 Mar 2020 17:00:31 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/FosterSS18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{jin20.normsubgaussian,
  author    = {Chi Jin and
               Praneeth Netrapalli and
               Rong Ge and
               Sham M. Kakade and
               Michael I. Jordan},
  title     = {A Short Note on Concentration Inequalities for Random Vectors with
               SubGaussian Norm},
  journal   = {arXiv},
  volume    = {abs/1902.03736},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1902.03736},
  timestamp = {Tue, 21 May 2019 18:03:38 +0200},
  biburl    = {https://dblp.org/rec/journals/arXiv/abs-1902-03736.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@inproceedings{soltanolkotabi2017relus,
  author    = {Mahdi Soltanolkotabi},
  title     = {Learning ReLUs via Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2017},
  timestamp = {Fri, 06 Mar 2020 16:55:47 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/Soltanolkotabi17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{soltanolkotabi2019theoretical,
  author    = {Mahdi Soltanolkotabi and
               Adel Javanmard and
               Jason D. Lee},
  title     = {Theoretical Insights Into the Optimization Landscape of Over-Parameterized
               Shallow Neural Networks},
  journal   = {{IEEE} Trans. Inf. Theory},
  volume    = {65},
  number    = {2},
  pages     = {742--769},
  year      = {2019},
  timestamp = {Tue, 10 Mar 2020 10:47:25 +0100},
  biburl    = {https://dblp.org/rec/journals/tit/SoltanolkotabiJ19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mei2016landscape,
author = "Mei, Song and Bai, Yu and Montanari, Andrea",
fjournal = "Annals of Statistics",
journal = "Ann. Statist.",
month = "12",
number = "6A",
pages = "2747--2774",
publisher = "The Institute of Mathematical Statistics",
title = "The landscape of empirical risk for nonconvex losses",
volume = "46",
year = "2018"
}

}



@inproceedings{goel2019relugaussian,
  author    = {Surbhi Goel and
               Sushrut Karmalkar and
               Adam R. Klivans},
  title     = {Time/Accuracy Tradeoffs for Learning a ReLU with respect to Gaussian
               Marginals},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2019},
  timestamp = {Fri, 06 Mar 2020 16:59:09 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/GoelKK19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}




@article{awasthi,
  author    = {Pranjal Awasthi and
               Maria{-}Florina Balcan and
               Philip M. Long},
  title     = {The Power of Localization for Efficiently Learning Linear Separators
               with Noise},
  journal   = {J. {ACM}},
  volume    = {63},
  number    = {6},
  pages     = {50:1--50:27},
  year      = {2017},
  timestamp = {Tue, 06 Nov 2018 12:51:46 +0100},
  biburl    = {https://dblp.org/rec/journals/jacm/AwasthiBL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{mukherjee,
title={A Study of Neural Training with Non-Gradient and Noise Assisted Gradient Methods},
    author={Anirbit Mukherjee and Ramchandran Muthukumar},
    year={2020},
    journal = {arXiv},
    eprint={2005.04211},
    volume = {abs/2005.04211}
}


@inproceedings{brutzkus2017,
author = {Brutzkus, Alon and Globerson, Amir},
title = {Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs},
year = {2017},
booktitle = {International Conference on Machine Learning (ICML)},
location = {Sydney, NSW, Australia},
}


@inproceedings{diakonikolas2020,
  author    = {Ilias Diakonikolas and
               Vasilis Kontonis and
               Christos Tzamos and
               Nikos Zarifis},
  title     = {Learning Halfspaces with Massart Noise Under Structured Distributions},
  year = {2020},
  booktitle = {Conference on Learning Theory (COLT)}
}


@article{shamir15,
  author  = {Ohad Shamir},
  title   = {The Sample Complexity of Learning Linear Predictors with the Squared Loss},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {108},
  pages   = {3475-3486}
}

@inproceedings{srebro.mirror,
title = {Smoothness, Low Noise and Fast Rates},
author = {Nathan Srebro and Sridharan, Karthik and Tewari, Ambuj},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2010}
}



@InProceedings{goel2layer,
  title = 	 {Learning Neural Networks with Two Nonlinear Layers in Polynomial Time},
  author = 	 {Goel, Surbhi and Klivans, Adam R.},
  booktitle = 	 {Conference on Learning Theory (COLT)},
  year = 	 {2019}
}

@article{hsu2014random,
  author = "Hsu, Daniel and Kakade, Sham M. and Zhang, Tong",
  journal = "Foundations of Computational Mathematics",
  number = "3",
  pages = "569--600",
  title = "Random design analysis of ridge regression",
  volume = "14",
  year = "2014"
}

@InProceedings{kalai2009isotron,
author = {Kalai, Adam Tauman and Sastry, Ravi},
title = {The Isotron Algorithm: High-Dimensional Isotonic Regression},
booktitle = {Conference on Learning Theory (COLT)},
year = {2009},
abstract = {The Perceptron algorithm elegantly solves binary classification problems that have a margin between positive and negative examples. Isotonic regression (fitting an arbitrary increasing function in one dimension) is also a natural problem with a simple solution. By combining the two, we get a new simple regression algorithm in high dimensions, with strong guarantees.},
}

@article{kearns.agnostic,
  title={Toward efficient agnostic learning},
  author={Kearns, Michael J and Schapire, Robert E and Sellie, Linda M},
  journal={Machine Learning},
  volume={17},
  number={2-3},
  pages={115--141},
  year={1994},
  publisher={Springer}
}


@article{kearns.probabilistic,
title = "Efficient distribution-free learning of probabilistic concepts",
journal = "Journal of Computer and System Sciences",
volume = "48",
number = "3",
pages = "464 - 497",
year = "1994",
issn = "0022-0000",
author = "Michael J. Kearns and Robert E. Schapire",
abstract = "In this paper we investigate a new formal model of machine learning in which the concept (Boolean function) to be learned may exhibit uncertain or probabilistic behavior—thus, the same input may sometimes be classified as a positive example and sometimes as a negative example. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. We adopt from the Valiant model of learining [28] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. In addition to giving many efficient algorithms for learning natural classes of p-concepts, we study and develop in detail an underlying theory of learning p-concepts."
}



@inproceedings{goel.convotron,
  author    = {Surbhi Goel and
               Adam R. Klivans and
               Raghu Meka},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Learning One Convolutional Layer with Overlapping Patches},
  booktitle = {International Conference on Machine Learning},
  year      = {2018},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/GoelKM18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{zhanggu2019,
  title = 	 {Learning One-hidden-layer ReLU Networks via Gradient Descent},
  author = 	 {Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  booktitle = 	 { Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = 	 {2019},
  abstract = 	 {We study the problem of learning one-hidden-layer neural networks with Rectified Linear Unit (ReLU) activation function, where the inputs are sampled from standard Gaussian distribution and the outputs are generated from a noisy teacher network. We analyze the performance of gradient descent for training such kind of neural networks based on empirical risk minimization, and provide algorithm-dependent guarantees. In particular, we prove that tensor initialization followed by gradient descent can converge to the ground-truth parameters at a linear rate up to some statistical error. To the best of our knowledge, this is the first work characterizing the recovery guarantee for practical learning of one-hidden-layer ReLU networks with multiple neurons. Numerical experiments verify our theoretical findings.}
}

@inproceedings{cao2019cnn,

title = {Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks},
author = {Cao, Yuan and Gu, Quanquan},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2019}}



@inproceedings{tian2017relu,
  author    = {Yuandong Tian},
  title     = {Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural
               Networks with ReLU nonlinearity},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2017},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{auer1995,
author = {Auer, Peter and Herbster, Mark and Warmuth, Manfred K.},
title = {Exponentially Many Local Minima for Single Neurons},
year = {1995},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
}

@inproceedings{helmbold95worstcase,
author = {Helmbold, David P. and Kivinen, Jyrki and Warmuth, Manfred K.},
title = {Worst-Case Loss Bounds for Single Neurons},
year = {1995},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@ARTICLE{helmbold99relativeloss,
    author = {David P. Helmbold and Jyrki Kivinen and Manfred K. Warmuth},
    title = {Relative loss bounds for single neurons},
    journal = {IEEE Transactions on Neural Networks},
    year = {1999}
}



@InProceedings{goel2017relupoly,
  title = 	 {Reliably Learning the ReLU in Polynomial Time},
  author = 	 {Surbhi Goel and Varun Kanade and Adam Klivans and Justin Thaler},
  booktitle = 	 {Conference on Learning Theory (COLT)},
  year = 	 {2017},
  abstract = 	 {We give the first dimension-efficient algorithms for learning Rectified Linear Units (ReLUs), which are functions of the form $\mathbf{x} \mapsto \mathsf{max}(0,  \mathbf{w} ⋅\mathbf{x})$ with $\mathbf{w} ∈\mathbb{S}^n-1$. Our algorithm works in the challenging Reliable Agnostic learning model of Kalai, Kanade and Mansour (2012) where the learner is given access to a distribution $\mathcal{D}$ on labeled examples but the labeling may be arbitrary.  We construct a hypothesis that simultaneously minimizes the false-positive rate and the loss on inputs given positive labels by $\mathcal{D}$, for any convex, bounded, and Lipschitz loss function. The algorithm runs in polynomial-time (in $n$) with respect to \em any distribution on $\mathbb{S}^n-1$ (the unit sphere in $n$ dimensions) and for any error parameter $ε= Ω(1 / \log n)$ (this yields a PTAS for a question raised by F. Bach on the complexity of maximizing ReLUs).  These results are in contrast to known efficient algorithms for reliably learning linear threshold functions, where $ε$ must be $Ω(1)$ and strong assumptions are required on the marginal distribution. We can compose our results to obtain the first set of efficient algorithms for learning constant-depth networks of ReLU with fixed polynomial-dependence in the dimension. For depth-2 networks of sigmoids, we obtain the first algorithms that have a polynomial dependency in \em all parameters. Our techniques combine kernel methods and polynomial approximations with a “dual-loss” approach to convex programming. As a byproduct we obtain a number of applications including the first set of efficient algorithms for “convex piecewise-linear fitting” and the first efficient algorithms for noisy polynomial reconstruction of low-weight polynomials on the unit sphere. }
}


@ARTICLE{brady1989,
  author={M. L. {Brady} and R. {Raghavan} and J. {Slawny}},
  journal={IEEE Transactions on Circuits and Systems}, 
  title={Back propagation fails to separate where perceptrons succeed}, 
  year={1989},
  volume={36},
  number={5},
  pages={665-674},}
  
  @article{lovasz,
author = {Lov\'{a}sz, L\'{a}szl\'{o} and Vempala, Santosh},
title = {The Geometry of Logconcave Functions and Sampling Algorithms},
year = {2007},
volume = {30},
number = {3},
issn = {1042-9832},
journal = {Random Struct. Algorithms},
pages = {307–358},
}
  

  
@inproceedings{diakonikolas2020approximation,
  title={Approximation Schemes for ReLU Regression},
  author={Diakonikolas, Ilias and Goel, Surbhi and Karmalkar, Sushrut and Klivans, Adam R and Soltanolkotabi, Mahdi},
  booktitle = {Conference on Learning Theory (COLT)},
  year={2020}
}


@article{manurangsi2018,
  author    = {Pasin Manurangsi and
               Daniel Reichman},
  title     = {The Computational Complexity of Training ReLU(s)},
  year      = {2018},
  archivePrefix = {arXiv},
  eprint    = {1810.04207}
}

@inproceedings{frei2019resnet,
  title={Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}


@inproceedings{frei2020singleneuron,
  title={Agnostic Learning of a Single Neuron with Gradient Descent},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{
brutzkus2018sgd,
title={{SGD} Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
author={Alon Brutzkus and Amir Globerson and Eran Malach and Shai Shalev-Shwartz},
booktitle={International Conference on Learning Representations (ICLR)},
year={2018},
}

@inproceedings{frei2020halfspace,
      title={Agnostic Learning of Halfspaces with Gradient Descent via Soft Margins}, 
      author={Spencer Frei and Yuan Cao and Quanquan Gu},
      year={2021},
      booktitle = {International Conference on Machine Learning (ICML)}
}

@inproceedings{klivans2004intersection,
  title={Learning intersections of halfspaces with a margin},
  author={Klivans, Adam R and Servedio, Rocco A},
  booktitle={International Conference on Computational Learning Theory},
  pages={348--362},
  year={2004},
  organization={Springer}
}

@inproceedings{diakonikolas2020massartstructured,
      title={Learning Halfspaces with Massart Noise Under Structured Distributions}, 
      author={Ilias Diakonikolas and Vasilis Kontonis and Christos Tzamos and Nikos Zarifis},
      year={2020},
      booktitle={Conference on Learning Theory (COLT)}
}

@article{shamir2020implicit,
      title={Implicit Regularization in ReLU Networks with the Square Loss}, 
      author={Ohad Shamir and Gal Vardi},
      year={2020},
      journal={Preprint, arXiv:2012.05156},
}

@inproceedings{diakonikolas2020nonconvex,
      title={Non-Convex SGD Learns Halfspaces with Adversarial Label Noise}, 
      author={Ilias Diakonikolas and Vasilis Kontonis and Christos Tzamos and Nikos Zarifis},
      year={2020},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}


@inproceedings{zhang2017rethinkinggeneralization,
      title={Understanding deep learning requires rethinking generalization}, 
      author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
      year={2017},
      booktitle={International Conference on Learning Representations (ICLR)}
}

@inproceedings{nakkiran2019sgd,
      title={SGD on Neural Networks Learns Functions of Increasing Complexity}, 
      author={Preetum Nakkiran and Gal Kaplun and Dimitris Kalimeris and Tristan Yang and Benjamin L. Edelman and Fred Zhang and Boaz Barak},
      year={2019},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{hu2020surprising,
      title={The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks}, 
      author={Wei Hu and Lechao Xiao and Ben Adlam and Jeffrey Pennington},
      year={2020},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{jacot2018ntk,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks}, 
      author={Arthur Jacot and Franck Gabriel and Clément Hongler},
      year={2018},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{allenzhu2019convergence,
      title={A Convergence Theory for Deep Learning via Over-Parameterization}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
      year={2019},
      booktitle={International Conference on Machine Learning (ICML)}
}


@inproceedings{du2019-1layer,
  author    = {Simon S. Du and
               Xiyu Zhai and
               Barnab{\'{a}}s P{\'{o}}czos and
               Aarti Singh},
  title     = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  year = {2019},
  booktitle = {International Conference on Learning Representations (ICLR)}
}

@inproceedings{du2019deep,
  author    = {Simon S. Du and
               Jason D. Lee and
               Haochuan Li and
               Liwei Wang and
               Xiyu Zhai},
  title     = {Gradient Descent Finds Global Minima of Deep Neural Networks},
  year      = {2018},
  booktitle = {International Conference on Machine Learning (ICML)}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}


@inproceedings{hu2020simple,
      title={Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee}, 
      author={Wei Hu and Zhiyuan Li and Dingli Yu},
      year={2020},
        booktitle = {International Conference on Learning Representations (ICLR)}
}

@inproceedings{li2019labelnoise,
      title={Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks}, 
      author={Mingchen Li and Mahdi Soltanolkotabi and Samet Oymak},
      year={2019},
      booktitle={Conference on Artificial
               Intelligence and Statistics (AISTATS)}
}

@inproceedings{wei2020regularization,
      title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel}, 
      author={Colin Wei and Jason D. Lee and Qiang Liu and Tengyu Ma},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
 }
 
 @inproceedings{li2020largelearningrate,
      title={Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks}, 
      author={Yuanzhi Li and Colin Wei and Tengyu Ma},
      eprint={1907.04595},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@inproceedings{woodworth2020kernelrich,
      title={Kernel and Rich Regimes in Overparametrized Models}, 
      author={Blake Woodworth and Suriya Gunasekar and Jason D. Lee and Edward Moroshko and Pedro Savarese and Itay Golan and Daniel Soudry and Nathan Srebro},
      year={2020},
      booktitle={Conference on Learning Theory (COLT)}
}

@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory (COLT)},
  year={2019}
}

@article{song2018mean,
  title={A mean field view of the landscape of two-layers neural networks},
  author={Song, Mei and Montanari, A and Nguyen, P},
  journal={PNAS},
  volume={115},
  pages={E7665--E7671},
  year={2018}
}

@inproceedings{chizat2018note,
  title={On Lazy Training in Differentiable Programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@inproceedings{chen2020generalized,
      title={A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks}, 
      author={Zixiang Chen and Yuan Cao and Quanquan Gu and Tong Zhang},
      year={2020},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
}

@inproceedings{daniely2016complexity,
  title={Complexity theoretic limitations on learning halfspaces},
  author={Daniely, Amit},
  booktitle={ACM Symposium on Theory of Computing (STOC)},
  pages={105--117},
  year={2016}
}

@article{diakonikolas2020hardnessmassart,
      title={Hardness of Learning Halfspaces with Massart Noise}, 
      author={Ilias Diakonikolas and Daniel M. Kane},
      year={2020},
      journal={Preprint, arXiv:2012.09720},
}


@article{massart2006noise,
  title={Risk bounds for statistical learning},
  author={Massart, Pascal and N{\'e}d{\'e}lec, {\'E}lodie and others},
  journal={The Annals of Statistics},
  volume={34},
  number={5},
  pages={2326--2366},
  year={2006},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{diakonikolas2019massart,
  title={Distribution-independent pac learning of halfspaces with massart noise},
  author={Diakonikolas, Ilias and Gouleakis, Themis and Tzamos, Christos},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{blum1998rcn,
  title={A polynomial-time algorithm for learning noisy linear threshold functions},
  author={Blum, Avrim and Frieze, Alan and Kannan, Ravi and Vempala, Santosh},
  journal={Algorithmica},
  volume={22},
  number={1-2},
  pages={35--52},
  year={1998},
  publisher={Springer}
}

@article{angluin1988rcn,
  title={Learning from noisy examples},
  author={Angluin, Dana and Laird, Philip},
  journal={Machine Learning},
  volume={2},
  number={4},
  pages={343--370},
  year={1988},
  publisher={Springer}
}


@inproceedings{goel2020superpolynomial,
      title={Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent}, 
      author={Surbhi Goel and Aravind Gollakota and Zhihan Jin and Sushrut Karmalkar and Adam Klivans},
      year={2020},
      booktitle = {International Conference on Machine Learning (ICML)}
}

@inproceedings{diakonikolas2020sqlowerbound,
      title={Algorithms and SQ Lower Bounds for PAC Learning One-Hidden-Layer ReLU Networks}, 
      author={Ilias Diakonikolas and Daniel M. Kane and Vasilis Kontonis and Nikos Zarifis},
      year={2020},
      booktitle = {Conference on Learning Theory (COLT)}
}

@inproceedings{arora2019finegrained,
      title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks}, 
      author={Sanjeev Arora and Simon S. Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
      year={2019},
      booktitle = {International Conference on Machine Learning (ICML)}
}

@article{klivans2009maliciousnoise,
  author  = {Adam R. Klivans and Philip M. Long and Rocco A. Servedio},
  title   = {Learning Halfspaces with Malicious Noise},
  journal = {Journal of Machine Learning Research (JMLR)},
  year    = {2009},
  volume  = {10},
  number  = {94},
  pages   = {2715-2740},
}


@inproceedings{cao2019generalization,
  title={Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={the Thirty-Fourth AAAI Conference on Artificial Intelligence},
  year={2020}
}





@inproceedings{li2020relubeyondntk,
      title={Learning Over-Parametrized Two-Layer ReLU Neural Networks beyond NTK}, 
      author={Yuanzhi Li and Tengyu Ma and Hongyang R. Zhang},
      year={2020},
      booktitle = {Conference on Learning Theory (COLT)}
}

@inproceedings{shamir2018resnetslinear,
      title={Are ResNets Provably Better than Linear Predictors?}, 
      author={Ohad Shamir},
      year={2018},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{yun2019deepresnetlinear,
      title={Are deep ResNets provably better than linear predictors?}, 
      author={Chulhee Yun and Suvrit Sra and Ali Jadbabaie},
      year={2019},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{nagarajan2019uniform,
      title={Uniform convergence may be unable to explain generalization in deep learning}, 
      author={Vaishnavh Nagarajan and J. Zico Kolter},
      year={2019},
      eprint={1902.04742},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{fort2020ntk,
      title={Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel}, 
      author={Stanislav Fort and Gintare Karolina Dziugaite and Mansheej Paul and Sepideh Kharaghani and Daniel M. Roy and Surya Ganguli},
      year={2020},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{jitelgarsky2019implicit,
author    = {Ziwei Ji and
               Matus Telgarsky},
      title={The implicit bias of gradient descent on nonseparable data},
      year={2019},
      booktitle = {Conference on Learning Theory (COLT)}
}


@article{soudry2018implicitbias,
  author  = {Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
  title   = {The Implicit Bias of Gradient Descent on Separable Data},
  journal = {Journal of Machine Learning Research (JMLR)},
  year    = {2018},
  volume  = {19},
  number  = {70},
  pages   = {1-57},
}

@inproceedings{ji2020directional,
      title={Directional convergence and alignment in deep learning}, 
      author={Ziwei Ji and Matus Telgarsky},
      year={2020},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
      
}

@inproceedings{moroshko2020implicit,
      title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy}, 
      author={Edward Moroshko and Suriya Gunasekar and Blake Woodworth and Jason D. Lee and Nathan Srebro and Daniel Soudry},
      year={2020},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{lyuli2020implicitbias,
      title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks}, 
      author={Kaifeng Lyu and Jian Li},
      year={2020},
      booktitle = {International Conference on Learning Representations (ICLR)}
}

@inproceedings{shah2020pitfalls,
      title={The Pitfalls of Simplicity Bias in Neural Networks}, 
      author={Harshay Shah and Kaustav Tamuly and Aditi Raghunathan and Prateek Jain and Praneeth Netrapalli},
      year={2020},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}



@inproceedings{cao2019generalizationsgd,
  title={Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{wang2020universalappx,
      title={Abstract Universal Approximation for Neural Networks}, 
      author={Zi Wang and Aws Albarghouthi and Somesh Jha},
      year={2020},
      journal={Preprint, arXiv:2007.06093},
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@inproceedings{madry2018adversarial,
      title={Towards Deep Learning Models Resistant to Adversarial Attacks}, 
      author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
      year={2018},
      booktitle = {International Conference on Learning Representations (ICLR)}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{awasthi2017acm.localization,
author = {Awasthi, Pranjal and Balcan, Maria Florina and Long, Philip M.},
title = {The Power of Localization for Efficiently Learning Linear Separators with Noise},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {6},
issn = {0004-5411},
journal = {J. ACM},
month = jan,
articleno = {50},
numpages = {27},
keywords = {malicious noise, agnostic learning, Learning theory, localization, noise-tolerant learning, linear classification, active learning}
}


 @InProceedings{awasthi2015massart,
 title = {Efficient Learning of Linear Separators under Bounded Noise},
 author = {Pranjal Awasthi and Maria-Florina Balcan and Nika Haghtalab and Ruth Urner},
 booktitle = {Conference on Learning Theory (COLT)},
 year = {2015}, } 
 
@incollection{balcan2020noise,
  author={Maria-Florina Balcan and Nika Haghtalab},
  title       = "Noise in Classification",
  editor      = "Tim Roughgarden",
  booktitle   = "Beyond Worst Case Analysis of Algorithms",
  publisher   = "Cambridge University Press",
  year        = "2021",
  chapter = "16"
}

 @InProceedings{awasthi20161bitcompressednoise,
 title = {Learning and 1-bit Compressed Sensing under Asymmetric Noise}, 
 author = {Pranjal Awasthi and Maria-Florina Balcan and Nika Haghtalab and Hongyang Zhang}, 
 booktitle = {Conference on Learning Theory (COLT)} ,
 year = {2016}
 
 } 
 
 @inproceedings{
li2020implicitadversarial,
title={Implicit Bias of Gradient Descent based Adversarial Training on Separable Data},
author={Yan Li and Ethan X.Fang and Huan Xu and Tuo Zhao},
booktitle={International Conference on Learning Representations (ICLR)},
year={2020},
}

@inproceedings{srebro2010smoothness,
 author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 title = {Smoothness, Low Noise and Fast Rates},
 year = {2010}
}


@article{chatterji2020linearnoise,
      title={Finite-sample analysis of interpolating linear classifiers in the overparameterized regime}, 
      author={Niladri S. Chatterji and Philip M. Long},
      year={2020},
      journal={arXiv:2004.12019}
}

@inproceedings{
hsu2021generalizationdistillation,
title={Generalization bounds via distillation},
author={Daniel Hsu and Ziwei Ji and Matus Telgarsky and Lan Wang},
booktitle={International Conference on Learning Representations (ICLR)},
year={2021}
}

@inproceedings{
wei2021selftraining,
title={Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
author={Colin Wei and Kendrick Shen and Yining Chen and Tengyu Ma},
booktitle={International Conference on Learning Representations (ICLR)},
year={2021}
}

@article{cai2021theory,
      title={A Theory of Label Propagation for Subpopulation Shift}, 
      author={Tianle Cai and Ruiqi Gao and Jason D. Lee and Qi Lei},
      year={2021},
      journal={Preprint, arXiv:2102.11203},
}

@inproceedings{zoufrei2021adversarial,
      title={Provable Robustness of Adversarial Training for Learning Halfspaces with Noise}, 
      author={Difan Zou and Spencer Frei and Quanquan Gu},
      year={2021},
      booktitle = {International Conference on Machine Learning (ICML)}

}

@inproceedings{frei2021twolayerhalfspace,
      title={Provable Generalization of SGD-trained Neural Networks of Any Width in the Presence of Adversarial Label Noise}, 
      author={Spencer Frei and Yuan Cao and Quanquan Gu},
      year={2021},
      booktitle = {International Conference on Machine Learning (ICML)}
}

@article{duchi2013distance,
  title={Distance-based and continuum Fano inequalities with applications to statistical estimation},
  author={Duchi, John C and Wainwright, Martin J},
  journal={arXiv preprint arXiv:1311.2669},
  year={2013}
}

@inproceedings{hinton2015distillation,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
booktitle	= {NeurIPS Deep Learning and Representation Learning Workshop}
}

@InProceedings{zou2019confidenceselftraining,
author = {Zou, Yang and Yu, Zhiding and Liu, Xiaofeng and Kumar, B.V.K. Vijaya and Wang, Jinsong},
title = {Confidence Regularized Self-Training},
booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
year = {2019}
} 


@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Conference of the North {A}merican Chapter of the Association for Computational Linguistics (NAACL)",
    year = "2019"
}

@inproceedings{chen2020simclr,
      title={A Simple Framework for Contrastive Learning of Visual Representations}, 
      author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
      year={2020},
      booktitle={International Conference on Machine Learning (ICML)}
}

@inproceedings{pham2021metapseudolabels,
title	= {Meta Pseudo Labels},
author	= {Hieu Pham and Zihang Dai and Qizhe Xie and Minh-Thang Luong and Quoc V. Le},
year	= {2021},
booktitle	= {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}

@inproceedings{rizve2021defensepseudolabel,
title={In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning},
author={Mamshad Nayeem Rizve and Kevin Duarte and Yogesh S Rawat and Mubarak Shah},
booktitle={International Conference on Learning Representations (ICLR)},
year={2021},
}

@inproceedings{lee2013pseudolabel,
    author = {Lee, Dong-Hyun},
    title = {Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
    year = {2013},
    booktitle={ICML Challenges in Representation Learning Workshop}
}

 @InProceedings{li2017minimaxgaussianmixture, 
 title = {{Minimax Gaussian Classification \& Clustering}}, 
 author = {Tianyang Li and Xinyang Yi and Constantine Carmanis and Pradeep Ravikumar}, 
 booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)}, 
 year = {2017}} 
 
 @article{schudder1965patternmachines,  
 author={Scudder, H.},  
 journal={IEEE Transactions on Information Theory},  
 title={Probability of error of some adaptive pattern-recognition machines},  
 year={1965},  volume={11},  number={3},  pages={363-371}}
 
 
@inproceedings{yarowsky1995unsupervised,
    title = "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods",
    author = "Yarowsky, David",
    booktitle = "Association for Computational Linguistics (ACL)",
    year = "1995",
}

@INPROCEEDINGS{bendavid2008unlabeledprovablyhelp,
    author = {Ben-David, Shai and Lu, Tyler and Pál, Dávid},
    title = {D.: Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning},
    booktitle = {Conference on Learning Theory (COLT)},
    year = {2008}
}

@inproceedings{singh2008unlabeleddatanowithelps,
 author = {Singh, Aarti and Nowak, Robert and Zhu, Jerry},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 title = {Unlabeled data: Now it helps, now it doesn\textquotesingle t},
 year = {2009}
}

@InProceedings{darnstdt2013unlabeleddatahelps,
  author =	{Malte Darnst{\"a}dt and Hans Ulrich Simon and Bal{\'a}zs Sz{\"o}r{\'e}nyi},
  title =	{{Unlabeled Data Does Provably Help}},
  booktitle =	{Symposium on Theoretical Aspects of Computer Science (STACS)},
  year =	{2013},
}

@article{balcan2010discriminative,
author = {Balcan, Maria-Florina and Blum, Avrim},
title = {A Discriminative Model for Semi-Supervised Learning},
year = {2010},
journal = {Journal of the ACM},
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{muller1997integral,
  title={Integral probability metrics and their generating classes of functions},
  author={M{\"u}ller, Alfred},
  journal={Advances in applied probability},
  volume={29},
  number={2},
  pages={429--443},
  year={1997},
  publisher={Cambridge University Press}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@InProceedings{gopfert2019unlabeleddatalr, 
 title = {When can unlabeled data improve the learning rate?}, 
 author = {G{\"o}pfert, Christina and Ben-David, Shai and Bousquet, Olivier and Gelly, Sylvain and Tolstikhin, Ilya and Urner, Ruth}, 
 booktitle = {Conference on Learning Theory (COLT)}, 
 year = {2019}
 } 
 
 
@article{lee2020predictingwhatyouknow,
      title={Predicting What You Already Know Helps: Provable Self-Supervised Learning}, 
      author={Jason D. Lee and Qi Lei and Nikunj Saunshi and Jiacheng Zhuo},
      year={2020},
      journal={Preprint, arXiv:2008.01064},
}

@inproceedings{cai2021labelpropagation,
      title={A Theory of Label Propagation for Subpopulation Shift}, 
      author={Tianle Cai and Ruiqi Gao and Jason D. Lee and Qi Lei},
      year={2021},
      booktitle = {International Conference on Machine Learning (ICML)}
}

@inproceedings{wei2021theoretical,
      title={Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data}, 
      author={Colin Wei and Kendrick Shen and Yining Chen and Tengyu Ma},
      year={2021},
      booktitle = {International Conference on Learning Representations (ICLR)}
}

@inproceedings{oymak2020selftraininginsights,
      title={Statistical and Algorithmic Insights for Semi-supervised Learning with Self-training}, 
      author={Samet Oymak and Talha Cihad Gulcu},
      year={2021},
      booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)}
}

@inproceedings{raghunathan2020mitigatingtradeoffrobustness,
      title={Understanding and Mitigating the Tradeoff Between Robustness and Accuracy}, 
      author={Aditi Raghunathan and Sang Michael Xie and Fanny Yang and John Duchi and Percy Liang},
      year={2020},
      booktitle = {International Conference on Machine Learning (ICML)}
}

@inproceedings{tosh2021contrastive,
      title={Contrastive learning, multi-view redundancy, and linear models}, 
      author={Christopher Tosh and Akshay Krishnamurthy and Daniel Hsu},
      year={2021},
      booktitle = {International Conference on Algorithmic Learning Theory (ALT)}
}

@inproceedings{lafferty2007semisupregression,
author = "John  Lafferty and Larry  Wasserman",
title = "{Statistical Analysis of Semi-Supervised Regression}",
year = "2007",
booktitle = "Advances in Neural Information Processing Systems (NeurIPS)"
}

@inproceedings{ratsaby1995mixturelabeledunlabeled,
author = {Ratsaby, Joel and Venkatesh, Santosh S.},
title = {Learning from a Mixture of Labeled and Unlabeled Examples with Parametric Side Information},
year = {1995},
booktitle = {Conference on Computational Learning Theory (COLT)}
}

@article{castelli1995exponentialvaluelabeled,
author = {Castelli, Vittorio and Cover, Thomas M.},
title = {On the Exponential Value of Labeled Samples},
year = {1995},
volume = {16},
number = {1},
pages = {105–111},
journal = {Pattern Recognition Letters}
}

@article{castelli1996relativevaluelabeled,
    author = {Vittorio Castelli and Thomas M Cover},
    title = {The relative value of labeled and unlabeled samples in pattern recognition in the regular parametric case,” in preparation},
    year = {1996},
    journal={IEEE Transactions on Information Theory}, 
     volume={42},  number={6},  pages={2102--2117}
}

@article{zhu2009semisupintro,
author = {Zhu, Xiaojin and Goldberg, Andrew B.},
title = {Introduction to Semi-Supervised Learning},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
volume = {3},
number = {1},
pages = {1-130},
year = {2009}
}




@book{chapelle2010semisupbook,
author = {Chapelle, Olivier and Schlkopf, Bernhard and Zien, Alexander},
title = {Semi-Supervised Learning},
year = {2010},
publisher = {The MIT Press},
edition = {1st},
}

@inproceedings{kumar2020selftrainingradualdomain,
      title={Understanding Self-Training for Gradual Domain Adaptation}, 
      author={Ananya Kumar and Tengyu Ma and Percy Liang},
      year={2020},
      booktitle = {International Conference on Machine Learning (ICML)}
}

@inproceedings{chen2020selftrainingavoidspurious,
      title={Self-training Avoids Using Spurious Features Under Domain Shift}, 
      author={Yining Chen and Colin Wei and Ananya Kumar and Tengyu Ma},
      year={2020},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{xie2021innout,
      title={In-N-Out: Pre-Training and Self-Training using Auxiliary Information for Out-of-Distribution Robustness}, 
      author={Sang Michael Xie and Ananya Kumar and Robbie Jones and Fereshte Khani and Tengyu Ma and Percy Liang},
      year={2021},
      booktitle={International Conference on Learning Representations (ICLR)}
}

@inproceedings{frei2021proxy,
      title={Proxy Convexity: A Unified Framework for the Analysis of Neural Networks Trained by Gradient Descent}, 
      author={Spencer Frei and Quanquan Gu},
      year={2021},
      booktitle = {Advances in Neural Information Processing Systems (NeurIPS)}
}


@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{hu2020strategies,
  title={Strategies For Pre-training Graph Neural Networks},
  author={Hu, W and Liu, B and Gomes, J and Zitnik, M and Liang, P and Pande, V and Leskovec, J},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{frei2022self,
  title={Self-training converts weak learners to strong learners in mixture models},
  author={Frei, Spencer and Zou, Difan and Chen, Zixiang and Gu, Quanquan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={8003--8021},
  year={2022},
  organization={PMLR}
}

@article{berthelot2019mixmatch,
  title={Mixmatch: A holistic approach to semi-supervised learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin A},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{laine2016temporal,
  title={Temporal ensembling for semi-supervised learning},
  author={Laine, Samuli and Aila, Timo},
  journal={arXiv preprint arXiv:1610.02242},
  year={2016}
}

@article{liu2021self,
  title={Self-supervised learning: Generative or contrastive},
  author={Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
  journal={IEEE transactions on knowledge and data engineering},
  volume={35},
  number={1},
  pages={857--876},
  year={2021},
  publisher={IEEE}
}

@article{jaiswal2020survey,
  title={A survey on contrastive self-supervised learning},
  author={Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
  journal={Technologies},
  volume={9},
  number={1},
  pages={2},
  year={2020},
  publisher={MDPI}
}

@article{deng2023phygcn,
  title={PhyGCN: Pre-trained Hypergraph Convolutional Neural Networks with Self-supervised Learning},
  author={Deng, Yihe and Zhang, Ruochi and Xu, Pan and Ma, Jian and Gu, Quanquan},
  journal={bioRxiv},
  pages={2023--10},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}
@misc{open-llm-leaderboard,
  author = {Edward Beeching and Clémentine Fourrier and Nathan Habib and Sheon Han and Nathan Lambert and Nazneen Rajani and Omar Sanseviero and Lewis Tunstall and Thomas Wolf},
  title = {Open LLM Leaderboard},
  year = {2023},
  publisher = {Hugging Face}
}


@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}


@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{schapire1990strength,
  title={The strength of weak learnability},
  author={Schapire, Robert E},
  journal={Machine learning},
  volume={5},
  pages={197--227},
  year={1990},
  publisher={Springer}
}

@article{freund1995boosting,
  title={Boosting a weak learning algorithm by majority},
  author={Freund, Yoav},
  journal={Information and computation},
  volume={121},
  number={2},
  pages={256--285},
  year={1995},
  publisher={Elsevier}
}

@article{kearns1994cryptographic,
  title={Cryptographic limitations on learning boolean formulae and finite automata},
  author={Kearns, Michael and Valiant, Leslie},
  journal={Journal of the ACM (JACM)},
  volume={41},
  number={1},
  pages={67--95},
  year={1994},
  publisher={ACM New York, NY, USA}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of computer and system sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{kou2022does,
  title={How Does Semi-supervised Learning with Pseudo-labelers Work? A Case Study},
  author={Kou, Yiwen and Chen, Zixiang and Cao, Yuan and Gu, Quanquan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1999},
  publisher={Springer science \& business media}
}

@article{grandvalet2004semi,
  title={Semi-supervised learning by entropy minimization},
  author={Grandvalet, Yves and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}

@misc{cui2023ultrafeedback,
      title={UltraFeedback: Boosting Language Models with High-quality Feedback},
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cheng2023adversarial,
      title={Adversarial Preference Optimization}, 
      author={Pengyu Cheng and Yifan Yang and Jian Li and Yong Dai and Nan Du},
      year={2023},
      eprint={2311.08045},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{xu2023some,
  title={Some things are more CRINGE than others: Preference Optimization with the Pairwise Cringe Loss},
  author={Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2312.16682},
  year={2023}
}