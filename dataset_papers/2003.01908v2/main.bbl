\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye \& Carlini(2018)Athalye and Carlini]{athalye2018robustness}
Athalye, A. and Carlini, N.
\newblock On the robustness of the cvpr 2018 white-box adversarial example
  defenses.
\newblock \emph{arXiv preprint arXiv:1804.03286}, 2018.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Athalye, A., Carlini, N., and Wagner, D.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock \emph{arXiv preprint arXiv:1802.00420}, 2018.

\bibitem[Cao \& Gong(2017)Cao and Gong]{cao2017mitigating}
Cao, X. and Gong, N.~Z.
\newblock Mitigating evasion attacks to deep neural networks via region-based
  classification.
\newblock In \emph{Proceedings of the 33rd Annual Computer Security
  Applications Conference}, pp.\  278--287. ACM, 2017.

\bibitem[Carlini \& Wagner(2017{\natexlab{a}})Carlini and
  Wagner]{carlini2017adversarial}
Carlini, N. and Wagner, D.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pp.\  3--14. ACM, 2017{\natexlab{a}}.

\bibitem[Carlini \& Wagner(2017{\natexlab{b}})Carlini and
  Wagner]{carlini2017towards}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 IEEE Symposium on Security and Privacy (SP)}, pp.\
  39--57. IEEE, 2017{\natexlab{b}}.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{cohen2019certified}
Cohen, J.~M., Rosenfeld, E., and Kolter, J.~Z.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock \emph{arXiv preprint arXiv:1902.02918}, 2019.

\bibitem[Croce et~al.(2018)Croce, Andriushchenko, and Hein]{croce2018provable}
Croce, F., Andriushchenko, M., and Hein, M.
\newblock Provable robustness of relu networks via maximization of linear
  regions.
\newblock \emph{arXiv preprint arXiv:1810.07481}, 2018.

\bibitem[Dvijotham et~al.(2018{\natexlab{a}})Dvijotham, Gowal, Stanforth,
  Arandjelovic, O'Donoghue, Uesato, and Kohli]{dvijotham2018training}
Dvijotham, K., Gowal, S., Stanforth, R., Arandjelovic, R., O'Donoghue, B.,
  Uesato, J., and Kohli, P.
\newblock Training verified learners with learned verifiers.
\newblock \emph{arXiv preprint arXiv:1805.10265}, 2018{\natexlab{a}}.

\bibitem[Dvijotham et~al.(2018{\natexlab{b}})Dvijotham, Stanforth, Gowal, Mann,
  and Kohli]{dvijotham2018dual}
Dvijotham, K., Stanforth, R., Gowal, S., Mann, T., and Kohli, P.
\newblock A dual approach to scalable verification of deep networks.
\newblock \emph{UAI}, 2018{\natexlab{b}}.

\bibitem[Dvijotham et~al.(2020)Dvijotham, Hayes, Balle, Kolter, Qin, Gyorgy,
  Xiao, Gowal, and Kohli]{dvijotham2020a}
Dvijotham, K.~D., Hayes, J., Balle, B., Kolter, Z., Qin, C., Gyorgy, A., Xiao,
  K., Gowal, S., and Kohli, P.
\newblock A framework for robustness certification of smoothed classifiers
  using f-divergences.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Gehr et~al.(2018)Gehr, Mirman, Drachsler-Cohen, Tsankov, Chaudhuri,
  and Vechev]{gehr2018ai2}
Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., and
  Vechev, M.
\newblock Ai2: Safety and robustness certification of neural networks with
  abstract interpretation.
\newblock In \emph{2018 IEEE Symposium on Security and Privacy (SP)}, pp.\
  3--18. IEEE, 2018.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{ICLR}, 2015.

\bibitem[Gowal et~al.(2018)Gowal, Dvijotham, Stanforth, Bunel, Qin, Uesato,
  Mann, and Kohli]{gowal2018effectiveness}
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin, C., Uesato, J., Mann,
  T., and Kohli, P.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock \emph{arXiv preprint arXiv:1810.12715}, 2018.

\bibitem[Gu \& Rigazio(2014)Gu and Rigazio]{gu2014towards}
Gu, S. and Rigazio, L.
\newblock Towards deep neural network architectures robust to adversarial
  examples.
\newblock \emph{arXiv preprint arXiv:1412.5068}, 2014.

\bibitem[Guo et~al.(2017)Guo, Rana, Cisse, and Van
  Der~Maaten]{guo2017countering}
Guo, C., Rana, M., Cisse, M., and Van Der~Maaten, L.
\newblock Countering adversarial images using input transformations.
\newblock \emph{arXiv preprint arXiv:1711.00117}, 2017.

\bibitem[Guo et~al.(2019)Guo, Gardner, You, Gordon~Wilson, and
  Q.~Weinberger]{guo2019simple}
Guo, C., Gardner, J.~R., You, Y., Gordon~Wilson, A., and Q.~Weinberger, K.
\newblock Simple black-box adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1905.07121}, 2019.

\bibitem[Gupta \& Rahtu(2019)Gupta and Rahtu]{Gupta_2019_ICCV}
Gupta, P. and Rahtu, E.
\newblock Ciidefence: Defeating adversarial attacks by fusing class-specific
  image inpainting and image denoising.
\newblock In \emph{The IEEE International Conference on Computer Vision
  (ICCV)}, October 2019.

\bibitem[Hu et~al.(2020)Hu, Swaminathan, Salman, and Yang]{hu2020improved}
Hu, J.~E., Swaminathan, A., Salman, H., and Yang, G.
\newblock Improved image wasserstein attacks and defenses.
\newblock \emph{arXiv preprint arXiv:2004.12478}, 2020.

\bibitem[Ilyas et~al.(2018)Ilyas, Engstrom, Athalye, and
  Lin]{pmlr-v80-ilyas18a}
Ilyas, A., Engstrom, L., Athalye, A., and Lin, J.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  2137--2146, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial machine learning at scale.
\newblock \emph{arXiv preprint arXiv:1611.01236}, 2016.

\bibitem[Laidlaw \& Feizi(2019)Laidlaw and Feizi]{laidlaw2019functional}
Laidlaw, C. and Feizi, S.
\newblock Functional adversarial attacks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10408--10418, 2019.

\bibitem[Lecuyer et~al.(2018)Lecuyer, Atlidakis, Geambasu, Hsu, and
  Jana]{lecuyer2018certified}
Lecuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock \emph{arXiv preprint arXiv:1802.03471}, 2018.

\bibitem[Lee et~al.(2019)Lee, Yuan, Chang, and Jaakkola]{NIPS2019_8737}
Lee, G.-H., Yuan, Y., Chang, S., and Jaakkola, T.
\newblock Tight certificates of adversarial robustness for randomly smoothed
  classifiers.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  4911--4922, 2019.

\bibitem[Li et~al.(2018)Li, Chen, Wang, and Carin]{li2018second}
Li, B., Chen, C., Wang, W., and Carin, L.
\newblock Second-order adversarial attack and certifiable robustness.
\newblock \emph{arXiv preprint arXiv:1809.03113}, 2018.

\bibitem[Li et~al.(2019)Li, Chen, Wang, and Carin]{NIPS2019_9143}
Li, B., Chen, C., Wang, W., and Carin, L.
\newblock Certified adversarial robustness with additive noise.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  9459--9469, 2019.

\bibitem[Liao et~al.(2018)Liao, Liang, Dong, Pang, Hu, and Zhu]{Liao_2018_CVPR}
Liao, F., Liang, M., Dong, Y., Pang, T., Hu, X., and Zhu, J.
\newblock Defense against adversarial attacks using high-level representation
  guided denoiser.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2018.

\bibitem[Liu et~al.(2018)Liu, Cheng, Zhang, and Hsieh]{liu2018towards}
Liu, X., Cheng, M., Zhang, H., and Hsieh, C.-J.
\newblock Towards robust neural networks via random self-ensemble.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  369--385, 2018.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Meng \& Chen(2017)Meng and Chen]{meng2017ccs}
Meng, D. and Chen, H.
\newblock Magnet: a two-pronged defense against adversarial examples.
\newblock In \emph{Conference on Computer and Communications Security (CCS)},
  2017.

\bibitem[Mirman et~al.(2018)Mirman, Gehr, and Vechev]{mirman2018differentiable}
Mirman, M., Gehr, T., and Vechev, M.
\newblock Differentiable abstract interpretation for provably robust neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3575--3583, 2018.

\bibitem[Prakash et~al.(2018)Prakash, Moran, Garber, DiLillo, and
  Storer]{Prakash_2018_CVPR}
Prakash, A., Moran, N., Garber, S., DiLillo, A., and Storer, J.
\newblock Deflecting adversarial attacks with pixel deflection.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2018.

\bibitem[Raghunathan et~al.(2018{\natexlab{a}})Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
Raghunathan, A., Steinhardt, J., and Liang, P.
\newblock Certified defenses against adversarial examples.
\newblock \emph{International Conference on Learning Representations (ICLR),
  arXiv preprint arXiv:1801.09344}, 2018{\natexlab{a}}.

\bibitem[Raghunathan et~al.(2018{\natexlab{b}})Raghunathan, Steinhardt, and
  Liang]{raghunathan2018semidefinite}
Raghunathan, A., Steinhardt, J., and Liang, P.~S.
\newblock Semidefinite relaxations for certifying robustness to adversarial
  examples.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10877--10887, 2018{\natexlab{b}}.

\bibitem[Salman et~al.(2019{\natexlab{a}})Salman, Li, Razenshteyn, Zhang,
  Zhang, Bubeck, and Yang]{salman2019provably}
Salman, H., Li, J., Razenshteyn, I., Zhang, P., Zhang, H., Bubeck, S., and
  Yang, G.
\newblock Provably robust deep learning via adversarially trained smoothed
  classifiers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11289--11300, 2019{\natexlab{a}}.

\bibitem[Salman et~al.(2019{\natexlab{b}})Salman, Yang, Zhang, Hsieh, and
  Zhang]{salman2019convex}
Salman, H., Yang, G., Zhang, H., Hsieh, C.-J., and Zhang, P.
\newblock A convex relaxation barrier to tight robustness verification of
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9832--9842, 2019{\natexlab{b}}.

\bibitem[Singh et~al.(2018)Singh, Gehr, Mirman, P{\"u}schel, and
  Vechev]{singh2018fast}
Singh, G., Gehr, T., Mirman, M., P{\"u}schel, M., and Vechev, M.
\newblock Fast and effective robustness certification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10825--10836, 2018.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Tai et~al.(2017)Tai, Yang, Liu, and Xu]{tai2017memnet}
Tai, Y., Yang, J., Liu, X., and Xu, C.
\newblock Memnet: A persistent memory network for image restoration.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  4539--4547, 2017.

\bibitem[Tramèr et~al.(2017)Tramèr, Papernot, Goodfellow, Boneh, and
  McDaniel]{tramer2017space}
Tramèr, F., Papernot, N., Goodfellow, I., Boneh, D., and McDaniel, P.
\newblock The space of transferable adversarial examples.
\newblock \emph{arXiv preprint arXiv:1704.03453}, 2017.

\bibitem[Uesato et~al.(2018)Uesato, O'Donoghue, Oord, and
  Kohli]{uesato2018adversarial}
Uesato, J., O'Donoghue, B., Oord, A. v.~d., and Kohli, P.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock \emph{arXiv preprint arXiv:1802.05666}, 2018.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Chen, Abdou, and
  Jana]{wang2018mixtrain}
Wang, S., Chen, Y., Abdou, A., and Jana, S.
\newblock Mixtrain: Scalable training of formally robust neural networks.
\newblock \emph{arXiv preprint arXiv:1811.02625}, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Pei, Whitehouse, Yang, and
  Jana]{wang2018efficient}
Wang, S., Pei, K., Whitehouse, J., Yang, J., and Jana, S.
\newblock Efficient formal safety analysis of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6369--6379, 2018{\natexlab{b}}.

\bibitem[Warren et~al.(2017)Warren, Wei, Chen, Carlini, and
  Song]{he2017ensemble}
Warren, H., Wei, J., Chen, X., Carlini, N., and Song, D.
\newblock Adversarial example defenses: Ensembles of weak defenses are not
  strong.
\newblock \emph{arXiv preprint arXiv:1706.04701}, 2017.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Song, Hsieh, Boning, Dhillon, and
  Daniel]{weng2018towards}
Weng, T.-W., Zhang, H., Chen, H., Song, Z., Hsieh, C.-J., Boning, D., Dhillon,
  I.~S., and Daniel, L.
\newblock Towards fast computation of certified robustness for {ReLU} networks.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Wong \& Kolter(2018)Wong and Kolter]{wong2018provable}
Wong, E. and Kolter, Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  5283--5292, 2018.

\bibitem[Wong et~al.(2018)Wong, Schmidt, Metzen, and Kolter]{wong2018scaling}
Wong, E., Schmidt, F., Metzen, J.~H., and Kolter, J.~Z.
\newblock Scaling provable adversarial defenses.
\newblock \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Wong et~al.(2019)Wong, Schmidt, and Kolter]{wong2019wasserstein}
Wong, E., Schmidt, F.~R., and Kolter, J.~Z.
\newblock Wasserstein adversarial examples via projected sinkhorn iterations.
\newblock \emph{arXiv preprint arXiv:1902.07906}, 2019.

\bibitem[Xie et~al.(2019)Xie, Wu, Maaten, Yuille, and He]{Xie_2019_CVPR}
Xie, C., Wu, Y., Maaten, L. v.~d., Yuille, A.~L., and He, K.
\newblock Feature denoising for improving adversarial robustness.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2019.

\bibitem[Xu et~al.(2018)Xu, Evans, and Qi]{xu2018feature}
Xu, W., Evans, D., and Qi, Y.
\newblock Feature squeezing: Detecting adversarial examples in deep neural
  networks.
\newblock In \emph{25th Annual Network and Distributed System Security
  Symposium, {NDSS}}, 2018.

\bibitem[Yang et~al.(2020)Yang, Duan, Hu, Salman, Razenshteyn, and
  Li]{yang2020randomized}
Yang, G., Duan, T., Hu, J.~E., Salman, H., Razenshteyn, I., and Li, J.
\newblock Randomized smoothing of all shapes and sizes, 2020.

\bibitem[Zhai et~al.(2020)Zhai, Dan, He, Zhang, Gong, Ravikumar, Hsieh, and
  Wang]{Zhai2020MACER:}
Zhai, R., Dan, C., He, D., Zhang, H., Gong, B., Ravikumar, P., Hsieh, C.-J.,
  and Wang, L.
\newblock Macer: Attack-free and scalable robust training via maximizing
  certified radius.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Weng, Chen, Hsieh, and
  Daniel]{zhang2018efficient}
Zhang, H., Weng, T.-W., Chen, P.-Y., Hsieh, C.-J., and Daniel, L.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4939--4948, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2017)Zhang, Zuo, Chen, Meng, and Zhang]{zhang2017beyond}
Zhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L.
\newblock Beyond a {Gaussian} denoiser: Residual learning of deep {CNN} for
  image denoising.
\newblock \emph{IEEE Transactions on Image Processing}, 26\penalty0
  (7):\penalty0 3142--3155, 2017.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Zuo, and
  Zhang]{zhang2018ffdnet}
Zhang, K., Zuo, W., and Zhang, L.
\newblock Ffdnet: Toward a fast and flexible solution for {CNN} based image
  denoising.
\newblock \emph{IEEE Transactions on Image Processing}, 2018{\natexlab{b}}.

\bibitem[Zheng et~al.(2016)Zheng, Song, Leung, and
  Goodfellow]{zheng2016improving}
Zheng, S., Song, Y., Leung, T., and Goodfellow, I.
\newblock Improving the robustness of deep neural networks via stability
  training.
\newblock In \emph{Proceedings of the ieee conference on computer vision and
  pattern recognition}, pp.\  4480--4488, 2016.

\end{thebibliography}
