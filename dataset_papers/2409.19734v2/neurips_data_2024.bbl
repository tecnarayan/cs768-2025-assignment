\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[pex()]{pexels}
Pexels.
\newblock \url{https://www.pexels.com/}.

\bibitem[AI@Meta(2024)]{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023stable}
A.~Blattmann, T.~Dockhorn, S.~Kulal, D.~Mendelevitch, M.~Kilian, D.~Lorenz, Y.~Levi, Z.~English, V.~Voleti, A.~Letts, et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Chang et~al.(2023)Chang, Yeh, Chiu, and Yu]{chang2023antifakeprompt}
Y.-M. Chang, C.~Yeh, W.-C. Chiu, and N.~Yu.
\newblock Antifakeprompt: Prompt-tuned vision-language models are fake image detectors.
\newblock \emph{arXiv preprint arXiv:2310.17419}, 2023.

\bibitem[Chen et~al.(2024)Chen, Zhang, Cun, Xia, Wang, Weng, and Shan]{chen2024videocrafter2}
H.~Chen, Y.~Zhang, X.~Cun, M.~Xia, X.~Wang, C.~Weng, and Y.~Shan.
\newblock Videocrafter2: Overcoming data limitations for high-quality video diffusion models.
\newblock \emph{arXiv preprint arXiv:2401.09047}, 2024.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, et~al.]{chiang2023vicuna}
W.-L. Chiang, Z.~Li, Z.~Lin, Y.~Sheng, Z.~Wu, H.~Zhang, L.~Zheng, S.~Zhuang, Y.~Zhuang, J.~E. Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality.
\newblock \emph{See https://vicuna. lmsys. org (accessed 14 April 2023)}, 2023.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scalingflant5}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang, M.~Dehghani, S.~Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Crone et~al.(2018)Crone, Bode, Murawski, and Laham]{crone2018socio}
D.~L. Crone, S.~Bode, C.~Murawski, and S.~M. Laham.
\newblock The socio-moral image database (smid): A novel stimulus set for the study of social, moral and affective processes.
\newblock \emph{PloS one}, 13\penalty0 (1):\penalty0 e0190954, 2018.

\bibitem[Dai et~al.()Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi]{instructblipgithub}
W.~Dai, J.~Li, D.~Li, A.~M.~H. Tiong, J.~Zhao, W.~Wang, B.~Li, P.~Fung, and S.~Hoi.
\newblock Lavis instructblip github page.
\newblock \url{https://github.com/salesforce/LAVIS/tree/main/projects/instructblip}.

\bibitem[Dai et~al.(2024)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi]{dai2024instructblip}
W.~Dai, J.~Li, D.~Li, A.~M.~H. Tiong, J.~Zhao, W.~Wang, B.~Li, P.~N. Fung, and S.~Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Eungyeom~Ha()]{hodgithub}
D.~N. Eungyeom~Ha, Heemook~Kim.
\newblock Hod github page.
\newblock \url{https://github.com/poori-nuna/HOD-Benchmark-Dataset/blob/main/codes/HOD_YOLOv5_all.ipynb}.

\bibitem[Fabbrizzi et~al.(2022)Fabbrizzi, Papadopoulos, Ntoutsi, and Kompatsiaris]{fabbrizzi2022survey}
S.~Fabbrizzi, S.~Papadopoulos, E.~Ntoutsi, and I.~Kompatsiaris.
\newblock A survey on bias in visual datasets.
\newblock \emph{Computer Vision and Image Understanding}, 223:\penalty0 103552, 2022.

\bibitem[Goyal et~al.(2017)Goyal, Khot, Summers-Stay, Batra, and Parikh]{goyal2017making}
Y.~Goyal, T.~Khot, D.~Summers-Stay, D.~Batra, and D.~Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6904--6913, 2017.

\bibitem[Guo et~al.(2023)Guo, Yang, Rao, Agrawala, Lin, and Dai]{guo2023sparsectrl}
Y.~Guo, C.~Yang, A.~Rao, M.~Agrawala, D.~Lin, and B.~Dai.
\newblock Sparsectrl: Adding sparse controls to text-to-video diffusion models.
\newblock \emph{arXiv preprint arXiv:2311.16933}, 2023.

\bibitem[Ha et~al.(2024)Ha, Kim, and Na]{ha2024hod}
E.~Ha, H.~Kim, and D.~Na.
\newblock Hod: New harmful object detection benchmarks for robust surveillance.
\newblock In \emph{WACV}, pages 183--192, 2024.

\bibitem[Henschel et~al.(2024)Henschel, Khachatryan, Hayrapetyan, Poghosyan, Tadevosyan, Wang, Navasardyan, and Shi]{henschel2024streamingt2v}
R.~Henschel, L.~Khachatryan, D.~Hayrapetyan, H.~Poghosyan, V.~Tadevosyan, Z.~Wang, S.~Navasardyan, and H.~Shi.
\newblock Streamingt2v: Consistent, dynamic, and extendable long video generation from text.
\newblock \emph{arXiv preprint arXiv:2403.14773}, 2024.

\bibitem[HiveAI({\natexlab{a}})]{hiveAI}
HiveAI.
\newblock Hive ai, {\natexlab{a}}.
\newblock \url{https://thehive.ai/}.

\bibitem[HiveAI({\natexlab{b}})]{hiveaiVisualModeration}
HiveAI.
\newblock Visual moderation service of hive ai., {\natexlab{b}}.
\newblock \url{https://hivemoderation.com/}.

\bibitem[Li et~al.(2024)Li, Zhang, Zhang, Guo, Zhang, Li, Zhang, Liu, and Li]{li2024llavanext-strong}
B.~Li, K.~Zhang, H.~Zhang, D.~Guo, R.~Zhang, F.~Li, Y.~Zhang, Z.~Liu, and C.~Li.
\newblock Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024.
\newblock URL \url{https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/}.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{li2022blip}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In \emph{International conference on machine learning}, pages 12888--12900. PMLR, 2022.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{International conference on machine learning}, pages 19730--19742. PMLR, 2023.

\bibitem[Liu et~al.(2023)Liu, Lin, Li, Wang, Yacoob, and Wang]{liu2023aligning}
F.~Liu, K.~Lin, L.~Li, J.~Wang, Y.~Yacoob, and L.~Wang.
\newblock Aligning large multi-modal model with robust instruction tuning.
\newblock \emph{arXiv preprint arXiv:2306.14565}, 2023.

\bibitem[Liu et~al.(2024)Liu, Li, Wu, and Lee]{liu2024visual}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2022)Lu, Mishra, Xia, Qiu, Chang, Zhu, Tafjord, Clark, and Kalyan]{lu2022learn}
P.~Lu, S.~Mishra, T.~Xia, L.~Qiu, K.-W. Chang, S.-C. Zhu, O.~Tafjord, P.~Clark, and A.~Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 2507--2521, 2022.

\bibitem[LUKAN()]{visualDataPercentageSource}
E.~LUKAN.
\newblock 50 video statistics you canâ€™t ignore in 2024.
\newblock \url{https://www.synthesia.io/post/video-statistics}.

\bibitem[Marino et~al.(2019)Marino, Rastegari, Farhadi, and Mottaghi]{marino2019ok}
K.~Marino, M.~Rastegari, A.~Farhadi, and R.~Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external knowledge.
\newblock In \emph{Proceedings of the IEEE/cvf conference on computer vision and pattern recognition}, pages 3195--3204, 2019.

\bibitem[Mazeika et~al.(2024)Mazeika, Phan, Yin, Zou, Wang, Mu, Sakhaee, Li, Basart, Li, et~al.]{mazeika2024harmbench}
M.~Mazeika, L.~Phan, X.~Yin, A.~Zou, Z.~Wang, N.~Mu, E.~Sakhaee, N.~Li, S.~Basart, B.~Li, et~al.
\newblock Harmbench: A standardized evaluation framework for automated red teaming and robust refusal.
\newblock \emph{arXiv preprint arXiv:2402.04249}, 2024.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min2022rethinking}
S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock \emph{arXiv preprint arXiv:2202.12837}, 2022.

\bibitem[Mishra et~al.(2019)Mishra, Shekhar, Singh, and Chakraborty]{mishra2019ocr}
A.~Mishra, S.~Shekhar, A.~K. Singh, and A.~Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In \emph{2019 international conference on document analysis and recognition (ICDAR)}, pages 947--952. IEEE, 2019.

\bibitem[Motoki et~al.(2024)Motoki, Pinho~Neto, and Rodrigues]{motoki2024more}
F.~Motoki, V.~Pinho~Neto, and V.~Rodrigues.
\newblock More human than human: Measuring chatgpt political bias.
\newblock \emph{Public Choice}, 198\penalty0 (1):\penalty0 3--23, 2024.

\bibitem[notAI tech(2021)]{nudenet}
notAI tech.
\newblock Nudenet, 2021.
\newblock \url{https://github.com/notAI-tech/NudeNet}.

\bibitem[Olmos et~al.(2018)Olmos, Tabik, and Herrera]{olmos2018automatic}
R.~Olmos, S.~Tabik, and F.~Herrera.
\newblock Automatic handgun detection alarm in videos using deep learning.
\newblock \emph{Neurocomputing}, 275:\penalty0 66--72, 2018.

\bibitem[OpenAI(2023{\natexlab{a}})]{gpt4v}
OpenAI.
\newblock Gpt-4v, 2023{\natexlab{a}}.
\newblock \url{https://cdn.openai.com/papers/GPTV_System_Card.pdf}.

\bibitem[OpenAI(2023{\natexlab{b}})]{gpt4vpreview}
OpenAI.
\newblock gpt-4-1106-vision-preview., 2023{\natexlab{b}}.
\newblock \url{https://openai.com/index/new-models-and-developer-products-announced-at-devday/}.

\bibitem[Podell et~al.(2023)Podell, English, Lacey, Blattmann, Dockhorn, M{\"u}ller, Penna, and Rombach]{podell2023sdxl}
D.~Podell, Z.~English, K.~Lacey, A.~Blattmann, T.~Dockhorn, J.~M{\"u}ller, J.~Penna, and R.~Rombach.
\newblock Sdxl: Improving latent diffusion models for high-resolution image synthesis.
\newblock \emph{arXiv preprint arXiv:2307.01952}, 2023.

\bibitem[Qing et~al.(2023)Qing, Zhang, Wang, Wang, Wei, Zhang, Gao, and Sang]{qing2023hierarchical}
Z.~Qing, S.~Zhang, J.~Wang, X.~Wang, Y.~Wei, Y.~Zhang, C.~Gao, and N.~Sang.
\newblock Hierarchical spatio-temporal decoupling for text-to-video generation.
\newblock \emph{arXiv preprint arXiv:2312.04483}, 2023.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
R.~Rafailov, A.~Sharma, E.~Mitchell, C.~D. Manning, S.~Ermon, and C.~Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Redmon et~al.(2016)Redmon, Divvala, Girshick, and Farhadi]{redmon2016yolo}
J.~Redmon, S.~Divvala, R.~Girshick, and A.~Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 779--788, 2016.

\bibitem[Schramowski et~al.()Schramowski, Tauchmann, and Kersting]{q16}
P.~Schramowski, C.~Tauchmann, and K.~Kersting.
\newblock Q16 official github page.
\newblock \url{https://github.com/ml-research/Q16}.

\bibitem[Schramowski et~al.(2022)Schramowski, Tauchmann, and Kersting]{schramowski2022can}
P.~Schramowski, C.~Tauchmann, and K.~Kersting.
\newblock Can machines help us answering question 16 in datasheets, and in turn reflecting on inappropriate content?
\newblock In \emph{Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency}, pages 1350--1361, 2022.

\bibitem[Schramowski et~al.(2023)Schramowski, Brack, Deiseroth, and Kersting]{schramowski2023safe}
P.~Schramowski, M.~Brack, B.~Deiseroth, and K.~Kersting.
\newblock Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models.
\newblock In \emph{CVPR}, pages 22522--22531, 2023.

\bibitem[Singh et~al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh, and Rohrbach]{singh2019towards}
A.~Singh, V.~Natarajan, M.~Shah, Y.~Jiang, X.~Chen, D.~Batra, D.~Parikh, and M.~Rohrbach.
\newblock Towards vqa models that can read.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Wang et~al.()Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, Song, Xu, Xu, Li, Dong, Ding, and Tang]{cogvlmCkpt}
W.~Wang, Q.~Lv, W.~Yu, W.~Hong, J.~Qi, Y.~Wang, J.~Ji, Z.~Yang, L.~Zhao, X.~Song, J.~Xu, B.~Xu, J.~Li, Y.~Dong, M.~Ding, and J.~Tang.
\newblock Model checkpoints of cogvlm.
\newblock \url{https://github.com/THUDM/CogVLM/#model-checkpoints}.

\bibitem[Wang et~al.(2023)Wang, Lv, Yu, Hong, Qi, Wang, Ji, Yang, Zhao, Song, et~al.]{wang2023cogvlm}
W.~Wang, Q.~Lv, W.~Yu, W.~Hong, J.~Qi, Y.~Wang, J.~Ji, Z.~Yang, L.~Zhao, X.~Song, et~al.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock \emph{arXiv preprint arXiv:2311.03079}, 2023.

\bibitem[Wu et~al.(2023)Wu, Bansal, Zhang, Wu, Li, Zhu, Jiang, Zhang, Zhang, Liu, Awadallah, White, Burger, and Wang]{wu2023autogen}
Q.~Wu, G.~Bansal, J.~Zhang, Y.~Wu, B.~Li, E.~Zhu, L.~Jiang, X.~Zhang, S.~Zhang, J.~Liu, A.~H. Awadallah, R.~W. White, D.~Burger, and C.~Wang.
\newblock Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
\newblock 2023.

\bibitem[Ye et~al.(2023)Ye, Xu, Ye, Yan, Liu, Qian, Zhang, Huang, and Zhou]{ye2023mplug}
Q.~Ye, H.~Xu, J.~Ye, M.~Yan, H.~Liu, Q.~Qian, J.~Zhang, F.~Huang, and J.~Zhou.
\newblock mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.
\newblock \emph{arXiv preprint arXiv:2311.04257}, 2023.

\bibitem[Yeh et~al.(2023)Yeh, Chi, Lian, and Hsieh]{yeh2023evaluating}
K.-C. Yeh, J.-A. Chi, D.-C. Lian, and S.-K. Hsieh.
\newblock Evaluating interfaced llm bias.
\newblock In \emph{Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023)}, pages 292--299, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Zhang, Gu, Zhou, Lipka, Yang, and Sun]{zhang2023llavar}
Y.~Zhang, R.~Zhang, J.~Gu, Y.~Zhou, N.~Lipka, D.~Yang, and T.~Sun.
\newblock Llavar: Enhanced visual instruction tuning for text-rich image understanding.
\newblock \emph{arXiv preprint arXiv:2306.17107}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Li, Liu, Lee, Gui, Fu, Feng, Liu, and Li]{zhang2024llavanextvideo}
Y.~Zhang, B.~Li, h.~Liu, Y.~j. Lee, L.~Gui, D.~Fu, J.~Feng, Z.~Liu, and C.~Li.
\newblock Llava-next: A strong zero-shot video understanding model, April 2024.
\newblock URL \url{https://llava-vl.github.io/blog/2024-04-30-llava-next-video/}.

\end{thebibliography}
