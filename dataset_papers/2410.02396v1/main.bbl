\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2023)Ainsworth, Hayase, and Srinivasa]{ainsworth2022git}
S.~K. Ainsworth, J.~Hayase, and S.~Srinivasa.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Alm et~al.(2005)Alm, Roth, and Sproat]{alm2005emotions}
C.~O. Alm, D.~Roth, and R.~Sproat.
\newblock Emotions from text: machine learning for text-based emotion prediction.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 579--586, 2005.

\bibitem[Arpit et~al.(2022)Arpit, Wang, Zhou, and Xiong]{arpit2022ensemble}
D.~Arpit, H.~Wang, Y.~Zhou, and C.~Xiong.
\newblock Ensemble of averages: Improving model selection and boosting performance in domain generalization.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 8265--8277, 2022.

\bibitem[Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma, Kim, Bari, Fevry, et~al.]{bach2022promptsource}
S.~H. Bach, V.~Sanh, Z.-X. Yong, A.~Webson, C.~Raffel, N.~V. Nayak, A.~Sharma, T.~Kim, M.~S. Bari, T.~Fevry, et~al.
\newblock Promptsource: An integrated development environment and repository for natural language prompts.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)}, 2022.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
R.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S. Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards, Y.~Burda, N.~Joseph, G.~Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Cheng et~al.(2017)Cheng, Han, and Lu]{cheng2017remote}
G.~Cheng, J.~Han, and X.~Lu.
\newblock Remote sensing image scene classification: Benchmark and state of the art.
\newblock \emph{Proceedings of the IEEE}, 105\penalty0 (10):\penalty0 1865--1883, 2017.

\bibitem[Choshen et~al.(2022)Choshen, Venezian, Slonim, and Katz]{choshen2022fusing}
L.~Choshen, E.~Venezian, N.~Slonim, and Y.~Katz.
\newblock Fusing finetuned models for better pretraining.
\newblock \emph{arXiv preprint arXiv:2204.03044}, 2022.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi]{dtd}
M.~Cimpoi, S.~Maji, I.~Kokkinos, S.~Mohamed, and A.~Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2014.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert, J.~Tworek, J.~Hilton, R.~Nakano, C.~Hesse, and J.~Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dolan and Brockett(2005)]{mrpc}
B.~Dolan and C.~Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{International Workshop on Paraphrasing (IWP)}, 2005.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit, and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Du et~al.(2024{\natexlab{a}})Du, Jiang, Yang, Li, Chen, Li, Goh, and Tang]{du2024impacts}
G.~Du, R.~Jiang, S.~Yang, H.~Li, W.~Chen, K.~Li, S.~K. Goh, and H.-K. Tang.
\newblock Impacts of darwinian evolution on pre-trained deep neural networks.
\newblock \emph{arXiv preprint arXiv:2408.05563}, 2024{\natexlab{a}}.

\bibitem[Du et~al.(2024{\natexlab{b}})Du, Li, Liu, Jiang, Yu, Guo, Goh, and Tang]{du2024knowledge}
G.~Du, J.~Li, H.~Liu, R.~Jiang, S.~Yu, Y.~Guo, S.~K. Goh, and H.-K. Tang.
\newblock Knowledge fusion by evolving weights of language models.
\newblock \emph{arXiv preprint arXiv:2406.12208}, 2024{\natexlab{b}}.

\bibitem[Ferbach et~al.(2024)Ferbach, Goujaud, Gidel, and Dieuleveut]{ferbach2024proving}
D.~Ferbach, B.~Goujaud, G.~Gidel, and A.~Dieuleveut.
\newblock Proving linear mode connectivity of neural networks via optimal transport.
\newblock In \emph{Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)}, pages 3853--3861, 2024.

\bibitem[Fifty et~al.(2021)Fifty, Amid, Zhao, Yu, Anil, and Finn]{fifty2021efficiently}
C.~Fifty, E.~Amid, Z.~Zhao, T.~Yu, R.~Anil, and C.~Finn.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 34:\penalty0 27503--27516, 2021.

\bibitem[Fisher(1922)]{fisher1922mathematical}
R.~A. Fisher.
\newblock On the mathematical foundations of theoretical statistics.
\newblock \emph{Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character}, 222\penalty0 (594-604):\penalty0 309--368, 1922.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and Wilson]{garipov2018loss}
T.~Garipov, P.~Izmailov, D.~Podoprikhin, D.~P. Vetrov, and A.~G. Wilson.
\newblock Loss surfaces, mode connectivity, and fast ensembling of dnns.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 31, 2018.

\bibitem[Giampiccolo et~al.(2007)Giampiccolo, Magnini, Dagan, and Dolan]{rte}
D.~Giampiccolo, B.~Magnini, I.~Dagan, and W.~B. Dolan.
\newblock The third pascal recognizing textual entailment challenge.
\newblock In \emph{Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing}, pages 1--9, 2007.

\bibitem[Gupta et~al.(2020)Gupta, Serrano, and DeCoste]{gupta2020stochastic}
V.~Gupta, S.~A. Serrano, and D.~DeCoste.
\newblock Stochastic weight averaging in parallel: Large-batch training that generalizes well.
\newblock \emph{arXiv preprint arXiv:2001.02312}, 2020.

\bibitem[Hansen and Ostermeier(1996)]{hansen1996adapting}
N.~Hansen and A.~Ostermeier.
\newblock Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation.
\newblock In \emph{Proceedings of IEEE International Conference on Evolutionary Computation (ICEC)}, pages 312--317, 1996.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and Borth]{eurosat}
P.~Helber, B.~Bischke, A.~Dengel, and D.~Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.
\newblock \emph{Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 12\penalty0 (7):\penalty0 2217--2226, 2019.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Huang et~al.(2023)Huang, Liu, Lin, Pang, Du, and Lin]{huang2023lorahub}
C.~Huang, Q.~Liu, B.~Y. Lin, T.~Pang, C.~Du, and M.~Lin.
\newblock Lorahub: Efficient cross-task generalization via dynamic lora composition.
\newblock \emph{arXiv preprint arXiv:2307.13269}, 2023.

\bibitem[Huang et~al.(2019)Huang, Le~Bras, Bhagavatula, and Choi]{huang2019cosmos}
L.~Huang, R.~Le~Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Cosmos qa: Machine reading comprehension with contextual commonsense reasoning.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2391--2401, 2019.

\bibitem[Ilharco et~al.(2022)Ilharco, Wortsman, Gadre, Song, Hajishirzi, Kornblith, Farhadi, and Schmidt]{ilharco2022patching}
G.~Ilharco, M.~Wortsman, S.~Y. Gadre, S.~Song, H.~Hajishirzi, S.~Kornblith, A.~Farhadi, and L.~Schmidt.
\newblock Patching open-vocabulary models by interpolating weights.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 29262--29277, 2022.

\bibitem[Ilharco et~al.(2023)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt, Hajishirzi, and Farhadi]{ilharco2022editing}
G.~Ilharco, M.~T. Ribeiro, M.~Wortsman, S.~Gururangan, L.~Schmidt, H.~Hajishirzi, and A.~Farhadi.
\newblock Editing models with task arithmetic.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Jiang et~al.(2024)Jiang, Du, Yu, Guo, Goh, and Tang]{jiang2024cade}
R.~Jiang, G.~Du, S.~Yu, Y.~Guo, S.~K. Goh, and H.-K. Tang.
\newblock Cade: Cosine annealing differential evolution for spiking neural network.
\newblock \emph{arXiv preprint arXiv:2406.02349}, 2024.

\bibitem[Jin et~al.(2023)Jin, Ren, Preotiuc-Pietro, and Cheng]{jin2023regmean}
X.~Jin, X.~Ren, D.~Preotiuc-Pietro, and P.~Cheng.
\newblock Dataless knowledge fusion by merging weights of language models.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Kenton and Toutanova(2019)]{devlin2018bert}
J.~D. M.-W.~C. Kenton and L.~K. Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)}, pages 4171--4186, 2019.

\bibitem[Khot et~al.(2020)Khot, Clark, Guerquin, Jansen, and Sabharwal]{khot2020qasc}
T.~Khot, P.~Clark, M.~Guerquin, P.~Jansen, and A.~Sabharwal.
\newblock Qasc: A dataset for question answering via sentence composition.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}, volume~34, pages 8082--8090, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2015.

\bibitem[Klimaszewski et~al.(2024)Klimaszewski, Andruszkiewicz, and Birch]{klimaszewski2024no}
M.~Klimaszewski, P.~Andruszkiewicz, and A.~Birch.
\newblock No train but gain: Language arithmetic for training-free language adapters enhancement.
\newblock \emph{arXiv preprint arXiv:2404.15737}, 2024.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{cars}
J.~Krause, M.~Stark, J.~Deng, and L.~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCVW)}, pages 554--561, 2013.

\bibitem[LeCun(1998)]{lecun1998mnist}
Y.~LeCun.
\newblock The mnist database of handwritten digits, 1998.
\newblock \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and Morgenstern]{wsc}
H.~Levesque, E.~Davis, and L.~Morgenstern.
\newblock The winograd schema challenge.
\newblock In \emph{Proceedings of the International Conference on the Principles of Knowledge Representation and Reasoning (KR)}, 2012.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Koto, Yang, Zhao, Gong, Duan, and Baldwin]{li2024cmmlu}
H.~Li, Y.~Zhang, F.~Koto, Y.~Yang, H.~Zhao, Y.~Gong, N.~Duan, and T.~Baldwin.
\newblock Cmmlu: Measuring massive multitask language understanding in chinese.
\newblock \emph{arXiv preprint arXiv:2306.09212}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Peng, Zhang, Ding, Hu, and Shen]{li2023deep}
W.~Li, Y.~Peng, M.~Zhang, L.~Ding, H.~Hu, and L.~Shen.
\newblock Deep model fusion: A survey.
\newblock \emph{arXiv preprint arXiv:2309.15698}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2015)Li, Yosinski, Clune, Lipson, and Hopcroft]{li2015convergent}
Y.~Li, J.~Yosinski, J.~Clune, H.~Lipson, and J.~Hopcroft.
\newblock Convergent learning: Do different neural networks learn the same representations?
\newblock \emph{arXiv preprint arXiv:1511.07543}, 2015.

\bibitem[Li et~al.(2017)Li, Su, Shen, Li, Cao, and Niu]{li2017dailydialog}
Y.~Li, H.~Su, X.~Shen, W.~Li, Z.~Cao, and S.~Niu.
\newblock Dailydialog: A manually labelled multi-turn dialogue dataset.
\newblock \emph{arXiv preprint arXiv:1710.03957}, 2017.

\bibitem[Liu et~al.(2022)Liu, Tam, Muqeeth, Mohta, Huang, Bansal, and Raffel]{liu2022few}
H.~Liu, D.~Tam, M.~Muqeeth, J.~Mohta, T.~Huang, M.~Bansal, and C.~A. Raffel.
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 1950--1965, 2022.

\bibitem[Liu et~al.(2017)Liu, Banea, and Mihalcea]{liu2017grounded}
V.~Liu, C.~Banea, and R.~Mihalcea.
\newblock Grounded emotions.
\newblock In \emph{Proceedings of the International Conference on Affective Computing and Intelligent Interaction (ACII)}, pages 477--483, 2017.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Marneffe et~al.(2019)Marneffe, Simons, and Tonhauser]{cb}
M.-C.~d. Marneffe, M.~Simons, and J.~Tonhauser.
\newblock {The CommitmentBank}: Investigating projection in naturally occurring discourse.
\newblock In \emph{proceedings of Sinn und Bedeutung}, volume~23, pages 107--124, 2019.

\bibitem[Matena and Raffel(2022)]{matena2022merging}
M.~S. Matena and C.~A. Raffel.
\newblock Merging models with fisher-weighted averaging.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 17703--17716, 2022.

\bibitem[Matthews(1975)]{matthews1975comparison}
B.~W. Matthews.
\newblock Comparison of the predicted and observed secondary structure of t4 phage lysozyme.
\newblock \emph{Biochimica et Biophysica Acta (BBA)-Protein Structure}, 1975.

\bibitem[Mohammad and Bravo-Marquez(2017)]{mohammad2017wassa}
S.~Mohammad and F.~Bravo-Marquez.
\newblock Wassa-2017 shared task on emotion intensity.
\newblock In \emph{Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)}, pages 34--49, 2017.

\bibitem[Mohammad(2012)]{mohammad2012emotional}
S.~M. Mohammad.
\newblock {\#}emotional tweets.
\newblock In \emph{Proceedings of the First Joint Conference on Lexical and Computational Semantics (SEM)}, pages 246--255, 2012.

\bibitem[Mohammad et~al.(2015)Mohammad, Zhu, Kiritchenko, and Martin]{mohammad2015sentiment}
S.~M. Mohammad, X.~Zhu, S.~Kiritchenko, and J.~Martin.
\newblock Sentiment, emotion, purpose, and style in electoral tweets.
\newblock \emph{Information Processing \& Management}, 51\penalty0 (4):\penalty0 480--499, 2015.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, Ng, et~al.]{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, A.~Y. Ng, et~al.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS workshop on deep learning and unsupervised feature learning}, volume 2011, page~7, 2011.

\bibitem[Nie et~al.(2020)Nie, Williams, Dinan, Bansal, Weston, and Kiela]{nie2019adversarial}
Y.~Nie, A.~Williams, E.~Dinan, M.~Bansal, J.~Weston, and D.~Kiela.
\newblock {Adversarial NLI}: A new benchmark for natural language understanding.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)}, 2020.

\bibitem[Oberl{\"a}nder and Klinger(2018)]{oberlander2018analysis}
L.~A.~M. Oberl{\"a}nder and R.~Klinger.
\newblock An analysis of annotated corpora for emotion classification in text.
\newblock In \emph{Proceedings of the International Conference on Computational Linguistics (COLING)}, pages 2104--2119, 2018.

\bibitem[Orgad et~al.(2023)Orgad, Kawar, and Belinkov]{orgad2023editing}
H.~Orgad, B.~Kawar, and Y.~Belinkov.
\newblock Editing implicit assumptions in text-to-image diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR)}, pages 7053--7061, 2023.

\bibitem[Ortiz-Jimenez et~al.(2024)Ortiz-Jimenez, Favero, and Frossard]{ortiz2024task}
G.~Ortiz-Jimenez, A.~Favero, and P.~Frossard.
\newblock Task arithmetic in the tangent space: Improved editing of pre-trained models.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 36, 2024.

\bibitem[Pilehvar and Camacho-Collados(2019)]{wic}
M.~T. Pilehvar and J.~Camacho-Collados.
\newblock {WiC}: The word-in-context dataset for evaluating context-sensitive meaning representations.
\newblock In \emph{Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)}, 2019.

\bibitem[Poth et~al.(2021)Poth, Pfeiffer, R{\"u}ckl{\'e}, and Gurevych]{poth2021pre}
C.~Poth, J.~Pfeiffer, A.~R{\"u}ckl{\'e}, and I.~Gurevych.
\newblock What to pre-train on? efficient intermediate task selection.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2021.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pages 8748--8763, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rame et~al.(2022)Rame, Kirchmeyer, Rahier, Rakotomamonjy, Gallinari, and Cord]{rame2022diverse}
A.~Rame, M.~Kirchmeyer, T.~Rahier, A.~Rakotomamonjy, P.~Gallinari, and M.~Cord.
\newblock Diverse weight averaging for out-of-distribution generalization.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 10821--10836, 2022.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, and Gordon]{copa}
M.~Roemmele, C.~A. Bejan, and A.~S. Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
\newblock In \emph{2011 AAAI Spring Symposium Series}, 2011.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, Downey, and Rumshisky]{quail_dataset}
A.~Rogers, O.~Kovaleva, M.~Downey, and A.~Rumshisky.
\newblock Getting closer to {AI} complete question answering: {A} set of prerequisite real tasks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)}, volume~34, pages 8722--8731, 2020.

\bibitem[Rozière et~al.(2023)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Sauvestre, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve]{rozière2024code}
B.~Rozière, J.~Gehring, F.~Gloeckle, S.~Sootla, I.~Gat, X.~E. Tan, Y.~Adi, J.~Liu, R.~Sauvestre, T.~Remez, J.~Rapin, A.~Kozhevnikov, I.~Evtimov, J.~Bitton, M.~Bhatt, C.~C. Ferrer, A.~Grattafiori, W.~Xiong, A.~Défossez, J.~Copet, F.~Azhar, H.~Touvron, L.~Martin, N.~Usunier, T.~Scialom, and G.~Synnaeve.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Sagi and Rokach(2018)]{sagi2018ensemble}
O.~Sagi and L.~Rokach.
\newblock Ensemble learning: A survey.
\newblock \emph{Wiley interdisciplinary reviews: data mining and knowledge discovery}, 8\penalty0 (4):\penalty0 e1249, 2018.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{winogrande}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106, 2021.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2022multitask}
V.~Sanh, A.~Webson, C.~Raffel, S.~H. Bach, L.~Sutawika, Z.~Alyafeai, A.~Chaffin, A.~Stiegler, T.~L. Scao, A.~Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Sap et~al.(2019)Sap, Rashkin, Chen, Le~Bras, and Choi]{sap2019social}
M.~Sap, H.~Rashkin, D.~Chen, R.~Le~Bras, and Y.~Choi.
\newblock Social iqa: Commonsense reasoning about social interactions.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 4463--4473, 2019.

\bibitem[Scherer and Wallbott(1994)]{scherer1994evidence}
K.~R. Scherer and H.~G. Wallbott.
\newblock Evidence for universality and cultural variation of differential emotion response patterning.
\newblock \emph{Journal of personality and social psychology}, 66\penalty0 (2):\penalty0 310, 1994.

\bibitem[Schuff et~al.(2017)Schuff, Barnes, Mohme, Pad{\'o}, and Klinger]{schuff2017annotation}
H.~Schuff, J.~Barnes, J.~Mohme, S.~Pad{\'o}, and R.~Klinger.
\newblock Annotation, modelling and analysis of fine-grained emotions on a stance and sentiment detection corpus.
\newblock In \emph{Proceedings of the Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)}, pages 13--23, 2017.

\bibitem[Sharma et~al.(2018)Sharma, Allen, Bakhshandeh, and Mostafazadeh]{sharma2018tackling}
R.~Sharma, J.~Allen, O.~Bakhshandeh, and N.~Mostafazadeh.
\newblock Tackling the story ending biases in the story cloze test.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)}, pages 752--757, 2018.

\bibitem[Shnarch et~al.(2022)Shnarch, Halfon, Gera, Danilevsky, Katsis, Choshen, Cooper, Epelboim, Zhang, Wang, et~al.]{shnarch2022label}
E.~Shnarch, A.~Halfon, A.~Gera, M.~Danilevsky, Y.~Katsis, L.~Choshen, M.~S. Cooper, D.~Epelboim, Z.~Zhang, D.~Wang, et~al.
\newblock Label sleuth: From unlabeled text to a classifier in a few hours.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2022.

\bibitem[Singh and Jaggi(2020)]{singh2020model}
S.~P. Singh and M.~Jaggi.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33:\penalty0 22045--22055, 2020.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{sst-2}
R.~Socher, A.~Perelygin, J.~Wu, J.~Chuang, C.~D. Manning, A.~Y. Ng, and C.~Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 1631--1642, 2013.

\bibitem[Stallkamp et~al.(2011)Stallkamp, Schlipsing, Salmen, and Igel]{gtsrb}
J.~Stallkamp, M.~Schlipsing, J.~Salmen, and C.~Igel.
\newblock The german traffic sign recognition benchmark: a multi-class classification competition.
\newblock In \emph{Proceedings of the International Joint Conference on Neural Networks (IJCNN)}, 2011.

\bibitem[Stoica et~al.(2024)Stoica, Bolya, Bjorner, Ramesh, Hearn, and Hoffman]{stoica2023zipit}
G.~Stoica, D.~Bolya, J.~Bjorner, P.~Ramesh, T.~Hearn, and J.~Hoffman.
\newblock Zipit! merging models from different tasks without training.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Strapparava and Mihalcea(2007)]{strapparava2007semeval}
C.~Strapparava and R.~Mihalcea.
\newblock Semeval-2007 task 14: Affective text.
\newblock In \emph{Proceedings of the International Workshop on Semantic Evaluations (SemEval)}, pages 70--74, 2007.

\bibitem[Sun et~al.(2022)Sun, Shao, Qian, Huang, and Qiu]{sun2022black}
T.~Sun, Y.~Shao, H.~Qian, X.~Huang, and X.~Qiu.
\newblock Black-box tuning for language-model-as-a-service.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pages 20841--20855, 2022.

\bibitem[Tafjord et~al.(2019)Tafjord, Gardner, Lin, and Clark]{tafjord2019quartz}
O.~Tafjord, M.~Gardner, K.~Lin, and P.~Clark.
\newblock Quartz: An open-domain dataset of qualitative relationship questions.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 5940--5945, 2019.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and Lai]{tatro2020optimizing}
N.~Tatro, P.-Y. Chen, P.~Das, I.~Melnyk, P.~Sattigeri, and R.~Lai.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33:\penalty0 15300--15311, 2020.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Wan et~al.(2024)Wan, Huang, Cai, Quan, Bi, and Shi]{wan2024knowledge}
F.~Wan, X.~Huang, D.~Cai, X.~Quan, W.~Bi, and S.~Shi.
\newblock Knowledge fusion of large language models.
\newblock \emph{arXiv preprint arXiv:2401.10491}, 2024.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2019.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and Khazaeni]{wang2020federated}
H.~Wang, M.~Yurochkin, Y.~Sun, D.~Papailiopoulos, and Y.~Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2020.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{cola}
A.~Warstadt, A.~Singh, and S.~Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 625--641, 2019.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac, T.~Rault, R.~Louf, M.~Funtowicz, et~al.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2019.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
M.~Wortsman, G.~Ilharco, S.~Y. Gadre, R.~Roelofs, R.~Gontijo-Lopes, A.~S. Morcos, H.~Namkoong, A.~Farhadi, Y.~Carmon, S.~Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, pages 23965--23998, 2022.

\bibitem[Xiao et~al.(2016)Xiao, Ehinger, Hays, Torralba, and Oliva]{xiao2016sun}
J.~Xiao, K.~A. Ehinger, J.~Hays, A.~Torralba, and A.~Oliva.
\newblock Sun database: Exploring a large collection of scene categories.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 119:\penalty0 3--22, 2016.

\bibitem[Xu et~al.(2024)Xu, Yuan, Wang, Wang, Song, and Song]{xu2024training}
Z.~Xu, K.~Yuan, H.~Wang, Y.~Wang, M.~Song, and J.~Song.
\newblock Training-free pretrained model merging.
\newblock \emph{arXiv preprint arXiv:2403.01753}, 2024.

\bibitem[Yadav et~al.(2024)Yadav, Tam, Choshen, Raffel, and Bansal]{ties}
P.~Yadav, D.~Tam, L.~Choshen, C.~A. Raffel, and M.~Bansal.
\newblock Ties-merging: Resolving interference when merging models.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 36, 2024.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Wang, Shen, Liu, Guo, Wang, and Tao]{yang2023adamerging}
E.~Yang, Z.~Wang, L.~Shen, S.~Liu, G.~Guo, X.~Wang, and D.~Tao.
\newblock Adamerging: Adaptive model merging for multi-task learning.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2024{\natexlab{a}}.

\bibitem[Yang et~al.(2015)Yang, Yih, and Meek]{yang2015wikiqa}
Y.~Yang, W.-t. Yih, and C.~Meek.
\newblock Wikiqa: A challenge dataset for open-domain question answering.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 2013--2018, 2015.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Du, Toa, Tang, and Goh]{yang2024evolutionary}
Y.~Yang, G.~Du, C.~K. Toa, H.-K. Tang, and S.~K. Goh.
\newblock Evolutionary neural architecture search for 3d point cloud analysis.
\newblock \emph{arXiv preprint arXiv:2408.05556}, 2024{\natexlab{b}}.

\bibitem[Yu et~al.(2023{\natexlab{a}})Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu]{yu2024metamath}
L.~Yu, W.~Jiang, H.~Shi, J.~Yu, Z.~Liu, Y.~Zhang, J.~T. Kwok, Z.~Li, A.~Weller, and W.~Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023{\natexlab{a}}.

\bibitem[Yu et~al.(2023{\natexlab{b}})Yu, Yu, Yu, Huang, and Li]{yu2023language}
L.~Yu, B.~Yu, H.~Yu, F.~Huang, and Y.~Li.
\newblock Language models are super mario: Absorbing abilities from homologous models as a free lunch.
\newblock \emph{arXiv preprint arXiv:2311.03099}, 2023{\natexlab{b}}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{zellers2019hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock {HellaSwag}: Can a machine really finish your sentence?
\newblock In \emph{Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)}, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Liu, He, et~al.]{zhang2023composing}
J.~Zhang, J.~Liu, J.~He, et~al.
\newblock Composing parameter-efficient modules with arithmetic operation.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 36:\penalty0 12589--12610, 2023.

\bibitem[Zhang et~al.(2019)Zhang, Baldridge, and He]{zhang2019paws}
Y.~Zhang, J.~Baldridge, and L.~He.
\newblock Paws: Paraphrase adversaries from word scrambling.
\newblock In \emph{Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)}, 2019.

\bibitem[Zhou et~al.(2024)Zhou, Yang, Yang, Yan, and Hu]{zhou2024going}
Z.~Zhou, Y.~Yang, X.~Yang, J.~Yan, and W.~Hu.
\newblock Going beyond linear mode connectivity: The layerwise linear feature connectivity.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 36, 2024.

\bibitem[Zhuang et~al.(2020)Zhuang, Qi, Duan, Xi, Zhu, Zhu, Xiong, and He]{zhuang2020comprehensive}
F.~Zhuang, Z.~Qi, K.~Duan, D.~Xi, Y.~Zhu, H.~Zhu, H.~Xiong, and Q.~He.
\newblock A comprehensive survey on transfer learning.
\newblock \emph{Proceedings of the IEEE}, 2020.

\end{thebibliography}
