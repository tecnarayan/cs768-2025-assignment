\begin{thebibliography}{10}

\bibitem{pseudoLabel2019}
Eric Arazo, Diego Ortego, Paul Albert, Noel~E O'Connor, and Kevin McGuinness.
\newblock Pseudo-labeling and confirmation bias in deep semi-supervised
  learning.
\newblock In {\em IJCNN}, 2020.

\bibitem{PAWS}
Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin,
  Nicolas Ballas, and Michael Rabbat.
\newblock Semi-supervised learning of visual features by non-parametrically
  predicting view assignments with support samples.
\newblock In {\em ICCV}, 2021.

\bibitem{ReMixMatch}
David Berthelot, Nicholas Carlini, Ekin~D Cubuk, Alex Kurakin, Kihyuk Sohn, Han
  Zhang, and Colin Raffel.
\newblock Remixmatch: Semi-supervised learning with distribution alignment and
  augmentation anchoring.
\newblock In {\em ICLR}, 2020.

\bibitem{MixMatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In {\em NeurIPS}, 2019.

\bibitem{CoTraining}
Avrim Blum and Tom Mitchell.
\newblock Combining labeled and unlabeled data with co-training.
\newblock In {\em Proceedings of the eleventh annual conference on
  Computational learning theory}, 1998.

\bibitem{Food-101}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In {\em ECCV}, 2014.

\bibitem{Simclrv2}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey
  Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock In {\em NeurIPS}, 2020.

\bibitem{MoCov2}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{DTD}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In {\em CVPR}, 2014.

\bibitem{STL10}
Adam Coates, Andrew Ng, and Honglak Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In {\em AISTATS}, 2011.

\bibitem{RandAugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em CVPR}, 2020.

\bibitem{GoodSSLBadGAN}
Zihang Dai, Zhilin Yang, Fan Yang, William~W Cohen, and Russ~R Salakhutdinov.
\newblock Good semi-supervised learning that requires a bad gan.
\newblock In {\em NeurIPS}, 2017.

\bibitem{deng_imagenet:_2009}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\bibitem{cite:NAACL19BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2019.

\bibitem{ALI}
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb,
  Martin Arjovsky, and Aaron Courville.
\newblock Adversarially learned inference.
\newblock In {\em ICLR}, 2017.

\bibitem{caltech}
Li~Fei-Fei, R.~Fergus, and P.~Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In {\em CVPR}, 2004.

\bibitem{MMT}
Yixiao Ge, Dapeng Chen, and Hongsheng Li.
\newblock Mutual mean-teaching: Pseudo label refinery for unsupervised domain
  adaptation on person re-identification.
\newblock In {\em ICLR}, 2020.

\bibitem{GAN}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em NeurIPS}, 2014.

\bibitem{adversarial_samples}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em ICLR}, 2015.

\bibitem{entropy_minimization}
Yves Grandvalet and Yoshua Bengio.
\newblock Semi-supervised learning by entropy minimization.
\newblock In {\em NeurIPS}, 2005.

\bibitem{cite:CVPR20MoCo}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em CVPR}, 2020.

\bibitem{ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{label_propagation}
Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum.
\newblock Label propagation for deep semi-supervised learning.
\newblock In {\em CVPR}, 2019.

\bibitem{jiang2022transferability}
Junguang Jiang, Yang Shu, Jianmin Wang, and Mingsheng Long.
\newblock Transferability in deep learning: A survey, 2022.

\bibitem{catastrophic_forgetting}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 2017.

\bibitem{cite:CVPR19DoBetterTransfer}
Simon Kornblith, Jonathon Shlens, and Quoc~V Le.
\newblock Do better imagenet models transfer better?
\newblock In {\em CVPR}, 2019.

\bibitem{Stanford-Cars}
Jonathan Krause, Jia Deng, Michael Stark, and Li~Fei-Fei.
\newblock Collecting a large-scale dataset of fine-grained cars.
\newblock In {\em FGVC}, 2013.

\bibitem{CIFAR}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Technical report, University of Toronto}, 2009.

\bibitem{Temporal_Ensembling}
Samuli Laine and Timo Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In {\em ICLR}, 2017.

\bibitem{pseudo_label}
Dong-Hyun Lee.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In {\em ICML}, 2013.

\bibitem{DivideMix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In {\em ICLR}, 2020.

\bibitem{CoMatch}
Junnan Li, Caiming Xiong, and Steven~CH Hoi.
\newblock Comatch: Semi-supervised learning with contrastive graph
  regularization.
\newblock In {\em ICCV}, 2021.

\bibitem{Aircraft}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock {\em arXiv preprint arXiv:1306.5151}, 2013.

\bibitem{VAT}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock In {\em TPAMI}, 2018.

\bibitem{SVHN}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NeurIPS}, 2011.

\bibitem{Flowers}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em ICVGIP}, 2008.

\bibitem{GANSSL}
Augustus Odena.
\newblock Semi-supervised learning with generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1606.01583}, 2016.

\bibitem{DASO}
Youngtaek Oh, Dong-Jin Kim, and In~So Kweon.
\newblock Daso: Distribution-aware semantics-oriented pseudo-label for
  imbalanced semi-supervised learning.
\newblock In {\em CVPR}, 2022.

\bibitem{VAdD}
Sungrae Park, JunKeon Park, Su-Jin Shin, and Il-Chul Moon.
\newblock Adversarial dropout for supervised and semi-supervised learning.
\newblock In {\em AAAI}, 2018.

\bibitem{Pets}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In {\em CVPR}, 2012.

\bibitem{Pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em NeurIPS}, 2019.

\bibitem{meta_pseudo_labels}
Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc~V Le.
\newblock Meta pseudo labels.
\newblock In {\em CVPR}, 2021.

\bibitem{CLIP}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, 2021.

\bibitem{Defense}
Mamshad~Nayeem Rizve, Kevin Duarte, Yogesh~S Rawat, and Mubarak Shah.
\newblock In defense of pseudo-labeling: An uncertainty-aware pseudo-label
  selection framework for semi-supervised learning.
\newblock In {\em ICLR}, 2021.

\bibitem{self_training_wacv}
Chuck Rosenberg, Martial Hebert, and Henry Schneiderman.
\newblock Semi-supervised self-training of object detection models.
\newblock In {\em WACV}, 2005.

\bibitem{MTTriTraining}
Sebastian Ruder and Barbara Plank.
\newblock Strong baselines for neural semi-supervised learning under domain
  shift.
\newblock In {\em ACL}, 2018.

\bibitem{ImproveGANs}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In {\em NeurIPS}, 2016.

\bibitem{transductive}
Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng~MaXiaoyu Tao, and Nanning Zheng.
\newblock Transductive semi-supervised deep learning using min-max features.
\newblock In {\em ECCV}, 2018.

\bibitem{FixMatch}
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini,
  Ekin~D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In {\em NeurIPS}, 2020.

\bibitem{Dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock In {\em ICML}, 2014.

\bibitem{Realistic_CVPR}
Jong-Chyi Su, Zezhou Cheng, and Subhransu Maji.
\newblock A realistic evaluation of semi-supervised learning for fine-grained
  classification.
\newblock In {\em CVPR}, 2021.

\bibitem{RAT}
Teppei Suzuki and Ikuro Sato.
\newblock Adversarial transformations for semi-supervised learning.
\newblock In {\em AAAI}, 2020.

\bibitem{Mean_Teacher}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In {\em NeurIPS}, 2017.

\bibitem{CUB200}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge
  Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock {\em Technical Report CNS-TR-2011-001, California Institute of
  Technology}, 2011.

\bibitem{Self-Tuning}
Ximei Wang, Jinghan Gao, Mingsheng Long, and Jianmin Wang.
\newblock Self-tuning for data-efficient deep learning.
\newblock In {\em ICML}, 2021.

\bibitem{DebiasMatch}
Xudong Wang, Zhirong Wu, Long Lian, and Stella~X Yu.
\newblock Debiased learning from naturally imbalanced pseudo-labels for
  zero-shot and semi-supervised learning.
\newblock In {\em CVPR}, 2022.

\bibitem{self_training_theory}
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma.
\newblock Theoretical analysis of self-training with deep networks on unlabeled
  data.
\newblock In {\em ICLR}, 2021.

\bibitem{SUN397}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In {\em CVPR}, 2010.

\bibitem{UDA}
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V Le.
\newblock Unsupervised data augmentation for consistency training.
\newblock In {\em NeurIPS}, 2020.

\bibitem{Noisy_Student}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In {\em CVPR}, 2020.

\bibitem{Dash}
Yi~Xu, Lei Shang, Jinxing Ye, Qi~Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong
  Jin.
\newblock Dash: Semi-supervised learning with dynamic thresholding.
\newblock In {\em ICML}, 2021.

\bibitem{self_training_acl}
David Yarowsky.
\newblock Unsupervised word sense disambiguation rivaling supervised methods.
\newblock In {\em ACL}, 1995.

\bibitem{WideResNet}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em BMVC}, 2016.

\bibitem{FlexMatch}
Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and
  Takahiro Shinozaki.
\newblock Flexmatch: Boosting semi-supervised learning with curriculum pseudo
  labeling.
\newblock In {\em NeurIPS}, 2021.

\bibitem{mutual}
Ying Zhang, Tao Xiang, Timothy~M Hospedales, and Huchuan Lu.
\newblock Deep mutual learning.
\newblock In {\em CVPR}, 2018.

\bibitem{cite:ICLR21InstanceTransfer}
Nanxuan Zhao, Zhirong Wu, Rynson W.~H. Lau, and Stephen Lin.
\newblock What makes instance discrimination good for transfer learning?
\newblock In {\em ICLR}, 2021.

\bibitem{place}
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Places: A 10 million image database for scene recognition.
\newblock In {\em TPAMI}, 2018.

\end{thebibliography}
