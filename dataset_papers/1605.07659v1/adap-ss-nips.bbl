\begin{thebibliography}{10}

\bibitem{bartlett2006convexity}
Peter~L Bartlett, Michael~I Jordan, and Jon~D McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock {\em Journal of the American Statistical Association},
  101(473):138--156, 2006.

\bibitem{beck2009fast}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM journal on imaging sciences}, 2(1):183--202, 2009.

\bibitem{bordes2009sgd}
Antoine Bordes, L{\'e}on Bottou, and Patrick Gallinari.
\newblock Sgd-qn: Careful quasi-newton stochastic gradient descent.
\newblock {\em The Journal of Machine Learning Research}, 10:1737--1754, 2009.

\bibitem{bousquet2008tradeoffs}
Olivier Bousquet and L{\'e}on Bottou.
\newblock The tradeoffs of large scale learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  161--168, 2008.

\bibitem{boyd04}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex Optimization}.
\newblock Cambridge University Press, New York, NY, USA, 2004.

\bibitem{daneshmand2016starting}
Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann.
\newblock Starting small--learning with adaptive sample sizes.
\newblock {\em arXiv preprint arXiv:1603.02839}, 2016.

\bibitem{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem{erdogdu2015convergence}
Murat~A Erdogdu and Andrea Montanari.
\newblock Convergence rates of sub-sampled newton methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3034--3042, 2015.

\bibitem{frostig2014competing}
Roy Frostig, Rong Ge, Sham~M Kakade, and Aaron Sidford.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock {\em arXiv preprint arXiv:1412.6606}, 2014.

\bibitem{gower2016stochastic}
Robert~M Gower, Donald Goldfarb, and Peter Richt{\'a}rik.
\newblock Stochastic block bfgs: Squeezing more curvature out of data.
\newblock {\em arXiv preprint arXiv:1603.09649}, 2016.

\bibitem{gurbuzbalaban2015globally}
Mert G{\"u}rb{\"u}zbalaban, Asuman Ozdaglar, and Pablo Parrilo.
\newblock A globally convergent incremental newton method.
\newblock {\em Mathematical Programming}, 151(1):283--313, 2015.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{konevcny2013semi}
Jakub Kone{\v{c}}n{\`y} and Peter Richt{\'a}rik.
\newblock Semi-stochastic gradient descent methods.
\newblock {\em arXiv preprint arXiv:1312.1666}, 2013.

\bibitem{mokhtari2014res}
Aryan Mokhtari and Alejandro Ribeiro.
\newblock Res: Regularized stochastic bfgs algorithm.
\newblock {\em Signal Processing, IEEE Transactions on}, 62(23):6089--6104,
  2014.

\bibitem{JMLR:v16:mokhtari15a}
Aryan Mokhtari and Alejandro Ribeiro.
\newblock Global convergence of online limited memory bfgs.
\newblock {\em Journal of Machine Learning Research}, 16:3151--3181, 2015.

\bibitem{moritz2015linearly}
Philipp Moritz, Robert Nishihara, and Michael~I Jordan.
\newblock A linearly-convergent stochastic l-bfgs algorithm.
\newblock {\em arXiv preprint arXiv:1508.02087}, 2015.

\bibitem{nesterov1998introductory}
Yu~Nesterov.
\newblock Introductory lectures on convex programming volume i: Basic course.
\newblock 1998.

\bibitem{nesterov2007gradient}
Yurii Nesterov et~al.
\newblock Gradient methods for minimizing composite objective function.
\newblock Technical report, UCL, 2007.

\bibitem{polyak1992acceleration}
Boris~T Polyak and Anatoli~B Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM Journal on Control and Optimization}, 30(4):838--855, 1992.

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem{roux2012stochastic}
Nicolas~L Roux, Mark Schmidt, and Francis~R Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2663--2671, 2012.

\bibitem{schraudolph2007stochastic}
Nicol~N Schraudolph, Jin Yu, and Simon G{\"u}nter.
\newblock A stochastic quasi-newton method for online convex optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 436--443, 2007.

\bibitem{shalev2010learnability}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock {\em The Journal of Machine Learning Research}, 11:2635--2670, 2010.

\bibitem{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss.
\newblock {\em The Journal of Machine Learning Research}, 14:567--599, 2013.

\bibitem{shalev2016accelerated}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock {\em Mathematical Programming}, 155(1-2):105--145, 2016.

\bibitem{vapnik1998statistical}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{xiao2014proximal}
Lin Xiao and Tong Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em SIAM Journal on Optimization}, 24(4):2057--2075, 2014.

\bibitem{zhang2013linear}
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin.
\newblock Linear convergence with condition number independent access of full
  gradients.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  980--988, 2013.

\end{thebibliography}
