\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[van Laarhoven(2017)]{vanlaarhoven2017l2}
Twan van Laarhoven.
\newblock L2 regularization versus batch and weight normalization, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and Vinyals]{10.1145/3446776}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking generalization.
\newblock \emph{Commun. ACM}, 64\penalty0 (3):\penalty0 107–115, feb 2021.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/3446776}.
\newblock URL \url{https://doi.org/10.1145/3446776}.

\bibitem[Zhang et~al.(2019)Zhang, Wang, Xu, and Grosse]{zhang2018three}
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse.
\newblock Three mechanisms of weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1lz-3Rct7}.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Xie et~al.(2023)Xie, zhiqiang xu, Zhang, Sato, and Sugiyama]{xie2023on}
Zeke Xie, zhiqiang xu, Jingzhao Zhang, Issei Sato, and Masashi Sugiyama.
\newblock On the overlooked pitfalls of weight decay and how to mitigate them: A gradient-norm perspective.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=vnGcubtzR1}.

\bibitem[Andriushchenko et~al.(2023)Andriushchenko, D'Angelo, Varre, and Flammarion]{andriushchenko2023need}
Maksym Andriushchenko, Francesco D'Angelo, Aditya Varre, and Nicolas Flammarion.
\newblock Why do we need weight decay in modern deep learning?, 2023.

\bibitem[Mackay(1995)]{Mackay}
David J~C Mackay.
\newblock Probable networks and plausible predictions — a review of practical bayesian methods for supervised neural networks.
\newblock \emph{Network: Computation in Neural Systems}, 6\penalty0 (3):\penalty0 469--505, 1995.

\bibitem[Krogh and Hertz(1991)]{Krogh1991ASW}
Anders Krogh and John~A. Hertz.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{Neural Information Processing Systems}, 1991.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:10137788}.

\bibitem[Ziyin and Wang(2023)]{pmlr-v202-ziyin23a}
Liu Ziyin and Zihao Wang.
\newblock spred: Solving l1 penalty with {SGD}.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 43407--43422. PMLR, 23--29 Jul 2023.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{DBLP:journals/corr/abs-1905-13655}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{CoRR}, abs/1905.13655, 2019.
\newblock URL \url{http://arxiv.org/abs/1905.13655}.

\bibitem[Li et~al.(2021)Li, Luo, and Lyu]{li2021towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=AHOs7Sm5H7R}.

\bibitem[Razin and Cohen(2020)]{razin2020implicit}
Noam Razin and Nadav Cohen.
\newblock Implicit regularization in deep learning may not be explainable by norms, 2020.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur, and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro.
\newblock Implicit regularization in matrix factorization, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformers}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2017.

\bibitem[Phuong and Hutter(2022)]{phuong2022formal}
Mary Phuong and Marcus Hutter.
\newblock Formal algorithms for transformers, 2022.

\bibitem[Srebro and Shraibman(2005)]{rank_srebro}
Nathan Srebro and Adi Shraibman.
\newblock Rank, trace-norm and max-norm.
\newblock In Peter Auer and Ron Meir, editors, \emph{Learning Theory}, pages 545--560, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.
\newblock ISBN 978-3-540-31892-7.

\bibitem[Tibshirani(2021)]{tibshirani2021equivalences}
Ryan~J Tibshirani.
\newblock Equivalences between sparse models and neural networks.
\newblock \emph{Working Notes. URL https://www. stat. cmu. edu/ryantibs/papers/sparsitynn. pdf}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{transformers_few_shot}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Hu et~al.(2021{\natexlab{a}})Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, and Chen]{lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{CoRR}, abs/2106.09685, 2021{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2106.09685}.

\bibitem[Wu et~al.(2020)Wu, Xu, Dai, Wan, Zhang, Yan, Tomizuka, Gonzalez, Keutzer, and Vajda]{wu2020visual}
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda.
\newblock Visual transformers: Token-based image representation and processing for computer vision, 2020.

\bibitem[Sun and Luo(2016)]{Sun_2016}
Ruoyu Sun and Zhi-Quan Luo.
\newblock Guaranteed matrix completion via non-convex factorization.
\newblock \emph{IEEE Transactions on Information Theory}, 62\penalty0 (11):\penalty0 6535–6579, November 2016.
\newblock ISSN 1557-9654.
\newblock \doi{10.1109/tit.2016.2598574}.
\newblock URL \url{http://dx.doi.org/10.1109/TIT.2016.2598574}.

\bibitem[Candes and Tao(2009)]{candes2009power}
Emmanuel~J. Candes and Terence Tao.
\newblock The power of convex relaxation: Near-optimal matrix completion, 2009.

\bibitem[Hu et~al.(2021{\natexlab{b}})Hu, Nie, Wang, and Li]{HU2021218}
Zhanxuan Hu, Feiping Nie, Rong Wang, and Xuelong Li.
\newblock Low rank regularization: A review.
\newblock \emph{Neural Networks}, 136:\penalty0 218--232, 2021{\natexlab{b}}.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/j.neunet.2020.09.021}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S089360802030352X}.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and Qu]{zhu2021geometric}
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu.
\newblock A geometric analysis of neural collapse with unconstrained features, 2021.

\bibitem[Jacot et~al.(2022)Jacot, Ged, Şimşek, Hongler, and Gabriel]{jacot2022saddletosaddle}
Arthur Jacot, François Ged, Berfin Şimşek, Clément Hongler, and Franck Gabriel.
\newblock Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity, 2022.

\bibitem[Jacot(2023)]{jacot2023implicit}
Arthur Jacot.
\newblock Implicit bias of large depth networks: a notion of rank for nonlinear functions, 2023.

\bibitem[Dai et~al.(2021)Dai, Karzand, and Srebro]{NEURIPS2021_e22cb9d6}
Zhen Dai, Mina Karzand, and Nathan Srebro.
\newblock Representation costs of linear neural networks: Analysis and design.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, volume~34, pages 26884--26896. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/e22cb9d6bbb4c290a94e4fff4d68a831-Paper.pdf}.

\bibitem[Galanti et~al.(2023)Galanti, Siegel, Gupte, and Poggio]{galanti2023characterizing}
Tomer Galanti, Zachary~S. Siegel, Aparna Gupte, and Tomaso Poggio.
\newblock Characterizing the implicit bias of regularized sgd in rank minimization, 2023.

\bibitem[Wang and Jacot(2023)]{wang2023implicit}
Zihan Wang and Arthur Jacot.
\newblock Implicit bias of sgd in $l_{2}$-regularized linear dnns: One-way jumps from high to low rank, 2023.

\bibitem[Khodak et~al.(2022)Khodak, Tenenholtz, Mackey, and Fusi]{khodak2022initialization}
Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicolò Fusi.
\newblock Initialization and regularization of factorized neural layers, 2022.

\bibitem[Bhojanapalli et~al.(2020)Bhojanapalli, Yun, Rawat, Reddi, and Kumar]{bhojanapalli2020lowrank}
Srinadh Bhojanapalli, Chulhee Yun, Ankit~Singh Rawat, Sashank~J. Reddi, and Sanjiv Kumar.
\newblock Low-rank bottleneck in multi-head attention models, 2020.

\bibitem[Sharma et~al.(2023)Sharma, Ash, and Misra]{sharma2023truth}
Pratyusha Sharma, Jordan~T. Ash, and Dipendra Misra.
\newblock The truth is in there: Improving reasoning in language models with layer-selective rank reduction, 2023.

\bibitem[Ziyin et~al.(2022)Ziyin, Li, and Meng]{ziyin2022exact}
Liu Ziyin, Botao Li, and Xiangming Meng.
\newblock Exact solutions of a deep linear network.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=X6bp8ri8dV}.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{exact_slutions}
Andrew~M. Saxe, James~L. McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, 2013.
\newblock URL \url{https://arxiv.org/abs/1312.6120}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{gao_pile_2020}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The pile: an {800GB} dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Irandoust et~al.(2022)Irandoust, Durand, Rakhmangulova, Zi, and Hajimirsadeghi]{irandoust2022training}
Saghar Irandoust, Thibaut Durand, Yunduz Rakhmangulova, Wenjie Zi, and Hossein Hajimirsadeghi.
\newblock Training a vision transformer from scratch in less than 24 hours with 1 gpu, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and Ré]{fu2023hungry}
Daniel~Y. Fu, Tri Dao, Khaled~K. Saab, Armin~W. Thomas, Atri Rudra, and Christopher Ré.
\newblock Hungry hungry hippos: Towards language modeling with state space models, 2023.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and Ré]{poli2023hyena}
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel~Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré.
\newblock Hyena hierarchy: Towards larger convolutional language models, 2023.

\bibitem[von Oswald et~al.(2023)von Oswald, Niklasson, Schlegel, Kobayashi, Zucchet, Scherrer, Miller, Sandler, y~Arcas, Vladymyrov, Pascanu, and Sacramento]{vonoswald2023uncovering}
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise~Agüera y~Arcas, Max Vladymyrov, Razvan Pascanu, and João Sacramento.
\newblock Uncovering mesa-optimization algorithms in transformers, 2023.

\bibitem[Singh et~al.(2023)Singh, Chan, Moskovitz, Grant, Saxe, and Hill]{singh2023transient}
Aaditya~K. Singh, Stephanie C.~Y. Chan, Ted Moskovitz, Erin Grant, Andrew~M. Saxe, and Felix Hill.
\newblock The transient nature of emergent in-context learning in transformers, 2023.

\bibitem[L\'evy(1940)]{CM_1940__7__283_0}
Paul L\'evy.
\newblock Sur certains processus stochastiques homog\`enes.
\newblock \emph{Compositio Mathematica}, 7:\penalty0 283--339, 1940.
\newblock URL \url{http://www.numdam.org/item/CM_1940__7__283_0/}.

\bibitem[Powers and St{\o}rmer(1970)]{powers1970free}
Robert~T Powers and Erling St{\o}rmer.
\newblock Free states of the canonical anticommutation relations.
\newblock \emph{Communications in Mathematical Physics}, 16\penalty0 (1):\penalty0 1--33, 1970.

\bibitem[Kingma and Ba(2015)]{kingma_adam_2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: a method for stochastic optimization.
\newblock In \emph{International {Conference} on {Learning} {Representations}}, 2015.

\bibitem[Hahnloser et~al.(2000)Hahnloser, Sarpeshkar, Mahowald, Douglas, and Seung]{hahnloser_digital_2000}
Richard H.~R. Hahnloser, Rahul Sarpeshkar, Misha~A. Mahowald, Rodney~J. Douglas, and H.~Sebastian Seung.
\newblock Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.
\newblock \emph{Nature}, 405\penalty0 (6789):\penalty0 947--951, 2000.

\end{thebibliography}
