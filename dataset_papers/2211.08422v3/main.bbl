\begin{thebibliography}{115}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2022)Ainsworth, Hayase, and Srinivasa]{ainsworth2022}
Ainsworth, S.~K., Hayase, J., and Srinivasa, S.
\newblock {Git re-basin: Merging models modulo permutation symmetries}.
\newblock \emph{arXiv preprint. arXiv:2209.04836}, 2022.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint. arXiv:1907.02893}, 2019.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock {On the optimization of deep networks: Implicit acceleration by
  overparameterization}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2018.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock {Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2019.

\bibitem[Arpit et~al.(2022)Arpit, Wang, Zhou, and Xiong]{arpit2021ensemble}
Arpit, D., Wang, H., Zhou, Y., and Xiong, C.
\newblock Ensemble of averages: Improving model selection and boosting
  performance in domain generalization.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[Balestriero(2017)]{balestriero2017neural}
Balestriero, R.
\newblock Neural decision trees.
\newblock \emph{arXiv preprint. arXiv:1702.07360}, 2017.

\bibitem[Balestriero \& Baraniuk(2018)Balestriero and
  Baraniuk]{balestriero2018mad}
Balestriero, R. and Baraniuk, R.
\newblock Mad max: Affine spline insights into deep learning.
\newblock \emph{arXiv preprint. arXiv:1805.06576}, 2018.

\bibitem[Balestriero et~al.(2018)]{balestriero2018spline}
Balestriero, R. et~al.
\newblock A spline theory of deep learning.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2018.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Beery, S., Van~Horn, G., and Perona, P.
\newblock {Recognition in terra incognita}.
\newblock In \emph{Proc.\ Euro.\ Conf.\ on Computer Vision (ECCV)}, 2018.

\bibitem[Benton et~al.(2021)Benton, Maddox, Lotfi, and Wilson]{benton2021loss}
Benton, G., Maddox, W., Lotfi, S., and Wilson, A. G.~G.
\newblock {Loss surface simplexes for mode connecting volumes and fast
  ensembling}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2021.

\bibitem[Besserve et~al.(2018{\natexlab{a}})Besserve, Mehrjou, Sun, and
  Sch{\"o}lkopf]{besserve2018counterfactuals}
Besserve, M., Mehrjou, A., Sun, R., and Sch{\"o}lkopf, B.
\newblock {Counterfactuals uncover the modular structure of deep generative
  models}.
\newblock \emph{arXiv preprint. arXiv:1812.03253}, 2018{\natexlab{a}}.

\bibitem[Besserve et~al.(2018{\natexlab{b}})Besserve, Shajarisales,
  Sch{\"o}lkopf, and Janzing]{besserve2018group}
Besserve, M., Shajarisales, N., Sch{\"o}lkopf, B., and Janzing, D.
\newblock {Group invariance principles for causal generative models}.
\newblock In \emph{Int.\ Conf.\ on Artificial Intelligence and Statistics
  (AISTATS)}, 2018{\natexlab{b}}.

\bibitem[Bronstein et~al.(2021)Bronstein, Bruna, Cohen, and
  Veli{\v{c}}kovi{\'c}]{bronstein2021geometric}
Bronstein, M.~M., Bruna, J., Cohen, T., and Veli{\v{c}}kovi{\'c}, P.
\newblock Geometric deep learning: Grids, groups, graphs, geodesics, and
  gauges.
\newblock \emph{arXiv preprint. arXiv:2104.13478}, 2021.

\bibitem[Dasgupta et~al.(2022)Dasgupta, Grant, and
  Griffiths]{dasgupta2022distinguishing}
Dasgupta, I., Grant, E., and Griffiths, T.
\newblock Distinguishing rule and exemplar-based generalization in learning
  systems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4816--4830. PMLR, 2022.

\bibitem[Dittadi et~al.(2020)Dittadi, Tr{\"a}uble, Locatello, W{\"u}thrich,
  Agrawal, Winther, Bauer, and Sch{\"o}lkopf]{dittadi2020transfer}
Dittadi, A., Tr{\"a}uble, F., Locatello, F., W{\"u}thrich, M., Agrawal, V.,
  Winther, O., Bauer, S., and Sch{\"o}lkopf, B.
\newblock {On the transfer of disentangled representations in realistic
  settings}.
\newblock \emph{arXiv preprint. arXiv:2010.14407}, 2020.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.
\newblock {Essentially no barriers in neural network energy landscape}.
\newblock In \emph{Proc.\ Int.\ conf.\ on machine learning (ICML)}, 2018.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock {Gradient descent finds global minima of deep neural networks}.
\newblock In \emph{Proc.\ Int.\ conf.\ on machine learning (ICML)}, 2019.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Hu, and Lee]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock {Gradient descent provably optimizes over-parameterized neural
  networks}.
\newblock \emph{arXiv preprint. arXiv:1810.02054}, 2018{\natexlab{b}}.

\bibitem[D’Amour et~al.(2020)D’Amour, Heller, Moldovan, Adlam, Alipanahi,
  Beutel, Chen, Deaton, Eisenstein, Hoffman, et~al.]{d2020underspecification}
D’Amour, A., Heller, K., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A.,
  Chen, C., Deaton, J., Eisenstein, J., Hoffman, M.~D., et~al.
\newblock Underspecification presents challenges for credibility in modern
  machine learning.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2020.

\bibitem[Entezari et~al.(2021)Entezari, Sedghi, Saukh, and
  Neyshabur]{entezari2021role}
Entezari, R., Sedghi, H., Saukh, O., and Neyshabur, B.
\newblock {The role of permutation invariance in linear mode connectivity of
  neural networks}.
\newblock \emph{arXiv preprint. arXiv:2110.06296}, 2021.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock {Linear mode connectivity and the lottery ticket hypothesis}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2020.

\bibitem[Freeman \& Bruna(2016)Freeman and Bruna]{freeman2016topology}
Freeman, C.~D. and Bruna, J.
\newblock {Topology and geometry of half-rectified network optimization}.
\newblock \emph{arXiv preprint. arXiv:1611.01540}, 2016.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock {Loss surfaces, mode connectivity, and fast ensembling of dnns}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem[Geirhos et~al.(2018)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{geirhos2018imagenet}
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.~A., and
  Brendel, W.
\newblock {ImageNet-trained CNNs are biased towards texture; increasing shape
  bias improves accuracy and robustness}.
\newblock \emph{arXiv preprint. arXiv:1811.12231}, 2018.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos2020shortcut}
Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge,
  M., and Wichmann, F.~A.
\newblock {Shortcut learning in deep neural networks}.
\newblock \emph{Nature Machine Intelligence}, 2020.

\bibitem[Gresele et~al.(2020)Gresele, Rubenstein, Mehrjou, Locatello, and
  Sch{\"o}lkopf]{gresele2020incomplete}
Gresele, L., Rubenstein, P.~K., Mehrjou, A., Locatello, F., and Sch{\"o}lkopf,
  B.
\newblock {The incomplete rosetta stone problem: Identifiability results for
  multi-view nonlinear ica}.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, 2020.

\bibitem[Gresele et~al.(2021)Gresele, Von~K{\"u}gelgen, Stimper, Sch{\"o}lkopf,
  and Besserve]{gresele2021independent}
Gresele, L., Von~K{\"u}gelgen, J., Stimper, V., Sch{\"o}lkopf, B., and
  Besserve, M.
\newblock {Independent mechanism analysis, a new concept?}
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem[He et~al.(2019)He, Girshick, and Doll{\'a}r]{he2019rethinking}
He, K., Girshick, R., and Doll{\'a}r, P.
\newblock Rethinking imagenet pre-training.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Computer Vision}, 2019.

\bibitem[Hecht-Nielsen(1990)]{hecht1990algebraic}
Hecht-Nielsen, R.
\newblock {On the algebraic structure of feedforward network weight spaces}.
\newblock In \emph{Advanced Neural Computers}. Elsevier, 1990.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,
  R., Zhu, T., Parajuli, S., Guo, M., et~al.
\newblock {The many faces of robustness: A critical analysis of
  out-of-distribution generalization}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Computer Vision (ICCV)}, 2021.

\bibitem[Hermann \& Lampinen(2020)Hermann and Lampinen]{hermann2020shapes}
Hermann, K. and Lampinen, A.
\newblock {What shapes feature representations? exploring datasets,
  architectures, and training}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Hermann et~al.(2020)Hermann, Chen, and Kornblith]{hermann2020origins}
Hermann, K., Chen, T., and Kornblith, S.
\newblock {The origins and prevalence of texture bias in convolutional neural
  networks}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2016beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock {beta-vae: Learning basic visual concepts with a constrained
  variational framework}.
\newblock \emph{In Proc.\ Int.\ Conf.\ on Learning Representations (ICLR)},
  2017.

\bibitem[Hooker et~al.(2019)Hooker, Courville, Clark, Dauphin, and
  Frome]{hooker2019compressed}
Hooker, S., Courville, A., Clark, G., Dauphin, Y., and Frome, A.
\newblock {What do compressed deep neural networks forget?}
\newblock \emph{arXiv preprint. arXiv:1911.05248}, 2019.

\bibitem[Hu et~al.(2020)Hu, Xiao, Adlam, and Pennington]{hu2020surprising}
Hu, W., Xiao, L., Adlam, B., and Pennington, J.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Hyvarinen \& Morioka(2016)Hyvarinen and
  Morioka]{hyvarinen2016unsupervised}
Hyvarinen, A. and Morioka, H.
\newblock {Unsupervised feature extraction by time-contrastive learning and
  nonlinear ica}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2016.

\bibitem[Hyvarinen \& Morioka(2017)Hyvarinen and
  Morioka]{hyvarinen2017nonlinear}
Hyvarinen, A. and Morioka, H.
\newblock {Nonlinear ICA of temporally dependent stationary sources}.
\newblock In \emph{Proc. Int.\ Conf.\ on Artificial Intelligence and Statistics
  (AISTATS)}, 2017.

\bibitem[Islam et~al.(2021)Islam, Kowal, Esser, Jia, Ommer, Derpanis, and
  Bruce]{islam2021shape}
Islam, M.~A., Kowal, M., Esser, P., Jia, S., Ommer, B., Derpanis, K.~G., and
  Bruce, N.
\newblock {Shape or texture: Understanding discriminative features in cnns}.
\newblock \emph{arXiv preprint. arXiv:2101.11604}, 2021.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock {Averaging weights leads to wider optima and better generalization}.
\newblock \emph{arXiv preprint. arXiv:1803.05407}, 2018.

\bibitem[Jacobsen et~al.(2018)Jacobsen, Behrmann, Zemel, and
  Bethge]{jacobsen2018excessive}
Jacobsen, J.-H., Behrmann, J., Zemel, R., and Bethge, M.
\newblock {Excessive invariance causes adversarial vulnerability}.
\newblock \emph{arXiv preprint. arXiv:1811.00401}, 2018.

\bibitem[Juneja et~al.(2022)Juneja, Bansal, Cho, Sedoc, and
  Saphra]{juneja2022linear}
Juneja, J., Bansal, R., Cho, K., Sedoc, J., and Saphra, N.
\newblock {Linear connectivity reveals generalization strategies}.
\newblock \emph{arXiv preprint. arXiv:2205.12411}, 2022.

\bibitem[Kaddour et~al.(2022)Kaddour, Liu, Silva, and Kusner]{kaddourflat}
Kaddour, J., Liu, L., Silva, R., and Kusner, M.
\newblock When do flat minima optimizers work?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Kaur et~al.(2022)Kaur, Kiciman, and Sharma]{kaur2022modeling}
Kaur, J.~N., Kiciman, E., and Sharma, A.
\newblock Modeling the data-generating process is necessary for
  out-of-distribution generalization.
\newblock \emph{arXiv preprint. arXiv:2206.07837}, 2022.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kawaguchi, K.
\newblock {Deep learning without poor local minima}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2016.

\bibitem[Kawaguchi \& Kaelbling(2020)Kawaguchi and
  Kaelbling]{kawaguchi2020elimination}
Kawaguchi, K. and Kaelbling, L.
\newblock {Elimination of all bad local minima in deep learning}.
\newblock In \emph{Int.\ Conf.\ on Artificial Intelligence and Statistics
  (AISTATS)}, 2020.

\bibitem[Khemakhem et~al.(2020)Khemakhem, Kingma, Monti, and
  Hyvarinen]{khemakhem2020variational}
Khemakhem, I., Kingma, D., Monti, R., and Hyvarinen, A.
\newblock {Variational autoencoders and nonlinear ica: A unifying framework}.
\newblock In \emph{Int.\ Conf.\ on Artificial Intelligence and Statistics
  (AISTATS)}, 2020.

\bibitem[Khemakhem et~al.(2021)Khemakhem, Monti, Leech, and
  Hyvarinen]{khemakhem2021causal}
Khemakhem, I., Monti, R., Leech, R., and Hyvarinen, A.
\newblock {Causal autoregressive flows}.
\newblock In \emph{Int.\ conf.\ on artificial intelligence and statistics
  (AISTATS)}, 2021.

\bibitem[Kirichenko et~al.(2022{\natexlab{a}})Kirichenko, Izmailov, Gruver, and
  Wilson]{flkirichenko2022}
Kirichenko, P., Izmailov, P., Gruver, N., and Wilson, A.~G.
\newblock {On feature learning in the presence of spurious correlations}.
\newblock \emph{arXiv preprint. arXiv:2210.11369}, 2022{\natexlab{a}}.

\bibitem[Kirichenko et~al.(2022{\natexlab{b}})Kirichenko, Izmailov, and
  Wilson]{kirichenko2022last}
Kirichenko, P., Izmailov, P., and Wilson, A.~G.
\newblock {Last layer re-training is sufficient for robustness to spurious
  correlations}.
\newblock \emph{arXiv preprint. arXiv:2204.02937}, 2022{\natexlab{b}}.

\bibitem[Klindt et~al.(2020)Klindt, Schott, Sharma, Ustyuzhaninov, Brendel,
  Bethge, and Paiton]{klindt2020towards}
Klindt, D., Schott, L., Sharma, Y., Ustyuzhaninov, I., Brendel, W., Bethge, M.,
  and Paiton, D.
\newblock {Towards nonlinear disentanglement in natural data with temporal
  sparse coding}.
\newblock \emph{arXiv preprint. arXiv:2007.10930}, 2020.

\bibitem[Kuditipudi et~al.(2019)Kuditipudi, Wang, Lee, Zhang, Li, Hu, Ge, and
  Arora]{kuditipudi2019explaining}
Kuditipudi, R., Wang, X., Lee, H., Zhang, Y., Li, Z., Hu, W., Ge, R., and
  Arora, S.
\newblock {Explaining landscape connectivity of low-cost solutions for
  multilayer nets}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022fine}
Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P.
\newblock {Fine-tuning can distort pretrained features and underperform
  out-of-distribution}.
\newblock \emph{arXiv preprint. arXiv:2202.10054}, 2022.

\bibitem[Kunin et~al.(2021)Kunin, Sagastuy-Brena, Ganguli, Yamins, and
  Tanaka]{kunin2021neural}
Kunin, D., Sagastuy-Brena, J., Ganguli, S., Yamins, D.~L., and Tanaka, H.
\newblock Neural mechanics: Symmetry and broken conservation laws in deep
  learning dynamics.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Learning Representations (ICLR)},
  2021.

\bibitem[Li et~al.(2018)Li, Gong, Tian, Liu, and Tao]{li2018domain}
Li, Y., Gong, M., Tian, X., Liu, T., and Tao, D.
\newblock Domain generalization via conditional invariant representations.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem[Locatello et~al.(2019)Locatello, Bauer, Lucic, Raetsch, Gelly,
  Sch{\"o}lkopf, and Bachem]{locatello2019challenging}
Locatello, F., Bauer, S., Lucic, M., Raetsch, G., Gelly, S., Sch{\"o}lkopf, B.,
  and Bachem, O.
\newblock {Challenging common assumptions in the unsupervised learning of
  disentangled representations}.
\newblock In \emph{Proc.\ int.\ conf.\ on machine learning (ICML)}, 2019.

\bibitem[Locatello et~al.(2020)Locatello, Poole, R{\"a}tsch, Sch{\"o}lkopf,
  Bachem, and Tschannen]{locatello2020weakly}
Locatello, F., Poole, B., R{\"a}tsch, G., Sch{\"o}lkopf, B., Bachem, O., and
  Tschannen, M.
\newblock {Weakly-supervised disentanglement without compromises}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2020.

\bibitem[Lovering et~al.(2021)Lovering, Jha, Linzen, and
  Pavlick]{lovering2021predicting}
Lovering, C., Jha, R., Linzen, T., and Pavlick, E.
\newblock Predicting inductive biases of pre-trained models.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=mNtmhaDkAr}.

\bibitem[Lubana et~al.(2021)Lubana, Trivedi, Koutra, and
  Dick]{lubana2021quadratic}
Lubana, E.~S., Trivedi, P., Koutra, D., and Dick, R.~P.
\newblock How do quadratic regularizers prevent catastrophic forgetting: The
  role of interpolation.
\newblock \emph{arXiv preprint. arXiv:2102.02805}, 2021.

\bibitem[Lyu \& Li(2019)Lyu and Li]{lyu2019gradient}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint. arXiv:1906.05890}, 2019.

\bibitem[Maini et~al.(2022)Maini, Garg, Lipton, and
  Kolter]{maini2022characterizing}
Maini, P., Garg, S., Lipton, Z.~C., and Kolter, J.~Z.
\newblock {Characterizing Datapoints via Second-Split Forgetting}.
\newblock In \emph{ICML 2022: Workshop on Spurious Correlations, Invariance and
  Stability}, 2022.

\bibitem[Mangalam \& Prabhu(2019)Mangalam and Prabhu]{mangalam2019deep}
Mangalam, K. and Prabhu, V.~U.
\newblock {Do deep neural networks learn shallow learnable examples first?}
\newblock In \emph{ICML 2019 Workshop on Identifying and Understanding Deep
  Learning Phenomena}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HkxHv4rn24}.

\bibitem[Mania et~al.(2019)Mania, Miller, Schmidt, Hardt, and
  Recht]{mania2019model}
Mania, H., Miller, J., Schmidt, L., Hardt, M., and Recht, B.
\newblock {Model similarity mitigates test set overuse}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[McCoy et~al.(2019)McCoy, Pavlick, and Linzen]{mccoy2019right}
McCoy, R.~T., Pavlick, E., and Linzen, T.
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock \emph{arXiv preprint arXiv:1902.01007}, 2019.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and
  Zettlemoyer]{min2022rethinking}
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and
  Zettlemoyer, L.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock \emph{arXiv preprint arXiv:2202.12837}, 2022.

\bibitem[Mireshghallah et~al.(2022)Mireshghallah, Uniyal, Wang, Evans, and
  Berg-Kirkpatrick]{mireshghallah2022empirical}
Mireshghallah, F., Uniyal, A., Wang, T., Evans, D.~K., and Berg-Kirkpatrick, T.
\newblock An empirical analysis of memorization in fine-tuned autoregressive
  language models.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1816--1826, 2022.

\bibitem[Mirzadeh et~al.(2020)Mirzadeh, Farajtabar, Gorur, Pascanu, and
  Ghasemzadeh]{mirzadeh2020linear}
Mirzadeh, S.~I., Farajtabar, M., Gorur, D., Pascanu, R., and Ghasemzadeh, H.
\newblock {Linear mode connectivity in multitask and continual learning}.
\newblock \emph{arXiv preprint. arXiv:2010.04495}, 2020.

\bibitem[Mitchell et~al.(2022)Mitchell, Lin, Bosselut, Finn, and
  Manning]{mitchell2021fast}
Mitchell, E., Lin, C., Bosselut, A., Finn, C., and Manning, C.~D.
\newblock Fast model editing at scale.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Learning Representations (ICLR)},
  2022.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{nacson2019convergence}
Nacson, M.~S., Lee, J., Gunasekar, S., Savarese, P. H.~P., Srebro, N., and
  Soudry, D.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{Int.\ Conf.\ on Artificial Intelligence and Statistics
  (AISTATS)}, 2019.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kalimeris, Kaplun, Edelman, Yang,
  Barak, and Zhang]{kalimeris2019sgd}
Nakkiran, P., Kalimeris, D., Kaplun, G., Edelman, B., Yang, T., Barak, B., and
  Zhang, H.
\newblock {Sgd on neural networks learns functions of increasing complexity}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[Nanda et~al.(2022)Nanda, Speicher, Kolling, Dickerson, Gummadi, and
  Weller]{nanda2022measuring}
Nanda, V., Speicher, T., Kolling, C., Dickerson, J.~P., Gummadi, K., and
  Weller, A.
\newblock Measuring representational robustness of neural networks through
  shared invariances.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2022.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and
  Zhang]{neyshabur2020being}
Neyshabur, B., Sedghi, H., and Zhang, C.
\newblock {What is being transferred in transfer learning?}
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Nguyen(2019)]{nguyen2019connected}
Nguyen, Q.
\newblock {On connected sublevel sets in deep learning}.
\newblock In \emph{Proc.\ Int.\ conf.\ on machine learning (ICML)}, 2019.

\bibitem[Nguyen \& Hein(2017)Nguyen and Hein]{nguyen2017loss}
Nguyen, Q. and Hein, M.
\newblock {The loss surface of deep and wide neural networks}.
\newblock In \emph{Proc.\ Int.\ conf.\ on machine learning (ICML)}, 2017.

\bibitem[Nguyen \& Hein(2018)Nguyen and Hein]{nguyen2018optimization}
Nguyen, Q. and Hein, M.
\newblock {Optimization landscape and expressivity of deep CNNs}.
\newblock In \emph{Proc.\ Int.\ conf.\ on machine learning (ICML)}, 2018.

\bibitem[Nguyen \& Mondelli(2020)Nguyen and Mondelli]{nguyen2020global}
Nguyen, Q. and Mondelli, M.
\newblock {Global convergence of deep networks with one wide layer followed by
  pyramidal topology}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Nguyen et~al.(2018)Nguyen, Mukkamala, and Hein]{nguyen2018loss}
Nguyen, Q., Mukkamala, M.~C., and Hein, M.
\newblock {On the loss landscape of a class of deep neural networks with no bad
  local valleys}.
\newblock \emph{arXiv preprint. arXiv:1809.10749}, 2018.

\bibitem[Nguyen et~al.(2021)Nguyen, Br{\'e}chet, and
  Mondelli]{nguyen2021solutions}
Nguyen, Q., Br{\'e}chet, P., and Mondelli, M.
\newblock {When are solutions connected in deep networks?}
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Panigrahi et~al.(2023)Panigrahi, Saunshi, Zhao, and
  Arora]{panigrahi2023task}
Panigrahi, A., Saunshi, N., Zhao, H., and Arora, S.
\newblock Task-specific skill localization in fine-tuned language models.
\newblock \emph{arXiv preprint arXiv:2302.06600}, 2023.

\bibitem[Peters et~al.(2017)Peters, Janzing, and
  Sch{\"o}lkopf]{peters2017elements}
Peters, J., Janzing, D., and Sch{\"o}lkopf, B.
\newblock \emph{{Elements of causal inference: foundations and learning
  algorithms}}.
\newblock The MIT Press, 2017.

\bibitem[Pittorino et~al.(2022)Pittorino, Ferraro, Perugini, Feinauer,
  Baldassi, and Zecchina]{pittorino2022deep}
Pittorino, F., Ferraro, A., Perugini, G., Feinauer, C., Baldassi, C., and
  Zecchina, R.
\newblock Deep networks on toroids: Removing symmetries reveals the structure
  of flat regions in the landscape geometry.
\newblock \emph{arXiv preprint arXiv:2202.03038}, 2022.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2019spectral}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A.
\newblock {On the spectral bias of neural networks}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2019.

\bibitem[Rame et~al.(2022)Rame, Kirchmeyer, Rahier, Rakotomamonjy, Gallinari,
  and Cord]{rame2022diverse}
Rame, A., Kirchmeyer, M., Rahier, T., Rakotomamonjy, A., Gallinari, P., and
  Cord, M.
\newblock {Diverse weight averaging for out-of-distribution generalization}.
\newblock \emph{arXiv preprint. arXiv:2205.09739}, 2022.

\bibitem[Ritter et~al.(2017)Ritter, Barrett, Santoro, and
  Botvinick]{ritter2017cognitive}
Ritter, S., Barrett, D.~G., Santoro, A., and Botvinick, M.~M.
\newblock {Cognitive psychology for deep neural networks: A shape bias case
  study}.
\newblock In \emph{Proc.\ Int.\ conf.\ on machine learning (ICML)}, 2017.

\bibitem[Roburin et~al.(2022)Roburin, de~Mont-Marin, Bursuc, Marlet, P{\'e}rez,
  and Aubry]{roburin2022spherical}
Roburin, S., de~Mont-Marin, Y., Bursuc, A., Marlet, R., P{\'e}rez, P., and
  Aubry, M.
\newblock Spherical perspective on learning with normalization layers.
\newblock \emph{Neurocomputing}, 2022.

\bibitem[Rosenfeld et~al.(2022)Rosenfeld, Ravikumar, and
  Risteski]{rosenfeld2022domain}
Rosenfeld, E., Ravikumar, P., and Risteski, A.
\newblock Domain-adjusted regression or: Erm may already learn features
  sufficient for out-of-distribution generalization.
\newblock \emph{arXiv preprint. arXiv:2202.06856}, 2022.

\bibitem[Santurkar et~al.(2021)Santurkar, Tsipras, Elango, Bau, Torralba, and
  Madry]{santurkar2021editing}
Santurkar, S., Tsipras, D., Elango, M., Bau, D., Torralba, A., and Madry, A.
\newblock {Editing a classifier by rewriting its prediction rules}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Sch{\"o}lkopf et~al.(2021)Sch{\"o}lkopf, Locatello, Bauer, Ke,
  Kalchbrenner, Goyal, and Bengio]{scholkopf2021towards}
Sch{\"o}lkopf, B., Locatello, F., Bauer, S., Ke, N.~R., Kalchbrenner, N.,
  Goyal, A., and Bengio, Y.
\newblock {Towards causal representation learning}.
\newblock \emph{arXiv preprint. arXiv:2102.11107}, 2021.

\bibitem[Scimeca et~al.(2021)Scimeca, Oh, Chun, Poli, and
  Yun]{scimeca2021shortcut}
Scimeca, L., Oh, S.~J., Chun, S., Poli, M., and Yun, S.
\newblock {Which shortcut cues will dnns choose? a study from the
  parameter-space perspective}.
\newblock \emph{arXiv preprint. arXiv:2110.03095}, 2021.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{shah2020pitfalls}
Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P.
\newblock {The pitfalls of simplicity bias in neural networks}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Simsek et~al.(2021)Simsek, Ged, Jacot, Spadaro, Hongler, Gerstner, and
  Brea]{simsek2021geometry}
Simsek, B., Ged, F., Jacot, A., Spadaro, F., Hongler, C., Gerstner, W., and
  Brea, J.
\newblock {Geometry of the loss landscape in overparameterized neural networks:
  Symmetries and invariances}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2021.

\bibitem[Singh \& Jaggi(2020)Singh and Jaggi]{singh2020model}
Singh, S.~P. and Jaggi, M.
\newblock {Model fusion via optimal transport}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Sinitsin et~al.(2020)Sinitsin, Plokhotnyuk, Pyrkin, Popov, and
  Babenko]{sinitsin2020editable}
Sinitsin, A., Plokhotnyuk, V., Pyrkin, D., Popov, S., and Babenko, A.
\newblock {Editable neural networks}.
\newblock \emph{arXiv preprint. arXiv:2004.00345}, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2018.

\bibitem[Sun \& Saenko(2016)Sun and Saenko]{sun2016deep}
Sun, B. and Saenko, K.
\newblock Deep coral: Correlation alignment for deep domain adaptation.
\newblock In \emph{European conference on computer vision}, pp.\  443--450.
  Springer, 2016.

\bibitem[Tanaka \& Kunin(2021)Tanaka and Kunin]{tanaka2021noether}
Tanaka, H. and Kunin, D.
\newblock Noether’s learning dynamics: Role of symmetry breaking in neural
  networks.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{taori2020measuring}
Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.
\newblock {Measuring robustness to natural distribution shifts in image
  classification}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Teney et~al.(2022)Teney, Peyrard, and Abbasnejad]{teney2022predicting}
Teney, D., Peyrard, M., and Abbasnejad, E.
\newblock Predicting is not understanding: Recognizing and addressing
  underspecification in machine learning.
\newblock \emph{arXiv preprint. arXiv:2207.02598}, 2022.

\bibitem[Thiagarajan et~al.(2021)Thiagarajan, Narayanaswamy, Rajan, Liang,
  Chaudhari, and Spanias]{thiagarajan2021designing}
Thiagarajan, J., Narayanaswamy, V.~S., Rajan, D., Liang, J., Chaudhari, A., and
  Spanias, A.
\newblock Designing counterfactual generators using deep model inversion.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16873--16884, 2021.

\bibitem[Toneva et~al.(2018)Toneva, Sordoni, Combes, Trischler, Bengio, and
  Gordon]{toneva2018empirical}
Toneva, M., Sordoni, A., Combes, R. T.~d., Trischler, A., Bengio, Y., and
  Gordon, G.~J.
\newblock {An empirical study of example forgetting during deep neural network
  learning}.
\newblock \emph{arXiv preprint. arXiv:1812.05159}, 2018.

\bibitem[Trivedi et~al.(2022{\natexlab{a}})Trivedi, Lubana, Heimann, Koutra,
  and Thiagarajan]{trivedi2022analyzing}
Trivedi, P., Lubana, E.~S., Heimann, M., Koutra, D., and Thiagarajan, J.~J.
\newblock {Analyzing Data-Centric Properties for Contrastive Learning on
  Graphs}.
\newblock \emph{arXiv preprint. arXiv:2208.02810}, 2022{\natexlab{a}}.

\bibitem[Trivedi et~al.(2022{\natexlab{b}})Trivedi, Lubana, Yan, Yang, and
  Koutra]{trivedi2022augmentations}
Trivedi, P., Lubana, E.~S., Yan, Y., Yang, Y., and Koutra, D.
\newblock Augmentations in graph contrastive learning: Current methodological
  flaws \& towards better practices.
\newblock In \emph{Proceedings of the ACM Web Conference 2022}, pp.\
  1538--1549, 2022{\natexlab{b}}.

\bibitem[Truncated Gaussian Distribution()]{trunc}
Truncated Gaussian Distribution.
\newblock Truncated normal distribution, 2022.
\newblock URL
  \url{{https://en.wikipedia.org/wiki/Truncated_normal_distribution}}.

\bibitem[Valle-Perez et~al.(2018)Valle-Perez, Camargo, and
  Louis]{valle2018deep}
Valle-Perez, G., Camargo, C.~Q., and Louis, A.~A.
\newblock {Deep learning generalizes because the parameter-function map is
  biased towards simple functions}.
\newblock \emph{arXiv preprint. arXiv:1805.08522}, 2018.

\bibitem[Van~Steenkiste et~al.(2019)Van~Steenkiste, Locatello, Schmidhuber, and
  Bachem]{van2019disentangled}
Van~Steenkiste, S., Locatello, F., Schmidhuber, J., and Bachem, O.
\newblock {Are disentangled representations helpful for abstract visual
  reasoning?}
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[Von~K{\"u}gelgen et~al.(2021)Von~K{\"u}gelgen, Sharma, Gresele,
  Brendel, Sch{\"o}lkopf, Besserve, and Locatello]{von2021self}
Von~K{\"u}gelgen, J., Sharma, Y., Gresele, L., Brendel, W., Sch{\"o}lkopf, B.,
  Besserve, M., and Locatello, F.
\newblock {Self-supervised learning with data augmentations provably isolates
  content from style}.
\newblock \emph{Adv.\ in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Wan et~al.(2020)Wan, Zhu, Zhang, and Sun]{wan2020spherical}
Wan, R., Zhu, Z., Zhang, X., and Sun, J.
\newblock Spherical motion dynamics: Learning dynamics of neural network with
  normalization, weight decay, and sgd.
\newblock \emph{arXiv preprint. arXiv:2006.08419}, 2020.

\bibitem[Wang et~al.(2018)Wang, Balestriero, and Baraniuk]{wang2018max}
Wang, Z., Balestriero, R., and Baraniuk, R.
\newblock A max-affine spline perspective of recurrent neural networks.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Learning Representations (ICLR)},
  2018.

\bibitem[Wiles et~al.(2021)Wiles, Gowal, Stimberg, Alvise-Rebuffi, Ktena,
  Cemgil, et~al.]{wiles2021fine}
Wiles, O., Gowal, S., Stimberg, F., Alvise-Rebuffi, S., Ktena, I., Cemgil, T.,
  et~al.
\newblock {A fine-grained analysis on distribution shift}.
\newblock \emph{arXiv preprint. arXiv:2110.11328}, 2021.

\bibitem[Wortsman et~al.(2021)Wortsman, Horton, Guestrin, Farhadi, and
  Rastegari]{wortsman2021learning}
Wortsman, M., Horton, M.~C., Guestrin, C., Farhadi, A., and Rastegari, M.
\newblock {Learning neural network subspaces}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)}, 2021.

\bibitem[Wortsman et~al.(2022{\natexlab{a}})Wortsman, Ilharco, Gadre, Roelofs,
  Gontijo-Lopes, Morcos, Namkoong, Farhadi, Carmon, Kornblith,
  et~al.]{wortsman2022model}
Wortsman, M., Ilharco, G., Gadre, S.~Y., Roelofs, R., Gontijo-Lopes, R.,
  Morcos, A.~S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et~al.
\newblock {Model soups: averaging weights of multiple fine-tuned models
  improves accuracy without increasing inference time}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Machine Learning (ICML)},
  2022{\natexlab{a}}.

\bibitem[Wortsman et~al.(2022{\natexlab{b}})Wortsman, Ilharco, Kim, Li,
  Kornblith, Roelofs, Lopes, Hajishirzi, Farhadi, Namkoong,
  et~al.]{wortsman2022robust}
Wortsman, M., Ilharco, G., Kim, J.~W., Li, M., Kornblith, S., Roelofs, R.,
  Lopes, R.~G., Hajishirzi, H., Farhadi, A., Namkoong, H., et~al.
\newblock {Robust fine-tuning of zero-shot models}.
\newblock In \emph{Proc.\ Int.\ Conf.\ on Computer Vision and Pattern
  Recognition (CVPR)}, 2022{\natexlab{b}}.

\bibitem[Xiao et~al.(2020)Xiao, Engstrom, Ilyas, and Madry]{xiao2020noise}
Xiao, K., Engstrom, L., Ilyas, A., and Madry, A.
\newblock {Noise or signal: The role of image backgrounds in object
  recognition}.
\newblock \emph{arXiv preprint. arXiv:2006.09994}, 2020.

\bibitem[Zhao et~al.(2020)Zhao, Chen, Das, Ramamurthy, and
  Lin]{zhao2020bridging}
Zhao, P., Chen, P.-Y., Das, P., Ramamurthy, K.~N., and Lin, X.
\newblock {Bridging mode connectivity in loss landscapes and adversarial
  robustness}.
\newblock \emph{arXiv preprint. arXiv:2005.00060}, 2020.

\end{thebibliography}
