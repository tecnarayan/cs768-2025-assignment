\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{Black_Anandan_2002}
Black, M., Anandan, P.: {A Framework for the Robust Estimation of Optical Flow}. In: 1993 (4th) International Conference on Computer Vision (Dec 2002)

\bibitem{Bruhn_Weickert_Schnörr_2005}
Bruhn, A., Weickert, J., Schnörr, C.: Lucas/kanade meets horn/schunck: combining local and global optic flow methods. International Journal of Computer Vision,International Journal of Computer Vision  (Feb 2005)

\bibitem{caelles2017one}
Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taix{\'e}, L., Cremers, D., Van~Gool, L.: {One-Shot Video Object Segmentation}. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 221--230 (2017)

\bibitem{carion2020end}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: {End-to-End Object Detection with Transformers}. In: European conference on computer vision. pp. 213--229. Springer (2020)

\bibitem{carreira2017quo}
Carreira, J., Zisserman, A.: {Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset}. In: proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 6299--6308 (2017)

\bibitem{doersch2022tap}
Doersch, C., Gupta, A., Markeeva, L., Recasens, A., Smaira, L., Aytar, Y., Carreira, J., Zisserman, A., Yang, Y.: {TAP-Vid: A Benchmark for Tracking Any Point in a Video}. Advances in Neural Information Processing Systems  \textbf{35},  13610--13626 (2022)

\bibitem{doersch2024bootstap}
Doersch, C., Yang, Y., Gokay, D., Luc, P., Koppula, S., Gupta, A., Heyward, J., Goroshin, R., Carreira, J., Zisserman, A.: {BootsTAP: Bootstrapped Training for Tracking-Any-Point}. arXiv preprint arXiv:2402.00847  (2024)

\bibitem{doersch2023tapir}
Doersch, C., Yang, Y., Vecerik, M., Gokay, D., Gupta, A., Aytar, Y., Carreira, J., Zisserman, A.: {TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement}. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10061--10072 (2023)

\bibitem{Dosovitskiy_Fischer_Ilg_Hausser_Hazirbas_Golkov_Smagt_Cremers_Brox_2015}
Dosovitskiy, A., Fischer, P., Ilg, E., Hausser, P., Hazirbas, C., Golkov, V., Smagt, P.v.d., Cremers, D., Brox, T.: {FlowNet: Learning Optical Flow with Convolutional Networks}. In: 2015 IEEE International Conference on Computer Vision (ICCV) (Dec 2015)

\bibitem{duisterhof2023md}
Duisterhof, B.P., Mandi, Z., Yao, Y., Liu, J.W., Shou, M.Z., Song, S., Ichnowski, J.: {MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly Deformable Scenes}. arXiv preprint arXiv:2312.00583  (2023)

\bibitem{greff2021kubric}
Greff, K., Belletti, F., Beyer, L., Doersch, C., Du, Y., Duckworth, D., Fleet, D.J., Gnanapragasam, D., Golemo, F., Herrmann, C., Kipf, T., Kundu, A., Lagun, D., Laradji, I., Liu, H.T.D., Meyer, H., Miao, Y., Nowrouzezahrai, D., Oztireli, C., Pot, E., Radwan, N., Rebain, D., Sabour, S., Sajjadi, M.S.M., Sela, M., Sitzmann, V., Stone, A., Sun, D., Vora, S., Wang, Z., Wu, T., Yi, K.M., Zhong, F., Tagliasacchi, A.: Kubric: a scalable dataset generator  (2022)

\bibitem{gu2023videoswap}
Gu, Y., Zhou, Y., Wu, B., Yu, L., Liu, J.W., Zhao, R., Wu, J.Z., Zhang, D.J., Shou, M.Z., Tang, K.: {VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence}. arXiv preprint arXiv:2312.02087  (2023)

\bibitem{harley2022particle}
Harley, A.W., Fang, Z., Fragkiadaki, K.: {Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories}. In: European Conference on Computer Vision. pp. 59--75. Springer (2022)

\bibitem{Horn_Schunck_1981}
Horn, B.K., Schunck, B.G.: {Determining Optical Flow}. Artificial Intelligence p. 185–203 (Aug 1981)

\bibitem{Huang_Shi_Zhang_Wang_Cheung_Qin_Dai_Li}
Huang, Z., Shi, X., Zhang, C., Wang, Q., Cheung, K., Qin, H., Dai, J., Li, H.: {FlowFormer: A Transformer Architecture for Optical Flow}

\bibitem{Ilg_Mayer_Saikia_Keuper_Dosovitskiy_Brox_2017}
Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: {FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks}. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Jul 2017)

\bibitem{jiang2024t}
Jiang, Q., Li, F., Zeng, Z., Ren, T., Liu, S., Zhang, L.: {T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy}. arXiv preprint arXiv:2403.14610  (2024)

\bibitem{Jiang_Lu_Li_Hartley_2021}
Jiang, S., Lu, Y., Li, H., Hartley, R.: {Learning Optical Flow from a Few Matches}. In: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Jun 2021)

\bibitem{karaev2023cotracker}
Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., Rupprecht, C.: {CoTracker: It is Better to Track Together}. arXiv preprint arXiv:2307.07635  (2023)

\bibitem{klinker2011exponential}
Klinker, F.: {Exponential moving average versus moving exponential average}. Mathematische Semesterberichte  \textbf{58},  97--107 (2011)

\bibitem{kratimenos2023dynmf}
Kratimenos, A., Lei, J., Daniilidis, K.: {DynMF: Neural Motion Factorization for Real-time Dynamic View Synthesis with 3D Gaussian Splatting}. arXiv preprint arXiv:2312.00112  (2023)

\bibitem{li2023visual}
Li, F., Jiang, Q., Zhang, H., Ren, T., Liu, S., Zou, X., Xu, H., Li, H., Li, C., Yang, J., et~al.: Visual in-context prompting. arXiv preprint arXiv:2311.13601  (2023)

\bibitem{li2023lite}
Li, F., Zeng, A., Liu, S., Zhang, H., Li, H., Zhang, L., Ni, L.M.: {Lite DETR: An Interleaved Multi-Scale Encoder for Efficient DETR}. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 18558--18567 (2023)

\bibitem{li2022dn}
Li, F., Zhang, H., Liu, S., Guo, J., Ni, L.M., Zhang, L.: {DN-DETR: Accelerate DETR Training by Introducing Query DeNoising}. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 13619--13627 (2022)

\bibitem{li2024taptr}
Li, H., Zhang, H., Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, L.: {TAPTR: Tracking Any Point with Transformers as Detection}. arXiv preprint arXiv:2403.13042  (2024)

\bibitem{liu2022dab}
Liu, S., Li, F., Zhang, H., Yang, X., Qi, X., Su, H., Zhu, J., Zhang, L.: {DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR}. arXiv preprint arXiv:2201.12329  (2022)

\bibitem{luiten2023dynamic}
Luiten, J., Kopanas, G., Leibe, B., Ramanan, D.: {Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis}. arXiv preprint arXiv:2308.09713  (2023)

\bibitem{meinhardt2022trackformer}
Meinhardt, T., Kirillov, A., Leal-Taixe, L., Feichtenhofer, C.: {TrackFormer: Multi-Object Tracking with Transformers}. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8844--8854 (2022)

\bibitem{meng2021conditional}
Meng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y., Sun, L., Wang, J.: {Conditional DETR for Fast Training Convergence}. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3651--3660 (2021)

\bibitem{moing2023dense}
Moing, G.L., Ponce, J., Schmid, C.: {Dense Optical Tracking: Connecting the Dots}. arXiv preprint arXiv:2312.00786  (2023)

\bibitem{neoral2024mft}
Neoral, M., {\v{S}}er{\`y}ch, J., Matas, J.: {MFT: Long-Term Tracking of Every Pixel}. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 6837--6847 (2024)

\bibitem{oh2019video}
Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using space-time memory networks. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 9226--9235 (2019)

\bibitem{pont20172017}
Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel{\'a}ez, P., Sorkine-Hornung, A., Van~Gool, L.: {The 2017 DAVIS Challenge on Video Object Segmentation}. arXiv preprint arXiv:1704.00675  (2017)

\bibitem{Shi_Huang_Li_Zhang_Cheung_See_Qin_Dai_Li_2023}
Shi, X., Huang, Z., Li, D., Zhang, M., Cheung, K., See, S., Qin, H., Dai, J., Li, H.: {FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation}  (Mar 2023)

\bibitem{song2024track}
Song, Y., Lei, J., Wang, Z., Liu, L., Daniilidis, K.: {Track Everything Everywhere Fast and Robustly}. arXiv preprint arXiv:2403.17931  (2024)

\bibitem{Sun_Yang_Liu_Kautz_2018}
Sun, D., Yang, X., Liu, M.Y., Kautz, J.: {PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume}. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (Jun 2018)

\bibitem{sun2012transtrack}
Sun, P., Cao, J., Jiang, Y., Zhang, R., Xie, E., Yuan, Z., Wang, C., Luo, P.: {TransTrack: Multiple Object Tracking with Transformer}. arXiv preprint arXiv:2012.15460  (2012)

\bibitem{teed2020raft}
Teed, Z., Deng, J.: {RAFT: Recurrent All-Pairs Field Transforms for Optical Flow}. In: Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16. pp. 402--419. Springer (2020)

\bibitem{vecerik2023robotap}
Vecerik, M., Doersch, C., Yang, Y., Davchev, T., Aytar, Y., Zhou, G., Hadsell, R., Agapito, L., Scholz, J.: {RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation}. arXiv preprint arXiv:2308.15975  (2023)

\bibitem{voigtlaender2019feelvos}
Voigtlaender, P., Chai, Y., Schroff, F., Adam, H., Leibe, B., Chen, L.C.: {FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation}. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 9481--9490 (2019)

\bibitem{Wang_Zhong_Dai_Zhang_Ji_Li_2020}
Wang, J., Zhong, Y., Dai, Y., Zhang, K., Ji, P., Li, H.: {Displacement-Invariant Matching Cost Learning for Accurate Optical Flow Estimation}. Cornell University - arXiv,Cornell University - arXiv  (Oct 2020)

\bibitem{wang2023tracking}
Wang, Q., Chang, Y.Y., Cai, R., Li, Z., Hariharan, B., Holynski, A., Snavely, N.: {Tracking Everything Everywhere All at Once}. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 19795--19806 (2023)

\bibitem{wang2023omnimotion}
Wang, Q., Chang, Y.Y., Cai, R., Li, Z., Hariharan, B., Holynski, A., Snavely, N.: {Tracking Everything Everywhere All at Once}. ICCV  (2023)

\bibitem{xiao2024spatialtracker}
Xiao, Y., Wang, Q., Zhang, S., Xue, N., Peng, S., Shen, Y., Zhou, X.: Spatialtracker: Tracking any 2d pixels in 3d space. arXiv preprint arXiv:2404.04319  (2024)

\bibitem{Xu_Yang_Cai_Zhang_Tong_2021}
Xu, H., Yang, J., Cai, J., Zhang, J., Tong, X.: {High-Resolution Optical Flow from 1D Attention and Correlation}. Cornell University - arXiv,Cornell University - arXiv  (Apr 2021)

\bibitem{Xu_Ranftl_Koltun_2017}
Xu, J., Ranftl, R., Koltun, V.: {Accurate Optical Flow via Direct Cost Volume Processing}. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Jul 2017)

\bibitem{xu2018youtube}
Xu, N., Yang, L., Fan, Y., Yue, D., Liang, Y., Yang, J., Huang, T.: {A Large-Scale Video Object Segmentation Benchmark}. arXiv preprint arXiv:1809.03327  (2018)

\bibitem{xu2022transcenter}
Xu, Y., Ban, Y., Delorme, G., Gan, C., Rus, D., Alameda-Pineda, X.: {TransCenter: Transformers with Dense Representations for Multiple-Object Tracking}. IEEE transactions on pattern analysis and machine intelligence  \textbf{45}(6),  7820--7835 (2022)

\bibitem{yang2021associating}
Yang, Z., Wei, Y., Yang, Y.: {Associating Objects with Transformers for Video Object Segmentation}. Advances in Neural Information Processing Systems  \textbf{34},  2491--2502 (2021)

\bibitem{yin20234dgen}
Yin, Y., Xu, D., Wang, Z., Zhao, Y., Wei, Y.: {4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency}. arXiv preprint arXiv:2312.17225  (2023)

\bibitem{zeng2022motr}
Zeng, F., Dong, B., Zhang, Y., Wang, T., Zhang, X., Wei, Y.: {MOTR: End-to-End Multiple-Object Tracking with Transformer}. In: European Conference on Computer Vision. pp. 659--675. Springer (2022)

\bibitem{Zhang_Woodford_Prisacariu_Torr_2021}
Zhang, F., Woodford, O.J., Prisacariu, V., Torr, P.H.S.: {Separable Flow: Learning Motion Cost Volumes for Optical Flow Estimation}. In: 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct 2021)

\bibitem{zhang2022dino}
Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L.M., Shum, H.Y.: {DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}. arXiv preprint arXiv:2203.03605  (2022)

\bibitem{Zhao_Zhao_Zhang_Zhou_Metaxas}
Zhao, S., Zhao, L., Zhang, Z., Zhou, E., Metaxas, D.: {Global Matching with Overlapping Attention for Optical Flow Estimation}

\bibitem{zheng2023pointodyssey}
Zheng, Y., Harley, A.W., Shen, B., Wetzstein, G., Guibas, L.J.: {PointOdyssey: A Large-Scale Synthetic Dataset for Long-Term Point Tracking}. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 19855--19865 (2023)

\bibitem{zhu2020deformable}
Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: {Deformable DETR: Deformable Transformers for End-to-End Object Detection}. arXiv preprint arXiv:2010.04159  (2020)

\bibitem{zhuang2022understanding}
Zhuang, Z., Liu, M., Cutkosky, A., Orabona, F.: {Understanding AdamW through Proximal Methods and Scale-Freeness}. arXiv preprint arXiv:2202.00089  (2022)

\end{thebibliography}
