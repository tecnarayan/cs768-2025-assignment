\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Arjovsky, M., Shah, A., and Bengio, Y.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1120--1128, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Britz et~al.(2017)Britz, Guan, and Luong]{britz2017efficient}
Britz, D., Guan, M.~Y., and Luong, M.-T.
\newblock Efficient attention using a fixed-size memory representation.
\newblock \emph{arXiv preprint arXiv:1707.00110}, 2017.

\bibitem[Chan et~al.(2016)Chan, Jaitly, Le, and Vinyals]{chan2016listen}
Chan, W., Jaitly, N., Le, Q., and Vinyals, O.
\newblock Listen, attend and spell: A neural network for large vocabulary
  conversational speech recognition.
\newblock In \emph{2016 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  4960--4964. IEEE, 2016.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Chiu \& Raffel(2017)Chiu and Raffel]{chiu2017monotonic}
Chiu, C.-C. and Raffel, C.
\newblock Monotonic chunkwise attention.
\newblock \emph{arXiv preprint arXiv:1712.05382}, 2017.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Cohen, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Cohen, W.~W., Carbonell, J., Le, Q.~V., and
  Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dong et~al.(2018)Dong, Xu, and Xu]{dong2018speech}
Dong, L., Xu, S., and Xu, B.
\newblock Speech-transformer: a no-recurrence sequence-to-sequence model for
  speech recognition.
\newblock In \emph{2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pp.\  5884--5888. IEEE, 2018.

\bibitem[Gao \& Pavel(2017)Gao and Pavel]{gao2017properties}
Gao, B. and Pavel, L.
\newblock On the properties of the softmax function with application in game
  theory and reinforcement learning, 2017.

\bibitem[Godfrey et~al.(1992)Godfrey, Holliman, and
  McDaniel]{godfrey1992switchboard}
Godfrey, J.~J., Holliman, E.~C., and McDaniel, J.
\newblock Switchboard: Telephone speech corpus for research and development.
\newblock In \emph{[Proceedings] ICASSP-92: 1992 IEEE International Conference
  on Acoustics, Speech, and Signal Processing}, volume~1, pp.\  517--520. IEEE,
  1992.

\bibitem[Graves et~al.(2006)Graves, Fern\'{a}ndez, Gomez, and
  Schmidhuber]{graves2006ctc}
Graves, A., Fern\'{a}ndez, S., Gomez, F., and Schmidhuber, J.
\newblock Connectionist temporal classification: Labelling unsegmented sequence
  data with recurrent neural networks.
\newblock In \emph{Proceedings of the 23rd International Conference on Machine
  Learning}, 2006.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Bengio, Frasconi, and
  Schmidhuber]{hochreiter2001gradientflow}
Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies, 2001.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y.~W.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Liu et~al.(2020)Liu, Jiang, He, Chen, Liu, Gao, and Han]{liu2020radam}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgz2aEKDr}.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Ma et~al.(2020)Ma, Pino, Cross, Puzon, and Gu]{Ma2020Monotonic}
Ma, X., Pino, J.~M., Cross, J., Puzon, L., and Gu, J.
\newblock Monotonic multihead attention.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Hyg96gBKPS}.

\bibitem[Maybury(1999)]{maybury1999advances}
Maybury, M.
\newblock \emph{Advances in automatic text summarization}.
\newblock MIT press, 1999.

\bibitem[Oord et~al.(2016)Oord, Dieleman, Zen, Simonyan, Vinyals, Graves,
  Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
Oord, A. v.~d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A.,
  Kalchbrenner, N., Senior, A., and Kavukcuoglu, K.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Paul \& Baker(1992)Paul and Baker]{douglas1992wsj}
Paul, D.~B. and Baker, J.~M.
\newblock The design for the wall street journal-based csr corpus.
\newblock In \emph{Proceedings of the Workshop on Speech and Natural Language},
  HLT ’91, 1992.

\bibitem[Povey et~al.(2011)Povey, Ghoshal, Boulianne, Burget, Glembek, Goel,
  Hannemann, Motlicek, Qian, Schwarz, Silovsky, Stemmer, and
  Vesely]{povey2011kaldi}
Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N.,
  Hannemann, M., Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer,
  G., and Vesely, K.
\newblock The kaldi speech recognition toolkit.
\newblock In \emph{IEEE Workshop on Automatic Speech Recognition and
  Understanding.} IEEE Signal Processing Society, 2011.

\bibitem[Povey et~al.(2015)Povey, Zhang, and Khudanpur]{povey2015parallel}
Povey, D., Zhang, X., and Khudanpur, S.
\newblock Parallel training of dnns with natural gradient and parameter
  averaging.
\newblock In \emph{In International Conference on Learning Representations:
  Workshop track}, 2015.

\bibitem[Povey et~al.(2016)Povey, Peddinti, Galvez, Ghahremani, Manohar, Na,
  Wang, and Khudanpur]{povey2016purely}
Povey, D., Peddinti, V., Galvez, D., Ghahremani, P., Manohar, V., Na, X., Wang,
  Y., and Khudanpur, S.
\newblock Purely sequence-trained neural networks for asr based on lattice-free
  mmi.
\newblock In \emph{Interspeech}, pp.\  2751--2755, 2016.

\bibitem[Rajpurkar et~al.(2018)Rajpurkar, Jia, and Liang]{rajpurkar2018know}
Rajpurkar, P., Jia, R., and Liang, P.
\newblock Know what you don’t know: Unanswerable questions for squad.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pp.\  784--789,
  2018.

\bibitem[Roy et~al.(2020)Roy, Saffar, Vaswani, and Grangier]{roy2020efficient}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{arXiv preprint arXiv:1908.03265}, 2020.

\bibitem[Shrivastava \& Li(2014)Shrivastava and Li]{shrivastava2014asymmetric}
Shrivastava, A. and Li, P.
\newblock Asymmetric lsh (alsh) for sublinear time maximum inner product search
  (mips).
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2321--2329, 2014.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{sukhbaatar2019adaptive}
Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.
\newblock Adaptive attention span in transformers.
\newblock \emph{arXiv preprint arXiv:1905.07799}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wangsmhlb19}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Xu et~al.(2015)Xu, Ba, Kiros, Cho, Courville, Salakhudinov, Zemel, and
  Bengio]{xu2015show}
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,
  and Bengio, Y.
\newblock Show, attend and tell: Neural image caption generation with visual
  attention.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2057, 2015.

\end{thebibliography}
