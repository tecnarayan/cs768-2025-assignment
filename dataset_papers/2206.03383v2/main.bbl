\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems},
  24:\penalty0 2312--2320, 2011.

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  104--114. PMLR, 2020.

\bibitem[Amit et~al.(2020)Amit, Meir, and Ciosek]{amit2020discount}
Amit, R., Meir, R., and Ciosek, K.
\newblock Discount factor as a regularizer in reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  269--278. PMLR, 2020.

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{an2021uncertainty}
An, G., Moon, S., Kim, J.-H., and Song, H.~O.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Argenson \& Dulac-Arnold(2020)Argenson and
  Dulac-Arnold]{argenson2020model}
Argenson, A. and Dulac-Arnold, G.
\newblock Model-based offline planning.
\newblock \emph{arXiv preprint arXiv:2008.05556}, 2020.

\bibitem[Brandfonbrener et~al.(2021)Brandfonbrener, Whitney, Ranganath, and
  Bruna]{brandfonbrener2021offline}
Brandfonbrener, D., Whitney, W.~F., Ranganath, R., and Bruna, J.
\newblock Offline rl without off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:2106.08909}, 2021.

\bibitem[Chen et~al.(2018)Chen, Kochenderfer, and Spaan]{chen2018improving}
Chen, Y.-C., Kochenderfer, M.~J., and Spaan, M.~T.
\newblock Improving offline value-function approximations for pomdps by
  reducing discount factors.
\newblock In \emph{2018 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  3531--3536. IEEE, 2018.

\bibitem[Fedus et~al.(2019)Fedus, Gelada, Bengio, Bellemare, and
  Larochelle]{fedus2019hyperbolic}
Fedus, W., Gelada, C., Bengio, Y., Bellemare, M.~G., and Larochelle, H.
\newblock Hyperbolic discounting and learning over multiple horizons.
\newblock \emph{arXiv preprint arXiv:1902.06865}, 2019.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021td3}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06860}, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2052--2062. PMLR, 2019.

\bibitem[Geer et~al.(2000)Geer, van~de Geer, and Williams]{geer2000empirical}
Geer, S.~A., van~de Geer, S., and Williams, D.
\newblock \emph{Empirical Processes in M-estimation}, volume~6.
\newblock Cambridge university press, 2000.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and
  Gu]{ghasemipour2021emaq}
Ghasemipour, S. K.~S., Schuurmans, D., and Gu, S.~S.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3682--3691. PMLR, 2021.

\bibitem[Hong et~al.(2020)Hong, Wai, Wang, and Yang]{hong2020two}
Hong, M., Wai, H.-T., Wang, Z., and Yang, Z.
\newblock A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic.
\newblock \emph{arXiv preprint arXiv:2007.05170}, 2020.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, Singh, and
  Lewis]{jiang2015dependence}
Jiang, N., Kulesza, A., Singh, S., and Lewis, R.
\newblock The dependence of effective planning horizon on model accuracy.
\newblock In \emph{Proceedings of the 2015 International Conference on
  Autonomous Agents and Multiagent Systems}, pp.\  1181--1189. Citeseer, 2015.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5084--5096. PMLR, 2021.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 11784--11794, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Ma et~al.(2021)Ma, Yang, Hu, Liu, Yang, Zhang, Zhao, and
  Liang]{ma2021offline}
Ma, X., Yang, Y., Hu, H., Liu, Q., Yang, J., Zhang, C., Zhao, Q., and Liang, B.
\newblock Offline reinforcement learning with value-based episodic memory.
\newblock \emph{arXiv preprint arXiv:2110.09796}, 2021.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020accelerating}
Nair, A., Dalal, M., Gupta, A., and Levine, S.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Petrik \& Scherrer(2008)Petrik and Scherrer]{petrik2008biasing}
Petrik, M. and Scherrer, B.
\newblock Biasing approximate dynamic programming with a lower discount factor.
\newblock \emph{Advances in neural information processing systems},
  21:\penalty0 1265--1272, 2008.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and
  Russell]{rashidinejad2021bridging}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \emph{arXiv preprint arXiv:2103.12021}, 2021.

\bibitem[Rathnam et~al.(2021)Rathnam, Murphy, and
  Doshi-Velez]{rathnam2021comparison}
Rathnam, S., Murphy, S.~A., and Doshi-Velez, F.
\newblock Comparison and unification of three regularization methods in batch
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.08134}, 2021.

\bibitem[Romoff et~al.(2019)Romoff, Henderson, Touati, Brunskill, Pineau, and
  Ollivier]{romoff2019separating}
Romoff, J., Henderson, P., Touati, A., Brunskill, E., Pineau, J., and Ollivier,
  Y.
\newblock Separating value functions across time-scales.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5468--5477. PMLR, 2019.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shalev-Shwartz, S., Shammah, S., and Shashua, A.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Sherstan et~al.(2020)Sherstan, Dohare, MacGlashan, G{\"u}nther, and
  Pilarski]{sherstan2020gamma}
Sherstan, C., Dohare, S., MacGlashan, J., G{\"u}nther, J., and Pilarski, P.~M.
\newblock Gamma-nets: Generalizing value estimation over timescale.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  5717--5725, 2020.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, Heess, and Riedmiller]{siegel2020keep}
Siegel, N.~Y., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert,
  M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Singh et~al.(2020)Singh, Yu, Yang, Zhang, Kumar, and
  Levine]{singh2020cog}
Singh, A., Yu, A., Yang, J., Zhang, J., Kumar, A., and Levine, S.
\newblock Cog: Connecting new skills to past experience with offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.14500}, 2020.

\bibitem[Swaminathan \& Joachims(2015)Swaminathan and
  Joachims]{swaminathan2015batch}
Swaminathan, A. and Joachims, T.
\newblock Batch learning from logged bandit feedback through counterfactual
  risk minimization.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1731--1755, 2015.

\bibitem[Uehara \& Sun(2021)Uehara and Sun]{uehara2021pessimistic}
Uehara, M. and Sun, W.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \emph{arXiv preprint arXiv:2107.06226}, 2021.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Wu et~al.(2021)Wu, Zhai, Srivastava, Susskind, Zhang, Salakhutdinov,
  and Goh]{wu2021uncertainty}
Wu, Y., Zhai, S., Srivastava, N., Susskind, J., Zhang, J., Salakhutdinov, R.,
  and Goh, H.
\newblock Uncertainty weighted actor-critic for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2105.08140}, 2021.

\bibitem[Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06926}, 2021.

\bibitem[Xu et~al.(2018)Xu, van Hasselt, and Silver]{xu2018meta}
Xu, Z., van Hasselt, H.~P., and Silver, D.
\newblock Meta-gradient reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 2396--2407, 2018.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004. PMLR, 2019.

\bibitem[Yang et~al.(2021)Yang, Ma, Li, Zheng, Zhang, Huang, Yang, and
  Zhao]{yang2021believe}
Yang, Y., Ma, X., Li, C., Zheng, Z., Zhang, Q., Huang, G., Yang, J., and Zhao,
  Q.
\newblock Believe what you see: Implicit constraint approach for offline
  multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.03400}, 2021.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{yu2021combo}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock \emph{arXiv preprint arXiv:2102.08363}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Laroche, van Seijen, Whiteson, and
  Combes]{zhang2020deeper}
Zhang, S., Laroche, R., van Seijen, H., Whiteson, S., and Combes, R. T.~d.
\newblock A deeper look at discounting mismatch in actor-critic algorithms.
\newblock \emph{arXiv preprint arXiv:2010.01069}, 2020.

\end{thebibliography}
