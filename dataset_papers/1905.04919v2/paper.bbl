\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(1998)]{amari1998natural}
Amari, S.-I.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Baker et~al.(2017)Baker, Gupta, Naik, and Raskar]{baker2016designing}
Baker, B., Gupta, O., Naik, N., and Raskar, R.
\newblock Designing neural network architectures using reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Bender et~al.(2018)Bender, Kindermans, Zoph, Vasudevan, and
  Le]{bender2018understanding}
Bender, G., Kindermans, P.-J., Zoph, B., Vasudevan, V., and Le, Q.
\newblock Understanding and simplifying one-shot architecture search.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  549--558, 2018.

\bibitem[Berger(2013)]{berger2013statistical}
Berger, J.~O.
\newblock \emph{Statistical decision theory and Bayesian analysis}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{botev2017practical}
Botev, A., Ritter, H., and Barber, D.
\newblock Practical gauss-newton optimisation for deep learning.
\newblock \emph{ICML}, 2017.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimisation}.
\newblock Cambridge university press, 2004.

\bibitem[Brock et~al.(2018)Brock, Lim, Ritchie, and Weston]{brock2017smash}
Brock, A., Lim, T., Ritchie, J., and Weston, N.
\newblock {SMASH}: One-shot model architecture search through hypernetworks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Cai et~al.(2018)Cai, Yang, Zhang, Han, and Yu]{cai2018path}
Cai, H., Yang, J., Zhang, W., Han, S., and Yu, Y.
\newblock Path-level network transformation for efficient architecture search.
\newblock In \emph{{ICML}}, volume~80 of \emph{Proceedings of Machine Learning
  Research}, pp.\  677--686. {PMLR}, 2018.

\bibitem[Cai et~al.(2019)Cai, Zhu, and Han]{cai2019iclr}
Cai, H., Zhu, L., and Han, S.
\newblock Proxyless{NAS}: Direct neural architecture search on target task and
  hardware.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://arxiv.org/pdf/1812.00332.pdf}.

\bibitem[Candes et~al.(2008)Candes, Wakin, and Boyd]{candes2008enhancing}
Candes, E.~J., Wakin, M.~B., and Boyd, S.~P.
\newblock Enhancing sparsity by reweighted $\ell_1$ minimization.
\newblock \emph{Journal of Fourier analysis and applications}, 14\penalty0
  (5-6):\penalty0 877--905, 2008.

\bibitem[Chauvin(1989)]{chauvin1989back}
Chauvin, Y.
\newblock A back-propagation algorithm with optimal use of hidden units.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  519--526, 1989.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3123--3131, 2015.

\bibitem[DeVries \& Taylor(2017)DeVries and Taylor]{devries2017improved}
DeVries, T. and Taylor, G.~W.
\newblock Improved regularization of convolutional neural networks with cutout,
  2017.

\bibitem[Dikov et~al.(2019)Dikov, van~der Smagt, and Bayer]{dikov2019bayesian}
Dikov, G., van~der Smagt, P., and Bayer, J.
\newblock Bayesian learning of neural network architectures.
\newblock \emph{AISTATS}, 2019.

\bibitem[Elsken et~al.(2019{\natexlab{a}})Elsken, Metzen, and
  Hutter]{elsken2018efficient}
Elsken, T., Metzen, J.~H., and Hutter, F.
\newblock Efficient multi-objective neural architecture search via lamarckian
  evolution.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Elsken et~al.(2019{\natexlab{b}})Elsken, Metzen, and Hutter]{survey}
Elsken, T., Metzen, J.~H., and Hutter, F.
\newblock Neural architecture search: A survey.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (55):\penalty0 1--21, 2019{\natexlab{b}}.

\bibitem[Gordon et~al.(2018)Gordon, Eban, Nachum, Chen, Wu, Yang, and
  Choi]{gordon2018morphnet}
Gordon, A., Eban, E., Nachum, O., Chen, B., Wu, H., Yang, T.-J., and Choi, E.
\newblock Morphnet: Fast \& simple resource-constrained structure learning of
  deep networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1586--1595, 2018.

\bibitem[Graves(2011)]{graves2011practical}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2348--2356, 2011.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  1379--1387, 2016.

\bibitem[Han et~al.(2017)Han, Kim, and Kim]{han2017deep}
Han, D., Kim, J., and Kim, J.
\newblock Deep pyramidal residual networks.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR), 2017 IEEE
  Conference on}, pp.\  6307--6315. IEEE, 2017.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[{Hassibi} et~al.(1993){Hassibi}, {Stork}, and
  {Wolff}]{hassibi1993optimal}
{Hassibi}, B., {Stork}, D.~G., and {Wolff}, G.~J.
\newblock Optimal brain surgeon and general network pruning.
\newblock In \emph{IEEE International Conference on Neural Networks}, pp.\
  293--299 vol.1, March 1993.
\newblock \doi{10.1109/ICNN.1993.298572}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1869, 2015.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the sixth annual conference on Computational
  learning theory}, pp.\  5--13. ACM, 1993.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications, 2017.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4700--4708, 2017.

\bibitem[Ishikawa(1996)]{ishikawa1996structural}
Ishikawa, M.
\newblock Structural learning with forgetting.
\newblock \emph{Neural networks}, 9\penalty0 (3):\penalty0 509--521, 1996.

\bibitem[Jyl{\"a}nki et~al.(2014)Jyl{\"a}nki, Nummenmaa, and
  Vehtari]{jylanki2014expectation}
Jyl{\"a}nki, P., Nummenmaa, A., and Vehtari, A.
\newblock Expectation propagation for neural networks with sparsity-promoting
  priors.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1849--1901, 2014.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{lecun1990optimal}
LeCun, Y., Denker, J.~S., and Solla, S.~A.
\newblock Optimal brain damage.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  598--605, 1990.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P. H.~S.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock 2019.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Zoph, Neumann, Shlens, Hua, Li,
  Fei-Fei, Yuille, Huang, and Murphy]{liu2018progressive}
Liu, C., Zoph, B., Neumann, M., Shlens, J., Hua, W., Li, L.-J., Fei-Fei, L.,
  Yuille, A., Huang, J., and Murphy, K.
\newblock Progressive neural architecture search.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  19--34, 2018{\natexlab{a}}.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Chen, Schroff, Adam, Hua, Yuille,
  and Fei-Fei]{liu2019auto}
Liu, C., Chen, L.-C., Schroff, F., Adam, H., Hua, W., Yuille, A., and Fei-Fei,
  L.
\newblock Auto-deeplab: Hierarchical neural architecture search for semantic
  image segmentation, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Simonyan, Vinyals, Fernando, and
  Kavukcuoglu]{liu2017hierarchical}
Liu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K.
\newblock Hierarchical representations for efficient architecture search.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=BJQRKzbA-}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Simonyan, and Yang]{liu2018darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock {DARTS}: Differentiable architecture search.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=S1eYHoC5FX}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Louizos et~al.(2017)Louizos, Ullrich, and
  Welling]{louizos2017bayesian}
Louizos, C., Ullrich, K., and Welling, M.
\newblock Bayesian compression for deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3288--3298, 2017.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{louizos2017learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $l_0$ regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=H1Y8hhg0b}.

\bibitem[Ma \& Lu(2017)Ma and Lu]{ma2017equivalence}
Ma, W. and Lu, J.
\newblock An equivalence of fully connected layer and convolutional layer,
  2017.

\bibitem[MacKay(1992{\natexlab{a}})]{mackay1992bayesian}
MacKay, D.~J.
\newblock Bayesian interpolation.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 415--447,
  1992{\natexlab{a}}.

\bibitem[MacKay(1992{\natexlab{b}})]{mackay1992practical}
MacKay, D.~J.
\newblock A practical bayesian framework for backpropagation networks.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 448--472,
  1992{\natexlab{b}}.

\bibitem[MacKay(1996)]{mackay1996bayesian}
MacKay, D.~J.
\newblock Bayesian methods for backpropagation networks.
\newblock In \emph{Models of neural networks III}, pp.\  211--254. Springer,
  1996.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417, 2015.

\bibitem[Miikkulainen et~al.(2019)Miikkulainen, Liang, Meyerson, Rawal, Fink,
  Francon, Raju, Shahrzad, Navruzyan, Duffy, et~al.]{miikkulainen2019evolving}
Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O.,
  Raju, B., Shahrzad, H., Navruzyan, A., Duffy, N., et~al.
\newblock Evolving deep neural networks.
\newblock In \emph{Artificial Intelligence in the Age of Neural Networks and
  Brain Computing}, pp.\  293--312. Elsevier, 2019.

\bibitem[Molchanov et~al.(2017{\natexlab{a}})Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, pp.\  2498--2507. JMLR.org,
  2017{\natexlab{a}}.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3305890.3305939}.

\bibitem[Molchanov et~al.(2017{\natexlab{b}})Molchanov, Tyree, Karras, Aila,
  and Kautz]{molchanov2016pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In \emph{International Conference on Learning Representations},
  2017{\natexlab{b}}.

\bibitem[Neal(1995)]{neal1995bayesian}
Neal, R.~M.
\newblock Bayesian learning for neural networks.
\newblock 1995.

\bibitem[Neklyudov et~al.(2017)Neklyudov, Molchanov, Ashukha, and
  Vetrov]{neklyudov2017structured}
Neklyudov, K., Molchanov, D., Ashukha, A., and Vetrov, D.~P.
\newblock Structured bayesian pruning via log-normal multiplicative noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6775--6784, 2017.

\bibitem[Nocedal \& Wright(2006)Nocedal and Wright]{NoceWrig06}
Nocedal, J. and Wright, S.~J.
\newblock \emph{Numerical Optimization}.
\newblock Springer, 2006.

\bibitem[Pan(2017)]{pan2017bayesian}
Pan, W.
\newblock \emph{Bayesian learning for nonlinear system identification}.
\newblock PhD thesis, Imperial College London, 2017.

\bibitem[Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean]{pham2018efficient}
Pham, H.~Q., Guan, M.~Y., Zoph, B., Le, Q.~V., and Dean, J.
\newblock Efficient neural architecture search via parameter sharing.
\newblock In \emph{ICML}, 2018.

\bibitem[Real et~al.(2017)Real, Moore, Selle, Saxena, Suematsu, Tan, Le, and
  Kurakin]{real2017large}
Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.~L., Tan, J., Le,
  Q.~V., and Kurakin, A.
\newblock Large-scale evolution of image classifiers.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2902--2911. JMLR. org, 2017.

\bibitem[Real et~al.(2019)Real, Aggarwal, Huang, and Le]{real2018regularized}
Real, E., Aggarwal, A., Huang, Y., and Le, Q.~V.
\newblock Regularized evolution for image classifier architecture search.
\newblock In \emph{AAAI}, 2019.

\bibitem[Saxena \& Verbeek(2016)Saxena and Verbeek]{saxena2016convolutional}
Saxena, S. and Verbeek, J.
\newblock Convolutional neural fabrics.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4053--4061, 2016.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1--9, 2015.

\bibitem[Tipping(2001)]{tipping2001sparse}
Tipping, M.~E.
\newblock Sparse bayesian learning and the relevance vector machine.
\newblock \emph{Journal of machine learning research}, 1\penalty0
  (Jun):\penalty0 211--244, 2001.

\bibitem[Ullrich et~al.(2017)Ullrich, Meeds, and Welling]{ullrich2017soft}
Ullrich, K., Meeds, E., and Welling, M.
\newblock Soft weight-sharing for neural network compression.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Weigend et~al.(1991)Weigend, Rumelhart, and
  Huberman]{weigend1991generalization}
Weigend, A.~S., Rumelhart, D.~E., and Huberman, B.~A.
\newblock Generalization by weight-elimination with application to forecasting.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  875--882, 1991.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2074--2082, 2016.

\bibitem[Xie \& Yuille(2017)Xie and Yuille]{xie2017genetic}
Xie, L. and Yuille, A.
\newblock Genetic cnn.
\newblock In \emph{2017 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  1388--1397. IEEE, 2017.

\bibitem[Xie et~al.(2019)Xie, Zheng, Liu, and Lin]{xie2019snas}
Xie, S., Zheng, H., Liu, C., and Lin, L.
\newblock {SNAS}: stochastic neural architecture search.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rylqooRqK7}.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Ren, and
  Urtasun]{zhang2018graph}
Zhang, C., Ren, M., and Urtasun, R.
\newblock Graph hypernetworks for neural architecture search.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=rkgW0oA9FX}.

\bibitem[Zhang et~al.(2018)Zhang, Zhou, Lin, and Sun]{zhang2018shufflenet}
Zhang, X., Zhou, X., Lin, M., and Sun, J.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  6848--6856, 2018.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Huang, and
  Wang]{zhang2018single}
Zhang, X., Huang, Z., and Wang, N.
\newblock Single shot neural architecture search via direct sparse
  optimization, 2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=ryxjH3R5KQ}.

\bibitem[Zhong et~al.(2018)Zhong, Yan, Wu, Shao, and Liu]{zhong2018practical}
Zhong, Z., Yan, J., Wu, W., Shao, J., and Liu, C.-L.
\newblock Practical block-wise neural network architecture generation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2423--2432, 2018.

\bibitem[Zoph \& Le(2017)Zoph and Le]{nas}
Zoph, B. and Le, Q.~V.
\newblock Neural architecture search with reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2017learning}
Zoph, B., Vasudevan, V., Shlens, J., and Le, Q.~V.
\newblock Learning transferable architectures for scalable image recognition.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  8697--8710, 2018.

\end{thebibliography}
