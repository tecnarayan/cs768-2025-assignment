\begin{thebibliography}{10}

\bibitem{arjevani2023lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock {\em Mathematical Programming}, 199(1):165--214, 2023.

\bibitem{arulkumaran2017deep}
Kai Arulkumaran, Marc~Peter Deisenroth, Miles Brundage, and Anil~Anthony
  Bharath.
\newblock Deep reinforcement learning: A brief survey.
\newblock {\em IEEE Signal Processing Magazine}, 34(6):26--38, 2017.

\bibitem{Asadi2017alternative}
Kavosh Asadi and Michael~L. Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, page 243–252, Sydney, NSW, Australia, 2017.
  JMLR.org.

\bibitem{Balaguer2022Good}
Jan Balaguer, Raphael Koster, Christopher Summerfield, and Andrea Tacchetti.
\newblock The good shepherd: An oracle agent for mechanism design.
\newblock In {\em ICLR 2022 Workshop on Gamification and Multiagent Solutions},
  2022.

\bibitem{bard2013practical}
Jonathan~F Bard.
\newblock {\em Practical bilevel optimization: algorithms and applications},
  volume~30.
\newblock Springer Science \& Business Media, 2013.

\bibitem{Baumann2020Adaptive}
Tobias Baumann, Thore Graepel, and John Shawe-Taylor.
\newblock Adaptive mechanism design: Learning to promote cooperation.
\newblock In {\em 2020 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--7. IEEE, 2020.

\bibitem{beck2023survey}
Jacob Beck, Risto Vuorio, Evan~Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea
  Finn, and Shimon Whiteson.
\newblock A survey of meta-reinforcement learning.
\newblock {\em arXiv preprint arXiv:2301.08028}, 2023.

\bibitem{ben2023principal}
Omer Ben-Porat, Yishay Mansour, Michal Moshkovitz, and Boaz Taitler.
\newblock Principal-agent reward shaping in mdps.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~38, pages 9502--9510, 2024.

\bibitem{Blondel2024Elements}
Mathieu Blondel and Vincent Roulet.
\newblock The elements of differentiable programming, 2024.

\bibitem{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{Brown2024Markov}
Seth Brown, Saumya Sinha, and Andrew~J. Schaefer.
\newblock Markov decision process design: A framework for integrating strategic
  and operational decisions.
\newblock {\em Operations Research Letters}, 54:107090, May 2024.

\bibitem{Cen2022Fast}
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi.
\newblock Fast global convergence of natural policy gradient methods with
  entropy regularization.
\newblock {\em Operations Research}, 70(4):2563--2578, July 2022.

\bibitem{chakraborty2023parl}
Souradip Chakraborty, Amrit Bedi, Alec Koppel, Huazheng Wang, Dinesh Manocha,
  Mengdi Wang, and Furong Huang.
\newblock {PARL}: A unified framework for policy alignment in reinforcement
  learning from human feedback.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{charpentier2021reinforcement}
Arthur Charpentier, Romuald Elie, and Carl Remlinger.
\newblock Reinforcement learning in economics and finance.
\newblock {\em Computational Economics}, pages 1--38, 2021.

\bibitem{chen2022adaptive}
Siyu Chen, Donglin Yang, Jiayang Li, Senmiao Wang, Zhuoran Yang, and Zhaoran
  Wang.
\newblock Adaptive model design for markov decision process.
\newblock In {\em International Conference on Machine Learning}, pages
  3679--3700. PMLR, 2022.

\bibitem{chen2021closing}
Tianyi Chen, Yuejiao Sun, and Wotao Yin.
\newblock Closing the gap: Tighter analysis of alternating stochastic gradient
  methods for bilevel problems.
\newblock {\em Advances in Neural Information Processing Systems},
  34:25294--25307, 2021.

\bibitem{Christiano2017Deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{Curry2024Automated}
Michael Curry, Vinzenz Thoma, Darshan Chakrabarti, Stephen McAleer, Christian
  Kroer, Tuomas Sandholm, Niao He, and Sven Seuken.
\newblock Automated design of affine maximizer mechanisms in dynamic settings.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  38(9):9626--9635, March 2024.

\bibitem{Curry2023Learning}
Michael Curry, Alexander Trott, Soham Phade, Yu~Bai, and Stephan Zheng.
\newblock Learning solutions in large economic networks using deep multi-agent
  reinforcement learning.
\newblock In {\em Proceedings of the 2023 International Conference on
  Autonomous Agents and Multiagent Systems}, AAMAS '23, page 2760–2762,
  Richland, SC, 2023. International Foundation for Autonomous Agents and
  Multiagent Systems.

\bibitem{Dai2018SBEED}
Bo~Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and
  Le~Song.
\newblock {SBEED}: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In Jennifer Dy and Andreas Krause, editors, {\em Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of {\em
  Proceedings of Machine Learning Research}, pages 1125--1134. PMLR, 10--15 Jul
  2018.

\bibitem{dempe2002foundations}
Stephan Dempe.
\newblock {\em Foundations of bilevel programming}.
\newblock Springer Science \& Business Media, 2002.

\bibitem{Dennis2020Emergent}
Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart
  Russell, Andrew Critch, and Sergey Levine.
\newblock Emergent complexity and zero-shot transfer via unsupervised
  environment design.
\newblock In {\em Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.

\bibitem{Diaz2022Generalization}
Manfred Diaz, Charlie Gauthier, Glen Berseth, and Liam Paull.
\newblock Generalization games for reinforcement learning.
\newblock In {\em ICLR Workshop on Agent Learning in Open-Endedness}, 2022.

\bibitem{Fiez2020Implicit}
Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff.
\newblock Implicit learning dynamics in stackelberg games: Equilibria
  characterization, convergence analysis, and empirical study.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 3133--3144. PMLR, 13--18 Jul
  2020.

\bibitem{Foerster2018Learning}
Jakob Foerster, Richard~Y. Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter
  Abbeel, and Igor Mordatch.
\newblock Learning with opponent-learning awareness.
\newblock In {\em Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, AAMAS '18, page 122–130,
  Richland, SC, 2018. International Foundation for Autonomous Agents and
  Multiagent Systems.

\bibitem{geist2019theory}
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin.
\newblock A theory of regularized markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  2160--2169. PMLR, 2019.

\bibitem{Gerstgrasser2023oracles}
Matthias Gerstgrasser and David~C. Parkes.
\newblock Oracles \& followers: Stackelberg equilibria in deep multi-agent
  reinforcement learning.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of {\em Proceedings
  of Machine Learning Research}, pages 11213--11236. PMLR, 23--29 Jul 2023.

\bibitem{ghadimi2018approximation}
Saeed Ghadimi and Mengdi Wang.
\newblock Approximation methods for bilevel programming.
\newblock {\em arXiv preprint arXiv:1802.02246}, 2018.

\bibitem{Giles2015Multilevel}
Michael~B. Giles.
\newblock Multilevel monte carlo methods.
\newblock {\em Acta Numerica}, 24:259–328, 2015.

\bibitem{Haarnoja2017Reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, page 1352–1361, Sydney, NSW, Australia,
  2017. JMLR.org.

\bibitem{HadfieldMenell2017Inversea}
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca~D.
  Dragan.
\newblock Inverse reward design.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, page 6768–6777, Red Hook, NY,
  USA, 2017. Curran Associates Inc.

\bibitem{hallak2015contextual}
Assaf Hallak, Dotan Di~Castro, and Shie Mannor.
\newblock Contextual markov decision processes.
\newblock {\em arXiv preprint arXiv:1502.02259}, 2015.

\bibitem{hambly2023recent}
Ben Hambly, Renyuan Xu, and Huining Yang.
\newblock Recent advances in reinforcement learning in finance.
\newblock {\em Mathematical Finance}, 33(3):437--503, 2023.

\bibitem{hill2021solving}
Edward Hill, Marco Bardoscia, and Arthur Turrell.
\newblock Solving heterogeneous general equilibrium economic models with deep
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2103.16977}, 2021.

\bibitem{hu2020sample}
Yifan Hu, Xin Chen, and Niao He.
\newblock Sample complexity of sample average approximation for conditional
  stochastic optimization.
\newblock {\em SIAM Journal on Optimization}, 30(3):2103--2133, 2020.

\bibitem{Hu2021Bias}
Yifan Hu, Xin Chen, and Niao He.
\newblock On the bias-variance-cost tradeoff of stochastic optimization.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, pages 22119--22131, 2021.

\bibitem{hu2024contextual}
Yifan Hu, Jie Wang, Yao Xie, Andreas Krause, and Daniel Kuhn.
\newblock Contextual stochastic bilevel optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{hu2020biased}
Yifan Hu, Siqi Zhang, Xin Chen, and Niao He.
\newblock Biased stochastic first-order methods for conditional stochastic
  optimization and applications in meta learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:2759--2770, 2020.

\bibitem{hu2020learning}
Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao,
  Feng Wu, and Changjie Fan.
\newblock Learning to utilize shaping rewards: A new approach of reward
  shaping.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 15931--15941. Curran Associates, Inc., 2020.

\bibitem{Huang2024Learning}
Jiawei Huang, Vinzenz Thoma, Zebang Shen, Heinrich~H. Nax, and Niao He.
\newblock Learning to steer markovian agents under model uncertainty, 2024.

\bibitem{Ivanov2024Principal}
Dima Ivanov, Paul Dütting, Inbal Talgam-Cohen, Tonghan Wang, and David~C.
  Parkes.
\newblock Principal-agent reinforcement learning, 2024.

\bibitem{Kakade2001natural}
Sham Kakade.
\newblock A natural policy gradient.
\newblock In {\em Proceedings of the 14th International Conference on Neural
  Information Processing Systems: Natural and Synthetic}, NIPS'01, page
  1531–1538, Cambridge, MA, USA, 2001. MIT Press.

\bibitem{khanduri2021near}
Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and
  Zhuoran Yang.
\newblock A near-optimal algorithm for stochastic bilevel optimization via
  double-momentum.
\newblock {\em Advances in neural information processing systems},
  34:30271--30283, 2021.

\bibitem{kwon2024complexity}
Jeongyeol Kwon, Dohyun Kwon, and Hanbaek Lyu.
\newblock On the complexity of first-order methods in stochastic bilevel
  optimization.
\newblock {\em arXiv preprint arXiv:2402.07101}, 2024.

\bibitem{kwon2023fully}
Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert~D Nowak.
\newblock A fully first-order method for stochastic bilevel optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  18083--18113. PMLR, 2023.

\bibitem{letchford2013optimal}
Joshua Letchford and Yevgeniy Vorobeychik.
\newblock Optimal interdiction of attack plans.
\newblock In {\em AAMAS}, pages 199--206. Citeseer, 2013.

\bibitem{Liu2022Inducing}
Boyi Liu, Jiayang Li, Zhuoran Yang, Hoi-To Wai, Mingyi Hong, Yu~Nie, and
  Zhaoran Wang.
\newblock Inducing equilibria via incentives: Simultaneous design-and-play
  ensures global convergence.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, {\em Advances in Neural Information Processing Systems}, volume~35,
  pages 29001--29013. Curran Associates, Inc., 2022.

\bibitem{Liu2021Value}
Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang.
\newblock A value-function-based interior-point method for non-convex bi-level
  optimization.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 6882--6892. PMLR, 18--24 Jul 2021.

\bibitem{Mei2020Global}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 6820--6829. PMLR, 13--18 Jul
  2020.

\bibitem{metelli2018configurable}
Alberto~Maria Metelli, Mirco Mutti, and Marcello Restelli.
\newblock Configurable markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  3491--3500. PMLR, 2018.

\bibitem{Nachum2017Bridging}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, page 2772–2782, Red Hook, NY,
  USA, 2017. Curran Associates Inc.

\bibitem{perera2021applications}
ATD Perera and Parameswaran Kamalaruban.
\newblock Applications of reinforcement learning in energy systems.
\newblock {\em Renewable and Sustainable Energy Reviews}, 137:110618, 2021.

\bibitem{puterman2014markov}
Martin~L Puterman.
\newblock {\em Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem{Qu2020Finite}
Guannan Qu and Adam Wierman.
\newblock Finite-time analysis of asynchronous stochastic approximation and
  $q$-learning.
\newblock In Jacob Abernethy and Shivani Agarwal, editors, {\em Proceedings of
  Thirty Third Conference on Learning Theory}, volume 125 of {\em Proceedings
  of Machine Learning Research}, pages 3185--3205. PMLR, 09--12 Jul 2020.

\bibitem{ramponi2021learning}
Giorgia Ramponi, Alberto~Maria Metelli, Alessandro Concetti, and Marcello
  Restelli.
\newblock Learning in non-cooperative configurable markov decision processes.
\newblock {\em Advances in Neural Information Processing Systems},
  34:22808--22821, 2021.

\bibitem{Rockafellar1998Variational}
R.~Tyrrell Rockafellar and Roger J.~B. Wets.
\newblock {\em Variational Analysis}.
\newblock Springer Berlin Heidelberg, 1998.

\bibitem{roth2016watch}
Aaron Roth, Jonathan Ullman, and Zhiwei~Steven Wu.
\newblock Watch and learn: Optimizing from revealed preferences feedback.
\newblock In {\em Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pages 949--962, 2016.

\bibitem{shen2024principled}
Han Shen, Zhuoran Yang, and Tianyi Chen.
\newblock Principled penalty-based methods for bilevel reinforcement learning
  and rlhf.
\newblock {\em arXiv preprint arXiv:2402.06886}, 2024.

\bibitem{shi2024sample}
Laixi Shi, Eric Mazumdar, Yuejie Chi, and Adam Wierman.
\newblock Sample-efficient robust multi-agent reinforcement learning in the
  face of environmental uncertainty.
\newblock {\em arXiv preprint arXiv:2404.18909}, 2024.

\bibitem{sinha2018stackelberg}
Arunesh Sinha, Fei Fang, Bo~An, Christopher Kiekintveld, and Milind Tambe.
\newblock Stackelberg security games: looking beyond a decade of success.
\newblock In {\em Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, pages 5494--5501, 2018.

\bibitem{Stackelberg1934Marktform}
Heinrich~von Stackelberg.
\newblock {\em Marktform und Gleichgewicht}.
\newblock Klassiker der Nationalökonomie. Verlag Wirtschaft und Finanzen,
  Düsseldorf, 1934.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{turchetta2020safe}
Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh
  Agarwal.
\newblock Safe reinforcement learning via curriculum induction.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12151--12162, 2020.

\bibitem{Vieillard2020Leverage}
Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, R\'{e}mi
  Munos, and Matthieu Geist.
\newblock Leverage the average: an analysis of kl regularization in
  reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS '20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.

\bibitem{Wang2023Differentiable}
Jing Wang, Meichen Song, Feng Gao, Boyi Liu, Zhaoran Wang, and Yi~Wu.
\newblock Differentiable arbitrating in zero-sum markov games.
\newblock In {\em Proceedings of the 2023 International Conference on
  Autonomous Agents and Multiagent Systems}, AAMAS '23, page 1034–1043,
  Richland, SC, 2023. International Foundation for Autonomous Agents and
  Multiagent Systems.

\bibitem{wang2022coordinating}
Kai Wang, Lily Xu, Andrew Perrault, Michael~K. Reiter, and Milind Tambe.
\newblock Coordinating followers to reach better equilibria: End-to-end
  gradient descent for stackelberg games.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  36(5):5219--5227, Jun. 2022.

\bibitem{wang2022deep}
Xu~Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai,
  and Qiguang Miao.
\newblock Deep reinforcement learning: A survey.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem{Williams1992Simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8:229--256, 1992.

\bibitem{Wu2024Contractual}
Jibang Wu, Siyu Chen, Mengdi Wang, Huazheng Wang, and Haifeng Xu.
\newblock Contractual reinforcement learning: Pulling arms with invisible
  hands, 2024.

\bibitem{Wu2024Robust}
Shuo Wu, Haoxiang Ma, Jie Fu, and Shuo Han.
\newblock Robust reward design for markov decision processes, 2024.

\bibitem{Xia2023Risk‐sensitive}
Li~Xia, Luyao Zhang, and Peter~W. Glynn.
\newblock Risk‐sensitive markov decision processes with long‐run cvar
  criterion.
\newblock {\em Production and Operations Management}, 32(12):4049--4067,
  December 2023.

\bibitem{xie2021diffusion}
Zeke Xie, Issei Sato, and Masashi Sugiyama.
\newblock A diffusion theory for deep learning dynamics: Stochastic gradient
  descent exponentially favors flat minima.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Yang2022Game}
Chang Yang, Yuiyu Wang, Xinrun Wang, and Zhen Wang.
\newblock A game-theoretic perspective of generalization in reinforcement
  learning.
\newblock In {\em Deep Reinforcement Learning Workshop NeurIPS 2022}, 2022.

\bibitem{Yang2020Learninga}
Jiachen Yang, Ang Li, Mehrdad Farajtabar, Peter Sunehag, Edward Hughes, and
  Hongyuan Zha.
\newblock Learning to incentivize other learning agents.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 15208--15219. Curran Associates, Inc., 2020.

\bibitem{yang2024bilevel}
Yan Yang, Bin Gao, and Ya~xiang Yuan.
\newblock Bilevel reinforcement learning via the development of hyper-gradient
  without lower-level convexity, 2024.

\bibitem{yu2021reinforcement}
Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin.
\newblock Reinforcement learning in healthcare: A survey.
\newblock {\em ACM Computing Surveys (CSUR)}, 55(1):1--36, 2021.

\bibitem{yu2022environment}
Guanghui Yu and Chien-Ju Ho.
\newblock Environment design for biased decision makers.
\newblock In Lud~De Raedt, editor, {\em Proceedings of the Thirty-First
  International Joint Conference on Artificial Intelligence, {IJCAI-22}}, pages
  592--598. International Joint Conferences on Artificial Intelligence
  Organization, 7 2022.
\newblock Main Track.

\bibitem{zhang2018learning}
Haifeng Zhang, Jun Wang, Zhiming Zhou, Weinan Zhang, Ying Wen, Yong Yu, and
  Wenxin Li.
\newblock Learning to design games: strategic environments in reinforcement
  learning.
\newblock In {\em Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, IJCAI'18, page 3068–3074. AAAI Press, 2018.

\bibitem{zhang2008valuebased}
Haoqi Zhang and David Parkes.
\newblock Value-based policy teaching with active indirect elicitation.
\newblock In {\em Proceedings of the 23rd National Conference on Artificial
  Intelligence - Volume 1}, AAAI'08, page 208–214. AAAI Press, 2008.

\bibitem{Zhang2020Global}
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Başar.
\newblock Global convergence of policy gradient methods to (almost) locally
  optimal policies.
\newblock {\em SIAM Journal on Control and Optimization}, 58(6):3586--3612,
  January 2020.

\bibitem{zhang2021multi}
Kaiqing Zhang, Zhuoran Yang, and Tamer Ba{\c{s}}ar.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock {\em Handbook of reinforcement learning and control}, pages 321--384,
  2021.

\bibitem{zheng2022ai}
Stephan Zheng, Alexander Trott, Sunil Srinivasa, David~C. Parkes, and Richard
  Socher.
\newblock The ai economist: Taxation policy design via two-level deep
  multiagent reinforcement learning.
\newblock {\em Science Advances}, 8(18):eabk2607, 2022.

\end{thebibliography}
