\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man{\'e}, D.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et~al.
\newblock Constitutional {AI}: Harmlessness from {AI} feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Bansal et~al.(2021)Bansal, Wu, Zhou, Fok, Nushi, Kamar, Ribeiro, and Weld]{bansal2021does}
Bansal, G., Wu, T., Zhou, J., Fok, R., Nushi, B., Kamar, E., Ribeiro, M.~T., and Weld, D.
\newblock Does the whole exceed its parts? the effect of ai explanations on complementary team performance.
\newblock In \emph{Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems}, pp.\  1--16, 2021.

\bibitem[Barnes(2020)]{barnes2020debate}
Barnes, B.
\newblock Debate update: Obfuscated arguments problem.
\newblock \emph{AI Alignment Forum}, 2020.

\bibitem[Bowman et~al.(2022)Bowman, Hyun, Perez, Chen, Pettit, Heiner, Lukosuite, Askell, Jones, Chen, et~al.]{bowman2022measuring}
Bowman, S.~R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukosuite, K., Askell, A., Jones, A., Chen, A., et~al.
\newblock Measuring progress on scalable oversight for large language models.
\newblock \emph{arXiv preprint arXiv:2211.03540}, 2022.

\bibitem[Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike, Sutskever, and Wu]{burns2023weaktostrong}
Burns, C., Izmailov, P., Kirchner, J.~H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., Sutskever, I., and Wu, J.
\newblock Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.
\newblock \emph{arXiv preprint arXiv:2312.09390}, 2023.

\bibitem[Christiano et~al.(2018)Christiano, Shlegeris, and Amodei]{christiano2018supervising}
Christiano, P., Shlegeris, B., and Amodei, D.
\newblock Supervising strong learners by amplifying weak experts.
\newblock \emph{arXiv preprint arXiv:1810.08575}, 2018.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chromik et~al.(2021)Chromik, Eiband, Buchner, Kr{\"u}ger, and Butz]{chromik2021think}
Chromik, M., Eiband, M., Buchner, F., Kr{\"u}ger, A., and Butz, A.
\newblock I think i get your point, ai! the illusion of explanatory depth in explainable ai.
\newblock In \emph{26th International Conference on Intelligent User Interfaces}, pp.\  307--317, 2021.

\bibitem[Cotra(2021)]{Cotra2021Aligning}
Cotra, A.
\newblock The case for aligning narrowly superhuman models.
\newblock \emph{AI Alignment Forum}, 2021.

\bibitem[Du et~al.(2023)Du, Li, Torralba, Tenenbaum, and Mordatch]{du2023improving}
Du, Y., Li, S., Torralba, A., Tenenbaum, J.~B., and Mordatch, I.
\newblock Improving factuality and reasoning in language models through multiagent debate.
\newblock \emph{arXiv preprint arXiv:2305.14325}, 2023.

\bibitem[Elo(1978)]{elo1978rating}
Elo, A.
\newblock \emph{The Rating of Chessplayers: Past and Present}.
\newblock Ishi Press International, 1978.
\newblock ISBN 9780923891275.
\newblock URL \url{https://books.google.com/books?id=syjcPQAACAAJ}.

\bibitem[{Gemini Team} et~al.(2023){Gemini Team}, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, et~al.]{geminiteam2023gemini}
{Gemini Team}, Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., Millican, K., et~al.
\newblock Gemini: A family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Goswami et~al.(2023)Goswami, Sanil, Choudhry, Srinivasan, Udompanyawit, and Dubrawski]{goswami2024aqua}
Goswami, M., Sanil, V., Choudhry, A., Srinivasan, A., Udompanyawit, C., and Dubrawski, A.
\newblock Aqua: A benchmarking tool for label quality assessment.
\newblock \emph{arXiv preprint arXiv:2306.09467}, 2023.

\bibitem[Greenblatt et~al.(2023)Greenblatt, Shlegeris, Sachan, and Roger]{greenblatt2023ai}
Greenblatt, R., Shlegeris, B., Sachan, K., and Roger, F.
\newblock Ai control: Improving safety despite intentional subversion.
\newblock \emph{arXiv preprint arXiv:2312.06942}, 2023.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycksmeasuring}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Herbrich et~al.(2006)Herbrich, Minka, and Graepel]{herbrich2006trueskill}
Herbrich, R., Minka, T., and Graepel, T.
\newblock Trueskillâ„¢: a bayesian skill rating system.
\newblock \emph{Advances in neural information processing systems}, 19, 2006.

\bibitem[Hubinger et~al.(2024)Hubinger, Denison, Mu, Lambert, Tong, MacDiarmid, Lanham, Ziegler, Maxwell, Cheng, et~al.]{hubinger2024sleeper}
Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler, D.~M., Maxwell, T., Cheng, N., et~al.
\newblock Sleeper agents: Training deceptive llms that persist through safety training.
\newblock \emph{arXiv preprint arXiv:2401.05566}, 2024.

\bibitem[Irving et~al.(2018)Irving, Christiano, and Amodei]{irving2018ai}
Irving, G., Christiano, P., and Amodei, D.
\newblock {AI} safety via debate.
\newblock \emph{arXiv preprint arXiv:1805.00899}, 2018.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Jiang, A.~Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Hanna, E.~B., Bressand, F., et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Lin et~al.(2022)Lin, Hilton, and Evans]{lin2022truthfulqa}
Lin, S., Hilton, J., and Evans, O.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3214--3252, 2022.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe, Alon, Dziri, Prabhumoye, Yang, Gupta, Majumder, Hermann, Welleck, Yazdanbakhsh, and Clark]{madaan2023selfrefine}
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B.~P., Hermann, K., Welleck, S., Yazdanbakhsh, A., and Clark, P.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023.

\bibitem[Menick et~al.(2022)Menick, Trebacz, Mikulik, Aslanides, Song, Chadwick, Glaese, Young, Campbell-Gillingham, Irving, et~al.]{menick2022teaching}
Menick, J., Trebacz, M., Mikulik, V., Aslanides, J., Song, F., Chadwick, M., Glaese, M., Young, S., Campbell-Gillingham, L., Irving, G., et~al.
\newblock Teaching language models to support answers with verified quotes.
\newblock \emph{arXiv preprint arXiv:2203.11147}, 2022.

\bibitem[Michael et~al.(2023)Michael, Mahdi, Rein, Petty, Dirani, Padmakumar, and Bowman]{michael2023debate}
Michael, J., Mahdi, S., Rein, D., Petty, J., Dirani, J., Padmakumar, V., and Bowman, S.~R.
\newblock Debate helps supervise unreliable experts.
\newblock \emph{arXiv preprint arXiv:2311.08702}, 2023.

\bibitem[Nikola(2023)]{nikola2023jailbreaking}
Nikola.
\newblock Jailbreaking gpt-4's code interpreter.
\newblock \emph{LessWrong}, July 2023.
\newblock URL \url{https://www.lesswrong.com/posts/KSroBnxCHodGmPPJ8/jailbreaking-gpt-4-s-code-interpreter}.

\bibitem[Nye et~al.(2022)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2022show}
Nye, M., Andreassen, A.~J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et~al.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock In \emph{Deep Learning for Code Workshop}, 2022.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pang et~al.(2022)Pang, Parrish, Joshi, Nangia, Phang, Chen, Padmakumar, Ma, Thompson, He, et~al.]{pang2022quality}
Pang, R.~Y., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V., Ma, J., Thompson, J., He, H., et~al.
\newblock {QuALITY}: Question answering with long input texts, yes!
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5336--5358, 2022.

\bibitem[Parrish et~al.(2022{\natexlab{a}})Parrish, Trivedi, Nangia, Phang, Padmakumar, Saimbhi, and Bowman]{parrishtwo}
Parrish, A., Trivedi, H., Nangia, N., Phang, J., Padmakumar, V., Saimbhi, A.~S., and Bowman, S.~R.
\newblock Two-turn debate does not help humans answer hard reading comprehension questions.
\newblock In \emph{NeurIPS ML Safety Workshop}, 2022{\natexlab{a}}.

\bibitem[Parrish et~al.(2022{\natexlab{b}})Parrish, Trivedi, Perez, Chen, Nangia, Phang, and Bowman]{parrish2022single}
Parrish, A., Trivedi, H., Perez, E., Chen, A., Nangia, N., Phang, J., and Bowman, S.
\newblock Single-turn debate does not help humans answer hard reading-comprehension questions.
\newblock In \emph{Proceedings of the First Workshop on Learning with Natural Language Supervision}, pp.\  17--28, 2022{\natexlab{b}}.

\bibitem[Perez et~al.(2019)Perez, Karamcheti, Fergus, Weston, Kiela, and Cho]{perez2019finding}
Perez, E., Karamcheti, S., Fergus, R., Weston, J., Kiela, D., and Cho, K.
\newblock Finding generalizable evidence by learning to convince {Q\&A} models.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  2402--2411, 2019.

\bibitem[Perez et~al.(2021)Perez, Kiela, and Cho]{perez2021true}
Perez, E., Kiela, D., and Cho, K.
\newblock True few-shot learning with language models.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 11054--11070, 2021.

\bibitem[Radhakrishnan(2023)]{radhakrishnan2023anthropic}
Radhakrishnan, A.
\newblock Anthropic fall 2023 debate progress update.
\newblock \emph{AI Alignment Forum}, 2023.

\bibitem[Radhakrishnan et~al.(2023)Radhakrishnan, Nguyen, Chen, Chen, Denison, Hernandez, Durmus, Hubinger, Kernion, Luko{\v{s}}i{\=u}t{\.e}, et~al.]{radhakrishnan2023question}
Radhakrishnan, A., Nguyen, K., Chen, A., Chen, C., Denison, C., Hernandez, D., Durmus, E., Hubinger, E., Kernion, J., Luko{\v{s}}i{\=u}t{\.e}, K., et~al.
\newblock Question decomposition improves the faithfulness of model-generated reasoning.
\newblock \emph{arXiv preprint arXiv:2307.11768}, 2023.

\bibitem[Rein et~al.(2023)Rein, Hou, Stickland, Petty, Pang, Dirani, Michael, and Bowman]{rein2023gpqa}
Rein, D., Hou, B.~L., Stickland, A.~C., Petty, J., Pang, R.~Y., Dirani, J., Michael, J., and Bowman, S.~R.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark.
\newblock \emph{arXiv preprint arXiv:2311.12022}, 2023.

\bibitem[Saunders et~al.(2022)Saunders, Yeh, Wu, Bills, Ouyang, Ward, and Leike]{saunders2022selfcritiquing}
Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J.
\newblock Self-critiquing models for assisting human evaluators.
\newblock \emph{arXiv preprint arXiv:2206.05802}, 2022.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{yao2023tree}
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.~L., Cao, Y., and Narasimhan, K.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock \emph{arXiv preprint arXiv:2305.10601}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et~al.
\newblock Judging {LLM-as-a-judge} with {MT}-bench and chatbot arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}, 2023.

\end{thebibliography}
