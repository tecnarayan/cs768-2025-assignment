\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  22--31. JMLR. org, 2017.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A.~C., and Bellemare, M.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Alegre(2019)]{alegre_sumorl_2019}
Alegre, L.~N.
\newblock {SUMO-RL}.
\newblock \url{https://github.com/LucasAlegre/sumo-rl}, 2019.

\bibitem[Arumugam et~al.(2021)Arumugam, Henderson, and
  Bacon]{arumugam_creditinfo_2021}
Arumugam, D., Henderson, P., and Bacon, P.
\newblock An information-theoretic perspective on credit assignment in
  reinforcement learning.
\newblock \emph{CoRR}, abs/2103.06224, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.06224}.

\bibitem[Ault \& Sharon(2021)Ault and Sharon]{ault_resco_2021}
Ault, J. and Sharon, G.
\newblock Reinforcement learning benchmarks for traffic signal control.
\newblock In \emph{Proceedings of the Thirty-fifth Conference on Neural
  Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track},
  December 2021.

\bibitem[Barnard \& Wessels(1992)Barnard and Wessels]{barnard_nnextra_1992}
Barnard, E. and Wessels, L.
\newblock Extrapolation and interpolation in neural network classifiers.
\newblock \emph{IEEE Control Systems Magazine}, 12\penalty0 (5):\penalty0
  50--53, 1992.
\newblock \doi{10.1109/37.158898}.

\bibitem[Behrisch et~al.(2011)Behrisch, Bieker-Walz, Erdmann, and
  Krajzewicz]{behrisch_sumo_2011}
Behrisch, M., Bieker-Walz, L., Erdmann, J., and Krajzewicz, D.
\newblock Sumo – simulation of urban mobility: An overview.
\newblock volume 2011, 10 2011.
\newblock ISBN 978-1-61208-169-4.

\bibitem[Berkenkamp et~al.(2017)Berkenkamp, Turchetta, Schoellig, and
  Krause]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  908--918, 2017.

\bibitem[Bertsekas(2015)]{bertsekas2015dynamic_vii}
Bertsekas, D.~P.
\newblock Dynamic programming and optimal control, 4th edition, volume {II}.
\newblock \emph{Athena Scientific}, 2015.

\bibitem[Buyukkoc et~al.(1985)Buyukkoc, Varaiya, and
  Walrand]{Buyukkoc1985TheCR}
Buyukkoc, C., Varaiya, P.~P., and Walrand, J.~C.
\newblock The c$\mu$ rule revisited.
\newblock \emph{Advances in Applied Probability}, 17:\penalty0 237 -- 238,
  1985.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:125747190}.

\bibitem[Chang et~al.(2019)Chang, Roohi, and Gao]{NEURIPS2019_2647c1db}
Chang, Y.-C., Roohi, N., and Gao, S.
\newblock Neural lyapunov control.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/2647c1dba23bc0e0f9cdf75339e120d2-Paper.pdf}.

\bibitem[Chen et~al.(2022)Chen, Sharma, Levine, and Finn]{chen_you_2022}
Chen, A.~S., Sharma, A., Levine, S., and Finn, C.
\newblock You {Only} {Live} {Once}: {Single}-{Life} {Reinforcement} {Learning},
  October 2022.
\newblock URL \url{http://arxiv.org/abs/2210.08863}.
\newblock arXiv:2210.08863 [cs].

\bibitem[Chollet(2017)]{challot_deep_2017}
Chollet, F.
\newblock \emph{Deep Learning with Python}.
\newblock Manning Publications Co., USA, 1st edition, 2017.
\newblock ISBN 1617294438.

\bibitem[Chow et~al.(2018)Chow, Nachum, Duenez-Guzman, and
  Ghavamzadeh]{chow2018lyapunov}
Chow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M.
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8092--8101, 2018.

\bibitem[Dai et~al.(2021)Dai, Landry, Yang, Pavone, and
  Tedrake]{dai2021lyapunovstable}
Dai, H., Landry, B., Yang, L., Pavone, M., and Tedrake, R.
\newblock Lyapunov-stable neural-network control, 2021.

\bibitem[Dai \& Gluzman(2022)Dai and Gluzman]{dai_ppo_2022}
Dai, J.~G. and Gluzman, M.
\newblock Queueing network controls via deep reinforcement learning.
\newblock \emph{Stochastic Systems}, 12\penalty0 (1):\penalty0 30--67, 2022.
\newblock \doi{10.1287/stsy.2021.0081}.
\newblock URL \url{https://doi.org/10.1287/stsy.2021.0081}.

\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecerik, Hester, Paduraru, and
  Tassa]{dalal2018safe}
Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y.
\newblock Safe exploration in continuous action spaces.
\newblock \emph{arXiv preprint arXiv:1801.08757}, 2018.

\bibitem[Dean et~al.(2019)Dean, Tu, Matni, and Recht]{dean2019safely}
Dean, S., Tu, S., Matni, N., and Recht, B.
\newblock Safely learning to control the constrained linear quadratic
  regulator.
\newblock In \emph{2019 American Control Conference (ACC)}, pp.\  5582--5588.
  IEEE, 2019.

\bibitem[Dohare et~al.(2023)Dohare, Lan, and Mahmood]{dohare2023overcoming}
Dohare, S., Lan, Q., and Mahmood, A.~R.
\newblock Overcoming policy collapse in deep reinforcement learning.
\newblock In \emph{Sixteenth European Workshop on Reinforcement Learning},
  2023.
\newblock URL \url{https://openreview.net/forum?id=m9Jfdz4ymO}.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gu, Ibarz, and
  Levine]{eysenbach_leave_2018}
Eysenbach, B., Gu, S., Ibarz, J., and Levine, S.
\newblock Leave no {Trace}: {Learning} to {Reset} for {Safe} and {Autonomous}
  {Reinforcement} {Learning}.
\newblock February 2018.
\newblock URL \url{https://openreview.net/forum?id=S1vuO-bCW}.

\bibitem[Fayolle et~al.(1993)Fayolle, Malyshev, Menshikov, and
  Sidorenko]{fayolle1993lyplinear}
Fayolle, G., Malyshev, V.~A., Menshikov, M.~V., and Sidorenko, A.~F.
\newblock Lyapunov functions for jackson networks.
\newblock \emph{Math. Oper. Res.}, 18\penalty0 (4):\penalty0 916–927, nov
  1993.
\newblock ISSN 0364-765X.

\bibitem[Ganti et~al.(2007)Ganti, Modiano, and
  Tsitsiklis]{ganti_qopen-prob_2007}
Ganti, A., Modiano, E., and Tsitsiklis, J.~N.
\newblock Optimal transmission scheduling in symmetric communication models
  with intermittent connectivity.
\newblock \emph{IEEE Transactions on Information Theory}, 53\penalty0
  (3):\penalty0 998--1008, March 2007.
\newblock ISSN 1557-9654.
\newblock \doi{10.1109/TIT.2006.890695}.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and
  Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Gupta et~al.(2021)Gupta, Yu, Zhao, Kumar, Rovinsky, Xu, Devlin, and
  Levine]{gupta_reset-free_2021}
Gupta, A., Yu, J., Zhao, T.~Z., Kumar, V., Rovinsky, A., Xu, K., Devlin, T.,
  and Levine, S.
\newblock Reset-{Free} {Reinforcement} {Learning} via {Multi}-{Task}
  {Learning}: {Learning} {Dexterous} {Manipulation} {Behaviors} without {Human}
  {Intervention}, April 2021.
\newblock URL \url{http://arxiv.org/abs/2104.11203}.
\newblock arXiv:2104.11203 [cs].

\bibitem[Gupta et~al.(2023)Gupta, Chandak, Jordan, Thomas, and
  da~Silva]{gupta2023behavior}
Gupta, D., Chandak, Y., Jordan, S.~M., Thomas, P.~S., and da~Silva, B.~C.
\newblock Behavior alignment via reward function optimization, 2023.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner_dreamerv3_2023}
Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T.
\newblock Mastering diverse domains through world models, 2023.

\bibitem[Hajek(1982)]{hajek1982hitting}
Hajek, B.
\newblock Hitting-time and occupation-time bounds implied by drift analysis
  with applications.
\newblock \emph{Advances in Applied probability}, 14\penalty0 (3):\penalty0
  502--525, 1982.

\bibitem[Haley \& Soloway(1992)Haley and Soloway]{haley_nnextra_1992}
Haley, P. and Soloway, D.
\newblock Extrapolation limitations of multilayer feedforward neural networks.
\newblock In \emph{[Proceedings 1992] IJCNN International Joint Conference on
  Neural Networks}, volume~4, pp.\  25--30 vol.4, 1992.
\newblock \doi{10.1109/IJCNN.1992.227294}.

\bibitem[Han et~al.(2015)Han, Levine, and Abbeel]{han_learning_2015}
Han, W., Levine, S., and Abbeel, P.
\newblock Learning compound multi-step controllers under unknown dynamics.
\newblock In \emph{2015 {IEEE}/{RSJ} {International} {Conference} on
  {Intelligent} {Robots} and {Systems} ({IROS})}, pp.\  6435--6442, September
  2015.
\newblock \doi{10.1109/IROS.2015.7354297}.

\bibitem[Hans et~al.(2008)Hans, Schneega{\ss}, Sch{\"a}fer, and
  Udluft]{hans2008safe}
Hans, A., Schneega{\ss}, D., Sch{\"a}fer, A.~M., and Udluft, S.
\newblock Safe exploration for reinforcement learning.
\newblock In \emph{ESANN}, pp.\  143--148, 2008.

\bibitem[Harrison(1998)]{harrisonNmodel1998}
Harrison, J.~M.
\newblock {Heavy traffic analysis of a system with parallel servers: asymptotic
  optimality of discrete-review policies}.
\newblock \emph{The Annals of Applied Probability}, 8\penalty0 (3):\penalty0
  822 -- 848, 1998.
\newblock \doi{10.1214/aoap/1028903452}.
\newblock URL \url{https://doi.org/10.1214/aoap/1028903452}.

\bibitem[Harutyunyan et~al.(2019)Harutyunyan, Dabney, Mesnard, Azar, Piot,
  Heess, van Hasselt, Wayne, Singh, Precup, and
  Munos]{harutyunyan2019hindsight}
Harutyunyan, A., Dabney, W., Mesnard, T., Azar, M., Piot, B., Heess, N., van
  Hasselt, H., Wayne, G., Singh, S., Precup, D., and Munos, R.
\newblock Hindsight credit assignment, 2019.

\bibitem[Huang et~al.(2022)Huang, Dossa, Ye, Braga, Chakraborty, Mehta, and
  Araújo]{huang2022cleanrl}
Huang, S., Dossa, R. F.~J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., and
  Araújo, J.~G.
\newblock Cleanrl: High-quality single-file implementations of deep
  reinforcement learning algorithms.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (274):\penalty0 1--18, 2022.
\newblock URL \url{http://jmlr.org/papers/v23/21-1342.html}.

\bibitem[Kapturowski et~al.(2019)Kapturowski, Ostrovski, Dabney, Quan, and
  Munos]{kapturowski_sqrt_2018}
Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J., and Munos, R.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=r1lyTjAqYX}.

\bibitem[Kellett \& Teel(2003)Kellett and Teel]{kelletstability2003}
Kellett, C. and Teel, A.
\newblock Results on discrete-time control-lyapunov functions.
\newblock In \emph{42nd IEEE International Conference on Decision and Control
  (IEEE Cat. No.03CH37475)}, volume~6, pp.\  5961--5966 Vol.6, 2003.
\newblock \doi{10.1109/CDC.2003.1271964}.

\bibitem[Koller et~al.(2018)Koller, Berkenkamp, Turchetta, and
  Krause]{koller2018learning}
Koller, T., Berkenkamp, F., Turchetta, M., and Krause, A.
\newblock Learning-based model predictive control for safe exploration.
\newblock In \emph{2018 IEEE Conference on Decision and Control (CDC)}, pp.\
  6059--6066. IEEE, 2018.

\bibitem[Krishnasamy et~al.(2019)Krishnasamy, Sen, Johari, and
  Shakkottai]{krishnasamy2019learning}
Krishnasamy, S., Sen, R., Johari, R., and Shakkottai, S.
\newblock Learning unknown service rates in queues: A multi-armed bandit
  approach, 2019.

\bibitem[Leon-Garcia(2008)]{garcia_little_2008}
Leon-Garcia, A.
\newblock \emph{Probability, Statistics, and Random Processes for Electrical
  Engineering}.
\newblock Pearson/Prentice Hall, Upper Saddle River, NJ, third edition, 2008.
\newblock ISBN 9780131471221 0131471228.

\bibitem[Liu et~al.(2019)Liu, Xie, and Modiano]{liu_rlqueueing_2019}
Liu, B., Xie, Q., and Modiano, E.
\newblock Reinforcement learning for optimal control of queueing systems.
\newblock In \emph{2019 57th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pp.\  663--670, 2019.
\newblock \doi{10.1109/ALLERTON.2019.8919665}.

\bibitem[Liu et~al.(2022)Liu, Xie, and Modiano]{bai_rlqn_2022}
Liu, B., Xie, Q., and Modiano, E.
\newblock Rl-qn: A reinforcement learning framework for optimal control of
  queueing systems.
\newblock \emph{ACM Trans. Model. Perform. Eval. Comput. Syst.}, 7\penalty0
  (1), aug 2022.
\newblock ISSN 2376-3639.
\newblock \doi{10.1145/3529375}.
\newblock URL \url{https://doi.org/10.1145/3529375}.

\bibitem[Maguluri \& Srikant(2016)Maguluri and Srikant]{maguluri2016heavy}
Maguluri, S.~T. and Srikant, R.
\newblock Heavy traffic queue length behavior in a switch under the maxweight
  algorithm.
\newblock \emph{Stochastic Systems}, 6\penalty0 (1):\penalty0 211--250, 2016.

\bibitem[Mahadevan(1996)]{mahadevan_average_1996}
Mahadevan, S.
\newblock Average reward reinforcement learning: {Foundations}, algorithms, and
  empirical results.
\newblock \emph{Machine Learning}, 22\penalty0 (1):\penalty0 159--195, March
  1996.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/BF00114727}.
\newblock URL \url{https://doi.org/10.1007/BF00114727}.

\bibitem[Mao et~al.(2019)Mao, Schwarzkopf, He, and Alizadeh]{mao2019TowardsSO}
Mao, H., Schwarzkopf, M., He, H., and Alizadeh, M.
\newblock Towards safe online reinforcement learning in computer systems.
\newblock 2019.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:204978262}.

\bibitem[Meyn(2022)]{meynCSRL2022}
Meyn, S.
\newblock \emph{Control Systems and Reinforcement Learning}.
\newblock Cambridge University Press, 2022.

\bibitem[Meyn \& Tweedie(2012)Meyn and Tweedie]{meyn2012markov}
Meyn, S.~P. and Tweedie, R.~L.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Naik et~al.(2019)Naik, Shariff, Yasui, and
  Sutton]{naik_discnotopt_2019}
Naik, A., Shariff, R., Yasui, N., and Sutton, R.~S.
\newblock Discounted reinforcement learning is not an optimization problem.
\newblock \emph{CoRR}, abs/1910.02140, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.02140}.

\bibitem[Naik et~al.(2021)Naik, Abbas, White, and Sutton]{naik_continuing_2021}
Naik, A., Abbas, Z., White, A., and Sutton, R.~S.
\newblock Towards reinforcement learning in the continuing setting.
\newblock 2021.
\newblock URL
  \url{https://drive.google.com/file/d/1xh7WjGP2VI_QdpjVWygRC1BuH6WB_gqi/view}.

\bibitem[Neely(2010)]{neely_networkcomm_2010}
Neely, M.~J.
\newblock \emph{Stochastic Network Optimization with Application to
  Communication and Queueing Systems}.
\newblock Morgan and Claypool Publishers, 2010.
\newblock ISBN 160845455X.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ngpotential1999}
Ng, A.~Y., Harada, D., and Russell, S.~J.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Proceedings of the Sixteenth International Conference on
  Machine Learning}, ICML '99, pp.\  278–287, San Francisco, CA, USA, 1999.
  Morgan Kaufmann Publishers Inc.
\newblock ISBN 1558606122.

\bibitem[Pignatelli et~al.(2023)Pignatelli, Ferret, Geist, Mesnard, van
  Hasselt, and Toni]{pignatelli2023CAPsurvey}
Pignatelli, E., Ferret, J., Geist, M., Mesnard, T., van Hasselt, H., and Toni,
  L.
\newblock A survey of temporal credit assignment in deep reinforcement
  learning, 2023.

\bibitem[Puterman(2014)]{puterman_mdp_2014}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman_ppo_2017}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.06347}.

\bibitem[Schwartz(1993)]{schwartz_reinforcement_1993}
Schwartz, A.
\newblock A {Reinforcement} {Learning} {Method} for {Maximizing} {Undiscounted}
  {Rewards}.
\newblock pp.\  298--305, December 1993.
\newblock ISBN 978-1-55860-307-3.
\newblock \doi{10.1016/B978-1-55860-307-3.50045-9}.

\bibitem[Shah et~al.(2020)Shah, Xie, and Xu]{shah_unbounded_2020}
Shah, D., Xie, Q., and Xu, Z.
\newblock Stable reinforcement learning with unbounded state space.
\newblock In Bayen, A.~M., Jadbabaie, A., Pappas, G., Parrilo, P.~A., Recht,
  B., Tomlin, C., and Zeilinger, M. (eds.), \emph{Proceedings of the 2nd
  Conference on Learning for Dynamics and Control}, volume 120 of
  \emph{Proceedings of Machine Learning Research}, pp.\  581--581. PMLR, 10--11
  Jun 2020.
\newblock URL \url{https://proceedings.mlr.press/v120/shah20a.html}.

\bibitem[Sharma et~al.(2021)Sharma, Xu, Sardana, Gupta, Hausman, Levine, and
  Finn]{sharma_autorl_2021}
Sharma, A., Xu, K., Sardana, N., Gupta, A., Hausman, K., Levine, S., and Finn,
  C.
\newblock Autonomous reinforcement learning: Formalism and benchmarking.
\newblock \emph{ArXiv}, abs/2112.09605, 2021.

\bibitem[Sharma et~al.(2022)Sharma, Ahmad, and
  Finn]{sharma2022statedistribution}
Sharma, A., Ahmad, R., and Finn, C.
\newblock A state-distribution matching approach to non-episodic reinforcement
  learning, 2022.

\bibitem[Srikant \& Ying(2014)Srikant and Ying]{srikant_comm_2014}
Srikant, R. and Ying, L.
\newblock \emph{Communication Networks: An Optimization, Control and Stochastic
  Networks Perspective}.
\newblock Cambridge University Press, USA, 2014.
\newblock ISBN 1107036054.

\bibitem[Stolyar(2004)]{stolyar_mw_2004}
Stolyar, A.~L.
\newblock Maxweight scheduling in a generalized switch: State space collapse
  and workload minimization in heavy traffic.
\newblock \emph{The Annals of Applied Probability}, 14\penalty0 (1):\penalty0
  1--53, 2004.
\newblock ISSN 10505164.
\newblock URL \url{http://www.jstor.org/stable/4140489}.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton_rlbook_2018}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock A Bradford Book, Cambridge, MA, USA, 2018.
\newblock ISBN 0262039249.

\bibitem[Tassiulas \& Ephremides(1990)Tassiulas and
  Ephremides]{tassiulas_mw_1990}
Tassiulas, L. and Ephremides, A.
\newblock Stability properties of constrained queueing systems and scheduling
  policies for maximum throughput in multihop radio networks.
\newblock In \emph{29th IEEE Conference on Decision and Control}, pp.\
  2130--2132 vol.4, 1990.
\newblock \doi{10.1109/CDC.1990.204000}.

\bibitem[Taylor et~al.(2019)Taylor, Dorobantu, Le, Yue, and
  Ames]{taylorLearningLyp2019}
Taylor, A.~J., Dorobantu, V.~D., Le, H.~M., Yue, Y., and Ames, A.~D.
\newblock Episodic learning with control lyapunov functions for uncertain
  robotic systems.
\newblock \emph{CoRR}, abs/1903.01577, 2019.
\newblock URL \url{http://arxiv.org/abs/1903.01577}.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorovmujoco2012}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033, 2012.
\newblock \doi{10.1109/IROS.2012.6386109}.

\bibitem[Vinogradska et~al.(2016)Vinogradska, Bischoff, Nguyen-Tuong, Romer,
  Schmidt, and Peters]{vinogradska2016stability}
Vinogradska, J., Bischoff, B., Nguyen-Tuong, D., Romer, A., Schmidt, H., and
  Peters, J.
\newblock Stability of controllers for gaussian process forward models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  545--554, 2016.

\bibitem[Wan et~al.(2021)Wan, Naik, and Sutton]{wan_learning_2021}
Wan, Y., Naik, A., and Sutton, R.~S.
\newblock Learning and {Planning} in {Average}-{Reward} {Markov} {Decision}
  {Processes}.
\newblock In \emph{Proceedings of the 38th {International} {Conference} on
  {Machine} {Learning}}, pp.\  10653--10662. PMLR, July 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/wan21a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Wei et~al.(2020)Wei, Jafarnia-Jahromi, Luo, Sharma, and
  Jain]{wei_model-free_2020}
Wei, C.-Y., Jafarnia-Jahromi, M., Luo, H., Sharma, H., and Jain, R.
\newblock Model-free {Reinforcement} {Learning} in {Infinite}-horizon
  {Average}-reward {Markov} {Decision} {Processes}, February 2020.
\newblock URL \url{http://arxiv.org/abs/1910.07072}.
\newblock arXiv:1910.07072 [cs, stat].

\bibitem[Wei et~al.(2023)Wei, Liu, Wang, and Ying]{wei2023mixedsystem}
Wei, H., Liu, X., Wang, W., and Ying, L.
\newblock Sample efficient reinforcement learning in mixed systems through
  augmented samples and its applications to queueing networks, 2023.

\bibitem[Westenbroek et~al.(2022)Westenbroek, Castaneda, Agrawal, Sastry, and
  Sreenath]{westenbroek_lyapunov_2022}
Westenbroek, T., Castaneda, F., Agrawal, A., Sastry, S., and Sreenath, K.
\newblock Lyapunov design for robust and efficient robotic reinforcement
  learning.
\newblock In \emph{6th Annual Conference on Robot Learning}, 2022.

\bibitem[Xu et~al.(2021)Xu, Zhang, Li, Du, Kawarabayashi, and
  Jegelka]{xu_nnextra_2021}
Xu, K., Zhang, M., Li, J., Du, S.~S., Kawarabayashi, K.-I., and Jegelka, S.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=UH-cmocLJC}.

\bibitem[Yu et~al.(2019)Yu, Yang, Kolar, and Wang]{yu2019convergent}
Yu, M., Yang, Z., Kolar, M., and Wang, Z.
\newblock Convergent policy optimization for safe reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3121--3133, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Wan, Sutton, and
  Whiteson]{zhang_average-reward_2021}
Zhang, S., Wan, Y., Sutton, R.~S., and Whiteson, S.
\newblock Average-{Reward} {Off}-{Policy} {Policy} {Evaluation} with {Function}
  {Approximation}.
\newblock In \emph{Proceedings of the 38th {International} {Conference} on
  {Machine} {Learning}}, pp.\  12578--12588. PMLR, July 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/zhang21u.html}.
\newblock ISSN: 2640-3498.

\bibitem[Zhang \& Ross(2021)Zhang and Ross]{zhang_artrpo_2021}
Zhang, Y. and Ross, K.~W.
\newblock On-policy deep reinforcement learning for the average-reward
  criterion.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  12535--12545. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/zhang21q.html}.

\bibitem[Zhu et~al.(2020)Zhu, Yu, Gupta, Shah, Hartikainen, Singh, Kumar, and
  Levine]{zhu_ingredients_2020}
Zhu, H., Yu, J., Gupta, A., Shah, D., Hartikainen, K., Singh, A., Kumar, V.,
  and Levine, S.
\newblock The {Ingredients} of {Real}-{World} {Robotic} {Reinforcement}
  {Learning}, April 2020.
\newblock URL \url{http://arxiv.org/abs/2004.12570}.
\newblock arXiv:2004.12570 [cs, stat].

\end{thebibliography}
