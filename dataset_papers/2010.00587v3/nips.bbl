\begin{thebibliography}{35}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Agarwal et~al.(2019)Agarwal, Kakade and Yang}]{agarwal2019model}
\textsc{Agarwal, A.}, \textsc{Kakade, S.} and \textsc{Yang, L.~F.} (2019).
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock \textit{arXiv preprint arXiv:1906.03804} .

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\textsc{Ayoub, A.}, \textsc{Jia, Z.}, \textsc{Szepesvari, C.}, \textsc{Wang,
  M.} and \textsc{Yang, L.~F.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \textit{arXiv preprint arXiv:2006.01107} .

\bibitem[{Azar et~al.(2013)Azar, Munos and Kappen}]{azar2013minimax}
\textsc{Azar, M.~G.}, \textsc{Munos, R.} and \textsc{Kappen, H.~J.} (2013).
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock \textit{Machine learning} \textbf{91} 325--349.

\bibitem[{Azar et~al.(2017)Azar, Osband and Munos}]{azar2017minimax}
\textsc{Azar, M.~G.}, \textsc{Osband, I.} and \textsc{Munos, R.} (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \textit{Proceedings of the 34th International Conference on
  Machine Learning-Volume 70}. JMLR. org.

\bibitem[{Brafman and Tennenholtz(2002)}]{brafman2002r}
\textsc{Brafman, R.~I.} and \textsc{Tennenholtz, M.} (2002).
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \textit{Journal of Machine Learning Research} \textbf{3} 213--231.

\bibitem[{Cesa-Bianchi and Lugosi(2006)}]{cesa2006prediction}
\textsc{Cesa-Bianchi, N.} and \textsc{Lugosi, G.} (2006).
\newblock \textit{Prediction, learning, and games}.
\newblock Cambridge university press.

\bibitem[{Dann and Brunskill(2015)}]{dann2015sample}
\textsc{Dann, C.} and \textsc{Brunskill, E.} (2015).
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Dann et~al.(2019)Dann, Li, Wei and Brunskill}]{dann2019policy}
\textsc{Dann, C.}, \textsc{Li, L.}, \textsc{Wei, W.} and \textsc{Brunskill, E.}
  (2019).
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Dong et~al.(2019)Dong, Wang, Chen and Wang}]{dong2019q}
\textsc{Dong, K.}, \textsc{Wang, Y.}, \textsc{Chen, X.} and \textsc{Wang, L.}
  (2019).
\newblock Q-learning with ucb exploration is sample efficient for
  infinite-horizon mdp.
\newblock \textit{arXiv preprint arXiv:1901.09311} .

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\textsc{Jaksch, T.}, \textsc{Ortner, R.} and \textsc{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research} \textbf{11} 1563--1600.

\bibitem[{Jin et~al.(2018)Jin, Allen-Zhu, Bubeck and Jordan}]{jin2018q}
\textsc{Jin, C.}, \textsc{Allen-Zhu, Z.}, \textsc{Bubeck, S.} and
  \textsc{Jordan, M.~I.} (2018).
\newblock Is q-learning provably efficient?
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Kakade et~al.(2003)}]{kakade2003sample}
\textsc{Kakade, S.~M.} \textsc{et~al.} (2003).
\newblock \textit{On the sample complexity of reinforcement learning}.
\newblock Ph.D. thesis, University of London London, England.

\bibitem[{Kearns and Singh(1999)}]{kearns1999finite}
\textsc{Kearns, M.~J.} and \textsc{Singh, S.~P.} (1999).
\newblock Finite-sample convergence rates for q-learning and indirect
  algorithms.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Lattimore and Hutter(2012)}]{lattimore2012pac}
\textsc{Lattimore, T.} and \textsc{Hutter, M.} (2012).
\newblock Pac bounds for discounted mdps.
\newblock In \textit{International Conference on Algorithmic Learning Theory}.
  Springer.

\bibitem[{Liu and Su(2020)}]{liu2020regret}
\textsc{Liu, S.} and \textsc{Su, H.} (2020).
\newblock Regret bounds for discounted mdps.

\bibitem[{Maurer and Pontil(2009)}]{maurer2009empirical}
\textsc{Maurer, A.} and \textsc{Pontil, M.} (2009).
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \textit{arXiv preprint arXiv:0907.3740} .

\bibitem[{Neu and Pike-Burke(2020)}]{neu2020unifying}
\textsc{Neu, G.} and \textsc{Pike-Burke, C.} (2020).
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2007.01891} .

\bibitem[{Osband and Van~Roy(2016)}]{osband2016lower}
\textsc{Osband, I.} and \textsc{Van~Roy, B.} (2016).
\newblock On lower bounds for regret in reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1608.02732} .

\bibitem[{Osband and Van~Roy(2017)}]{osband2017posterior}
\textsc{Osband, I.} and \textsc{Van~Roy, B.} (2017).
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Pacchiano et~al.(2020)Pacchiano, Ball, Parker-Holder, Choromanski and
  Roberts}]{pacchiano2020optimism}
\textsc{Pacchiano, A.}, \textsc{Ball, P.}, \textsc{Parker-Holder, J.},
  \textsc{Choromanski, K.} and \textsc{Roberts, S.} (2020).
\newblock On optimism in model-based reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2006.11911} .

\bibitem[{Russo(2019)}]{russo2019worst}
\textsc{Russo, D.} (2019).
\newblock Worst-case regret bounds for exploration via randomized value
  functions.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Sidford et~al.(2018{\natexlab{a}})Sidford, Wang, Wu, Yang and
  Ye}]{sidford2018near}
\textsc{Sidford, A.}, \textsc{Wang, M.}, \textsc{Wu, X.}, \textsc{Yang, L.~F.}
  and \textsc{Ye, Y.} (2018{\natexlab{a}}).
\newblock Near-optimal time and sample complexities for for solving discounted
  markov decision process with a generative model.
\newblock \textit{arXiv preprint arXiv:1806.01492} .

\bibitem[{Sidford et~al.(2018{\natexlab{b}})Sidford, Wang, Wu and
  Ye}]{sidford2018variance}
\textsc{Sidford, A.}, \textsc{Wang, M.}, \textsc{Wu, X.} and \textsc{Ye, Y.}
  (2018{\natexlab{b}}).
\newblock Variance reduced value iteration and faster algorithms for solving
  markov decision processes.
\newblock In \textit{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium
  on Discrete Algorithms}. SIAM.

\bibitem[{Simchowitz and Jamieson(2019)}]{simchowitz2019non}
\textsc{Simchowitz, M.} and \textsc{Jamieson, K.~G.} (2019).
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Strehl et~al.(2006)Strehl, Li, Wiewiora, Langford and
  Littman}]{strehl2006pac}
\textsc{Strehl, A.~L.}, \textsc{Li, L.}, \textsc{Wiewiora, E.},
  \textsc{Langford, J.} and \textsc{Littman, M.~L.} (2006).
\newblock Pac model-free reinforcement learning.
\newblock In \textit{Proceedings of the 23rd international conference on
  Machine learning}.

\bibitem[{Strehl and Littman(2008)}]{strehl2008analysis}
\textsc{Strehl, A.~L.} and \textsc{Littman, M.~L.} (2008).
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock \textit{Journal of Computer and System Sciences} \textbf{74}
  1309--1331.

\bibitem[{Szita and Szepesv{\'a}ri(2010)}]{szita2010model}
\textsc{Szita, I.} and \textsc{Szepesv{\'a}ri, C.} (2010).
\newblock Model-based reinforcement learning with nearly tight exploration
  complexity bounds .

\bibitem[{Wainwright(2019)}]{wainwright2019variance}
\textsc{Wainwright, M.~J.} (2019).
\newblock Variance-reduced $ q $-learning is minimax optimal.
\newblock \textit{arXiv preprint arXiv:1906.04697} .

\bibitem[{Wang(2017)}]{wang2017randomized}
\textsc{Wang, M.} (2017).
\newblock Randomized linear programming solves the discounted markov decision
  problem in nearly-linear running time.
\newblock \textit{arXiv preprint arXiv:1704.01869} .

\bibitem[{Yang et~al.(2021)Yang, Yang and Du}]{yang2020q}
\textsc{Yang, K.}, \textsc{Yang, L.} and \textsc{Du, S.} (2021).
\newblock Q-learning with logarithmic regret  1576--1584.

\bibitem[{Zanette and Brunskill(2019)}]{zanette2019tighter}
\textsc{Zanette, A.} and \textsc{Brunskill, E.} (2019).
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock \textit{arXiv preprint arXiv:1901.00210} .

\bibitem[{Zhang et~al.(2020{\natexlab{a}})Zhang, Zhou and Ji}]{zhang2020almost}
\textsc{Zhang, Z.}, \textsc{Zhou, Y.} and \textsc{Ji, X.} (2020{\natexlab{a}}).
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock \textit{arXiv preprint arXiv:2004.10019} .

\bibitem[{Zhang et~al.(2020{\natexlab{b}})Zhang, Zhou and Ji}]{zhang2020model}
\textsc{Zhang, Z.}, \textsc{Zhou, Y.} and \textsc{Ji, X.} (2020{\natexlab{b}}).
\newblock Model-free reinforcement learning: from clipped pseudo-regret to
  sample complexity.
\newblock \textit{arXiv preprint arXiv:2006.03864} .

\bibitem[{Zhou et~al.(2021{\natexlab{a}})Zhou, Gu and
  Szepesvari}]{zhou2020nearly}
\textsc{Zhou, D.}, \textsc{Gu, Q.} and \textsc{Szepesvari, C.}
  (2021{\natexlab{a}}).
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \textit{COLT}.

\bibitem[{Zhou et~al.(2021{\natexlab{b}})Zhou, He and Gu}]{zhou2020provably}
\textsc{Zhou, D.}, \textsc{He, J.} and \textsc{Gu, Q.} (2021{\natexlab{b}}).
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\end{thebibliography}
