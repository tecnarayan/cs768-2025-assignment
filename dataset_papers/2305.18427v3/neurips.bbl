\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and
  Zaremba]{andrychowicz2017hindsight}
M.~Andrychowicz, F.~Wolski, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, O.~Pieter~Abbeel, and W.~Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Arjona-Medina et~al.(2019)Arjona-Medina, Gillhofer, Widrich,
  Unterthiner, Brandstetter, and Hochreiter]{rudder}
J.~A. Arjona-Medina, M.~Gillhofer, M.~Widrich, T.~Unterthiner, J.~Brandstetter,
  and S.~Hochreiter.
\newblock Rudder: Return decomposition for delayed rewards.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Chen et~al.(2019)Chen, Yuan, and Tomizuka]{autonumous_driving2}
J.~Chen, B.~Yuan, and M.~Tomizuka.
\newblock Model-free deep reinforcement learning for urban autonomous driving.
\newblock In \emph{2019 IEEE Intelligent Transportation Systems Conference
  (ITSC)}, pages 2765--2771. IEEE, 2019.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
P.~F. Christiano, J.~Leike, T.~Brown, M.~Martic, S.~Legg, and D.~Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Clavera et~al.(2018)Clavera, Rothfuss, Schulman, Fujita, Asfour, and
  Abbeel]{clavera2018model}
I.~Clavera, J.~Rothfuss, J.~Schulman, Y.~Fujita, T.~Asfour, and P.~Abbeel.
\newblock Model-based reinforcement learning via meta-policy optimization.
\newblock In \emph{Conference on Robot Learning}, pages 617--629. PMLR, 2018.

\bibitem[Colas et~al.(2020)Colas, Karch, Lair, Dussoux, Moulin-Frier, Dominey,
  and Oudeyer]{curiosity_language}
C.~Colas, T.~Karch, N.~Lair, J.-M. Dussoux, C.~Moulin-Frier, P.~Dominey, and
  P.-Y. Oudeyer.
\newblock Language as a cognitive tool to imagine goals in curiosity driven
  exploration.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3761--3774, 2020.

\bibitem[Du et~al.(2019)Du, Han, Fang, Dai, Liu, and Tao]{du2019liir}
Y.~Du, L.~Han, M.~Fang, T.~Dai, J.~Liu, and D.~Tao.
\newblock Liir: learning individual intrinsic reward in multi-agent
  reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, pages 4403--4414, 2019.

\bibitem[Eysenbach et~al.(2020)Eysenbach, Geng, Levine, and
  Salakhutdinov]{eysenbach2020rewriting}
B.~Eysenbach, X.~Geng, S.~Levine, and R.~R. Salakhutdinov.
\newblock Rewriting history with inverse rl: Hindsight inference for policy
  improvement.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 14783--14795, 2020.

\bibitem[Fang et~al.(2018)Fang, Zhou, Shi, Gong, Xu, and Zhang]{fang2018dher}
M.~Fang, C.~Zhou, B.~Shi, B.~Gong, J.~Xu, and T.~Zhang.
\newblock Dher: Hindsight experience replay for dynamic goals.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Fang et~al.(2019{\natexlab{a}})Fang, Zhou, Shi, Gong, Xu, and
  Zhang]{fang2019dher}
M.~Fang, C.~Zhou, B.~Shi, B.~Gong, J.~Xu, and T.~Zhang.
\newblock Dher: Hindsight experience replay for dynamic goals.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Fang et~al.(2019{\natexlab{b}})Fang, Zhou, Du, Han, and
  Zhang]{fang2019curriculum}
M.~Fang, T.~Zhou, Y.~Du, L.~Han, and Z.~Zhang.
\newblock Curriculum-guided hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Feng et~al.(2022)Feng, Huang, Zhang, and
  Magliacane]{factored_adaption_huang2022}
F.~Feng, B.~Huang, K.~Zhang, and S.~Magliacane.
\newblock Factored adaptation for non-stationary reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and Meger]{td3}
S.~Fujimoto, H.~Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pages
  1587--1596. PMLR, 2018.

\bibitem[Gangwani et~al.(2020)Gangwani, Zhou, and
  Peng]{uniform_return_redistribution}
T.~Gangwani, Y.~Zhou, and J.~Peng.
\newblock Learning guidance rewards with trajectory-space smoothing.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Grimbly et~al.(2021)Grimbly, Shock, and Pretorius]{grimbly2021causal}
S.~J. Grimbly, J.~Shock, and A.~Pretorius.
\newblock Causal multi-agent reinforcement learning: Review and open problems.
\newblock In \emph{Cooperative AI Workshop, Advances in Neural Information
  Processing Systems}, 2021.

\bibitem[Guestrin et~al.(2001)Guestrin, Koller, and
  Parr]{guestrin2001multiagent}
C.~Guestrin, D.~Koller, and R.~Parr.
\newblock Multiagent planning with factored mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 14, 2001.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018.

\bibitem[Hallak et~al.(2015)Hallak, Schnitzler, Mann, and
  Mannor]{factored_mdp_1}
A.~Hallak, F.~Schnitzler, T.~Mann, and S.~Mannor.
\newblock Off-policy model-based learning under unknown factored dynamics.
\newblock In \emph{International Conference on Machine Learning}, pages
  711--719. PMLR, 2015.

\bibitem[Han et~al.(2022)Han, Ren, Wu, Zhou, and Peng]{han2022off}
B.~Han, Z.~Ren, Z.~Wu, Y.~Zhou, and J.~Peng.
\newblock Off-policy reinforcement learning with delayed rewards.
\newblock In \emph{International Conference on Machine Learning}, pages
  8280--8303. PMLR, 2022.

\bibitem[Han et~al.(2019)Han, Sun, Du, Xiong, Wang, Sun, Liu, and
  Zhang]{han2019grid}
L.~Han, P.~Sun, Y.~Du, J.~Xiong, Q.~Wang, X.~Sun, H.~Liu, and T.~Zhang.
\newblock Grid-wise control for multi-agent reinforcement learning in video
  game ai.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  2576--2585. PMLR, 2019.

\bibitem[Hu et~al.(2020)Hu, Wang, Jia, Wang, Chen, Hao, Wu, and
  Fan]{reward_shaping}
Y.~Hu, W.~Wang, H.~Jia, Y.~Wang, Y.~Chen, J.~Hao, F.~Wu, and C.~Fan.
\newblock Learning to utilize shaping rewards: A new approach of reward
  shaping.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15931--15941, 2020.

\bibitem[Huang et~al.(2021)Huang, Feng, Lu, Magliacane, and
  Zhang]{huang2021adarl}
B.~Huang, F.~Feng, C.~Lu, S.~Magliacane, and K.~Zhang.
\newblock Adarl: What, where, and how to adapt in transfer reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Huang et~al.(2022)Huang, Lu, Leqi, Hern{\'a}ndez-Lobato, Glymour,
  Sch{\"o}lkopf, and Zhang]{huang2022action}
B.~Huang, C.~Lu, L.~Leqi, J.~M. Hern{\'a}ndez-Lobato, C.~Glymour,
  B.~Sch{\"o}lkopf, and K.~Zhang.
\newblock Action-sufficient state representation learning for control with
  structural constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  9260--9279. PMLR, 2022.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{gumble}
E.~Jang, S.~Gu, and B.~Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019trust}
M.~Janner, J.~Fu, M.~Zhang, and S.~Levine.
\newblock When to trust your model: Model-based policy optimization.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Jaques et~al.(2019)Jaques, Lazaridou, Hughes, Gulcehre, Ortega,
  Strouse, Leibo, and De~Freitas]{jaques2019social}
N.~Jaques, A.~Lazaridou, E.~Hughes, C.~Gulcehre, P.~Ortega, D.~Strouse, J.~Z.
  Leibo, and N.~De~Freitas.
\newblock Social influence as intrinsic motivation for multi-agent deep
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3040--3049. PMLR, 2019.

\bibitem[Ke et~al.(2018)Ke, ALIAS PARTH~GOYAL, Bilaniuk, Binas, Mozer, Pal, and
  Bengio]{ke2018sparse}
N.~R. Ke, A.~G. ALIAS PARTH~GOYAL, O.~Bilaniuk, J.~Binas, M.~C. Mozer, C.~Pal,
  and Y.~Bengio.
\newblock Sparse attentive backtracking: Temporal credit assignment through
  reminding.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kearns and Koller(1999)]{kearns1999efficient}
M.~J. Kearns and D.~Koller.
\newblock Efficient reinforcement learning in factored mdps.
\newblock In T.~Dean, editor, \emph{Proceedings of the Sixteenth International
  Joint Conference on Artificial Intelligence}, pages 740--747. Morgan
  Kaufmann, 1999.

\bibitem[Kiran et~al.(2021)Kiran, Sobh, Talpaert, Mannion, Al~Sallab, Yogamani,
  and P{\'e}rez]{autonumous_driving1}
B.~R. Kiran, I.~Sobh, V.~Talpaert, P.~Mannion, A.~A. Al~Sallab, S.~Yogamani,
  and P.~P{\'e}rez.
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 2021.

\bibitem[Liu et~al.(2019)Liu, Luo, Zhong, Chen, Liu, and Peng]{pengjian_arxiv}
Y.~Liu, Y.~Luo, Y.~Zhong, X.~Chen, Q.~Liu, and J.~Peng.
\newblock Sequence modeling of temporal credit assignment for episodic
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1905.13420}, 2019.

\bibitem[Mesnard et~al.(2021)Mesnard, Weber, Viola, Thakoor, Saade,
  Harutyunyan, Dabney, Stepleton, Heess, Guez, et~al.]{causal_pg}
T.~Mesnard, T.~Weber, F.~Viola, S.~Thakoor, A.~Saade, A.~Harutyunyan,
  W.~Dabney, T.~S. Stepleton, N.~Heess, A.~Guez, et~al.
\newblock Counterfactual credit assignment in model-free reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  7654--7664. PMLR, 2021.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{A3C}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~P. Lillicrap, T.~Harley,
  D.~Silver, and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock \emph{CoRR}, abs/1602.01783, 2016.

\bibitem[Murphy(2002)]{murphy2002dynamic}
K.~P. Murphy.
\newblock \emph{Dynamic bayesian networks: representation, inference and
  learning}.
\newblock University of California, Berkeley, 2002.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
A.~Y. Ng, D.~Harada, and S.~J. Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Proceedings of the Sixteenth International Conference on
  Machine Learning}, pages 278--287, 1999.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{curiosity_exploration}
D.~Pathak, P.~Agrawal, A.~A. Efros, and T.~Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  2778--2787. PMLR, 2017.

\bibitem[Patil et~al.(2022)Patil, Hofmarcher, Dinu, Dorfer, Blies,
  Brandstetter, Arjona-Medina, and Hochreiter]{align_rudder}
V.~P. Patil, M.~Hofmarcher, M.-C. Dinu, M.~Dorfer, P.~M. Blies,
  J.~Brandstetter, J.~A. Arjona-Medina, and S.~Hochreiter.
\newblock Align-rudder: Learning from few demonstrations by reward
  redistribution.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2022.

\bibitem[Pearl(2000)]{d-separation}
J.~Pearl.
\newblock Causality: Models, reasoning, and inference, 2000.

\bibitem[Pitis et~al.(2022)Pitis, Creager, Mandlekar, and
  Garg]{varing_causal_structure}
S.~Pitis, E.~Creager, A.~Mandlekar, and A.~Garg.
\newblock Mocoda: Model-based counterfactual data augmentation.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Rajeswar et~al.(2022)Rajeswar, Ibrahim, Surya, Golemo, Vazquez,
  Courville, and Pinheiro]{rajeswar2022haptics}
S.~Rajeswar, C.~Ibrahim, N.~Surya, F.~Golemo, D.~Vazquez, A.~Courville, and
  P.~O. Pinheiro.
\newblock Haptics-based curiosity for sparse-reward tasks.
\newblock In \emph{Conference on Robot Learning}, pages 395--405. PMLR, 2022.

\bibitem[Ramsauer et~al.(2020)Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich,
  Adler, Gruber, Holzleitner, Pavlovi{\'c}, Sandve,
  et~al.]{ramsauer2020hopfield}
H.~Ramsauer, B.~Sch{\"a}fl, J.~Lehner, P.~Seidl, M.~Widrich, T.~Adler,
  L.~Gruber, M.~Holzleitner, M.~Pavlovi{\'c}, G.~K. Sandve, et~al.
\newblock Hopfield networks is all you need.
\newblock \emph{arXiv preprint arXiv:2008.02217}, 2020.

\bibitem[Ren et~al.(2022)Ren, Guo, Zhou, and
  Peng]{randomized_return_decomposition}
Z.~Ren, R.~Guo, Y.~Zhou, and J.~Peng.
\newblock Learning long-term reward redistribution via randomized return
  decomposition.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Reynolds et~al.(2009)]{reynolds2009gaussian}
D.~A. Reynolds et~al.
\newblock Gaussian mixture models.
\newblock \emph{Encyclopedia of biometrics}, 741\penalty0 (659-663), 2009.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{ddpg}
D.~Silver, G.~Lever, N.~Heess, T.~Degris, D.~Wierstra, and M.~Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International conference on machine learning}, pages
  387--395. Pmlr, 2014.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, Lillicrap, Simonyan, and
  Hassabis]{alphazero}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez,
  M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, T.~Lillicrap, K.~Simonyan, and
  D.~Hassabis.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.
\newblock \doi{10.1126/science.aar6404}.
\newblock URL \url{https://www.science.org/doi/abs/10.1126/science.aar6404}.

\bibitem[Spirtes et~al.(2000)Spirtes, Glymour, Scheines, and
  Heckerman]{Spirtes1993CausationPA}
P.~Spirtes, C.~N. Glymour, R.~Scheines, and D.~Heckerman.
\newblock \emph{Causation, prediction, and search}.
\newblock MIT press, 2000.

\bibitem[Sutton and Barto(2018)]{rl_intro}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tambwekar et~al.(2019)Tambwekar, Dhuliawala, Martin, Mehta, Harrison,
  and Riedl]{reward_shaping_neural_story_plot_generation}
P.~Tambwekar, M.~Dhuliawala, L.~J. Martin, A.~Mehta, B.~Harrison, and M.~O.
  Riedl.
\newblock Controllable neural story plot generation via reward shaping.
\newblock In \emph{Proceedings of the Sixteenth International Joint Conference
  on Artificial Intelligence}, pages 5982--5988, 2019.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.
\newblock \doi{10.1109/IROS.2012.6386109}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{attention_is_all_you_need}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Xiao, Xu, Zhu, and Stone]{wang2022causal}
Z.~Wang, X.~Xiao, Z.~Xu, Y.~Zhu, and P.~Stone.
\newblock Causal dynamics learning for task-independent state abstraction.
\newblock \emph{International Conference on Machine Learning}, pages
  23151--23180, 2022.

\bibitem[Widrich et~al.(2021)Widrich, Hofmarcher, Patil, Bitto-Nemling, and
  Hochreiter]{widrich2021modern}
M.~Widrich, M.~Hofmarcher, V.~P. Patil, A.~Bitto-Nemling, and S.~Hochreiter.
\newblock Modern hopfield networks for return decomposition for delayed
  rewards.
\newblock In \emph{Deep RL Workshop NeurIPS 2021}, 2021.

\bibitem[Williams and Rasmussen(2006)]{williams2006gaussian}
C.~K. Williams and C.~E. Rasmussen.
\newblock \emph{Gaussian processes for machine learning}, volume~2.
\newblock MIT press Cambridge, MA, 2006.

\bibitem[Yang et~al.(2020)Yang, Liu, Zhong, and Walid]{yang2020deep}
H.~Yang, X.-Y. Liu, S.~Zhong, and A.~Walid.
\newblock Deep reinforcement learning for automated stock trading: An ensemble
  strategy.
\newblock In \emph{Proceedings of the First ACM International Conference on AI
  in Finance}, pages 1--8, 2020.

\bibitem[Yu et~al.(2021)Yu, Liu, Nemati, and Yin]{yu2021reinforcement}
C.~Yu, J.~Liu, S.~Nemati, and G.~Yin.
\newblock Reinforcement learning in healthcare: A survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 55\penalty0 (1):\penalty0 1--36,
  2021.

\bibitem[Yu et~al.(2020)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{yu2020meta}
T.~Yu, D.~Quillen, Z.~He, R.~Julian, K.~Hausman, C.~Finn, and S.~Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on robot learning}, pages 1094--1100. PMLR, 2020.

\bibitem[Zhang and Bareinboim(2016)]{zhang2016markov}
J.~Zhang and E.~Bareinboim.
\newblock Markov decision processes with unobserved confounders: A causal
  approach.
\newblock Technical report, Technical report, Technical Report R-23, Purdue AI
  Lab, 2016.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Kumor, and
  Bareinboim]{zhang2020causal}
J.~Zhang, D.~Kumor, and E.~Bareinboim.
\newblock Causal imitation learning with unobserved confounders.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12263--12274, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2019)Zhang, Vikram, Smith, Abbeel, Johnson, and
  Levine]{zhang2019solar}
M.~Zhang, S.~Vikram, L.~Smith, P.~Abbeel, M.~Johnson, and S.~Levine.
\newblock Solar: Deep structured representations for model-based reinforcement
  learning.
\newblock In \emph{International conference on machine learning}, pages
  7444--7453. PMLR, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Zohren, and
  Roberts]{zhang2020deep}
Z.~Zhang, S.~Zohren, and S.~Roberts.
\newblock Deep reinforcement learning for trading.
\newblock \emph{The Journal of Financial Data Science}, 2\penalty0
  (2):\penalty0 25--40, 2020{\natexlab{b}}.

\bibitem[Zheng et~al.(2021)Zheng, Chen, Wang, He, Hu, Chen, Fan, Gao, and
  Zhang]{curiosity_episodic}
L.~Zheng, J.~Chen, J.~Wang, J.~He, Y.~Hu, Y.~Chen, C.~Fan, Y.~Gao, and
  C.~Zhang.
\newblock Episodic multi-agent reinforcement learning with curiosity-driven
  exploration.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 3757--3769, 2021.

\end{thebibliography}
