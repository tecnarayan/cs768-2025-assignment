% general LM
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% source code lm
@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}
@article{fried2022incoder,
  title={Incoder: A generative model for code infilling and synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2022}
}
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{kocetkov2022stack,
  title={The stack: 3 tb of permissively licensed source code},
  author={Kocetkov, Denis and Li, Raymond and Allal, Loubna Ben and Li, Jia and Mou, Chenghao and Ferrandis, Carlos Mu{\~n}oz and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and others},
  journal={arXiv preprint arXiv:2211.15533},
  year={2022}
}
@article{wang2023codet5+,
  title={Codet5+: Open code large language models for code understanding and generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023}
}
@article{wei2023coeditor,
  title={Coeditor: Leveraging contextual changes for multi-round code auto-editing},
  author={Wei, Jiayi and Durrett, Greg and Dillig, Isil},
  journal={arXiv preprint arXiv:2305.18584},
  year={2023}
}
@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}
@misc{rozière2024code,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Romain Sauvestre and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2024},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lozhkov2024starcoder,
      title={StarCoder 2 and The Stack v2: The Next Generation}, 
      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2024},
      eprint={2402.19173},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@misc{guo2024deepseekcoder,
      title={DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence}, 
      author={Daya Guo and Qihao Zhu and Dejian Yang and Zhenda Xie and Kai Dong and Wentao Zhang and Guanting Chen and Xiao Bi and Y. Wu and Y. K. Li and Fuli Luo and Yingfei Xiong and Wenfeng Liang},
      year={2024},
      eprint={2401.14196},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

% soft prompting
@misc{li2021prefixtuning,
      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, 
      author={Xiang Lisa Li and Percy Liang},
      year={2021},
      eprint={2101.00190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{liu2022ptuning,
      title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks}, 
      author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
      year={2022},
      eprint={2110.07602},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


% evaluation
@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@misc{wang2023large,
      title={Large Language Models are not Fair Evaluators}, 
      author={Peiyi Wang and Lei Li and Liang Chen and Zefan Cai and Dawei Zhu and Binghuai Lin and Yunbo Cao and Qi Liu and Tianyu Liu and Zhifang Sui},
      year={2023},
      eprint={2305.17926},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% retrieval
@article{robertson1976relevance,
  title={Relevance weighting of search terms},
  author={Robertson, Stephen E and Jones, K Sparck},
  journal={Journal of the American Society for Information science},
  volume={27},
  number={3},
  pages={129--146},
  year={1976},
  publisher={Wiley Online Library}
}

% rag tasks
@inproceedings{liu2020retrieval,
  title={Retrieval-Augmented Generation for Code Summarization via Hybrid GNN},
  author={Liu, Shangqing and Chen, Yu and Xie, Xiaofei and Siow, Jing Kai and Liu, Yang},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{wang2022retrieval,
  title={Retrieval-based Controllable Molecule Generation},
  author={Wang, Zichao and Nie, Weili and Qiao, Zhuoran and Xiao, Chaowei and Baraniuk, Richard and Anandkumar, Anima},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@inproceedings{zhou2022docprompting,
  title={DocPrompting: Generating Code by Retrieving the Docs},
  author={Zhou, Shuyan and Alon, Uri and Xu, Frank F and Jiang, Zhengbao and Neubig, Graham},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


% binary recover task related
@inproceedings{chen2022augmenting,
  title={Augmenting decompiler output with learned variable names and types},
  author={Chen, Qibin and Lacomis, Jeremy and Schwartz, Edward J and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
  booktitle={31st USENIX Security Symposium (USENIX Security 22)},
  pages={4327--4343},
  year={2022}
}
@inproceedings{jin2022symlm,
  title={Symlm: Predicting function names in stripped binaries via context-sensitive execution-aware code embeddings},
  author={Jin, Xin and Pei, Kexin and Won, Jun Yeon and Lin, Zhiqiang},
  booktitle={Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1631--1645},
  year={2022}
}
@article{wang2022jtrans,
  title={jtrans: Jump-aware transformer for binary code similarity},
  author={Wang, Hao and Qu, Wenjie and Katz, Gilad and Zhu, Wenyu and Gao, Zeyu and Qiu, Han and Zhuge, Jianwei and Zhang, Chao},
  journal={arXiv preprint arXiv:2205.12713},
  year={2022}
}
@inproceedings{al2023extending,
  title={Extending source code pre-trained language models to summarise decompiled binarie},
  author={Al-Kaswan, Ali and Ahmed, Toufique and Izadi, Maliheh and Sawant, Anand Ashok and Devanbu, Premkumar and van Deursen, Arie},
  booktitle={2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  pages={260--271},
  year={2023},
  organization={IEEE}
}
@inproceedings{xiong2023hext5,
  title={HexT5: Unified Pre-Training for Stripped Binary Code Information Inference},
  author={Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
  booktitle={2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={774--786},
  year={2023},
  organization={IEEE}
}
@article{jiang2023nova,
  title={Nova $^{+} $: Generative Language Models for Binaries},
  author={Jiang, Nan and Wang, Chengxiao and Liu, Kevin and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2311.13721},
  year={2023}
}
@article{su2024codeart,
  title={Codeart: Better code models by attention regularization when symbols are lacking},
  author={Su, Zian and Xu, Xiangzhe and Huang, Ziyang and Zhang, Zhuo and Ye, Yapeng and Huang, Jianjun and Zhang, Xiangyu},
  journal={Proceedings of the ACM on Software Engineering},
  volume={1},
  number={FSE},
  pages={562--585},
  year={2024},
  publisher={ACM New York, NY, USA}
}
@inproceedings{jiang2024binaryai,
  title={BinaryAI: Binary Software Composition Analysis via Intelligent Binary Source Code Matching},
  author={Jiang, Ling and An, Junwen and Huang, Huihui and Tang, Qiyi and Nie, Sen and Wu, Shi and Zhang, Yuqun},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--13},
  year={2024}
}
@article{jin2023binary,
  title={Binary code summarization: Benchmarking chatgpt/gpt-4 and other large language models},
  author={Jin, Xin and Larson, Jonathan and Yang, Weiwei and Lin, Zhiqiang},
  journal={arXiv preprint arXiv:2312.09601},
  year={2023}
}
@inproceedings{ye2023cp,
  title={CP-BCS: Binary Code Summarization Guided by Control Flow Graph and Pseudo Code},
  author={Ye, Tong and Wu, Lingfei and Ma, Tengfei and Zhang, Xuhong and Du, Yangkai and Liu, Peiyu and Ji, Shouling and Wang, Wenhai},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={14740--14752},
  year={2023}
}
@article{shang2024far,
  title={How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models},
  author={Shang, Xiuwei and Cheng, Shaoyin and Chen, Guoqiang and Zhang, Yanming and Hu, Li and Yu, Xiao and Li, Gangyang and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2404.09836},
  year={2024}
}

% retrieval-augmentation
@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@inproceedings{karpukhin2020dense,
  title={Dense Passage Retrieval for Open-Domain Question Answering},
  author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6769--6781},
  year={2020}
}
@inproceedings{izacard-grave-2021-leveraging,
    title = "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering",
    author = "Izacard, Gautier  and
      Grave, Edouard",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.74",
    doi = "10.18653/v1/2021.eacl-main.74",
    pages = "874--880",
    abstract = "Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.",
}
@article{weijia2023replug,
  title={REPLUG: Retrieval-augmented black-box language models},
  author={Weijia, Shi and Sewon, Min and Michihiro, Yasunaga and Minjoon, Seo and Rich, James and Mike, Lewis and others},
  journal={ArXiv: 2301.12652},
  year={2023}
}

% miscellaneous
@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}
@inproceedings{petroni2019language,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2463--2473},
  year={2019}
}
@article{zhai2023investigating,
  title={Investigating the catastrophic forgetting in multimodal large language models},
  author={Zhai, Yuexiang and Tong, Shengbang and Li, Xiao and Cai, Mu and Qu, Qing and Lee, Yong Jae and Ma, Yi},
  journal={arXiv preprint arXiv:2309.10313},
  year={2023}
}
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}
@article{levine2022standing,
  title={Standing on the shoulders of giant frozen language models},
  author={Levine, Yoav and Dalmedigos, Itay and Ram, Ori and Zeldes, Yoel and Jannai, Daniel and Muhlgay, Dor and Osin, Yoni and Lieber, Opher and Lenz, Barak and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2204.10019},
  year={2022}
}
@inproceedings{yu2022generate,
  title={Generate rather than Retrieve: Large Language Models are Strong Context Generators},
  author={Yu, Wenhao and Iter, Dan and Wang, Shuohang and Xu, Yichong and Ju, Mingxuan and Sanyal, Soumya and Zhu, Chenguang and Zeng, Michael and Jiang, Meng},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}
@article{dettmers2024qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% LMM
@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@article{moon2023anymal,
  title={Anymal: An efficient and scalable any-modality augmented language model},
  author={Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Nagarajan, Tushar and Smith, Matt and Jain, Shashank and Yeh, Chun-Fu and Murugesan, Prakash and Heidari, Peyman and Liu, Yue and others},
  journal={arXiv preprint arXiv:2309.16058},
  year={2023}
}
@article{wang2024can,
  title={Can language models solve graph problems in natural language?},
  author={Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{guo2023gpt4graph,
  title={Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking},
  author={Guo, Jiayan and Du, Lun and Liu, Hengyu},
  journal={arXiv preprint arXiv:2305.15066},
  year={2023}
}

@article{ye2023natural,
  title={Natural language is all a graph needs},
  author={Ye, Ruosong and Zhang, Caiqi and Wang, Runhui and Xu, Shuyuan and Zhang, Yongfeng},
  journal={arXiv preprint arXiv:2308.07134},
  year={2023}
}

@article{wang2024visionllm,
  title={Visionllm: Large language model is also an open-ended decoder for vision-centric tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{huang2024audiogpt,
  title={Audiogpt: Understanding and generating speech, music, sound, and talking head},
  author={Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={21},
  pages={23802--23804},
  year={2024}
}

@article{chai2023graphllm,
  title={Graphllm: Boosting graph reasoning ability of large language model},
  author={Chai, Ziwei and Zhang, Tianjie and Wu, Liang and Han, Kaiqiao and Hu, Xiaohai and Huang, Xuanwen and Yang, Yang},
  journal={arXiv preprint arXiv:2310.05845},
  year={2023}
}

@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}

@article{zhang2023graph,
  title={Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt},
  author={Zhang, Jiawei},
  journal={arXiv preprint arXiv:2304.11116},
  year={2023}
}

@article{wei2023unleashing,
  title={Unleashing the power of graph learning through llm-based autonomous agents},
  author={Wei, Lanning and He, Zhiqiang and Zhao, Huan and Yao, Quanming},
  journal={arXiv preprint arXiv:2309.04565},
  year={2023}
}

@article{tang2023graphgpt,
  title={Graphgpt: Graph instruction tuning for large language models},
  author={Tang, Jiabin and Yang, Yuhao and Wei, Wei and Shi, Lei and Su, Lixin and Cheng, Suqi and Yin, Dawei and Huang, Chao},
  journal={arXiv preprint arXiv:2310.13023},
  year={2023}
}

@inproceedings{xu2015show,
  title={Show, attend and tell: Neural image caption generation with visual attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2048--2057},
  year={2015},
  organization={PMLR}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@inproceedings{das2017visual,
  title={Visual dialog},
  author={Das, Abhishek and Kottur, Satwik and Gupta, Khushi and Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={326--335},
  year={2017}
}

@inproceedings{anderson2018vision,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3674--3683},
  year={2018}
}

@inproceedings{li2019visual,
  title={Visual semantic reasoning for image-text matching},
  author={Li, Kunpeng and Zhang, Yulun and Li, Kai and Li, Yuanyuan and Fu, Yun},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4654--4662},
  year={2019}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{dai2024instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2023improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{gao2023llama,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@inproceedings{zhou2022conditional,
  title={Conditional prompt learning for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16816--16825},
  year={2022}
}

@inproceedings{du2022learning,
  title={Learning to prompt for open-vocabulary object detection with vision-language model},
  author={Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14084--14093},
  year={2022}
}

@inproceedings{burk2022decomperson,
  title={Decomperson: How humans decompile and what we can learn from it},
  author={Burk, Kevin and Pagani, Fabio and Kruegel, Christopher and Vigna, Giovanni},
  booktitle={31st USENIX Security Symposium (USENIX Security 22)},
  pages={2765--2782},
  year={2022}
}

@book{bryant2012understanding,
  title={Understanding how reverse engineers make sense of programs from assembly language representations},
  author={Bryant, Adam R},
  year={2012},
  publisher={Air Force Institute of Technology}
}



@article{pei2020trex,
  title={Trex: Learning execution semantics from micro-traces for binary similarity},
  author={Pei, Kexin and Xuan, Zhou and Yang, Junfeng and Jana, Suman and Ray, Baishakhi},
  journal={arXiv preprint arXiv:2012.08680},
  year={2020}
}
@inproceedings{diemph,
author = {Xu, Xiangzhe and Feng, Shiwei and Ye, Yapeng and Shen, Guangyu and Su, Zian and Cheng, Siyuan and Tao, Guanhong and Shi, Qingkai and Zhang, Zhuo and Zhang, Xiangyu},
title = {Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598121},
doi = {10.1145/3597926.3598121},
abstract = {Given a function in the binary executable form, binary code similarity analysis determines a set of similar functions from a large pool of candidate functions. These similar functions are usually compiled from the same source code with different compilation setups. Such analysis has a large number of applications, such as malware detection, code clone detection, and automatic software patching. The state-of-the art methods utilize complex Deep Learning models such as Transformer models. We observe that these models suffer from undesirable instruction distribution biases caused by specific compiler conventions. We develop a novel technique to detect such biases and repair them by removing the corresponding instructions from the dataset and finetuning the models. This entails synergy between Deep Learning model analysis and program analysis. Our results show that we can substantially improve the state-of-the-art models’ performance by up to 14.4\% in the most challenging cases where test data may be out of the distributions of training data.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1106–1118},
numpages = {13},
keywords = {Transformer, Program Analysis, Binary Similarity Analysis},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}
@inproceedings{pei2021stateformer,
  title={StateFormer: Fine-grained type recovery from binaries using generative state modeling},
  author={Pei, Kexin and Guan, Jonas and Broughton, Matthew and Chen, Zhongtian and Yao, Songchen and Williams-King, David and Ummadisetty, Vikas and Yang, Junfeng and Ray, Baishakhi and Jana, Suman},
  booktitle={Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={690--702},
  year={2021}
}


@inproceedings {dirty,
author = {Qibin Chen and Jeremy Lacomis and Edward J. Schwartz and Claire Le Goues and Graham Neubig and Bogdan Vasilescu},
title = {Augmenting Decompiler Output with Learned Variable Names and Types},
booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
year = {2022},
isbn = {978-1-939133-31-1},
address = {Boston, MA},
pages = {4327--4343},
url = {https://www.usenix.org/conference/usenixsecurity22/presentation/chen-qibin},
publisher = {USENIX Association},
month = aug,
}

@inproceedings{pei2020xda,
  title={Xda: Accurate, robust disassembly with transfer learning},
  author={Pei, Kexin and Guan, Jonas and Williams-King, David and Yang, Junfeng and Jana, Suman},
  booktitle = {{NDSS}},
  publisher = {The Internet Society},
  year      = {2021},
  timestamp = {Thu, 17 Jun 2021 16:04:48 +0200},
}


@inproceedings{varbert,
  title={“Len or index or count, anything but v1”: Predicting Variable Names in Decompilation Output with Transfer Learning},
  author={Pal, Kuntal Kumar and Bajaj, Ati Priya and Banerjee, Pratyay and Dutcher, Audrey and Nakamura, Mutsumi and Basque, Zion Leonahenahe and Gupta, Himanshu and Sawant, Saurabh Arjun and Anantheswaran, Ujjwala and Shoshitaishvili, Yan and others},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
  pages={152--152},
  year={2024},
  organization={IEEE Computer Society}
}


@article{banerjee2021variable,
  title={Variable name recovery in decompiled binary code using constrained masked language modeling},
  author={Banerjee, Pratyay and Pal, Kuntal Kumar and Wang, Fish and Baral, Chitta},
  journal={arXiv preprint arXiv:2103.12801},
  year={2021}
}

@inproceedings{lacomis2019dire,
  title={Dire: A neural approach to decompiled identifier naming},
  author={Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
  booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={628--639},
  year={2019},
  organization={IEEE}
}
@article{xu2023lmpa,
  title={Lmpa: Improving decompilation by synergy of large language model and program analysis},
  author={Xu, Xiangzhe and Zhang, Zhuo and Feng, Shiwei and Ye, Yapeng and Su, Zian and Jiang, Nan and Cheng, Siyuan and Tan, Lin and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2306.02546v1},
  year={2023}
}
@article{xu2024leveraging,
  title={Leveraging Generative Models to Recover Variable Names from Stripped Binary},
  author={Xu, Xiangzhe and Zhang, Zhuo and Su, Zian and Huang, Ziyang and Feng, Shiwei and Ye, Yapeng and Jiang, Nan and Xie, Danning and Cheng, Siyuan and Tan, Lin and others},
  journal={arXiv preprint arXiv:2306.02546v3},
  year={2023}
}
@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{popovic2015chrf,
  title={chrF: character n-gram F-score for automatic MT evaluation},
  author={Popovi{\'c}, Maja},
  booktitle={Proceedings of the tenth workshop on statistical machine translation},
  pages={392--395},
  year={2015}
}

@inproceedings{lin2018nl2bash,
  title={NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System},
  author={Lin, Xi Victoria and Wang, Chenglong and Zettlemoyer, Luke and Ernst, Michael D},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}
@inproceedings{shi2022natural,
  title={Natural Language to Code Translation with Execution},
  author={Shi, Freda and Fried, Daniel and Ghazvininejad, Marjan and Zettlemoyer, Luke and Wang, Sida I},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={3533--3546},
  year={2022}
}