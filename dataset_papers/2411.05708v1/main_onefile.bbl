\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DPVLB24}

\bibitem[AAM23]{abbe2023sgd}
E.~Abbe, E.~B. Adsera, and T.~Misiakiewicz.
\newblock Sgd learning on neural networks: leap complexity and saddle-to-saddle
  dynamics.
\newblock In {\em The Thirty Sixth Annual Conference on Learning Theory}, pages
  2552--2623. PMLR, 2023.

\bibitem[ADGM17]{anandkumar2017homotopy}
A.~Anandkumar, Y.~Deng, R.~Ge, and H.~Mobahi.
\newblock Homotopy analysis for tensor pca.
\newblock In {\em Conference on Learning Theory}, pages 79--104. PMLR, 2017.

\bibitem[AS68]{abramowitz1968handbook}
M.~Abramowitz and I.~A. Stegun.
\newblock {\em Handbook of mathematical functions with formulas, graphs, and
  mathematical tables}, volume~55.
\newblock US Government printing office, 1968.

\bibitem[ATV23]{ATV22}
P.~Awasthi, A.~Tang, and A.~Vijayaraghavan.
\newblock Agnostic learning of general {ReLU} activation using gradient
  descent.
\newblock In {\em The Eleventh International Conference on Learning
  Representations, {ICLR}}, 2023.

\bibitem[BAGJ20]{arous2020algorithmic}
G.~Ben~Arous, R.~Gheissari, and A.~Jagannath.
\newblock Algorithmic thresholds for tensor pca.
\newblock {\em The Annals of Probability}, 48(4):2052--2087, 2020.

\bibitem[BAGJ21]{arous2021online}
G.~Ben~Arous, R.~Gheissari, and A.~Jagannath.
\newblock Online stochastic gradient descent on non-convex losses from
  high-dimensional inference.
\newblock {\em Journal of Machine Learning Research}, 22(106):1--51, 2021.

\bibitem[BvH22]{brailovskaya2022universality}
T.~Brailovskaya and R.~van Handel.
\newblock Universality and sharp matrix concentration inequalities.
\newblock {\em arXiv preprint arXiv:2201.05142}, 2022.

\bibitem[CCFM19]{chen2019gradient}
Y.~Chen, Y.~Chi, J.~Fan, and C.~Ma.
\newblock Gradient descent with random initialization: Fast global convergence
  for nonconvex phase retrieval.
\newblock {\em Mathematical Programming}, 176:5--37, 2019.

\bibitem[CLS15]{candes2015phase}
E.~J. Candes, X.~Li, and M.~Soltanolkotabi.
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 61(4):1985--2007,
  2015.

\bibitem[CM20]{chen2020learning}
S.~Chen and R.~Meka.
\newblock Learning polynomials in few relevant dimensions.
\newblock In {\em Conference on Learning Theory}, pages 1161--1227. PMLR, 2020.

\bibitem[DGK{\etalchar{+}}20]{DGKKS20}
I.~Diakonikolas, S.~Goel, S.~Karmalkar, A.~R. Klivans, and M.~Soltanolkotabi.
\newblock Approximation schemes for {ReLU} regression.
\newblock In {\em Conference on Learning Theory, {COLT}}, volume 125 of {\em
  Proceedings of Machine Learning Research}, pages 1452--1485. {PMLR}, 2020.

\bibitem[DH18]{DudejaH18}
R.~Dudeja and D.~Hsu.
\newblock Learning single-index models in {Gaussian} space.
\newblock In {\em Conference on Learning Theory, {COLT}}, volume~75 of {\em
  Proceedings of Machine Learning Research}, pages 1887--1930. {PMLR}, 2018.

\bibitem[DH21]{dudeja2021statistical}
R.~Dudeja and D.~Hsu.
\newblock Statistical query lower bounds for tensor pca.
\newblock {\em Journal of Machine Learning Research}, 22(83):1--51, 2021.

\bibitem[DJS08]{dalalyan2008new}
A.~S. Dalalyan, A.~Juditsky, and V.~Spokoiny.
\newblock A new algorithm for estimating the effective dimension-reduction
  subspace.
\newblock {\em The Journal of Machine Learning Research}, 9:1647--1678, 2008.

\bibitem[DKPZ21]{DKPZ21}
I.~Diakonikolas, D.~M. Kane, T.~Pittas, and N.~Zarifis.
\newblock The optimality of polynomial regression for agnostic learning under
  {Gaussian} marginals in the {SQ} model.
\newblock In {\em Proceedings of The 34\textsuperscript{th} Conference on
  Learning Theory, {COLT}}, 2021.

\bibitem[DKR23]{DKR23}
I.~Diakonikolas, D.~M. Kane, and L.~Ren.
\newblock Near-optimal cryptographic hardness of agnostically learning
  halfspaces and {ReLU} regression under {Gaussian} marginals.
\newblock In {\em ICML}, 2023.

\bibitem[DKTZ22]{DKTZ22}
I.~Diakonikolas, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning a single neuron with adversarial label noise via gradient
  descent.
\newblock In {\em Conference on Learning Theory (COLT)}, pages 4313--4361,
  2022.

\bibitem[DKZ20]{DKZ20}
I.~Diakonikolas, D.~M. Kane, and N.~Zarifis.
\newblock Near-optimal {SQ} lower bounds for agnostically learning halfspaces
  and {ReLUs} under {G}aussian marginals.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}, 2020.

\bibitem[DLS22]{damian2022neural}
A.~Damian, J.~Lee, and M.~Soltanolkotabi.
\newblock Neural networks can learn representations with gradient descent.
\newblock In {\em Conference on Learning Theory}, pages 5413--5452. PMLR, 2022.

\bibitem[DNGL23]{damian2023smoothing}
A.~Damian, E.~Nichani, R.~Ge, and J.~D. Lee.
\newblock Smoothing the landscape boosts the signal for sgd: Optimal sample
  complexity for learning single index models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[DPVLB24]{damian2024computational}
A.~Damian, L.~Pillaud-Vivien, J.~D. Lee, and J.~Bruna.
\newblock The computational complexity of learning gaussian single-index
  models.
\newblock {\em arXiv preprint arXiv:2403.05529}, 2024.

\bibitem[FCG20]{FCG20}
S.~Frei, Y.~Cao, and Q.~Gu.
\newblock Agnostic learning of a single neuron with gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}, 2020.

\bibitem[FFRS21]{fiorenza2021detailed}
A.~Fiorenza, M.~R. Formica, T.~G. Roskovec, and F.~Soudsk{\`y}.
\newblock Detailed proof of classical gagliardo--nirenberg interpolation
  inequality with historical remarks.
\newblock {\em Zeitschrift f{\"u}r Analysis und ihre Anwendungen},
  40(2):217--236, 2021.

\bibitem[GGK20]{GGK20}
S.~Goel, A.~Gollakota, and A.~R. Klivans.
\newblock Statistical-query lower bounds via functional gradients.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}, 2020.

\bibitem[GGKS23]{GGKS23}
A.~Gollakota, P.~Gopalan, A.~R. Klivans, and K.~Stavropoulos.
\newblock Agnostically learning single-index models using omnipredictors.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem[Hau92]{Haussler:92}
D.~Haussler.
\newblock {Decision theoretic generalizations of the PAC model for neural net
  and other learning applications}.
\newblock {\em Information and Computation}, 100:78--150, 1992.

\bibitem[HJS01]{hristache2001direct}
M.~Hristache, A.~Juditsky, and V.~Spokoiny.
\newblock Direct estimation of the index coefficient in a single-index model.
\newblock {\em Annals of Statistics}, pages 595--623, 2001.

\bibitem[HMS{\etalchar{+}}04]{hardle2004nonparametric}
W.~H{\"a}rdle, M.~M{\"u}ller, S.~Sperlich, A.~Werwatz, et~al.
\newblock {\em Nonparametric and semiparametric models}, volume~1.
\newblock Springer, 2004.

\bibitem[HSS15]{hopkins2015tensor}
S.~B. Hopkins, J.~Shi, and D.~Steurer.
\newblock Tensor principal component analysis via sum-of-square proofs.
\newblock In {\em Conference on Learning Theory}, pages 956--1006. PMLR, 2015.

\bibitem[HSSS16]{hopkins2016fast}
S.~B. Hopkins, T.~Schramm, J.~Shi, and D.~Steurer.
\newblock Fast spectral algorithms from sum-of-squares proofs: tensor
  decomposition and planted sparse vectors.
\newblock In {\em Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pages 178--191, 2016.

\bibitem[Ich93]{ichimura1993semiparametric}
H.~Ichimura.
\newblock Semiparametric least squares {(SLS)} and weighted {SLS} estimation of
  single-index models.
\newblock {\em Journal of econometrics}, 58(1-2):71--120, 1993.

\bibitem[KKSK11]{kakade2011efficient}
S.~M Kakade, V.~Kanade, O.~Shamir, and A.~Kalai.
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock {\em Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[KS09]{kalai2009isotron}
A.~T. Kalai and R.~Sastry.
\newblock The isotron algorithm: High-dimensional isotonic regression.
\newblock In {\em COLT}, 2009.

\bibitem[KSS94]{KSS:94}
M.~Kearns, R.~Schapire, and L.~Sellie.
\newblock Toward efficient agnostic learning.
\newblock {\em Machine Learning}, 17(2/3):115--141, 1994.

\bibitem[MBM18]{MBM2018}
S.~Mei, Y.~Bai, and A.~Montanari.
\newblock The landscape of empirical risk for nonconvex losses.
\newblock {\em The Annals of Statistics}, 46(6A):2747--2774, 2018.

\bibitem[Rah17]{rahman2017wiener}
S.~Rahman.
\newblock Wiener--hermite polynomial expansion for multivariate gaussian
  probability measures.
\newblock {\em Journal of Mathematical Analysis and Applications},
  454(1):303--334, 2017.

\bibitem[RM14]{richard2014statistical}
E.~Richard and A.~Montanari.
\newblock A statistical model for tensor {PCA}.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem[Ros58]{R58}
F.~Rosenblatt.
\newblock The perceptron: A probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological Review}, 65(6):386--408, 1958.

\bibitem[Sol17]{Sol17}
M.~Soltanolkotabi.
\newblock Learning {ReLUs} via gradient descent.
\newblock In {\em Advances in neural information processing systems}, pages
  2007--2017, 2017.

\bibitem[SQW18]{sun2018geometric}
J.~Sun, Q.~Qu, and J.~Wright.
\newblock A geometric analysis of phase retrieval.
\newblock {\em Foundations of Computational Mathematics}, 18:1131--1198, 2018.

\bibitem[WZDD23]{WZDD2023}
P.~Wang, N.~Zarifis, I.~Diakonikolas, and J.~Diakonikolas.
\newblock Robustly learning a single neuron via sharpness.
\newblock {\em 40th International Conference on Machine Learning}, 2023.

\bibitem[ZWDD24]{ZWDD2024}
N.~Zarifis, P.~Wang, I.~Diakonikolas, and J.~Diakonikolas.
\newblock Robustly learning single-index models via alignment sharpness.
\newblock {\em 41th International Conference on Machine Learning, arXiv
  preprint: 2402.17756}, 2024.

\end{thebibliography}
