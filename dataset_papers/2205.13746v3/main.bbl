\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 64--66. PMLR, 2020.

\bibitem[Bai and Jin(2020)]{bai2020provable}
Yu~Bai and Chi Jin.
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  551--560. PMLR, 2020.

\bibitem[Bernhard and Rapaport(1995)]{bernhard1995theorem}
Pierre Bernhard and Alain Rapaport.
\newblock On a theorem of danskin with an application to a theorem of von
  neumann-sion.
\newblock \emph{Nonlinear Analysis: Theory, Methods \& Applications},
  24\penalty0 (8):\penalty0 1163--1181, 1995.

\bibitem[Cen et~al.(2021)Cen, Wei, and Chi]{cen2021fast2}
Shicong Cen, Yuting Wei, and Yuejie Chi.
\newblock Fast policy extragradient methods for competitive games with entropy
  regularization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Chavdarova et~al.(2019)Chavdarova, Gidel, Fleuret, and
  Lacoste-Julien]{chavdarova2019reducing}
Tatjana Chavdarova, Gauthier Gidel, Fran{\c{c}}ois Fleuret, and Simon
  Lacoste-Julien.
\newblock Reducing noise in gan training with variance reduced extragradient.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Chen et~al.(2021)Chen, Ma, and Zhou]{chen2021sample}
Ziyi Chen, Shaocong Ma, and Yi~Zhou.
\newblock Sample efficient stochastic policy extragradient algorithm for
  zero-sum markov game.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Das et~al.(2017)Das, Parthasarathy, and Ravindran]{das2017completely}
Purba Das, T~Parthasarathy, and G~Ravindran.
\newblock On completely mixed stochastic games.
\newblock \emph{arXiv preprint arXiv:1703.04619}, 2017.

\bibitem[Daskalakis et~al.(2017)Daskalakis, Ilyas, Syrgkanis, and
  Zeng]{daskalakis2017training}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training {GANs} with optimism.
\newblock \emph{arXiv preprint arXiv:1711.00141}, 2017.

\bibitem[Daskalakis et~al.(2020)Daskalakis, Foster, and
  Golowich]{daskalakis2020independent}
Constantinos Daskalakis, Dylan~J Foster, and Noah Golowich.
\newblock Independent policy gradient methods for competitive reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing systems},
  33:\penalty0 5527--5540, 2020.

\bibitem[Jin et~al.(2020)Jin, Netrapalli, and Jordan]{jin2020local}
Chi Jin, Praneeth Netrapalli, and Michael Jordan.
\newblock What is local optimality in nonconvex-nonconcave minimax
  optimization?
\newblock In \emph{International Conference on Machine Learning}, pages
  4880--4889. PMLR, 2020.

\bibitem[Kaplansky(1995)]{kaplansky1995contribution}
Irving Kaplansky.
\newblock A contribution to von neumann's theory of games. ii.
\newblock \emph{Linear algebra and its applications}, 226:\penalty0 371--373,
  1995.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem[Khaled and Richt{\'a}rik(2020)]{khaled2020better}
Ahmed Khaled and Peter Richt{\'a}rik.
\newblock Better theory for {SGD} in the nonconvex world.
\newblock \emph{arXiv preprint arXiv:2002.03329}, 2020.

\bibitem[Lan(2022)]{lan2022policy}
Guanghui Lan.
\newblock Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes.
\newblock \emph{Mathematical programming}, pages 1--48, 2022.

\bibitem[Lanctot et~al.(2019)Lanctot, Lockhart, Lespiau, Zambaldi, Upadhyay,
  P{\'e}rolat, Srinivasan, Timbers, Tuyls, Omidshafiei,
  et~al.]{lanctot2019openspiel}
Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi,
  Satyaki Upadhyay, Julien P{\'e}rolat, Sriram Srinivasan, Finbarr Timbers,
  Karl Tuyls, Shayegan Omidshafiei, et~al.
\newblock Openspiel: A framework for reinforcement learning in games.
\newblock \emph{arXiv preprint arXiv:1908.09453}, 2019.

\bibitem[Li et~al.(2022)Li, Yu, Loizou, Gidel, Ma, Le~Roux, and
  Jordan]{li2022convergence}
Chris~Junchi Li, Yaodong Yu, Nicolas Loizou, Gauthier Gidel, Yi~Ma, Nicolas
  Le~Roux, and Michael Jordan.
\newblock On the convergence of stochastic extragradient for bilinear games
  using restarted iteration averaging.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 9793--9826. PMLR, 2022.

\bibitem[Lin et~al.(2020{\natexlab{a}})Lin, Jin, and Jordan]{lin2020gradient}
Tianyi Lin, Chi Jin, and Michael Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  6083--6093. PMLR, 2020{\natexlab{a}}.

\bibitem[Lin et~al.(2020{\natexlab{b}})Lin, Jin, and Jordan]{lin2020near}
Tianyi Lin, Chi Jin, and Michael~I Jordan.
\newblock Near-optimal algorithms for minimax optimization.
\newblock In \emph{Conference on Learning Theory}, pages 2738--2779. PMLR,
  2020{\natexlab{b}}.

\bibitem[McKelvey and Palfrey(1995)]{mckelvey1995quantal}
Richard~D McKelvey and Thomas~R Palfrey.
\newblock Quantal response equilibria for normal form games.
\newblock \emph{Games and Economic Behavior}, 10\penalty0 (1):\penalty0 6--38,
  1995.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  6820--6829. PMLR, 2020.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Ozdaglar, and
  Pattathil]{mokhtari2020unified}
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: Proximal point approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1497--1507. PMLR, 2020.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017bridging}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Gergely Neu, Anders Jonsson, and Vicen{\c{c}} G{\'o}mez.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{nouiehed2019solving}
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason~D Lee, and Meisam
  Razaviyayn.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Ostrovskii et~al.(2021)Ostrovskii, Lowy, and
  Razaviyayn]{ostrovskii2021efficient}
Dmitrii~M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn.
\newblock Efficient search of first-order nash equilibria in nonconvex-concave
  smooth min-max problems.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (4):\penalty0
  2508--2538, 2021.

\bibitem[Perolat et~al.(2015)Perolat, Scherrer, Piot, and
  Pietquin]{perolat2015approximate}
Julien Perolat, Bruno Scherrer, Bilal Piot, and Olivier Pietquin.
\newblock Approximate dynamic programming for two-player zero-sum markov games.
\newblock In \emph{International Conference on Machine Learning}, pages
  1321--1329. PMLR, 2015.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2817--2826. PMLR, 2017.

\bibitem[Raghavan(1978)]{raghavan1978completely}
TES Raghavan.
\newblock Completely mixed games and {M}-matrices.
\newblock \emph{Linear Algebra and its Applications}, 21\penalty0 (1):\penalty0
  35--45, 1978.

\bibitem[Riedmiller and Gabel(2007)]{riedmiller2007experiences}
Martin Riedmiller and Thomas Gabel.
\newblock On experiences in a complex and competitive gaming domain:
  Reinforcement learning meets {RoboCup}.
\newblock In \emph{2007 IEEE Symposium on Computational Intelligence and
  Games}, pages 17--23. IEEE, 2007.

\bibitem[Sayin et~al.(2022)Sayin, Parise, and Ozdaglar]{sayin2022fictitious}
Muhammed~O Sayin, Francesca Parise, and Asuman Ozdaglar.
\newblock Fictitious play in zero-sum stochastic games.
\newblock \emph{SIAM Journal on Control and Optimization}, 60\penalty0
  (4):\penalty0 2095--2114, 2022.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Shapley(1953)]{shapley1953stochastic}
Lloyd~S Shapley.
\newblock Stochastic games.
\newblock \emph{Proceedings of the National Academy of Sciences}, 39\penalty0
  (10):\penalty0 1095--1100, 1953.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in {StarCraft} {II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang and Li(2020)]{wang2020improved}
Yuanhao Wang and Jian Li.
\newblock Improved algorithms for convex-concave minimax optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4800--4810, 2020.

\bibitem[Wei et~al.(2021)Wei, Lee, Zhang, and Luo]{wei2021last}
Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo.
\newblock Last-iterate convergence of decentralized optimistic gradient
  descent/ascent in infinite-horizon competitive markov games.
\newblock In \emph{Conference on Learning Theory}, pages 4259--4299. PMLR,
  2021.

\bibitem[Xie et~al.(2020)Xie, Chen, Wang, and Yang]{xie2020learning}
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang.
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock In \emph{Conference on Learning Theory}, pages 3674--3682. PMLR,
  2020.

\bibitem[Yang et~al.(2020)Yang, Kiyavash, and He]{yang2020global}
Junchi Yang, Negar Kiyavash, and Niao He.
\newblock Global convergence and variance-reduced optimization for a class of
  nonconvex-nonconcave minimax problems.
\newblock \emph{arXiv preprint arXiv:2002.09621}, 2020.

\bibitem[Ying et~al.(2022)Ying, Ding, and Lavaei]{ying2022dual}
Donghao Ying, Yuhao Ding, and Javad Lavaei.
\newblock A dual approach to constrained markov decision processes with entropy
  regularization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1887--1909. PMLR, 2022.

\bibitem[Yu and Jin(2019)]{yu2019computation}
Hao Yu and Rong Jin.
\newblock On the computation and communication complexity of parallel sgd with
  dynamic batch sizes for stochastic non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7174--7183. PMLR, 2019.

\bibitem[Zeng et~al.(2021{\natexlab{a}})Zeng, Anwar, Doan, Raychowdhury, and
  Romberg]{zeng2021decentralized}
Sihan Zeng, Malik~Aqeel Anwar, Thinh~T Doan, Arijit Raychowdhury, and Justin
  Romberg.
\newblock A decentralized policy gradient approach to multi-task reinforcement
  learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1002--1012.
  PMLR, 2021{\natexlab{a}}.

\bibitem[Zeng et~al.(2021{\natexlab{b}})Zeng, Doan, and Romberg]{zeng2021two}
Sihan Zeng, Thinh~T Doan, and Justin Romberg.
\newblock A two-time-scale stochastic optimization framework with applications
  in control and reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.14756}, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2021)Zhao, Tian, Lee, and Du]{zhao2021provably}
Yulai Zhao, Yuandong Tian, Jason~D Lee, and Simon~S Du.
\newblock Provably efficient policy gradient methods for two-player zero-sum
  markov games.
\newblock \emph{arXiv preprint arXiv:2102.08903}, 2021.

\end{thebibliography}
