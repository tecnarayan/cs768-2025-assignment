\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2012)Ahn, Balan, and Welling]{ahn:welling:balan:2012}
S.~Ahn, A.~K. Balan, and M.~Welling.
\newblock Bayesian posterior sampling via stochastic gradient {F}isher scoring.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning, {ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
  2012.

\bibitem[Ahn et~al.(2014)Ahn, Shahbaba, and
  Welling]{Ahn:Shahababa:Welling:2014}
S.~Ahn, B.~Shahbaba, and M.~Welling.
\newblock Distributed stochastic gradient {MCMC}.
\newblock In E.~P. Xing and T.~Jebara, editors, \emph{Proceedings of the 31st
  International Conference on Machine Learning}, volume~32 of \emph{Proceedings
  of Machine Learning Research}, pages 1044--1052, Bejing, China, 22--24 Jun
  2014. PMLR.

\bibitem[{Baker} et~al.(2017){Baker}, {Fearnhead}, {Fox}, and
  {Nemeth}]{baker:fearnhead:fox:nemeth:2017}
J.~{Baker}, P.~{Fearnhead}, E.~B. {Fox}, and C.~{Nemeth}.
\newblock Control variates for stochastic gradient {MCMC}.
\newblock \emph{ArXiv e-prints 1706.05439}, June 2017.

\bibitem[Bardenet et~al.(2017)Bardenet, Doucet, and
  Holmes]{bardenet:doucet:holmes:2017}
R.~Bardenet, A.~Doucet, and C.~Holmes.
\newblock On {M}arkov chain {M}onte {C}arlo methods for tall data.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (47):\penalty0 1--43, 2017.

\bibitem[{Brosse} et~al.(2017){Brosse}, {Durmus}, {Moulines}, and
  {Sabanis}]{2017arXiv171005559B}
N.~{Brosse}, A.~{Durmus}, {\'E}.~{Moulines}, and S.~{Sabanis}.
\newblock {The Tamed Unadjusted {L}angevin Algorithm}.
\newblock \emph{ArXiv e-prints}, Oct. 2017.

\bibitem[{Chatterji} et~al.(2018){Chatterji}, {Flammarion}, {Ma}, {Bartlett},
  and {Jordan}]{flammarion:jordan:2018}
N.~S. {Chatterji}, N.~{Flammarion}, Y.-A. {Ma}, P.~L. {Bartlett}, and M.~I.
  {Jordan}.
\newblock On the theory of variance reduction for stochastic gradient {M}onte
  {C}arlo.
\newblock \emph{ArXiv e-prints 1802.05431}, Feb. 2018.

\bibitem[Chen et~al.(2015)Chen, Ding, and Carin]{chen:ding:carin:2015}
C.~Chen, N.~Ding, and L.~Carin.
\newblock On the convergence of {Stochastic Gradient MCMC} algorithms with
  high-order integrators.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 28}, pages
  2278--2286. Curran Associates, Inc., 2015.

\bibitem[{Chen} et~al.(2017){Chen}, {Wang}, {Zhang}, {Su}, and
  {Carin}]{Chen:Wang:Zhang:2017}
C.~{Chen}, W.~{Wang}, Y.~{Zhang}, Q.~{Su}, and L.~{Carin}.
\newblock A convergence analysis for a class of practical variance-reduction
  stochastic gradient {MCMC}.
\newblock \emph{ArXiv e-prints 1709.01180}, Sept. 2017.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014stochastic}
T.~Chen, E.~Fox, and C.~Guestrin.
\newblock Stochastic gradient hamiltonian {M}onte {C}arlo.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, pages 1683--1691, 2014.

\bibitem[Dalalyan(2017{\natexlab{a}})]{dalalyan:2014}
A.~Dalalyan.
\newblock Theoretical guarantees for approximate sampling from smooth and
  log‚Äêconcave densities.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 79\penalty0 (3):\penalty0 651--676, 2017{\natexlab{a}}.

\bibitem[Dalalyan(2017{\natexlab{b}})]{dalalyan:colt:2017}
A.~Dalalyan.
\newblock Further and stronger analogy between sampling and optimization:
  {L}angevin {M}onte {C}arlo and gradient descent.
\newblock In S.~Kale and O.~Shamir, editors, \emph{Proceedings of the 2017
  Conference on Learning Theory}, volume~65 of \emph{Proceedings of Machine
  Learning Research}, pages 678--689, Amsterdam, Netherlands, 07--10 Jul
  2017{\natexlab{b}}. PMLR.

\bibitem[{Dalalyan} and {Karagulyan}(2017)]{dalalyan:karagulyan:2017}
A.~S. {Dalalyan} and A.~G. {Karagulyan}.
\newblock {User-friendly guarantees for the {L}angevin {M}onte {C}arlo with
  inaccurate gradient}.
\newblock \emph{ArXiv e-prints 1710.00095}, Sept. 2017.

\bibitem[{Dieuleveut} et~al.(2017){Dieuleveut}, {Durmus}, and
  {Bach}]{dieuleveut:durmus:bach:2017}
A.~{Dieuleveut}, A.~{Durmus}, and F.~{Bach}.
\newblock {Bridging the Gap between Constant Step Size Stochastic Gradient
  Descent and Markov Chains}.
\newblock \emph{ArXiv e-prints}, July 2017.

\bibitem[Ding et~al.(2014)Ding, Fang, Babbush, Chen, Skeel, and
  Neven]{Ding:2014}
N.~Ding, Y.~Fang, R.~Babbush, C.~Chen, R.~D. Skeel, and H.~Neven.
\newblock Bayesian sampling using stochastic gradient thermostats.
\newblock In \emph{Proceedings of the 27th International Conference on Neural
  Information Processing Systems - Volume 2}, NIPS'14, pages 3203--3211,
  Cambridge, MA, USA, 2014. MIT Press.

\bibitem[Dubey et~al.(2016)Dubey, J.~Reddi, Williamson, Poczos, Smola, and
  Xing]{Dubey:2016}
K.~A. Dubey, S.~J.~Reddi, S.~A. Williamson, B.~Poczos, A.~J. Smola, and E.~P.
  Xing.
\newblock Variance reduction in stochastic gradient {L}angevin dynamics.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 29}, pages
  1154--1162. Curran Associates, Inc., 2016.

\bibitem[{Durmus} and {Moulines}(2016)]{durmus:moulines:highdimULA}
A.~{Durmus} and E.~{Moulines}.
\newblock {High-dimensional Bayesian inference via the unadjusted {L}angevin
  algorithm}.
\newblock \emph{ArXiv e-prints 1605.01559}, May 2016.

\bibitem[Durmus and Moulines(2017)]{durmus:moulines:2015}
A.~Durmus and E.~Moulines.
\newblock Nonasymptotic convergence analysis for the unadjusted {L}angevin
  algorithm.
\newblock \emph{Ann. Appl. Probab.}, 27\penalty0 (3):\penalty0 1551--1587, 06
  2017.
\newblock \doi{10.1214/16-AAP1238}.

\bibitem[Gelbrich()]{gelbrich:1990}
M.~Gelbrich.
\newblock On a formula for the l2 wasserstein metric between measures on
  euclidean and hilbert spaces.
\newblock \emph{Mathematische Nachrichten}, 147\penalty0 (1):\penalty0
  185--203.
\newblock \doi{10.1002/mana.19901470121}.

\bibitem[Grenander(1983)]{grenander:1983}
U.~Grenander.
\newblock Tutorial in pattern theory.
\newblock Division of Applied Mathematics, Brown University, Providence, 1983.

\bibitem[Grenander and Miller(1994)]{grenander:miller:1994}
U.~Grenander and M.~I. Miller.
\newblock Representations of knowledge in complex systems.
\newblock \emph{J. Roy. Statist. Soc. Ser. B}, 56\penalty0 (4):\penalty0
  549--603, 1994.
\newblock ISSN 0035-9246.
\newblock With discussion and a reply by the authors.

\bibitem[Hasenclever et~al.(2017)Hasenclever, Webb, Lienart, Vollmer,
  Lakshminarayanan, Blundell, and Teh]{JMLR:v18:16-478}
L.~Hasenclever, S.~Webb, T.~Lienart, S.~Vollmer, B.~Lakshminarayanan,
  C.~Blundell, and Y.~W. Teh.
\newblock Distributed {B}ayesian learning with stochastic natural gradient
  expectation propagation and the posterior server.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (106):\penalty0 1--37, 2017.

\bibitem[Jones et~al.(2001)Jones, Oliphant, Peterson, et~al.]{scipy}
E.~Jones, T.~Oliphant, P.~Peterson, et~al.
\newblock {SciPy}: Open source scientific tools for {Python}, 2001.

\bibitem[Karatzas and Shreve(1991)]{karatzas:shreve:1991}
I.~Karatzas and S.~Shreve.
\newblock \emph{Brownian motion and stochastic calculus}.
\newblock Graduate Texts in Mathematics. Springer New York, 1991.
\newblock ISBN 9780387976556.

\bibitem[Korattikara et~al.(2014)Korattikara, Chen, and
  Welling]{Korattikara:Chen:Welling:2014}
A.~Korattikara, Y.~Chen, and M.~Welling.
\newblock Austerity in {MCMC} land: cutting the {M}etropolis-hastings budget.
\newblock In \emph{Proceedings of the 31st International Conference on
  International Conference on Machine Learning - Volume 32}, ICML'14, pages
  I--181--I--189. JMLR.org, 2014.

\bibitem[Li et~al.(2016{\natexlab{a}})Li, Chen, Carlson, and
  Carin]{Li:2016:PSG:3016100.3016149}
C.~Li, C.~Chen, D.~Carlson, and L.~Carin.
\newblock Preconditioned stochastic gradient {L}angevin dynamics for deep
  neural networks.
\newblock In \emph{Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence}, AAAI'16, pages 1788--1794. AAAI Press, 2016{\natexlab{a}}.

\bibitem[Li et~al.(2016{\natexlab{b}})Li, Ahn, and Welling]{li2016scalable}
W.~Li, S.~Ahn, and M.~Welling.
\newblock Scalable {MCMC} for mixed membership stochastic blockmodels.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 723--731,
  2016{\natexlab{b}}.

\bibitem[Ma et~al.(2015)Ma, Chen, and Fox]{Ma:Chen:Fox:2015}
Y.-A. Ma, T.~Chen, and E.~Fox.
\newblock A complete recipe for stochastic gradient {MCMC}.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 28}, pages
  2917--2925. Curran Associates, Inc., 2015.

\bibitem[{Nagapetyan} et~al.(2017){Nagapetyan}, {Duncan}, {Hasenclever},
  {Vollmer}, {Szpruch}, and {Zygalakis}]{vollmer:nagapetyan:2017}
T.~{Nagapetyan}, A.~B. {Duncan}, L.~{Hasenclever}, S.~J. {Vollmer},
  L.~{Szpruch}, and K.~{Zygalakis}.
\newblock The true cost of stochastic gradient {L}angevin dynamics.
\newblock \emph{ArXiv e-prints 1706.02692}, June 2017.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski:juditsky:lan:shapiro:2009}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.
\newblock \doi{10.1137/070704277}.

\bibitem[Patterson and Teh(2013)]{Teh:Patterson:2013}
S.~Patterson and Y.~W. Teh.
\newblock Stochastic gradient riemannian {L}angevin dynamics on the probability
  simplex.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  26}, pages 3102--3110. Curran Associates, Inc., 2013.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Roberts and Tweedie(1996)]{roberts:tweedie-Langevin:1996}
G.~O. Roberts and R.~L. Tweedie.
\newblock Exponential convergence of {L}angevin distributions and their
  discrete approximations.
\newblock \emph{Bernoulli}, 2\penalty0 (4):\penalty0 341--363, 1996.
\newblock ISSN 1350-7265.
\newblock \doi{10.2307/3318418}.

\bibitem[Sato and Nakagawa(2014)]{pmlr-v32-satoa14}
I.~Sato and H.~Nakagawa.
\newblock Approximation analysis of stochastic gradient {L}angevin dynamics by
  using {F}okker-{P}lanck equation and {I}to process.
\newblock In E.~P. Xing and T.~Jebara, editors, \emph{Proceedings of the 31st
  International Conference on Machine Learning}, volume~32 of \emph{Proceedings
  of Machine Learning Research}, pages 982--990, Bejing, China, 22--24 Jun
  2014. PMLR.

\bibitem[Teh et~al.(2016)Teh, Thiery, and Vollmer]{teh2016consistency}
Y.~W. Teh, A.~H. Thiery, and S.~J. Vollmer.
\newblock Consistency and fluctuations for stochastic gradient {L}angevin
  dynamics.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 193--225, 2016.

\bibitem[Villani(2009)]{VillaniTransport}
C.~Villani.
\newblock \emph{Optimal transport : old and new}.
\newblock Grundlehren der mathematischen Wissenschaften. Springer, Berlin,
  2009.
\newblock ISBN 978-3-540-71049-3.

\bibitem[Vollmer et~al.(2016)Vollmer, Zygalakis, and Teh]{JMLR:v17:15-494}
S.~J. Vollmer, K.~C. Zygalakis, and Y.~W. Teh.
\newblock Exploration of the (non-)asymptotic bias and variance of stochastic
  gradient {L}angevin dynamics.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (159):\penalty0 1--48, 2016.

\bibitem[Welling and Teh(2011)]{Welling:Teh:2011}
M.~Welling and Y.~W. Teh.
\newblock Bayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, ICML'11, pages 681--688, USA,
  2011. Omnipress.
\newblock ISBN 978-1-4503-0619-5.

\bibitem[Zhu and Marcotte(1996)]{Zhu:Marcotte:1996}
D.~L. Zhu and P.~Marcotte.
\newblock Co-coercivity and its role in the convergence of iterative schemes
  for solving variational inequalities.
\newblock \emph{SIAM J. on Optimization}, 6\penalty0 (3):\penalty0 714--726,
  Mar. 1996.
\newblock ISSN 1052-6234.
\newblock \doi{10.1137/S1052623494250415}.

\end{thebibliography}
