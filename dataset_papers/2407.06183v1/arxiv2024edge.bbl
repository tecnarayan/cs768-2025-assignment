\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and
  Han]{liu2019variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock \emph{arXiv preprint arXiv:1908.03265}, 2019.

\bibitem[Shi et~al.(2020)Shi, Su, and Jordan]{shi2020learning}
Bin Shi, Weijie~J Su, and Michael~I Jordan.
\newblock On learning rates and {Schr\"odinger} operators.
\newblock \emph{arXiv preprint arXiv:2004.06977}, 2020.

\bibitem[Armijo(1966)]{armijo1966minimization}
Larry Armijo.
\newblock Minimization of functions having lipschitz continuous first partial
  derivatives.
\newblock \emph{Pacific Journal of mathematics}, 16\penalty0 (1):\penalty0
  1--3, 1966.

\bibitem[Nocedal and Wright(1999)]{nocedal1999numerical}
Jorge Nocedal and Stephen~J Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer, 1999.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{{USSR} computational mathematics and mathematical physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Liu and Nocedal(1989)]{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 1989.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste-Julien]{vaswani2019painless}
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
  Simon Lacoste-Julien.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Galli et~al.(2023)Galli, Rauhut, and Schmidt]{galli2023don}
Leonardo Galli, Holger Rauhut, and Mark Schmidt.
\newblock Don't be so monotone: Relaxing stochastic line search in
  over-parameterized models.
\newblock \emph{arXiv preprint arXiv:2306.12747}, 2023.

\bibitem[Mutschler and Zell(2020)]{mutschler2020parabolic}
Maximus Mutschler and Andreas Zell.
\newblock Parabolic approximation line search for dnns.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5405--5416, 2020.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and
  Lacoste-Julien]{loizou2021stochastic}
Nicolas Loizou, Sharan Vaswani, Issam~Hadj Laradji, and Simon Lacoste-Julien.
\newblock Stochastic polyak step-size for sgd: An adaptive learning rate for
  fast convergence.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2021.

\bibitem[Berrada et~al.(2020)Berrada, Zisserman, and
  Kumar]{berrada2020training}
Leonard Berrada, Andrew Zisserman, and M~Pawan Kumar.
\newblock Training neural networks for and by interpolation.
\newblock In \emph{International conference on machine learning}. PMLR, 2020.

\bibitem[Defazio and Mishchenko(2023)]{defazio2023learning}
Aaron Defazio and Konstantin Mishchenko.
\newblock Learning-rate-free learning by d-adaptation.
\newblock \emph{arXiv preprint arXiv:2301.07733}, 2023.

\bibitem[Cutkosky et~al.(2023)Cutkosky, Defazio, and
  Mehta]{cutkosky2023mechanic}
Ashok Cutkosky, Aaron Defazio, and Harsh Mehta.
\newblock Mechanic: A learning rate tuner.
\newblock \emph{arXiv preprint arXiv:2306.00144}, 2023.

\bibitem[Mishchenko and Defazio(2023)]{mishchenko2023prodigy}
Konstantin Mishchenko and Aaron Defazio.
\newblock Prodigy: An expeditiously adaptive parameter-free learner.
\newblock \emph{arXiv preprint arXiv:2306.06101}, 2023.

\bibitem[Ivgi et~al.(2023)Ivgi, Hinder, and Carmon]{ivgi2023dog}
Maor Ivgi, Oliver Hinder, and Yair Carmon.
\newblock Dog is sgd's best friend: A parameter-free dynamic step size
  schedule.
\newblock \emph{arXiv preprint arXiv:2302.12022}, 2023.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Bengio, Frasconi, Schmidhuber,
  et~al.]{hochreiter2001gradient}
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J{\"u}rgen Schmidhuber, et~al.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies, 2001.

\bibitem[Jastrz{\k{e}}bski et~al.(2018)Jastrz{\k{e}}bski, Kenton, Ballas,
  Fischer, Bengio, and Storkey]{jastrzkebski2018relation}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Nicolas Ballas, Asja Fischer,
  Yoshua Bengio, and Amos Storkey.
\newblock On the relation between the sharpest directions of dnn loss and the
  sgd step length.
\newblock \emph{arXiv preprint arXiv:1807.05031}, 2018.

\bibitem[Wu et~al.(2018)Wu, Ma, et~al.]{wu2018sgd}
Lei Wu, Chao Ma, et~al.
\newblock How sgd selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Kopitkov and Indelman(2020)]{kopitkov2020neural}
Dmitry Kopitkov and Vadim Indelman.
\newblock Neural spectrum alignment: Empirical study.
\newblock In \emph{Artificial Neural Networks and Machine Learning--ICANN 2020:
  29th International Conference on Artificial Neural Networks, Bratislava,
  Slovakia, September 15--18, 2020, Proceedings, Part II 29}, pages 168--179.
  Springer, 2020.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho, and Geras]{jastrzebski2020break}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
  Tabor, Kyunghyun Cho, and Krzysztof Geras.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.09572}, 2020.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock \emph{arXiv preprint arXiv:2103.00065}, 2021.

\bibitem[Cohen et~al.(2022)Cohen, Ghorbani, Krishnan, Agarwal, Medapati,
  Badura, Suo, Cardoze, Nado, Dahl, et~al.]{cohen2022adaptive}
Jeremy~M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh
  Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George~E
  Dahl, et~al.
\newblock Adaptive gradient methods at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2207.14484}, 2022.

\bibitem[Gilmer et~al.(2021)Gilmer, Ghorbani, Garg, Kudugunta, Neyshabur,
  Cardoze, Dahl, Nado, and Firat]{gilmer2021loss}
Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam
  Neyshabur, David Cardoze, George Dahl, Zachary Nado, and Orhan Firat.
\newblock A loss curvature perspective on training instability in deep
  learning.
\newblock \emph{arXiv preprint arXiv:2110.04369}, 2021.

\bibitem[Hinton et~al.(2012)Hinton, Nitish, and Swersky]{hinton2012divide}
G~Hinton, S~Nitish, and K~Swersky.
\newblock Divide the gradient by a running average of its recent magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Schaul et~al.(2013)Schaul, Zhang, and LeCun]{schaul2013no}
Tom Schaul, Sixin Zhang, and Yann LeCun.
\newblock No more pesky learning rates.
\newblock In \emph{International conference on machine learning}, pages
  343--351. PMLR, 2013.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pages
  2408--2417. PMLR, 2015.

\bibitem[Nesterov et~al.(2018)]{nesterov2018lectures}
Yurii Nesterov et~al.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Roulet et~al.(2023)Roulet, Agarwala, and
  Pedregosa]{roulet2023interplay}
Vincent Roulet, Atish Agarwala, and Fabian Pedregosa.
\newblock On the interplay between stepsize tuning and progressive sharpening.
\newblock In \emph{OPT 2023: Optimization for Machine Learning}, 2023.

\bibitem[Jastrzkebski et~al.(2018)Jastrzkebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{jastrzebski_three_2018}
Stanis{\l}aw Jastrzkebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja
  Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three {{Factors Influencing Minima}} in {{SGD}}, September 2018.

\bibitem[Agarwala and Pennington(2024)]{agarwala_high_2024}
Atish Agarwala and Jeffrey Pennington.
\newblock High dimensional analysis reveals conservative sharpening and a
  stochastic edge of stability, April 2024.

\bibitem[Damian et~al.(2022)Damian, Nichani, and Lee]{damian2022self}
Alex Damian, Eshaan Nichani, and Jason~D Lee.
\newblock Self-stabilization: The implicit bias of gradient descent at the edge
  of stability.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Agarwala et~al.(2022)Agarwala, Pedregosa, and
  Pennington]{agarwala_secondorder_2022}
Atish Agarwala, Fabian Pedregosa, and Jeffrey Pennington.
\newblock Second-order regression models exhibit progressive sharpening to the
  edge of stability, October 2022.

\bibitem[Agarwala and Fisher(2019)]{agarwala_adaptive_2019}
Atish Agarwala and Daniel~S. Fisher.
\newblock Adaptive walks on high-dimensional fitness landscapes and seascapes
  with distance-dependent statistics.
\newblock \emph{Theoretical Population Biology}, 130:\penalty0 13--49, 2019.
\newblock ISSN 0040-5809.
\newblock \doi{https://doi.org/10.1016/j.tpb.2019.09.011}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0040580919301765}.

\bibitem[Isidori(1995)]{isidori1985nonlinear}
Alberto Isidori.
\newblock \emph{Nonlinear control systems}.
\newblock Springer, third edition, 1995.

\bibitem[Almeida et~al.(1999)Almeida, Langlois, Amaral, and
  Plakhov]{almeida1999parameter}
Lu{\'\i}s~B Almeida, Thibault Langlois, Jos{\'e}~D Amaral, and Alexander
  Plakhov.
\newblock Parameter adaptation in stochastic optimization.
\newblock In \emph{On-line learning in neural networks}, pages 111--134, 1999.

\bibitem[Baydin et~al.(2017)Baydin, Cornish, Rubio, Schmidt, and
  Wood]{baydin2017online}
Atilim~Gunes Baydin, Robert Cornish, David~Martinez Rubio, Mark Schmidt, and
  Frank Wood.
\newblock Online learning rate adaptation with hypergradient descent.
\newblock \emph{arXiv preprint arXiv:1703.04782}, 2017.

\bibitem[Riedmiller and Braun(1992)]{riedmiller1992rprop}
Martin Riedmiller and Heinrich Braun.
\newblock Rprop: a fast adaptive learning algorithm.
\newblock In \emph{Proc. of the Int. Symposium on Computer and Information
  Science VII}, 1992.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, ON, Canada, 2009.

\bibitem[Karpathy(2015)]{karpathy2015rnn}
Andrej Karpathy.
\newblock The unreasonable effectiveness of recurrent neural networks.
\newblock \url{http://karpathy.github.io/2015/05/21/rnn-effectiveness/}, 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255. IEEE, 2009.

\bibitem[Howard(2019)]{imagenette}
Jeremy Howard.
\newblock imagenette, 2019.
\newblock URL \url{https://github.com/fastai/imagenette/}.

\bibitem[Dehghani et~al.(2022)Dehghani, Gritsenko, Arnab, Minderer, and
  Tay]{dehghani2021scenic}
Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and
  Yi~Tay.
\newblock Scenic: A jax library for computer vision research and beyond.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 21393--21398, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. pmlr, 2015.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 24261--24272, 2021.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[DeepMind et~al.(2020)DeepMind, Babuschkin, Baumli, Bell, Bhupatiraju,
  Bruce, Buchlovsky, Budden, Cai, Clark, Danihelka, Dedieu, Fantacci, Godwin,
  Jones, Hemsley, Hennigan, Hessel, Hou, Kapturowski, Keck, Kemaev, King,
  Kunesch, Martens, Merzic, Mikulik, Norman, Papamakarios, Quan, Ring, Ruiz,
  Sanchez, Sartran, Schneider, Sezener, Spencer, Srinivasan, Stanojevi\'{c},
  Stokowiec, Wang, Zhou, and Viola]{deepmind2020jax}
DeepMind, Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake
  Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo
  Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones,
  Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski,
  Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza
  Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan,
  Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia
  Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, Milo\v{s}
  Stanojevi\'{c}, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio
  Viola.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/google-deepmind}.

\bibitem[Blondel and Roulet(2024)]{blondel2024elements}
Mathieu Blondel and Vincent Roulet.
\newblock The {E}lements of {D}ifferentiable {P}rogramming.
\newblock \emph{arXiv preprint arXiv:2403.14606}, 2024.

\bibitem[Dagr\'eou et~al.(2024)Dagr\'eou, Ablin, Vaiter, and
  Moreau]{dagreou2024howtocompute}
Mathieu Dagr\'eou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau.
\newblock How to compute hessian-vector products?
\newblock In \emph{ICLR Blogposts 2024}, 2024.
\newblock URL \url{https://iclr-blogposts.github.io/2024/blog/bench-hvp/}.
\newblock https://iclr-blogposts.github.io/2024/blog/bench-hvp/.

\bibitem[Liu et~al.(2023)Liu, Li, Hall, Liang, and Ma]{liu2023sophia}
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma.
\newblock Sophia: A scalable stochastic second-order optimizer for language
  model pre-training.
\newblock \emph{arXiv preprint arXiv:2305.14342}, 2023.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and
  Xiao]{ghorbani2019investigation}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock In \emph{International Conference on Machine Learning}, pages
  2232--2241. PMLR, 2019.

\end{thebibliography}
