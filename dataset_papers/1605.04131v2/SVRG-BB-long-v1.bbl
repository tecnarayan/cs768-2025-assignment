\begin{thebibliography}{10}

\bibitem{allen2016variance}
Z.~Allen-Zhu and E.~Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock {\em arXiv preprint arXiv:1603.05643}, 2016.

\bibitem{harikandeh2015stopwasting}
R.~Babanezhad, M.~O. Ahmed, A.~Virani, M.~Schmidt, K.~Kone{\v{c}}n{\`y}, and
  S.~Sallinen.
\newblock Stop wasting my gradients: Practical {SVRG}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2242--2250, 2015.

\bibitem{barzilai1988two}
J.~Barzilai and J.~M. Borwein.
\newblock Two-point step size gradient methods.
\newblock {\em IMA Journal of Numerical Analysis}, 8(1):141--148, 1988.

\bibitem{Dai-2013}
Y.-H. Dai.
\newblock A new analysis on the {B}arzilai-{B}orwein gradient method.
\newblock {\em Journal of Operations Research Society of China}, 1(2):187--198,
  2013.

\bibitem{Dai-Fletcher-2005}
Y.-H. Dai and R.~Fletcher.
\newblock Projected {B}arzilai-{B}orwein methods for large-scale
  box-constrained quadratic programming.
\newblock {\em Numerische Mathematik}, 100(1):21--47, 2005.

\bibitem{Dai-CBB-2006}
Y.-H. Dai, W.~W. Hager, K.~Schittkowski, and H.~Zhang.
\newblock The cyclic {B}arzilai-{B}orwein method for unconstrained
  optimization.
\newblock {\em IMA Journal of Numerical Analysis}, 26(3):604--627, 2006.

\bibitem{defazio2014saga}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em The Journal of Machine Learning Research}, 12:2121--2159, 2011.

\bibitem{fletcher2005barzilai}
R.~Fletcher.
\newblock On the {B}arzilai-{B}orwein method.
\newblock In {\em Optimization and control with applications}, pages 235--256.
  Springer, 2005.

\bibitem{svrg}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{Kesten-1958}
H.~Kesten.
\newblock Accelerated stochastic approximation.
\newblock {\em The Annals of Mathematical Statistics}, 29(1):41--59, 1958.

\bibitem{konevcny2013semi}
J.~Kone{\v{c}}n{\`y} and P.~Richt{\'a}rik.
\newblock Semi-stochastic gradient descent methods.
\newblock {\em arXiv preprint arXiv:1312.1666}, 2013.

\bibitem{mahsereci2015probabilistic}
M.~Mahsereci and P.~Hennig.
\newblock Probabilistic line searches for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1502.02846}, 2015.

\bibitem{masse2015speed}
P.~Y. Mass{\'e} and Y.~Ollivier.
\newblock Speed learning on the fly.
\newblock {\em arXiv preprint arXiv:1511.02540}, 2015.

\bibitem{Needell-NIPS-2014}
D.~Needell, N.~Srebro, and R.~Ward.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock In {\em NIPS}, 2014.

\bibitem{nesterov2004introductory}
Y.~Nesterov.
\newblock {\em Introductory lectures on convex optimization}, volume~87.
\newblock Springer Science \& Business Media, 2004.

\bibitem{nitanda2014stochastic}
A.~Nitanda.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1574--1582, 2014.

\bibitem{Polyak-average-sgd-1992}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM J. Control and Optimization}, 30:838--855, 1992.

\bibitem{raydan1993barzilai}
M.~Raydan.
\newblock On the {B}arzilai and {B}orwein choice of steplength for the gradient
  method.
\newblock {\em IMA Journal of Numerical Analysis}, 13(3):321--326, 1993.

\bibitem{raydan1997barzilai}
M.~Raydan.
\newblock The {B}arzilai and {B}orwein gradient method for the large scale
  unconstrained minimization problem.
\newblock {\em SIAM Journal on Optimization}, 7(1):26--33, 1997.

\bibitem{Reddi2016SVRG}
S.~J. Reddi, A.~Hefny, S.~Sra, B.~Poczos, and A.~Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock 2016.

\bibitem{schmidt2013minimizing}
R.~L. Roux, M.~Schmidt, and F.~Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2663--2671, 2012.

\bibitem{schraudolph2007stochastic}
N.~N. Schraudolph, J.~Yu, and S.~G{\"u}nter.
\newblock A stochastic quasi-newton method for online convex optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 436--443, 2007.

\bibitem{Shai-Zhang-SDCA}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Jornal of Machine Learning Research}, 14:567--599, 2013.

\bibitem{sopyla2015stochastic}
K.~Sopy{\l}a and P.~Drozda.
\newblock Stochastic gradient descent with {B}arzilai-{B}orwein update step for
  svm.
\newblock {\em Information Sciences}, 316:218--233, 2015.

\bibitem{Wang-Ma-2007}
Y.~Wang and S.~Ma.
\newblock Projected {B}arzilai-{B}orwein methods for large scale nonnegative
  image restorations.
\newblock {\em Inverse Problems in Science and Engineering}, 15(6):559--583,
  2007.

\bibitem{Wen-SISC-2010}
Z.~Wen, W.~Yin, D.~Goldfarb, and Y.~Zhang.
\newblock A fast algorithm for sparse reconstruction based on shrinkage,
  subspace optimization, and continuation.
\newblock {\em SIAM J. SCI. COMPUT}, 32(4):1832--1857, 2010.

\bibitem{sparsa-2009}
S.~J. Wright, R.~D. Nowak, and M.~A.~T. Figueiredo.
\newblock Sparse reconstruction by separable approximation.
\newblock {\em IEEE Transactions on Signal Processing}, 57(7):2479--2493, 2009.

\bibitem{xiao2014proximal}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em SIAM Journal on Optimization}, 24(4):2057--2075, 2014.

\bibitem{Zhang-importance-sampling-2015}
P.~Zhao and T.~Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em ICML}, 2015.

\end{thebibliography}
