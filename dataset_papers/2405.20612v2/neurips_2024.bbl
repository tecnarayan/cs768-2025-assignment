\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine learning challenges workshop}, pages 177--190. Springer, 2005.

\bibitem[Dar et~al.(2023)Dar, Geva, Gupta, and Berant]{dar-etal-2023-analyzing}
Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.
\newblock Analyzing transformers in embedding space.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 16124--16170, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.893}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.893}.

\bibitem[Dong et~al.(2023)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, Li, and Sui]{dong2023survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, Lei Li, and Zhifang Sui.
\newblock A survey on in-context learning, 2023.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Fei et~al.(2023)Fei, Hou, Chen, and Bosselut]{fei-etal-2023-mitigating}
Yu~Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut.
\newblock Mitigating label biases for in-context learning.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 14014--14031, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.783}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.783}.

\bibitem[Feng et~al.(2024)Feng, Zhou, ZHU, Qian, and Mao]{fengunveiling}
Zijian Feng, Hanzhang Zhou, ZIXIAO ZHU, Junlang Qian, and Kezhi Mao.
\newblock Unveiling and manipulating prompt influence in large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Geva et~al.(2021)Geva, Schuster, Berant, and Levy]{geva-etal-2021-transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 5484--5495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.446}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.446}.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva-etal-2022-transformer}
Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg.
\newblock Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 30--45, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.3}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.3}.

\bibitem[Han et~al.(2023)Han, Hao, Dong, Sun, and Wei]{han2022prototypical}
Zhixiong Han, Yaru Hao, Li~Dong, Yutao Sun, and Furu Wei.
\newblock Prototypical calibration for few-shot learning of language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Hanna et~al.(2023)Hanna, Liu, and Variengien]{NEURIPS2023_efbba771}
Michael Hanna, Ollie Liu, and Alexandre Variengien.
\newblock How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 76033--76060. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/efbba7719cc5172d175240f24be11280-Paper-Conference.pdf}.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hu and Liu(2004)]{hu2004mining}
Minqing Hu and Bing Liu.
\newblock Mining and summarizing customer reviews.
\newblock In \emph{Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining}, pages 168--177, 2004.

\bibitem[Lu et~al.(2022)Lu, Bartolo, Moore, Riedel, and Stenetorp]{lu-etal-2022-fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8086--8098, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.556}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.556}.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min-etal-2022-rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 11048--11064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.759}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.759}.

\bibitem[Nostalgebraist(2020)]{Nostalgebraist2020}
Nostalgebraist.
\newblock Interpreting gpt: the logit lens, 2020.
\newblock URL \url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}.

\bibitem[Pang and Lee(2005)]{pang-lee-2005-seeing}
Bo~Pang and Lillian Lee.
\newblock Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.
\newblock In Kevin Knight, Hwee~Tou Ng, and Kemal Oflazer, editors, \emph{Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)}, pages 115--124, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics.
\newblock \doi{10.3115/1219840.1219855}.
\newblock URL \url{https://aclanthology.org/P05-1015}.

\bibitem[Pilehvar and Camacho-Collados(2019)]{pilehvar-camacho-collados-2019-wic}
Mohammad~Taher Pilehvar and Jose Camacho-Collados.
\newblock {W}i{C}: the word-in-context dataset for evaluating context-sensitive meaning representations.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 1267--1273, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1128}.
\newblock URL \url{https://aclanthology.org/N19-1128}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, and Gordon]{roemmele2011choice}
Melissa Roemmele, Cosmin~Adrian Bejan, and Andrew~S Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal reasoning.
\newblock In \emph{2011 AAAI Spring Symposium Series}, 2011.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning, Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, \emph{Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, pages 1631--1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/D13-1170}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Voorhees and Tice(2000)]{voorhees2000building}
Ellen~M Voorhees and Dawn~M Tice.
\newblock Building a question answering test collection.
\newblock In \emph{Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval}, pages 200--207, 2000.

\bibitem[Wang and Komatsuzaki(2021)]{wang2021gpt}
Ben Wang and Aran Komatsuzaki.
\newblock Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2022interpretability}
Kevin~Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
\newblock Interpretability in the wild: a circuit for indirect object identification in gpt-2 small.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Li, Dai, Chen, Zhou, Meng, Zhou, and Sun]{wang-etal-2023-label}
Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu~Sun.
\newblock Label words are anchors: An information flow perspective for understanding in-context learning.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 9840--9855, Singapore, December 2023{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.609}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.609}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Li, Chen, Zhu, Lin, Cao, Liu, Liu, and Sui]{wang2023large}
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu, Tianyu Liu, and Zhifang Sui.
\newblock Large language models are not fair evaluators.
\newblock \emph{arXiv preprint arXiv:2305.17926}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams-etal-2018-broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through inference.
\newblock In Marilyn Walker, Heng Ji, and Amanda Stent, editors, \emph{Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pages 1112--1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N18-1101}.
\newblock URL \url{https://aclanthology.org/N18-1101}.

\bibitem[Yu et~al.(2023)Yu, Merullo, and Pavlick]{yu-etal-2023-characterizing}
Qinan Yu, Jack Merullo, and Ellie Pavlick.
\newblock Characterizing mechanisms for factual recall in language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 9924--9959, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.615}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.615}.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{NIPS2015_250cf8b5}
Xiang Zhang, Junbo Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~28. Curran Associates, Inc., 2015.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf}.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and Singh]{pmlr-v139-zhao21c}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language models.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pages 12697--12706. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/zhao21c.html}.

\bibitem[Zheng et~al.(2023)Zheng, Zhou, Meng, Zhou, and Huang]{zheng2023large}
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang.
\newblock Large language models are not robust multiple choice selectors.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Qian, Feng, Hui, Zhu, and Mao]{zhou2024llms}
Hanzhang Zhou, Junlang Qian, Zijian Feng, Lu~Hui, Zixiao Zhu, and Kezhi Mao.
\newblock Llms learn task heuristics from demonstrations: A heuristic-driven prompting strategy for document-level event argument extraction.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 11972--11990, 2024.

\end{thebibliography}
