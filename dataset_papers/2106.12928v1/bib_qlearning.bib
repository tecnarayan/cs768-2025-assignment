 @article{BaileyArxivVerlet,
    title={{Finite Regret and Cycles with Fixed Step-Size via Alternating Gradient Descent-Ascent}},
    author={Bailey, P.~J. and Gauthier, G. and Piliouras, G.},
    year={2019},
  journal = {ArXiv e-prints},
    eprint={1907.04392},
    archivePrefix={arXiv},
    primaryClass={cs.GT},
   adsurl = {https://arxiv.org/abs/1905.04532}
}

@ARTICLE{2018arXiv180702629M,
   author = {{Mertikopoulos}, P. and {Zenati}, H. and {Lecouat}, B. and {Foo}, C.-S. and 
	{Chandrasekhar}, V. and {Piliouras}, G.},
    title = "{Mirror descent in saddle-point problems: Going the extra (gradient) mile}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1807.02629},
 keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Mathematics - Optimization and Control, Statistics - Machine Learning},
     year = 2018,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180702629M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{hsieh2019convergence,
  title={On the convergence of single-call stochastic extra-gradient methods},
  author={Hsieh, Y.-G. and Iutzeler, F. and Malick, J. and Mertikopoulos, P.},
  journal={arXiv preprint arXiv:1908.08465},
  year={2019}
}

@inproceedings{
wei2021linear,
title={{Linear Last-iterate Convergence in Constrained Saddle-point Optimization}},
author={Wei, C.-Y. and Lee, C.-W. and Zhang, M. and Luo, H.},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=dx11_7vm5_r}
}

@article{antonakopoulos2020adaptive,
  title={Adaptive extra-gradient methods for min-max optimization and games},
  author={Antonakopoulos, K. and Belmega, E.~V. and Mertikopoulos, P.},
  journal={arXiv preprint arXiv:2010.12100},
  year={2020}
}
@article{Neumann1928,
title = {Zur Theorie der Gesellschaftsspiele},
author = {{von Neumann}, J.},
journal = {Mathematische Annalen},
volume ={100},
pages= {295-300 },
year = {1928}
}

@ARTICLE{Pan17,
       author = {{Pangallo}, M. and {Sanders}, J. and {Galla}, T. and {Farmer}, D.},
        title = "{A taxonomy of learning dynamics in 2 x 2 games}",
      journal = {arXiv e-prints},
     keywords = {Economics - General Economics, Nonlinear Sciences - Chaotic Dynamics},
         year = 2017,
        month = jan,
          eid = {arXiv:1701.09043},
        pages = {arXiv:1701.09043},
archivePrefix = {arXiv},
       eprint = {1701.09043},
 primaryClass = {econ.GN},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170109043P},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Smi17,
author = {{Smith}, L.~N. and {Topin}, N.},
title = "{Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates}",
journal = {arXiv e-prints},
keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
year = 2017,
pages = {arXiv:1708.07120},
archivePrefix = {arXiv},
eprint = {1708.07120},
primaryClass = {cs.LG},
}

@incollection{Li18,
title = {{Visualizing the Loss Landscape of Neural Nets}},
author = {Li, H. and Xu, Z. and Taylor, G. and Studer, C. and Goldstein, T.},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6389--6399},
year = {2018},
publisher = {Curran Associates, Inc.},
}

@inproceedings{Bai20,
author = {Bai, Y. and Jin, C.},
title = {{Provable Self-Play Algorithms for Competitive Reinforcement Learning}},
year = {2020},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
pages = { },
numpages = {10},
location = {Austria},
series = {ICML’20}
}

@article{Mck95,
title = "{Quantal Response Equilibria for Normal Form Games}",
journal = "Games and Economic Behavior",
volume = "10",
number = "1",
pages = "6--38",
year = "1995",
doi = "10.1006/game.1995.1023",
author = "McKelvey, R.~D. and Palfrey, T.~R.",
abstract = "We investigate the use of standard statistical models for quantal choice in a game theoretic setting. Players choose strategies based on relative expected utility and assume other players do so as well. We define a quantal response equilibrium (ORE) as a fixed point of this process and establish existence. For a logit specification of the error structure, we show that as the error goes to zero, QRE approaches a subset of Nash equilibria and also implies a unique selection from the set of Nash equilibria in generic games. We fit the model to a variety of experimental data sets by using maximum likelihood estimation. Journal of Economic Literature Classification Numbers: C19, C44, C72, C92."
}

@article {Gal13,
	author = {Galla, T. and Farmer, J.~D.},
	title = {Complex dynamics in learning complicated games},
	volume = {110},
	number = {4},
	pages = {1232--1236},
	year = {2013},
	doi = {10.1073/pnas.1109672110},
	publisher = {National Academy of Sciences},
	abstract = {Game theory is the standard tool used to model strategic interactions in evolutionary biology and social science. Traditionally, game theory studies the equilibria of simple games. However, is this useful if the game is complicated, and if not, what is? We define a complicated game as one with many possible moves, and therefore many possible payoffs conditional on those moves. We investigate two-person games in which the players learn based on a type of reinforcement learning called experience-weighted attraction (EWA). By generating games at random, we characterize the learning dynamics under EWA and show that there are three clearly separated regimes: (i) convergence to a unique fixed point, (ii) a huge multiplicity of stable fixed points, and (iii) chaotic behavior. In case (iii), the dimension of the chaotic attractors can be very high, implying that the learning dynamics are effectively random. In the chaotic regime, the total payoffs fluctuate intermittently, showing bursts of rapid change punctuated by periods of quiescence, with heavy tails similar to what is observed in fluid turbulence and financial markets. Our results suggest that, at least for some learning algorithms, there is a large parameter regime for which complicated strategic interactions generate inherently unpredictable behavior that is best described in the language of dynamical systems theory.},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences}
}

@inproceedings{Wun10,
author = {Wunder, M. and Littman, M. and Babes, M.},
title = {{Classes of Multiagent Q-Learning Dynamics with $\epsilon$-Greedy Exploration}},
year = {2010},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {1167–1174},
numpages = {8},
location = {Haifa, Israel},
series = {ICML’10}
}

@inproceedings{Bai18,
author = {Bailey, P.~J. and Piliouras, G.},
title = {{Multiplicative Weights Update in Zero-Sum Games}},
year = {2018},
isbn = {9781450358293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3219166.3219235},
booktitle = {Proceedings of the 2018 ACM Conference on Economics and Computation},
pages = {321–338},
numpages = {18},
keywords = {convex analysis, follow-the-regularized-leader, multiplicative weights, online learning, learning in games, gradient descent},
location = {Ithaca, NY, USA},
series = {EC '18}
}

@phdthesis{Wat89,
  title={Learning from delayed rewards},
  author={Watkins, C.J.C.H.},
  year={1989},
  school={King's College, Cambridge United Kingdom}
}

@article{Wat92,
author={Watkins, C.J.C.H. and Dayan, P.},
title={{Technical Note: Q-Learning}},
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
abstract="{\$}{\$}{\backslash}mathcal{\{}Q{\}}{\$}{\$}-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
doi="10.1023/A:1022676722315",
}

@article{Cai16,
author = {Cai, Y. and Candogan, O. and Daskalakis, C. and Papadimitriou, C.},
title = {{Zero-Sum Polymatrix Games: A Generalization of Minmax}},
journal = {Mathematics of Operations Research},
volume = {41},
number = {2},
pages = {648-655},
year = {2016},
doi = {10.1287/moor.2015.0745},
abstract = { We show that in zero-sum polymatrix games, a multiplayer generalization of two-person zero-sum games, Nash equilibria can be found efficiently with linear programming. We also show that the set of coarse correlated equilibria collapses to the set of Nash equilibria. In contrast, other important properties of two-person zero-sum games are not preserved: Nash equilibrium payoffs need not be unique, and Nash equilibrium strategies need not be exchangeable or max-min. }
}

@article{EWA3,
author = {Ho, T.-H. and  Camerer, C.~F. and Chong, J.-K.}, 
title = {Self-tuning experience weighted attraction learning in games},
journal = {Journal of economic theory},
volume = {133}, 
issue = {1},
year = {2007},
pages = {177-198}
}

@article{EWA1,
author = {Ho, T.-H. and  Camerer, C.~F.}, 
title = {Experience-weighted attraction learning in normal form games},
journal = {Econometrica},
volume = {67}, 
issue = {4},
year = {1999},
pages = {827-874}
}

@article{EWA2,
author = {Ho, T.-H. and  Camerer, C.}, 
title = {Experience-weighted attraction learning in coordination games: Probability rules, heterogeneity, and time-variation},
journal = {Journal of mathematical psychology},
volume = {42}, 
issue = {2-3},
year = {1998},
pages = {305-326}
}

@article{Sat05,
title = "{Stability and diversity in collective adaptation}",
journal = "Physica D:Nonlinear Phenomena",
volume = "210",
number = "1",
pages = "21--57",
year = "2005",
author = "Y. Sato and E. Akiyama and J.~P. Crutchfield",
keywords = "Collective adaptation, Game theory, Information theory, Dynamical systems",
abstract = "We derive a class of macroscopic differential equations that describe collective adaptation, starting from a discrete-time stochastic microscopic model. The behavior of each agent is a dynamic balance between adaptation that locally achieves the best action and memory loss that leads to randomized behavior. We show that, although individual agents interact with their environment and other agents in a purely self-interested way, macroscopic behavior can be interpreted as game dynamics. Application to several familiar, explicit game interactions shows that the adaptation dynamics exhibits a diversity of collective behaviors. The simplicity of the assumptions underlying the macroscopic equations suggests that these behaviors should be expected broadly in collective adaptation. We also analyze the adaptation dynamics from an information-theoretic viewpoint and discuss self-organization induced by the dynamics of uncertainty, giving a novel view of collective adaptation."
}

@article{Bow02,
title = "{Multiagent learning using a variable learning rate}",
journal = "Artificial Intelligence",
volume = "136",
number = "2",
pages = "215--250",
year = "2002",
doi = "10.1016/S0004-3702(02)00121-2",
author = "Bowling, M. and Veloso, M.",
keywords = "Multiagent learning, Reinforcement learning, Game theory",
abstract = "Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents' policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, “Win or Learn Fast”, for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method."
}

@inproceedings{Leo21,
author = {{Leonardos}, S. and {Piliouras}, G.},
title = "{Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory}",
series = {AAAI-21},
keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems, Mathematics - Dynamical Systems},
year = {2021},
location = {Virtual},
booktitle = {Proceedings of the 35th AAAI Conference on Artificial Intelligence},
}

@inproceedings{Cla98,
author = {Claus, C. and Boutilier, C.},
title = {{The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems}},
year = {1998},
abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multi agent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-leaming in cooperative multi agent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.},
booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
pages = {746–752},
numpages = {7},
location = {Madison, Wisconsin, USA},
series = {AAAI '98/IAAI '98}
}

@inproceedings{Adl09,
  title={A Note on Strictly Competitive Games},
  author={Adler, I. and Daskalakis, C. and Papadimitriou, C.~H.},
  booktitle={WINE},
  pages={471--474},
  year={2009},
  organization={Springer}
}

@Inbook{Aum17,
author="Aumann, R.~J.",
title="Game Theory",
bookTitle="The New Palgrave Dictionary of Economics",
year="2017",
publisher="Palgrave Macmillan UK",
address="London",
pages="1--40",
abstract="Game theory concerns the behaviour of decision makers whose decisions affect each other. Its analysis is from a rational rather than a psychological or sociological viewpoint. It is indeed a sort of umbrella theory for the rational side of social science, where `social' is interpreted broadly, to include human as well as non-human players (computers, animals, plants). Its methodologies apply in principle to all interactive situations, especially in economics, political science, evolutionary biology, and computer science. There are also important connections with accounting, statistics, the foundations of mathematics, social psychology, law, business, and branches of philosophy such as epistemology and ethics.",
isbn="978-1-349-95121-5",
doi="10.1057/978-1-349-95121-5_942-2",
}


@article{Pan05,
author={Panait, L. and Luke, S.},
title={{Cooperative Multi-Agent Learning: The State of the Art}},
journal={Autonomous Agents and Multi-Agent Systems},
year={2005},
month={Nov},
day={01},
volume={11},
number={3},
pages={387--434},
abstract={Cooperative multi-agent systems (MAS) are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task of programming solutions to MAS problems has spawned increasing interest in machine learning techniques to automate the search and optimization process. We provide a broad survey of the cooperative multi-agent learning literature. Previous surveys of this area have largely focused on issues common to specific subareas (for example, reinforcement learning, RL or robotics). In this survey we attempt to draw from multi-agent learning work in a spectrum of areas, including RL, evolutionary computation, game theory, complex systems, agent modeling, and robotics. We find that this broad view leads to a division of the work into two categories, each with its own special issues: applying a single learner to discover joint solutions to multi-agent problems (team learning), or using multiple simultaneous learners, often one per agent (concurrent learning). Additionally, we discuss direct and indirect communication in connection with learning, plus open issues in task decomposition, scalability, and adaptive dynamics. We conclude with a presentation of multi-agent learning problem domains, and a list of multi-agent learning resources.},
doi={10.1007/s10458-005-2631-2},
}

@article{Rom15,
title = "{The effect of hysteresis on equilibrium selection in coordination games}",
journal = "Journal of Economic Behavior \& Organization",
volume = "111",
pages = "88--105",
year = "2015",
author = "Romero, J.",
keywords = "Hysteresis, Minimum-effort coordination game, Logit equilibrium, Experimental economics, Equilibrium selection",
abstract = "One of the fundamental problems in both economics and organization is to understand how individuals coordinate. The widely used minimum-effort coordination game has served as a simplified model to better understand this problem. This paper first presents theoretical results that give conditions under which the minimum-effort coordination game exhibits hysteresis. It then uses these theoretical results to develop and confirm some experimental hypotheses using human subjects in the laboratory. The main insight is that play in a given game is heavily dependent on the history of parameters leading up to that game. For example, the experiments show that when cost c=0.5 in the minimum-effort coordination game, there is significantly more high effort if the cost has increased to c=0.5 compared to when the cost has decreased to c=0.5. One implication of this is that a temporary change in parameters may be able move the economic system from a bad equilibrium to a good equilibrium."
}
@article{Mer16,
author = {Mertikopoulos, P. and Sandholm, W.~H.},
title = {{Learning in Games via Reinforcement and Regularization}},
journal = {Mathematics of Operations Research},
volume = {41},
number = {4},
pages = {1297-1324},
year = {2016},
doi = {10.1287/moor.2016.0778},
abstract = {We investigate a class of reinforcement learning dynamics where players adjust their strategies based on their actions’ cumulative payoffs over time—specifically, by playing mixed strategies that maximize their expected cumulative payoff minus a regularization term. A widely studied example is exponential reinforcement learning, a process induced by an entropic regularization term which leads mixed strategies to evolve according to the replicator dynamics. However, in contrast to the class of regularization functions used to define smooth best responses in models of stochastic fictitious play, the functions used in this paper need not be infinitely steep at the boundary of the simplex; in fact, dropping this requirement gives rise to an important dichotomy between steep and nonsteep cases. In this general framework, we extend several properties of exponential learning, including the elimination of dominated strategies, the asymptotic stability of strict Nash equilibria, and the convergence of time-averaged trajectories in zero-sum games with an interior Nash equilibrium.}
}

@InProceedings{Abe21, title = {Last-Iterate Convergence Rates for Min-Max Optimization: Convergence of Hamiltonian Gradient Descent and Consensus Optimization}, author = {Jacob, A. and Kevin, A.~L. and Wibisono, A.}, booktitle = {Proceedings of the 32nd International Conference on Algorithmic Learning Theory}, pages = {3--47}, year = {2021}, editor = {Feldman, V. and Ligett, K. and Sabato, S.}, volume = {132}, series = {Proceedings of Machine Learning Research}, month = {16--19 Mar}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v132/abernethy21a/abernethy21a.pdf}, url = { http://proceedings.mlr.press/v132/abernethy21a.html }, abstract = {While classic work in convex-concave min-max optimization relies on average-iterate convergence results, the emergence of nonconvex applications such as training Generative Adversarial Networks has led to renewed interest in last-iterate convergence guarantees. Proving last-iterate convergence is challenging because many natural algorithms, such as Simultaneous Gradient Descent/Ascent, provably diverge or cycle even in simple convex-concave min-max settings, and there are relatively few papers that prove global last-iterate convergence rates beyond the bilinear and convex-strongly concave settings. In this work, we show that the Hamiltonian Gradient Descent (HGD) algorithm achieves linear convergence in a variety of more general settings, including convex-concave problems that satisfy a "sufficiently bilinear" condition. We also prove convergence rates for stochastic HGD and for some parameter settings of the Consensus Optimization algorithm of Mescheder et al. (2017).} }

@inproceedings{Das18,
  title={{Training GANs with Optimism}},
  author={Daskalakis, C. and Ilyas, A. and Syrgkanis, V. and Zeng, H.},
  booktitle ={ICLR},
  year={2018}
}

@InProceedings{Das18l,
  author =	{Daskalakis, C. and Panageas, I.},
  title = {{Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization}},
  booktitle =	{10th Innovations in Theoretical Computer Science  Conference (ITCS 2019)},
  pages =	{27:1--27:18},
  ISBN =	{978-3-95977-095-8},
  ISSN =	{1868-8969},
  year =	{2018},
  volume =	{124},
  editor =	{A. Blum},
  URL =		{http://drops.dagstuhl.de/opus/volltexte/2018/10120},
  URN =		{urn:nbn:de:0030-drops-101204},
  doi =		{10.4230/LIPIcs.ITCS.2019.27},
  annote =	{Keywords: No regret learning, Zero-sum games, Convergence, Dynamical Systems, KL divergence}
}

@inproceedings{Kai11,
author = {Kaisers, M. and Tuyls, K.},
title = {{FAQ-Learning in Matrix Games: Demonstrating Convergence near Nash Equilibria, and Bifurcation of Attractors in the Battle of Sexes}},
year = {2011},
publisher = {AAAI Press},
abstract = {This article studies Frequency Adjusted Q-learning (FAQ-learning), a variation of Q-learning that simulates simultaneous value function updates. The main contributions are empirical and theoretical support for the convergence of FAQ-learning to attractors near Nash equilibria in two-agent two-action matrix games. The games can be divided into three types: Matching pennies, Prisoners' Dilemma and Battle of Sexes. This article shows that the Matching pennies and Prisoners' Dilemma yield one attractor of the learning dynamics, while the Battle of Sexes exhibits a supercritical pitchfork bifurcation at a critical temperature of τ, where one attractor splits into two attractors and one repellent fixed point. Experiments illustrate that the distance between fixed points of the FAQ-learning dynamics and Nash equilibria tends to zero as the exploration parameter τ of FAQ-learning approaches zero.},
booktitle = {Proceedings of the 13th AAAI Conference on Interactive Decision Theory and Game Theory},
pages = {36–42},
numpages = {7},
series = {AAAIWS'11-13}
}


@inproceedings{Kai10,
author = {Kaisers, M. and Tuyls, K.},
title = {Frequency Adjusted Multi-Agent Q-Learning},
year = {2010},
abstract = {Multi-agent learning is a crucial method to control or find solutions for systems, in which more than one entity needs to be adaptive. In today's interconnected world, such systems are ubiquitous in many domains, including auctions in economics, swarm robotics in computer science, and politics in social sciences. Multi-agent learning is inherently more complex than single-agent learning and has a relatively thin theoretical framework supporting it. Recently, multi-agent learning dynamics have been linked to evolutionary game theory, allowing the interpretation of learning as an evolution of competing policies in the mind of the learning agents. The dynamical system from evolutionary game theory that has been linked to Q-learning predicts the expected behavior of the learning agents. Closer analysis however allows for two interesting observations: the predicted behavior is not always the same as the actual behavior, and in case of deviation, the predicted behavior is more desirable. This discrepancy is elucidated in this article, and based on these new insights Frequency Adjusted Q- (FAQ-) learning is proposed. This variation of Q-learning perfectly adheres to the predictions of the evolutionary model for an arbitrarily large part of the policy space. In addition to the theoretical discussion, experiments in the three classes of two-agent two-action games illustrate the superiority of FAQ-learning.},
booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
pages = {309–316},
numpages = {8},
keywords = {Q-learning, multi-agent learning, evolutionary game theory, replicator dynamics},
location = {Toronto, Canada},
series = {AAMAS '10}
}

@inproceedings{Das20,
 author = {Daskalakis, C. and Foster, D.J. and Golowich, N.},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {5527--5540},
 publisher = {Curran Associates, Inc.},
 title = {{Independent Policy Gradient Methods for Competitive Reinforcement Learning}},
 url = {https://proceedings.neurips.cc/paper/2020/file/3b2acfe2e38102074656ed938abf4ac3-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{Kae96,
author = {Kaelbling, L.~P. and Littman, M.~L. and Moore, A.~W.},
title = {Reinforcement Learning: A Survey},
year = {1996},
issue_date = {Jnauary 1996},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {4},
number = {1},
journal = {Journal of Artificial Intelligence Research},
doi =  {10.1613/jair.301},
pages = {237--285},
numpages = {49}
}

@article{Alo10,
title = "{The logit-response dynamics}",
journal = "Games and Economic Behavior",
volume = "68",
number = "2",
pages = "413--427",
year = "2010",
doi = "10.1016/j.geb.2009.08.004",
author = "C. Alós-Ferrer and N. Netzer",
keywords = "Learning in games, Logit-response dynamics, Best-response potential games",
abstract = "We develop a characterization of stochastically stable states for the logit-response learning dynamics in games, with arbitrary specification of revision opportunities. The result allows us to show convergence to the set of Nash equilibria in the class of best-response potential games and the failure of the dynamics to select potential maximizers beyond the class of exact potential games. We also study to which extent equilibrium selection is robust to the specification of revision opportunities. Our techniques can be extended and applied to a wide class of learning dynamics in games."
}
@ARTICLE{Daf20,
       author = {{Dafoe}, A. and {Hughes}, E. and {Bachrach}, Y. and {Collins}, T. and {McKee}, K.~R. and {Leibo}, J.~Z. and {Larson}, K. and {Graepel}, T.},
        title = "{Open Problems in Cooperative AI}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
         year = 2020,
        month = dec,
archivePrefix = {arXiv},
       eprint = {2012.08630},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv201208630D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Wol12,
  title = {{Hysteresis effects of changing the parameters of noncooperative games}},
  author = {Wolpert, D.~H. and Harr{\'e}, M. and Olbrich, E. and Bertschinger, N. and Jost, J.},
  journal = {Phys. Rev. E},
  volume = {85},
  issue = {3},
  pages = {036102},
  numpages = {12},
  year = {2012},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.85.036102},
}

@article{Kim96,
title = "{Equilibrium Selection in n-Person Coordination Games}",
journal = "Games and Economic Behavior",
volume = "15",
number = "2",
pages = "203--227",
year = "1996",
doi = "10.1006/game.1996.0066",
author = "Y. Kim",
abstract = "This paper investigates several approaches to equilibrium selection and the relationships between them. The class of games we study aren-person generalized coordination games with multiple Pareto rankable strict Nash equilibria. The main result is that all selection criteria select the same outcome (namely the risk dominant equilibrium) in two-person games, and that most equivalences break for games with more than two players. All criteria select the Pareto efficient equilibrium in voting games, of which pure coordination games are special cases.Journal of Economic LiteratureClassification Numbers: C70, C72, D82."
}


@ARTICLE{Bus08,
  author={Busoniu, L. and Babuska, R. and {De Schutter}, B.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
  title={{A Comprehensive Survey of Multiagent Reinforcement Learning}}, 
  year={2008},
  volume={38},
  number={2},
  pages={156-172},
  doi={10.1109/TSMCC.2007.913919}}

@article{Zha19,
       author = {{Zhang}, K. and {Yang}, Z. and {Ba{\c{s}}ar}, T.},
        title = "{Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Statistics - Machine Learning},
         year = 2019,
          eid = {arXiv:1911.10635},
        pages = {arXiv:1911.10635},
archivePrefix = {arXiv},
       eprint = {1911.10635},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191110635Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@misc{Lan20,
      title={OpenSpiel: A Framework for Reinforcement Learning in Games}, 
      author={M. Lanctot and E. Lockhart and J.-B. Lespiau and V. Zambaldi and S. Upadhyay and J. P{\'e}rolat and S. Srinivasan and F. Timbers and K. Tuyls and S. Omidshafiei and D. Hennes and D. Morrill and P. Muller and T. Ewalds and R. Faulkner and J. Kram{\'a}r and B. {De Vylder} and B. Saeta and J. Bradbury and D. Ding and S. Borgeaud and M. Lai and J. Schrittwieser and T. Anthony and E. Hughes and I. Danihelka and J. Ryan-Davis},
      year={2020},
      eprint={1908.09453},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Sil16,
author={Silver, D.
and Huang, A.
and Maddison, C.~J.
and Guez, A.
and Sifre, L.
and {van den Driessche}, G.
and Schrittwieser, J.
and Antonoglou, I.
and Panneershelvam, V.
and Lanctot, M.
and Dieleman, S.
and Grewe, D.
and Nham, J.
and Kalchbrenner, N.
and Sutskever, I.
and Lillicrap, T.
and Leach, M.
and Kavukcuoglu, K.
and Graepel, T.
and Hassabis, D.},
title={{Mastering the game of Go with deep neural networks and tree search}},
journal={Nature},
year={2016},
month={Jan},
day={01},
volume={529},
number={7587},
pages={484-489},
abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
issn={1476-4687},
doi={10.1038/nature16961},
}

@Article{Vin19,
author={Vinyals, O.
and Babuschkin, I.
and Czarnecki, W.~M.
and Mathieu, M.
and Dudzik, A.
and Chung, J.
and Choi, D.~H.
and Powell, R.
and Ewalds, T.
and Georgiev, P.
and Oh, J.
and Horgan, D.
and Kroiss, M.
and Danihelka, I.
and Huang, A.
and Sifre, L.
and Cai, T.
and Agapiou, J.~P.
and Jaderberg, M.
and Vezhnevets, A.~S.
and Leblond, R.
and Pohlen, T.
and Dalibard, V.
and Budden, D.
and Sulsky, Y.
and Molloy, J.
and Paine, T.~L.
and Gulcehre, C.
and Wang, Z.
and Pfaff, T.
and Wu, Y.
and Ring, R.
and Yogatama, D.
and W{\"u}nsch, D.
and McKinney, K.
and Smith, O.
and Schaul, T.
and Lillicrap, T.
and Kavukcuoglu, K.
and Hassabis, D.
and Apps, C.
and Silver, D.},
title={{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
journal={Nature},
year={2019},
month={Nov},
day={01},
volume={575},
number={7782},
pages={350-354},
abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
issn={1476-4687},
doi={10.1038/s41586-019-1724-z},
}

@misc{Per20,
      title={From Poincar\'e Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization}, 
      author={Perolat, J. and Munos, R. and Lespiau, J.-B. and Omidshafiei, S. and Rowland, M. and Ortega, P. and Burch, N. and Anthony, T. and Balduzzi, D. and {De Vylder}, B. and Piliouras, G. and Lanctot, M. and Tuyls, K.},
      year={2020},
      eprint={2002.08456},
      archivePrefix={arXiv},
      primaryClass={cs.GT}
}

@inproceedings{Mer18,
author = {Mertikopoulos, P. and Papadimitriou, C. and Piliouras, G.},
title = {{Cycles in Adversarial Regularized Learning}},
year = {2018},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
booktitle = {Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {2703–2717},
numpages = {15},
location = {New Orleans, Louisiana},
series = {SODA ’18}
}

@article{Cou15,
author = {Coucheney, P. and Gaujal, B. and Mertikopoulos, P.},
title = {{Penalty-Regulated Dynamics and Robust Learning Procedures in Games}},
journal = {Mathematics of Operations Research},
volume = {40},
number = {3},
pages = {611--633},
year = {2015},
doi = {10.1287/moor.2014.0687},
abstract = {Starting from a heuristic learning scheme for strategic N-person games, we derive a new class of continuous-time learning dynamics consisting of a replicator-like drift adjusted by a penalty term that renders the boundary of the game’s strategy space repelling. These penalty-regulated dynamics are equivalent to players keeping an exponentially discounted aggregate of their ongoing payoffs and then using a smooth best response to pick an action based on these performance scores. Owing to this inherent duality, the proposed dynamics satisfy a variant of the folk theorem of evolutionary game theory and they converge to (arbitrarily precise) approximations of Nash equilibria in potential games. Motivated by applications to traffic engineering, we exploit this duality further to design a discrete-time, payoff-based learning algorithm that retains these convergence properties and only requires players to observe their in-game payoffs. Moreover, the algorithm remains robust in the presence of stochastic perturbations and observation errors, and it does not require any synchronization between players. }
}

@article{San18,
title = "{The prevalence of chaotic dynamics in games with many players}",
journal = "Scientific Reports",
volume = {8},
number = "1",
pages = {4902},
year = "2018",
doi = "10.1038/s41598-018-22013-5",
author = {Sanders, J.~B.~T. and Farmer, J. D. and Galla, T.},
keywords = "Learning in games, Logit-response dynamics, Best-response potential games",
abstract = "We study adaptive learning in a typical p-player game. The payoffs of the games are randomly generated and then held fixed. The strategies of the players evolve through time as the players learn. The trajectories in the strategy space display a range of qualitatively different behaviours, with attractors that include unique fixed points, multiple fixed points, limit cycles and chaos. In the limit where the game is complicated, in the sense that the players can take many possible actions, we use a generating-functional approach to establish the parameter range in which learning dynamics converge to a stable fixed point. The size of this region goes to zero as the number of players goes to infinity, suggesting that complex non-equilibrium behaviour, exemplified by chaos, is the norm for complicated games with many players."
}


@article{Kia12,
title = {{Dynamics of Boltzmann $Q$ learning in two-player two-action games}},
author = {Kianercy, A. and Galstyan, A.},
journal = {Phys. Rev. E},
volume = {85},
issue = {4},
pages = {041145},
numpages = {9},
year = {2012},
month = {Apr},
publisher = {American Physical Society},
doi = {10.1103/PhysRevE.85.041145},
}

@article{Sat03,
  title = {{Coupled replicator equations for the dynamics of learning in multiagent systems}},
  author = {Sato, Y. and Crutchfield, J.~P.},
  journal = {Phys. Rev. E},
  volume = {67},
  issue = {1},
  pages = {015206},
  numpages = {4},
  year = {2003},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.67.015206},
}

@inproceedings{Tuy03,
author = {Tuyls, K. and Verbeeck, K. and Lenaerts, T.},
title = {{A Selection-mutation Model for Q-learning in Multi-agent Systems}},
booktitle = {Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems},
series = {AAMAS '03},
year = {2003},
location = {Melbourne, Australia},
pages = {693--700},
numpages = {8},
doi = {10.1145/860575.860687},
acmid = {860687},
keywords = {multi-agent systems, reinforcement learning, replicator dynamics},
} 

@inproceedings{Gid19,
title={A Variational Inequality Perspective on Generative Adversarial Networks},
author={G. Gidel and H. Berard and G. Vignoud and P. Vincent and S. Lacoste-Julien},
booktitle={ICLR},
year={2019},
url={https://openreview.net/forum?id=r1laEnA5Ym},
}

@inproceedings{Mer19,
title={Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile},
author={Mertikopoulos, P. and Lecouat, B. and Zenati, H. and Foo, C.-S. and Chandrasekhar, V. and Piliouras, G.},
booktitle={ICLR},
year={2019},
url={https://openreview.net/forum?id=Bkg8jjC9KQ},
}

@inproceedings{Bai19,
  author    = {Bailey, P.~J. and
               Piliouras, G.},
  title     = {{Multi-Agent Learning in Network Zero-Sum Games is a Hamiltonian System}},
  booktitle = {18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  year      = {2019}
 }

@inproceedings{Gid19n,
  title={Negative momentum for improved game dynamics},
  author={Gidel, G. and Hemmat, R.~A. and Pezeshki, M. and Huang, G. and Lepriol, R. and Lacoste-Julien, S. and Mitliagkas, I.},
  booktitle={AISTATS},
  year={2019}
}

@inproceedings{Bal18,
  title={The mechanics of n-player differentiable games},
  author={Balduzzi, D. and Racaniere, S. and Martens, J. and Foerster, J. and Tuyls, K. and Graepel, T.},
  booktitle={ICML},
  year={2018}
}

@inproceedings{Rak13,
  title={{Optimization, learning, and games with predictable sequences}},
  author={Rakhlin, S. and Sridharan, K.},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3066--3074},
  year={2013}
}