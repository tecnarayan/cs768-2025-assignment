\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bachlechner et~al.(2021)Bachlechner, Majumder, Mao, Cottrell, and
  McAuley]{bachlechner2021rezero}
Thomas Bachlechner, Bodhisattwa~Prasad Majumder, Henry Mao, Gary Cottrell, and
  Julian McAuley.
\newblock Rezero is all you need: Fast convergence at large depth.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1352--1361.
  PMLR, 2021.

\bibitem[Bai et~al.(2021)Bai, Zhang, Hou, Shang, Jin, Jiang, Liu, Lyu, and
  King]{Bai2021BinaryBERTPT}
Haoli Bai, Wei Zhang, Lu~Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
  Michael~R. Lyu, and Irwin King.
\newblock Binarybert: Pushing the limit of bert quantization.
\newblock \emph{ArXiv}, abs/2012.15701, 2021.

\bibitem[Blake et~al.(2023)Blake, Orr, and Luschi]{blake2023unit}
Charlie Blake, Douglas Orr, and Carlo Luschi.
\newblock Unit scaling: Out-of-the-box low-precision training.
\newblock \emph{arXiv preprint arXiv:2303.11257}, 2023.

\bibitem[Brock et~al.(2021)Brock, De, Smith, and Simonyan]{brock2021high}
Andy Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1059--1071. PMLR, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss,
  et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.
\newblock \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Cambier et~al.(2020)Cambier, Bhiwandiwalla, Gong, Elibol, Nekuii, and
  Tang]{cambier2020shiftsqueeze}
L{\'{e}}opold Cambier, Anahita Bhiwandiwalla, Ting Gong, Oguz~H. Elibol, Mehran
  Nekuii, and Hanlin Tang.
\newblock Shifted and squeezed 8-bit floating point format for low-precision
  training of deep neural networks.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=Bkxe2AVtPS}.

\bibitem[Chen et~al.(2023)Chen, Liang, Huang, Real, Wang, Liu, Pham, Dong,
  Luong, Hsieh, et~al.]{chen2023symbolic}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu
  Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et~al.
\newblock Symbolic discovery of optimization algorithms.
\newblock \emph{arXiv preprint arXiv:2302.06675}, 2023.

\bibitem[Chen et~al.(2021)Chen, Xie, and He]{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9640--9649, 2021.

\bibitem[Cherti et~al.(2022)Cherti, Beaumont, Wightman, Wortsman, Ilharco,
  Gordon, Schuhmann, Schmidt, and Jitsev]{cherti2022reproducible}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel
  Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock \emph{arXiv preprint arXiv:2212.07143}, 2022.

\bibitem[Cho et~al.(2021)Cho, Vahid, Adya, and Rastegari]{cho2021dkm}
Minsik Cho, Keivan~A Vahid, Saurabh Adya, and Mohammad Rastegari.
\newblock Dkm: Differentiable k-means clustering layer for neural network
  compression.
\newblock \emph{arXiv preprint arXiv:2108.12659}, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Cohen et~al.(2022)Cohen, Ghorbani, Krishnan, Agarwal, Medapati,
  Badura, Suo, Cardoze, Nado, Dahl, et~al.]{cohen2022adaptive}
Jeremy~M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh
  Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George~E
  Dahl, et~al.
\newblock Adaptive gradient methods at the edge of stability.
\newblock \emph{arXiv preprint arXiv:2207.14484}, 2022.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean{-}Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In Corinna Cortes, Neil~D. Lawrence, Daniel~D. Lee, Masashi Sugiyama,
  and Roman Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 28: Annual Conference on Neural Information Processing Systems 2015,
  December 7-12, 2015, Montreal, Quebec, Canada}, pages 3123--3131, 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract.html}.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
  Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
  Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock \emph{arXiv preprint arXiv:2302.05442}, 2023.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2009.
\newblock \url{https://ieeexplore.ieee.org/document/5206848}.

\bibitem[Dettmers and Zettlemoyer(2022)]{dettmers2022case}
Tim Dettmers and Luke Zettlemoyer.
\newblock The case for 4-bit precision: k-bit inference scaling laws.
\newblock \emph{arXiv preprint arXiv:2212.09720}, 2022.

\bibitem[Dettmers et~al.(2022{\natexlab{a}})Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022{\natexlab{a}}.

\bibitem[Dettmers et~al.(2022{\natexlab{b}})Dettmers, Lewis, Shleifer, and
  Zettlemoyer]{dettmers2022optimizers}
Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization.
\newblock \emph{9th International Conference on Learning Representations,
  ICLR}, 2022{\natexlab{b}}.

\bibitem[Dinan et~al.(2023)Dinan, Yaida, and Zhang]{dinan2023effective}
Emily Dinan, Sho Yaida, and Susan Zhang.
\newblock Effective theory of transformers at initialization.
\newblock \emph{arXiv preprint arXiv:2304.02034}, 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.
\newblock \url{https://arxiv.org/abs/2010.11929}.

\bibitem[Drumond et~al.(2018)Drumond, Lin, Jaggi, and
  Falsafi]{drumond2018hybridblock}
Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi.
\newblock Training dnns with hybrid block floating point.
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman,
  Nicol{\`{o}} Cesa{-}Bianchi, and Roman Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 31: Annual Conference on Neural
  Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
  Montr{\'{e}}al, Canada}, pages 451--461, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/hash/6a9aeddfc689c1d0e3b9ccc3ab651bc5-Abstract.html}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Fan et~al.(2020)Fan, Stock, Graham, Grave, Gribonval, Jegou, and
  Joulin]{fan2020training}
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R{\'e}mi Gribonval,
  Herve Jegou, and Armand Joulin.
\newblock Training with quantization noise for extreme model compression.
\newblock \emph{arXiv preprint arXiv:2004.07320}, 2020.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Gilmer et~al.()Gilmer, Schioppa, and Cohen]{gilmerspikes}
Justin Gilmer, Andrea Schioppa, and Jeremy Cohen.
\newblock Intriguing properties of transformer training instabilities.
\newblock To appear.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.
\newblock \url{https://arxiv.org/abs/1512.03385}.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini,
  Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and
  Schmidt]{openclip}
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas
  Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John
  Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
\newblock Openclip, July 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5143773}.
\newblock If you use this software, please cite it as below.

\bibitem[Jacob et~al.(2017)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{jacob2017quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference. arxiv e-prints, art.
\newblock \emph{arXiv preprint arXiv:1712.05877}, 2017.

\bibitem[Khudia et~al.(2021)Khudia, Huang, Basu, Deng, Liu, Park, and
  Smelyanskiy]{fbgemm}
Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park,
  and Mikhail Smelyanskiy.
\newblock Fbgemm: Enabling high-performance low-precision deep learning
  inference.
\newblock \emph{arXiv preprint arXiv:2101.05615}, 2021.

\bibitem[Kim et~al.(2021)Kim, Gholami, Yao, Mahoney, and Keutzer]{kim2021bert}
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael~W Mahoney, and Kurt Keutzer.
\newblock I-bert: Integer-only bert quantization.
\newblock In \emph{International conference on machine learning}, pages
  5506--5518. PMLR, 2021.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2014.
\newblock \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Kumar et~al.(2023)Kumar, Dehghani, and Houlsby]{kumar2023dual}
Manoj Kumar, Mostafa Dehghani, and Neil Houlsby.
\newblock Dual patchnorm.
\newblock \emph{arXiv preprint arXiv:2302.01327}, 2023.

\bibitem[Li et~al.(2022)Li, Fan, Hu, Feichtenhofer, and He]{li2022scaling}
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
\newblock Scaling language-image pre-training via masking.
\newblock \emph{arXiv preprint arXiv:2212.00794}, 2022.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.
\newblock \url{https://arxiv.org/abs/1608.03983}.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.
\newblock \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Mellempudi et~al.(2019)Mellempudi, Srinivasan, Das, and
  Kaul]{mellempudi2019bit8}
Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul.
\newblock Mixed precision training with 8-bit floating point.
\newblock \emph{CoRR}, abs/1905.12334, 2019.
\newblock URL \url{http://arxiv.org/abs/1905.12334}.

\bibitem[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh,
  et~al.]{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, et~al.
\newblock Mixed precision training.
\newblock \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem[Micikevicius et~al.(2022)Micikevicius, Stosic, Burgess, Cornea, Dubey,
  Grisenthwaite, Ha, Heinecke, Judd, Kamalu, et~al.]{micikevicius2022fp8}
Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey,
  Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John
  Kamalu, et~al.
\newblock Fp8 formats for deep learning.
\newblock \emph{arXiv preprint arXiv:2209.05433}, 2022.

\bibitem[Molybog et~al.(2023)Molybog, Albert, Chen, DeVito, Esiobu, Goyal,
  Koura, Narang, Poulton, Silva, et~al.]{molybog2023theory}
Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman
  Goyal, Punit~Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et~al.
\newblock A theory on adam instability in large-scale machine learning.
\newblock \emph{arXiv preprint arXiv:2304.09871}, 2023.

\bibitem[Park et~al.(2022)Park, Park, Kwon, Kim, Lee, and Lee]{park2022nuqmm}
Gunho Park, Baeseong Park, Se~Jung Kwon, Byeongwook Kim, Youngjoo Lee, and
  Dongsoo Lee.
\newblock nuqmm: Quantized matmul for efficient inference of large-scale
  generative language models.
\newblock \emph{arXiv preprint arXiv:2206.09557}, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.
\newblock \url{https://arxiv.org/abs/1912.01703}.

\bibitem[Pham et~al.(2021)Pham, Dai, Ghiasi, Liu, Yu, Luong, Tan, and
  Le]{pham2021scaling}
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams~Wei Yu, Minh-Thang
  Luong, Mingxing Tan, and Quoc~V. Le.
\newblock Combined scaling for zero-shot transfer learning, 2021.
\newblock \url{https://arxiv.org/abs/2111.10050}.

\bibitem[Qin et~al.(2020)Qin, Gong, Liu, Bai, Song, and
  Sebe]{qin2020survey1bit}
Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu
  Sebe.
\newblock Binary neural networks: {A} survey.
\newblock \emph{CoRR}, abs/2004.03333, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.03333}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.
\newblock \url{https://arxiv.org/abs/2103.00020}.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10684--10695, 2022.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock \emph{arXiv preprint arXiv:2210.08402}, 2022.

\bibitem[Shazeer and Stern(2018)]{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{International Conference on Machine Learning}, pages
  4596--4604. PMLR, 2018.

\bibitem[Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer]{shen2020q}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer.
\newblock Q-bert: Hessian based ultra low precision quantization of bert.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8815--8821, 2020.

\bibitem[Sun et~al.(2019)Sun, Choi, Chen, Wang, Venkataramani, Srinivasan, Cui,
  Zhang, and Gopalakrishnan]{sun2019hybrid8bit}
Xiao Sun, Jungwook Choi, Chia{-}Yu Chen, Naigang Wang, Swagath Venkataramani,
  Vijayalakshmi Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash
  Gopalakrishnan.
\newblock Hybrid 8-bit floating point {(HFP8)} training and inference for deep
  neural networks.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pages 4901--4910, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/65fc9fb4897a89789352e211ca2d398f-Abstract.html}.

\bibitem[Tillet et~al.(2019)Tillet, Kung, and Cox]{tillet2019triton}
Philippe Tillet, Hsiang-Tsung Kung, and David Cox.
\newblock Triton: an intermediate language and compiler for tiled neural
  network computations.
\newblock In \emph{Proceedings of the 3rd ACM SIGPLAN International Workshop on
  Machine Learning and Programming Languages}, pages 10--19, 2019.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Sablayrolles, Synnaeve, and
  J{\'e}gou]{touvron2021going}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 32--42, 2021.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Choi, Brand, Chen, and
  Gopalakrishnan]{NEURIPS2018_335d3d1c}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
  Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/335d3d1cd7ef05ec77714a215134914c-Paper.pdf}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Choi, Brand, Chen, and
  Gopalakrishnan]{wang2018training8bit}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia{-}Yu Chen, and Kailash
  Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman,
  Nicol{\`{o}} Cesa{-}Bianchi, and Roman Garnett, editors, \emph{Advances in
  Neural Information Processing Systems 31: Annual Conference on Neural
  Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
  Montr{\'{e}}al, Canada}, pages 7686--7695, 2018{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/hash/335d3d1cd7ef05ec77714a215134914c-Abstract.html}.

\bibitem[Wang and Kanwar(2019)]{wang2019bfloat16}
Shibo Wang and Pankaj Kanwar.
\newblock Bfloat16: The secret to high performance on cloud tpus.
\newblock 2019.
\newblock
  \url{https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus}.

\bibitem[Wortsman(2023)]{giantclip}
Mitchell Wortsman.
\newblock Reaching 80\% accuracy with openclip, 2023.
\newblock \url{https://laion.ai/blog/giant-openclip/}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael
  Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,
  Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock In \emph{International Conference on Machine Learning}, pages
  23965--23998. PMLR, 2022.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and
  Han]{xiao2022smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2022.

\bibitem[Yao et~al.(2022)Yao, Aminabadi, Zhang, Wu, Li, and
  He]{yao2022zeroquant}
Zhewei Yao, Reza~Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
  Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock \emph{arXiv preprint arXiv:2206.01861}, 2022.

\bibitem[Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia,
  et~al.]{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock \emph{arXiv preprint arXiv:2210.02414}, 2022.

\bibitem[Zhai et~al.(2023{\natexlab{a}})Zhai, Likhomanenko, Littwin, Busbridge,
  Ramapuram, Zhang, Gu, and Susskind]{zhai2023stabilizing}
Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason
  Ramapuram, Yizhe Zhang, Jiatao Gu, and Josh Susskind.
\newblock Stabilizing transformer training by preventing attention entropy
  collapse.
\newblock \emph{arXiv preprint arXiv:2303.06296}, 2023{\natexlab{a}}.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2021scaling}
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
\newblock Scaling vision transformers, 2021.
\newblock \url{https://arxiv.org/abs/2106.04560}.

\bibitem[Zhai et~al.(2023{\natexlab{b}})Zhai, Mustafa, Kolesnikov, and
  Beyer]{zhai2023sigmoid}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock \emph{arXiv preprint arXiv:2303.15343}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019.

\bibitem[Zhang(2023)]{optvideo}
Susan Zhang.
\newblock Open pretrained transformers lecture, 2023.
\newblock \url{https://www.youtube.com/watch?v=p9IxoSkvZ-M}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Hou, Yin, Shang, Chen, Jiang, and
  Liu]{Zhang2020TernaryBERTDU}
Wei Zhang, Lu~Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu.
\newblock Ternarybert: Distillation-aware ultra-low bit bert.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Zhao et~al.(2021{\natexlab{a}})Zhao, Hua, Shen, Lou, and
  Jin]{zhao2021automatic}
Changsheng Zhao, Ting Hua, Yilin Shen, Qian Lou, and Hongxia Jin.
\newblock Automatic mixed-precision quantization search of bert.
\newblock \emph{arXiv preprint arXiv:2112.14938}, 2021{\natexlab{a}}.

\bibitem[Zhao et~al.(2021{\natexlab{b}})Zhao, Huang, Pan, Li, Zhang, Gu, and
  Xu]{zhao2021distribution}
Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and
  Yinghui Xu.
\newblock Distribution adaptive int8 quantization for training cnns.
\newblock In \emph{Proceedings of the Thirty-Fifth AAAI Conference on
  Artificial Intelligence}, 2021{\natexlab{b}}.

\bibitem[Zhu et~al.(2017)Zhu, Han, Mao, and Dally]{zhu2017ternary}
Chenzhuo Zhu, Song Han, Huizi Mao, and William~J. Dally.
\newblock Trained ternary quantization.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=S1\_pAu9xl}.

\bibitem[Zhu et~al.(2020)Zhu, Gong, Yu, Liu, Wang, Li, Yang, and
  Yan]{zhu2020towards}
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li,
  Xiuqi Yang, and Junjie Yan.
\newblock Towards unified int8 training for convolutional neural network.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 1969--1979, 2020.

\end{thebibliography}
