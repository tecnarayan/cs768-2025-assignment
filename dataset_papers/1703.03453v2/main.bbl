\begin{thebibliography}{19}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bastani(2014)]{Bastani2014}
M.~Bastani.
\newblock Model-free intelligent diabetes management using machine learning.
\newblock Master's thesis, Department of Computing Science, University of
  Alberta, 2014.

\bibitem[Brunskill and Li(2014)]{brunskill2014pac}
Emma Brunskill and Lihong Li.
\newblock Pac-inspired option discovery in lifelong reinforcement learning.
\newblock In \emph{ICML}, pages 316--324, 2014.

\bibitem[Dietterich(2000)]{dietterich2000hierarchical}
Thomas~G Dietterich.
\newblock Hierarchical reinforcement learning with the maxq value function
  decomposition.
\newblock \emph{J. Artif. Intell. Res.(JAIR)}, 13:\penalty0 227--303, 2000.

\bibitem[Dud\'{i}k et~al.(2011)Dud\'{i}k, Langford, and Li]{Dudik2011}
M.~Dud\'{i}k, J.~Langford, and L.~Li.
\newblock Doubly robust policy evaluation and learning.
\newblock In \emph{Proceedings of the Twenty-Eighth International Conference on
  Machine Learning}, pages 1097--1104, 2011.

\bibitem[Hallak et~al.(2015{\natexlab{a}})Hallak, Schnitzler, Mann, and
  Mannor]{hallak2015off}
Assaf Hallak, Fran{\c{c}}ois Schnitzler, Timothy~Arthur Mann, and Shie Mannor.
\newblock Off-policy model-based learning under unknown factored dynamics.
\newblock In \emph{ICML}, pages 711--719, 2015{\natexlab{a}}.

\bibitem[Hallak et~al.(2015{\natexlab{b}})Hallak, Tamar, Munos, and
  Mannor]{hallak2015generalized}
Assaf Hallak, Aviv Tamar, R{\'e}mi Munos, and Shie Mannor.
\newblock Generalized emphatic temporal difference learning: Bias-variance
  analysis.
\newblock \emph{arXiv preprint arXiv:1509.05172}, 2015{\natexlab{b}}.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pages 652--661, 2016.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesvari]{pmlr-v38-li15b}
Lihong Li, Remi Munos, and Csaba Szepesvari.
\newblock {Toward Minimax Off-policy Value Estimation}.
\newblock In Guy Lebanon and S.~V.~N. Vishwanathan, editors, \emph{Proceedings
  of the Eighteenth International Conference on Artificial Intelligence and
  Statistics}, volume~38 of \emph{Proceedings of Machine Learning Research},
  pages 608--616, San Diego, California, USA, 09--12 May 2015. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v38/li15b.html}.

\bibitem[Mankowitz et~al.(2014)Mankowitz, Mann, and Mannor]{mankowitz2014time}
Daniel~J Mankowitz, Timothy~A Mann, and Shie Mannor.
\newblock Time regularized interrupting options.
\newblock In \emph{Internation Conference on Machine Learning}, 2014.

\bibitem[Mann and Mannor(2013)]{mann2013advantage}
Timothy~A Mann and Shie Mannor.
\newblock The advantage of planning with options.
\newblock \emph{RLDM 2013}, page~9, 2013.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
R{\'e}mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1046--1054, 2016.

\bibitem[Precup(2000)]{precup2000eligibility}
Doina Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series},
  page~80, 2000.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
Doina Precup, Richard~S Sutton, and Sanjoy Dasgupta.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock In \emph{ICML}, pages 417--424, 2001.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between {MDPs} and semi-{MDPs}mdps: A framework for temporal
  abstraction in reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Theocharous et~al.(2015)Theocharous, Thomas, and
  Ghavamzadeh]{Theocharous2015}
G.~Theocharous, P.~S. Thomas, and M.~Ghavamzadeh.
\newblock Personalized ad recommendation systems for life-time value
  optimization with guarantees.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence}, 2015.

\bibitem[Thomas and Brunskill(2016)]{Thomas2016}
P.~S. Thomas and E.~Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Thomas and Brunskill(2017)]{Thomas2017}
P.~S. Thomas and E.~Brunskill.
\newblock Importance sampling with unequal support.
\newblock \emph{AAAI}, 2017.

\bibitem[Thomas et~al.(2015{\natexlab{a}})Thomas, Theocharous, and
  Ghavamzadeh]{Thomas2015}
P.~S. Thomas, G.~Theocharous, and M.~Ghavamzadeh.
\newblock High confidence off-policy evaluation.
\newblock In \emph{Proceedings of the Twenty-Ninth Conference on Artificial
  Intelligence}, 2015{\natexlab{a}}.

\bibitem[Thomas et~al.(2015{\natexlab{b}})Thomas, Niekum, Theocharous, and
  Konidaris]{NIPS2015_5807}
Philip~S Thomas, Scott Niekum, Georgios Theocharous, and George Konidaris.
\newblock Policy evaluation using the {$\Omega$}-return.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 28}, pages
  334--342. Curran Associates, Inc., 2015{\natexlab{b}}.
\newblock URL
  \url{http://papers.nips.cc/paper/5807-policy-evaluation-using-the-return.pdf}.

\end{thebibliography}
