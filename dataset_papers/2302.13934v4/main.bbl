\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajay et~al.(2022)Ajay, Gupta, Ghosh, Levine, and
  Agrawal]{ajay2022distributionally}
Anurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, and Pulkit Agrawal.
\newblock Distributionally adaptive meta reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv:1907.02893}, 2019.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 2002.

\bibitem[Bartlett et~al.(2005)Bartlett, Bousquet, and
  Mendelson]{bartlett2005local}
Peter~L Bartlett, Olivier Bousquet, and Shahar Mendelson.
\newblock Local rademacher complexities.
\newblock \emph{The Annals of Statistics}, 2005.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Pascal Massart.
\newblock \emph{Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford University Press, 2013.

\bibitem[Bousquet(2002)]{bousquet2002bennett}
Olivier Bousquet.
\newblock A {B}ennett concentration inequality and its application to suprema
  of empirical processes.
\newblock \emph{Comptes Rendus Mathematique}, 2002.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2002.

\bibitem[Chernozhukov et~al.(2017)Chernozhukov, Chetverikov, Demirer, Duflo,
  Hansen, and Newey]{chernozhukov2017double}
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian
  Hansen, and Whitney Newey.
\newblock Double/debiased/{N}eyman machine learning of treatment effects.
\newblock \emph{American Economic Review}, 2017.

\bibitem[Dong and Ma(2023)]{dong2023first}
Kefan Dong and Tengyu Ma.
\newblock First steps toward understanding the extrapolation of nonlinear
  models to unseen domains.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=7wrq3vHcMM}.

\bibitem[Dudley(1967)]{dudley1967sizes}
Richard~M Dudley.
\newblock The sizes of compact subsets of {H}ilbert space and continuity of
  {G}aussian processes.
\newblock \emph{Journal of Functional Analysis}, 1967.

\bibitem[Foster and Syrgkanis(2019)]{foster2019orthogonal}
Dylan~J Foster and Vasilis Syrgkanis.
\newblock Orthogonal statistical learning.
\newblock \emph{arXiv:1901.09036}, 2019.

\bibitem[Gupta et~al.(2018)Gupta, Mendonca, Liu, Abbeel, and
  Levine]{gupta2018meta}
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan,
  Kumar, Zhu, Gupta, Abbeel, and Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,
  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey
  Levine.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv:1812.05905}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2016.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv:1704.04861}, 2017.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2017.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv:1412.6980}, 2014.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, Lee, David, Stavnass, Guo, Earnshaw, Haque, Beery,
  Leskovec, Kundaje, Pierson, Levine, Finn, and Liang]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, Tony Lee, Etienne David, Ian Stavnass, Wei Guo, Berton Earnshaw,
  Imran Haque, Sara~M Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson,
  Sergey Levine, Chelsea Finn, and Percy Liang.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Lei et~al.(2021)Lei, Hu, and Lee]{lei2021near}
Qi~Lei, Wei Hu, and Jason Lee.
\newblock Near-optimal linear regression under distribution shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  6164--6174. PMLR, 2021.

\bibitem[Liang et~al.(2015)Liang, Rakhlin, and Sridharan]{liang2015learning}
Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan.
\newblock Learning with square loss: Localization through offset rademacher
  complexity.
\newblock In \emph{Conference on Learning Theory}, 2015.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{IEEE International Conference on Computer Vision}, 2015.

\bibitem[Ma et~al.(2022)Ma, Pathak, and Wainwright]{ma2022optimally}
Cong Ma, Reese Pathak, and Martin~J Wainwright.
\newblock Optimally tackling covariate shift in {RKHS}-based nonparametric
  regression.
\newblock \emph{arXiv:2205.02986}, 2022.

\bibitem[Mackey et~al.(2018)Mackey, Syrgkanis, and Zadik]{mackey2018orthogonal}
Lester Mackey, Vasilis Syrgkanis, and Ilias Zadik.
\newblock Orthogonal machine learning: Power and limitations.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Mendelson(2015)]{mendelson2015learning}
Shahar Mendelson.
\newblock Learning without concentration.
\newblock \emph{Journal of the ACM}, 2015.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Pathak et~al.(2022)Pathak, Ma, and Wainwright]{pathak2022new}
Reese Pathak, Cong Ma, and Martin Wainwright.
\newblock A new similarity measure for covariate shift with applications to
  nonparametric regression.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Polyanskiy and Wu(2022)]{polyanskiy2022information}
Yury Polyanskiy and Yihong Wu.
\newblock Information theory: From coding to learning, 2022.

\bibitem[Rahimian and Mehrotra(2019)]{rahimian19dro}
Hamed Rahimian and Sanjay Mehrotra.
\newblock Distributionally robust optimization: A review.
\newblock \emph{arxiv:1908.05659}, 2019.

\bibitem[Rakhlin(2022)]{rakhlinnotes}
Alexander Rakhlin.
\newblock {IDS}.160 â€“ {M}athematical {S}tatistics: A non-asymptotic approach,
  2022.
\newblock URL
  \url{http://www.mit.edu/~rakhlin/courses/mathstat/rakhlin_mathstat_sp22.pdf}.

\bibitem[Robinson(1988)]{robinson1988root}
Peter~M Robinson.
\newblock Root-n-consistent semiparametric regression.
\newblock \emph{Econometrica: Journal of the Econometric Society}, 1988.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Santurkar et~al.(2020)Santurkar, Tsipras, and
  Madry]{santurkar2020breeds}
Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry.
\newblock Breeds: Benchmarks for subpopulation shift.
\newblock \emph{arXiv:2008.04859}, 2020.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  Madry]{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{sinha18dro}
Aman Sinha, Hongseok Namkoong, and John~C. Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sun and Saenko(2016)]{sun2016deep}
Baochen Sun and Kate Saenko.
\newblock Deep coral: Correlation alignment for deep domain adaptation.
\newblock In \emph{European Conference on Computer Vision}, 2016.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[van~der Vaart and Wellner(1996)]{van1996weak}
Aad~W van~der Vaart and Jon~A Wellner.
\newblock \emph{Weak convergence}.
\newblock Springer, 1996.

\bibitem[Vapnik(2006)]{vapnik2006estimation}
Vladimir Vapnik.
\newblock \emph{Estimation of dependences based on empirical data}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}.
\newblock Cambridge University Press, 2018.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint}.
\newblock Cambridge University Press, 2019.

\bibitem[Xiao et~al.(2020)Xiao, Engstrom, Ilyas, and Madry]{xiao2020noise}
Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry.
\newblock Noise or signal: The role of image backgrounds in object recognition.
\newblock \emph{arXiv preprint arXiv:2006.09994}, 2020.

\bibitem[Xie and Jiang(2020)]{xie2020q}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pages
  550--559. PMLR, 2020.

\bibitem[Xie et~al.(2022)Xie, Foster, Bai, Jiang, and Kakade]{xie2022role}
Tengyang Xie, Dylan~J Foster, Yu~Bai, Nan Jiang, and Sham~M Kakade.
\newblock The role of coverage in online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2210.04157}, 2022.

\bibitem[Zhou et~al.(2022)Zhou, Liu, Qiao, Xiang, and Loy]{zhou2022domain}
Kaiyang Zhou, Ziwei Liu, Yu~Qiao, Tao Xiang, and Chen~Change Loy.
\newblock Domain generalization: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\end{thebibliography}
