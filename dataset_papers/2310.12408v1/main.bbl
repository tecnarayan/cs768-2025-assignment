\begin{thebibliography}{124}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe et~al.(2022{\natexlab{a}})Abbe, Adsera, and Misiakiewicz]{abbe2022merged}
Emmanuel Abbe, Enric~Boix Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks.
\newblock In \emph{Conference on Learning Theory}. PMLR, 2022{\natexlab{a}}.

\bibitem[Abbe et~al.(2022{\natexlab{b}})Abbe, Bengio, Cornacchia, Kleinberg, Lotfi, Raghu, and Zhang]{abbe2022learning}
Emmanuel Abbe, Samy Bengio, Elisabetta Cornacchia, Jon Kleinberg, Aryo Lotfi, Maithra Raghu, and Chiyuan Zhang.
\newblock Learning to reason with neural networks: Generalization, unseen data and boolean measures.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{b}}.

\bibitem[Akiyama and Suzuki(2021)]{akiyama2021learnability}
Shunta Akiyama and Taiji Suzuki.
\newblock On learnability via gradient method for two-layer relu neural networks in teacher-student setting.
\newblock In \emph{International Conference on Machine Learning}, pages 152--162. PMLR, 2021.

\bibitem[Akiyama and Suzuki(2023)]{akiyama2022excess}
Shunta Akiyama and Taiji Suzuki.
\newblock Excess risk of two-layer re{LU} neural networks in teacher-student settings and its superiority to kernel methods.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Allen-Zhu and Li(2019)]{allen2019canb}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Allen-Zhu and Li(2020)]{allen2020backward}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Backward feature correction: How deep learning performs deep learning.
\newblock \emph{arXiv preprint arXiv:2001.04413}, 2020.

\bibitem[Allen-Zhu and Li(2022)]{allen2020feature}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Feature purification: How adversarial training performs robust deep learning.
\newblock In \emph{2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)}, pages 977--988. IEEE, 2022.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and Liang]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks, going beyond two layers.
\newblock In \emph{Advances in neural information processing systems}, 2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and Song]{allenzhu2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2018)Arora, Cohen, Golowich, and Hu]{arora2018convergence}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 322--332. PMLR, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{arXiv preprint arXiv:1904.11955}, 2019{\natexlab{b}}.

\bibitem[Ba et~al.(2022)Ba, Erdogdu, Suzuki, Wang, Wu, and Yang]{ba2022high}
Jimmy Ba, Murat~A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang.
\newblock High-dimensional asymptotics of feature learning: How one gradient step improves the representation.
\newblock \emph{arXiv preprint arXiv:2205.01445}, 2022.

\bibitem[Bai and Lee(2019)]{bai2020linearization}
Yu~Bai and Jason~D Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of wide neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Barak et~al.(2022{\natexlab{a}})Barak, Edelman, Goel, Kakade, Malach, and Zhang]{barak2022hidden}
Boaz Barak, Benjamin~L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock \emph{arXiv preprint arXiv:2207.08799}, 2022{\natexlab{a}}.

\bibitem[Barak et~al.(2022{\natexlab{b}})Barak, Edelman, Goel, Kakade, Zhang, et~al.]{barakhidden}
Boaz Barak, Benjamin~L Edelman, Surbhi Goel, Sham~M Kakade, Cyril Zhang, et~al.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{b}}.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and Tsigler]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2020.

\bibitem[Bietti et~al.(2022)Bietti, Bruna, Sanford, and Song]{bietti2022learning}
Alberto Bietti, Joan Bruna, Clayton Sanford, and Min~Jae Song.
\newblock Learning single-index models with shallow neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Blum and Rivest(1989)]{blum1989training}
Avrim Blum and Ronald~L Rivest.
\newblock Training a 3-node neural network is np-complete.
\newblock In \emph{Advances in neural information processing systems}, pages 494--501, 1989.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Cao et~al.(2020)Cao, Fang, Wu, Zhou, and Gu]{cao2020understanding}
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu.
\newblock Towards understanding the spectral bias of deep learning, 2020.

\bibitem[Cao et~al.(2022)Cao, Chen, Belkin, and Gu]{cao2022benign}
Yuan Cao, Zixiang Chen, Misha Belkin, and Quanquan Gu.
\newblock Benign overfitting in two-layer convolutional neural networks.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Chatterji et~al.(2021)Chatterji, Long, and Bartlett]{chatterji2021does}
Niladri~S Chatterji, Philip~M Long, and Peter~L Bartlett.
\newblock When does gradient descent with logistic loss find interpolating two-layer networks?
\newblock \emph{Journal of Machine Learning Research}, pages 1--48, 2021.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Jiang, Liao, and Zhao]{chen2019efficient}
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao.
\newblock Efficient approximation of deep relu networks for functions on low dimensional manifolds.
\newblock \emph{Advances in neural information processing systems}, 32:\penalty0 8174--8184, 2019{\natexlab{a}}.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Jiang, Liao, and Zhao]{chen2019nonparametric}
Minshuo Chen, Haoming Jiang, Wenjing Liao, and Tuo Zhao.
\newblock Nonparametric regression on low-dimensional manifolds using deep relu networks: Function approximation and statistical recovery.
\newblock \emph{arXiv preprint arXiv:1908.01842}, 2019{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Bai, Lee, Zhao, Wang, Xiong, and Socher]{chen2020towards}
Minshuo Chen, Yu~Bai, Jason~D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher.
\newblock Towards understanding hierarchical learning: Benefits of neural representations.
\newblock \emph{arXiv preprint arXiv:2006.13436}, 2020.

\bibitem[Chen et~al.(2022)Chen, Vanden-Eijnden, and Bruna]{chen2022feature}
Zhengdao Chen, Eric Vanden-Eijnden, and Joan Bruna.
\newblock On feature learning in neural networks with global convergence guarantees.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Chizat and Bach(2018{\natexlab{a}})]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018{\natexlab{a}}.

\bibitem[Chizat and Bach(2018{\natexlab{b}})]{chizat2018note}
Lenaic Chizat and Francis Bach.
\newblock A note on lazy training in supervised differentiable programming.
\newblock \emph{arXiv preprint arXiv:1812.07956}, 2018{\natexlab{b}}.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}. PMLR, 2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2020lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Damian et~al.(2022)Damian, Lee, and Soltanolkotabi]{damian2022neural}
Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi.
\newblock Neural networks can learn representations with gradient descent.
\newblock In \emph{Conference on Learning Theory}. PMLR, 2022.

\bibitem[Daniely and Malach(2020)]{daniely2020learning}
Amit Daniely and Eran Malach.
\newblock Learning parities with neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Daniely and Vardi(2020)]{daniely2020hardness}
Amit Daniely and Gal Vardi.
\newblock Hardness of learning neural networks with natural weights.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 930--940, 2020.

\bibitem[Daniely et~al.(2023)Daniely, Srebro, and Vardi]{daniely2023efficiently}
Amit Daniely, Nathan Srebro, and Gal Vardi.
\newblock Efficiently learning neural networks: What assumptions may suffice?
\newblock \emph{arXiv preprint arXiv:2302.07426}, 2023.

\bibitem[Ding et~al.(2022)Ding, Chen, Li, and Wright]{ding2022overparameterization}
Zhiyan Ding, Shi Chen, Qin Li, and Stephen~J Wright.
\newblock Overparameterization of deep resnet: zero loss and mean-field analysis.
\newblock \emph{The Journal of Machine Learning Research}, 2022.

\bibitem[Dou and Liang(2020)]{Dou_2020}
Xialiang Dou and Tengyuan Liang.
\newblock Training neural networks as learning data-adaptive kernels: Provable representation and approximation benefits.
\newblock \emph{Journal of the American Statistical Association}, 2020.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Feng and Tu(2021)]{feng2021phases}
Yu~Feng and Yuhai Tu.
\newblock Phases of learning dynamics in artificial neural networks: in the absence or presence of mislabeled data.
\newblock \emph{Machine Learning: Science and Technology}, 2021.

\bibitem[Frankle and Carbin(2018)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and Carbin]{frankle2019stabilizing}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Stabilizing the lottery ticket hypothesis.
\newblock \emph{arXiv preprint arXiv:1903.01611}, 2019.

\bibitem[Frei and Gu(2021)]{frei2021proxy}
Spencer Frei and Quanquan Gu.
\newblock Proxy convexity: A unified framework for the analysis of neural networks trained by gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Frei et~al.(2021)Frei, Cao, and Gu]{frei2021provable}
Spencer Frei, Yuan Cao, and Quanquan Gu.
\newblock Provable generalization of sgd-trained neural networks of any width in the presence of adversarial label noise.
\newblock \emph{arXiv preprint arXiv:2101.01152}, 2021.

\bibitem[Frei et~al.(2022{\natexlab{a}})Frei, Chatterji, and Bartlett]{frei2022random}
Spencer Frei, Niladri~S Chatterji, and Peter~L Bartlett.
\newblock Random feature amplification: Feature learning and generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:2202.07626}, 2022{\natexlab{a}}.

\bibitem[Frei et~al.(2022{\natexlab{b}})Frei, Vardi, Bartlett, Srebro, and Hu]{frei2022implicit}
Spencer Frei, Gal Vardi, Peter~L Bartlett, Nathan Srebro, and Wei Hu.
\newblock Implicit bias in leaky relu networks trained on high-dimensional data.
\newblock \emph{arXiv preprint arXiv:2210.07082}, 2022{\natexlab{b}}.

\bibitem[Frei et~al.(2023{\natexlab{a}})Frei, Vardi, Bartlett, and Srebro]{frei2023benign}
Spencer Frei, Gal Vardi, Peter~L Bartlett, and Nathan Srebro.
\newblock Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization.
\newblock \emph{arXiv preprint arXiv:2303.01462}, 2023{\natexlab{a}}.

\bibitem[Frei et~al.(2023{\natexlab{b}})Frei, Vardi, Bartlett, and Srebro]{frei2023double}
Spencer Frei, Gal Vardi, Peter~L Bartlett, and Nathan Srebro.
\newblock The double-edged sword of implicit bias: Generalization vs. robustness in relu networks.
\newblock \emph{arXiv preprint arXiv:2303.01456}, 2023{\natexlab{b}}.

\bibitem[Geiger et~al.(2020)Geiger, Spigler, Jacot, and Wyart]{Geiger_2020}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, page 113301, 2020.

\bibitem[Geiger et~al.(2021)Geiger, Petrini, and Wyart]{geiger2021landscape}
Mario Geiger, Leonardo Petrini, and Matthieu Wyart.
\newblock Landscape and training regimes in deep learning.
\newblock \emph{Physics Reports}, 924:\penalty0 1--18, 2021.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and Montanari]{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural networks.
\newblock \emph{arXiv preprint arXiv:1906.08899}, 2019.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and Montanari]{ghorbani2020neural}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Girshick et~al.(2014)Girshick, Donahue, Darrell, and Malik]{girshick2014rich}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic segmentation.
\newblock In \emph{Computer Vision and Pattern Recognition}, 2014.

\bibitem[Goldt et~al.(2019)Goldt, Advani, Saxe, Krzakala, and Zdeborov{\'a}]{goldt2019dynamics}
Sebastian Goldt, Madhu Advani, Andrew~M Saxe, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages 1832--1841. PMLR, 2018.

\bibitem[Hanin and Nica(2019)]{hanin2019finite}
Boris Hanin and Mihai Nica.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Huang and Yau(2020)]{huang2020dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In \emph{International conference on machine learning}, pages 4542--4551. PMLR, 2020.

\bibitem[Jacot(2023)]{jacot2023implicit}
Arthur Jacot.
\newblock Implicit bias of large depth networks: a notion of rank for nonlinear functions.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In \emph{Advances in neural information processing systems}, 2018.

\bibitem[Ji and Telgarsky(2019{\natexlab{a}})]{ji2019implicit}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pages 1772--1798. PMLR, 2019{\natexlab{a}}.

\bibitem[Ji and Telgarsky(2019{\natexlab{b}})]{ji2020polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow relu networks.
\newblock In \emph{International Conference on Learning Representations}, 2019{\natexlab{b}}.

\bibitem[Ji and Telgarsky(2020)]{ji2020directional}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 17176--17186, 2020.

\bibitem[Kamath et~al.(2020)Kamath, Montasser, and Srebro]{kamath2020approximate}
Pritish Kamath, Omar Montasser, and Nathan Srebro.
\newblock Approximate is good enough: Probabilistic variants of dimensional and margin complexity.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem[Kohler and Lucchi(2017)]{kohler2017sub}
Jonas~Moritz Kohler and Aurelien Lucchi.
\newblock Sub-sampled cubic regularization for non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2017.

\bibitem[Kornowski et~al.(2023)Kornowski, Yehudai, and Shamir]{kornowski2023tempered}
Guy Kornowski, Gilad Yehudai, and Ohad Shamir.
\newblock From tempered to benign overfitting in relu neural networks.
\newblock \emph{arXiv preprint arXiv:2305.15141}, 2023.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and Sohl-Dickstein]{lee2018deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein, and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under gradient descent.
\newblock \emph{Advances in neural information processing systems}, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and Sohl-Dickstein]{lee2020finite}
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15156--15172, 2020.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Li et~al.(2020)Li, Ma, and Zhang]{li2020learning}
Yuanzhi Li, Tengyu Ma, and Hongyang~R Zhang.
\newblock Learning over-parametrized two-layer neural networks beyond ntk.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Luo et~al.(2021)Luo, Xu, Ma, and Zhang]{luo2021phase}
Tao Luo, Zhi-Qin~John Xu, Zheng Ma, and Yaoyu Zhang.
\newblock Phase diagram for two-layer relu neural networks at infinite-width limit.
\newblock \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
\newblock Gradient descent on two-layer nets: Margin maximization and simplicity bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 12978--12991, 2021.

\bibitem[Malach et~al.(2021)Malach, Kamath, Abbe, and Srebro]{malach2021quantifying}
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro.
\newblock Quantifying the benefit of using differentiable learning over tangent kernels.
\newblock \emph{arXiv preprint arXiv:2103.01210}, 2021.

\bibitem[Manning et~al.(2020)Manning, Clark, Hewitt, Khandelwal, and Levy]{manning2020emergent}
Christopher~D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy.
\newblock Emergent linguistic structure in artificial neural networks trained by self-supervision.
\newblock \emph{Proceedings of the National Academy of Sciences}, pages 30046--30054, 2020.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and Ghahramani]{matthews2018gaussian}
Alexander G de~G Matthews, Mark Rowland, Jiri Hron, Richard~E Turner, and Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2018.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pages 2388--2464. PMLR, 2019.

\bibitem[Moniri et~al.(2023)Moniri, Lee, Hassani, and Dobriban]{moniri2023theory}
Behrad Moniri, Donghwan Lee, Hamed Hassani, and Edgar Dobriban.
\newblock A theory of non-linear feature learning with one gradient step in two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:2310.07891}, 2023.

\bibitem[Montanari and Zhong(2022)]{montanari2022interpolation}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: Memorization and generalization under lazy training.
\newblock \emph{The Annals of Statistics}, 2022.

\bibitem[Moroshko et~al.(2020)Moroshko, Woodworth, Gunasekar, Lee, Srebro, and Soudry]{moroshko2020implicit}
Edward Moroshko, Blake~E Woodworth, Suriya Gunasekar, Jason~D Lee, Nati Srebro, and Daniel Soudry.
\newblock Implicit bias in deep linear classification: Initialization scale vs training accuracy.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Mousavi-Hosseini et~al.(2022)Mousavi-Hosseini, Park, Girotti, Mitliagkas, and Erdogdu]{mousavi2022neural}
Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat~A Erdogdu.
\newblock Neural networks efficiently learn low-dimensional representations with sgd.
\newblock \emph{arXiv preprint arXiv:2209.14863}, 2022.

\bibitem[Nacson et~al.(2019{\natexlab{a}})Nacson, Gunasekar, Lee, Srebro, and Soudry]{nacson2019lexicographic}
Mor~Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry.
\newblock Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models.
\newblock In \emph{International Conference on Machine Learning}, pages 4683--4692. PMLR, 2019{\natexlab{a}}.

\bibitem[Nacson et~al.(2019{\natexlab{b}})Nacson, Lee, Gunasekar, Savarese, Srebro, and Soudry]{nacson2019convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique~Pamplona Savarese, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}. PMLR, 2019{\natexlab{b}}.

\bibitem[Nacson et~al.(2019{\natexlab{c}})Nacson, Srebro, and Soudry]{nacson2019stochastic}
Mor~Shpigel Nacson, Nathan Srebro, and Daniel Soudry.
\newblock Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pages 3051--3059. PMLR, 2019{\natexlab{c}}.

\bibitem[Nagarajan and Kolter(2019)]{nagarajan2019uniform}
Vaishnavh Nagarajan and J~Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Kalimeris, Yang, Edelman, Zhang, and Barak]{nakkiran2019sgd}
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin~L Edelman, Fred Zhang, and Boaz Barak.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock \emph{arXiv preprint arXiv:1905.11604}, 2019.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Behnam Neyshabur.
\newblock Implicit regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1709.01953}, 2017.

\bibitem[Novak et~al.(2019)Novak, Xiao, Lee, Bahri, Abolafia, Pennington, and Sohl-Dickstein]{novak2019bayesian}
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel~A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
\newblock Bayesian convolutional neural networks with many channels are gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[O'Donnell(2014)]{o2014analysis}
Ryan O'Donnell.
\newblock \emph{Analysis of boolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Oymak and Soltanolkotabi(2019)]{oymak2019overparameterized}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the shortest path?
\newblock In \emph{International Conference on Machine Learning}, pages 4951--4960. PMLR, 2019.

\bibitem[Oymak and Soltanolkotabi(2020)]{oymak2020toward}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, pages 84--105, 2020.

\bibitem[Oymak et~al.(2019)Oymak, Fabian, Li, and Soltanolkotabi]{oymak2019generalization}
Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi.
\newblock Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian.
\newblock \emph{arXiv preprint arXiv:1906.05392}, 2019.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, pages 24652--24663, 2020.

\bibitem[Radhakrishnan et~al.(2023)Radhakrishnan, Beaglehole, Pandit, and Belkin]{radhakrishnan2023mechanism}
Adityanarayanan Radhakrishnan, Daniel Beaglehole, Parthe Pandit, and Mikhail Belkin.
\newblock Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features, 2023.

\bibitem[Rahimi and Recht(2008)]{rahimi2007random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2008.

\bibitem[Refinetti et~al.(2021)Refinetti, Goldt, Krzakala, and Zdeborov]{refinetti2021classifying}
Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov.
\newblock Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed.
\newblock In \emph{International Conference on Machine Learning}, pages 8936--8947. PMLR, 2021.

\bibitem[Ren et~al.(2023)Ren, Zhou, and Ge]{ren2023depth}
Yunwei Ren, Mo~Zhou, and Rong Ge.
\newblock Depth separation with multilayer mean-field networks.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Safran et~al.(2019)Safran, Eldan, and Shamir]{safran2019depth}
Itay Safran, Ronen Eldan, and Ohad Shamir.
\newblock Depth separations in neural networks: what is actually being separated?
\newblock In \emph{Conference on Learning Theory}, pages 2664--2666. PMLR, 2019.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and Netrapalli]{shah2020pitfalls}
Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli.
\newblock The pitfalls of simplicity bias in neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Shalev-Shwartz et~al.(2017)Shalev-Shwartz, Shamir, and Shammah]{shalev2017failures}
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah.
\newblock Failures of gradient-based deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages 3067--3075. PMLR, 2017.

\bibitem[Shi et~al.(2022{\natexlab{a}})Shi, Ming, Fan, Sala, and Liang]{shi2022domain}
Zhenmei Shi, Yifei Ming, Ying Fan, Frederic Sala, and Yingyu Liang.
\newblock Domain generalization with nuclear norm regularization.
\newblock In \emph{NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications}, 2022{\natexlab{a}}.

\bibitem[Shi et~al.(2022{\natexlab{b}})Shi, Wei, and Liang]{feature}
Zhenmei Shi, Junyi Wei, and Yingyu Liang.
\newblock A theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features.
\newblock In \emph{International Conference on Learning Representations}, 2022{\natexlab{b}}.

\bibitem[Sirignano and Spiliopoulos(2020)]{sirignano2020mean}
Justin Sirignano and Konstantinos Spiliopoulos.
\newblock Mean field analysis of neural networks: A central limit theorem.
\newblock \emph{Stochastic Processes and their Applications}, pages 1820--1852, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, pages 2822--2878, 2018.

\bibitem[St{\"o}ger and Soltanolkotabi(2021)]{stoger2021small}
Dominik St{\"o}ger and Mahdi Soltanolkotabi.
\newblock Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23831--23843, 2021.

\bibitem[Telgarsky(2022)]{telgarsky2022feature}
Matus Telgarsky.
\newblock Feature selection with gradient descent on two-layer networks in low-rotation regimes.
\newblock \emph{arXiv preprint arXiv:2208.02789}, 2022.

\bibitem[Veiga et~al.(2022)Veiga, Stephan, Loureiro, Krzakala, and Zdeborov{\'a}]{veiga2022phase}
Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:2202.00293}, 2022.

\bibitem[Wang et~al.(2020)Wang, Lacotte, and Pilanci]{wang2020hidden}
Yifei Wang, Jonathan Lacotte, and Mert Pilanci.
\newblock The hidden convex optimization landscape of two-layer relu neural networks: an exact characterization of the optimal solutions.
\newblock \emph{arXiv e-prints}, pages arXiv--2006, 2020.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural nets vs their induced kernel.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese, Golan, Soudry, and Srebro]{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Yang(2019)]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Yang and Hu(2020)]{yang2020feature}
Greg Yang and Edward~J Hu.
\newblock Feature learning in infinite-width neural networks.
\newblock \emph{arXiv preprint arXiv:2011.14522}, 2020.

\bibitem[Yehudai and Shamir(2019)]{yehudai2020power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Zeiler and Fergus(2014)]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European Conference on Computer Vision}, 2014.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Bengio, and Singer]{zhang2019all}
Chiyuan Zhang, Samy Bengio, and Yoram Singer.
\newblock Are all layers created equal?
\newblock \emph{arXiv preprint arXiv:1902.01996}, 2019.

\bibitem[Zhou et~al.(2021)Zhou, Ge, and Jin]{zhou2021local}
Mo~Zhou, Rong Ge, and Chi Jin.
\newblock A local convergence theory for mildly over-parameterized two-layer neural network.
\newblock In \emph{COLT}, 2021.

\bibitem[Zou and Gu(2019)]{zou2019improved}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock {Gradient descent optimizes over-parameterized deep ReLU networks}.
\newblock \emph{Machine Learning}, 109:\penalty0 467--492, 2020.

\bibitem[Zou et~al.(2023)Zou, Cao, Li, and Gu]{pmlr-v202-zou23a}
Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu.
\newblock The benefits of mixup for feature learning.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, Proceedings of Machine Learning Research, pages 43423--43479. PMLR, 2023.

\end{thebibliography}
