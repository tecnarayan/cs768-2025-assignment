@article{maurer2009empirical,
  title={Empirical bernstein bounds and sample variance penalization},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:0907.3740},
  year={2009}
}


@article{blanchet2023double,
  title={Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage},
  author={Blanchet, Jose and Lu, Miao and Zhang, Tong and Zhong, Han},
  journal={arXiv preprint arXiv:2305.09659},
  year={2023}
}

@article{shi2023curious,
  title={The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model},
  author={Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Geist, Matthieu and Chi, Yuejie},
  journal={arXiv preprint arXiv:2305.16589},
  year={2023}
}


@article{wang2023sample,
  title={Sample Complexity of Variance-reduced Distributionally Robust Q-learning},
  author={Wang, Shengbo and Si, Nian and Blanchet, Jose and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2305.18420},
  year={2023}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}


@article{gilbert1952comparison,
  title={A comparison of signalling alphabets},
  author={Gilbert, Edgar N},
  journal={The Bell system technical journal},
  volume={31},
  number={3},
  pages={504--522},
  year={1952},
  publisher={Nokia Bell Labs}
}

@article{agarwal2019reinforcement,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  pages={10--4},
  year={2019}
}

@inproceedings{si2020distributionally,
  title={Distributionally robust policy evaluation and learning in offline contextual bandits},
  author={Si, Nian and Zhang, Fan and Zhou, Zhengyuan and Blanchet, Jose},
  booktitle={International Conference on Machine Learning},
  pages={8884--8894},
  year={2020},
  organization={PMLR}
}
@inproceedings{si2020distributionally,
  title={Distributionally robust policy evaluation and learning in offline contextual bandits},
  author={Si, Nian and Zhang, Fan and Zhou, Zhengyuan and Blanchet, Jose},
  booktitle={International Conference on Machine Learning},
  pages={8884--8894},
  year={2020},
  organization={PMLR}
}

@article{yang2022toward,
  title={Toward theoretical understandings of robust Markov decision processes: Sample complexity and asymptotics},
  author={Yang, Wenhao and Zhang, Liangyu and Zhang, Zhihua},
  journal={The Annals of Statistics},
  volume={50},
  number={6},
  pages={3223--3248},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{sadeghi2016cad2rl,
  title={Cad2rl: Real single-image flight without a single real image},
  author={Sadeghi, Fereshteh and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.04201},
  year={2016}
}

@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{kiran2021deep,
  title={Deep reinforcement learning for autonomous driving: A survey},
  author={Kiran, B Ravi and Sobh, Ibrahim and Talpaert, Victor and Mannion, Patrick and Al Sallab, Ahmad A and Yogamani, Senthil and P{\'e}rez, Patrick},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={23},
  number={6},
  pages={4909--4926},
  year={2021},
  publisher={IEEE}
}

@inproceedings{panaganti2022sample,
  title={Sample Complexity of Robust Reinforcement Learning with a Generative Model},
  author={Panaganti, Kishan and Kalathil, Dileep},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={9582--9602},
  year={2022},
  organization={PMLR}
}

@inproceedings{zhou2021finite,
  title={Finite-Sample Regret Bound for Distributionally Robust Offline Tabular Reinforcement Learning},
  author={Zhou, Zhengqing and Zhou, Zhengyuan and Bai, Qinxun and Qiu, Linhai and Blanchet, Jose and Glynn, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3331--3339},
  year={2021},
  organization={PMLR}
}

@article{wiesemann2013robust,
  title={Robust Markov decision processes},
  author={Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber{\c{c}}},
  journal={Mathematics of Operations Research},
  volume={38},
  number={1},
  pages={153--183},
  year={2013},
  publisher={INFORMS}
}

@article{iyengar2005robust,
  title={Robust dynamic programming},
  author={Iyengar, Garud N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={2},
  pages={257--280},
  year={2005},
  publisher={INFORMS}
}

@article{ma2022distributionally,
  title={Distributionally robust offline reinforcement learning with linear function approximation},
  author={Ma, Xiaoteng and Liang, Zhipeng and Xia, Li and Zhang, Jiheng and Blanchet, Jose and Liu, Mingwen and Zhao, Qianchuan and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2209.06620},
  year={2022}
}

@article{shi2022distributionally,
  title={Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity},
  author={Shi, Laixi and Chi, Yuejie},
  journal={arXiv preprint arXiv:2208.05767},
  year={2022}
}

@article{panaganti2022robust,
  title={Robust reinforcement learning using offline data},
  author={Panaganti, Kishan and Xu, Zaiyan and Kalathil, Dileep and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:2208.05129},
  year={2022}
}

@article{uehara2021pessimistic,
  title={Pessimistic model-based offline reinforcement learning under partial coverage},
  author={Uehara, Masatoshi and Sun, Wen},
  journal={arXiv preprint arXiv:2107.06226},
  year={2021}
}

@article{neufeld2022robust,
  title={Robust $ Q $-learning Algorithm for Markov Decision Processes under Wasserstein Uncertainty},
  author={Neufeld, Ariel and Sester, Julian},
  journal={arXiv preprint arXiv:2210.00898},
  year={2022}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@inproceedings{kearns1999efficient,
  title={Efficient reinforcement learning in factored MDPs},
  author={Kearns, Michael and Koller, Daphne},
  booktitle={IJCAI},
  volume={16},
  pages={740--747},
  year={1999}
}

@article{dong2022online,
  title={Online Policy Optimization for Robust MDP},
  author={Dong, Jing and Li, Jingwei and Wang, Baoxiang and Zhang, Jingzhao},
  journal={arXiv preprint arXiv:2209.13841},
  year={2022}
}

@article{ho2022robust,
  title={Robust $\phi$-Divergence MDPs},
  author={Ho, Chin Pang and Petrik, Marek and Wiesemann, Wolfram},
  year={2022}
}

@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline rl?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{hu2013kullback,
  title={Kullback-Leibler divergence constrained distributionally robust optimization},
  author={Hu, Zhaolin and Hong, L Jeff},
  journal={Available at Optimization Online},
  pages={1695--1724},
  year={2013}
}

@inproceedings{sun2019model,
  title={Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches},
  author={Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Conference on learning theory},
  pages={2898--2933},
  year={2019},
  organization={PMLR}
}

@article{neu2020unifying,
  title={A unifying view of optimism in episodic reinforcement learning},
  author={Neu, Gergely and Pike-Burke, Ciara},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1392--1403},
  year={2020}
}

@inproceedings{liu2022distributionally,
  title={Distributionally Robust $ Q $-Learning},
  author={Liu, Zijian and Bai, Qinxun and Blanchet, Jose and Dong, Perry and Xu, Wei and Zhou, Zhengqing and Zhou, Zhengyuan},
  booktitle={International Conference on Machine Learning},
  pages={13623--13643},
  year={2022},
  organization={PMLR}
}

@inproceedings{liu2022welfare,
  title={Welfare maximization in competitive equilibrium: Reinforcement learning for markov exchange economy},
  author={Liu, Zhihan and Lu, Miao and Wang, Zhaoran and Jordan, Michael and Yang, Zhuoran},
  booktitle={International Conference on Machine Learning},
  pages={13870--13911},
  year={2022},
  organization={PMLR}
}

@book{van2000empirical,
  title={Empirical Processes in M-estimation},
  author={Van de Geer, Sara A},
  volume={6},
  year={2000},
  publisher={Cambridge university press}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{pan2017agile,
  title={Agile autonomous driving using end-to-end deep imitation learning},
  author={Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  journal={arXiv preprint arXiv:1709.07174},
  year={2017}
}

@inproceedings{wang2018supervised,
  title={Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation},
  author={Wang, Lu and Zhang, Wei and He, Xiaofeng and Zha, Hongyuan},
  booktitle={Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery \& data mining},
  pages={2447--2456},
  year={2018}
}

@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{cheng2022adversarially,
  title={Adversarially trained actor critic for offline reinforcement learning},
  author={Cheng, Ching-An and Xie, Tengyang and Jiang, Nan and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2202.02446},
  year={2022}
}

@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={6683--6694},
  year={2021}
}

@article{openai2018learning,
  title={Learning Dexterous In-Hand Manipulation},
  author={OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Józefowicz, Rafał and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  journal={CoRR},
  year={2018},
  url={http://arxiv.org/abs/1808.00177}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{peng2018sim,
  title={Sim-to-real transfer of robotic control with dynamics randomization},
  author={Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={3803--3810},
  year={2018},
  organization={IEEE}
}

@inproceedings{zhao2020sim,
  title={Sim-to-real transfer in deep reinforcement learning for robotics: a survey},
  author={Zhao, Wenshuai and Queralta, Jorge Pe{\~n}a and Westerlund, Tomi},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={737--744},
  year={2020},
  organization={IEEE}
}

@article{hu2022provable,
  title={Provable Sim-to-real Transfer in Continuous Domain with Partial Observations},
  author={Hu, Jiachen and Zhong, Han and Jin, Chi and Wang, Liwei},
  journal={arXiv preprint arXiv:2210.15598},
  year={2022}
}

@inproceedings{mannor2004bias,
  title={Bias and variance in value function estimation},
  author={Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={72},
  year={2004}
}

@article{el2005robust,
  title={Robust solutions to markov decision problems with uncertain transition matrices},
  author={El Ghaoui, Laurent and Nilim, Arnab},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005}
}

@article{morimoto2005robust,
  title={Robust reinforcement learning},
  author={Morimoto, Jun and Doya, Kenji},
  journal={Neural computation},
  volume={17},
  number={2},
  pages={335--359},
  year={2005},
  publisher={MIT Press}
}

@inproceedings{pinto2017robust,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={International Conference on Machine Learning},
  pages={2817--2826},
  year={2017},
  organization={PMLR}
}

@inproceedings{pinto2017supervision,
  title={Supervision via competition: Robot adversaries for learning tasks},
  author={Pinto, Lerrel and Davidson, James and Gupta, Abhinav},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1601--1608},
  year={2017},
  organization={IEEE}
}

@article{pattanaik2017robust,
  title={Robust deep reinforcement learning with adversarial attacks},
  author={Pattanaik, Anay and Tang, Zhenyi and Liu, Shuijing and Bommannan, Gautham and Chowdhary, Girish},
  journal={arXiv preprint arXiv:1712.03632},
  year={2017}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: A tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11702--11716},
  year={2021}
}

@article{zanette2021provable,
  title={Provable benefits of actor-critic methods for offline reinforcement learning},
  author={Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13626--13640},
  year={2021}
}

@article{xiong2022nearly,
  title={Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game},
  author={Xiong, Wei and Zhong, Han and Shi, Chengshuai and Shen, Cong and Wang, Liwei and Zhang, Tong},
  journal={arXiv preprint arXiv:2205.15512},
  year={2022}
}

@article{shi2022pessimistic,
  title={Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity},
  author={Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
  journal={arXiv preprint arXiv:2202.13890},
  year={2022}
}

@article{li2022settling,
  title={Settling the sample complexity of model-based offline reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Chi, Yuejie and Wei, Yuting},
  journal={arXiv preprint arXiv:2204.05275},
  year={2022}
}

@article{xie2021policy,
  title={Policy finetuning: Bridging sample-efficient offline and online reinforcement learning},
  author={Xie, Tengyang and Jiang, Nan and Wang, Huan and Xiong, Caiming and Bai, Yu},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={27395--27407},
  year={2021}
}

@inproceedings{zhan2022offline,
  title={Offline reinforcement learning with realizability and single-policy concentrability},
  author={Zhan, Wenhao and Huang, Baihe and Huang, Audrey and Jiang, Nan and Lee, Jason},
  booktitle={Conference on Learning Theory},
  pages={2730--2775},
  year={2022},
  organization={PMLR}
}

@article{rashidinejad2022optimal,
  title={Optimal conservative offline rl with general function approximation via augmented lagrangian},
  author={Rashidinejad, Paria and Zhu, Hanlin and Yang, Kunhe and Russell, Stuart and Jiao, Jiantao},
  journal={arXiv preprint arXiv:2211.00716},
  year={2022}
}

@article{yin2021towards,
  title={Towards instance-optimal offline reinforcement learning with pessimism},
  author={Yin, Ming and Wang, Yu-Xiang},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={4065--4078},
  year={2021}
}

@inproceedings{munos2005error,
  title={Error bounds for approximate value iteration},
  author={Munos, R{\'e}mi},
  booktitle={Proceedings of the National Conference on Artificial Intelligence},
  volume={20},
  number={2},
  pages={1006},
  year={2005},
  organization={Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}
}

@article{antos2008learning,
  title={Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@article{xu2010distributionally,
  title={Distributionally robust Markov decision processes},
  author={Xu, Huan and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}

@inproceedings{badrinath2021robust,
  title={Robust reinforcement learning using least squares policy iteration with provable performance guarantees},
  author={Badrinath, Kishan Panaganti and Kalathil, Dileep},
  booktitle={International Conference on Machine Learning},
  pages={511--520},
  year={2021},
  organization={PMLR}
}

@article{wang2021online,
  title={Online robust reinforcement learning with model uncertainty},
  author={Wang, Yue and Zou, Shaofeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7193--7206},
  year={2021}
}

@article{nilim2005robust,
  title={Robust control of Markov decision processes with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005},
  publisher={INFORMS}
}

@article{cai2020optimistic,
  title={Optimistic Policy Optimization with General Function Approximations},
  author={Cai, Qi and Yang, Zhuoran and Szepesvari, Csaba and Wang, Zhaoran},
  year={2020}
}

@article{yang2020provably,
  title={Provably efficient reinforcement learning with kernel and neural function approximations},
  author={Yang, Zhuoran and Jin, Chi and Wang, Zhaoran and Wang, Mengdi and Jordan, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13903--13916},
  year={2020}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{yang2019fine,
  title={A fine-grained spectral perspective on neural networks},
  author={Yang, Greg and Salman, Hadi},
  journal={arXiv preprint arXiv:1907.10599},
  year={2019}
}

@book{steinwart2008support,
  title={Support vector machines},
  author={Steinwart, Ingo and Christmann, Andreas},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@inproceedings{jin2022power,
  title={The power of exploiter: Provable multi-agent rl in large state spaces},
  author={Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={10251--10279},
  year={2022},
  organization={PMLR}
}

@article{li2022learning,
  title={Learning Two-Player Mixture Markov Games: Kernel Function Approximation and Correlated Equilibrium},
  author={Li, Chris Junchi and Zhou, Dongruo and Gu, Quanquan and Jordan, Michael I},
  journal={arXiv preprint arXiv:2208.05363},
  year={2022}
}

@inproceedings{wang2023finite,
  title={A Finite Sample Complexity Bound for Distributionally Robust Q-learning},
  author={Wang, Shengbo and Si, Nian and Blanchet, Jose and Zhou, Zhengyuan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3370--3398},
  year={2023},
  organization={PMLR}
}

@article{si2023distributionally,
  title={Distributionally Robust Batch Contextual Bandits},
  author={Si, Nian and Zhang, Fan and Zhou, Zhengyuan and Blanchet, Jose},
  journal={Management Science},
  year={2023},
  publisher={INFORMS}
}

@inproceedings{wang2022policy,
  title={Policy gradient method for robust reinforcement learning},
  author={Wang, Yue and Zou, Shaofeng},
  booktitle={International Conference on Machine Learning},
  pages={23484--23526},
  year={2022},
  organization={PMLR}
}

@article{wang2022convergence,
  title={On the Convergence of Policy Gradient in Robust MDPs},
  author={Wang, Qiuhao and Ho, Chin Pang and Petrik, Marek},
  journal={arXiv preprint arXiv:2212.10439},
  year={2022}
}

@article{yang2023avoiding,
  title={Avoiding Model Estimation in Robust Markov Decision Processes with a Generative Model},
  author={Yang, Wenhao and Wang, Han and Kozuno, Tadashi and Jordan, Scott M and Zhang, Zhihua},
  journal={arXiv preprint arXiv:2302.01248},
  year={2023}
}

@inproceedings{kuang2022learning,
  title={Learning robust policy against disturbance in transition dynamics via state-conservative policy optimization},
  author={Kuang, Yufei and Lu, Miao and Wang, Jie and Zhou, Qi and Li, Bin and Li, Houqiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={7247--7254},
  year={2022}
}

@article{zhang2020robust,
  title={Robust deep reinforcement learning against adversarial perturbations on state observations},
  author={Zhang, Huan and Chen, Hongge and Xiao, Chaowei and Li, Bo and Liu, Mingyan and Boning, Duane and Hsieh, Cho-Jui},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21024--21037},
  year={2020}
}

@inproceedings{mandlekar2017adversarially,
  title={Adversarially robust policy learning: Active construction of physically-plausible perturbations},
  author={Mandlekar, Ajay and Zhu, Yuke and Garg, Animesh and Fei-Fei, Li and Savarese, Silvio},
  booktitle={2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3932--3939},
  year={2017},
  organization={IEEE}
}

@inproceedings{tessler2019action,
  title={Action robust reinforcement learning and applications in continuous control},
  author={Tessler, Chen and Efroni, Yonathan and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={6215--6224},
  year={2019},
  organization={PMLR}
}

@article{lu2022pessimism,
  title={Pessimism in the face of confounders: Provably efficient offline reinforcement learning in partially observable markov decision processes},
  author={Lu, Miao and Min, Yifei and Wang, Zhaoran and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2205.13589},
  year={2022}
}

@article{clavier2023towards,
  title={Towards Minimax Optimality of Model-based Robust Reinforcement Learning},
  author={Clavier, Pierre and Pennec, Erwan Le and Geist, Matthieu},
  journal={arXiv preprint arXiv:2302.05372},
  year={2023}
}

@inproceedings{xu2023improved,
  title={Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning},
  author={Xu, Zaiyan and Panaganti, Kishan and Kalathil, Dileep},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={9728--9754},
  year={2023},
  organization={PMLR}
}

@article{liang2023single,
  title={Single-Trajectory Distributionally Robust Reinforcement Learning},
  author={Liang, Zhipeng and Ma, Xiaoteng and Blanchet, Jose and Zhang, Jiheng and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2301.11721},
  year={2023}
}

@article{jaksch2010near,
  title={Near-optimal Regret Bounds for Reinforcement Learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  pages={1563--1600},
  year={2010}
}


@article{cui2022offline,
  title={When is Offline Two-Player Zero-Sum Markov Game Solvable?},
  author={Cui, Qiwen and Du, Simon S},
  journal={arXiv preprint arXiv:2201.03522},
  year={2022}
}

@inproceedings{zhong2022pessimistic,
  title={Pessimistic minimax value iteration: Provably efficient equilibrium learning from offline datasets},
  author={Zhong, Han and Xiong, Wei and Tan, Jiyuan and Wang, Liwei and Zhang, Tong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={International Conference on Machine Learning},
  pages={27117--27142},
  year={2022},
  organization={PMLR}
}

@article{cui2022provably,
  title={Provably efficient offline multi-agent reinforcement learning via strategy-wise bonus},
  author={Cui, Qiwen and Du, Simon S},
  journal={arXiv preprint arXiv:2206.00159},
  year={2022}
}


@article{yan2022model,
  title={Model-based reinforcement learning is minimax-optimal for offline zero-sum markov games},
  author={Yan, Yuling and Li, Gen and Chen, Yuxin and Fan, Jianqing},
  journal={arXiv preprint arXiv:2206.04044},
  year={2022}
}

@article{zhang2023offline,
  title={Offline Learning in Markov Games with General Function Approximation},
  author={Zhang, Yuheng and Bai, Yu and Jiang, Nan},
  journal={arXiv preprint arXiv:2302.02571},
  year={2023}
}

@article{kardes2005robust,
  title={Robust stochastic games and applications to counter-terrorism strategies},
  author={Kardes, Erim},
  journal={CREATE report},
  year={2005},
  publisher={Citeseer}
}

@article{kardecs2011discounted,
  title={Discounted robust stochastic games and an application to queueing control},
  author={Karde{\c{s}}, Erim and Ord{\'o}{\~n}ez, Fernando and Hall, Randolph W},
  journal={Operations research},
  volume={59},
  number={2},
  pages={365--382},
  year={2011},
  publisher={INFORMS}
}

@misc{
ma2023decentralized,
title={Decentralized Robust V-learning for Solving Markov Games with Model Uncertainty},
author={Shaocong Ma and Ziyi Chen and Shaofeng Zou and Yi Zhou},
year={2023},
url={https://openreview.net/forum?id=QTbAoQ5yMCg}
}

@book{filar2012competitive,
  title={Competitive Markov decision processes},
  author={Filar, Jerzy and Vrieze, Koos},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}

@article{zhangk2020robust,
  title={Robust multi-agent reinforcement learning with model uncertainty},
  author={Zhang, Kaiqing and Sun, Tao and Tao, Yunzhe and Genc, Sahika and Mallya, Sunil and Basar, Tamer},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={10571--10583},
  year={2020}
}

@inproceedings{zhang2021reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={4528--4531},
  year={2021},
  organization={PMLR}
}

@article{zhang2023settling,
  title={Settling the sample complexity of online reinforcement learning},
  author={Zhang, Zihan and Chen, Yuxin and Lee, Jason D and Du, Simon S},
  journal={arXiv preprint arXiv:2307.13586},
  year={2023}
}

@article{zhong2022gec,
  title={Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond},
  author={Zhong, Han and Xiong, Wei and Zheng, Sirui and Wang, Liwei and Wang, Zhaoran and Yang, Zhuoran and Zhang, Tong},
  journal={arXiv preprint arXiv:2211.01962},
  year={2022}
}

@inproceedings{wu2022nearly,
  title={Nearly optimal policy optimization with stable at any time guarantee},
  author={Wu, Tianhao and Yang, Yunchang and Zhong, Han and Wang, Liwei and Du, Simon and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={24243--24265},
  year={2022},
  organization={PMLR}
}

@article{zhong2023theoretical,
  title={A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes},
  author={Zhong, Han and Zhang, Tong},
  journal={arXiv preprint arXiv:2305.08841},
  year={2023}
}

@article{liu2023one,
  title={One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration},
  author={Liu, Zhihan and Lu, Miao and Xiong, Wei and Zhong, Han and Hu, Hao and Zhang, Shenao and Zheng, Sirui and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2305.18258},
  year={2023}
}

@article{foster2021statistical,
  title={The statistical complexity of interactive decision making},
  author={Foster, Dylan J and Kakade, Sham M and Qian, Jian and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2112.13487},
  year={2021}
}

@article{jin2021bellman,
  title={Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13406--13418},
  year={2021}
}

@inproceedings{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon and Kakade, Sham and Lee, Jason and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={2826--2836},
  year={2021},
  organization={PMLR}
}

@inproceedings{xu2023bayesian,
  title={Bayesian design principles for frequentist sequential learning},
  author={Xu, Yunbei and Zeevi, Assaf},
  booktitle={International Conference on Machine Learning},
  pages={38768--38800},
  year={2023},
  organization={PMLR}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low bellman rank are pac-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@inproceedings{he2023nearly,
  title={Nearly minimax optimal reinforcement learning for linear markov decision processes},
  author={He, Jiafan and Zhao, Heyang and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={12790--12822},
  year={2023},
  organization={PMLR}
}

@inproceedings{agarwal2023vo,
  title={VO $ Q $ L: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation},
  author={Agarwal, Alekh and Jin, Yujia and Zhang, Tong},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={987--1063},
  year={2023},
  organization={PMLR}
}

@inproceedings{zhou2021nearly,
  title={Nearly minimax optimal reinforcement learning for linear mixture markov decision processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  pages={4532--4576},
  year={2021},
  organization={PMLR}
}

@inproceedings{ayoub2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={463--474},
  year={2020},
  organization={PMLR}
}

@article{huang2023horizon,
  title={Horizon-Free and Instance-Dependent Regret Bounds for Reinforcement Learning with General Function Approximation},
  author={Huang, Jiayi and Zhong, Han and Wang, Liwei and Yang, Lin F},
  journal={arXiv preprint arXiv:2312.04464},
  year={2023}
}

@article{huang2023tackling,
  title={Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds},
  author={Huang, Jiayi and Zhong, Han and Wang, Liwei and Yang, Lin F},
  journal={arXiv preprint arXiv:2306.06836},
  year={2023}
}


@InProceedings{pmlr-v202-wang23i,
  title = 	 {Policy Gradient in Robust {MDP}s with Global Convergence Guarantee},
  author =       {Wang, Qiuhao and Ho, Chin Pang and Petrik, Marek},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {35763--35797},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/wang23i/wang23i.pdf},
  url = 	 {https://proceedings.mlr.press/v202/wang23i.html},
  abstract = 	 {Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.}
}

@inproceedings{yu2023fast,
  title={Fast Bellman Updates for Wasserstein Distributionally Robust MDPs},
  author={Yu, Zhuodong and Dai, Ling and Xu, Shaohang and Gao, Siyang and Ho, Chin Pang},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@inproceedings{zhou2023natural,
  title={Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation},
  author={Zhou, Ruida and Liu, Tao and Cheng, Min and Kalathil, Dileep and Kumar, Panganamala and Tian, Chao},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@article{wang2023foundation,
  title={On the foundation of distributionally robust reinforcement learning},
  author={Wang, Shengbo and Si, Nian and Blanchet, Jose and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2311.09018},
  year={2023}
}

@article{li2023first,
  title={First-order Policy Optimization for Robust Policy Evaluation},
  author={Li, Yan and Lan, Guanghui},
  journal={arXiv preprint arXiv:2307.15890},
  year={2023}
}

@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}

@article{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{zhang2020almost,
  title={Almost optimal model-free reinforcement learningvia reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}

@article{li2023q,
  title={Is Q-learning minimax optimal? a tight sample complexity analysis},
  author={Li, Gen and Cai, Changxiao and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={Operations Research},
  year={2023},
  publisher={INFORMS}
}

@inproceedings{menard2021ucb,
  title={UCB Momentum Q-learning: Correcting the bias without forgetting},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Shang, Xuedong and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={7609--7618},
  year={2021},
  organization={PMLR}
}

@article{ye2023corruption,
  title={Corruption-Robust Offline Reinforcement Learning with General Function Approximation},
  author={Ye, Chenlu and Yang, Rui and Gu, Quanquan and Zhang, Tong},
  journal={arXiv preprint arXiv:2310.14550},
  year={2023}
}

@inproceedings{zhang2022corruption,
  title={Corruption-robust offline reinforcement learning},
  author={Zhang, Xuezhou and Chen, Yiding and Zhu, Xiaojin and Sun, Wen},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5757--5773},
  year={2022},
  organization={PMLR}
}

@inproceedings{wei2022model,
  title={A model selection approach for corruption robust reinforcement learning},
  author={Wei, Chen-Yu and Dann, Christoph and Zimmert, Julian},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={1043--1096},
  year={2022},
  organization={PMLR}
}

@inproceedings{lykouris2021corruption,
  title={Corruption-robust exploration in episodic reinforcement learning},
  author={Lykouris, Thodoris and Simchowitz, Max and Slivkins, Alex and Sun, Wen},
  booktitle={Conference on Learning Theory},
  pages={3242--3245},
  year={2021},
  organization={PMLR}
}

@article{yang2023towards,
  title={Towards Robust Offline Reinforcement Learning under Diverse Data Corruption},
  author={Yang, Rui and Zhong, Han and Xu, Jiawei and Zhang, Amy and Zhang, Chongjie and Han, Lei and Zhang, Tong},
  journal={arXiv preprint arXiv:2310.12955},
  year={2023}
}

@inproceedings{ye2023corruptiononline,
  title={Corruption-robust algorithms with uncertainty weighting for nonlinear contextual bandits and markov decision processes},
  author={Ye, Chenlu and Xiong, Wei and Gu, Quanquan and Zhang, Tong},
  booktitle={International Conference on Machine Learning},
  pages={39834--39863},
  year={2023},
  organization={PMLR}
}

@article{moos2022robust,
  title={Robust reinforcement learning: A review of foundations and recent advances},
  author={Moos, Janosch and Hansel, Kay and Abdulsamad, Hany and Stark, Svenja and Clever, Debora and Peters, Jan},
  journal={Machine Learning and Knowledge Extraction},
  volume={4},
  number={1},
  pages={276--315},
  year={2022},
  publisher={MDPI}
}

@article{ye2024towards,
  title={Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption},
  author={Ye, Chenlu and He, Jiafan and Gu, Quanquan and Zhang, Tong},
  journal={arXiv preprint arXiv:2402.08991},
  year={2024}
}

@article{liu2024distributionally,
  title={Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation},
  author={Liu, Zhishuai and Xu, Pan},
  journal={arXiv preprint arXiv:2402.15399},
  year={2024}
}

@article{ding2024seeing,
  title={Seeing is not believing: Robust reinforcement learning against spurious correlation},
  author={Ding, Wenhao and Shi, Laixi and Chi, Yuejie and Zhao, Ding},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2024minimax,
  title={Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning},
  author={Liu, Zhishuai and Xu, Pan},
  journal={arXiv preprint arXiv:2403.09621},
  year={2024}
}

@article{wang2024sample,
  title={Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes},
  author={Wang, He and Shi, Laixi and Chi, Yuejie},
  journal={arXiv preprint arXiv:2403.12946},
  year={2024}
}