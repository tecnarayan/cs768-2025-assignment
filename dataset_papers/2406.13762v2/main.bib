@inproceedings{
nguyen2023a,
title={A Primal-Dual Framework for Transformers and Neural Networks},
author={Tan Minh Nguyen and Tam Minh Nguyen and Nhat Ho and Andrea L. Bertozzi and Richard Baraniuk and Stanley Osher},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=U_T8-5hClV}
}
@inproceedings{amos2017optnet,
  title={Optnet: Differentiable optimization as a layer in neural networks},
  author={Amos, Brandon and Kolter, J Zico},
  booktitle={International conference on machine learning},
  pages={136--145},
  year={2017},
  organization={PMLR}
}

@article{bai2019deep,
  title={Deep equilibrium models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{DBLP:journals/corr/CarliniW16a,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={2017 ieee symposium on security and privacy (sp)},
  pages={39--57},
  year={2017},
  organization={Ieee}
}

@article{chen2023primalattention,
  title={Primal-attention: Self-attention through asymmetric kernel svd in primal representation},
  author={Chen, Yingyi and Tao, Qinghua and Tonin, Francesco and Suykens, Johan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{
geng2021attention,
title={Is Attention Better Than Matrix Decomposition?},
author={Zhengyang Geng and Meng-Hao Guo and Hongxu Chen and Xia Li and Ke Wei and Zhouchen Lin},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=1FvkSpWosOl}
}


@article{geshkovski2023mathematical,
  title={A mathematical perspective on transformers},
  author={Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  journal={arXiv preprint arXiv:2312.10794},
  year={2023}
}

@article{geshkovski2024emergence,
  title={The emergence of clusters in self-attention dynamics},
  author={Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{DBLP:journals/corr/abs-2006-16236,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@inproceedings{
DBLP:journals/corr/abs-1903-12261,
title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
author={Dan Hendrycks and Thomas Dietterich},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJz6tiCqYm},
}

@inproceedings{DBLP:journals/corr/abs-2006-16241,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8340--8349},
  year={2021}
}

@inproceedings{croce2020reliable,
  title={Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
  author={Croce, Francesco and Hein, Matthias},
  booktitle={International conference on machine learning},
  pages={2206--2216},
  year={2020},
  organization={PMLR}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}

@inproceedings{DBLP:journals/corr/abs-2103-14586,
  title={Understanding robustness of transformers for image classification},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Glasner, Daniel and Li, Daliang and Unterthiner, Thomas and Veit, Andreas},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10231--10241},
  year={2021}
}

@inproceedings{yang2021graph,
  title={Graph neural networks inspired by classical iterative algorithms},
  author={Yang, Yongyi and Liu, Tang and Wang, Yangkun and Zhou, Jinjing and Gan, Quan and Wei, Zhewei and Zhang, Zheng and Huang, Zengfeng and Wipf, David},
  booktitle={International Conference on Machine Learning},
  pages={11773--11783},
  year={2021},
  organization={PMLR}
}

@inproceedings{morris2020textattack,
  title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},
  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={119--126},
  year={2020}
}
@article{tarzanagh2023transformers,
  title={Transformers as support vector machines},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv preprint arXiv:2308.16898},
  year={2023}
}
@ARTICLE{1189643,
  author={Suykens, J.A.K. and Van Gestel, T. and Vandewalle, J. and De Moor, B.},
  journal={IEEE Transactions on Neural Networks}, 
  title={A support vector machine formulation to PCA analysis and its kernel version}, 
  year={2003},
  volume={14},
  number={2},
  pages={447-450},
  keywords={Support vector machines;Principal component analysis;Kernel;Support vector machine classification;Analysis of variance;Least squares methods;Scattering;Constraint optimization;Knowledge management;Predictive models},
  doi={10.1109/TNN.2003.809414}}
@INPROCEEDINGS{8100027,
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Scene Parsing through ADE20K Dataset}, 
  year={2017},
  volume={},
  number={},
  pages={5122-5130},
  keywords={Image segmentation;Semantics;Sun;Labeling;Visualization;Neural networks;Computer vision},
  doi={10.1109/CVPR.2017.544}}

@inproceedings{hendrycks2021natural,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15262--15271},
  year={2021}
}

@inproceedings{uesato2018adversarial,
  title={Adversarial risk and the dangers of evaluating against weak attacks},
  author={Uesato, Jonathan and Oâ€™donoghue, Brendan and Kohli, Pushmeet and Oord, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5025--5034},
  year={2018},
  organization={PMLR}
}

@inproceedings{
shi2022revisiting,
title={Revisiting Over-smoothing in {BERT} from the Perspective of Graph},
author={Han Shi and Jiahui Gao and Hang Xu and Xiaodan Liang and Zhenguo Li and Lingpeng Kong and Stephen M. S. Lee and James Kwok},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=dUV91uaXm3}
}

@article{subramanya2022backdoor,
  title={Backdoor attacks on vision transformers},
  author={Subramanya, Akshayvarun and Saha, Aniruddha and Koohpayegani, Soroush Abbasi and Tejankar, Ajinkya and Pirsiavash, Hamed},
  journal={arXiv preprint arXiv:2206.08477},
  year={2022}
}

@inproceedings{zhou2022understanding,
  title={Understanding the robustness in vision transformers},
  author={Zhou, Daquan and Yu, Zhiding and Xie, Enze and Xiao, Chaowei and Anandkumar, Animashree and Feng, Jiashi and Alvarez, Jose M},
  booktitle={International Conference on Machine Learning},
  pages={27378--27394},
  year={2022},
  organization={PMLR}
}


@InProceedings{zheng2022online,
  title = 	 {Online Decision Transformer},
  author =       {Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27042--27059},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zheng22c/zheng22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zheng22c.html},
  abstract = 	 {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via task-specific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.}
}


@article{yuan2009sparse,
  title={Sparse and low-rank matrix decomposition via alternating direction method},
  author={Xiaoming Yuan and Junfeng Yang},
  journal={Pacific Journal of Optimization},
  year={2013},
  volume={9},
  pages={167},
  url={https://api.semanticscholar.org/CorpusID:2345482}
}

@article{wang2022transtab,
  title={Transtab: Learning transferable tabular transformers across tables},
  author={Wang, Zifeng and Sun, Jimeng},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2902--2915},
  year={2022}
}

@article{DBLP:journals/corr/abs-2006-04768,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{DBLP:journals/corr/abs-1904-13000,
  title={Adversarial training and robustness for multiple perturbations},
  author={Tramer, Florian and Boneh, Dan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{DBLP:journals/corr/abs-2012-12877,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International conference on machine learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{strudel2021segmenter,
  title={Segmenter: Transformer for semantic segmentation},
  author={Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={7262--7272},
  year={2021}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186"
}

@inproceedings{al2019character,
  title={Character-level language modeling with deeper self-attention},
  author={Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={3159--3166},
  year={2019}
}

@inproceedings{dai2019transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988"
}

@inproceedings{
dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyzdRiR9Y7},
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}

@inproceedings{
gal2023an,
title={An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
author={Rinon Gal and Yuval Alaluf and Yuval Atzmon and Or Patashnik and Amit Haim Bermano and Gal Chechik and Daniel Cohen-or},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NAQvF08TcyG}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{
baevski2018adaptive,
title={Adaptive Input Representations for Neural Language Modeling},
author={Alexei Baevski and Michael Auli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ByxZX20qFQ},
}

@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}


@inproceedings{dong2021attention,
  title={Attention is not all you need: Pure attention loses rank doubly exponentially with depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  pages={2793--2803},
  year={2021},
  organization={PMLR}
}

@inproceedings{wang2022antioversmooth,
title={Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice},
author={Wang, Peihao and Zheng, Wenqing and Chen, Tianlong and Wang, Zhangyang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=O476oWmiNNp},
}

@article{lin2010augmented,
  title={The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices},
  author={Lin, Zhouchen and Chen, Minming and Ma, Yi},
  journal={arXiv preprint arXiv:1009.5055},
  year={2010}
}

@inproceedings{de2001robust,
  title={Robust principal component analysis for computer vision},
  author={De la Torre, Fernando and Black, Michael J},
  booktitle={Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},
  volume={1},
  pages={362--369},
  year={2001},
  organization={IEEE}
}

@article{nguyen2008robust,
  title={Robust kernel principal component analysis},
  author={Nguyen, Minh and Torre, Fernando},
  journal={Advances in Neural Information Processing Systems},
  volume={21},
  year={2008}
}

@article{hubert2005robpca,
  title={ROBPCA: a new approach to robust principal component analysis},
  author={Hubert, Mia and Rousseeuw, Peter J and Vanden Branden, Karlien},
  journal={Technometrics},
  volume={47},
  number={1},
  pages={64--79},
  year={2005},
  publisher={Taylor \& Francis}
}

@article{pearson1901liii,
  title={LIII. On lines and planes of closest fit to systems of points in space},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin philosophical magazine and journal of science},
  volume={2},
  number={11},
  pages={559--572},
  year={1901},
  publisher={Taylor \& Francis}
}

@inproceedings{scholkopf1997kernel,
  title={Kernel principal component analysis},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
  booktitle={International conference on artificial neural networks},
  pages={583--588},
  year={1997},
  organization={Springer}
}

@inproceedings{
bahdanau2014neural,
title={Neural Machine Translation by Jointly Learning to Align and Translate},
author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2015}
}

@inproceedings{andriushchenko2020square,
  title={Square attack: a query-efficient black-box adversarial attack via random search},
  author={Andriushchenko, Maksym and Croce, Francesco and Flammarion, Nicolas and Hein, Matthias},
  booktitle={European conference on computer vision},
  pages={484--501},
  year={2020},
  organization={Springer}
}

@article{nielsen2024elliptical,
  title={Elliptical Attention},
  author={Nielsen, Stefan K and Abdullaev, Laziz U and Teo, Rachel and Nguyen, Tan M},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{tran2024equivariant,
  title={Equivariant Neural Functional Networks for Transformers},
  author={Tran, Viet-Hoang and Vo, Thieu N and The, An Nguyen and Huu, Tho Tran and Nguyen-Nhat, Minh-Khoi and Tran, Thanh and Pham, Duy-Tung and Nguyen, Tan Minh},
  journal={arXiv preprint arXiv:2410.04209},
  year={2024}
}




@inproceedings{nguyen2023probabilistic,
  title={A Probabilistic framework for pruning transformers via a finite admixture of keys},
  author={Nguyen, Tan M and Nguyen, Tam and Bui, Long and Do, Hai and Nguyen, Duy Khuong and Le, Dung D and Tran-The, Hung and Ho, Nhat and Osher, Stan J and Baraniuk, Richard G},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{nguyen2022head,
  title={Improving transformer with an admixture of attention heads},
  author={Nguyen, Tan and Nguyen, Tam and Do, Hai and Nguyen, Khai and Saragadam, Vishwanath and Pham, Minh and Nguyen, Khuong Duy and Ho, Nhat and Osher, Stanley},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27937--27952},
  year={2022}
}

@inproceedings{
bui2024revisiting,
title={Revisiting Kernel Attention with Correlated Gaussian Process Representation},
author={Long Minh Bui and Tho Tran Huu and Duy Dinh and Tan Minh Nguyen and Trong Nghia Hoang},
booktitle={The 40th Conference on Uncertainty in Artificial Intelligence},
year={2024},
url={https://openreview.net/forum?id=xlIK0vu3MW}
}

@inproceedings{
chen2023calibrating,
title={Calibrating Transformers via Sparse Gaussian Processes},
author={Wenlong Chen and Yingzhen Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=jPVAFXHlbL}
}

@article{han2024designing,
  title={Designing robust transformers using robust kernel density estimation},
  author={Han, Xing and Ren, Tongzheng and Nguyen, Tan and Nguyen, Khai and Ghosh, Joydeep and Ho, Nhat},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{nguyen2024pidformer,
  title={PIDformer: Transformer Meets Control Theory},
  author={Nguyen, Tam and Uribe, C{\'e}sar A and Nguyen, Tan M and Baraniuk, Richard G},
  booktitle={International Conference on Machine Learning},
  year={2024},
  organization={PMLR}
}

@article{nguyen2023mitigating,
  title={Mitigating over-smoothing in transformers via regularized nonlocal functionals},
  author={Nguyen, Tam and Nguyen, Tan and Baraniuk, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={80233--80256},
  year={2023}
}

@article{nguyen2021fmmformer,
  title={Fmmformer: Efficient and flexible transformer via decomposed near-field and far-field attention},
  author={Nguyen, Tan and Suliafu, Vai and Osher, Stanley and Chen, Long and Wang, Bao},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29449--29463},
  year={2021}
}

@article{nguyen2022fourierformer,
  title={Fourierformer: Transformer meets generalized fourier integral theorem},
  author={Nguyen, Tan and Pham, Minh and Nguyen, Tam and Nguyen, Khai and Osher, Stanley and Ho, Nhat},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29319--29335},
  year={2022}
}

@inproceedings{nguyen2022improving,
  title={Improving transformers with probabilistic attention keys},
  author={Nguyen, Tam Minh and Nguyen, Tan Minh and Le, Dung DD and Nguyen, Duy Khuong and Tran, Viet-Anh and Baraniuk, Richard and Ho, Nhat and Osher, Stanley},
  booktitle={International Conference on Machine Learning},
  pages={16595--16621},
  year={2022},
  organization={PMLR}
}

@inproceedings{10.1007/BFb0020217,
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	address = {Berlin, Heidelberg},
	author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
	booktitle = {Artificial Neural Networks --- ICANN'97},
	editor = {Gerstner, Wulfram and Germond, Alain and Hasler, Martin and Nicoud, Jean-Daniel},
	isbn = {978-3-540-69620-9},
	pages = {583--588},
	publisher = {Springer Berlin Heidelberg},
	title = {Kernel principal component analysis},
	year = {1997}}

@inproceedings{subakan2021attention,
  title={Attention is all you need in speech separation},
  author={Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Bronzi, Mirko and Zhong, Jianyuan},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={21--25},
  year={2021},
  organization={IEEE}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12077--12090},
  year={2021}
}

@inproceedings{tenney-etal-2019-bert,
    title = "{BERT} Rediscovers the Classical {NLP} Pipeline",
    author = "Tenney, Ian  and
      Das, Dipanjan  and
      Pavlick, Ellie",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1452",
    doi = "10.18653/v1/P19-1452",
    pages = "4593--4601"
}

@inproceedings{hewitt-liang-2019-designing,
    title = "Designing and Interpreting Probes with Control Tasks",
    author = "Hewitt, John  and
      Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1275",
    doi = "10.18653/v1/D19-1275",
    pages = "2733--2743"
}

@inproceedings{vig-belinkov-2019-analyzing,
    title = "Analyzing the Structure of Attention in a Transformer Language Model",
    author = "Vig, Jesse  and
      Belinkov, Yonatan",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-4808",
    doi = "10.18653/v1/W19-4808",
    pages = "63--76"
}

@inproceedings{voita-etal-2019-analyzing,
    title = "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
    author = "Voita, Elena  and
      Talbot, David  and
      Moiseev, Fedor  and
      Sennrich, Rico  and
      Titov, Ivan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1580",
    doi = "10.18653/v1/P19-1580",
    pages = "5797--5808"
}

@inproceedings{
lin2017a,
title={A Structured Self-attentive Sentence Embedding},
author={Zhouhan Lin and Minwei Feng and Cicero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJC_jUqxe}
}

@inproceedings{DBLP:journals/corr/abs-2105-07581,
  title={Vision transformers are robust learners},
  author={Paul, Sayak and Chen, Pin-Yu},
  booktitle={Proceedings of the AAAI conference on Artificial Intelligence},
  volume={36},
  number={2},
  pages={2071--2081},
  year={2022}
}

@inproceedings{mao2022robust,
  title={Towards robust vision transformer},
  author={Mao, Xiaofeng and Qi, Gege and Chen, Yuefeng and Li, Xiaodan and Duan, Ranjie and Ye, Shaokai and He, Yuan and Xue, Hui},
  booktitle={Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition},
  pages={12042--12051},
  year={2022}
}

@inproceedings{
DBLP:journals/corr/MerityXBS16,
title={Pointer Sentinel Mixture Models},
author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Byj72udxe}
}


@article{lu2019understanding,
  title={Understanding and improving transformer from a multi-particle dynamic system point of view},
  author={Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1906.02762},
  year={2019}
}

@inproceedings{parikh-etal-2016-decomposable,
    title = "A Decomposable Attention Model for Natural Language Inference",
    author = {Parikh, Ankur  and
      T{\"a}ckstr{\"o}m, Oscar  and
      Das, Dipanjan  and
      Uszkoreit, Jakob},
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1244",
    doi = "10.18653/v1/D16-1244",
    pages = "2249--2255",
}

@inproceedings{NEURIPS2022_8bb0d291,
 author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {22199--22213},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}

@article{JMLR:v24:22-1144,
  author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  url     = {http://jmlr.org/papers/v24/22-1144.html}
}

@inproceedings{schlag2021linear,
  title={Linear transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}

@inproceedings{DBLP:journals/corr/abs-2110-11773,
  title={Sinkformers: Transformers with doubly stochastic attention},
  author={Sander, Michael E and Ablin, Pierre and Blondel, Mathieu and Peyr{\'e}, Gabriel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3515--3530},
  year={2022},
  organization={PMLR}
}

@InProceedings{pmlr-v139-rao21a,
  title = 	 {MSA Transformer},
  author =       {Rao, Roshan M and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8844--8856},
  year = 	 {2021},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rao21a/rao21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rao21a.html}
}


@inproceedings{zhuang-etal-2021-robustly,
    title = "A Robustly Optimized {BERT} Pre-training Approach with Post-training",
    author = "Zhuang, Liu  and
      Wayne, Lin  and
      Ya, Shi  and
      Jun, Zhao",
    booktitle = "Proceedings of the 20th Chinese National Conference on Computational Linguistics",
    month = aug,
    year = "2021",
    address = "Huhhot, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2021.ccl-1.108",
    pages = "1218--1227"
}


@inproceedings{NEURIPS2019_dc6a7e65,
 author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
 volume = {32},
 year = {2019}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI report},
  year={2018}
}

@article{zhang2019deep,
  title={Deep learning based recommender system: A survey and new perspectives},
  author={Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={1},
  pages={1--38},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  year={2021},
  publisher={National Acad Sciences}
}

@article{lee2022multi,
  title={Multi-game decision transformers},
  author={Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao Sherry and Lee, Lisa and Freeman, Daniel and Guadarrama, Sergio and Fischer, Ian and Xu, Winnie and Jang, Eric and Michalewski, Henryk and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27921--27936},
  year={2022}
}

@article{chen2022adaptformer,
  title={Adaptformer: Adapting vision transformers for scalable visual recognition},
  author={Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16664--16678},
  year={2022}
}

@article{janner2021offline,
  title={Offline reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1273--1286},
  year={2021}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{xiao2021early,
  title={Early convolutions help transformers see better},
  author={Xiao, Tete and Singh, Mannat and Mintun, Eric and Darrell, Trevor and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={30392--30400},
  year={2021}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{ijcai2018p407,
	author = {Zhiqiang Xu and Xin Cao and Xin Gao},
	booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
	doi = {10.24963/ijcai.2018/407},
	month = {7},
	pages = {2933--2939},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {Convergence Analysis of Gradient Descent for Eigenvector Computation},
	url = {https://doi.org/10.24963/ijcai.2018/407},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2018/407}}

@article{candes2009robust,
  title={Robust principal component analysis?},
  author={Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and Wright, John},
  journal={Journal of the ACM (JACM)},
  volume={58},
  number={3},
  pages={1--37},
  year={2011},
  publisher={ACM New York, NY, USA}
}

@inproceedings{tsai-etal-2019-transformer,
    title = "Transformer Dissection: An Unified Understanding for Transformer{'}s Attention via the Lens of Kernel",
    author = "Tsai, Yao-Hung Hubert  and
      Bai, Shaojie  and
      Yamada, Makoto  and
      Morency, Louis-Philippe  and
      Salakhutdinov, Ruslan",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1443",
    doi = "10.18653/v1/D19-1443",
    pages = "4344--4353",
    abstract = "Transformer is a powerful architecture that achieves superior performance on various sequence learning tasks, including neural machine translation, language understanding, and sequence prediction. At the core of the Transformer is the attention mechanism, which concurrently processes all inputs in the streams. In this paper, we present a new formulation of attention via the lens of the kernel. To be more precise, we realize that the attention can be seen as applying kernel smoother over the inputs with the kernel scores being the similarities between inputs. This new formulation gives us a better way to understand individual components of the Transformer{'}s attention, such as the better way to integrate the positional embedding. Another important advantage of our kernel-based formulation is that it paves the way to a larger space of composing Transformer{'}s attention. As an example, we propose a new variant of Transformer{'}s attention which models the input as a product of symmetric kernels. This approach achieves competitive performance to the current state of the art model with less computation. In our experiments, we empirically study different kernel construction strategies on two widely used tasks: neural machine translation and sequence prediction.",
}

@inproceedings{
choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}

@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}

@article{DBLP:journals/corr/abs-1711-07971,
  author       = {Xiaolong Wang and
                  Ross B. Girshick and
                  Abhinav Gupta and
                  Kaiming He},
  title        = {Non-local Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1711.07971},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.07971},
  eprinttype    = {arXiv},
  eprint       = {1711.07971},
  timestamp    = {Fri, 05 Apr 2019 07:29:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-07971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2106-03893,
  author       = {Devin Kreuzer and
                  Dominique Beaini and
                  William L. Hamilton and
                  Vincent L{\'{e}}tourneau and
                  Prudencio Tossou},
  title        = {Rethinking Graph Transformers with Spectral Attention},
  journal      = {CoRR},
  volume       = {abs/2106.03893},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.03893},
  eprinttype    = {arXiv},
  eprint       = {2106.03893},
  timestamp    = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-03893.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang-feng-2021-modeling-concentrated,
    title = "Modeling Concentrated Cross-Attention for Neural Machine Translation with {G}aussian Mixture Model",
    author = "Zhang, Shaolei  and
      Feng, Yang",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.121",
    doi = "10.18653/v1/2021.findings-emnlp.121",
    pages = "1401--1411",
    abstract = "Cross-attention is an important component of neural machine translation (NMT), which is always realized by dot-product attention in previous methods. However, dot-product attention only considers the pair-wise correlation between words, resulting in dispersion when dealing with long sentences and neglect of source neighboring relationships. Inspired by linguistics, the above issues are caused by ignoring a type of cross-attention, called concentrated attention, which focuses on several central words and then spreads around them. In this work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention in cross-attention. Experiments and analyses we conducted on three datasets show that the proposed method outperforms the baseline and has significant improvement on alignment quality, N-gram accuracy, and long sentence translation.",
}

@inproceedings{
tang2021probabilistic,
title={Probabilistic Transformer For Time Series Analysis},
author={Binh Tang and David S. Matteson},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=HfpNVDg3ExA}
}

@inproceedings{NEURIPS2021_23937b42,
	author = {Gabbur, Prasad and Bilkhu, Manjot and Movellan, Javier},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {4448--4460},
	publisher = {Curran Associates, Inc.},
	title = {Probabilistic Attention for Interactive Segmentation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/23937b42f9273974570fb5a56a6652ee-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/23937b42f9273974570fb5a56a6652ee-Paper.pdf}}

@article{khan2022transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@article{tang2019exponentially,
  title={Exponentially convergent stochastic k-PCA without variance reduction},
  author={Tang, Cheng},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{zou2006sparse,
  title={Sparse principal component analysis},
  author={Zou, Hui and Hastie, Trevor and Tibshirani, Robert},
  journal={Journal of computational and graphical statistics},
  volume={15},
  number={2},
  pages={265--286},
  year={2006},
  publisher={Taylor \& Francis}
}

@inproceedings{43405,title	= {Explaining and Harnessing Adversarial Examples},author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},year	= {2015},URL	= {http://arxiv.org/abs/1412.6572},booktitle	= {International Conference on Learning Representations}}

@inproceedings{
madry2018towards,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJzIBfZAb},
}


@article{papernot2018cleverhans,
  title={Technical Report on the CleverHans v2.1.0 Adversarial Examples Library},
  author={Nicolas Papernot and Fartash Faghri and Nicholas Carlini and
  Ian Goodfellow and Reuben Feinman and Alexey Kurakin and Cihang Xie and
  Yash Sharma and Tom Brown and Aurko Roy and Alexander Matyasko and
  Vahid Behzadan and Karen Hambardzumyan and Zhishuai Zhang and
  Yi-Lin Juang and Zhi Li and Ryan Sheatsley and Abhibhav Garg and
  Jonathan Uesato and Willi Gierke and Yinpeng Dong and David Berthelot and
  Paul Hendricks and Jonas Rauber and Rujun Long},
  journal={arXiv preprint arXiv:1610.00768},
  year={2018}
}