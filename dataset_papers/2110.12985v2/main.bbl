\begin{thebibliography}{}

\bibitem[Anderson et~al., 2018]{anderson2018vision}
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S{\"u}nderhauf, N.,
  Reid, I., Gould, S., and van~den Hengel, A. (2018).
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3674--3683.

\bibitem[Andrychowicz et~al., 2017]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. (2017).
\newblock Hindsight experience replay.
\newblock {\em arXiv preprint arXiv:1707.01495}.

\bibitem[Bahdanau et~al., 2014]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y. (2014).
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}.

\bibitem[Barati and Chen, 2019]{barati2019actor}
Barati, E. and Chen, X. (2019).
\newblock An actor-critic-attention mechanism for deep reinforcement learning
  in multi-view environments.
\newblock In {\em IJCAI}, pages 2002--2008.

\bibitem[Chaplot et~al., 2018]{chaplot2018gated}
Chaplot, D.~S., Sathyendra, K.~M., Pasumarthi, R.~K., Rajagopal, D., and
  Salakhutdinov, R. (2018).
\newblock Gated-attention architectures for task-oriented language grounding.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32.

\bibitem[Chen et~al., 2020a]{chen2020a2c}
Chen, H., Liu, Y., Zhou, Z., and Zhang, M. (2020a).
\newblock A2c: Attention-augmented contrastive learning for state
  representation extraction.
\newblock {\em Applied Sciences}, 10(17):5902.

\bibitem[Chen et~al., 2020b]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020b).
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock {\em arXiv preprint arXiv:2002.05709}.

\bibitem[Cheng et~al., 2018]{cheng2018reinforcement}
Cheng, R., Agarwal, A., and Fragkiadaki, K. (2018).
\newblock Reinforcement learning of active vision for manipulating objects
  under occlusions.
\newblock In {\em Conference on Robot Learning}, pages 422--431. PMLR.

\bibitem[Choi et~al., 2017]{choi2017multi}
Choi, J., Lee, B.-J., and Zhang, B.-T. (2017).
\newblock Multi-focus attention network for efficient deep reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1712.04603}.

\bibitem[Colas et~al., 2019]{colas2019curious}
Colas, C., Fournier, P., Chetouani, M., Sigaud, O., and Oudeyer, P.-Y. (2019).
\newblock Curious: intrinsically motivated modular multi-goal reinforcement
  learning.
\newblock In {\em International conference on machine learning}, pages
  1331--1340. PMLR.

\bibitem[Deisenroth and Fox, 2011]{deisenroth2011multiple}
Deisenroth, M.~P. and Fox, D. (2011).
\newblock Multiple-target reinforcement learning with a single policy.

\bibitem[Devo et~al., 2020]{devo2020towards}
Devo, A., Mezzetti, G., Costante, G., Fravolini, M.~L., and Valigi, P. (2020).
\newblock Towards generalization in target-driven visual navigation by using
  deep reinforcement learning.
\newblock {\em IEEE Transactions on Robotics}, 36(5):1546--1561.

\bibitem[Dhiman et~al., 2018]{dhiman2018learning}
Dhiman, V., Banerjee, S., Siskind, J.~M., and Corso, J.~J. (2018).
\newblock Learning goal-conditioned value functions with one-step path rewards
  rather than goal-rewards.
\newblock {\em ICLR}.

\bibitem[Florensa et~al., 2018]{florensa2018automatic}
Florensa, C., Held, D., Geng, X., and Abbeel, P. (2018).
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In {\em International conference on machine learning}, pages
  1515--1528. PMLR.

\bibitem[Greydanus et~al., 2018]{greydanus2018visualizing}
Greydanus, S., Koul, A., Dodge, J., and Fern, A. (2018).
\newblock Visualizing and understanding atari agents.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 1792--1801.

\bibitem[Ha and Schmidhuber, 2018]{ha2018world}
Ha, D. and Schmidhuber, J. (2018).
\newblock World models.
\newblock {\em arXiv preprint arXiv:1803.10122}.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1870.

\bibitem[Harries et~al., 2019]{harrieslee2019}
Harries, L., Lee, S., Rzepecki, J., Hofmann, K., and Devlin, S. (2019).
\newblock Mazeexplorer: A customisable 3d benchmark for assessing
  generalisation in reinforcement learning.
\newblock {\em Proceedings of the IEEE Conference on Games}.

\bibitem[Higgins et~al., 2017]{higgins2016beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A. (2017).
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock {\em ICLR}.

\bibitem[Hochreiter and Schmidhuber, 1997]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780.

\bibitem[Jaderberg et~al., 2017]{jaderberg2016reinforcement}
Jaderberg, M., Mnih, V., Czarnecki, W.~M., Schaul, T., Leibo, J.~Z., Silver,
  D., and Kavukcuoglu, K. (2017).
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock {\em International Conference in Learning Representations}.

\bibitem[Kalashnikov et~al., 2018]{pmlr-v87-kalashnikov18a}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., and Levine, S.
  (2018).
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In Billard, A., Dragan, A., Peters, J., and Morimoto, J., editors,
  {\em Proceedings of The 2nd Conference on Robot Learning}, volume~87 of {\em
  Proceedings of Machine Learning Research}, pages 651--673. PMLR.

\bibitem[Kempka et~al., 2016]{kempka2016vizdoom}
Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Ja{\'s}kowski, W. (2016).
\newblock Vizdoom: A doom-based ai research platform for visual reinforcement
  learning.
\newblock In {\em IEEE Conference on Computational Intelligence and Games
  (CIG)}, pages 1--8.

\bibitem[Kingma and Welling, 2013]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[Laskin et~al., 2020a]{laskin2020reinforcement}
Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A.
  (2020a).
\newblock Reinforcement learning with augmented data.
\newblock {\em arXiv preprint arXiv:2004.14990}.

\bibitem[Laskin et~al., 2020b]{laskin_srinivas2020curl}
Laskin, M., Srinivas, A., and Abbeel, P. (2020b).
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock {\em Proceedings of the 37th International Conference on Machine
  Learning}.

\bibitem[Liu et~al., 2020]{liu2020robot}
Liu, L., Dugas, D., Cesari, G., Siegwart, R., and Dub{\'e}, R. (2020).
\newblock Robot navigation in crowded environments using deep reinforcement
  learning.
\newblock pages 5671--5677.

\bibitem[Mnih et~al., 2016]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K. (2016).
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937.

\bibitem[Mott et~al., 2019]{mott2019towards}
Mott, A., Zoran, D., Chrzanowski, M., Wierstra, D., and Jimenez~Rezende, D.
  (2019).
\newblock Towards interpretable reinforcement learning using attention
  augmented agents.
\newblock {\em Advances in Neural Information Processing Systems},
  32:12350--12359.

\bibitem[Mousavian et~al., 2019]{mousavian2019visual}
Mousavian, A., Toshev, A., Fi{\v{s}}er, M., Ko{\v{s}}eck{\'a}, J., Wahid, A.,
  and Davidson, J. (2019).
\newblock Visual representations for semantic target driven navigation.
\newblock In {\em 2019 International Conference on Robotics and Automation
  (ICRA)}, pages 8846--8852. IEEE.

\bibitem[Nair et~al., 2018]{nair2018visual}
Nair, A.~V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S. (2018).
\newblock Visual reinforcement learning with imagined goals.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc.

\bibitem[Nair et~al., 2020]{nair2020goal}
Nair, S., Savarese, S., and Finn, C. (2020).
\newblock Goal-aware prediction: Learning to model what matters.
\newblock In {\em International Conference on Machine Learning}, pages
  7207--7219.

\bibitem[Narasimhan et~al., 2018]{narasimhan2018grounding}
Narasimhan, K., Barzilay, R., and Jaakkola, T. (2018).
\newblock Grounding language for transfer in deep reinforcement learning.
\newblock {\em Journal of Artificial Intelligence Research}, 63:849--874.

\bibitem[Pardo et~al., 2020]{pardo2020scaling}
Pardo, F., Levdik, V., and Kormushev, P. (2020).
\newblock Scaling all-goals updates in reinforcement learning using
  convolutional neural networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5355--5362.

\bibitem[Plappert et~al., 2018]{plappert2018multi}
Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G.,
  Schneider, J., Tobin, J., Chociej, M., Welinder, P., et~al. (2018).
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock {\em arXiv preprint arXiv:1802.09464}.

\bibitem[Rajeswaran et~al., 2018]{Rajeswaran-RSS-18}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S. (2018).
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock In {\em Proceedings of Robotics: Science and Systems}, Pittsburgh,
  Pennsylvania.

\bibitem[Schaul et~al., 2015]{schaul2015universal}
Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).
\newblock Universal value function approximators.
\newblock In {\em International conference on machine learning}, pages
  1312--1320. PMLR.

\bibitem[Shelhamer et~al., 2016]{shelhamer2016loss}
Shelhamer, E., Mahmoudieh, P., Argus, M., and Darrell, T. (2016).
\newblock Loss is its own reward: Self-supervision for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1612.07307}.

\bibitem[Shen et~al., 2019]{shen2019situational}
Shen, W.~B., Xu, D., Zhu, Y., Guibas, L.~J., Fei-Fei, L., and Savarese, S.
  (2019).
\newblock Situational fusion of visual representation for visual navigation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2881--2890.

\bibitem[Shridhar et~al., 2020]{shridhar2020alfred}
Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R.,
  Zettlemoyer, L., and Fox, D. (2020).
\newblock Alfred: A benchmark for interpreting grounded instructions for
  everyday tasks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 10740--10749.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Todorov et~al., 2012]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y. (2012).
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em IEEE/RSJ International Conference on Intelligent Robots and
  Systems}, pages 5026--5033.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008.

\bibitem[Wu et~al., 2018]{wu2018building}
Wu, Y., Wu, Y., Gkioxari, G., and Tian, Y. (2018).
\newblock Building generalizable agents with a realistic and rich 3d
  environment.
\newblock {\em arXiv preprint arXiv:1801.02209}.

\bibitem[Yarats et~al., 2019]{yarats2019improving}
Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R.
  (2019).
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock {\em arXiv preprint arXiv:1910.01741}.

\bibitem[Ye et~al., 2019]{ye2019gaple}
Ye, X., Lin, Z., Lee, J.-Y., Zhang, J., Zheng, S., and Yang, Y. (2019).
\newblock Gaple: Generalizable approaching policy learning for robotic object
  searching in indoor environment.
\newblock {\em IEEE Robotics and Automation Letters}, 4(4):4003--4010.

\bibitem[Zhao et~al., 2019]{zhao2019maximum}
Zhao, R., Sun, X., and Tresp, V. (2019).
\newblock Maximum entropy-regularized multi-goal reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  7553--7562. PMLR.

\bibitem[Zhu et~al., 2019]{zhu2019dexterous}
Zhu, H., Gupta, A., Rajeswaran, A., Levine, S., and Kumar, V. (2019).
\newblock Dexterous manipulation with deep reinforcement learning: Efficient,
  general, and low-cost.
\newblock In {\em 2019 International Conference on Robotics and Automation
  (ICRA)}, pages 3651--3657. IEEE.

\bibitem[Zhu et~al., 2017]{zhu2017target}
Zhu, Y., Mottaghi, R., Kolve, E., Lim, J.~J., Gupta, A., Fei-Fei, L., and
  Farhadi, A. (2017).
\newblock Target-driven visual navigation in indoor scenes using deep
  reinforcement learning.
\newblock In {\em IEEE international conference on robotics and automation},
  pages 3357--3364.

\end{thebibliography}
