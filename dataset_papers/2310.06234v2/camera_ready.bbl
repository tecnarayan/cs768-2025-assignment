% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{dosovitskiyvit}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' in \emph{International Conference on Learning Representations},
  2020.

\bibitem{he2022masked}
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'a}r, and R.~Girshick, ``Masked
  autoencoders are scalable vision learners,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  16\,000--16\,009.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in \emph{International conference
  on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  8748--8763.

\bibitem{zhai2022scaling}
X.~Zhai, A.~Kolesnikov, N.~Houlsby, and L.~Beyer, ``Scaling vision
  transformers,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, 2022, pp. 12\,104--12\,113.

\bibitem{zhou2022conditional}
K.~\vspace{0mm}Zhou, J.~Yang, C.~C. Loy, and Z.~Liu, ``Conditional prompt
  learning for vision-language models,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  16\,816--16\,825.

\bibitem{jia2022visual}
M.~Jia, L.~Tang, B.-C. Chen, C.~Cardie, S.~Belongie, B.~Hariharan, and S.-N.
  Lim, ``Visual prompt tuning,'' in \emph{Proceedings of the European
  conference on computer vision (ECCV)}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2022, pp. 709--727.

\bibitem{houlsby2019parameter}
N.~Houlsby, A.~Giurgiu, S.~Jastrzebski, B.~Morrone, Q.~De~Laroussilhe,
  A.~Gesmundo, M.~Attariyan, and S.~Gelly, ``Parameter-efficient transfer
  learning for nlp,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp. 2790--2799.

\bibitem{chen2022adaptformer}
S.~Chen, C.~Ge, Z.~Tong, J.~Wang, Y.~Song, J.~Wang, and P.~Luo, ``Adaptformer:
  Adapting vision transformers for scalable visual recognition,'' in
  \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem{ShiftAndScale}
D.~Lian, D.~Zhou, J.~Feng, and X.~Wang, ``Scaling \& shifting your features: A
  new baseline for efficient model tuning,'' in \emph{Advances in Neural
  Information Processing Systems (NeurIPS)}, 2022.

\bibitem{luo2023towards}
G.~Luo, M.~Huang, Y.~Zhou, X.~Sun, G.~Jiang, Z.~Wang, and R.~Ji, ``Towards
  efficient visual adaption via structural re-parameterization,'' \emph{arXiv
  preprint arXiv:2302.08106}, 2023.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF international conference on computer
  vision}, 2021, pp. 10\,012--10\,022.

\bibitem{zhuang2020comprehensive}
F.~Zhuang, Z.~Qi, K.~Duan, D.~Xi, Y.~Zhu, H.~Zhu, H.~Xiong, and Q.~He, ``A
  comprehensive survey on transfer learning,'' \emph{Proceedings of the IEEE},
  vol. 109, no.~1, pp. 43--76, 2020.

\bibitem{pan2010survey}
S.~J. Pan and Q.~Yang, ``A survey on transfer learning,'' \emph{IEEE
  Transactions on knowledge and data engineering}, vol.~22, no.~10, pp.
  1345--1359, 2010.

\bibitem{iman2023review}
M.~Iman, H.~R. Arabnia, and K.~Rasheed, ``A review of deep transfer learning
  and recent advancements,'' \emph{Technologies}, vol.~11, no.~2, p.~40, 2023.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A
  large-scale hierarchical image database,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}.\hskip 1em
  plus 0.5em minus 0.4em\relax Ieee, 2009, pp. 248--255.

\bibitem{ridnik2021imagenet}
T.~Ridnik, E.~Ben-Baruch, A.~Noy, and L.~Zelnik-Manor, ``Imagenet-21k
  pretraining for the masses,'' in \emph{Advances in Neural Information
  Processing Systems (NeurIPS)}, 2021.

\bibitem{sun2017revisiting}
C.~Sun, A.~Shrivastava, S.~Singh, and A.~Gupta, ``Revisiting unreasonable
  effectiveness of data in deep learning era,'' in \emph{Proceedings of the
  IEEE international conference on computer vision}, 2017, pp. 843--852.

\bibitem{mahajan2018exploring}
D.~Mahajan, R.~Girshick, V.~Ramanathan, K.~He, M.~Paluri, Y.~Li, A.~Bharambe,
  and L.~Van Der~Maaten, ``Exploring the limits of weakly supervised
  pretraining,'' in \emph{Proceedings of the European conference on computer
  vision (ECCV)}, 2018, pp. 181--196.

\bibitem{kay2017kinetics}
W.~Kay, J.~Carreira, K.~Simonyan, B.~Zhang, C.~Hillier, S.~Vijayanarasimhan,
  F.~Viola, T.~Green, T.~Back, P.~Natsev \emph{et~al.}, ``The kinetics human
  action video dataset,'' \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem{han2021transformer}
K.~Han, A.~Xiao, E.~Wu, J.~Guo, C.~Xu, and Y.~Wang, ``Transformer in
  transformer,'' \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, vol.~34, pp. 15\,908--15\,919, 2021.

\bibitem{zhou2021deepvit}
D.~Zhou, B.~Kang, X.~Jin, L.~Yang, X.~Lian, Z.~Jiang, Q.~Hou, and J.~Feng,
  ``Deepvit: Towards deeper vision transformer,'' \emph{arXiv preprint
  arXiv:2103.11886}, 2021.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in Neural Information
  Processing Systems (NeurIPS)}, vol.~33, pp. 1877--1901, 2020.

\bibitem{chen2021empirical}
X.~Chen, S.~Xie, and K.~He, ``An empirical study of training self-supervised
  vision transformers,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 9640--9649.

\bibitem{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and
  W.~Chen, ``Lora: Low-rank adaptation of large language models,'' \emph{arXiv
  preprint arXiv:2106.09685}, 2021.

\bibitem{lester2021power}
B.~Lester, R.~Al-Rfou, and N.~Constant, ``The power of scale for
  parameter-efficient prompt tuning,'' in \emph{Proceedings of the 2021
  Conference on Empirical Methods in Natural Language Processing}, 2021, pp.
  3045--3059.

\bibitem{li2021prefix}
X.~L. Li and P.~Liang, ``Prefix-tuning: Optimizing continuous prompts for
  generation,'' \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{liu2023pre}
P.~Liu, W.~Yuan, J.~Fu, Z.~Jiang, H.~Hayashi, and G.~Neubig, ``Pre-train,
  prompt, and predict: A systematic survey of prompting methods in natural
  language processing,'' \emph{ACM Computing Surveys}, vol.~55, no.~9, pp.
  1--35, 2023.

\bibitem{shin2020autoprompt}
T.~Shin, Y.~Razeghi, R.~L. Logan~IV, E.~Wallace, and S.~Singh, ``Autoprompt:
  Eliciting knowledge from language models with automatically generated
  prompts,'' \emph{arXiv preprint arXiv:2010.15980}, 2020.

\bibitem{sung2022vl}
Y.-L. Sung, J.~Cho, and M.~Bansal, ``Vl-adapter: Parameter-efficient transfer
  learning for vision-and-language tasks,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  5227--5237.

\bibitem{zhang2022neural}
Y.~Zhang, K.~Zhou, and Z.~Liu, ``Neural prompt search,'' \emph{arXiv preprint
  arXiv:2206.04673}, 2022.

\bibitem{wah2011caltech}
C.~Wah, S.~Branson, P.~Welinder, P.~Perona, and S.~Belongie, ``The caltech-ucsd
  birds-200-2011 dataset,'' 2011.

\bibitem{van2015building}
G.~Van~Horn, S.~Branson, R.~Farrell, S.~Haber, J.~Barry, P.~Ipeirotis,
  P.~Perona, and S.~Belongie, ``Building a bird recognition app and large scale
  dataset with citizen scientists: The fine print in fine-grained dataset
  collection,'' in \emph{Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition}, 2015, pp. 595--604.

\bibitem{nilsback2008automated}
M.-E. Nilsback and A.~Zisserman, ``Automated flower classification over a large
  number of classes,'' in \emph{2008 Sixth Indian Conference on Computer
  Vision, Graphics \& Image Processing}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2008, pp. 722--729.

\bibitem{khosla2011novel}
A.~Khosla, N.~Jayadevaprakash, B.~Yao, and F.-F. Li, ``Novel dataset for
  fine-grained image categorization: Stanford dogs,'' in \emph{Proc. CVPR
  workshop on fine-grained visual categorization (FGVC)}, vol.~2, no.~1.\hskip
  1em plus 0.5em minus 0.4em\relax Citeseer, 2011.

\bibitem{gebru2017fine}
T.~Gebru, J.~Krause, Y.~Wang, D.~Chen, J.~Deng, and L.~Fei-Fei, ``Fine-grained
  car detection for visual census estimation,'' in \emph{Proceedings of the
  AAAI Conference on Artificial Intelligence}, vol.~31, no.~1, 2017.

\bibitem{zhai2019large}
X.~Zhai, J.~Puigcerver, A.~Kolesnikov, P.~Ruyssen, C.~Riquelme, M.~Lucic,
  J.~Djolonga, A.~S. Pinto, M.~Neumann, A.~Dosovitskiy \emph{et~al.}, ``A
  large-scale study of representation learning with the visual task adaptation
  benchmark,'' \emph{arXiv preprint arXiv:1910.04867}, 2019.

\bibitem{zaken2022bitfit}
E.~B. Zaken, Y.~Goldberg, and S.~Ravfogel, ``Bitfit: Simple parameter-efficient
  fine-tuning for transformer-based masked language-models,'' in
  \emph{Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 2: Short Papers)}, 2022, pp. 1--9.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative
  style, high-performance deep learning library,'' \emph{Advances in Neural
  Information Processing Systems (NeurIPS)}, vol.~32, pp. 8026--8037, 2019.

\bibitem{yun2019cutmix}
S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo, ``Cutmix:
  Regularization strategy to train strong classifiers with localizable
  features,'' in \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, 2019, pp. 6023--6032.

\bibitem{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz, ``mixup: Beyond empirical
  risk minimization,'' \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna, ``Rethinking the
  inception architecture for computer vision,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2016, pp.
  2818--2826.

\bibitem{cai2020tinytl}
H.~Cai, C.~Gan, L.~Zhu, and S.~Han, ``Tinytl: Reduce memory, not parameters for
  efficient on-device learning,'' \emph{Advances in Neural Information
  Processing Systems (NeurIPS)}, vol.~33, pp. 11\,285--11\,297, 2020.

\end{thebibliography}
