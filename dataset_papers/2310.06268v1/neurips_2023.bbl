\begin{thebibliography}{10}

\bibitem{agarwal2020optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov decision processes.
\newblock In {\em Conference on Learning Theory}, pages 64--66. PMLR, 2020.

\bibitem{anthony1999neural}
Martin Anthony, Peter~L Bartlett, Peter~L Bartlett, et~al.
\newblock {\em Neural Network Learning: Theoretical Foundations}, volume~9.
\newblock Cambridge University Press, 1999.

\bibitem{antos2007value}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Value-iteration based fitted policy iteration: learning with a single trajectory.
\newblock In {\em 2007 IEEE international symposium on approximate dynamic programming and reinforcement learning}, pages 330--337. IEEE, 2007.

\bibitem{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path.
\newblock {\em Machine Learning}, 71(1):89--129, 2008.

\bibitem{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function approximation.
\newblock In {\em Machine Learning Proceedings 1995}, pages 30--37. Elsevier, 1995.

\bibitem{bao2011improving}
Jiansong Bao, Heather~R Gilbertson, Robyn Gray, Diane Munns, Gabrielle Howard, Peter Petocz, Stephen Colagiuri, and Jennie~C Brand-Miller.
\newblock Improving the estimation of mealtime insulin dose in adults with type 1 diabetes: the normal insulin demand for dose adjustment (nidda) study.
\newblock {\em Diabetes Care}, 34(10):2146--2151, 2011.

\bibitem{beck2017first}
Amir Beck.
\newblock {\em First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem{blumer1989learnability}
Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred~K Warmuth.
\newblock Learnability and the vapnik-chervonenkis dimension.
\newblock {\em Journal of the ACM (JACM)}, 36(4):929--965, 1989.

\bibitem{boyd2004convex}
Stephen Boyd, Stephen~P Boyd, and Lieven Vandenberghe.
\newblock {\em Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 1042--1051. PMLR, 2019.

\bibitem{chen2022offline}
Jinglin Chen and Nan Jiang.
\newblock Offline reinforcement learning under value and density-ratio realizability: the power of gaps.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 378--388. PMLR, 2022.

\bibitem{cheng2022adversarially}
Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal.
\newblock Adversarially trained actor critic for offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 3852--3878. PMLR, 2022.

\bibitem{choi2003feature}
Euisun Choi and Chulhee Lee.
\newblock Feature extraction based on bhattacharyya distance.
\newblock {\em Pattern Recognition}, 36(8):1703--1709, 2003.

\bibitem{dai2020coindice}
Bo~Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesv{\'a}ri, and Dale Schuurmans.
\newblock Coindice: Off-policy confidence interval estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 33:9398--9411, 2020.

\bibitem{dai2018sbeed}
Bo~Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le~Song.
\newblock Sbeed: Convergent reinforcement learning with nonlinear function approximation.
\newblock In {\em International Conference on Machine Learning}, pages 1125--1134. PMLR, 2018.

\bibitem{duan2020minimax}
Yaqi Duan, Zeyu Jia, and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function approximation.
\newblock In {\em International Conference on Machine Learning}, pages 2701--2709. PMLR, 2020.

\bibitem{ernst2005tree}
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
\newblock Tree-based batch mode reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 6:503--556, 2005.

\bibitem{farahmand2016regularized}
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv{\'a}ri, and Shie Mannor.
\newblock Regularized policy iteration with nonparametric function spaces.
\newblock {\em The Journal of Machine Learning Research}, 17(1):4809--4874, 2016.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{fudenberg1991game}
Drew Fudenberg and Jean Tirole.
\newblock {\em Game theory}.
\newblock MIT press, 1991.

\bibitem{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:20132--20145, 2021.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages 2052--2062. PMLR, 2019.

\bibitem{ghavamzadeh2016safe}
Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow.
\newblock Safe policy improvement by minimizing robust baseline regret.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{gretton2012kernel}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and Alexander Smola.
\newblock A kernel two-sample test.
\newblock {\em The Journal of Machine Learning Research}, 13(1):723--773, 2012.

\bibitem{haarnoja2018softac}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock {\em arXiv preprint arXiv:1812.05905}, 2018.

\bibitem{haussler1995sphere}
David Haussler.
\newblock Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-chervonenkis dimension.
\newblock {\em Journal of Combinatorial Theory, Series A}, 69(2):217--232, 1995.

\bibitem{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In {\em 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018}, pages 876--885. Association For Uncertainty in Artificial Intelligence (AUAI), 2018.

\bibitem{jiang2020minimax}
Nan Jiang and Jiawei Huang.
\newblock Minimax value interval for off-policy evaluation and policy optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33:2747--2758, 2020.

\bibitem{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR, 2020.

\bibitem{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In {\em International Conference on Machine Learning}, pages 5084--5096. PMLR, 2021.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{konda1999actor}
Vijay Konda and John Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock {\em Advances in neural information processing systems}, 12, 1999.

\bibitem{konda2003onactor}
Vijay~R Konda and John~N Tsitsiklis.
\newblock Onactor-critic algorithms.
\newblock {\em SIAM journal on Control and Optimization}, 42(4):1143--1166, 2003.

\bibitem{kostrikov2021offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock {\em arXiv preprint arXiv:2110.06169}, 2021.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33:1179--1191, 2020.

\bibitem{lagoudakis2003least}
Michail~G Lagoudakis and Ronald Parr.
\newblock Least-squares policy iteration.
\newblock {\em The Journal of Machine Learning Research}, 4:1107--1149, 2003.

\bibitem{laroche2019safe}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In {\em International conference on machine learning}, pages 3652--3661. PMLR, 2019.

\bibitem{lee2020batch}
Byungjun Lee, Jongmin Lee, Peter Vrancx, Dongho Kim, and Kee-Eung Kim.
\newblock Batch reinforcement learning with hyperparameter gradients.
\newblock In {\em International Conference on Machine Learning}, pages 5725--5735. PMLR, 2020.

\bibitem{lee2021optidice}
Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim.
\newblock Optidice: Offline policy optimization via stationary distribution correction estimation.
\newblock In {\em International Conference on Machine Learning}, pages 6120--6130. PMLR, 2021.

\bibitem{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem{li2015toward}
Lihong Li, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Toward minimax off-policy value estimation.
\newblock In {\em Artificial Intelligence and Statistics}, pages 608--616. PMLR, 2015.

\bibitem{li2023quasi}
Yuhan Li, Wenzhuo Zhou, and Ruoqing Zhu.
\newblock Quasi-optimal reinforcement learning with continuous actions.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{liu2020understanding}
Yao Liu, Pierre-Luc Bacon, and Emma Brunskill.
\newblock Understanding the curse of horizon in off-policy evaluation via conditional importance sampling.
\newblock In {\em International Conference on Machine Learning}, pages 6184--6193. PMLR, 2020.

\bibitem{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch off-policy reinforcement learning without great exploration.
\newblock {\em Advances in neural information processing systems}, 33:1264--1274, 2020.

\bibitem{luckett2020estimating}
Daniel~J Luckett, Eric~B Laber, Anna~R Kahkoska, David~M Maahs, Elizabeth Mayer-Davis, and Michael~R Kosorok.
\newblock Estimating dynamic treatment regimes in mobile health using v-learning.
\newblock {\em Journal of the American Statistical Association}, 115(530):692--706, 2020.

\bibitem{mandel2014offline}
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic.
\newblock Offline policy evaluation across representations with applications to educational games.
\newblock In {\em AAMAS}, volume 1077, 2014.

\bibitem{marling2020ohiot1dm}
Cindy Marling and Razvan Bunescu.
\newblock The ohiot1dm dataset for blood glucose level prediction: Update 2020.
\newblock {\em KHD@ IJCAI}, 2020.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness, Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{mokhtari2020unified}
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil.
\newblock A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 1497--1507. PMLR, 2020.

\bibitem{murphy2001marginal}
Susan~A Murphy, Mark~J van~der Laan, James~M Robins, and Conduct Problems Prevention~Research Group.
\newblock Marginal mean models for dynamic regimes.
\newblock {\em Journal of the American Statistical Association}, 96(456):1410--1423, 2001.

\bibitem{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{nachum2020reinforcement}
Ofir Nachum and Bo~Dai.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock {\em arXiv preprint arXiv:2001.01866}, 2021.

\bibitem{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock {\em arXiv preprint arXiv:1912.02074}, 2019.

\bibitem{nesterov2018lectures}
Yurii Nesterov et~al.
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock {\em Foundations and trends{\textregistered} in Optimization}, 1(3):127--239, 2014.

\bibitem{pollard1990empirical}
David Pollard.
\newblock Empirical processes: Theory and applications.
\newblock In {\em NSF-CBMS Regional Conference Series in Probability and Statistics}, pages i--86. JSTOR, 1990.

\bibitem{precup2000eligibility}
Doina Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock {\em Computer Science Department Faculty Publication Series}, page~80, 2000.

\bibitem{puterman2014markov}
Martin~L Puterman.
\newblock {\em Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A tale of pessimism.
\newblock {\em Advances in Neural Information Processing Systems}, 34:11702--11716, 2021.

\bibitem{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em International conference on machine learning}, pages 314--323. PMLR, 2016.

\bibitem{reem2019re}
Daniel Reem, Simeon Reich, and Alvaro De~Pierro.
\newblock Re-examination of bregman functions and new properties of their divergences.
\newblock {\em Optimization}, 68(1):279--348, 2019.

\bibitem{renyi1961measures}
Alfr{\'e}d R{\'e}nyi.
\newblock On measures of entropy and information.
\newblock In {\em Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics}, volume~4, pages 547--562. University of California Press, 1961.

\bibitem{riedmiller2005neural}
Martin Riedmiller.
\newblock Neural fitted q iteration--first experiences with a data efficient neural reinforcement learning method.
\newblock In {\em European conference on machine learning}, pages 317--328. Springer, 2005.

\bibitem{rodbard2009interpretation}
David Rodbard.
\newblock Interpretation of continuous glucose monitoring data: glycemic variability and quality of glycemic control.
\newblock {\em Diabetes Technology \& Therapeutics}, 11(S1):S--55, 2009.

\bibitem{scherrer2012use}
Bruno Scherrer and Boris Lesner.
\newblock On the use of non-stationary policies for stationary infinite-horizon markov decision processes.
\newblock {\em Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem{seno2022d3rlpy}
Takuma Seno and Michita Imai.
\newblock d3rlpy: An offline deep reinforcement learning library.
\newblock {\em The Journal of Machine Learning Research}, 23(1):14205--14224, 2022.

\bibitem{shi2022minimax}
Chengchun Shi, Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages 20057--20094. PMLR, 2022.

\bibitem{shi2020statistical}
Chengchun Shi, Sheng Zhang, Wenbin Lu, and Rui Song.
\newblock Statistical inference of the value function for reinforcement learning in infinite horizon settings.
\newblock {\em arXiv preprint arXiv:2001.04515}, 2020.

\bibitem{silverman1986density}
Bernard~W Silverman.
\newblock {\em Density estimation for statistics and data analysis}, volume~26.
\newblock CRC press, 1986.

\bibitem{sriperumbudur2011universality}
Bharath~K Sriperumbudur, Kenji Fukumizu, and Gert~RG Lanckriet.
\newblock Universality, characteristic kernels and rkhs embedding of measures.
\newblock {\em Journal of Machine Learning Research}, 12(7), 2011.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{theocharous2020reinforcement}
Georgios Theocharous, Yash Chandak, Philip~S Thomas, and Frits de~Nijs.
\newblock Reinforcement learning for strategic recommendations.
\newblock {\em arXiv preprint arXiv:2009.07346}, 2020.

\bibitem{thomas2015high}
Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.
\newblock High-confidence off-policy evaluation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~29, 2015.

\bibitem{uehara2020minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock Minimax weight and q-function learning for off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages 9659--9668. PMLR, 2020.

\bibitem{uehara2022pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline reinforcement learning under partial coverage.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{vapnik2015uniform}
Vladimir~N Vapnik and A~Ya Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their probabilities.
\newblock {\em Measures of Complexity: Festschrift for Alexey Chervonenkis}, pages 11--30, 2015.

\bibitem{von2010market}
Heinrich Von~Stackelberg.
\newblock {\em Market structure and equilibrium}.
\newblock Springer Science \& Business Media, 2010.

\bibitem{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint}, volume~48.
\newblock Cambridge University Press, 2019.

\bibitem{wang2021instabilities}
Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham Kakade.
\newblock Instabilities of offline rl with pre-trained neural representation.
\newblock In {\em International Conference on Machine Learning}, pages 10948--10960. PMLR, 2021.

\bibitem{wu2021offline}
Runzhe Wu, Yufeng Zhang, Zhuoran Yang, and Zhaoran Wang.
\newblock Offline constrained multi-objective reinforcement learning via pessimistic dual value iteration.
\newblock {\em Advances in Neural Information Processing Systems}, 34:25439--25451, 2021.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{wu2021uncertainty}
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh.
\newblock Uncertainty weighted actor-critic for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2105.08140}, 2021.

\bibitem{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34, 2021.

\bibitem{xie2020q}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A theoretical comparison.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}, pages 550--559. PMLR, 2020.

\bibitem{xie2019towards}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{yang2020reinforcement}
Lin Yang and Mengdi Wang.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound.
\newblock In {\em International Conference on Machine Learning}, pages 10746--10756. PMLR, 2020.

\bibitem{yin2021near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near-optimal offline reinforcement learning via double variance reduction.
\newblock {\em Advances in neural information processing systems}, 34:7677--7688, 2021.

\bibitem{yu2021combo}
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock {\em Advances in neural information processing systems}, 34:28954--28967, 2021.

\bibitem{zanette2021cautiously}
Andrea Zanette, Ching-An Cheng, and Alekh Agarwal.
\newblock Cautiously optimistic policy optimization and exploration with linear function approximation.
\newblock In {\em Conference on Learning Theory}, pages 4473--4525. PMLR, 2021.

\bibitem{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:13626--13640, 2021.

\bibitem{zhan2022offline}
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee.
\newblock Offline reinforcement learning with realizability and single-policy concentrability.
\newblock In {\em Conference on Learning Theory}, pages 2730--2775. PMLR, 2022.

\bibitem{zhou2023distributional}
Wenzhuo Zhou, Yuhan Li, Ruoqing Zhu, and Annie Qu.
\newblock Distributional shift-aware off-policy interval estimation: A unified error quantification framework.
\newblock {\em arXiv preprint arXiv:2309.13278}, 2023.

\bibitem{zhou2022estimating}
Wenzhuo Zhou, Ruoqing Zhu, and Annie Qu.
\newblock Estimating optimal infinite horizon dynamic treatment regimes via pt-learning.
\newblock {\em Journal of the American Statistical Association}, pages 1--14, 2022.

\bibitem{zhu2020causal}
Liangyu Zhu, Wenbin Lu, and Rui Song.
\newblock Causal effect estimation and optimal dose suggestions in mobile health.
\newblock In {\em International Conference on Machine Learning}, pages 11588--11598. PMLR, 2020.

\end{thebibliography}
