@inproceedings{dann2019policy,
author = {Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
booktitle = {International Conference on Machine Learning},
title = {{Policy certificates: Towards accountable reinforcement learning}},
url = {https://arxiv.org/pdf/1811.03056.pdf},
year = {2019}
}



@inproceedings{talebi2018variance,
author = {Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
booktitle = {Algorithmic Learning Theory},
title = {{Variance-aware regret bounds for undiscounted reinforcement learning in MDPs}},
url = {https://arxiv.org/pdf/1803.01626.pdf},
year = {2018}
}



@inproceedings{sidford2018near,
author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F. and Ye, Yinyu},
booktitle = {Neural Information Processing Systems},
title = {{Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model}},
url = {https://arxiv.org/pdf/1806.01492.pdf},
year = {2018}
}





@inproceedings{agarwal2020model,
author = {Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
booktitle = {Conference on Learning Theory},
title = {{Model-based reinforcement learning with a generative model is minimax optimal}},
url = {https://arxiv.org/pdf/1906.03804.pdf},
year = {2020}
}



@inproceedings{bartlett2009regal,
author = {Bartlett, Peter L. and Tewari, Ambuj},
booktitle = {Uncertainty in Artificial Intelligence},
keywords = {bandits},
mendeley-tags = {bandits},
title = {{REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs}},
url = {https://arxiv.org/pdf/1205.2661.pdf},
year = {2009}
}



@inproceedings{auer2009near,
author = {Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
booktitle = {Neural Information Processing Systems},
title = {{Near-optimal regret bounds for reinforcement learning}},
url = {https://papers.nips.cc/paper/3401-near-optimal-regret-bounds-for-reinforcement-learning.pdf},
year = {2009}
}



@article{azar2013minimax,
abstract = {We consider the problems of learning the optimal action-value function and the optimal policy in discounted-reward Markov decision processes (MDPs). We prove new PAC bounds on the sample-complexity of two well-known model-based reinforcement learning (RL) algorithms in the presence of a generative model of the MDP: value iteration and policy iteration. The first result indicates that for an MDP with N state-action pairs and the discount factor $\gamma${\^{a}}̂̂[0,1) only O(Nlog(N/$\delta$)/((1-$\gamma$)3 $\epsilon$ 2)) state-transition samples are required to find an $\epsilon$-optimal estimation of the action-value function with the probability (w.p.) 1-$\delta$. Further, we prove that, for small values of $\epsilon$, an order of O(Nlog(N/$\delta$)/((1-$\gamma$)3 $\epsilon$ 2)) samples is required to find an $\epsilon$-optimal policy w.p. 1-$\delta$. We also prove a matching lower bound of $\Theta$(Nlog(N/$\delta$)/((1-$\gamma$)3 $\epsilon$ 2)) on the sample complexity of estimating the optimal action-value function with $\epsilon$ accuracy. To the best of our knowledge, this is the first minimax result on the sample complexity of RL: the upper bounds match the lower bound in terms of N, $\epsilon$, $\delta$ and 1/(1-$\gamma$) up to a constant factor. Also, both our lower bound and upper bound improve on the state-of-the-art in terms of their dependence on 1/(1-$\gamma$). {\textcopyright} 2013 The Author(s).},
author = {Azar,  Mohammad Gheshlaghi and Munos, R{\'{e}}mi and Kappen, Hilbert J.},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Gheshlaghi Azar et al. - 2013 - Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model bounds on.pdf:pdf},
journal = {Machine Learning},
number = {3},
pages = {325--349},
title = {{Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model}},
url = {https://hal.archives-ouvertes.fr/hal-00831875},
volume = {91},
year = {2013}
}


@article{zhang2020task-agnostic,
author = {Zhang, Xuezhou and Ma, Yuzhe and Singla, Adish},
journal = {arXiv preprint: arXiv:2006.09497},
title = {{Task-agnostic exploration in reinforcement learning}},
url = {https://arxiv.org/pdf/2006.09497.pdf},
year = {2020}
}


@article{wang2020on,
archivePrefix = {arXiv},
arxivId = {cs.LG/2006.11274},
author = {Wang, Ruosong and Du, Simon S and Yang, Lin F and Salakhutdinov, Ruslan},
eprint = {2006.11274},
journal = {arXiv preprint arXiv:2006.11274},
primaryClass = {cs.LG},
title = {{On reward-free reinforcement learning with linear function approximation}},
url = {https://arxiv.org/pdf/2006.11274.pdf},
year = {2020}
}



@article{garivier2019explore,
author = {Garivier, Aur{\'{e}}lien and M{\'{e}}nard, Pierre and Stoltz, Gilles},
journal = {Mathematics of Operations Research},
number = {2},
pages = {377--399},
title = {{Explore first, exploit next: The true shape of regret in bandit problems}},
url = {https://arxiv.org/pdf/1602.07182.pdf},
volume = {44},
year = {2019}
}



@book{boucheron2013concentration,
author = {Boucheron, St{\'{e}}phane and Lugosi, G{\'{a}}bor and Massart, Pascal},
publisher = {Oxford University Press},
title = {{Concentration inequalities}},
url = {https://www.hse.ru/data/2016/11/24/1113029206/Concentration inequalities.pdf},
year = {2013}
}


@article{jonsson2020planning,
abstract = {We propose MDP-GapE, a new trajectory-based Monte-Carlo Tree Search algorithm for planning in a Markov Decision Process in which transitions have a finite support. We prove an upper bound on the number of calls to the generative models needed for MDP-GapE to identify a near-optimal action with high probability. This problem-dependent sample complexity result is expressed in terms of the sub-optimality gaps of the state-action pairs that are visited during exploration. Our experiments reveal that MDP-GapE is also effective in practice, in contrast with other algorithms with sample complexity guarantees in the fixed-confidence setting, that are mostly theoretical.},
author = {Jonsson, Anders and Kaufmann, Emilie and M{\'{e}}nard, Pierre and Domingues, Omar Darwiche and Leurent, Edouard and Valko, Michal},
journal = {arXiv preprint arXiv:2006.05879},
month = {jun},
title = {{Planning in markov decision processes with gap-dependent sample complexity}},
url = {http://arxiv.org/abs/2006.05879},
year = {2020}
}






@article{domingues2020regret,
  title={Regret bounds for kernel-based reinforcement learning},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Pirotta, Matteo and Kaufmann, Emilie and Valko, Michal},
  journal={arXiv preprint arXiv:2004.05599},
  url = {http://arxiv.org/abs/2004.05599},
  year={2020}
}


@article{lattimore2016end,
  title={The end of optimism? an asymptotic analysis of finite-armed linear bandits},
  author={Lattimore, Tor and Szepesvari, Csaba},
  journal={arXiv preprint arXiv:1610.04491},
  year={2016}
}

@article{kaufmann2020adaptive,
abstract = {Reward-free exploration is a reinforcement learning setting recently studied by Jin et al., who address it by running several algorithms with regret guarantees in parallel. In our work, we instead propose a more adaptive approach for reward-free exploration which directly reduces upper bounds on the maximum MDP estimation error. We show that, interestingly, our reward-free UCRL algorithm can be seen as a variant of an algorithm of Fiechter from 1994, originally proposed for a different objective that we call best-policy identification. We prove that RF-UCRL needs O((SAH4/$\epsilon$2)ln(1/$\delta$)) episodes to output, with probability 1−$\delta$, an $\epsilon$-approximation of the optimal policy for any reward function. We empirically compare it to oracle strategies using a generative model.},
archivePrefix = {arXiv},
arxivId = {2006.06294},
author = {Kaufmann, Emilie and M{\'{e}}nard, Pierre and Domingues, Omar Darwiche and Jonsson, Anders and Leurent, Edouard and Valko, Michal},
eprint = {2006.06294},
journal = {arXiv preprint arXiv:2006.06294},
title = {{Adaptive reward-free exploration}},
url = {https://arxiv.org/pdf/2006.06294.pdf},
year = {2020}
}



@inproceedings{hazan2019provably,
author = {Hazan, Elad and Kakade, Sham and Singh, Karan and Soest, Abby Van},
booktitle = {International Conference on Machine Learning},
title = {{Provably efficient maximum entropy exploration}},
url = {https://arxiv.org/pdf/1812.02690.pdf},
year = {2019}
}


@inproceedings{tang2017exploration,
  title={\# exploration: A study of count-based exploration for deep reinforcement learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and DeTurck, Filip and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2753--2762},
  year={2017}
}

@inproceedings{strehl2006pac,
  title={PAC model-free reinforcement learning},
  author={Strehl, Alexander L and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={881--888},
  year={2006}
}

@inproceedings{ostrovski2017count,
  title={Count-based exploration with neural density models},
  author={Ostrovski, Georg and Bellemare, Marc G and van den Oord, A{\"a}ron and Munos, R{\'e}mi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2721--2730},
  year={2017},
  organization={JMLR. org}
}


@misc{gajane2019autonomous,
    title={Autonomous exploration for navigating in non-stationary CMPs},
    author={Pratik Gajane and Ronald Ortner and Peter Auer and Csaba Szepesvari},
    year={2019},
    eprint={1910.08446},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{lim2012autonomous,
  title={Autonomous exploration for navigating in mdps},
  author={Lim, Shiau Hong and Auer, Peter},
  booktitle={Conference on Learning Theory},
  pages={40--1},
  year={2012}
}



@article{cohen2020near,
  title={Near-optimal Regret Bounds for Stochastic Shortest Path},
  author={Cohen, Alon and Kaplan, Haim and Mansour, Yishay and Rosenberg, Aviv},
  journal={arXiv preprint arXiv:2002.09869},
  year={2020}
}

@article{tarbouriech2019no,
  title={No-Regret Exploration in Goal-Oriented Reinforcement Learning},
  author={Tarbouriech, Jean and Garcelon, Evrard and Valko, Michal and Pirotta, Matteo and Lazaric, Alessandro},
  journal={arXiv preprint arXiv:1912.03517},
  year={2019}
}


@inproceedings{schmidhuber1991possibility,
  title={A possibility for implementing curiosity and boredom in model-building neural controllers},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Proc. of the international conference on simulation of adaptive behavior: From animals to animats},
  pages={222--227},
  year={1991}
}


@inproceedings{hren2008optimistic,
	title = {{Optimistic planning of deterministic systems}},
	year = {2008},
	booktitle = {European Workshop on Reinforcement Learning},
	author = {Hren, Jean-Francois and Munos, Rémi}
}

@inproceedings{coquelin2007bandit,
	title = {{Bandit algorithms for tree search}},
	year = {2007},
	booktitle = {Uncertainty in Artificial Intelligence},
	author = {Coquelin, Pierre-Arnaud and Munos, Rémi},
	url = {https://arxiv.org/pdf/1408.2028.pdf}
}

@inproceedings{Zanette19Euler,
  author    = {Andrea Zanette and
               Emma Brunskill},
  title     = {Tighter Problem-Dependent Regret Bounds in Reinforcement Learning
               without Domain Knowledge using Value Function Bounds},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               (ICML)},
  year      = {2019}
}

@inproceedings{hamrick2019combining,
title={Combining Q-Learning and Search with Amortized Value Estimates},
author={Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Tobias Pfaff and Theophane Weber and Lars Buesing and Peter W. Battaglia},
booktitle={International Conference on Learning Representations},
year={2020}
}

@article{kearns02E3,
  author    = {Michael J. Kearns and
               Satinder P. Singh},
  title     = {Near-Optimal Reinforcement Learning in Polynomial Time},
  journal   = {Machine Learning},
  volume    = {49},
  number    = {2-3},
  pages     = {209--232},
  year      = {2002}
}


@book{cover2006elements,
author = {Cover, Thomas M. and Thomas, Joy A.},
publisher = {John Wiley {\&} Sons},
title = {{Elements of information theory}},
url = {https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954},
year = {2006}
}


@article{de2004self,
author = {de la Pe{\~{n}}a, Victor H. and Klass, Michael J. and Lai, Tze Leung},
journal = {Annals of probability},
pages = {1902--1933},
title = {{Self-normalized processes: {E}xponential inequalities, moment bounds and iterated logarithm laws}},
url = {https://arxiv.org/pdf/math/0410102.pdf},
volume = {32},
year = {2004}
}




@inproceedings{garivier2011kl,
  title={The KL-UCB algorithm for bounded stochastic bandits and beyond},
  author={Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  booktitle={Proceedings of the 24th annual conference on learning theory},
  pages={359--376},
  year={2011}
}

@Article{KLUCBJournal,
  Title                    = {{{K}ullback-{L}eibler upper confidence bounds for optimal sequential allocation}},
  Author                   = {Capp{\'e}, O. and Garivier, A. and Maillard, O-A. and Munos, R. and Stoltz, G.},
  Journal                  = {Annals of Statistics},
  Year                     = {2013},
  Pages                    = {1516--1541},
  Volume                   = {41(3)}
}

@inproceedings{dann2017unifying,
author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
booktitle = {Neural Information Processing Systems},
 year={2017},
title = {{Unifying {PAC} and regret: Uniform {PAC} bounds for episodic reinforcement learning}},
url = {https://arxiv.org/pdf/1703.07710.pdf}
}


@inproceedings{jin2020reward-free,
abstract = {Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose a new "reward-free RL" framework. In the exploration phase, the agent first collects trajectories from an MDP {\$}\backslashmathcal{\{}M{\}}{\$} without a pre-specified reward function. After exploration, it is tasked with computing near-optimal policies under for {\$}\backslashmathcal{\{}M{\}}{\$} for a collection of given reward functions. This framework is particularly suitable when there are many reward functions of interest, or when the reward function is shaped by an external agent to elicit desired behavior. We give an efficient algorithm that conducts {\$}\backslashtilde{\{}\backslashmathcal{\{}O{\}}{\}}(S{\^{}}2A\backslashmathrm{\{}poly{\}}(H)/\backslashepsilon{\^{}}2){\$} episodes of exploration and returns {\$}\backslashepsilon{\$}-suboptimal policies for an arbitrary number of reward functions. We achieve this by finding exploratory policies that visit each "significant" state with probability proportional to its maximum visitation probability under any possible policy. Moreover, our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. We also give a nearly-matching {\$}\backslashOmega(S{\^{}}2AH{\^{}}2/\backslashepsilon{\^{}}2){\$} lower bound, demonstrating the near-optimality of our algorithm in this setting.},
archivePrefix = {arXiv},
arxivId = {2002.02794},
author = {Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
booktitle = {International Conference on Machine Learning},
eprint = {2002.02794},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - 2020 - Reward-Free Exploration for Reinforcement Learning.pdf:pdf},
title = {{Reward-free exploration for reinforcement learning}},
url = {http://arxiv.org/abs/2002.02794},
year = {2020}
}


@inproceedings{jin2018is,
author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, S{\'{e}}bastien and Jordan, Michael I.},
booktitle = {Neural Information Processing Systems},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - 2018 - Is Q-learning Provably Efficient.pdf:pdf},
title = {{Is Q-learning provably efficient?}},
url = {https://arxiv.org/pdf/1807.03765.pdf},
year = {2018}
}




@article{garivier2018kl,
  title={KL-UCB-switch: optimal regret bounds for stochastic bandits from both a distribution-dependent and a distribution-free viewpoints},
  author={Garivier, Aur{\'e}lien and Hadiji, H{\'e}di and Menard, Pierre and Stoltz, Gilles},
  journal={arXiv preprint arXiv:1805.05071},
  year={2018}
}

@book{puterman1994markov,
address = {New York, NY},
author = {Puterman, Martin L.},
howpublished = {Hardcover},
isbn = {0471619779},
publisher = {John Wiley {\&} Sons},
title = {{Markov decision processes: Discrete stochastic dynamic programming}},
url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887},
year = {1994}
}



@Book{SurveyRemiMCTS,
  Title                    = {From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning.},
  Author                   = {Munos, R.},
  Publisher                = {Foundations and Trends in Machine Learning},
  Year                     = {2014},
  Number                   = {1},
  Volume                   = {7}
}

@inproceedings{SmoothCruiser19,
  author    = {Jean{-}Bastien Grill and
               Omar Darwiche Domingues and
               Pierre M{\'{e}}nard and
               R{\'{e}}mi Munos and
               Michal Valko},
  title     = {Planning in entropy-regularized Markov decision processes and games},
  booktitle = {Neural Information Processing Systems},
  year      = {2019}
}

@inproceedings{Huang17StructuredBAI,
  author    = {Ruitong Huang and
               Mohammad M. Ajallooeian and
               Csaba Szepesv{\'{a}}ri and
               Martin M{\"{u}}ller},
  title     = {Structured Best Arm Identification with Fixed Confidence},
  booktitle = {International Conference on Algorithmic Learning Theory (ALT)},
  year      = {2017}
}


@InProceedings{Kocsis06UCT,
  Title                    = {Bandit Based Monte-carlo Planning},
  Author                   = {Kocsis, Levente and Szepesv\'{a}ri, Csaba},
  Booktitle                = {Proceedings of the 17th European Conference on Machine Learning (ECML)},
  Year                     = {2006}
}


@article{even2006action,
author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
journal = {Journal of Machine Learning Research},
pages = {1079--1105},
title = {{Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems}},
url = {https://jmlr.csail.mit.edu/papers/volume7/evendar06a/evendar06a.pdf},
volume = {7},
year = {2006}
}


@book{BanditBook,
author = {Lattimore, Tor and Szepesvari, Csaba},
publisher = {Cambridge University Press},
title = {{Ba
@inproceedinndit Algorithms}},
year = {2019}
}

@Article{Aueral02,
  Title                    = {{Finite-time analysis of the multiarmed bandit problem}},
  Author                   = {Auer, P. and Cesa-Bianchi, N. and Fischer, P.},
  Journal                  = {Machine Learning},
  Year                     = {2002},
  Number                   = {2},
  Pages                    = {235--256},
  Volume                   = {47},
  Publisher                = {Springer}
}

@article{jaksch2010near,
author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
journal = {Journal of Machine Learning Research},
keywords = {bandits},
mendeley-tags = {bandits},
pages = {1563--1600},
title = {{Near-optimal regret bounds for reinforcement learning}},
url = {http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf},
volume = {99},
year = {2010}
}

@inproceedings{Tolpin12SRMCTS,
  author    = {David Tolpin and
               Solomon Eyal Shimony},
  title     = {{MCTS} Based on Simple Regret},
  booktitle = {Proceedings of the Twenty-Sixth {AAAI} Conference on Artificial Intelligence,
               July 22-26, 2012, Toronto, Ontario, Canada.},
  year      = {2012}
}

@inproceedings{Pepels14SimpleMCTS,
  author    = {Tom Pepels and
               Tristan Cazenave and
               Mark H. M. Winands and
               Marc Lanctot},
  title     = {Minimizing Simple and Cumulative Regret in Monte-Carlo Tree Search},
  booktitle = {Third Workshop on Computer Games (CGW)},
  pages     = {1--15},
  year      = {2014}
}

@article{Feldman14BRUE,
  author    = {Zohar Feldman and
               Carmel Domshlak},
  title     = {Simple Regret Optimization in Online Planning for Markov Decision
               Processes},
  journal   = {Journal of Artifial Intelligence Research},
  volume    = {51},
  pages     = {165--205},
  year      = {2014}
}



@article{Kearns02SS,
  author    = {Michael J. Kearns and
               Yishay Mansour and
               Andrew Y. Ng},
  title     = {A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov
               Decision Processes},
  journal   = {Machine Learning},
  volume    = {49},
  number    = {2-3},
  pages     = {193--208},
  year      = {2002}
}

@InProceedings{STOP14,
  Title                    = {Optimistic Planning in Markov Decision Processes using a generative model},
  Author                   = {Szorenyi, B. and Kedenburg, G. and Munos, R.},
  Booktitle                = {Advances in Neural Information Processing Systems (NIPS)},
  Year                     = {2014}
}

@Book{SuttonBarto98,
  Title                    = {Reinforcement Learning: an Introduction},
  Author                   = {Sutton, R. and Barto, A.},
  Publisher                = {MIT press},
  Year                     = {1998},

  Owner                    = {emilie},
  Timestamp                = {2016.11.07}
}


@article{AlphaZero,
  author    = {David Silver and
               Thomas Hubert and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Matthew Lai and
               Arthur Guez and
               Marc Lanctot and
               Laurent Sifre and
               Dharshan Kumaran and
               Thore Graepel and
               Timothy P. Lillicrap and
               Karen Simonyan and
               Demis Hassabis},
  title     = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  journal   = {Science},
  volume    = {362},
  issue = {6419},
  page = {1140-1144},
  year      = {2018},
}

@inproceedings{dann2015sample,
author = {Dann, Christoph and Brunskill, Emma},
booktitle = {Neural Information Processing Systems},
title = {{Sample complexity of episodic fixed-horizon reinforcement learning}},
url = {https://arxiv.org/pdf/1510.08906.pdf},
year = {2015}
}

@article{MuZero,
  author    = {Julian Schrittwieser and
               Ioannis Antonoglou and
               Thomas Hubert and
               Karen Simonyan and
               Laurent Sifre and
               Simon Schmitt and
               Arthur Guez and
               Edward Lockhart and
               Demis Hassabis and
               Thore Graepel and
               Timothy P. Lillicrap and
               David Silver},
  title     = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  journal   = {arXiv:1911.08265},
  year      = {2019}
}


@Article{SurveyMCTS12,
  Title                    = {A Survey of Monte Carlo Tree Search Methods},
  Author                   = {Browne, C. and Powley, E. and Whitehouse, D. and Lucas, S. and Cowling, P. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
  Journal                  = {IEEE Transactions on Computational Intelligence and AI in games,},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {1-49},
  Volume                   = {4}
}


@InProceedings{TrailBlazer16,
  Title                    = {Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning},
  Author                   = {Grill, J.-B. and Valko, M. and Munos, R.},
  Booktitle                = {Neural Information Processing Systems (NIPS)},
  Year                     = {2016}
}



@inproceedings{gabillon2012best,
  title={Best arm identification: A unified approach to fixed budget and fixed confidence},
  author={Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3212--3220},
  year={2012}
}



@inproceedings{azar2017minimax,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.05449v2},
author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'{e}}mi},
booktitle = {International Conference on Machine Learning},
eprint = {arXiv:1703.05449v2},
file = {:Users/valkom/Library/Application Support/Mendeley Desktop/Downloaded/Azar, Osband, Munos - 2017 - Minimax regret bounds for reinforcement learning.pdf:pdf},
title = {{Minimax regret bounds for reinforcement learning}},
url = {https://arxiv.org/pdf/1703.05449.pdf},
year = {2017}
}



@inproceedings{azar2012on,
author = {Azar, Mohammad Gheshlaghi and Munos, R{\'{e}}mi and Kappen, Bert},
booktitle = {International Conference on Machine Learning},
title = {{On the sample complexity of reinforcement learning with a generative model}},
url = {https://arxiv.org/pdf/1206.6461.pdf},
year = {2012}
}


@inproceedings{kearns1998finite-sample,
author = {Kearns, Michael J. and Singh, Satinder P.},
booktitle = {Neural Information Processing Systems},
title = {{Finite-sample convergence rates for Q-learning and indirect algorithms}},
url = {http://papers.neurips.cc/paper/1531-finite-sample-convergence-rates-for-q-learning-and-indirect-algorithms.pdf},
year = {1998}
}



@inproceedings{fiechter1994efficient,
author = {Fiechter, Claude-Nicolas},
booktitle = {Conference on Learning Theory},
title = {{Efficient reinforcement learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7F5F8FCD1AA7ED07356410DDD5B384FE?doi=10.1.1.49.8652\&rep=rep1\&type=pdf},
year = {1994}
}

@inproceedings{Fiechter97,
  author    = {Claude{-}Nicolas Fiechter},
  title     = {Expected Mistake Bound Model for On-Line Reinforcement Learning},
  booktitle = {Proceedings of the Fourteenth International Conference on Machine Learning (ICML)},
  year      = {1997}
}

@phdthesis{kakade2013on,
author = {Kakade, Sham},
school = {University College London},
title = {{On the sample complexity of reinforcement learning}},
url = {https://homes.cs.washington.edu/~sham/papers/thesis/sham_thesis.pdf},
year = {2003}
}



@InProceedings{filippi2010optimism,
  Title                    = {{Optimism in Reinforcement Learning and {K}ullback-{L}eibler Divergence}},
  Author                   = {Filippi, S. and Capp{\'e}, O. and Garivier, A.},
  Booktitle                = {{Allerton Conference on Communication, Control, and Computing}},
  Year                     = {2010},
}

@article{Brafman02RMAX,
  author    = {Ronen I. Brafman and
               Moshe Tennenholtz},
  title     = {{R-MAX} - {A} General Polynomial Time Algorithm for Near-Optimal Reinforcement
               Learning},
  journal   = {Journal of Machine Learning Research},
  volume    = {3},
  pages     = {213--231},
  year      = {2002}
}

@inproceedings{Strehl06DelayedQL,
  author    = {Alexander L. Strehl and
               Lihong Li and
               Eric Wiewiora and
               John Langford and
               Michael L. Littman},
  title     = {{PAC} model-free reinforcement learning},
  booktitle = {Proceedings of the Twenty-Third International Conference on Machine Learning (ICML},
  year      = {2006}
}

@article{Strehl08MBIE,
  author    = {Alexander L. Strehl and
               Michael L. Littman},
  title     = {An analysis of model-based Interval Estimation for Markov Decision
               Processes},
  journal   = {Journal of Computer and System Sciences},
  volume    = {74},
  number    = {8},
  pages     = {1309--1331},
  year      = {2008}
}

@inproceedings{bubeck2010open,
  title={Open Loop Optimistic Planning},
  author={Bubeck, S and Munos, R},
  booktitle={Conference on Learning Theory},
  year={2010}
}


@inproceedings{leurent2019practical,
	title={Practical Open-Loop Optimistic Planning},
	author={Edouard Leurent and Odalric-Ambrym Maillard},
	year={2019},
	booktitle={Proceedings of the 19th European Conference on Machine Learning and Principles and Practice (ECML-PKDD)}
}

@inproceedings{busoniu2012optimistic,
  title={Optimistic planning for Markov decision processes},
  author={Busoniu, Lucian and Munos, R{\'e}mi},
  booktitle={Artificial Intelligence and Statistics},
  pages={182--189},
  year={2012}
}

@incollection{NIPS2015_5668,
title = {Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning},
author = {Mohamed, Shakir and Jimenez Rezende, Danilo},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2125--2133},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5668-variational-information-maximisation-for-intrinsically-motivated-reinforcement-learning.pdf}
}

@misc{montufar2016information,
    title={Information Theoretically Aided Reinforcement Learning for Embodied Agents},
    author={Guido Montufar and Keyan Ghazi-Zahedi and Nihat Ay},
    year={2016},
    eprint={1605.09735},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article {PMID:22791268,
	Title = {An information-theoretic approach to curiosity-driven reinforcement learning},
	Author = {Still, Susanne and Precup, Doina},
	DOI = {10.1007/s12064-011-0142-z},
	Number = {3},
	Volume = {131},
	Month = {September},
	Year = {2012},
	Journal = {Theory in biosciences = Theorie in den Biowissenschaften},
	ISSN = {1431-7613},
	Pages = {139—148},
	URL = {https://doi.org/10.1007/s12064-011-0142-z},
}

@incollection{NIPS2004_2552,
title = {Intrinsically Motivated Reinforcement Learning},
author = {Nuttapong Chentanez and Andrew G. Barto and Satinder P. Singh},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {L. K. Saul and Y. Weiss and L. Bottou},
pages = {1281--1288},
year = {2005},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2552-intrinsically-motivated-reinforcement-learning.pdf}
}

@inproceedings{zanette2019tighter,
author = {Zanette, Andrea and Brunskill, Emma},
booktitle = {International Conference on Machine Learning},
title = {{Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds}},
url = {https://arxiv.org/pdf/1901.00210.pdf},
year = {2019}
}

@book{Boucheron2013,
	TITLE = {{Concentration inequalities : a non asymptotic theory of independence}},
	AUTHOR = {Boucheron, St{\'e}phane and Lugosi, Gabor and Massart, Pascal},
	URL = {https://hal.inria.fr/hal-00942704},
	PUBLISHER = {{Oxford University Press}},
	PAGES = {481},
	YEAR = {2013},
	HAL_ID = {hal-00942704},
	HAL_VERSION = {v1},
}

@book{durrett2010probability,
author = {Durrett, Rick},
edition = {4},
isbn = {978-0-521-76539-8},
publisher = {Cambridge University Press},
series = {Cambridge Series in Statistical and Probabilistic Mathematics},
title = {{Probability: Theory and Examples}},
url = {https://services.math.duke.edu/{~}rtd/PTE/PTE5{\_}011119.pdf},
year = {2010}
}
