\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asuncion et~al.(2008)Asuncion, Smyth, and
  Welling]{asuncion:2008-async-lda}
A.~U. Asuncion, P.~Smyth, and M.~Welling.
\newblock Asynchronous distributed learning of topic models.
\newblock In \emph{Advances in Neural Information Processing Systems 21}, NIPS
  '08, pages 81--88, 2008.

\bibitem[Bardenet et~al.(2014)Bardenet, Doucet, and
  Holmes]{bardenet:2014-subsampling}
R.~Bardenet, A.~Doucet, and C.~Holmes.
\newblock Towards scaling up {M}arkov chain {M}onte {C}arlo: {A}n adaptive
  subsampling approach.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, ICML~'14, 2014.

\bibitem[Bertsekas(1990)]{bertsekas:1999-nonlinear}
D.~P. Bertsekas.
\newblock \emph{Nonlinear Programming}.
\newblock Athena Scientific, Belmont, MA, 2nd edition, 1990.

\bibitem[Broderick et~al.(2013)Broderick, Boyd, Wibisono, Wilson, and
  Jordan]{broderick:2013-svb}
T.~Broderick, N.~Boyd, A.~Wibisono, A.~C. Wilson, and M.~I. Jordan.
\newblock Streaming variational {B}ayes.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, NIPS
  '13, pages 1727--1735, 2013.

\bibitem[Campbell and How(2014)]{campbell:2014-decentralized}
T.~Campbell and J.~P. How.
\newblock Approximate decentralized {B}ayesian inference.
\newblock In \emph{30th Conference on Uncertainty in Artificial Intelligence},
  UAI '14, 2014.

\bibitem[Cover and Thomas(2006)]{cover:2006-info-theory}
T.~M. Cover and J.~A. Thomas.
\newblock \emph{Elements of Information Theory (Wiley Series in
  Telecommunications and Signal Processing)}.
\newblock Wiley-Interscience, 2006.

\bibitem[Dean and Ghemawat(2008)]{dean:2008-mapreduce}
J.~Dean and S.~Ghemawat.
\newblock {MapReduce}: {S}implified data processing on large clusters.
\newblock \emph{Communications of the ACM}, 51\penalty0 (1):\penalty0 107--113,
  Jan. 2008.

\bibitem[Doshi-Velez et~al.(2009)Doshi-Velez, Knowles, Mohamed, and
  Ghahramani]{doshi-velez:2009-p-ibp}
F.~Doshi-Velez, D.~A. Knowles, S.~Mohamed, and Z.~Ghahramani.
\newblock Large scale nonparametric {B}ayesian inference: Data parallelisation
  in the {I}ndian buffet process.
\newblock In \emph{Advances in Neural Information Processing Systems 22}, NIPS
  '09, pages 1294--1302, 2009.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi:2011-adagrad}
J.~C. Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Gelman et~al.(2013)Gelman, Carlin, Stern, Dunson, Vehtari, and
  Rubin]{gelman:2013-bda}
A.~Gelman, J.~B. Carlin, H.~S. Stern, D.~B. Dunson, A.~Vehtari, and D.~B.
  Rubin.
\newblock \emph{Bayesian Data Analysis, Third Edition}.
\newblock Chapman and Hall/CRC, 2013.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman:2013-svi}
M.~D. Hoffman, D.~M. Blei, C.~Wang, and J.~Paisley.
\newblock Stochastic variational inference.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, May 2013.

\bibitem[Johnson et~al.(2013)Johnson, Saunderson, and
  Willsky]{johnson:2013-analyzing}
M.~Johnson, J.~Saunderson, and A.~Willsky.
\newblock Analyzing {H}ogwild parallel {G}aussian {G}ibbs sampling.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages
  2715--2723, 2013.

\bibitem[Johnson and Zhang(2013)]{johnson:2013-svrg}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, NIPS
  '13, pages 315--323, 2013.

\bibitem[Korattikara et~al.(2014)Korattikara, Chen, and
  Welling]{korattikara-2014-austerity}
A.~Korattikara, Y.~Chen, and M.~Welling.
\newblock Austerity in {MCMC} land: {C}utting the {M}etropolis-{H}astings
  budget.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, ICML '14, 2014.

\bibitem[Kuhn(1955)]{kuhn:1955-match}
H.~W. Kuhn.
\newblock The {H}ungarian method for the assignment problem.
\newblock \emph{Naval Research Logistics Quarterly}, 2\penalty0 (1-2):\penalty0
  83--97, 1955.

\bibitem[Maclaurin and Adams(2014)]{maclaurin:2014-firefly}
D.~Maclaurin and R.~P. Adams.
\newblock Firefly {M}onte {C}arlo: {E}xact {MCMC} with subsets of data.
\newblock In \emph{Proceedings of 30th Conference on Uncertainty in Artificial
  Intelligence}, UAI '14, 2014.

\bibitem[Mandt and Blei(2014)]{mandt:2014-smoothed}
S.~Mandt and D.~M. Blei.
\newblock Smoothed gradients for stochastic variational inference.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, NIPS
  '14, pages 2438--2446, 2014.

\bibitem[Neiswanger et~al.(2014)Neiswanger, Wang, and
  Xing]{xing-2014-embarrassing}
W.~Neiswanger, C.~Wang, and E.~Xing.
\newblock Asymptotically exact, embarrassingly parallel {MCMC}.
\newblock In \emph{30th Conference on Uncertainty in Artificial Intelligence},
  UAI '14, 2014.

\bibitem[Nishihara et~al.(2014)Nishihara, Murray, and
  Adams]{nishihara:2014-gess}
R.~Nishihara, I.~Murray, and R.~P. Adams.
\newblock Parallel {MCMC} with generalized elliptical slice sampling.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 2087--2112,
  2014.

\bibitem[Niu et~al.(2011)Niu, Recht, R\'{e}, and Wright]{niu:2011-hogwild}
F.~Niu, B.~Recht, C.~R\'{e}, and S.~Wright.
\newblock Hogwild!: {A} lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in Neural Information Processing Systems 24},
  NIPS~'11, pages 693--701, 2011.

\bibitem[Ranganath et~al.(2013)Ranganath, Wang, Blei, and
  Xing]{ranganath:2013-adaptive}
R.~Ranganath, C.~Wang, D.~M. Blei, and E.~P. Xing.
\newblock An adaptive learning rate for stochastic variational inference.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, ICML '13, pages 298--306, 2013.

\bibitem[Scott et~al.(2013)Scott, Blocker, and Bonassi]{scott-2013-consensus}
S.~L. Scott, A.~W. Blocker, and F.~V. Bonassi.
\newblock Bayes and big data: The consensus {M}onte {C}arlo algorithm.
\newblock In \emph{Bayes 250}, 2013.

\bibitem[Strathmann et~al.(2015)Strathmann, Sejdinovic, and
  Girolami]{strathmann:2015-unbiased}
H.~Strathmann, D.~Sejdinovic, and M.~Girolami.
\newblock Unbiased {B}ayes for big data: {P}aths of partial posteriors.
\newblock \emph{arXiv preprint arXiv:1501.03326}, 2015.

\bibitem[Wang and Dunson(2013)]{dunson-2013-weierstrass}
X.~Wang and D.~B. Dunson.
\newblock Parallel {MCMC} via {W}eierstrass sampler.
\newblock \emph{arXiv preprint arXiv:1312.4605}, 2013.

\bibitem[Welling and Teh(2011)]{welling-2011-langevin}
M.~Welling and Y.~W. Teh.
\newblock {B}ayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning}, ICML '11, 2011.

\end{thebibliography}
