\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anschel et~al.(2017)Anschel, Baram, and Shimkin]{anschel17}
Anschel, O., Baram, N., and Shimkin, N.
\newblock {Averaged-DQN: Variance Reduction and Stabilization for Deep
  Reinforcement Learning}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  176--185, 2017.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and Bowling]{ale}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock {The Arcade Learning Environment: An evaluation platform for general
  agents}.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2016)Bellemare, Ostrovski, Guez, Thomas, and
  Munos]{bellemare16}
Bellemare, M.~G., Ostrovski, G., Guez, A., Thomas, P.~S., and Munos, R.
\newblock {Increasing the Action Gap: New Operators for Reinforcement
  Learning}.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{bellemare17dist}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock {A Distributional Perspective on Reinforcement Learning}.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 2017.

\bibitem[Bowling et~al.(2015)Bowling, Burch, Johanson, and Tammelin]{bowling15}
Bowling, M., Burch, N., Johanson, M., and Tammelin, O.
\newblock {Heads-up limit hold'em poker is solved}.
\newblock \emph{Science}, 347\penalty0 (6218):\penalty0 145--149, 2015.

\bibitem[Cesa-Bianchi \& Lugosi(2003)Cesa-Bianchi and Lugosi]{cesa03}
Cesa-Bianchi, N. and Lugosi, G.
\newblock {Potential-Based Algorithms in On-Line Prediction and Game Theory}.
\newblock \emph{Machine Learning}, 51\penalty0 (3):\penalty0 239--261, 2003.

\bibitem[Dick(2015)]{dick15}
Dick, T.
\newblock {Policy Gradient Reinforcement Learning Without Regret}.
\newblock Master's thesis, University of Alberta, 2015.

\bibitem[Dosovitskiy \& Koltun(2017)Dosovitskiy and Koltun]{dosovitskiy17}
Dosovitskiy, A. and Koltun, V.
\newblock {Learning to Act by Predicting the Future}.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Gordon(2007)]{gordon07}
Gordon, G.~J.
\newblock {No-regret Algorithms for Online Convex Programs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2007.

\bibitem[Gu et~al.(2017)Gu, Lillicrap, Ghahramani, Turner, Sch{\"o}lkopf, and
  Levine]{gu17_ipg}
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R.~E., Sch{\"o}lkopf, B., and
  Levine, S.
\newblock {Interpolated Policy Gradient: Merging On-Policy and Off-Policy
  Gradient Estimation for Deep Reinforcement Learning}.
\newblock \emph{arXiv preprint arXiv:1706.00387}, 2017.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{haarnoja17}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock {Reinforcement Learning with Deep Energy-Based Policies}.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja18}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
  Learning with a Stochastic Actor}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Hart \& Mas-Colell(2000)Hart and Mas-Colell]{rm}
Hart, S. and Mas-Colell, A.
\newblock {A Simple Adaptive Procedure Leading to Correlated Equilibrium}.
\newblock \emph{Econometrica}, 68\penalty0 (5):\penalty0 1127--1150, 2000.

\bibitem[Hausknecht \& Stone(2017)Hausknecht and Stone]{drqn}
Hausknecht, M. and Stone, P.
\newblock {Deep Recurrent Q-Learning for Partially Observable MDPs}.
\newblock \emph{arXiv preprint arXiv:1507.06527}, 2017.

\bibitem[Heess et~al.(2015)Heess, Hunt, Lillicrap, and Silver]{heess15rdpg}
Heess, N., Hunt, J.~J., Lillicrap, T.~P., and Silver, D.
\newblock {Memory-based control with recurrent neural networks}.
\newblock \emph{arXiv preprint arXiv:1512.04455}, 2015.

\bibitem[Jaakkola et~al.(1994)Jaakkola, Singh, and Jordan]{singh94}
Jaakkola, T., Singh, S.~P., and Jordan, M.~I.
\newblock {Reinforcement Learning Algorithm for Partially Observable Markov
  Decision Problems}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1994.

\bibitem[Johnson et~al.(2016)Johnson, Hofmann, Hutton, and Bignell]{malmo}
Johnson, M., Hofmann, K., Hutton, T., and Bignell, D.
\newblock {The Malmo Platform for Artificial Intelligence Experimentation}.
\newblock In \emph{Proceedings of the 25th International Joint Conference on
  Artificial Intelligence}, 2016.

\bibitem[Kempka et~al.(2016)Kempka, Wydmuch, Runc, Toczek, and
  Ja{\'s}kowski]{vizdoom}
Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Ja{\'s}kowski, W.
\newblock {ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement
  Learning}.
\newblock \emph{arXiv preprint arXiv:1605.02097}, 2016.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.~L.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kroer \& Sandholm(2014)Kroer and Sandholm]{kroer14}
Kroer, C. and Sandholm, T.
\newblock {Extensive-Form Game Abstraction With Bounds}.
\newblock In \emph{Proceedings of the 15th ACM Conference on Economics and
  Computation}, 2014.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine_finn16}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock {Continuous control with deep reinforcement learning}.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Littman(1994)]{littman94}
Littman, M.~L.
\newblock {Markov games as a framework for multi-agent reinforcement learning}.
\newblock In \emph{Proceedings of the 11th International Conference on Machine
  Learning}, pp.\  157--163, 1994.

\bibitem[Matiisen et~al.(2017)Matiisen, Oliver, Cohen, and
  Schulman]{matiisen17}
Matiisen, T., Oliver, A., Cohen, T., and Schulman, J.
\newblock {Teacher-Student Curriculum Learning}.
\newblock \emph{arXiv preprint arXiv:1707.00183}, 2017.

\bibitem[McCallum(1997)]{mccallum97}
McCallum, A.~K.
\newblock {Efficient Exploration in Reinforcement Learning with Hidden State}.
\newblock In \emph{AAAI Fall Symposium on Model-directed Autonomous Systems},
  1997.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock {Playing Atari with Deep Reinforcement Learning}.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{dqn-nature}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock {Human-level control through deep reinforcement learning}.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{a3c}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1928--1937, 2016.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and Schuurmans]{nachum17}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock {Bridging the Gap Between Value and Policy Based Reinforcement
  Learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[O'Donoghue et~al.(2017)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{odonoghue17}
O'Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
\newblock {Combining policy gradient and Q-learning}.
\newblock \emph{arXiv preprint arXiv:1611.01626}, 2017.

\bibitem[Oh et~al.(2015)Oh, Guo, Lee, Lewis, and Singh]{oh2015action}
Oh, J., Guo, X., Lee, H., Lewis, R.~L., and Singh, S.
\newblock {Action-Conditional Video Prediction using Deep Networks in Atari
  Games}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Oh et~al.(2016)Oh, Chockalingam, Singh, and Lee]{oh2016minecraft}
Oh, J., Chockalingam, V., Singh, S., and Lee, H.
\newblock Control of memory, active perception, and action in minecraft.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pp.\  2790--2799, 2016.

\bibitem[Peng et~al.(2016)Peng, Berseth, and van~de Penne]{peng16}
Peng, X.~B., Berseth, G., and van~de Penne, M.
\newblock {Terrain-Adaptive Locomotion Skills Using Deep Reinforcement
  Learning}.
\newblock \emph{ACM Transactions on Graphics}, 35\penalty0 (4):\penalty0 81,
  2016.

\bibitem[Ross \& Bagnell(2014)Ross and Bagnell]{ross+bagnell14}
Ross, S. and Bagnell, J.~A.
\newblock {Reinforcement and Imitation Learning via Interactive No-Regret
  Learning}.
\newblock \emph{arXiv preprint arXiv:1406.5979}, 2014.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross11}
Ross, S., Gordon, G.~J., and Bagnell, J.~A.
\newblock {A Reduction of Imitation Learning and Structured Prediction to
  No-Regret Online Learning}.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  627--635, 2011.

\bibitem[Sandholm \& Singh(2012)Sandholm and Singh]{sandholm12}
Sandholm, T. and Singh, S.
\newblock {Lossy Stochastic Game Abstraction with Bounds}.
\newblock In \emph{Proceedings of the 13th ACM Conference on Economics and
  Computation}, 2012.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML-15)}, pp.\  1889--1897, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{trpo_gae}
Schulman, J., Moritz, P., Levine, S., Jordan, M.~I., and Abbeel, P.
\newblock {High-Dimensional Continuous Control Using Generalized Advantage
  Estimation}.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Chen, and Abbeel]{schulman17}
Schulman, J., Chen, X., and Abbeel, P.
\newblock {Equivalence Between Policy Gradients and Soft Q-Learning}.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017.

\bibitem[Szepesv{\'a}ri(1998)]{szepesvari98}
Szepesv{\'a}ri, C.
\newblock {The Asymptotic Convergence-Rate of Q-learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1998.

\bibitem[Tammelin(2014)]{tammelin14}
Tammelin, O.
\newblock {Solving Large Imperfect Information Games Using CFR+}.
\newblock \emph{arXiv preprint arXiv:1407.5042}, 2014.

\bibitem[Tammelin et~al.(2015)Tammelin, Burch, Johanson, and
  Bowling]{tammelin15}
Tammelin, O., Burch, N., Johanson, M., and Bowling, M.
\newblock {Solving Heads-up Limit Texas Hold'em}.
\newblock In \emph{Proceedings of the 24th International Joint Conference on
  Artificial Intelligence}, 2015.

\bibitem[van Hasselt \& Wiering(2007)van Hasselt and Wiering]{van-hasselt07}
van Hasselt, H. and Wiering, M.~A.
\newblock {Reinforcement Learning in Continuous Action Spaces}.
\newblock In \emph{Proceedings of the 2007 IEEE Symposium on Approximate
  Dynamic Programming and Reinforcement Learning}, pp.\  272--279, 2007.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, and Silver]{ddqn}
van Hasselt, H., Guez, A., and Silver, D.
\newblock {Deep Reinforcement Learning and Double Q-Learning}.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, van Hasselt, Lanctot, and
  de~Freitas]{dueling}
Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and de~Freitas,
  N.
\newblock {Dueling Network Architectures for Deep Reinforcement Learning}.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, pp.\  1995--2003, 2016.

\bibitem[Wang et~al.(2017)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{retrace}
Wang, Z., Bapst, V., Heess, N., Mnih, V., Munos, R., Kavukcuoglu, K., and
  de~Freitas, N.
\newblock {Sample Efficient Actor-Critic with Experience Replay}.
\newblock \emph{arXiv preprint arXiv:1611.01224}, 2017.

\bibitem[Waugh et~al.(2015)Waugh, Morrill, Bagnell, and Bowling]{waugh15}
Waugh, K., Morrill, D., Bagnell, J.~A., and Bowling, M.
\newblock {Solving Games with Functional Regret Estimation}.
\newblock In \emph{Workshops at the Twenty-Ninth AAAI Conference on Artificial
  Intelligence}, 2015.
\newblock Supplementary material in \emph{arXiv preprint arXiv:1411.7974}.

\bibitem[Zinkevich et~al.(2007)Zinkevich, Johanson, Bowling, and
  Piccione]{zinkevich07}
Zinkevich, M., Johanson, M., Bowling, M.~H., and Piccione, C.
\newblock {Regret Minimization in Games with Incomplete Information}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2007.

\end{thebibliography}
