% from before 
@article{Cranmer2020,
   author = {Kyle Cranmer and Johann Brehmer and Gilles Louppe},
   issn = {0027-8424},
   doi = {10.1073/pnas.1912789117},
   journal = {Proceedings of the National Academy of Sciences (PNAS)},
   keywords = {and cosmology,astrophysics,climate science,ecology,economics,epidemiology,molecular dynamics,neuroscience,physics,popula-tion genetics,protein folding},
   pages = {30055-30062},
   title = {The frontier of simulation-based inference},
   volume = {117},
   year = {2020},
}

@misc{Python36,
  title = {Python {{Language Reference}}, Version 3.6},
  author = {{Python Software Fundation}},
  year = {2017},
  pages = {https://python.org}
}

@article{nflows,
  author       = {Conor Durkan and
                  Artur Bekasov and
                  Iain Murray and
                  George Papamakarios},
  title        = {{nflows}: normalizing flows in {PyTorch}},
  month        = nov,
  year         = 2020,
  publisher    = {Zenodo},
  version      = {v0.14},
  doi          = {10.5281/zenodo.4296287},
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{Paszke2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High}}-{{Performance Deep Learning Library}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  pages = {12},
  address = {{Vancouver, BC, Canada}},
  language = {en}
}

@article{tejero-cantero2020sbi,
  doi = {10.21105/joss.02505},
  year = {2020},
  publisher = {The Open Journal},
  volume = {5},
  number = {52},
  pages = {2505},
  author = {Alvaro Tejero-Cantero and Jan Boelts and Michael Deistler and Jan-Matthis Lueckmann and Conor Durkan and Pedro J. Gonçalves and David S. Greenberg and Jakob H. Macke},
  title = {sbi: A toolkit for simulation-based inference},
  journal = {Journal of Open Source Software}
}



@BOOK{Efron2016,
  title     = "Institute of mathematical statistics monographs: Computer age
               statistical inference: Algorithms, evidence, and data science
               series number 5",
  author    = "Efron, Bradley and Hastie, Trevor",
  publisher = "Cambridge University Press",
  month     =  jul,
  year      =  2016,
  address   = "Cambridge, England"
}

@inproceedings{Durkan2019,
 author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Spline Flows},
 volume = {32},
 year = {2019}
}


@BOOK{Hastie2009,
  title     = "The elements of statistical learning",
  author    = "Hastie, Trevor and Tibshirani, Robert and Friedman, J H",
  publisher = "Springer",
  series    = "Springer series in statistics",
  edition   =  2,
  month     =  dec,
  year      =  2009,
  address   = "New York, NY",
  language  = "en"
}

@article{Papamakarios2021,
   author = {George Papamakarios and Eric Nalisnick and Danilo Jimenez Rezende and Shakir Mohamed and Balaji Lakshminarayanan},
   journal = {Journal of Machine Learning Research},
   keywords = {generative models,invertible neural networks,normalizing flows,proba-bilistic inference,probabilistic modeling},
   pages = {1-64},
   title = {Normalizing Flows for Probabilistic Modeling and Inference},
   volume = {22},
   year = {2021},
   doi = {10.48550/arxiv.1912.02762},
   issn = {15337928},
}

@article{Jallais2022,
    title = "Inverting brain grey matter models with likelihood-free inference: a tool for trustable cytoarchitecture measurements",
    author = "Jallais, Maëliss and Rodrigues, Pedro L. C. and Gramfort, Alexandre and Wassermann, Demian",
    journal = "Machine Learning for Biomedical Imaging",
    volume = "1",
    issue = "IPMI 2021 special issue",
    year = "2022",
    pages = "1--28",
    issn = "2766-905X",
    url = "https://melba-journal.org/2022:010"
}

@article{Vasist2023,
	author = {{Vasist, Malavika} and {Rozet, Fran\c{c}ois} and {Absil, Olivier} and {Molli\`ere, Paul} and {Nasedkin, Evert} and {Louppe, Gilles}},
	title = {Neural posterior estimation for exoplanetary atmospheric retrieval},
	DOI= "10.1051/0004-6361/202245263",
	url= "https://doi.org/10.1051/0004-6361/202245263",
	journal = {A\&A},
	year = 2023,
	volume = 672,
	pages = "A147",
}


@article{Dax2021gwaves,
  title = {Real-Time Gravitational Wave Science with Neural Posterior Estimation},
  author = {Dax, Maximilian and Green, Stephen R. and Gair, Jonathan and Macke, Jakob H. and Buonanno, Alessandra and Sch\"olkopf, Bernhard},
  journal = {Phys. Rev. Lett.},
  volume = {127},
  issue = {24},
  pages = {241103},
  numpages = {7},
  year = {2021},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.127.241103},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.127.241103}
}


@article{Goncalves2020,
   author = {Pedro J. Gonçalves and Jan Matthis Lueckmann and Michael Deistler and Marcel Nonnenmacher and Kaan Öcal and Giacomo Bassetto and Chaitanya Chintaluri and William F. Podlaski and Sara A. Haddad and Tim P. Vogels and David S. Greenberg and Jakob H. Macke},
   doi = {10.7554/ELIFE.56261},
   issn = {2050084X},
   journal = {eLife},
   month = {9},
   pages = {1-46},
   pmid = {32940606},
   publisher = {eLife Sciences Publications Ltd},
   title = {Training deep neural density estimators to identify mechanistic models of neural dynamics},
   volume = {9},
   year = {2020},
}

@article{Lueckman2021,
   author = {Jan-Matthis Lueckmann and Jan Boelts and David S Greenberg and Pedro J Gonçalves and Jakob H Macke},
   journal = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics (PMLR)},
   month = {4},
   pages = {343-351},
   title = {Benchmarking Simulation-Based Inference},
   volume = {130},
   year = {2021},
   doi = {10.48550/arxiv.2101.04653},
}

@article{Bittner2021,
   author = {Sean R. Bittner and Agostina Palmigiano and Alex T. Piet and Chunyu A. Duan and Carlos D. Brody and Kenneth D. Miller and John Cunningham},
   doi = {10.7554/eLife.56265},
   issn = {2050084X},
   journal = {eLife},
   month = {7},
   pmid = {34323690},
   publisher = {eLife Sciences Publications Ltd},
   title = {Interrogating theoretical models of neural computation with emergent property inference},
   volume = {10},
   year = {2021},
}

@article{Dalmasso2021,
   author = {Niccolò Dalmasso and David Zhao and Rafael Izbicki and Ann B. Lee},
   doi = {10.48550/arxiv.2107.03920},
   month = {7},
   title = {Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage},
   year = {2021},
}

@misc{gelman2020bayesian,
      title={Bayesian Workflow}, 
      author={Andrew Gelman and Aki Vehtari and Daniel Simpson and Charles C. Margossian and Bob Carpenter and Yuling Yao and Lauren Kennedy and Jonah Gabry and Paul-Christian Bürkner and Martin Modrák},
      year={2020},
      eprint={2011.01808},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@InProceedings{pmlr-v119-dalmasso20a,
  title = 	 {Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting},
  author =       {Dalmasso, Niccolo and Izbicki, Rafael and Lee, Ann},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2323--2334},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
}


@InProceedings{pmlr-v80-hebert-johnson18a,
  title = 	 {Multicalibration: Calibration for the ({C}omputationally-Identifiable) Masses},
  author =       {Hebert-Johnson, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1939--1948},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/hebert-johnson18a/hebert-johnson18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/hebert-johnson18a.html},
  abstract = 	 {We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations. The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group. We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.}
}

@InProceedings{Dheur2023,
  title = 	 {A Large-Scale Study of Probabilistic Calibration in Neural Network Regression},
  author =       {Dheur, Victor and Taieb, Souhaib Ben},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
  note = "To appear"
}

@InProceedings{Zhao2021,
  title = 	 {Diagnostics for conditional density models and Bayesian inference algorithms},
  author =       {Zhao, David and Dalmasso, Niccol\`o and Izbicki, Rafael and Lee, Ann B.},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1830--1840},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/zhao21b/zhao21b.pdf},
  url = 	 {https://proceedings.mlr.press/v161/zhao21b.html},
  abstract = 	 {There has been growing interest in the AI community for precise uncertainty quantification. Conditional density models f(y|x), where x represents potentially high-dimensional features, are an integral part of uncertainty quantification in prediction and Bayesian inference. However, it is challenging to assess conditional density estimates and gain insight into modes of failure. While existing diagnostic tools can determine whether an approximated conditional density is compatible overall with a data sample, they lack a principled framework for identifying, locating, and interpreting the nature of statistically significant discrepancies over the entire feature space. In this paper, we present rigorous and easy-to-interpret diagnostics such as (i) the “Local Coverage Test” (LCT), which distinguishes an arbitrarily misspecified model from the true conditional density of the sample, and (ii) “Amortized Local P-P plots” (ALP) which can quickly provide interpretable graphical summaries of distributional differences at any location x in the feature space. Our validation procedures scale to high dimensions and can potentially adapt to any type of data at hand. We demonstrate the effectiveness of LCT and ALP through a simulated experiment and applications to prediction and parameter inference for image data.}
}


@article{Jansen1995,
   author = {Ben H. Jansen and Vincent G. Rit},
   doi = {10.1007/BF00199471},
   issn = {1432-0770},
   issue = {4},
   journal = {Biological Cybernetics 1995 73:4},
   keywords = {Bioinformatics,Complex Systems,Computer Appl. in Life Sciences,Neurobiology,Neurosciences},
   month = {9},
   pages = {357-366},
   pmid = {7578475},
   publisher = {Springer},
   title = {Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns},
   volume = {73},
   year = {1995},
}

@article{Frazier2019,
  TITLE = {{Model misspecification in approximate Bayesian computation: consequences and diagnostics}},
  AUTHOR = {Frazier, David and Robert, Christian and Rousseau, Judith},
  JOURNAL = {{Journal of the Royal Statistical Society: Series B}},
  PUBLISHER = {{Royal Statistical Society}},
  VOLUME = {82},
  NUMBER = {2},
  PAGES = {421-444},
  YEAR = {2019},
  DOI = {10.1111/rssb.12356},
  HAL_ID = {hal-01961101},
  HAL_VERSION = {v2},
}

@InProceedings{Rezende2015,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Rezende, Danilo and Mohamed, Shakir},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/rezende15.html},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}


@BOOK{Robert2005,
  title     = "Monte Carlo statistical methods",
  author    = "Robert, Christian and Casella, George",
  publisher = "Springer",
  series    = "Springer Texts in Statistics",
  edition   =  2,
  month     =  jul,
  year      =  2005,
  address   = "New York, NY",
  language  = "en"
}

@article{carpenter2017stan,
title={Stan: A probabilistic programming language},
author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
journal={Journal of statistical software},
volume={76},
number={1},
year={2017},
publisher={Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA (United States)}
}

@inproceedings{
geffner2022score,
title={Score Modeling for Simulation-based Inference},
author={Tomas Geffner and George Papamakarios and Andriy Mnih},
booktitle={NeurIPS 2022 Workshop on Score-Based Methods},
year={2022},
url={https://openreview.net/forum?id=_184Njdw7WL}
}

@inproceedings{Rodrigues2021,
  TITLE = {{HNPE: Leveraging Global Parameters for Neural Posterior Estimation}},
  AUTHOR = {Coelho Rodrigues, Pedro Luiz and Moreau, Thomas and Louppe, Gilles and Gramfort, Alexandre},
  URL = {https://hal.science/hal-03139916},
  BOOKTITLE = {{NeurIPS 2021}},
  ADDRESS = {Sydney (Online), Australia},
  YEAR = {2021},
  MONTH = Dec,
  PDF = {https://hal.science/hal-03139916v3/file/neurips_2021.pdf},
  HAL_ID = {hal-03139916},
  HAL_VERSION = {v3},
}

@article{Papamakarios2017,
   author = {George Papamakarios and Theo Pavlakou and Iain Murray},
   journal = {Advances in Neural Information Processing Systems (NeurIPS)},
   month = {12},
   pages = {2339-2348},
   title = {Masked Autoregressive Flow for Density Estimation},
   year = {2017},
   doi = {10.48550/arxiv.1705.07057},
   issn = {10495258},
}

@article{Tejero2020,
   author = {Álvaro Tejero-Cantero and Jan Boelts and Michael Deistler and Jan-Matthis Lueckmann and Conor Durkan and Pedro J Gonçalves and David S Greenberg and Jakob H Macke and Computational Neuroengineering and T U Munich},
   doi = {10.21105/joss.02505},
   issn = {2475-9066},
   issue = {52},
   journal = {Journal of Open Source Software},
   month = {7},
   pages = {2505},
   publisher = {The Open Journal},
   title = {SBI -- A toolkit for simulation-based inference},
   volume = {5},
   year = {2020},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@InProceedings{Hermans2020,
  title = 	 {Likelihood-free {MCMC} with Amortized Approximate Ratio Estimators},
  author =       {Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4239--4248},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/hermans20a/hermans20a.pdf},
  doi = {10.48550/arxiv.1903.04057},
}

% added
@article{Masserano2022,
   author = {Luca Masserano and Tommaso Dorigo and Rafael Izbicki and Mikael Kuusela and Ann B. Lee},
   month = {5},
   title = {Simulation-Based Inference with WALDO: Perfectly Calibrated Confidence Regions Using Any Prediction or Posterior Estimation Algorithm},
   year = {2022},
   doi = {10.48550/arxiv.2205.15680},
}

@article{Talts2018,
   author = {Sean Talts and Michael Betancourt and Daniel Simpson and Aki Vehtari and Andrew Gelman},
   month = {4},
   title = {Validating Bayesian Inference Algorithms with Simulation-Based Calibration},
   year = {2018},
   doi = {10.48550/arxiv.1804.06788},
}

@article{Kim2018,
   author = {Ilmun Kim and Ann B. Lee and Jing Lei},
   doi = {10.48550/arxiv.1812.08927},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   month = {12},
   pages = {5253-5305},
   publisher = {Institute of Mathematical Statistics},
   title = {Global and Local Two-Sample Tests via Regression},
   volume = {13},
   year = {2018},
}

@article{Boelts2022,
   author = {Jan Boelts and Jan-Matthis Lueckmann and Richard Gao and Jakob H. Macke},
   doi = {10.1101/2021.12.22.473472},
   issn = {2050084X},
   journal = {bioRxiv},
   month = {6},
   pages = {2021.12.22.473472},
   publisher = {Cold Spring Harbor Laboratory},
   title = {Flexible and efficient simulation-based inference for models of decision-making},
   year = {2022},
}

@article{Bland1995,
   author = {J. Martin Bland and Douglas G. Altman},
   doi = {10.1136/BMJ.310.6973.170},
   issn = {0959-8138},
   issue = {6973},
   journal = {BMJ (Clinical research ed.)},
   keywords = {Clinical Trials as Topic / methods*,D G Altman,Data Interpretation,Humans,J M Bland,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC2548561,Probability,PubMed Abstract,Statistical*,doi:10.1136/bmj.310.6973.170,pmid:7833759},
   month = {1},
   pages = {170},
   pmid = {7833759},
   publisher = {BMJ},
   title = {Multiple significance tests: the Bonferroni method},
   volume = {310},
   year = {1995},
}

@article{Miller1993,
   author = {John W. Miller and Rod Goodman and Padhraic Smyth},
   issue = {4},
   journal = {IEEE TRANSACTIONS ON INFORMAITON THEORY},
   title = {On Loss Functions Which Minimii to Conditional Expected Values and Posterior Probabilities},
   volume = {39},
   year = {1993},
}

@inproceedings{Rouillard2022,
  TITLE = {{ADAVI: Automatic Dual Amortized Variational Inference Applied To Pyramidal Bayesian Models}},
  AUTHOR = {Rouillard, Louis and Wassermann, Demian},
  BOOKTITLE = {{ICLR 2022}},
  ADDRESS = {Virtual, France},
  YEAR = {2022},
  MONTH = Apr,
  DOI = {10.48550/arXiv.2106.12248},
  HAL_ID = {hal-03267956},
  HAL_VERSION = {v4},
}

@article{Papamakarios2019, 
title = {Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows}, 
author = {Papamakarios, George and Sterratt, David and Murray, Iain}, 
pages = {837--848}, year = {2019}, 
editor = {Kamalika Chaudhuri and Masashi Sugiyama}, volume = {89}, 
series = {Proceedings of Machine Learning Research}, 
address = {}, month = {16--18 Apr}, 
publisher = {PMLR}
}

 @InProceedings{Greenberg2019,
 title = {Automatic Posterior Transformation for Likelihood-Free Inference},
 author = {Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
 booktitle = {Proceedings of the 36th International Conference on Machine Learning},
 pages = {2404--2414}, year = {2019},
 editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97},
 month = {09--15 Jun}, publisher = {PMLR}
 } 


%% new 
@book{lehmann2006testing,
  title={Testing Statistical Hypotheses},
  author={Lehmann, E.L. and Romano, J.P.},
  isbn={9780387276052},
  lccn={2004051464},
  series={Springer Texts in Statistics},
  url={https://books.google.fr/books?id=K6t5qn-SEp8C},
  year={2006},
  publisher={Springer New York}
}

@article{Lopez2016,
   author = {David Lopez-Paz and Maxime Oquab},
   doi = {10.48550/arxiv.1610.06545},
   journal = {5th International Conference on Learning Representations, ICLR 2017},
   month = {10},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Revisiting Classifier Two-Sample Tests},
   url = {https://arxiv.org/abs/1610.06545v4},
   year = {2016},
}

@article{Kim2016,
  author    = {Aaditya Ramdas and
               Aarti Singh and
               Larry A. Wasserman},
  title     = {Classification Accuracy as a Proxy for Two Sample Testing},
  journal   = {CoRR},
  volume    = {abs/1602.02210},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02210},
  eprinttype = {arXiv},
  eprint    = {1602.02210},
  timestamp = {Mon, 13 Aug 2018 16:48:16 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RamdasSW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%%% simulators and applications in sbi
@article{Sanz2013,
   author = {Paula Sanz Leon and Stuart A. Knock and M. Marmaduke Woodman and Lia Domide and Jochen Mersmann and Anthony R. Mcintosh and Viktor Jirsa},
   doi = {10.3389/FNINF.2013.00010},
   issn = {1662-5196},
   issue = {MAY},
   journal = {Frontiers in neuroinformatics},
   keywords = {MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PMC3678125,Paula Sanz Leon,PubMed Abstract,Stuart A Knock,Viktor Jirsa,doi:10.3389/fninf.2013.00010,pmid:23781198},
   month = {6},
   pmid = {23781198},
   publisher = {Front Neuroinform},
   title = {The Virtual Brain: a simulator of primate brain network dynamics},
   volume = {7},
   url = {https://pubmed.ncbi.nlm.nih.gov/23781198/},
   year = {2013},
}

@article{David2003,
   author = {Olivier David and Karl J Friston},
   doi = {10.1016/j.neuroimage.2003.07.015},
   issue = {3},
   journal = {NeuroImage},
   pages = {1743-1755},
   title = {A neural mass model for MEG/EEG: coupling and neuronal dynamics},
   volume = {20},
   year = {2003},
}

@article{Tavare1997,
   author = {Simon Tavaré and David J. Balding and R. C. Griffiths and Peter Donnelly},
   doi = {10.1093/GENETICS/145.2.505},
   issn = {00166731},
   issue = {2},
   journal = {Genetics},
   month = {2},
   pages = {505-518},
   pmid = {9071603},
   publisher = {Oxford Academic},
   title = {Inferring Coalescence Times From {DNA} Sequence Data},
   volume = {145},
   year = {1997},
}

@article{Makinen2021,
   author = {T. Lucas Makinen and Tom Charnock and Justin Alsing and Benjamin D. Wandelt},
   doi = {10.1088/1475-7516/2021/11/049},
   issue = {11},
   journal = {Journal of Cosmology and Astroparticle Physics},
   keywords = {cosmological parameters from LSS,cosmological simulations,dark matter simulations,power spectrum},
   month = {7},
   publisher = {IOP Publishing Ltd},
   title = {Lossless, Scalable Implicit Likelihood Inference for Cosmological Fields},
   volume = {2021},
   year = {2021},
}

@inproceedings{
Dax2021,
title={Group equivariant neural posterior estimation},
author={Maximilian Dax and Stephen R Green and Jonathan Gair and Michael Deistler and Bernhard Sch{\"o}lkopf and Jakob H. Macke},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=u6s8dSporO8}
}


@article{Buckwar2020,
   author = {Evelyn Buckwar and Massimiliano Tamborrino and Irene Tubikanec},
   doi = {10.1007/S11222-019-09909-6/FIGURES/12},
   issn = {15731375},
   issue = {3},
   journal = {Statistics and Computing},
   keywords = {Approximate Bayesian computation,Invariant measure,Likelihood-free inference,Neural mass models,Numerical splitting schemes,Stochastic differential equations},
   month = {5},
   pages = {627-648},
   publisher = {Springer},
   title = {Spectral density-based and measure-preserving ABC for partially observed diffusion processes. An illustration on Hamiltonian SDEs},
   volume = {30},
   year = {2020},
}

@article{Hashemi2022,
   author = {Meysam Hashemi and Anirudh N. Vattikonda and Jayant Jha and Viktor Sip and Marmaduke M. Woodman and Fabrice Bartolomei and Viktor K. Jirsa},
   doi = {10.1101/2022.06.02.22275860},
   journal = {medRxiv},
   keywords = {ANNs, artificial neural networks,Bayesian inference, Artificial neural networks, Simulation-based-inference, Whole-brain network modeling, Epilepsy, Stereoelectroencephalography (SEEG), Abbreviations: VEP, virtual epileptic patient,EZ, epilepto-genic zone,HMC, Hamiltonian Monte Carlo,HZ, healthy zone,MAF, masked autoregres-sive flow,MCMC, Markov chain Monte Carlo,MDN, mixture density network,NFs, normalizing flows,NSFs, neural spline flows,PZ, propagation zone,SBI, simulation-based-inference,SEEG, stereoelectroencephalography,SNLE, sequential neural likelihood estimation,SNPE, sequential neural posterior estimation,SNRE, sequential neural ratio estimation,TVB, the virtual brain,VI, variational in-ference},
   month = {6},
   pages = {2022.06.02.22275860},
   publisher = {Cold Spring Harbor Laboratory Press},
   title = {Simulation-Based Inference for Whole-Brain Network Modeling of Epilepsy using Deep Neural Density Estimators},
   url = {https://www.medrxiv.org/content/10.1101/2022.06.02.22275860v1},
   year = {2022},
}


%  attempts at better sbi algorithms
@article{
Hermans2021,
title={A Crisis In Simulation-Based Inference? Beware, Your Posterior Approximations Can Be Unfaithful},
author={Joeri Hermans and Arnaud Delaunoy and Fran{\c{c}}ois Rozet and Antoine Wehenkel and Volodimir Begy and Gilles Louppe},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=LHAbHkt6Aq},
note={}
}

@inproceedings{
Delaunoy2022,
title={Towards Reliable Simulation-Based Inference with Balanced Neural Ratio Estimation},
author={Arnaud Delaunoy and Joeri Hermans and Fran{\c{c}}ois Rozet and Antoine Wehenkel and Gilles Louppe},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=o762mMj4XK}
}

@inproceedings{
Ward2022,
title={Robust Neural Posterior Estimation and Statistical Model Criticism},
author={Daniel Ward and Patrick Cannon and Mark Beaumont and Matteo Fasiolo and Sebastian M Schmon},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=MHE27tjD8m3}
}

@article{Lemos2022,
doi = {10.1088/2632-2153/acbb53},
url = {https://dx.doi.org/10.1088/2632-2153/acbb53},
year = {2023},
month = {feb},
publisher = {IOP Publishing},
volume = {4},
number = {1},
pages = {01LT01},
author = {Pablo Lemos and Miles Cranmer and Muntazir Abidi and ChangHoon Hahn and Michael Eickenberg and Elena Massara and David Yallup and Shirley Ho},
title = {Robust simulation-based inference in cosmology with Bayesian neural networks},
journal = {Machine Learning: Science and Technology},
abstract = {Simulation-based inference (SBI) is rapidly establishing itself as a standard machine learning technique for analyzing data in cosmological surveys. Despite continual improvements to the quality of density estimation by learned models, applications of such techniques to real data are entirely reliant on the generalization power of neural networks far outside the training distribution, which is mostly unconstrained. Due to the imperfections in scientist-created simulations, and the large computational expense of generating all possible parameter combinations, SBI methods in cosmology are vulnerable to such generalization issues. Here, we discuss the effects of both issues, and show how using a Bayesian neural network framework for training SBI can mitigate biases, and result in more reliable inference outside the training set. We introduce cosmoSWAG, the first application of stochastic weight averaging to cosmology, and apply it to SBI trained for inference on the cosmic microwave background.}
}



% c2st variants & applications
@misc{Pandeva2022,
      title={E-Valuating Classifier Two-Sample Tests}, 
      author={Teodora Pandeva and Tim Bakker and Christian A. Naesseth and Patrick Forré},
      year={2022},
      eprint={2210.13027},
      archivePrefix={arXiv},
      primaryClass={stat.ME}
}

@article{Gutmann2010,
  author  = {Michael U. Gutmann and Aapo Hyv{{\"a}}rinen},
  title   = {Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {11},
  pages   = {307--361},
  url     = {http://jmlr.org/papers/v13/gutmann12a.html}
}

@article{Goodfellow2014,
   author = {Ian Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
   doi = {10.1145/3422622},
   issn = {15577317},
   issue = {11},
   journal = {Communications of the ACM},
   month = {6},
   pages = {139-144},
   publisher = {Association for Computing Machinery},
   title = {Generative Adversarial Networks},
   volume = {63},
   year = {2014},
}

%  sbc 

@article{Modrak2022,
   author = {Martin Modrák and Angie H. Moon and Shinyoung Kim and Paul Bürkner and Niko Huurre and Kateřina Faltejsková and Andrew Gelman and Aki Vehtari},
   doi = {10.48550/arxiv.2211.02383},
   month = {11},
   title = {Simulation-Based Calibration Checking for Bayesian Computation: The Choice of Test Quantities Shapes Sensitivity},
   url = {https://arxiv.org/abs/2211.02383v1},
   year = {2022},
}

% local valdiags
@misc{Linhart2022,
      title={Validation Diagnostics for {SBI} algorithms based on Normalizing Flows}, 
      author={Julia Linhart and Alexandre Gramfort and Pedro L. C. Rodrigues},
      year={2022},
      eprint={2211.09602},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

% conclusion
@article{Gao2020,
  author = {Ruiqi Gao and Erik Nijkamp and Diederik P Kingma and Zhen Xu and Andrew M Dai and Ying Nian Wu},
   title = {Flow Contrastive Estimation of Energy-Based Models},
   year={2020},
}

@article{Azadi2018,
   author = {Samaneh Azadi and Augustus Odena and Catherine Olsson and Trevor Darrell and Ian Goodfellow},
   journal = {7th International Conference on Learning Representations, ICLR 2019},
   month = {10},
   publisher = {International Conference on Learning Representations, ICLR},
   title = {Discriminator Rejection Sampling},
   url = {https://arxiv.org/abs/1810.06758v3},
   year = {2018},
}

@article{Dey2022,
   author = {Biprateep Dey and David Zhao and Jeffrey A. Newman and Brett H. Andrews and Rafael Izbicki and Ann B. Lee},
   doi = {10.48550/arxiv.2205.14568},
   month = {5},
   title = {Calibrated Predictive Distributions via Diagnostics for Conditional Coverage},
   url = {https://arxiv.org/abs/2205.14568v2},
   year = {2022},
}

@article{Yao2023,
   author = {Yuling Yao and Justin Domke},
   keywords = {amortized inference,diagnostics,hypothesis testing,likelihood-free,simulation based inference},
   title = {Discriminative calibration},
   year = {2023},
}