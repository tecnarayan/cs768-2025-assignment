\begin{thebibliography}{10}

\bibitem{taylor2022galactica}
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
\newblock Galactica: A large language model for science.
\newblock {\em arXiv preprint arXiv:2211.09085}, 2022.

\bibitem{ai4science2023impact}
Microsoft~Research AI4Science and Microsoft~Azure Quantum.
\newblock The impact of large language models on scientific discovery: a preliminary study using gpt-4.
\newblock {\em arXiv preprint arXiv:2311.07361}, 2023.

\bibitem{Chen2023.07.05.547496}
Bo~Chen, Xingyi Cheng, Yangli ao~Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, and Le~Song.
\newblock xtrimopglm: Unified 100b-scale pre-trained transformer for deciphering the language of protein.
\newblock {\em bioRxiv}, 2023.

\bibitem{weather_forecasting}
Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi~Tian.
\newblock Accurate medium-range global weather forecasting with 3d neural networks.
\newblock {\em Nature}, 619:1--6, 07 2023.

\bibitem{deng2023learning}
Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Le~Zhou, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, et~al.
\newblock Learning a foundation language model for geoscience knowledge understanding and utilization.
\newblock {\em arXiv preprint arXiv:2306.05064}, 2023.

\bibitem{sun2023scieval}
Liangtai Sun, Yang Han, Zihan Zhao, Da~Ma, Zhennan Shen, Baocai Chen, Lu~Chen, and Kai Yu.
\newblock Scieval: A multi-level large language model evaluation benchmark for scientific research.
\newblock {\em arXiv preprint arXiv:2308.13149}, 2023.

\bibitem{wang2023scibench}
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun~R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang.
\newblock Scibench: Evaluating college-level scientific problem-solving abilities of large language models.
\newblock {\em arXiv preprint arXiv:2307.10635}, 2023.

\bibitem{rein2023gpqa}
David Rein, Betty~Li Hou, Asa~Cooper Stickland, Jackson Petty, Richard~Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel~R Bowman.
\newblock Gpqa: A graduate-level google-proof q\&a benchmark.
\newblock {\em arXiv preprint arXiv:2311.12022}, 2023.

\bibitem{zhang2024rest}
Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, and Jie Tang.
\newblock Rest-mcts*: Llm self-training via process reward guided tree search.
\newblock {\em arXiv preprint arXiv:2406.03816}, 2024.

\bibitem{yue2023MAmmoTH}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock Mammoth: Building math generalist models through hybrid instruction tuning.
\newblock {\em arXiv preprint arXiv:2309.05653}, 2023.

\bibitem{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock {\em arXiv preprint arXiv:2309.12284}, 2023.

\bibitem{yue2024mammoth2}
Xiang Yue, Tuney Zheng, Ge~Zhang, and Wenhu Chen.
\newblock Mammoth2: Scaling instructions from the web.
\newblock {\em arXiv preprint arXiv:2405.03548}, 2024.

\bibitem{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank infilling.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, 2022.

\bibitem{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock {\em arXiv preprint arXiv:2210.02414}, 2022.

\bibitem{MetaAI2024}
{Meta AI}.
\newblock Meta llama 3.
\newblock \url{https://ai.meta.com/blog/meta-llama-3/}, 2024.
\newblock Accessed: 2024-05-22.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{zeng2023agenttuning}
Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang.
\newblock Agenttuning: Enabling generalized agent abilities for llms.
\newblock {\em arXiv preprint arXiv:2310.12823}, 2023.

\bibitem{barras1997coq}
Bruno Barras, Samuel Boutin, Cristina Cornes, Judica{\"e}l Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et~al.
\newblock {\em The {Coq} proof assistant reference manual: Version 6.1}.
\newblock PhD thesis, Inria, 1997.

\bibitem{nipkow2002isabelle}
Tobias Nipkow, Markus Wenzel, and Lawrence~C Paulson.
\newblock {\em Isabelle/HOL: a proof assistant for higher-order logic}.
\newblock Springer, 2002.

\bibitem{de2015lean}
Leonardo De~Moura, Soonho Kong, Jeremy Avigad, Floris Van~Doorn, and Jakob von Raumer.
\newblock The lean theorem prover (system description).
\newblock In {\em Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25}, pages 378--388. Springer, 2015.

\bibitem{yang2023leandojo}
Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar.
\newblock {LeanDojo}: Theorem proving with retrieval-augmented language models.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem{cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~H. Chi, Quoc~V. Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, {\em Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem{shinn2023reflexion}
Noah Shinn, Beck Labash, and Ashwin Gopinath.
\newblock Reflexion: an autonomous agent with dynamic memory and self-reflection.
\newblock {\em arXiv preprint arXiv:2303.11366}, 2023.

\bibitem{li2023reflection}
Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, and Tianyi Zhou.
\newblock Reflection-tuning: Data recycling improves llm instruction-tuning.
\newblock {\em arXiv preprint arXiv:2310.11716}, 2023.

\bibitem{DBLP:journals/corr/abs-2305-20050}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock {\em CoRR}, abs/2305.20050, 2023.

\bibitem{DBLP:journals/corr/abs-2307-13702}
Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukosiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, Saurav Kadavath, Shannon Yang, Thomas Henighan, Timothy Maxwell, Timothy Telleen{-}Lawton, Tristan Hume, Zac Hatfield{-}Dodds, Jared Kaplan, Jan Brauner, Samuel~R. Bowman, and Ethan Perez.
\newblock Measuring faithfulness in chain-of-thought reasoning.
\newblock {\em CoRR}, abs/2307.13702, 2023.

\bibitem{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In {\em SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem{huang2023ceval}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et~al.
\newblock C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models.
\newblock {\em arXiv preprint arXiv:2305.08322}, 2023.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock {\em arXiv preprint arXiv:2009.03300}, 2020.

\bibitem{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock {\em arXiv preprint arXiv:2103.03874}, 2021.

\bibitem{davies2021advancing}
Alex Davies, Petar Veli{\v{c}}kovi{\'c}, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Toma{\v{s}}ev, Richard Tanburn, Peter Battaglia, Charles Blundell, Andr{\'a}s Juh{\'a}sz, et~al.
\newblock Advancing mathematics by guiding human intuition with ai.
\newblock {\em Nature}, 600(7887):70--74, 2021.

\bibitem{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
\newblock Agieval: A human-centric benchmark for evaluating foundation models.
\newblock {\em arXiv preprint arXiv:2304.06364}, 2023.

\bibitem{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock {\em arXiv preprint arXiv:2108.07732}, 2021.

\bibitem{openai2023gpt4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{glm2024chatglm}
Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da~Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et~al.
\newblock Chatglm: A family of large language models from glm-130b to glm-4 all tools.
\newblock {\em arXiv preprint arXiv:2406.12793}, 2024.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{yuan2023scaling}
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou.
\newblock Scaling relationship on learning mathematical reasoning with large language models.
\newblock {\em arXiv preprint arXiv:2308.01825}, 2023.

\bibitem{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock {\em arXiv preprint arXiv:2305.11206}, 2023.

\bibitem{chen2021evaluating_codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{DBLP:journals/corr/abs-2203-07814}
Yujia Li, David~H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R{\'{e}}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin~Dal Lago, Thomas Hubert, Peter Choy, Cyprien de~Masson~d'Autume, Igor Babuschkin, Xinyun Chen, Po{-}Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel~J. Mankowitz, Esme~Sutherland Robson, Pushmeet Kohli, Nando de~Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
\newblock Competition-level code generation with alphacode.
\newblock {\em CoRR}, abs/2203.07814, 2022.

\bibitem{lewkowycz2022solving}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:3843--3857, 2022.

\bibitem{luo2023wizardmath}
Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.
\newblock Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.
\newblock {\em arXiv preprint arXiv:2308.09583}, 2023.

\bibitem{koncel2015parsing}
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena~Dumas Ang.
\newblock Parsing algebraic word problems into equations.
\newblock {\em Transactions of the Association for Computational Linguistics}, 3:585--597, 2015.

\bibitem{roy2016solving}
Subhro Roy and Dan Roth.
\newblock Solving general arithmetic word problems.
\newblock {\em arXiv preprint arXiv:1608.01413}, 2016.

\bibitem{patel2021nlp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are nlp models really able to solve simple math word problems?
\newblock {\em arXiv preprint arXiv:2103.07191}, 2021.

\bibitem{hosseini2014learning}
Mohammad~Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman.
\newblock Learning to solve arithmetic word problems with verb categorization.
\newblock In {\em Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 523--533, 2014.

\bibitem{ling2017program}
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
\newblock Program induction by rationale generation: Learning to solve and explain algebraic word problems.
\newblock {\em arXiv preprint arXiv:1705.04146}, 2017.

\bibitem{amini2019mathqa}
Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.
\newblock Mathqa: Towards interpretable math word problem solving with operation-based formalisms.
\newblock {\em arXiv preprint arXiv:1905.13319}, 2019.

\bibitem{azerbayev2023llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
\newblock Llemma: An open language model for mathematics.
\newblock {\em arXiv preprint arXiv:2310.10631}, 2023.

\bibitem{mishra2022numglue}
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan.
\newblock Numglue: A suite of fundamental yet challenging mathematical reasoning tasks.
\newblock {\em arXiv preprint arXiv:2204.05660}, 2022.

\bibitem{mishra2022lila}
Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et~al.
\newblock Lila: A unified benchmark for mathematical reasoning.
\newblock {\em arXiv preprint arXiv:2210.17517}, 2022.

\bibitem{chen2023theoremqa}
Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia.
\newblock Theoremqa: A theorem-driven question answering dataset.
\newblock {\em arXiv preprint arXiv:2305.12524}, 2023.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824--24837, 2022.

\bibitem{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em Advances in neural information processing systems}, 35:22199--22213, 2022.

\bibitem{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock {\em arXiv preprint arXiv:2203.11171}, 2022.

\bibitem{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock {\em arXiv preprint arXiv:2210.09261}, 2022.

\bibitem{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock {\em arXiv preprint arXiv:2112.00114}, 2021.

\bibitem{wang2022iteratively}
Boshi Wang, Xiang Deng, and Huan Sun.
\newblock Iteratively prompt pre-trained language models for chain of thought.
\newblock {\em arXiv preprint arXiv:2203.08383}, 2022.

\bibitem{wang2022towards}
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.
\newblock Towards understanding chain-of-thought prompting: An empirical study of what matters.
\newblock {\em arXiv preprint arXiv:2212.10001}, 2022.

\bibitem{li2023making}
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.
\newblock Making language models better reasoners with step-aware verifier.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 5315--5333, 2023.

\bibitem{wang2023making}
Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui.
\newblock Making large language models better reasoners with alignment.
\newblock {\em arXiv preprint arXiv:2309.02144}, 2023.

\bibitem{drozdov2022compositional}
Andrew Drozdov, Nathanael Sch{\"a}rli, Ekin Aky{\"u}rek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou.
\newblock Compositional semantic parsing with large language models.
\newblock {\em arXiv preprint arXiv:2209.15003}, 2022.

\bibitem{zhou2022least}
Denny Zhou, Nathanael Sch{\"a}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et~al.
\newblock Least-to-most prompting enables complex reasoning in large language models.
\newblock {\em arXiv preprint arXiv:2205.10625}, 2022.

\bibitem{lilong2024autore}
Xue Lilong, Zhang Dan, Dong Yuxiao, and Tang Jie.
\newblock Autore: Document-level relation extraction with large language models.
\newblock {\em arXiv preprint arXiv:2403.14888}, 2024.

\bibitem{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock {\em arXiv preprint arXiv:2210.03629}, 2022.

\bibitem{chen2022program}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W Cohen.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.
\newblock {\em arXiv preprint arXiv:2211.12588}, 2022.

\bibitem{gou2023critic}
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
\newblock Critic: Large language models can self-correct with tool-interactive critiquing.
\newblock {\em arXiv preprint arXiv:2305.11738}, 2023.

\bibitem{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock {\em arXiv preprint arXiv:2111.02080}, 2021.

\bibitem{wang2023plan}
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.
\newblock Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models.
\newblock {\em arXiv preprint arXiv:2305.04091}, 2023.

\bibitem{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex instructions.
\newblock {\em arXiv preprint arXiv:2304.12244}, 2023.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{sanh2021multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock {\em arXiv preprint arXiv:2110.08207}, 2021.

\bibitem{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock {\em arXiv preprint arXiv:2301.13688}, 2023.

\bibitem{wang2022self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated instructions.
\newblock {\em arXiv preprint arXiv:2212.10560}, 2022.

\bibitem{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock {\em arXiv preprint arXiv:2304.03277}, 2023.

\bibitem{wang2023far}
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi~Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah~A Smith, Iz~Beltagy, et~al.
\newblock How far can camels go? exploring the state of instruction tuning on open resources.
\newblock {\em arXiv preprint arXiv:2306.04751}, 2023.

\bibitem{lee2023platypus}
Ariel~N Lee, Cole~J Hunter, and Nataniel Ruiz.
\newblock Platypus: Quick, cheap, and powerful refinement of llms.
\newblock {\em arXiv preprint arXiv:2308.07317}, 2023.

\end{thebibliography}
