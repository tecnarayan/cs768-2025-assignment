\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, McGrew, Tobin, Pieter~Abbeel, and Zaremba]{hindsight_openai_2017}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter~Abbeel, and Wojciech Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{mbpp}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Chaudhary(2023)]{codealpaca}
Sahil Chaudhary.
\newblock Code alpaca: An instruction-following llama model for code generation, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{humaneval}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2022)Chen, Ma, Wang, and Cohen]{pot}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W. Cohen.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.
\newblock \emph{CoRR}, abs/2211.12588, 2022.

\bibitem[Chen et~al.(2023)Chen, Lin, Sch{\"a}rli, and Zhou]{chen2023teaching}
Xinyun Chen, Maxwell Lin, Nathanael Sch{\"a}rli, and Denny Zhou.
\newblock Teaching large language models to self-debug.
\newblock \emph{arXiv preprint arXiv:2304.05128}, 2023.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[codefuse ai(2023{\natexlab{a}})]{codeexercise}
codefuse ai.
\newblock The codeexercise-python-27k dataset.
\newblock \url{https://huggingface.co/datasets/codefuse-ai/CodeExercise-Python-27k}, 2023{\natexlab{a}}.

\bibitem[codefuse ai(2023{\natexlab{b}})]{evolinstructcode}
codefuse ai.
\newblock The evolinstrutcode dataset.
\newblock \url{https://huggingface.co/datasets/codefuse-ai/Evol-instruction-66k}, 2023{\natexlab{b}}.

\bibitem[Dong et~al.(2023)Dong, Yuan, Lu, Li, Xue, Liu, Wang, Yuan, Zhou, and Zhou]{catastrophic_forgetting_ref_1}
Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou.
\newblock How abilities in large language models are affected by supervised fine-tuning data composition.
\newblock \emph{arXiv preprint arXiv:2310.05492}, 2023.

\bibitem[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red}
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.
\newblock Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.
\newblock \emph{arXiv preprint arXiv:2209.07858}, 2022.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, et~al.]{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, et~al.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}, 2023.

\bibitem[Guo et~al.(2024)Guo, Zhu, Yang, Xie, Dong, Zhang, Chen, Bi, Wu, Li, et~al.]{deepseek-coder}
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y~Wu, YK~Li, et~al.
\newblock Deepseek-coder: When the large language model meets programming--the rise of code intelligence.
\newblock \emph{arXiv preprint arXiv:2401.14196}, 2024.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, et~al.]{ivison2023camels}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al.
\newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2.
\newblock \emph{arXiv preprint arXiv:2311.10702}, 2023.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
Leslie~Pack Kaelbling.
\newblock Learning to achieve goals.
\newblock In \emph{IJCAI}, volume~2, pages 1094--8. Citeseer, 1993.

\bibitem[Kocetkov et~al.(2022)Kocetkov, Li, Allal, Li, Mou, Ferrandis, Jernite, Mitchell, Hughes, Wolf, et~al.]{kocetkov2022stack}
Denis Kocetkov, Raymond Li, Loubna~Ben Allal, Jia Li, Chenghao Mou, Carlos~Mu{\~n}oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et~al.
\newblock The stack: 3 tb of permissively licensed source code.
\newblock \emph{arXiv preprint arXiv:2211.15533}, 2022.

\bibitem[Korbak et~al.(2023)Korbak, Shi, Chen, Bhalerao, Buckley, Phang, Bowman, and Perez]{korbak2023pretraining}
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika~Vinayak Bhalerao, Christopher Buckley, Jason Phang, Samuel~R Bowman, and Ethan Perez.
\newblock Pretraining language models with human preferences.
\newblock In \emph{International Conference on Machine Learning}, pages 17506--17533. PMLR, 2023.

\bibitem[Kotha et~al.(2023)Kotha, Springer, and Raghunathan]{catastrophic_forgetting_ref_3}
Suhas Kotha, Jacob~Mitchell Springer, and Aditi Raghunathan.
\newblock Understanding catastrophic forgetting in language models via implicit inference.
\newblock \emph{arXiv preprint arXiv:2309.10105}, 2023.

\bibitem[Lai et~al.(2023)Lai, Li, Wang, Zhang, Zhong, Zettlemoyer, Yih, Fried, Wang, and Yu]{ds-1000}
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
\newblock Ds-1000: A natural and reliable benchmark for data science code generation.
\newblock In \emph{International Conference on Machine Learning}, pages 18319--18345. PMLR, 2023.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, et~al.]{alphacode}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago, et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092--1097, 2022.

\bibitem[Liang et~al.(2023)Liang, Huang, Xia, Xu, Hausman, Ichter, Florence, and Zeng]{code_as_policy}
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng.
\newblock Code as policies: Language model programs for embodied control.
\newblock In \emph{ICRA}, pages 9493--9500. {IEEE}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Sferrazza, and Abbeel]{chain_of_hindsight}
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel.
\newblock Chain of hindsight aligns language models with feedback.
\newblock \emph{arXiv preprint arXiv:2302.02676}, 3, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Xia, Wang, and Zhang]{humaneval+}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
\newblock \emph{arXiv preprint arXiv:2305.01210}, 2023{\natexlab{b}}.

\bibitem[Loshchilov and Hutter(2017)]{adam}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Luo et~al.(2023{\natexlab{a}})Luo, Yang, Meng, Li, Zhou, and Zhang]{catastrophic_forgetting_ref_2}
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.
\newblock An empirical study of catastrophic forgetting in large language models during continual fine-tuning.
\newblock \emph{arXiv preprint arXiv:2308.08747}, 2023{\natexlab{a}}.

\bibitem[Luo et~al.(2023{\natexlab{b}})Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{wizardcoder}
Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.
\newblock Wizardcoder: Empowering code large language models with evol-instruct.
\newblock \emph{arXiv preprint arXiv:2306.08568}, 2023{\natexlab{b}}.

\bibitem[McInnes et~al.(2018)McInnes, Healy, and Melville]{mcinnes2018umap}
Leland McInnes, John Healy, and James Melville.
\newblock Umap: Uniform manifold approximation and projection for dimension reduction.
\newblock \emph{arXiv preprint arXiv:1802.03426}, 2018.

\bibitem[Nijkamp et~al.(2022)Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese, and Xiong]{nijkamp2022codegen}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.
\newblock Codegen: An open large language model for code with multi-turn program synthesis.
\newblock \emph{arXiv preprint arXiv:2203.13474}, 2022.

\bibitem[OpenAI(2022)]{chatgpt}
OpenAI.
\newblock Chatgpt: Optimizing language models for dialogue.
\newblock \url{https://openai.com/blog/chatgpt/}, 2022.

\bibitem[OpenAI(2023)]{GPT-4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock Technical report, 2023.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{codellama}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{bbh}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem[theblackcat102(2023)]{evolcodealpaca}
theblackcat102.
\newblock The evolved code alpaca dataset.
\newblock \url{https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Cheng, Zhan, Li, Song, and Liu]{wang2023openchat}
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.
\newblock Openchat: Advancing open-source language models with mixed-quality data.
\newblock \emph{arXiv preprint arXiv:2309.11235}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Ivison, Dasigi, Hessel, Khot, Chandu, Wadden, MacMillan, Smith, Beltagy, et~al.]{tulu}
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi~Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah~A Smith, Iz~Beltagy, et~al.
\newblock How far can camels go? exploring the state of instruction tuning on open resources.
\newblock \emph{arXiv preprint arXiv:2306.04751}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2023)Wei, Wang, Liu, Ding, and Zhang]{wei2023magicoder}
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.
\newblock Magicoder: Source code is all you need.
\newblock \emph{arXiv preprint arXiv:2312.02120}, 2023.

\bibitem[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex instructions.
\newblock \emph{arXiv preprint arXiv:2304.12244}, 2023.

\bibitem[Yang et~al.(2024)Yang, Liu, Wu, Yang, Fung, Li, Huang, Cao, Wang, Wang, Ji, and Zhai]{agent_code_survey}
Ke~Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi~R. Fung, Sha Li, Zixuan Huang, Xu~Cao, Xingyao Wang, Yiquan Wang, Heng Ji, and Chengxiang Zhai.
\newblock If {LLM} is the wizard, then code is the wand: {A} survey on how code empowers large language models to serve as intelligent agents.
\newblock \emph{CoRR}, abs/2401.00812, 2024.

\bibitem[Yu et~al.(2023)Yu, Zhang, Shang, Huang, Xu, Zhao, Hu, and Yin]{wavecoder}
Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin.
\newblock Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation.
\newblock \emph{arXiv preprint arXiv:2312.14187}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Liu, Wong, Abbeel, and Gonzalez]{hindsight_wisdom_2023}
Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph~E Gonzalez.
\newblock The wisdom of hindsight makes language models better instruction followers.
\newblock \emph{arXiv preprint arXiv:2302.05206}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Xia, Zou, Dong, Wang, Xue, Wang, Shen, Wang, Li, et~al.]{humanevalx}
Qinkai Zheng, Xiao Xia, Xu~Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et~al.
\newblock Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x.
\newblock \emph{arXiv preprint arXiv:2303.17568}, 2023.

\end{thebibliography}
