\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye et~al.(2017)Athalye, Engstrom, Ilyas, and
  Kwok]{athalye2017synthesizing}
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock \emph{arXiv preprint arXiv:1707.07397}, 2017.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock \emph{ICML}, arXiv preprint arXiv:1802.00420, 2018.

\bibitem[Bau et~al.(2017)Bau, Zhou, Khosla, Oliva, and
  Torralba]{bau2017network}
David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Network dissection: Quantifying interpretability of deep visual
  representations.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  6541--6549, 2017.

\bibitem[Brown et~al.(2017)Brown, Man{\'e}, Roy, Abadi, and
  Gilmer]{brown2017adversarial}
Tom~B Brown, Dandelion Man{\'e}, Aurko Roy, Mart{\'\i}n Abadi, and Justin
  Gilmer.
\newblock Adversarial patch.
\newblock \emph{arXiv preprint arXiv:1712.09665}, 2017.

\bibitem[Carlini(2019)]{carlini2019ami}
Nicholas Carlini.
\newblock Is ami (attacks meet interpretability) robust to adversarial
  examples?
\newblock \emph{arXiv preprint arXiv:1902.02322}, 2019.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{IEEE Symposium on Security and Privacy (SP)}, pp.\  39--57.
  IEEE, 2017.

\bibitem[Chattopadhay et~al.(2018)Chattopadhay, Sarkar, Howlader, and
  Balasubramanian]{chattopadhay2018grad}
Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth~N
  Balasubramanian.
\newblock Grad-{CAM}++: Generalized gradient-based visual explanations for deep
  convolutional networks.
\newblock In \emph{2018 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  839--847. IEEE, 2018.

\bibitem[Chen et~al.(2019)Chen, Wu, Rastogi, Liang, and Jha]{chen2019robust}
Jiefeng Chen, Xi~Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha.
\newblock Robust attribution regularization, 2019.

\bibitem[Chen et~al.(2018)Chen, Sharma, Zhang, Yi, and Hsieh]{chen2017ead}
Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh.
\newblock Ead: elastic-net attacks to deep neural networks via adversarial
  examples.
\newblock \emph{AAAI}, 2018.

\bibitem[Dombrowski et~al.(2019)Dombrowski, Alber, Anders, Ackermann,
  M{\"u}ller, and Kessel]{dombrowski2019explanations}
Ann-Kathrin Dombrowski, Maximilian Alber, Christopher~J Anders, Marcel
  Ackermann, Klaus-Robert M{\"u}ller, and Pan Kessel.
\newblock Explanations can be manipulated and geometry is to blame.
\newblock \emph{arXiv preprint arXiv:1906.07983}, 2019.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, Tsipras, Tran, and
  Madry]{engstrom2019learning}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial robustness as a prior for learned representations, 2019.

\bibitem[Eykholt et~al.(2018)Eykholt, Evtimov, Fernandes, Li, Rahmati, Xiao,
  Prakash, Kohno, and Song]{eykholt2018robust}
K.~Eykholt, I.~Evtimov, E.~Fernandes, B.~Li, A.~Rahmati, C.~Xiao, A.~Prakash,
  T.~Kohno, and D.~Song.
\newblock Robust physical-world attacks on deep learning visual classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1625--1634, 2018.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Abid, and
  Zou]{ghorbani2019interpretation}
Amirata Ghorbani, Abubakar Abid, and James Zou.
\newblock Interpretation of neural networks is fragile.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  3681--3688, 2019.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{Goodfellow2015explaining}
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{ICLR}, arXiv preprint arXiv:1412.6572, 2015.

\bibitem[Kang et~al.(2019)Kang, Sun, Hendrycks, Brown, and
  Steinhardt]{kang2019testing}
Daniel Kang, Yi~Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt.
\newblock Testing robustness against unforeseen adversaries.
\newblock \emph{arXiv preprint arXiv:1908.08016}, 2019.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{ICLR}, arXiv preprint arXiv:1706.06083, 2018.

\bibitem[Moosavi-Dezfooli et~al.(2019)Moosavi-Dezfooli, Fawzi, Uesato, and
  Frossard]{moosavi2019robustness}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal
  Frossard.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  9078--9086, 2019.

\bibitem[Papernot et~al.(2016{\natexlab{a}})Papernot, McDaniel, Jha,
  Fredrikson, Celik, and Swami]{papernot2016limitations}
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z~Berkay
  Celik, and Ananthram Swami.
\newblock The limitations of deep learning in adversarial settings.
\newblock In \emph{IEEE European Symposium on Security and Privacy (EuroS\&P)},
  pp.\  372--387. IEEE, 2016{\natexlab{a}}.

\bibitem[Papernot et~al.(2016{\natexlab{b}})Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{2016 IEEE Symposium on Security and Privacy (SP)}, pp.\
  582--597. IEEE, 2016{\natexlab{b}}.

\bibitem[Petsiuk et~al.(2018)Petsiuk, Das, and Saenko]{petsiuk2018rise}
Vitali Petsiuk, Abir Das, and Kate Saenko.
\newblock Rise: Randomized input sampling for explanation of black-box models.
\newblock \emph{arXiv preprint arXiv:1806.07421}, 2018.

\bibitem[Ross \& Doshi-Velez(2018)Ross and Doshi-Velez]{ross2018improving}
Andrew~Slavin Ross and Finale Doshi-Velez.
\newblock Improving the adversarial robustness and interpretability of deep
  neural networks by regularizing their input gradients.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[Selvaraju et~al.(2017)Selvaraju, Cogswell, Das, Vedantam, Parikh, and
  Batra]{selvaraju2017grad}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-{CAM}: Visual explanations from deep networks via gradient-based
  localization.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  618--626, 2017.

\bibitem[Simonyan et~al.(2013)Simonyan, Vedaldi, and
  Zisserman]{simonyan2013deep}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock \emph{arXiv preprint arXiv:1312.6034}, 2013.

\bibitem[Smilkov et~al.(2017)Smilkov, Thorat, Kim, Vi{\'e}gas, and
  Wattenberg]{smilkov2017smoothgrad}
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi{\'e}gas, and Martin
  Wattenberg.
\newblock Smoothgrad: removing noise by adding noise.
\newblock \emph{arXiv preprint arXiv:1706.03825}, 2017.

\bibitem[Springenberg et~al.(2014)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg2014striving}
Jost~Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin
  Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{arXiv preprint arXiv:1412.6806}, 2014.

\bibitem[Su et~al.(2018)Su, Zhang, Chen, Yi, Chen, and Gao]{su2018robustness}
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao.
\newblock Is robustness the cost of accuracy?--a comprehensive study on the
  robustness of 18 deep image classification models.
\newblock \emph{arXiv preprint arXiv:1808.01688}, 2018.

\bibitem[Subramanya et~al.(2018)Subramanya, Pillai, and
  Pirsiavash]{subramanya2018towards}
Akshayvarun Subramanya, Vipin Pillai, and Hamed Pirsiavash.
\newblock Towards hiding adversarial examples from network interpretation.
\newblock \emph{arXiv preprint arXiv:1812.02843}, 2018.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and
  Yan]{sundararajan2017axiomatic}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  3319--3328. JMLR. org, 2017.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2014intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{ICLR}, arXiv preprint arXiv:1312.6199, 2014.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SyxAb30cY7}.

\bibitem[Xu et~al.(2019{\natexlab{a}})Xu, Liu, Zhang, Sun, Zhao, Fan, Gan, and
  Lin]{xu2019interpreting}
Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu~Zhao, Quanfu Fan, Chuang
  Gan, and Xue Lin.
\newblock Interpreting adversarial examples by activation promotion and
  suppression.
\newblock \emph{arXiv preprint arXiv:1904.02057}, 2019{\natexlab{a}}.

\bibitem[Xu et~al.(2019{\natexlab{b}})Xu, Liu, Zhao, Chen, Zhang, Fan,
  Erdogmus, Wang, and Lin]{xu2018structured}
Kaidi Xu, Sijia Liu, Pu~Zhao, Pin-Yu Chen, Huan Zhang, Quanfu Fan, Deniz
  Erdogmus, Yanzhi Wang, and Xue Lin.
\newblock Structured adversarial attack: Towards general implementation and
  better interpretability.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Yeh et~al.(2019)Yeh, Hsieh, Suggala, Inouye, and
  Ravikumar]{yeh2019infidelity}
Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun~Sai Suggala, David Inouye, and Pradeep
  Ravikumar.
\newblock On the (in)fidelity and sensitivity for explanations, 2019.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{BMVC}, 2016.

\bibitem[Zeiler \& Fergus(2014)Zeiler and Fergus]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European conference on computer vision}, pp.\  818--833.
  Springer, 2014.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric~P Xing, Laurent~El Ghaoui, and
  Michael~I Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock \emph{arXiv preprint arXiv:1901.08573}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Wang, Ji, Shen, and Wang]{ZWJ+18}
Xinyang Zhang, Ningfei Wang, Shouling Ji, Hua Shen, and Ting Wang.
\newblock Interpretable deep learning under fire.
\newblock \emph{CoRR}, abs/1812.00891, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.00891}.

\bibitem[Zhou et~al.(2016)Zhou, Khosla, Lapedriza, Oliva, and
  Torralba]{zhou2016learning}
B.~Zhou, A.~Khosla, A.~Lapedriza, A.~Oliva, and A.~Torralba.
\newblock Learning deep features for discriminative localization.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2921--2929, 2016.

\end{thebibliography}
