\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Arora, Anand, and Arora]{agarwal2020contextual}
Agarwal, S., Arora, H., Anand, S., and Arora, C.
\newblock Contextual diversity for active learning.
\newblock In \emph{ECCV}, pp.\  137--153. Springer, 2020.

\bibitem[Alain et~al.(2015)Alain, Lamb, Sankar, Courville, and Bengio]{alain2015variance}
Alain, G., Lamb, A., Sankar, C., Courville, A., and Bengio, Y.
\newblock Variance reduction in sgd by distributed importance sampling.
\newblock \emph{ArXiv preprint}, abs/1511.06481, 2015.

\bibitem[Bach(2013)]{DBLP:journals/ftml/Bach13}
Bach, F.~R.
\newblock Learning with submodular functions: {A} convex optimization perspective.
\newblock \emph{Found. Trends Mach. Learn.}, 6\penalty0 (2-3):\penalty0 145--373, 2013.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
\newblock Curriculum learning.
\newblock In \emph{{ICML}}, volume 382, pp.\  41--48, 2009.

\bibitem[Chen et~al.(2017)Chen, Papandreou, Schroff, and Adam]{DBLP:journals/corr/ChenPSA17}
Chen, L., Papandreou, G., Schroff, F., and Adam, H.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock \emph{CoRR}, abs/1706.05587, 2017.

\bibitem[Chen et~al.(2018)Chen, Zhu, Papandreou, Schroff, and Adam]{DBLP:conf/eccv/ChenZPSA18}
Chen, L., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H.
\newblock Encoder-decoder with atrous separable convolution for semantic image segmentation.
\newblock In \emph{{ECCV}}, volume 11211 of \emph{Lecture Notes in Computer Science}, pp.\  833--851. Springer, 2018.

\bibitem[Coleman et~al.(2020)Coleman, Yeh, Mussmann, Mirzasoleiman, Bailis, Liang, Leskovec, and Zaharia]{DBLP:conf/iclr/ColemanYMMBLLZ20}
Coleman, C., Yeh, C., Mussmann, S., Mirzasoleiman, B., Bailis, P., Liang, P., Leskovec, J., and Zaharia, M.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock In \emph{{ICLR}}. OpenReview.net, 2020.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{DBLP:conf/nips/CubukZS020}
Cubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.
\newblock Randaugment: Practical automated data augmentation with a reduced search space.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Cui et~al.(2019)Cui, Jia, Lin, Song, and Belongie]{DBLP:conf/cvpr/CuiJLSB19}
Cui, Y., Jia, M., Lin, T., Song, Y., and Belongie, S.~J.
\newblock Class-balanced loss based on effective number of samples.
\newblock In \emph{CVPR}, pp.\  9268--9277. Computer Vision Foundation / {IEEE}, 2019.

\bibitem[Das et~al.(2023)Das, Bhatt, Bhalerao, Gao, Yang, and Bilmes]{das2023accelerating}
Das, A.~M., Bhatt, G., Bhalerao, M.~M., Gao, V.~R., Yang, R., and Bilmes, J.
\newblock Accelerating batch active learning using continual learning techniques.
\newblock \emph{TMLR}, 2023.
\newblock ISSN 2835-8856.

\bibitem[Deng et~al.(2023)Deng, Cui, and Zhu]{deng2023towards}
Deng, Z., Cui, P., and Zhu, J.
\newblock Towards accelerated model training via bayesian data selection.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Fan et~al.(2022)Fan, Wang, Yao, Lyu, Zhang, and Tian]{fan2022fedskip}
Fan, Z., Wang, Y., Yao, J., Lyu, L., Zhang, Y., and Tian, Q.
\newblock Fedskip: Combatting statistical heterogeneity with federated skip aggregation.
\newblock In \emph{ICDM}, pp.\  131--140. IEEE, 2022.

\bibitem[Fan et~al.(2023)Fan, Yao, Han, Zhang, Wang, et~al.]{fan2023federateda}
Fan, Z., Yao, J., Han, B., Zhang, Y., Wang, Y., et~al.
\newblock Federated learning with bilateral curation for partially class-disjoint data.
\newblock In \emph{NeurIPS}, volume~36, 2023.

\bibitem[Fan et~al.(2024)Fan, Hu, Yao, Niu, Zhang, Sugiyama, and Wang]{FedLESAM}
Fan, Z., Hu, S., Yao, J., Niu, G., Zhang, Y., Sugiyama, M., and Wang, Y.
\newblock Locally estimated global perturbations are better than local perturbations for federated sharpness-aware minimization.
\newblock In \emph{ICML}, 2024.

\bibitem[Guo et~al.(2022)Guo, Zhao, and Bai]{deepcore}
Guo, C., Zhao, B., and Bai, Y.
\newblock Deepcore: A comprehensive library for coreset selection in deep learning.
\newblock In Strauss, C., Cuzzocrea, A., Kotsis, G., Tjoa, A.~M., and Khalil, I. (eds.), \emph{DESA}, pp.\  181--195. Springer International Publishing, 2022.
\newblock ISBN 978-3-031-12423-5.

\bibitem[Hong et~al.(2023)Hong, Yao, Zhou, Zhang, and Wang]{hong2023long}
Hong, F., Yao, J., Zhou, Z., Zhang, Y., and Wang, Y.
\newblock Long-tailed partial label learning via dynamic rebalancing.
\newblock In \emph{{ICLR}}. OpenReview.net, 2023.

\bibitem[Hong et~al.(2024)Hong, Yao, Lyu, Zhou, Tsang, Zhang, and Wang]{hong2024on}
Hong, F., Yao, J., Lyu, Y., Zhou, Z., Tsang, I., Zhang, Y., and Wang, Y.
\newblock On harmonizing implicit subpopulations.
\newblock In \emph{{ICLR}}. OpenReview.net, 2024.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand, Andreetto, and Adam]{DBLP:journals/corr/HowardZCKWWAA17}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision applications.
\newblock \emph{CoRR}, abs/1704.04861, 2017.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen]{DBLP:conf/iclr/HuSWALWWC22}
Hu, E.~J., Shen, Y., Wallis, P., Allen{-}Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Hu et~al.(2021)Hu, Peng, Zhu, Zhen, and Lin]{DBLP:conf/cvpr/00020ZZ021}
Hu, P., Peng, X., Zhu, H., Zhen, L., and Lin, J.
\newblock Learning cross-modal retrieval with noisy labels.
\newblock In \emph{{CVPR}}, pp.\  5403--5413. Computer Vision Foundation / {IEEE}, 2021.

\bibitem[Jiang et~al.(2019)Jiang, Wong, Zhou, Andersen, Dean, Ganger, Joshi, Kaminsky, Kozuch, Lipton, and Pillai]{Jiang2019AcceleratingDL}
Jiang, A.~H., Wong, D. L.-K., Zhou, G., Andersen, D.~G., Dean, J., Ganger, G.~R., Joshi, G., Kaminsky, M., Kozuch, M.~A., Lipton, Z.~C., and Pillai, P.
\newblock Accelerating deep learning by focusing on the biggest losers.
\newblock \emph{ArXiv}, abs/1910.00762, 2019.

\bibitem[Jiang et~al.(2014)Jiang, Meng, Yu, Lan, Shan, and Hauptmann]{DBLP:conf/nips/JiangMYLSH14}
Jiang, L., Meng, D., Yu, S., Lan, Z., Shan, S., and Hauptmann, A.~G.
\newblock Self-paced learning with diversity.
\newblock In \emph{NeurIPS}, pp.\  2078--2086, 2014.

\bibitem[Jiang et~al.(2015)Jiang, Meng, Zhao, Shan, and Hauptmann]{jiang2015self}
Jiang, L., Meng, D., Zhao, Q., Shan, S., and Hauptmann, A.~G.
\newblock Self-paced curriculum learning.
\newblock In \emph{AAAI}, pp.\  2694--2700. {AAAI} Press, 2015.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa, Bates, Bhatia, Boden, Borchers, et~al.]{jouppi2017datacenter}
Jouppi, N.~P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et~al.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock In \emph{ISCA}, pp.\  1--12, 2017.

\bibitem[Katharopoulos \& Fleuret(2018{\natexlab{a}})Katharopoulos and Fleuret]{DBLP:conf/icml/KatharopoulosF18}
Katharopoulos, A. and Fleuret, F.
\newblock Not all samples are created equal: Deep learning with importance sampling.
\newblock In \emph{{ICML}}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  2530--2539. {PMLR}, 2018{\natexlab{a}}.

\bibitem[Katharopoulos \& Fleuret(2018{\natexlab{b}})Katharopoulos and Fleuret]{pmlr-v80-katharopoulos18a}
Katharopoulos, A. and Fleuret, F.
\newblock Not all samples are created equal: Deep learning with importance sampling.
\newblock In Dy, J. and Krause, A. (eds.), \emph{ICML}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  2525--2534. PMLR, 10--15 Jul 2018{\natexlab{b}}.

\bibitem[Kawaguchi \& Lu(2020)Kawaguchi and Lu]{DBLP:conf/aistats/KawaguchiL20}
Kawaguchi, K. and Lu, H.
\newblock Ordered {SGD:} {A} new stochastic optimization framework for empirical risk minimization.
\newblock In \emph{{AISTATS}}, volume 108 of \emph{Proceedings of Machine Learning Research}, pp.\  669--679. {PMLR}, 2020.

\bibitem[Killamsetty et~al.(2021)Killamsetty, Sivasubramanian, Ramakrishnan, and Iyer]{DBLP:conf/aaai/KillamsettySRI21}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., and Iyer, R.~K.
\newblock {GLISTER:} generalization based data subset selection for efficient and robust learning.
\newblock In \emph{{AAAI}}, pp.\  8110--8118. {AAAI} Press, 2021.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson, Xiao, Whitehead, Berg, Lo, Dollar, and Girshick]{Kirillov_2023_ICCV}
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.~C., Lo, W.-Y., Dollar, P., and Girshick, R.
\newblock Segment anything.
\newblock In \emph{ICCV}, pp.\  4015--4026, October 2023.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kulesza et~al.(2012)Kulesza, Taskar, et~al.]{kulesza2012determinantal}
Kulesza, A., Taskar, B., et~al.
\newblock Determinantal point processes for machine learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 5\penalty0 (2--3):\penalty0 123--286, 2012.

\bibitem[Lau \& Baldwin(2016)Lau and Baldwin]{DBLP:conf/rep4nlp/LauB16}
Lau, J.~H. and Baldwin, T.
\newblock An empirical evaluation of doc2vec with practical insights into document embedding generation.
\newblock In \emph{Rep4NLP@ACL 2016}, pp.\  78--86. Association for Computational Linguistics, 2016.

\bibitem[Le \& Yang(2015)Le and Yang]{Le2015TinyIV}
Le, Y. and Yang, X.~S.
\newblock Tiny imagenet visual recognition challenge.
\newblock 2015.

\bibitem[Loshchilov \& Hutter(2015)Loshchilov and Hutter]{Loshchilov2015OnlineBS}
Loshchilov, I. and Hutter, F.
\newblock Online batch selection for faster training of neural networks.
\newblock \emph{ArXiv}, abs/1511.06343, 2015.

\bibitem[Menon et~al.(2021)Menon, Jayasumana, Rawat, Jain, Veit, and Kumar]{DBLP:conf/iclr/MenonJRJVK21}
Menon, A.~K., Jayasumana, S., Rawat, A.~S., Jain, H., Veit, A., and Kumar, S.
\newblock Long-tail learning via logit adjustment.
\newblock In \emph{{ICLR}}. OpenReview.net, 2021.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch, Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, and Gal]{pmlr-v162-mindermann22a}
Mindermann, S., Brauner, J.~M., Razzak, M.~T., Sharma, M., Kirsch, A., Xu, W., H{\"o}ltgen, B., Gomez, A.~N., Morisot, A., Farquhar, S., and Gal, Y.
\newblock Prioritized training on points that are learnable, worth learning, and not yet learnt.
\newblock In \emph{ICML}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  15630--15649. PMLR, 17--23 Jul 2022.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Bilmes, and Leskovec]{DBLP:conf/icml/MirzasoleimanBL20}
Mirzasoleiman, B., Bilmes, J.~A., and Leskovec, J.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{{ICML}}, volume 119 of \emph{Proceedings of Machine Learning Research}, pp.\  6950--6960. {PMLR}, 2020.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and Fisher]{DBLP:journals/mp/NemhauserWF78}
Nemhauser, G.~L., Wolsey, L.~A., and Fisher, M.~L.
\newblock An analysis of approximations for maximizing submodular set functions - {I}.
\newblock \emph{Math. Program.}, 14\penalty0 (1):\penalty0 265--294, 1978.

\bibitem[Novikova et~al.(2017)Novikova, Dusek, and Rieser]{DBLP:conf/sigdial/NovikovaDR17}
Novikova, J., Dusek, O., and Rieser, V.
\newblock The {E2E} dataset: New challenges for end-to-end generation.
\newblock In \emph{Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue}, pp.\  201--206. Association for Computational Linguistics, 2017.

\bibitem[OpenAI(2023)]{DBLP:journals/corr/abs-2303-08774}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.

\bibitem[Paul et~al.(2021)Paul, Ganguli, and Dziugaite]{DBLP:conf/nips/PaulGD21}
Paul, M., Ganguli, S., and Dziugaite, G.~K.
\newblock Deep learning on a data diet: Finding important examples early in training.
\newblock In \emph{NeurIPS}, pp.\  20596--20607, 2021.

\bibitem[Pleiss et~al.(2020)Pleiss, Zhang, Elenberg, and Weinberger]{DBLP:conf/nips/Pleiss0EW20}
Pleiss, G., Zhang, T., Elenberg, E.~R., and Weinberger, K.~Q.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Qin et~al.(2024)Qin, Wang, Zheng, Gu, Peng, xu~Zhao~Pan, Zhou, Shang, Sun, Xie, and You]{qin2024infobatch}
Qin, Z., Wang, K., Zheng, Z., Gu, J., Peng, X., xu~Zhao~Pan, Zhou, D., Shang, L., Sun, B., Xie, X., and You, Y.
\newblock Infobatch: Lossless training speed up by unbiased dynamic data pruning.
\newblock In \emph{ICLR}, 2024.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{DBLP:conf/icml/RadfordKHRGASAM21}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{{ICML}}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  8748--8763. {PMLR}, 2021.

\bibitem[Rasiwasia et~al.(2010)Rasiwasia, Pereira, Coviello, Doyle, Lanckriet, Levy, and Vasconcelos]{DBLP:conf/mm/RasiwasiaPCDLLV10}
Rasiwasia, N., Pereira, J.~C., Coviello, E., Doyle, G., Lanckriet, G. R.~G., Levy, R., and Vasconcelos, N.
\newblock A new approach to cross-modal multimedia retrieval.
\newblock In \emph{Proceedings of the 18th International Conference on Multimedia}, pp.\  251--260. {ACM}, 2010.

\bibitem[Ren et~al.(2021)Ren, Xiao, Chang, Huang, Li, Gupta, Chen, and Wang]{ren2021survey}
Ren, P., Xiao, Y., Chang, X., Huang, P.-Y., Li, Z., Gupta, B.~B., Chen, X., and Wang, X.
\newblock A survey of deep active learning.
\newblock \emph{ACM computing surveys (CSUR)}, 54\penalty0 (9):\penalty0 1--40, 2021.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{Rombach_2022_CVPR}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{CVPR}, pp.\  10684--10695, June 2022.

\bibitem[Sener \& Savarese(2018)Sener and Savarese]{sener2018active}
Sener, O. and Savarese, S.
\newblock Active learning for convolutional neural networks: A core-set approach.
\newblock In \emph{ICLR}, 2018.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{DBLP:journals/corr/SimonyanZ14a}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{{ICLR}}, 2015.

\bibitem[Sinha et~al.(2020)Sinha, Garg, and Larochelle]{sinha2020curriculum}
Sinha, S., Garg, A., and Larochelle, H.
\newblock Curriculum by smoothing.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Toneva et~al.(2019)Toneva, Sordoni, des Combes, Trischler, Bengio, and Gordon]{DBLP:conf/iclr/TonevaSCTBG19}
Toneva, M., Sordoni, A., des Combes, R.~T., Trischler, A., Bengio, Y., and Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network learning.
\newblock In \emph{{ICLR}}. OpenReview.net, 2019.

\bibitem[Tremblay et~al.(2019)Tremblay, Barthelm{\'e}, and Amblard]{tremblay2019determinantal}
Tremblay, N., Barthelm{\'e}, S., and Amblard, P.-O.
\newblock Determinantal point processes for coresets.
\newblock \emph{J. Mach. Learn. Res.}, 20:\penalty0 168--1, 2019.

\bibitem[van~der Maaten \& Hinton(2008)van~der Maaten and Hinton]{JMLR:v9:vandermaaten08a}
van~der Maaten, L. and Hinton, G.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (86):\penalty0 2579--2605, 2008.

\bibitem[Wei et~al.(2015)Wei, Iyer, and Bilmes]{DBLP:conf/icml/WeiIB15}
Wei, K., Iyer, R.~K., and Bilmes, J.~A.
\newblock Submodularity in data subset selection and active learning.
\newblock In Bach, F.~R. and Blei, D.~M. (eds.), \emph{{ICML}}, volume~37 of \emph{{JMLR} Workshop and Conference Proceedings}, pp.\  1954--1963. JMLR.org, 2015.

\bibitem[Xia et~al.(2023)Xia, Liu, Yu, Shen, Han, and Liu]{DBLP:conf/iclr/XiaL0S0L23}
Xia, X., Liu, J., Yu, J., Shen, X., Han, B., and Liu, T.
\newblock Moderate coreset: {A} universal method of data selection for real-world data-efficient deep learning.
\newblock In \emph{{ICLR}}. OpenReview.net, 2023.

\bibitem[Xie et~al.(2023)Xie, Santurkar, Ma, and Liang]{DBLP:journals/corr/abs-2302-03169}
Xie, S.~M., Santurkar, S., Ma, T., and Liang, P.
\newblock Data selection for language models via importance resampling.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Xin et~al.(2024)Xin, Jiawei, Yunsong, Weiying, and Zhou]{zhang2024TDDS}
Xin, Z., Jiawei, D., Yunsong, L., Weiying, X., and Zhou, J.~T.
\newblock Spanning training progress: Temporal dual-depth scoring (tdds) for enhanced dataset pruning.
\newblock In \emph{CVPR}. Computer Vision Foundation / {IEEE}, 2024.

\bibitem[Yang et~al.(2023)Yang, Zhang, Katabi, and Ghassemi]{DBLP:conf/icml/YangZKG23}
Yang, Y., Zhang, H., Katabi, D., and Ghassemi, M.
\newblock Change is hard: {A} closer look at subpopulation shift.
\newblock In \emph{ICML}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  39584--39622. {PMLR}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Liu, Lai, and Prakash]{DBLP:conf/iclr/ZhengLL023}
Zheng, H., Liu, R., Lai, F., and Prakash, A.
\newblock Coverage-centric coreset selection for high pruning rates.
\newblock In \emph{{ICLR}}. OpenReview.net, 2023.

\bibitem[Zhou \& Bilmes(2018)Zhou and Bilmes]{DBLP:conf/iclr/ZhouB18}
Zhou, T. and Bilmes, J.~A.
\newblock Minimax curriculum learning: Machine teaching with desirable difficulties and scheduled diversity.
\newblock In \emph{6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}. OpenReview.net, 2018.

\bibitem[Zhou et~al.(2021)Zhou, Wang, and Bilmes]{DBLP:conf/aistats/ZhouWB21}
Zhou, T., Wang, S., and Bilmes, J.~A.
\newblock Curriculum learning by optimizing learning dynamics.
\newblock In Banerjee, A. and Fukumizu, K. (eds.), \emph{{AISTATS}}, volume 130 of \emph{Proceedings of Machine Learning Research}, pp.\  433--441. {PMLR}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Yang, Wong, Wan, and Chao]{zhou2020uncertainty}
Zhou, Y., Yang, B., Wong, D.~F., Wan, Y., and Chao, L.~S.
\newblock Uncertainty-aware curriculum learning for neural machine translation.
\newblock In \emph{ACL}, pp.\  6934--6944, Online, 2020. Association for Computational Linguistics.

\bibitem[Zhou et~al.(2022)Zhou, Yao, Wang, Han, and Zhang]{DBLP:conf/icml/ZhouYWHZ22}
Zhou, Z., Yao, J., Wang, Y., Han, B., and Zhang, Y.
\newblock Contrastive learning with boosted memorization.
\newblock In \emph{ICML}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  27367--27377. {PMLR}, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Yao, Hong, Zhang, Han, and Wang]{DBLP:conf/nips/0002Y0ZHW23}
Zhou, Z., Yao, J., Hong, F., Zhang, Y., Han, B., and Wang, Y.
\newblock Combating representation learning disparity with geometric harmonization.
\newblock In \emph{NeurIPS}, 2023.

\end{thebibliography}
