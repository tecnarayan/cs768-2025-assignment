\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{option-critic}
Bacon, P.-L., Harb, J., and Precup, D.
\newblock The option-critic architecture.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Bahl et~al.(2020)Bahl, Mukadam, Gupta, and Pathak]{NDP}
Bahl, S., Mukadam, M., Gupta, A., and Pathak, D.
\newblock Neural dynamic policies for end-to-end sensorimotor learning.
\newblock \emph{arXiv preprint arXiv:2012.02788}, 2020.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  Tb, Muldal, Heess, and Lillicrap]{d4pg}
Barth-Maron, G., Hoffman, M.~W., Budden, D., Dabney, W., Horgan, D., Tb, D.,
  Muldal, A., Heess, N., and Lillicrap, T.
\newblock Distributed distributional deterministic policy gradients.
\newblock \emph{arXiv preprint arXiv:1804.08617}, 2018.

\bibitem[Biedenkapp et~al.(2020)Biedenkapp, Rajan, Hutter, and
  Lindauer]{act-rep-cr}
Biedenkapp, A., Rajan, R., Hutter, F., and Lindauer, M.
\newblock Towards temporl: Learning when to act.
\newblock In \emph{Workshop on Inductive Biases, Invariances and Generalization
  in Reinforcement Learning (BIG@ ICMLâ€™20)}, 2020.

\bibitem[Braylan et~al.(2015)Braylan, Hollenbeck, Meyerson, and
  Miikkulainen]{frameskip-importance}
Braylan, A., Hollenbeck, M., Meyerson, E., and Miikkulainen, R.
\newblock Frame skip is a powerful parameter for learning to play atari.
\newblock In \emph{Workshops at the Twenty-Ninth AAAI Conference on Artificial
  Intelligence}, 2015.

\bibitem[Chandak et~al.(2019)Chandak, Theocharous, Kostas, Jordan, and
  Thomas]{single-action-rep-discrete-2}
Chandak, Y., Theocharous, G., Kostas, J., Jordan, S., and Thomas, P.~S.
\newblock Learning action representations for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1902.00183}, 2019.

\bibitem[Co-Reyes et~al.(2018)Co-Reyes, Liu, Gupta, Eysenbach, Abbeel, and
  Levine]{traj-embeddings-auto-encoder}
Co-Reyes, J.~D., Liu, Y., Gupta, A., Eysenbach, B., Abbeel, P., and Levine, S.
\newblock Self-consistent trajectory autoencoder: Hierarchical reinforcement
  learning with trajectory embeddings.
\newblock \emph{arXiv preprint arXiv:1806.02813}, 2018.

\bibitem[Dabney et~al.(2020)Dabney, Ostrovski, and Barreto]{tempExt0}
Dabney, W., Ostrovski, G., and Barreto, A.
\newblock Temporally-extended $\{\backslash$epsilon$\}$-greedy exploration.
\newblock \emph{arXiv preprint arXiv:2006.01782}, 2020.

\bibitem[Dayan \& Hinton(1993)Dayan and Hinton]{opt-sub-1}
Dayan, P. and Hinton, G.~E.
\newblock Feudal reinforcement learning.
\newblock In Hanson, S., Cowan, J., and Giles, C. (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~5, pp.\  271--278.
  Morgan-Kaufmann, 1993.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Paper.pdf}.

\bibitem[Dietterich(2000)]{opt-sub-2}
Dietterich, T.~G.
\newblock Hierarchical reinforcement learning with the maxq value function
  decomposition.
\newblock \emph{Journal of artificial intelligence research}, 13:\penalty0
  227--303, 2000.

\bibitem[Dulac-Arnold et~al.(2015)Dulac-Arnold, Evans, van Hasselt, Sunehag,
  Lillicrap, Hunt, Mann, Weber, Degris, and
  Coppin]{single-action-rep-discrete-1}
Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt,
  J., Mann, T., Weber, T., Degris, T., and Coppin, B.
\newblock Deep reinforcement learning in large discrete action spaces.
\newblock \emph{arXiv preprint arXiv:1512.07679}, 2015.

\bibitem[Dulac-Arnold et~al.(2019)Dulac-Arnold, Mankowitz, and
  Hester]{challenges-rl}
Dulac-Arnold, G., Mankowitz, D., and Hester, T.
\newblock Challenges of real-world reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1904.12901}, 2019.

\bibitem[Florensa et~al.(2017)Florensa, Duan, and
  Abbeel]{hierarchical-subgoals-small-prior}
Florensa, C., Duan, Y., and Abbeel, P.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1704.03012}, 2017.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Van~Hoof, and Meger]{td3}
Fujimoto, S., Van~Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{arXiv preprint arXiv:1802.09477}, 2018.

\bibitem[Gregor et~al.(2016)Gregor, Rezende, and
  Wierstra]{hierarchical-subgoals-intrinsic-2}
Gregor, K., Rezende, D.~J., and Wierstra, D.
\newblock Variational intrinsic control.
\newblock \emph{arXiv preprint arXiv:1611.07507}, 2016.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{sac-alg}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Hausman et~al.(2018)Hausman, Springenberg, Wang, Heess, and
  Riedmiller]{hierarchical-subgoals-robot}
Hausman, K., Springenberg, J.~T., Wang, Z., Heess, N., and Riedmiller, M.
\newblock Learning an embedding space for transferable robot skills.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{hierarchical-subgoals-intrinsic-1}
Kulkarni, T.~D., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 3675--3683, 2016.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Sharma, and
  Ravindran]{action-reps-discrete}
Lakshminarayanan, A., Sharma, S., and Ravindran, B.
\newblock Dynamic action repetition for deep reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun-deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2020)Lee, Lee, and Kim]{actpers1}
Lee, J., Lee, B.-J., and Kim, K.-E.
\newblock Reinforcement learning for control with multiple frequencies.
\newblock In \emph{Thirty-fourth Conference on Neural Information Processing
  Systems (NeurIPS 2020)}. Neural information processing systems foundation,
  2020.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Mahmood et~al.(2018)Mahmood, Korenkevych, Vasan, Ma, and
  Bergstra]{rl-real-world-design}
Mahmood, A.~R., Korenkevych, D., Vasan, G., Ma, W., and Bergstra, J.
\newblock Benchmarking reinforcement learning algorithms on real-world robots.
\newblock \emph{arXiv preprint arXiv:1809.07731}, 2018.

\bibitem[Metelli et~al.(2020)Metelli, Mazzolini, Bisi, Sabbioni, and
  Restelli]{actpers0}
Metelli, A.~M., Mazzolini, F., Bisi, L., Sabbioni, L., and Restelli, M.
\newblock Control frequency adaptation via action persistence in batch
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6862--6873. PMLR, 2020.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{A3c}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937, 2016.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{HIRO}
Nachum, O., Gu, S.~S., Lee, H., and Levine, S.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3303--3313, 2018.

\bibitem[Neunert et~al.(2020)Neunert, Abdolmaleki, Wulfmeier, Lampe,
  Springenberg, Hafner, Romano, Buchli, Heess, and Riedmiller]{actreps_mot2}
Neunert, M., Abdolmaleki, A., Wulfmeier, M., Lampe, T., Springenberg, T.,
  Hafner, R., Romano, F., Buchli, J., Heess, N., and Riedmiller, M.
\newblock Continuous-discrete reinforcement learning for hybrid control in
  robotics.
\newblock In \emph{Conference on Robot Learning}, pp.\  735--751. PMLR, 2020.

\bibitem[Precup(2001)]{opt-intro2}
Precup, D.
\newblock Temporal abstraction in reinforcement learning.
\newblock 2001.

\bibitem[Precup et~al.(1997)Precup, Sutton, and Singh]{macro-intro}
Precup, D., Sutton, R.~S., and Singh, S.~P.
\newblock Planning with closed-loop macro actions.
\newblock In \emph{Working notes of the 1997 AAAI Fall Symposium on
  Model-directed Autonomous Systems}, pp.\  70--76, 1997.

\bibitem[Schoknecht \& Riedmiller(2002)Schoknecht and Riedmiller]{actreps_mot0}
Schoknecht, R. and Riedmiller, M.
\newblock Speeding-up reinforcement learning with multi-step actions.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  813--818. Springer, 2002.

\bibitem[Schoknecht \& Riedmiller(2003)Schoknecht and Riedmiller]{actreps_mot1}
Schoknecht, R. and Riedmiller, M.
\newblock Reinforcement learning on explicitly specified time scales.
\newblock \emph{Neural Computing \& Applications}, 12\penalty0 (2):\penalty0
  61--80, 2003.

\bibitem[Sharma et~al.(2017)Sharma, Lakshminarayanan, and
  Ravindran]{action-reps-policy-factor}
Sharma, S., Lakshminarayanan, A.~S., and Ravindran, B.
\newblock Learning to repeat: Fine grained action repetition for deep
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1702.06054}, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{dpg}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock 2014.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{alphazero}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton-reinforcement-n-step}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{opt-intro1}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and Mansour]{pg-thm}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Sweatt(2009)]{human-motor-learning}
Sweatt, J.~D.
\newblock \emph{Mechanisms of memory}.
\newblock Academic Press, 2009.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, Casas, Budden,
  Abdolmaleki, Merel, Lefrancq, et~al.]{dmc}
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., Casas, D. d.~L., Budden,
  D., Abdolmaleki, A., Merel, J., Lefrancq, A., et~al.
\newblock Deepmind control suite.
\newblock \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Tennenholtz \& Mannor(2019)Tennenholtz and Mannor]{demos-act-rep}
Tennenholtz, G. and Mannor, S.
\newblock The natural language of actions.
\newblock \emph{arXiv preprint arXiv:1902.01119}, 2019.

\bibitem[Tessler et~al.(2017)Tessler, Givony, Zahavy, Mankowitz, and
  Mannor]{hierarchical-subgoals-mine}
Tessler, C., Givony, S., Zahavy, T., Mankowitz, D., and Mannor, S.
\newblock A deep hierarchical approach to lifelong learning in minecraft.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~31, 2017.

\bibitem[Vezhnevets et~al.(2016)Vezhnevets, Mnih, Osindero, Graves, Vinyals,
  Agapiou, et~al.]{macro-STRAWS}
Vezhnevets, A., Mnih, V., Osindero, S., Graves, A., Vinyals, O., Agapiou, J.,
  et~al.
\newblock Strategic attentive writer for learning macro-actions.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 3486--3494, 2016.

\bibitem[Vezhnevets et~al.(2017)Vezhnevets, Osindero, Schaul, Heess, Jaderberg,
  Silver, and Kavukcuoglu]{FuN}
Vezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver,
  D., and Kavukcuoglu, K.
\newblock Feudal networks for hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.01161}, 2017.

\bibitem[Whitney et~al.(2019)Whitney, Agarwal, Cho, and
  Gupta]{actseq-rep-dynamics-aware-emb}
Whitney, W., Agarwal, R., Cho, K., and Gupta, A.
\newblock Dynamics-aware embeddings.
\newblock \emph{arXiv preprint arXiv:1908.09357}, 2019.

\bibitem[Ziebart(2010)]{maxentobj}
Ziebart, B.~D.
\newblock Modeling purposeful adaptive behavior with the principle of maximum
  causal entropy.
\newblock 2010.

\end{thebibliography}
