\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abeyruwan(2013)]{RLLibCapital}
Abeyruwan, S.
\newblock {RLLib}: Lightweight standard and on/off policy reinforcement
  learning library ({C}++).
\newblock \url{http://web.cs.miami.edu/home/saminda/rilib.html}, 2013.

\bibitem[Bagnell \& Schneider(2003)Bagnell and Schneider]{Bagnell03Cov}
Bagnell, J.~A. and Schneider, J.
\newblock Covariant policy search.
\newblock pp.\  1019--1024. IJCAI, 2003.

\bibitem[Bakker(2001)]{bakker2001reinforcement}
Bakker, B.
\newblock Reinforcement learning with long short-term memory.
\newblock In \emph{NIPS}, pp.\  1475--1482, 2001.

\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and
  {Bowling}]{Bellemare13ALE}
{Bellemare}, M.~G., {Naddaf}, Y., {Veness}, J., and {Bowling}, M.
\newblock The {Arcade Learning Environment}: An evaluation platform for general
  agents.
\newblock \emph{J. Artif. Intell. Res.}, 47:\penalty0 253--279, 2013.

\bibitem[Bellman(1957)]{Bellman57}
Bellman, R.
\newblock \emph{Dynamic Programming}.
\newblock Princeton University Press, 1957.

\bibitem[Bertsekas \& Tsitsiklis(1995)Bertsekas and
  Tsitsiklis]{Bertsekas95Neuro}
Bertsekas, Dimitri~P and Tsitsiklis, John~N.
\newblock Neuro-dynamic programming: an overview.
\newblock In \emph{CDC}, pp.\  560--564, 1995.

\bibitem[Busoniu(2010)]{ApproxRL}
Busoniu, L.
\newblock {ApproxRL}: A {Matlab} toolbox for approximate {RL} and {DP}.
\newblock \url{http://busoniu.net/files/repository/readme-approxrl. html},
  2010.

\bibitem[Catto(2011)]{Box2D}
Catto, E.
\newblock {Box2D}: A {2D} physics engine for games, 2011.

\bibitem[Coulom(2002)]{coulom2002reinforcement}
Coulom, R{\'e}mi.
\newblock \emph{Reinforcement learning using neural networks, with applications
  to motor control}.
\newblock PhD thesis, Institut National Polytechnique de Grenoble-INPG, 2002.

\bibitem[Dann et~al.(2014)Dann, Neumann, and Peters]{tdlearn}
Dann, C., Neumann, G., and Peters, J.
\newblock Policy evaluation with temporal differences: A survey and comparison.
\newblock \emph{J. Mach. Learn. Res.}, 15\penalty0 (1):\penalty0 809--883,
  2014.

\bibitem[Degris et~al.(2013)Degris, B\'{e}chu, White, Modayil, Pilarski, and
  Denk]{RLPark}
Degris, T., B\'{e}chu, J., White, A., Modayil, J., Pilarski, P.~M., and Denk,
  C.
\newblock {RLPark}.
\newblock \url{http://rlpark.github.io}, 2013.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, and
  Peters]{Deisenroth2013PSSurvey}
Deisenroth, M.~P., Neumann, G., and Peters, J.
\newblock A survey on policy search for robotics, foundations and trends in
  robotics.
\newblock \emph{Found. Trends Robotics}, 2\penalty0 (1-2):\penalty0 1--142,
  2013.

\bibitem[DeJong \& Spong(1994)DeJong and Spong]{dejong1994swinging}
DeJong, G. and Spong, M.~W.
\newblock Swinging up the {Acrobot}: An example of intelligent control.
\newblock In \emph{ACC}, pp.\  2158--2162, 1994.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{Deng09ImageNet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, pp.\  248--255, 2009.

\bibitem[Dietterich(2000)]{Dietterich:2000}
Dietterich, T.~G.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function
  decomposition.
\newblock \emph{J. Artif. Intell. Res}, 13:\penalty0 227--303, 2000.

\bibitem[Dimitrakakis et~al.(2007)Dimitrakakis, Tziortziotis, and
  Tossou]{BeliefBox}
Dimitrakakis, C., Tziortziotis, N., and Tossou, A.
\newblock Beliefbox: A framework for statistical methods in sequential decision
  making.
\newblock \url{http://code.google.com/p/beliefbox/}, 2007.

\bibitem[Dimitrakakis et~al.(2014)Dimitrakakis, Li, and
  Tziortziotis]{dimitrakakis2014reinforcement}
Dimitrakakis, Christos, Li, Guangliang, and Tziortziotis, Nikoalos.
\newblock The reinforcement learning competition 2014.
\newblock \emph{AI Magazine}, 35\penalty0 (3):\penalty0 61--65, 2014.

\bibitem[Donaldson(1960)]{donaldson1960error}
Donaldson, P. E.~K.
\newblock Error decorrelation: a technique for matching a class of functions.
\newblock In \emph{Proc. 3th Intl. Conf. Medical Electronics}, pp.\  173--178,
  1960.

\bibitem[Doya(2000)]{doya2000reinforcement}
Doya, K.
\newblock Reinforcement learning in continuous time and space.
\newblock \emph{Neural Comput.}, 12\penalty0 (1):\penalty0 219--245, 2000.

\bibitem[Dutech et~al.(2005)Dutech, Edmunds, Kok, Lagoudakis, Littman,
  Riedmiller, Russell, Scherrer, Sutton, Timmer,
  et~al.]{dutech2005reinforcement}
Dutech, Alain, Edmunds, Timothy, Kok, Jelle, Lagoudakis, Michail, Littman,
  Michael, Riedmiller, Martin, Russell, Bryan, Scherrer, Bruno, Sutton,
  Richard, Timmer, Stephan, et~al.
\newblock Reinforcement learning benchmarks and bake-offs ii.
\newblock \emph{Advances in Neural Information Processing Systems (NIPS)}, 17,
  2005.

\bibitem[Erez et~al.(2011)Erez, Tassa, and Todorov]{erez2011infinite}
Erez, Tom, Tassa, Yuval, and Todorov, Emanuel.
\newblock Infinite horizon model predictive control for nonlinear periodic
  tasks.
\newblock \emph{Manuscript under review}, 4, 2011.

\bibitem[Everingham et~al.(2010)Everingham, Van~Gool, Williams, Winn, and
  Zisserman]{everingham2010pascal}
Everingham, M., Van~Gool, L., Williams, C. K.~I., Winn, J., and Zisserman, A.
\newblock The pascal visual object classes ({VOC}) challenge.
\newblock \emph{Int. J. Comput. Vision}, 88\penalty0 (2):\penalty0 303--338,
  2010.

\bibitem[Fei-Fei et~al.(2006)Fei-Fei, Fergus, and Perona]{fei2006one}
Fei-Fei, L., Fergus, R., and Perona, P.
\newblock One-shot learning of object categories.
\newblock \emph{IEEE Trans. Pattern Anal. Mach. Intell.}, 28\penalty0
  (4):\penalty0 594--611, 2006.

\bibitem[Furuta et~al.(1978)Furuta, Okutani, and Sone]{Furuta78}
Furuta, K., Okutani, T., and Sone, H.
\newblock Computer control of a double inverted pendulum.
\newblock \emph{Comput. Electr. Eng.}, 5\penalty0 (1):\penalty0 67--84, 1978.

\bibitem[Garofolo et~al.(1993)Garofolo, Lamel, Fisher, Fiscus, and
  Pallett]{garofolo1993darpa}
Garofolo, J.~S., Lamel, L.~F., Fisher, W.~M., Fiscus, J.~G., and Pallett, D.~S.
\newblock {DARPA} {TIMIT} acoustic-phonetic continuous speech corpus {CD-ROM}.
  {NIST} speech disc 1-1.1.
\newblock \emph{NASA STI/Recon Technical Report N}, 93, 1993.

\bibitem[Godfrey et~al.(1992)Godfrey, Holliman, and McDaniel]{sb}
Godfrey, J.~J., Holliman, E.~C., and McDaniel, J.
\newblock {SWITCHBOARD}: Telephone speech corpus for research and development.
\newblock In \emph{ICASSP}, pp.\  517--520, 1992.

\bibitem[Gomez \& Miikkulainen(1998)Gomez and Miikkulainen]{gomez19982}
Gomez, F. and Miikkulainen, R.
\newblock 2-d pole balancing with recurrent evolutionary networks.
\newblock In \emph{ICANN}, pp.\  425--430. 1998.

\bibitem[Guo et~al.(2014)Guo, Singh, Lee, Lewis, and Wang]{NIPS2014_5421}
Guo, X., Singh, S., Lee, H., Lewis, R.~L., and Wang, X.
\newblock Deep learning for real-time {Atari} game play using offline
  monte-carlo tree search planning.
\newblock In \emph{NIPS}, pp.\  3338--3346. 2014.

\bibitem[Hansen \& Ostermeier(2001)Hansen and Ostermeier]{Hansen2001CMAES}
Hansen, N. and Ostermeier, A.
\newblock Completely derandomized self-adaptation in evolution strategies.
\newblock \emph{Evol. Comput.}, 9\penalty0 (2):\penalty0 159--195, 2001.

\bibitem[Heess et~al.(2015{\natexlab{a}})Heess, Hunt, Lillicrap, and
  Silver]{heess2015memory}
Heess, N., Hunt, J., Lillicrap, T., and Silver, D.
\newblock Memory-based control with recurrent neural networks.
\newblock \emph{arXiv:1512.04455}, 2015{\natexlab{a}}.

\bibitem[Heess et~al.(2015{\natexlab{b}})Heess, Wayne, Silver, Lillicrap, Erez,
  and Tassa]{Heess15}
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and Tassa, T.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In \emph{NIPS}, pp.\  2926--2934. 2015{\natexlab{b}}.

\bibitem[Hester \& Stone(2013)Hester and Stone]{hester2013open}
Hester, T. and Stone, P.
\newblock The open-source {TEXPLORE} code release for reinforcement learning on
  robots.
\newblock In \emph{RoboCup 2013: Robot World Cup XVII}, pp.\  536--543. 2013.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Dahl, and Kingsbury]{Hinton12}
Hinton, G., Deng, L., Yu, D., Mohamed, A.-R., Jaitly, N., Senior, A.,
  Vanhoucke, V., Nguyen, P., Dahl, T. S.~G., and Kingsbury, B.
\newblock Deep neural networks for acoustic modeling in speech recognition.
\newblock \emph{IEEE Signal Process. Mag}, 29\penalty0 (6):\penalty0 82--97,
  2012.

\bibitem[Hirsch \& Pearce(2000)Hirsch and Pearce]{hirsch2000aurora}
Hirsch, H.-G. and Pearce, D.
\newblock The {Aurora} experimental framework for the performance evaluation of
  speech recognition systems under noisy conditions.
\newblock In \emph{ASR2000-Automatic Speech Recognition: Challenges for the new
  Millenium ISCA Tutorial and Research Workshop (ITRW)}, 2000.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Comput.}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Kakade(2002)]{Kakade02NPG}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock In \emph{NIPS}, pp.\  1531--1538. 2002.

\bibitem[Kimura \& Kobayashi(1999)Kimura and Kobayashi]{815604}
Kimura, H. and Kobayashi, S.
\newblock Stochastic real-valued reinforcement learning to solve a nonlinear
  control problem.
\newblock In \emph{IEEE SMC}, pp.\  510--515, 1999.

\bibitem[Kober \& Peters(2009)Kober and Peters]{Kober09POWER}
Kober, J. and Peters, J.
\newblock Policy search for motor primitives in robotics.
\newblock In \emph{NIPS}, pp.\  849--856, 2009.

\bibitem[Kochenderfer(2006)]{JRLF}
Kochenderfer, M.
\newblock {JRLF}: {Java} reinforcement learning framework.
\newblock \url{http://mykel.kochenderfer.com/jrlf}, 2006.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Krizhevsky12}
Krizhevsky, A., Sutskever, I., and Hinton, G.
\newblock {ImageNet} classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, pp.\  1097--1105. 2012.

\bibitem[LeCun et~al.(1998)LeCun, Cortes, and Burges]{lecun1998mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock The {MNIST} database of handwritten digits, 1998.

\bibitem[Levine \& Koltun(2013)Levine and Koltun]{levine2013guided}
Levine, S. and Koltun, V.
\newblock Guided policy search.
\newblock In \emph{ICML}, pp.\  1--9, 2013.

\bibitem[Levine et~al.(2015)Levine, Finn, Darrell, and Abbeel]{Levine15}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{arXiv:1504.00702}, 2015.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{Lillicrap15}
Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver,
  D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock arXiv:1509.02971, 2015.

\bibitem[Martin et~al.(2001)Martin, C.~Fowlkes, and Malik]{MartinFTM01}
Martin, D., C.~Fowlkes, D.~Tal, and Malik, J.
\newblock A database of human segmented natural images and its application to
  evaluating segmentation algorithms and measuring ecological statistics.
\newblock In \emph{ICCV}, pp.\  416--423, 2001.

\bibitem[Metzen \& Edgington(2011)Metzen and Edgington]{MMLF}
Metzen, J.~M. and Edgington, M.
\newblock Maja machine learning framework.
\newblock \url{http://mloss.org/software/view/220/}, 2011.

\bibitem[Michie \& Chambers(1968)Michie and Chambers]{boxes}
Michie, D. and Chambers, R.~A.
\newblock {BOXES}: An experiment in adaptive control.
\newblock \emph{Machine Intelligence}, 2:\penalty0 137--152, 1968.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, A., I., Kumaran,
  Wierstra, Legg, and Hassabis]{Mnih15}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Moore(1990)]{Moore90MountainCar}
Moore, A.
\newblock Efficient memory-based learning for robot control.
\newblock Technical report, University of Cambridge, Computer Laboratory, 1990.

\bibitem[Murray \& Hauser(1991)Murray and Hauser]{Murray:M91/46}
Murray, R.~M. and Hauser, J.
\newblock A case study in approximate linearization: The {Acrobot} example.
\newblock Technical report, UC Berkeley, EECS Department, 1991.

\bibitem[Murthy \& Raibert(1984)Murthy and Raibert]{murthy19843d}
Murthy, S.~S. and Raibert, M.~H.
\newblock {3D} balance in legged locomotion: modeling and simulation for the
  one-legged case.
\newblock \emph{ACM SIGGRAPH Computer Graphics}, 18\penalty0 (1):\penalty0
  27--27, 1984.

\bibitem[Neumann(2006)]{RLToolbox}
Neumann, G.
\newblock A reinforcement learning toolbox and {RL} benchmarks for the control
  of dynamical systems.
\newblock \emph{Dynamical principles for neuroscience and intelligent
  biomimetic devices}, pp.\  113, 2006.

\bibitem[Papis \& Wawrzy\'{n}ski(2013)Papis and Wawrzy\'{n}ski]{dotRL}
Papis, B. and Wawrzy\'{n}ski, P.
\newblock dotrl: A platform for rapid reinforcement learning methods
  development and validation.
\newblock In \emph{FedCSIS}, pp.\  pages 129--136., 2013.

\bibitem[Parr \& Russell(1998)Parr and Russell]{parr1998reinforcement}
Parr, Ronald and Russell, Stuart.
\newblock Reinforcement learning with hierarchies of machines.
\newblock \emph{Advances in neural information processing systems}, pp.\
  1043--1049, 1998.

\bibitem[Peters(2002)]{PolicyGradientToolbox}
Peters, J.
\newblock {Policy Gradient Toolbox}.
\newblock \url{http://www.ausy.tu-darmstadt.de/Research/PolicyGradientToolbox},
  2002.

\bibitem[Peters \& Schaal(2007)Peters and Schaal]{Peters07RWR}
Peters, J. and Schaal, S.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In \emph{ICML}, pp.\  745--750, 2007.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Peters et~al.(2003)Peters, Vijaykumar, and Schaal]{Peters03PG}
Peters, J., Vijaykumar, S., and Schaal, S.
\newblock Policy gradient methods for robot control.
\newblock Technical report, 2003.

\bibitem[Peters et~al.(2010)Peters, M{\"u}lling, and Alt{\"u}n]{Peters10REPS}
Peters, J., M{\"u}lling, K., and Alt{\"u}n, Y.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI}, pp.\  1607--1612, 2010.

\bibitem[Purcell(1977)]{PurcellSwimmer}
Purcell, E.~M.
\newblock Life at low {Reynolds} number.
\newblock \emph{Am. J. Phys}, 45\penalty0 (1):\penalty0 3--11, 1977.

\bibitem[Raibert \& Hodgins(1991)Raibert and Hodgins]{raibert1991animation}
Raibert, M.~H. and Hodgins, J.~K.
\newblock Animation of dynamic legged locomotion.
\newblock In \emph{ACM SIGGRAPH Computer Graphics}, volume~25, pp.\  349--358,
  1991.

\bibitem[Riedmiller et~al.(2012)Riedmiller, Blum, and Lampe]{CLSquare}
Riedmiller, M., Blum, M., and Lampe, T.
\newblock {CLS2}: Closed loop simulation system.
\newblock \url{http://ml.informatik.uni-freiburg.de/research/clsquare}, 2012.

\bibitem[Rubinstein(1999)]{Rubinstein99CEM}
Rubinstein, R.
\newblock The cross-entropy method for combinatorial and continuous
  optimization.
\newblock \emph{Methodol. Comput. Appl. Probab.}, 1\penalty0 (2):\penalty0
  127--190, 1999.

\bibitem[Sch{\"a}fer \& Udluft(2005)Sch{\"a}fer and Udluft]{schafer2005solving}
Sch{\"a}fer, A.~M. and Udluft, S.
\newblock Solving partially observable reinforcement learning problems with
  recurrent neural networks.
\newblock In \emph{ECML Workshops}, pp.\  71--81, 2005.

\bibitem[Schaul et~al.(2010)Schaul, Bayer, Wierstra, Sun, Felder, Sehnke,
  R\"{u}ckstie\ss, and Schmidhuber]{PyBrain}
Schaul, T., Bayer, J., Wierstra, D., Sun, Y., Felder, M., Sehnke, F.,
  R\"{u}ckstie\ss, T., and Schmidhuber, J.
\newblock {PyBrain}.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 743--746, 2010.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{Schulman15TRPO}
Schulman, J., Levine, S., Abbeel, P., Jordan, M.~I., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{ICML}, pp.\  1889--1897, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{Schulman15GAE}
Schulman, J., Moritz, P., Levine, S., Jordan, M.~I., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock arXiv:1506.02438, 2015{\natexlab{b}}.

\bibitem[Stephenson(1908)]{stephenson1908xx}
Stephenson, A.
\newblock On induced stability.
\newblock \emph{Philos. Mag.}, 15\penalty0 (86):\penalty0 233--236, 1908.

\bibitem[Stone et~al.(2005)Stone, Kuhlmann, Taylor, and Liu]{stone2005keepaway}
Stone, Peter, Kuhlmann, Gregory, Taylor, Matthew~E, and Liu, Yaxin.
\newblock Keepaway soccer: From machine learning testbed to benchmark.
\newblock In \emph{RoboCup 2005: Robot Soccer World Cup IX}, pp.\  93--105.
  Springer, 2005.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, Richard~S, Precup, Doina, and Singh, Satinder.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1):\penalty0 181--211,
  1999.

\bibitem[Szita \& L\H{o}rincz(2006)Szita and L\H{o}rincz]{Szita06CEM}
Szita, I. and L\H{o}rincz, A.
\newblock Learning {Tetris} using the noisy cross-entropy method.
\newblock \emph{Neural Comput.}, 18\penalty0 (12):\penalty0 2936--2941, 2006.

\bibitem[Szita et~al.(2003)Szita, Tak{\'a}cs, and
  L{\"o}rincz]{szita2003varepsilon}
Szita, I., Tak{\'a}cs, B., and L{\"o}rincz, A.
\newblock $\varepsilon$-{MDPs}: Learning in varying environments.
\newblock \emph{J. Mach. Learn. Res.}, 3:\penalty0 145--174, 2003.

\bibitem[Tassa et~al.(2012)Tassa, Erez, and Todorov]{tassa2012synthesis}
Tassa, Yuval, Erez, Tom, and Todorov, Emanuel.
\newblock Synthesis and stabilization of complex behaviors through online
  trajectory optimization.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pp.\  4906--4913. IEEE, 2012.

\bibitem[Tesauro(1995)]{Tesauro95TDGammon}
Tesauro, G.
\newblock Temporal difference learning and {TD-Gammon}.
\newblock \emph{Commun. ACM}, 38\penalty0 (3):\penalty0 58--68, 1995.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{MuJoCo}
Todorov, E., Erez, T., and Tassa, Y.
\newblock {MuJoCo}: A physics engine for model-based control.
\newblock In \emph{IROS}, pp.\  5026--5033, 2012.

\bibitem[van Hoof et~al.(2015)van Hoof, Peters, and Neumann]{HofNeuPet15}
van Hoof, H., Peters, J., and Neumann, G.
\newblock Learning of non-parametric control policies with high-dimensional
  state features.
\newblock In \emph{AISTATS}, pp.\  995--1003, 2015.

\bibitem[Watter et~al.(2015)Watter, Springenberg, Boedecker, and
  Riedmiller]{Watter15E2C}
Watter, M., Springenberg, J., Boedecker, J., and Riedmiller, M.
\newblock Embed to control: A locally linear latent dynamics model for control
  from raw images.
\newblock In \emph{NIPS}, pp.\  2728--2736, 2015.

\bibitem[Wawrzy{\'n}ski(2007)]{wawrzynski2007learning}
Wawrzy{\'n}ski, P.
\newblock Learning to control a 6-degree-of-freedom walking robot.
\newblock In \emph{IEEE EUROCON}, pp.\  698--705, 2007.

\bibitem[Widrow(1964)]{widrow1964pattern}
Widrow, B.
\newblock Pattern recognition and adaptive control.
\newblock \emph{IEEE Trans. Ind. Appl.}, 83\penalty0 (74):\penalty0 269--277,
  1964.

\bibitem[Wierstra et~al.(2007)Wierstra, Foerster, Peters, and
  Schmidhuber]{wierstra2007solving}
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J.
\newblock Solving deep memory {POMDPs} with recurrent policy gradients.
\newblock In \emph{ICANN}, pp.\  697--706. 2007.

\bibitem[Williams(1992)]{Williams92VPG}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Mach. Learn.}, 8:\penalty0 229--256, 1992.

\bibitem[Yamaguchi \& Ogasawara(2010)Yamaguchi and Ogasawara]{SkyAI}
Yamaguchi, A. and Ogasawara, T.
\newblock {SkyAI}: Highly modularized reinforcement learning library.
\newblock In \emph{IEEE-RAS Humanoids}, pp.\  118--123, 2010.

\bibitem[Yu et~al.(2007)Yu, Ju, Wang, Zweig, and Acero]{Dong07VoiceSearch}
Yu, D., Ju, Y.-C., Wang, Y.-Y., Zweig, G., and Acero, A.
\newblock Automated directory assistance system - from theory to practice.
\newblock In \emph{Interspeech}, pp.\  2709--2712, 2007.

\end{thebibliography}
