\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Vapnik(1998)]{Vap98}
V.~N. Vapnik.
\newblock \emph{Statistical Learning Theory}.
\newblock Wiley, New York, 1998.

\bibitem[Bartlett et~al.(1998)Bartlett, Freund, Lee, and
  Schapire]{Bartlett1998}
Peter Bartlett, Yoav Freund, Wee~Sun Lee, and Robert~E. Schapire.
\newblock {Boosting the margin: a new explanation for the effectiveness of
  voting methods}.
\newblock \emph{The Annals of Statistics}, 26\penalty0 (5):\penalty0 1651 --
  1686, 1998.

\bibitem[Bartlett and Shawe-Taylor(1999)]{Bartlett1999}
Peter Bartlett and John Shawe-Taylor.
\newblock \emph{Generalization Performance of Support Vector Machines and Other
  Pattern Classifiers}, page 43–54.
\newblock MIT Press, 1999.

\bibitem[Koltchinskii and Panchenko(2002)]{Koltchinskii2002}
V.~Koltchinskii and D.~Panchenko.
\newblock {Empirical Margin Distributions and Bounding the Generalization Error
  of Combined Classifiers}.
\newblock \emph{The Annals of Statistics}, 30\penalty0 (1):\penalty0 1 -- 50,
  2002.

\bibitem[Langford and Shawe-Taylor(2003)]{Langford2003}
J.~Langford and J.~Shawe-Taylor.
\newblock {PAC-Bayes and Margins}.
\newblock In \emph{Advances of Neural Information Processing Systems (NIPS)},
  2003.

\bibitem[McAllester(2004)]{McAllester2004}
D.~A. McAllester.
\newblock Pac-bayesian stochastic model selection.
\newblock \emph{Machine Learning}, 51, 2004.

\bibitem[A.~Ambroladze and ShaweTaylor(2007)]{Ambroladze2007}
E.~Parrado-Hern'{'}andez A.~Ambroladze and J.~ShaweTaylor.
\newblock Tighter {PAC-Bayes} bounds.
\newblock In \emph{NIPS}, 2007.

\bibitem[Dziugaite and Roy.(2017)]{Dziugaite2017}
G.~K. Dziugaite and D.~M. Roy.
\newblock { Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.}
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, 2017.

\bibitem[W.~Zhou and Orbanz.(2019)]{Zhou2019}
M.~Austern R. P.~Adams W.~Zhou, V.~Veitch and P.~Orbanz.
\newblock {Non-vacuous generalization bounds at the imagenet scale: a
  PAC-Bayesian compression approach.}
\newblock In \emph{The International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Biggs and Guedj(2021)]{Biggs2021}
F.~Biggs and B.~Guedj.
\newblock Differentiable {PAC-Bayes} objectives with partially aggregated
  neural networks.
\newblock \emph{Entropy}, 23, 2021.

\bibitem[McAllester(1998)]{McAllester1998}
A.~McAllester.
\newblock Some {PAC-Bayesian} theorems.
\newblock In \emph{{Conference on Learning Theory (COLT)}}, 1998.

\bibitem[Clerico et~al.(2021{\natexlab{a}})Clerico, Deligiannidis, and
  Doucet]{Eugenio2021a}
Eugenio Clerico, George Deligiannidis, and Arnaud Doucet.
\newblock {Wide stochastic networks: Gaussian limit and PACBayesian training}.
\newblock \emph{Arxiv: 2106.09798}, 2021{\natexlab{a}}.

\bibitem[Clerico et~al.(2021{\natexlab{b}})Clerico, Deligiannidis, and
  Doucet]{Eugenio2021}
Eugenio Clerico, George Deligiannidis, and Arnaud Doucet.
\newblock {Conditional Gaussian PAC-Bayes}.
\newblock \emph{Arxiv: 2110.1188}, 2021{\natexlab{b}}.

\bibitem[Xu and Raginsky(2017)]{XuRaginskyNIPS17}
A.~Xu and M.~Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock In \emph{Advances of Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Esposito et~al.(2021)Esposito, Gastpar, and Issa]{Esposito2021}
Amedeo~Roberto Esposito, Michael Gastpar, and Ibrahim Issa.
\newblock Generalization error bounds via {Rényi}-f-divergences and maximal
  leakage.
\newblock \emph{IEEE Transactions on Information Theory}, 67\penalty0
  (8):\penalty0 4986--5004, 2021.

\bibitem[Jakubovitz et~al.(2018)Jakubovitz, Giryes, and
  Rodrigues]{Jakubovitz2108}
D.~Jakubovitz, R.~Giryes, and M.~R.~D. Rodrigues.
\newblock {Generalization Error in Deep Learning}.
\newblock \emph{Arxiv: 1808.01174}, 30, 2018.

\bibitem[MacKay(1992)]{MacKay1992}
David~J.C. MacKay.
\newblock \emph{Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology, United States, 1992.

\bibitem[Mackay(1995)]{Mackay1995ProbableNA}
David J.~C. Mackay.
\newblock Probable networks and plausible predictions - a review of practical
  bayesian methods for supervised neural networks.
\newblock \emph{Network: Computation In Neural Systems}, 6:\penalty0 469--505,
  1995.

\bibitem[Wilson and Izmailov(2020)]{Wilson20}
A.~G. Wilson and P.~Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of model
  construction.
\newblock In \emph{Proc. 37th ICML}. Morgan Kaufmann, 2020.

\bibitem[Duchi et~al.(2011)Duchi, Agarwal, Johansson, and
  Jordan]{Duchi2011ErgodicMD}
John~C. Duchi, Alekh Agarwal, Mikael Johansson, and Michael~I. Jordan.
\newblock Ergodic mirror descent.
\newblock \emph{2011 49th Annual Allerton Conference on Communication, Control,
  and Computing (Allerton)}, pages 701--706, 2011.

\bibitem[Wang et~al.(2019)Wang, Li, and Giannakis]{Wang2019AML}
Gang Wang, Bingcong Li, and Georgios~B. Giannakis.
\newblock A multistep lyapunov approach for finite-time analysis of biased
  stochastic approximation.
\newblock \emph{ArXiv}, abs/1909.04299, 2019.

\bibitem[Truong(2022{\natexlab{a}})]{Truong2022OnLM}
Lan~V. Truong.
\newblock On linear model with markov signal priors.
\newblock In \emph{AISTATS}, 2022{\natexlab{a}}.

\bibitem[Tuominen and Tweedie(1979)]{TR1979}
Pekka Tuominen and Richard~L. Tweedie.
\newblock {Markov Chains with Continuous Components}.
\newblock \emph{Proceedings of the London Mathematical Society}, s3-38\penalty0
  (1):\penalty0 89--114, 01 1979.

\bibitem[Paulin(2015)]{Daniel2015}
Daniel Paulin.
\newblock {Concentration inequalities for Markov chains by Marton couplings and
  spectral methods}.
\newblock \emph{Electronic Journal of Probability}, 20\penalty0 (79):\penalty0
  1 -- 32, 2015.

\bibitem[Wolfer and Kontorovich(2019)]{WK19ALT}
G.~Wolfer and A.~Kontorovich.
\newblock Estimating the mixing time of ergodic markov chains.
\newblock In \emph{32nd Annual Conference on Learning Theory}, 2019.

\bibitem[Combes and Touati(2019)]{Combes2019EE}
R.~Combes and M.~Touati.
\newblock Computationally efficient estimation of the spectral gap of a markov
  chain.
\newblock \emph{Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 3:\penalty0 1 -- 21, 2019.

\bibitem[Van Der~Vaart and Wellner(1996{\natexlab{a}})]{Vaartbook1996}
A.~W. Van Der~Vaart and Wellner.
\newblock \emph{Weak convergence and Empirical Processes.}
\newblock Springer, New York, 1996{\natexlab{a}}.

\bibitem[Rudolf(2011)]{Rudolf2011}
D.~Rudolf.
\newblock {Explicit error bounds for Markov chain Monte Carlo}.
\newblock \emph{Arxiv: 1108.3201}, 2011.

\bibitem[Lezaud(2001)]{Pascal2001}
Pascal Lezaud.
\newblock Chernoff and {B}erry-{E}ss\'{e}en inequalities for {M}arkov
  processes.
\newblock \emph{ESAIM: Probability and Statistics, EDP Sciences}, 5:\penalty0
  183--201, 2001.

\bibitem[Ledoux and Talagrand(1991)]{LedouxT1991book}
M.~Ledoux and M.~Talagrand.
\newblock \emph{Probability in Banach Spaces}.
\newblock Springer, New York., 1991.

\bibitem[Truong(2022{\natexlab{b}})]{Truong2022OnRC}
Lan~V. Truong.
\newblock On rademacher complexity-based generalization bounds for deep
  learning.
\newblock \emph{ArXiv}, abs/2208.04284, 2022{\natexlab{b}}.

\bibitem[Van Der~Vaart and Wellner(1996{\natexlab{b}})]{VaartWellnerbook}
A.~W. Van Der~Vaart and J.~A. Wellner.
\newblock \emph{Weak convergence and Empirical Processes.}
\newblock Springer, New York., 1996{\natexlab{b}}.

\bibitem[Billingsley(1995)]{Billingsley}
P.~Billingsley.
\newblock \emph{Probability and Measure}.
\newblock Wiley-Interscience, 3rd edition, 1995.

\bibitem[Kontoyiannis et~al.(2005)Kontoyiannis, Lastras-Monta{\~{n}}o, and
  Meyn]{Kont05}
I.~Kontoyiannis, L.~A. Lastras-Monta{\~{n}}o, and S.~P. Meyn.
\newblock Relative entropy and exponential deviation bounds for general
  {M}arkov chains.
\newblock In \emph{Proc. of Intl. Symp. on Inform. Th.}, 2005.

\end{thebibliography}
