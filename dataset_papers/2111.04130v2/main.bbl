\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beltagy et~al.(2019)Beltagy, Lo, and Cohan]{scibert}
Beltagy, I., Lo, K., and Cohan, A.
\newblock Sci{BERT}: {A} pretrained language model for scientific text.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  3613--3618, Hongkong,
  China, 2019. Association for Computational Linguistics.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS 2020)}, Vancouver, Canada, 2020.

\bibitem[Chen et~al.(2021)Chen, Cheng, Wang, Gan, Wang, and Liu]{Chen_2021}
Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., and Liu, J.
\newblock Early{BERT}: Efficient {BERT} training via early-bird lottery
  tickets.
\newblock In \emph{Proceedings of the Joint Conference of the 59th Annual
  Meeting of the Association for Computational Linguistics and the 11th
  International Joint Conference on Natural Language Processing}, 2021.

\bibitem[Clark et~al.(2020)Clark, Luong, Le, and Manning]{clark2020electra}
Clark, K., Luong, M.-T., Le, Q.~V., and Manning, C.~D.
\newblock {ELECTRA}: Pre-training text encoders as discriminators rather than
  generators.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[de~Wynter \& Perry(2020)de~Wynter and Perry]{optimalsubarchitecture}
de~Wynter, A. and Perry, D.~J.
\newblock Optimal subarchitecture extraction for {BERT}.
\newblock \emph{CoRR}, abs/2010.10499, 2020.

\bibitem[Dernoncourt \& Lee(2017)Dernoncourt and Lee]{RCT}
Dernoncourt, F. and Lee, J.~Y.
\newblock Pubmed 200k {RCT:} a dataset for sequential sentence classification
  in medical abstracts.
\newblock In \emph{{IJCNLP(2)}}, pp.\  308--313. Asian Federation of Natural
  Language Processing, 2017.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, 2019. Association for Computational Linguistics.

\bibitem[Gordon et~al.(2020)Gordon, Duh, and Andrews]{weightpruning}
Gordon, M., Duh, K., and Andrews, N.
\newblock Compressing bert: Studying the effects of weight pruning on transfer
  learning.
\newblock In \emph{Proceedings of the 5th Workshop on Representation Learning
  for NLP}, pp.\  143--155. Association for Computational Linguistics, 2020.

\bibitem[Gu et~al.(2020)Gu, Zhang, Wang, Liu, and Sun]{gu2020train}
Gu, Y., Zhang, Z., Wang, X., Liu, Z., and Sun, M.
\newblock Train no evil: Selective masking for task-guided pre-training.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  6966--6974, Online, November
  2020. Association for Computational Linguistics.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovic, Swayamdipta, Lo,
  Beltagy, Downey, and Smith]{dontstoppretraining}
Gururangan, S., Marasovic, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey,
  D., and Smith, N.~A.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock In \emph{{ACL}}, pp.\  8342--8360. Association for Computational
  Linguistics, 2020.

\bibitem[He et~al.(2021)He, Liu, Gao, and Chen]{he2020deberta}
He, P., Liu, X., Gao, J., and Chen, W.
\newblock De{BERT}a: Decoding-enhanced bert with disentangled attention.
\newblock In \emph{2021 International Conference on Learning Representations},
  May 2021.

\bibitem[Jiao et~al.(2020)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{jiao2019tinybert}
Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., and Liu,
  Q.
\newblock {T}iny{BERT}: Distilling {BERT} for natural language understanding.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pp.\  4163--4174, Online, November 2020. Association for
  Computational Linguistics.

\bibitem[Jurgens et~al.(2018)Jurgens, Kumar, Hoover, McFarland, and
  Jurafsky]{ACLARC}
Jurgens, D., Kumar, S., Hoover, R., McFarland, D.~A., and Jurafsky, D.
\newblock Measuring the evolution of a scientific field through citation
  frames.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 6:\penalty0 391--406, 2018.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen,
  and Yih]{karpukhin2020dense}
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and
  Yih, W.-t.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  6769--6781, Online, November
  2020. Association for Computational Linguistics.

\bibitem[Kiesel et~al.(2019)Kiesel, Mestre, Shukla, Vincent, Adineh, Corney,
  Stein, and Potthast]{hyperpartisan}
Kiesel, J., Mestre, M., Shukla, R., Vincent, E., Adineh, P., Corney, D. P.~A.,
  Stein, B., and Potthast, M.
\newblock Semeval-2019 task 4: Hyperpartisan news detection.
\newblock In \emph{SemEval@NAACL-HLT}, pp.\  829--839. Association for
  Computational Linguistics, 2019.

\bibitem[Kim et~al.(2021)Kim, Gholami, Yao, Mahoney, and Keutzer]{IBERT}
Kim, S., Gholami, A., Yao, Z., Mahoney, M.~W., and Keutzer, K.
\newblock {I-BERT:} integer-only {BERT} quantization.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Kovaleva et~al.(2019)Kovaleva, Romanov, Rogers, and
  Rumshisky]{kovaleva-etal-2019-revealing}
Kovaleva, O., Romanov, A., Rogers, A., and Rumshisky, A.
\newblock Revealing the dark secrets of {BERT}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4365--4374, Hongkong,
  China, 2019. Association for Computational Linguistics.

\bibitem[Kringelum et~al.(2016)Kringelum, Kj{\ae}rulff, Brunak, Lund, Oprea,
  and Taboureau]{chemprot}
Kringelum, J., Kj{\ae}rulff, S.~K., Brunak, S., Lund, O., Oprea, T.~I., and
  Taboureau, O.
\newblock Chemprot-3.0: a global chemical biology diseases mapping.
\newblock \emph{Database J. Biol. Databases Curation}, 2016, 2016.

\bibitem[Lee et~al.(2020)Lee, Yoon, Kim, Kim, Kim, So, and
  Kang]{lee2020biobert}
Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.~H., and Kang, J.
\newblock Bio{BERT}: a pre-trained biomedical language representation model for
  biomedical text mining.
\newblock \emph{Bioinformatics}, 36\penalty0 (4):\penalty0 1234--1240, 2020.

\bibitem[Li et~al.(2020)Li, Zhang, Zhao, Zhou, and Zhou]{li2020task}
Li, J., Zhang, Z., Zhao, H., Zhou, X., and Zhou, X.
\newblock Task-specific objectives of pre-trained language models for dialogue
  adaptation.
\newblock \emph{arXiv preprint arXiv:2009.04984}, 2020.

\bibitem[Liu et~al.(2020)Liu, Zhou, Wang, Zhao, Deng, and Ju]{FastBERT}
Liu, W., Zhou, P., Wang, Z., Zhao, Z., Deng, H., and Ju, Q.
\newblock {F}ast{BERT}: a self-distilling {BERT} with adaptive inference time.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  6035--6044, Online, July 2020.
  Association for Computational Linguistics.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Ro{BERT}a: A robustly optimized bert pretraining approach, 2019.

\bibitem[Luan et~al.(2018)Luan, He, Ostendorf, and Hajishirzi]{SCIERC}
Luan, Y., He, L., Ostendorf, M., and Hajishirzi, H.
\newblock Multi-task identification of entities, relations, and coreference for
  scientific knowledge graph construction.
\newblock In \emph{{EMNLP}}, pp.\  3219--3232. Association for Computational
  Linguistics, 2018.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{imdb}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{{ACL}}, pp.\  142--150. The Association for Computer
  Linguistics, 2011.

\bibitem[McAuley et~al.(2015)McAuley, Targett, Shi, and van~den
  Hengel]{helpfulness}
McAuley, J.~J., Targett, C., Shi, Q., and van~den Hengel, A.
\newblock Image-based recommendations on styles and substitutes.
\newblock In \emph{{SIGIR}}, pp.\  43--52. {ACM}, 2015.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{sixtneeneheads}
Michel, P., Levy, O., and Neubig, G.
\newblock Are sixteen heads really better than one?
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Qiao et~al.(2018)Qiao, Shen, Zhang, Wang, and Yuille]{cotraining1}
Qiao, S., Shen, W., Zhang, Z., Wang, B., and Yuille, A.
\newblock Deep co-training for semi-supervised image recognition.
\newblock \emph{Lecture Notes in Computer Science}, pp.\  142â€“159, 2018.
\newblock ISSN 1611-3349.
\newblock \doi{10.1007/978-3-030-01267-0_9}.
\newblock URL \url{http://dx.doi.org/10.1007/978-3-030-01267-0_9}.

\bibitem[Radford et~al.(2018)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2018gpt}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock 2018.
\newblock URL
  \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf}.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer, 2019.

\bibitem[Robertson \& Zaragoza(2009)Robertson and Zaragoza]{bm25paper}
Robertson, S.~E. and Zaragoza, H.
\newblock The probabilistic relevance framework: {BM25} and beyond.
\newblock \emph{Found. Trends Inf. Retr.}, 3\penalty0 (4):\penalty0 333--389,
  2009.

\bibitem[Rose et~al.(2010)Rose, Engel, Cramer, and Cowley]{rose2010automatic}
Rose, S., Engel, D., Cramer, N., and Cowley, W.
\newblock Automatic keyword extraction from individual documents.
\newblock \emph{Text mining: applications and theory}, 1:\penalty0 1--20, 2010.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distil{BERT}, a distilled version of {BERT}: smaller, faster, cheaper
  and lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Shen et~al.(2020)Shen, Dong, Ye, Ma, Yao, Gholami, Mahoney, and
  Keutzer]{QBERT}
Shen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A., Mahoney, M.~W., and
  Keutzer, K.
\newblock Q-{BERT}: Hessian based ultra low precision quantization of {BERT}.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  34\penalty0 (05):\penalty0 8815--8821, Apr. 2020.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatronlm}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-{LM:} training multi-billion parameter language models using
  model parallelism, 2019.

\bibitem[So et~al.(2021)So, MaÅ„ke, Liu, Dai, Shazeer, and Le]{so2021primer}
So, D.~R., MaÅ„ke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q.~V.
\newblock Primer: Searching for efficient transformers for language modeling,
  2021.

\bibitem[Sun et~al.(2020)Sun, Yu, Song, Liu, Yang, and Zhou]{MobileBERT}
Sun, Z., Yu, H., Song, X., Liu, R., Yang, Y., and Zhou, D.
\newblock {M}obile{BERT}: a compact task-agnostic {BERT} for resource-limited
  devices.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2158--2170, Online, July 2020.
  Association for Computational Linguistics.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Rao, Fedus, Abnar, Chung, Narang,
  Yogatama, Vaswani, and Metzler]{tay2021scale}
Tay, Y., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H.~W., Narang, S.,
  Yogatama, D., Vaswani, A., and Metzler, D.
\newblock Scale efficiently: Insights from pre-training and fine-tuning
  transformers, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30. Curran Associates, Inc., 2017.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{voita-etal-2019-analyzing}
Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  5797--5808, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
  Analyzing and Interpreting Neural Networks for {NLP}}, pp.\  353--355,
  Brussels, Belgium, November 2018. Association for Computational Linguistics.

\bibitem[Wang et~al.(2017)Wang, Min, Zhang, and Wu]{al2}
Wang, M., Min, F., Zhang, Z.-H., and Wu, Y.-X.
\newblock Active learning through density clustering.
\newblock \emph{Expert Systems with Applications}, 85:\penalty0 305--317, 2017.
\newblock ISSN 0957-4174.
\newblock \doi{https://doi.org/10.1016/j.eswa.2017.05.046}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S095741741730369X}.

\bibitem[Wang et~al.(2020)Wang, Wohlwend, and Lei]{structuredpruning}
Wang, Z., Wohlwend, J., and Lei, T.
\newblock Structured pruning of large language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  6151--6162, Online, November
  2020. Association for Computational Linguistics.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2021finetuned}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,
  A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Xu et~al.(2020)Xu, Zhou, Ge, Wei, and Zhou]{BERT-of-Theseus}
Xu, C., Zhou, W., Ge, T., Wei, F., and Zhou, M.
\newblock {BERT}-of-theseus: Compressing {BERT} by progressive module
  replacing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  7859--7869, Online, November
  2020. Association for Computational Linguistics.

\bibitem[Yang et~al.(2021)Yang, Wang, Gao, Shrivastava, Weinberger, Chao, and
  Lim]{cotraining2}
Yang, L., Wang, Y., Gao, M., Shrivastava, A., Weinberger, K.~Q., Chao, W.-L.,
  and Lim, S.-N.
\newblock Deep co-training with task decomposition for semi-supervised domain
  adaptation.
\newblock \emph{2021 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, Oct 2021.
\newblock \doi{10.1109/iccv48922.2021.00878}.
\newblock URL \url{http://dx.doi.org/10.1109/iccv48922.2021.00878}.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2020xlnet}
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.~R., and Le, Q.~V.
\newblock {XLN}et: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
You, Y., Li, J., Reddi, S.~J., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{8th International Conference on Learning Representations
  (ICLR 2020)}, Addis Ababa, Ethiopia, 2020. OpenReview.net.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and Wasserblat]{Q8BERT}
Zafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M.
\newblock {Q8BERT:} quantized 8bit {BERT}.
\newblock \emph{2019 Fifth Workshop on Energy Efficient Machine Learning and
  Cognitive Computing - NeurIPS Edition (EMC2-NIPS)}, Dec 2019.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{agnews}
Zhang, X., Zhao, J., and LeCun, Y.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~28, pp.\  649--657. Curran Associates, Inc., 2015.

\bibitem[Zhang et~al.(2019)Zhang, Shapiro, Kumar, McNamee, Carpuat, and
  Duh]{Zhang_2019}
Zhang, X., Shapiro, P., Kumar, G., McNamee, P., Carpuat, M., and Duh, K.
\newblock Curriculum learning for domain adaptation in neural machine
  translation.
\newblock \emph{Proceedings of the 2019 Conference of the North}, 2019.

\bibitem[Zhong et~al.(2021)Zhong, Lee, Zhang, and Klein]{leeadapting}
Zhong, R., Lee, K., Zhang, Z., and Klein, D.
\newblock Adapting language models for zero-shot learning by meta-tuning on
  dataset and prompt collections.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pp.\  2856--2878, Punta Cana, Dominican Republic, November 2021.
  Association for Computational Linguistics.

\bibitem[Zhu et~al.(2010)Zhu, Wang, Tsou, and Ma]{al1}
Zhu, J., Wang, H., Tsou, B.~K., and Ma, M.
\newblock Active learning with sampling by uncertainty and density for data
  annotations.
\newblock \emph{IEEE Transactions on Audio, Speech, and Language Processing},
  18\penalty0 (6):\penalty0 1323--1331, 2010.
\newblock \doi{10.1109/TASL.2009.2033421}.

\end{thebibliography}
