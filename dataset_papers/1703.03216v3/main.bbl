\begin{thebibliography}{10}

\bibitem{azmandian2012local}
F.~Azmandian, J.~G. Dy, J.~A. Aslam, and D.~R. Kaeli.
\newblock Local kernel density ratio-based feature selection for outlier
  detection.
\newblock In {\em Proceedings of 8th Asian Conference on Machine Learning
  (ACML2012), JMLR Workshop and Conference Proceedings}, pages 49--64, 2012.

\bibitem{Boyd2014}
S.~Boyd.
\newblock Subgradient methods.
\newblock Technical report, Stanford University, 2014.
\newblock Notes for EE364b, Stanford University, Spring 2013–14.

\bibitem{Cleveland1979}
W.~S. Cleveland.
\newblock Robust locally weighted regression and smoothing scatterplots.
\newblock {\em Journal of the American Statistical Association},
  74(368):829--836, 1979.

\bibitem{Cristianini2000}
N.~Cristianini and J.~Shawe-Taylor.
\newblock {\em An Introduction to Support Vector Machines and Other
  Kernel-based Learning Methods}.
\newblock Cambridge University Press, 2000.

\bibitem{Efron1996}
B.~Efron and R.~Tibshirani.
\newblock Using specially designed exponential families for density estimation.
\newblock {\em The Annals of Statistics}, 24(6):2431--2461, 1996.

\bibitem{Fazayeli2016}
F.~Fazayeli and A.~Banerjee.
\newblock Generalized direct change estimation in ising model structure.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning (ICML2016)}, page 2281–2290, 2016.

\bibitem{Fithian2015}
W.~Fithian and S.~Wager.
\newblock Semiparametric exponential families for heavy-tailed data.
\newblock {\em Biometrika}, 102(2):486--493, 2015.

\bibitem{Fokianos2004}
K.~Fokianos.
\newblock Merging information for semiparametric density estimation.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 66(4):941--958, 2004.

\bibitem{Goodfellow2014}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{Hadi1997}
A.~S. Hadi and A.~Luceno.
\newblock Maximum trimmed likelihood estimators: a unified approach, examples,
  and algorithms.
\newblock {\em Computational Statistics \& Data Analysis}, 25(3):251 -- 272,
  1997.

\bibitem{Huang2007}
J.~Huang, A.~Gretton, K.~M Borgwardt, B.~Sch{\"o}lkopf, and A.~J Smola.
\newblock Correcting sample selection bias by unlabeled data.
\newblock In {\em Advances in neural information processing systems}, pages
  601--608, 2007.

\bibitem{Huber1964}
P.~J. Huber.
\newblock Robust estimation of a location parameter.
\newblock {\em The Annals of Mathematical Statistics}, 35(1):73--101, 03 1964.

\bibitem{Kawahara2012}
Y.~Kawahara and M.~Sugiyama.
\newblock Sequential change-point detection based on direct density-ratio
  estimation.
\newblock {\em Statistical Analysis and Data Mining}, 5(2):114--127, 2012.

\bibitem{Liu2016a}
S.~Liu, T.~Suzuki, R.~Relator, J.~Sese, M.~Sugiyama, and K.~Fukumizu.
\newblock Support consistency of direct sparse-change learning in {Markov}
  networks.
\newblock {\em Annals of Statistics}, 45(3):959--990, 06 2017.

\bibitem{Loh2015}
P.-L. Loh and M.~J. Wainwright.
\newblock Regularized m-estimators with nonconvexity: Statistical and
  algorithmic theory for local optima.
\newblock {\em Journal of Machine Learning Research}, 16:559--616, 2015.

\bibitem{Nedic2009}
A.~Nedi{\'{c}} and A.~Ozdaglar.
\newblock Subgradient methods for saddle-point problems.
\newblock {\em Journal of Optimization Theory and Applications},
  142(1):205--228, 2009.

\bibitem{Neykov1990}
N.~Neykov and P.~N. Neytchev.
\newblock Robust alternative of the maximum likelihood estimators.
\newblock {\em COMPSTAT'90, Short Communications}, pages 99--100, 1990.

\bibitem{Nguyen2010}
X.~Nguyen, M.~J. Wainwright, and M.~I. Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock {\em IEEE Transactions on Information Theory}, 56(11):5847--5861,
  2010.

\bibitem{Nowozin2016}
S.~Nowozin, B.~Cseke, and R.~Tomioka.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  271--279, 2016.

\bibitem{Pitman1936}
E.~J.~G. Pitman.
\newblock Sufficient statistics and intrinsic accuracy.
\newblock {\em Mathematical Proceedings of the Cambridge Philosophical
  Society}, 32(4):567–579, 1936.

\bibitem{Raskutti2010}
G.~Raskutti, M.~J. Wainwright, and B.~Yu.
\newblock Restricted eigenvalue properties for correlated gaussian designs.
\newblock {\em Journal of Machine Learning Research}, 11:2241--2259, 2010.

\bibitem{Rudelson2013}
M.~Rudelson and S.~Zhou.
\newblock Reconstruction from anisotropic random measurements.
\newblock {\em IEEE Transactions on Information Theory}, 59(6):3434--3447,
  2013.

\bibitem{Scholkopf2001}
B.~Scholkopf and A.~J. Smola.
\newblock {\em Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2001.

\bibitem{Schoelkopf2000}
B.~Sch\"{o}lkopf, R.~C. Williamson, Smola~A. J., Shawe-Taylor J., and Platt
  J.C.
\newblock Support vector method for novelty detection.
\newblock In {\em Advances in Neural Information Processing Systems 12}, pages
  582--588. MIT Press, 2000.

\bibitem{Shimodaira2000}
A.~Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock {\em Journal of Statistical Planning and Inference}, 90(2):227 --
  244, 2000.

\bibitem{Smola2009}
A.~Smola, L.~Song, and C.~H. Teo.
\newblock Relative novelty detection.
\newblock In {\em Proceedings of the Twelth International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, volume~5, pages 536--543,
  2009.

\bibitem{Sugiyama2012}
M.~Sugiyama, T.~Suzuki, and T.~Kanamori.
\newblock {\em Density Ratio Estimation in Machine Learning}.
\newblock Cambridge University Press, 2012.

\bibitem{Sugiyama2008}
M.~Sugiyama, T.~Suzuki, S.~Nakajima, H.~Kashima, P.~von B\"unau, and
  M.~Kawanabe.
\newblock Direct importance estimation for covariate shift adaptation.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  60(4):699--746, 2008.

\bibitem{Suykens2002}
J.~A.~K. Suykens, J.~De~Brabanter, L.~Lukas, and J.~Vandewalle.
\newblock Weighted least squares support vector machines: robustness and sparse
  approximation.
\newblock {\em Neurocomputing}, 48(1):85--105, 2002.

\bibitem{Tsuboi2009}
Y.~Tsuboi, H.~Kashima, S.~Hido, S.~Bickel, and M.~Sugiyama.
\newblock Direct density ratio estimation for large-scale covariate shift
  adaptation.
\newblock {\em Journal of Information Processing}, 17:138--155, 2009.

\bibitem{Vandev1998}
D.~L. Vandev and N.~M. Neykov.
\newblock About regression estimators with high breakdown point.
\newblock {\em Statistics: A Journal of Theoretical and Applied Statistics},
  32(2):111--129, 1998.

\bibitem{Wornowizki2016}
M.~Wornowizki and R.~Fried.
\newblock Two-sample homogeneity tests based on divergence measures.
\newblock {\em Computational Statistics}, 31(1):291--313, 2016.

\bibitem{Yamada2013}
M.~Yamada, T.~Suzuki, T.~Kanamori, H.~Hachiya, and M.~Sugiyama.
\newblock Relative density-ratio estimation for robust distribution comparison.
\newblock {\em Neural Computation}, 25(5):1324--1370, 2013.

\bibitem{yang2016high}
E.~Yang, A.~Lozano, and A.~Aravkin.
\newblock High-dimensional trimmed estimators: A general framework for robust
  structured estimation.
\newblock {\em arXiv preprint arXiv:1605.08299}, 2016.

\bibitem{Yang2015}
E.~Yang and A.~C. Lozano.
\newblock Robust gaussian graphical modeling with the trimmed graphical lasso.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2602--2610, 2015.

\end{thebibliography}
