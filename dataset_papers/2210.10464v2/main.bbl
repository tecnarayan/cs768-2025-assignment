\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20095--20107, 2020.

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Baker, Chociej, Jozefowicz,
  McGrew, Pachocki, Petron, Plappert, Powell, Ray,
  et~al.]{andrychowicz2020learning}
Andrychowicz, O.~M., Baker, B., Chociej, M., Jozefowicz, R., McGrew, B.,
  Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray, A., et~al.
\newblock Learning dexterous in-hand manipulation.
\newblock \emph{The International Journal of Robotics Research}, 39\penalty0
  (1):\penalty0 3--20, 2020.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.~E.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM journal on computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272. PMLR, 2017.

\bibitem[Azizzadenesheli et~al.(2016)Azizzadenesheli, Lazaric, and
  Anandkumar]{azizzadenesheli2016reinforcement}
Azizzadenesheli, K., Lazaric, A., and Anandkumar, A.
\newblock Reinforcement learning of pomdps using spectral methods.
\newblock In \emph{Conference on Learning Theory}, pp.\  193--256. PMLR, 2016.

\bibitem[Bousquet \& Elisseeff(2002)Bousquet and
  Elisseeff]{bousquet2002stability}
Bousquet, O. and Elisseeff, A.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Brunskill \& Li(2013)Brunskill and Li]{brunskill2013sample}
Brunskill, E. and Li, L.
\newblock Sample complexity of multi-task reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1309.6821}, 2013.

\bibitem[Cai et~al.(2017)Cai, Ren, Zhang, Malialis, Wang, Yu, and
  Guo]{cai2017real}
Cai, H., Ren, K., Zhang, W., Malialis, K., Wang, J., Yu, Y., and Guo, D.
\newblock Real-time bidding by reinforcement learning in display advertising.
\newblock In \emph{Proceedings of the Tenth ACM International Conference on Web
  Search and Data Mining}, pp.\  661--670, 2017.

\bibitem[Chen et~al.(2021)Chen, Hu, Yang, and Wang]{chen2021near}
Chen, X., Hu, J., Yang, L.~F., and Wang, L.
\newblock Near-optimal reward-free exploration for linear mixture mdps with
  plug-in solver.
\newblock \emph{arXiv preprint arXiv:2110.03244}, 2021.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and
  Langford]{du2019provably}
Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J.
\newblock Provably efficient rl with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1665--1674. PMLR, 2019.

\bibitem[Farebrother et~al.(2018)Farebrother, Machado, and
  Bowling]{farebrother2018generalization}
Farebrother, J., Machado, M.~C., and Bowling, M.
\newblock Generalization and regularization in dqn.
\newblock \emph{arXiv preprint arXiv:1810.00123}, 2018.

\bibitem[Ghosh et~al.(2021)Ghosh, Rahme, Kumar, Zhang, Adams, and
  Levine]{ghosh2021generalization}
Ghosh, D., Rahme, J., Kumar, A., Zhang, A., Adams, R.~P., and Levine, S.
\newblock Why generalization in rl is difficult: Epistemic pomdps and implicit
  partial observability.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Guo et~al.(2016)Guo, Doroudi, and Brunskill]{guo2016pac}
Guo, Z.~D., Doroudi, S., and Brunskill, E.
\newblock A pac rl algorithm for episodic pomdps.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  510--518.
  PMLR, 2016.

\bibitem[Hu et~al.(2021)Hu, Chen, Jin, Li, and Wang]{hu2021near}
Hu, J., Chen, X., Jin, C., Li, L., and Wang, L.
\newblock Near-optimal representation learning for linear bandits and linear
  rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4349--4358. PMLR, 2021.

\bibitem[James et~al.(2019)James, Wohlhart, Kalakrishnan, Kalashnikov, Irpan,
  Ibarz, Levine, Hadsell, and Bousmalis]{james2019sim}
James, S., Wohlhart, P., Kalakrishnan, M., Kalashnikov, D., Irpan, A., Ibarz,
  J., Levine, S., Hadsell, R., and Bousmalis, K.
\newblock Sim-to-real via sim-to-sim: Data-efficient robotic grasping via
  randomized-to-canonical adaptation networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12627--12637, 2019.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Kakade, Krishnamurthy, and
  Liu]{jin2020sample}
Jin, C., Kakade, S., Krishnamurthy, A., and Liu, Q.
\newblock Sample-efficient reinforcement learning of undercomplete pomdps.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18530--18539, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020{\natexlab{b}}.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 13406--13418, 2021.

\bibitem[Kawaguchi et~al.(2017)Kawaguchi, Kaelbling, and
  Bengio]{kawaguchi2017generalization}
Kawaguchi, K., Kaelbling, L.~P., and Bengio, Y.
\newblock Generalization in deep learning.
\newblock \emph{arXiv preprint arXiv:1710.05468}, 2017.

\bibitem[Kirk et~al.(2021)Kirk, Zhang, Grefenstette, and
  Rockt{\"a}schel]{kirk2021survey}
Kirk, R., Zhang, A., Grefenstette, E., and Rockt{\"a}schel, T.
\newblock A survey of generalisation in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2111.09794}, 2021.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Kober, J., Bagnell, J.~A., and Peters, J.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Kormushev et~al.(2013)Kormushev, Calinon, and
  Caldwell]{kormushev2013reinforcement}
Kormushev, P., Calinon, S., and Caldwell, D.~G.
\newblock Reinforcement learning in robotics: Applications and real-world
  challenges.
\newblock \emph{Robotics}, 2\penalty0 (3):\penalty0 122--148, 2013.

\bibitem[Kwon et~al.(2021{\natexlab{a}})Kwon, Efroni, Caramanis, and
  Mannor]{kwon2021reinforcement}
Kwon, J., Efroni, Y., Caramanis, C., and Mannor, S.
\newblock Reinforcement learning in reward-mixing mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Kwon et~al.(2021{\natexlab{b}})Kwon, Efroni, Caramanis, and
  Mannor]{kwon2021rl}
Kwon, J., Efroni, Y., Caramanis, C., and Mannor, S.
\newblock Rl for latent mdps: Regret guarantees and a lower bound.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and
  Szepesv{\'a}ri]{banditbook}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Li et~al.(2022)Li, Wang, and Yang]{li2022settling}
Li, Y., Wang, R., and Yang, L.~F.
\newblock Settling the horizon-dependence of sample complexity in reinforcement
  learning.
\newblock In \emph{2021 IEEE 62nd Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  965--976. IEEE, 2022.

\bibitem[Lu et~al.(2021)Lu, Huang, and Du]{lu2021power}
Lu, R., Huang, G., and Du, S.~S.
\newblock On the power of multitask representation learning in linear mdp.
\newblock \emph{arXiv preprint arXiv:2106.08053}, 2021.

\bibitem[Malik et~al.(2021)Malik, Li, and Ravikumar]{malik2021generalizable}
Malik, D., Li, Y., and Ravikumar, P.
\newblock When is generalizable reinforcement learning tractable?
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mao et~al.(2016)Mao, Alizadeh, Menache, and Kandula]{mao2016resource}
Mao, H., Alizadeh, M., Menache, I., and Kandula, S.
\newblock Resource management with deep reinforcement learning.
\newblock In \emph{Proceedings of the 15th ACM workshop on hot topics in
  networks}, pp.\  50--56, 2016.

\bibitem[Mitchell et~al.(1986)Mitchell, Keller, and
  Kedar-Cabelli]{mitchell1986explanation}
Mitchell, T.~M., Keller, R.~M., and Kedar-Cabelli, S.~T.
\newblock Explanation-based generalization: A unifying view.
\newblock \emph{Machine learning}, 1\penalty0 (1):\penalty0 47--80, 1986.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[O'Donoghue(2021)]{o2021variational}
O'Donoghue, B.
\newblock Variational bayesian reinforcement learning with regret bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Osband \& Van~Roy(2017)Osband and Van~Roy]{osband2017posterior}
Osband, I. and Van~Roy, B.
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In \emph{International conference on machine learning}, pp.\
  2701--2710. PMLR, 2017.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Packer et~al.(2018)Packer, Gao, Kos, Kr{\"a}henb{\"u}hl, Koltun, and
  Song]{packer2018assessing}
Packer, C., Gao, K., Kos, J., Kr{\"a}henb{\"u}hl, P., Koltun, V., and Song, D.
\newblock Assessing generalization in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1810.12282}, 2018.

\bibitem[Peng et~al.(2018)Peng, Andrychowicz, Zaremba, and Abbeel]{peng2018sim}
Peng, X.~B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Sim-to-real transfer of robotic control with dynamics randomization.
\newblock In \emph{2018 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  3803--3810. IEEE, 2018.

\bibitem[Rajeswaran et~al.(2016)Rajeswaran, Ghotra, Ravindran, and
  Levine]{rajeswaran2016epopt}
Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S.
\newblock Epopt: Learning robust neural network policies using model ensembles.
\newblock \emph{arXiv preprint arXiv:1610.01283}, 2016.

\bibitem[Ren et~al.(2021)Ren, Li, Dai, Du, and Sanghavi]{ren2021nearly}
Ren, T., Li, J., Dai, B., Du, S.~S., and Sanghavi, S.
\newblock Nearly horizon-free offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave,
  Wiele, Mnih, Heess, and Springenberg]{riedmiller2018learning}
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Wiele, T.,
  Mnih, V., Heess, N., and Springenberg, J.~T.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In \emph{International conference on machine learning}, pp.\
  4344--4353. PMLR, 2018.

\bibitem[Rusu et~al.(2017)Rusu, Ve{\v{c}}er{\'\i}k, Roth{\"o}rl, Heess,
  Pascanu, and Hadsell]{rusu2017sim}
Rusu, A.~A., Ve{\v{c}}er{\'\i}k, M., Roth{\"o}rl, T., Heess, N., Pascanu, R.,
  and Hadsell, R.
\newblock Sim-to-real robot learning from pixels with progressive nets.
\newblock In \emph{Conference on Robot Learning}, pp.\  262--270. PMLR, 2017.

\bibitem[Sallab et~al.(2017)Sallab, Abdou, Perot, and Yogamani]{sallab2017deep}
Sallab, A.~E., Abdou, M., Perot, E., and Yogamani, S.
\newblock Deep reinforcement learning framework for autonomous driving.
\newblock \emph{Electronic Imaging}, 2017\penalty0 (19):\penalty0 70--76, 2017.

\bibitem[Shalev-Shwartz et~al.(2016)Shalev-Shwartz, Shammah, and
  Shashua]{shalev2016safe}
Shalev-Shwartz, S., Shammah, S., and Shashua, A.
\newblock Safe, multi-agent, reinforcement learning for autonomous driving.
\newblock \emph{arXiv preprint arXiv:1610.03295}, 2016.

\bibitem[Shani et~al.(2005)Shani, Heckerman, Brafman, and
  Boutilier]{shani2005mdp}
Shani, G., Heckerman, D., Brafman, R.~I., and Boutilier, C.
\newblock An mdp-based recommender system.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (9), 2005.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sutton(1995)]{sutton1995generalization}
Sutton, R.~S.
\newblock Generalization in reinforcement learning: Successful examples using
  sparse coarse coding.
\newblock \emph{Advances in neural information processing systems}, 8, 1995.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tirinzoni et~al.(2020)Tirinzoni, Poiani, and
  Restelli]{tirinzoni2020sequential}
Tirinzoni, A., Poiani, R., and Restelli, M.
\newblock Sequential transfer in reinforcement learning with a generative
  model.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9481--9492. PMLR, 2020.

\bibitem[Vecerik et~al.(2017)Vecerik, Hester, Scholz, Wang, Pietquin, Piot,
  Heess, Roth{\"o}rl, Lampe, and Riedmiller]{vecerik2017leveraging}
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess,
  N., Roth{\"o}rl, T., Lampe, T., and Riedmiller, M.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2019)Wang, Zheng, Xiong, and
  Socher]{wang2019generalization}
Wang, H., Zheng, S., Xiong, C., and Socher, R.
\newblock On the generalization gap in reparameterizable reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6648--6658. PMLR, 2019.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020reinforcement}
Wang, R., Salakhutdinov, R.~R., and Yang, L.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6123--6135, 2020.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and
  Weinberger]{weissman2003inequalities}
Weissman, T., Ordentlich, E., Seroussi, G., Verdu, S., and Weinberger, M.~J.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock \emph{Hewlett-Packard Labs, Tech. Rep}, 2003.

\bibitem[Yu et~al.(2021)Yu, Liu, Nemati, and Yin]{yu2021reinforcement}
Yu, C., Liu, J., Nemati, S., and Yin, G.
\newblock Reinforcement learning in healthcare: A survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 55\penalty0 (1):\penalty0 1--36,
  2021.

\bibitem[Zhang et~al.(2020)Zhang, McAllister, Calandra, Gal, and
  Levine]{zhang2020learning}
Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine, S.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock \emph{arXiv preprint arXiv:2006.10742}, 2020.

\bibitem[Zhang \& Wang(2021)Zhang and Wang]{zhang2021provably}
Zhang, C. and Wang, Z.
\newblock Provably efficient multi-task reinforcement learning with model
  transfer.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Ji, and Du]{zhang2021reinforcement}
Zhang, Z., Ji, X., and Du, S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \emph{Conference on Learning Theory}, pp.\  4528--4531. PMLR,
  2021.

\bibitem[Zheng et~al.(2018)Zheng, Zhang, Zheng, Xiang, Yuan, Xie, and
  Li]{zheng2018drn}
Zheng, G., Zhang, F., Zheng, Z., Xiang, Y., Yuan, N.~J., Xie, X., and Li, Z.
\newblock Drn: A deep reinforcement learning framework for news recommendation.
\newblock In \emph{Proceedings of the 2018 World Wide Web Conference}, pp.\
  167--176, 2018.

\end{thebibliography}
