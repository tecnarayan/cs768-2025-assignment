% Generated by Paperpile. Check out https://paperpile.com for more information.
% BibTeX export options can be customized via Settings -> BibTeX.

@ARTICLE{Cisse2017-oz,
  title         = "Parseval Networks: Improving Robustness to Adversarial
                   Examples",
  author        = "Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard
                   and Dauphin, Yann and Usunier, Nicolas",
  abstract      = "We introduce Parseval networks, a form of deep neural
                   networks in which the Lipschitz constant of linear,
                   convolutional and aggregation layers is constrained to be
                   smaller than 1. Parseval networks are empirically and
                   theoretically motivated by an analysis of the robustness of
                   the predictions made by deep neural networks when their
                   input is subject to an adversarial perturbation. The most
                   important feature of Parseval networks is to maintain weight
                   matrices of linear and convolutional layers to be
                   (approximately) Parseval tight frames, which are extensions
                   of orthogonal matrices to non-square matrices. We describe
                   how these constraints can be maintained efficiently during
                   SGD. We show that Parseval networks match the
                   state-of-the-art in terms of accuracy on CIFAR-10/100 and
                   Street View House Numbers (SVHN) while being more robust
                   than their vanilla counterpart against adversarial examples.
                   Incidentally, Parseval networks also tend to train faster
                   and make a better usage of the full capacity of the
                   networks.",
  month         =  apr,
  year          =  2017,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1704.08847"
}

@ARTICLE{Lomuscio2017-ol,
  title         = "An approach to reachability analysis for feed-forward {ReLU}
                   neural networks",
  author        = "Lomuscio, Alessio and Maganti, Lalit",
  abstract      = "We study the reachability problem for systems implemented as
                   feed-forward neural networks whose activation function is
                   implemented via ReLU functions. We draw a correspondence
                   between establishing whether some arbitrary output can ever
                   be outputed by a neural system and linear problems
                   characterising a neural system of interest. We present a
                   methodology to solve cases of practical interest by means of
                   a state-of-the-art linear programs solver. We evaluate the
                   technique presented by discussing the experimental results
                   obtained by analysing reachability properties for a number
                   of benchmarks in the literature.",
  month         =  jun,
  year          =  2017,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1706.07351"
}

@ARTICLE{Fischetti2018-mi,
  title    = "Deep neural networks and mixed integer linear optimization",
  author   = "Fischetti, Matteo and Jo, Jason",
  abstract = "Deep Neural Networks (DNNs) are very popular these days, and are
              the subject of a very intense investigation. A DNN is made up of
              layers of internal units (or neurons), each of which computes an
              affine combination of the output of the units in the previous
              layer, applies a nonlinear operator, and outputs the
              corresponding value (also known as activation). A commonly-used
              nonlinear operator is the so-called rectified linear unit (ReLU),
              whose output is just the maximum between its input value and
              zero. In this (and other similar cases like max pooling, where
              the max operation involves more than one input value), for fixed
              parameters one can model the DNN as a 0-1 Mixed Integer Linear
              Program (0-1 MILP) where the continuous variables correspond to
              the output values of each unit, and a binary variable is
              associated with each ReLU to model its yes/no nature. In this
              paper we discuss the peculiarity of this kind of 0-1 MILP models,
              and describe an effective bound-tightening technique intended to
              ease its solution. We also present possible applications of the
              0-1 MILP model arising in feature visualization and in the
              construction of adversarial examples. Computational results are
              reported, aimed at investigating (on small DNNs) the
              computational performance of a state-of-the-art MILP solver when
              applied to a known test case, namely, hand-written digit
              recognition.",
  journal  = "Constraints",
  volume   =  23,
  number   =  3,
  pages    = "296--309",
  month    =  jul,
  year     =  2018,
  keywords = "LipMip citations"
}

@ARTICLE{Dutta2017-mn,
  title         = "Output Range Analysis for Deep Neural Networks",
  author        = "Dutta, Souradeep and Jha, Susmit and Sanakaranarayanan,
                   Sriram and Tiwari, Ashish",
  abstract      = "Deep neural networks (NN) are extensively used for machine
                   learning tasks such as image classification, perception and
                   control of autonomous systems. Increasingly, these deep NNs
                   are also been deployed in high-assurance applications. Thus,
                   there is a pressing need for developing techniques to verify
                   neural networks to check whether certain user-expected
                   properties are satisfied. In this paper, we study a specific
                   verification problem of computing a guaranteed range for the
                   output of a deep neural network given a set of inputs
                   represented as a convex polyhedron. Range estimation is a
                   key primitive for verifying deep NNs. We present an
                   efficient range estimation algorithm that uses a combination
                   of local search and linear programming problems to
                   efficiently find the maximum and minimum values taken by the
                   outputs of the NN over the given input set. In contrast to
                   recently proposed ``monolithic'' optimization approaches, we
                   use local gradient descent to repeatedly find and eliminate
                   local minima of the function. The final global optimum is
                   certified using a mixed integer programming instance. We
                   implement our approach and compare it with Reluplex, a
                   recently proposed solver for deep neural networks. We
                   demonstrate the effectiveness of the proposed approach for
                   verification of NNs used in automated control as well as
                   those used in classification.",
  month         =  sep,
  year          =  2017,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1709.09130"
}

@ARTICLE{SinghGagandeep2019-ki,
  title     = "An abstract domain for certifying neural networks",
  author    = "{Singh} and {Gehr} and {P{\"u}schel} and
               {Vechev}",
  abstract  = "We present a novel method for scalable and precise certification
               of deep neural networks. The key technical insight behind our
               approach is a new abstract domain which combines floating point
               polyhe...",
  journal   = "Proceedings of the ACM on Programming Languages",
  publisher = "ACM PUB27 New York, NY, USA",
  month     =  jan,
  year      =  2019,
  keywords  = "LipMip citations",
  language  = "en"
}

@ARTICLE{Bartlett2017-od,
  title         = "Spectrally-normalized margin bounds for neural networks",
  author        = "Bartlett, Peter and Foster, Dylan J and Telgarsky, Matus",
  abstract      = "This paper presents a margin-based multiclass generalization
                   bound for neural networks that scales with their
                   margin-normalized ``spectral complexity'': their Lipschitz
                   constant, meaning the product of the spectral norms of the
                   weight matrices, times a certain correction factor. This
                   bound is empirically investigated for a standard AlexNet
                   network trained with SGD on the mnist and cifar10 datasets,
                   with both original and random labels; the bound, the
                   Lipschitz constants, and the excess risks are all in direct
                   correlation, suggesting both that SGD selects predictors
                   whose complexity scales with the difficulty of the learning
                   task, and secondly that the presented bound is sensitive to
                   this complexity.",
  month         =  jun,
  year          =  2017,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1706.08498"
}

@ARTICLE{Cisse2017-it,
  title         = "Parseval Networks: Improving Robustness to Adversarial
                   Examples",
  author        = "Cisse, Moustapha and Bojanowski, Piotr and Grave, Edouard
                   and Dauphin, Yann and Usunier, Nicolas",
  abstract      = "We introduce Parseval networks, a form of deep neural
                   networks in which the Lipschitz constant of linear,
                   convolutional and aggregation layers is constrained to be
                   smaller than 1. Parseval networks are empirically and
                   theoretically motivated by an analysis of the robustness of
                   the predictions made by deep neural networks when their
                   input is subject to an adversarial perturbation. The most
                   important feature of Parseval networks is to maintain weight
                   matrices of linear and convolutional layers to be
                   (approximately) Parseval tight frames, which are extensions
                   of orthogonal matrices to non-square matrices. We describe
                   how these constraints can be maintained efficiently during
                   SGD. We show that Parseval networks match the
                   state-of-the-art in terms of accuracy on CIFAR-10/100 and
                   Street View House Numbers (SVHN) while being more robust
                   than their vanilla counterpart against adversarial examples.
                   Incidentally, Parseval networks also tend to train faster
                   and make a better usage of the full capacity of the
                   networks.",
  month         =  apr,
  year          =  2017,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1704.08847"
}

@ARTICLE{lipopt,
  title    = "Lipschitz constant estimation of Neural Networks via sparse
              polynomial optimization",
  author   = "Latorre, Fabian and Rolland, Paul and Cevher, Volkan",
  abstract = "We introduce LiPopt, a polynomial optimization framework for
              computing increasingly tighter upper bound on the Lipschitz
              constant of neural networks. The underlying optimization problems
              boil down to either linear (LP) or semidefinite (SDP)
              programming. We show how to use the sparse connectivity of a
              network, to significantly reduce the complexity of computation.
              This is specially useful for convolutional as well as pruned
              neural networks. We conduct experiments on networks with random
              weights as well as networks trained on MNIST, showing that in the
              particular case of the $\ell_\infty$-Lipschitz constant, our
              approach yields superior estimates as compared to other baselines
              available in the literature.",
  month    =  sep,
  year     =  2019,
  keywords = "LipMip citations"
}



@ARTICLE{Tjeng2017-qp,
  title         = "Evaluating Robustness of Neural Networks with Mixed Integer
                   Programming",
  author        = "Tjeng, Vincent and Xiao, Kai and Tedrake, Russ",
  abstract      = "Neural networks have demonstrated considerable success on a
                   wide variety of real-world problems. However, networks
                   trained only to optimize for training accuracy can often be
                   fooled by adversarial examples - slightly perturbed inputs
                   that are misclassified with high confidence. Verification of
                   networks enables us to gauge their vulnerability to such
                   adversarial examples. We formulate verification of
                   piecewise-linear neural networks as a mixed integer program.
                   On a representative task of finding minimum adversarial
                   distortions, our verifier is two to three orders of
                   magnitude quicker than the state-of-the-art. We achieve this
                   computational speedup via tight formulations for
                   non-linearities, as well as a novel presolve algorithm that
                   makes full use of all information available. The
                   computational speedup allows us to verify properties on
                   convolutional networks with an order of magnitude more ReLUs
                   than networks previously verified by any complete verifier.
                   In particular, we determine for the first time the exact
                   adversarial accuracy of an MNIST classifier to perturbations
                   with bounded $l_\infty$ norm $\epsilon=0.1$: for this
                   classifier, we find an adversarial example for 4.38\% of
                   samples, and a certificate of robustness (to perturbations
                   with bounded norm) for the remainder. Across all robust
                   training procedures and network architectures considered, we
                   are able to certify more samples than the state-of-the-art
                   and find more adversarial examples than a strong first-order
                   attack.",
  month         =  nov,
  year          =  2017,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1711.07356"
}

@ARTICLE{Cheng2017-xq,
  title         = "Maximum Resilience of Artificial Neural Networks",
  author        = "Cheng, Chih-Hong and N{\"u}hrenberg, Georg and Ruess, Harald",
  abstract      = "The deployment of Artificial Neural Networks (ANNs) in
                   safety-critical applications poses a number of new
                   verification and certification challenges. In particular,
                   for ANN-enabled self-driving vehicles it is important to
                   establish properties about the resilience of ANNs to noisy
                   or even maliciously manipulated sensory input. We are
                   addressing these challenges by defining resilience
                   properties of ANN-based classifiers as the maximal amount of
                   input or sensor perturbation which is still tolerated. This
                   problem of computing maximal perturbation bounds for ANNs is
                   then reduced to solving mixed integer optimization problems
                   (MIP). A number of MIP encoding heuristics are developed for
                   drastically reducing MIP-solver runtimes, and using
                   parallelization of MIP-solvers results in an almost linear
                   speed-up in the number (up to a certain limit) of computing
                   cores in our experiments. We demonstrate the effectiveness
                   and scalability of our approach by means of computing
                   maximal resilience bounds for a number of ANN benchmark sets
                   ranging from typical image recognition scenarios to the
                   autonomous maneuvering of robots.",
  month         =  apr,
  year          =  2017,
  keywords      = "Adversarial Examples;geocert\_citations;LipMip
                   citations;geocert\_shared",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1705.01040"
}

@ARTICLE{Katz2017-qz,
  title         = "Reluplex: An Efficient {SMT} Solver for Verifying Deep
                   Neural Networks",
  author        = "Katz, Guy and Barrett, Clark and Dill, David and Julian,
                   Kyle and Kochenderfer, Mykel",
  abstract      = "Deep neural networks have emerged as a widely used and
                   effective means for tackling complex, real-world problems.
                   However, a major obstacle in applying them to
                   safety-critical systems is the great difficulty in providing
                   formal guarantees about their behavior. We present a novel,
                   scalable, and efficient technique for verifying properties
                   of deep neural networks (or providing counter-examples). The
                   technique is based on the simplex method, extended to handle
                   the non-convex Rectified Linear Unit (ReLU) activation
                   function, which is a crucial ingredient in many modern
                   neural networks. The verification procedure tackles neural
                   networks as a whole, without making any simplifying
                   assumptions. We evaluated our technique on a prototype deep
                   neural network implementation of the next-generation
                   airborne collision avoidance system for unmanned aircraft
                   (ACAS Xu). Results show that our technique can successfully
                   prove properties of networks that are an order of magnitude
                   larger than the largest networks verified using existing
                   methods.",
  month         =  feb,
  year          =  2017,
  keywords      = "Adversarial Examples;geocert\_citations;LipMip
                   citations;geocert\_shared",
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1702.01135"
}

@ARTICLE{Xiao2018-aj,
  title         = "Training for Faster Adversarial Robustness Verification via
                   Inducing {ReLU} Stability",
  author        = "Xiao, Kai Y and Tjeng, Vincent and Shafiullah, Nur Muhammad
                   and Madry, Aleksander",
  abstract      = "We explore the concept of co-design in the context of neural
                   network verification. Specifically, we aim to train deep
                   neural networks that not only are robust to adversarial
                   perturbations but also whose robustness can be verified more
                   easily. To this end, we identify two properties of network
                   models - weight sparsity and so-called ReLU stability - that
                   turn out to significantly impact the complexity of the
                   corresponding verification task. We demonstrate that
                   improving weight sparsity alone already enables us to turn
                   computationally intractable verification problems into
                   tractable ones. Then, improving ReLU stability leads to an
                   additional 4-13x speedup in verification times. An important
                   feature of our methodology is its ``universality,'' in the
                   sense that it can be used with a broad range of training
                   procedures and verification approaches.",
  month         =  sep,
  year          =  2018,
  keywords      = "Adversarial Examples;geocert\_citations;LipMip
                   citations;geocert\_shared",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1809.03008"
}

@ARTICLE{Arjovsky2017-ko,
  title         = "Wasserstein {GAN}",
  author        = "Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on",
  abstract      = "We introduce a new algorithm named WGAN, an alternative to
                   traditional GAN training. In this new model, we show that we
                   can improve the stability of learning, get rid of problems
                   like mode collapse, and provide meaningful learning curves
                   useful for debugging and hyperparameter searches.
                   Furthermore, we show that the corresponding optimization
                   problem is sound, and provide extensive theoretical work
                   highlighting the deep connections to other distances between
                   distributions.",
  month         =  jan,
  year          =  2017,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1701.07875"
}

@ARTICLE{Goodfellow2014-vh,
  title         = "Explaining and Harnessing Adversarial Examples",
  author        = "Goodfellow, Ian J and Shlens, Jonathon and Szegedy,
                   Christian",
  abstract      = "Several machine learning models, including neural networks,
                   consistently misclassify adversarial examples---inputs
                   formed by applying small but intentionally worst-case
                   perturbations to examples from the dataset, such that the
                   perturbed input results in the model outputting an incorrect
                   answer with high confidence. Early attempts at explaining
                   this phenomenon focused on nonlinearity and overfitting. We
                   argue instead that the primary cause of neural networks'
                   vulnerability to adversarial perturbation is their linear
                   nature. This explanation is supported by new quantitative
                   results while giving the first explanation of the most
                   intriguing fact about them: their generalization across
                   architectures and training sets. Moreover, this view yields
                   a simple and fast method of generating adversarial examples.
                   Using this approach to provide examples for adversarial
                   training, we reduce the test set error of a maxout network
                   on the MNIST dataset.",
  month         =  dec,
  year          =  2014,
  keywords      = "Adversarial Examples;LipMip citations;Unsup final
                   project;Adversarial Examples (DG)",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1412.6572"
  }
  
  
@ARTICLE{Zuckerman2007-dm,
  title     = "Linear Degree Extractors and the Inapproximability of Max Clique
               and Chromatic Number",
  author    = "Zuckerman, David",
  journal   = "Theory of Computing",
  publisher = "Theory of Computing",
  volume    =  3,
  number    =  6,
  pages     = "103--128",
  year      =  2007
}


@ARTICLE{Petzka2017-cf,
  title         = "On the regularization of Wasserstein {GANs}",
  author        = "Petzka, Henning and Fischer, Asja and Lukovnicov, Denis",
  abstract      = "Since their invention, generative adversarial networks
                   (GANs) have become a popular approach for learning to model
                   a distribution of real (unlabeled) data. Convergence
                   problems during training are overcome by Wasserstein GANs
                   which minimize the distance between the model and the
                   empirical distribution in terms of a different metric, but
                   thereby introduce a Lipschitz constraint into the
                   optimization problem. A simple way to enforce the Lipschitz
                   constraint on the class of functions, which can be modeled
                   by the neural network, is weight clipping. It was proposed
                   that training can be improved by instead augmenting the loss
                   by a regularization term that penalizes the deviation of the
                   gradient of the critic (as a function of the network's
                   input) from one. We present theoretical arguments why using
                   a weaker regularization term enforcing the Lipschitz
                   constraint is preferable. These arguments are supported by
                   experimental results on toy data sets.",
  month         =  sep,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1709.08894"
}


@ARTICLE{Szegedy2013-yt,
  title         = "Intriguing properties of neural networks",
  author        = "Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya
                   and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and
                   Fergus, Rob",
  abstract      = "Deep neural networks are highly expressive models that have
                   recently achieved state of the art performance on speech and
                   visual recognition tasks. While their expressiveness is the
                   reason they succeed, it also causes them to learn
                   uninterpretable solutions that could have counter-intuitive
                   properties. In this paper we report two such properties.
                   First, we find that there is no distinction between
                   individual high level units and random linear combinations
                   of high level units, according to various methods of unit
                   analysis. It suggests that it is the space, rather than the
                   individual units, that contains of the semantic information
                   in the high layers of neural networks. Second, we find that
                   deep neural networks learn input-output mappings that are
                   fairly discontinuous to a significant extend. We can cause
                   the network to misclassify an image by applying a certain
                   imperceptible perturbation, which is found by maximizing the
                   network's prediction error. In addition, the specific nature
                   of these perturbations is not a random artifact of learning:
                   the same perturbation can cause a different network, that
                   was trained on a different subset of the dataset, to
                   misclassify the same input.",
  month         =  dec,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1312.6199"
}


@ARTICLE{Paulavicius2006-no,
  title     = "Analysis of different norms and corresponding Lipschitz
               constants for global optimization",
  author    = "Paulavi{\v c}ius, Remigijus and {\v Z}ilinskas, Julius",
  abstract  = "Abstract The paper discusses how the used norm and corresponding
               Lipschitz constant influence the speed of algorithms for global
               optimization. For this reason Lipschitz constants corresponding
               to different norms were estimated. Different test functions for
               global optimization were solved using branch?and?bound algorithm
               for Lipschitz optimization with different norms. Experiments
               have shown that the best results are achieved when combination
               of extreme (infinite and first) and sometimes Euclidean norms is
               used.",
  journal   = "Ukio Technol. Ekonominis Vystymas",
  publisher = "Taylor \& Francis",
  volume    =  12,
  number    =  4,
  pages     = "301--306",
  month     =  jan,
  year      =  2006
}


@INCOLLECTION{Jordan2019-fv,
  title     = "Provable Certificates for Adversarial Examples: Fitting a Ball
               in the Union of Polytopes",
  booktitle = "Advances in Neural Information Processing Systems 32",
  author    = "Jordan, Matt and Lewis, Justin and Dimakis, Alexandros G",
  editor    = "Wallach, H and Larochelle, H and Beygelzimer, A and
               d Alche-Buc, F and Fox, E and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "14059--14069",
  year      =  2019
}


@ARTICLE{Hanin2019-xv,
  title         = "Deep {ReLU} Networks Have Surprisingly Few Activation
                   Patterns",
  author        = "Hanin, Boris and Rolnick, David",
  abstract      = "The success of deep networks has been attributed in part to
                   their expressivity: per parameter, deep networks can
                   approximate a richer class of functions than shallow
                   networks. In ReLU networks, the number of activation
                   patterns is one measure of expressivity; and the maximum
                   number of patterns grows exponentially with the depth.
                   However, recent work has showed that the practical
                   expressivity of deep networks - the functions they can learn
                   rather than express - is often far from the theoretical
                   maximum. In this paper, we show that the average number of
                   activation patterns for ReLU networks at initialization is
                   bounded by the total number of neurons raised to the input
                   dimension. We show empirically that this bound, which is
                   independent of the depth, is tight both at initialization
                   and during training, even on memorization tasks that should
                   maximize the number of activation patterns. Our work
                   suggests that realizing the full expressivity of deep
                   networks may not be possible in practice, at least with
                   current methods.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1906.00904"
}




@INCOLLECTION{Pennington2017-qu,
  title     = "Nonlinear random matrix theory for deep learning",
  booktitle = "Advances in Neural Information Processing Systems 30",
  author    = "Pennington, Jeffrey and Worah, Pratik",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "2637--2646",
  year      =  2017
}

@article{clarke1975generalized,
  title={Generalized gradients and applications},
  author={Clarke, Frank H},
  journal={Transactions of the American Mathematical Society},
  volume={205},
  pages={247--262},
  year={1975}
}
@inproceedings{kakade2018provably,
  title={Provably correct automatic sub-differentiation for qualified programs},
  author={Kakade, Sham M and Lee, Jason D},
  booktitle={Advances in neural information processing systems},
  pages={7125--7135},
  year={2018}
}
@inproceedings{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS-W},
  year={2017}
}

@misc{yang2020adversarial,
    title={Adversarial Robustness Through Local Lipschitzness},
    author={Yao-Yuan Yang and Cyrus Rashtchian and Hongyang Zhang and Ruslan Salakhutdinov and Kamalika Chaudhuri},
    year={2020},
    eprint={2003.02460},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{yurochkin2020training,
  title={Training individually fair ML models with sensitive subspace robustness},
  author={Yurochkin, Mikhail and Bower, Amanda and Sun, Yuekai},
  booktitle={International Conference on Learning Representations, Addis Ababa, Ethiopia},
  year={2020}
}

@misc{dwork2011fairness,
    title={Fairness Through Awareness},
    author={Cynthia Dwork and Moritz Hardt and Toniann Pitassi and Omer Reingold and Rich Zemel},
    year={2011},
    eprint={1104.3913},
    archivePrefix={arXiv},
    primaryClass={cs.CC}
}

@INPROCEEDINGS{impagliazzo-eth,
  author={R. {Impagliazzo} and R. {Paturi}},
  booktitle={Proceedings. Fourteenth Annual IEEE Conference on Computational Complexity (Formerly: Structure in Complexity Theory Conference) (Cat.No.99CB36317)}, 
  title={Complexity of k-SAT}, 
  year={1999},
  volume={},
  number={},
  pages={237-240},}
  
  @book{griewank2008evaluating,
  title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  volume={105},
  year={2008},
  publisher={Siam}
}

@BOOK{Heinonen2005-rp,
  title     = "Lectures on Lipschitz analysis",
  author    = "Heinonen, Juha",
  publisher = "University of Jyv{\"a}skyl{\"a}",
  year      =  2005
}

@article{khan2013evaluating,
  title={Evaluating an element of the Clarke generalized Jacobian of a composite piecewise differentiable function},
  author={Khan, Kamil A and Barton, Paul I},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={39},
  number={4},
  pages={1--28},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@inproceedings{singh2018fast,
  title={Fast and effective robustness certification},
  author={Singh, Gagandeep and Gehr, Timon and Mirman, Matthew and P{\"u}schel, Markus and Vechev, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10802--10813},
  year={2018}
}


@misc{tran2020verification,
    title={Verification of Deep Convolutional Neural Networks Using ImageStars},
    author={Hoang-Dung Tran and Stanley Bak and Weiming Xiang and Taylor T. Johnson},
    year={2020},
    eprint={2004.05511},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



@book{hromkovivc2013algorithmics,
  title={Algorithmics for hard problems: introduction to combinatorial optimization, randomization, approximation, and heuristics},
  author={Hromkovi{\v{c}}, Juraj},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@book{williamson2011design,
  title={The design of approximation algorithms},
  author={Williamson, David P and Shmoys, David B},
  year={2011},
  publisher={Cambridge university press}
}


@MISC{Demaine,
    author={Demaine, Erik},
    year={2014},
  title        = "6.892 Algorithmic Lower Bounds: Fun with Hardness Proofs
                  (Spring '19)",
  howpublished = "\url{http://courses.csail.mit.edu/6.892/spring19/}",
  note         = "Accessed: 2020-2-01"
}


@ARTICLE{Weng2018-gr,
  title         = "Towards Fast Computation of Certified Robustness for {ReLU}
                   Networks",
  author        = "Weng, Tsui-Wei and Zhang, Huan and Chen, Hongge and Song,
                   Zhao and Hsieh, Cho-Jui and Boning, Duane and Dhillon,
                   Inderjit S and Daniel, Luca",
  abstract      = "Verifying the robustness property of a general Rectified
                   Linear Unit (ReLU) network is an NP-complete problem [Katz,
                   Barrett, Dill, Julian and Kochenderfer CAV17]. Although
                   finding the exact minimum adversarial distortion is hard,
                   giving a certified lower bound of the minimum distortion is
                   possible. Current available methods of computing such a
                   bound are either time-consuming or delivering low quality
                   bounds that are too loose to be useful. In this paper, we
                   exploit the special structure of ReLU networks and provide
                   two computationally efficient algorithms Fast-Lin and
                   Fast-Lip that are able to certify non-trivial lower bounds
                   of minimum distortions, by bounding the ReLU units with
                   appropriate linear functions Fast-Lin, or by bounding the
                   local Lipschitz constant Fast-Lip. Experiments show that (1)
                   our proposed methods deliver bounds close to (the gap is
                   2-3X) exact minimum distortion found by Reluplex in small
                   MNIST networks while our algorithms are more than 10,000
                   times faster; (2) our methods deliver similar quality of
                   bounds (the gap is within 35\% and usually around 10\%;
                   sometimes our bounds are even better) for larger networks
                   compared to the methods based on solving linear programming
                   problems but our algorithms are 33-14,000 times faster; (3)
                   our method is capable of solving large MNIST and CIFAR
                   networks up to 7 layers with more than 10,000 neurons within
                   tens of seconds on a single CPU core. In addition, we show
                   that, in fact, there is no polynomial time algorithm that
                   can approximately find the minimum $\ell_1$ adversarial
                   distortion of a ReLU network with a $0.99\ln n$
                   approximation ratio unless $\mathsf\{NP\}$=$\mathsf\{P\}$,
                   where $n$ is the number of neurons in the network.",
  month         =  apr,
  year          =  2018,
  keywords      = "LipMip citations",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1804.09699"
}



@ARTICLE{Weng2018-lf,
  title    = "Evaluating the Robustness of Neural Networks: An Extreme Value
              Theory Approach",
  author   = "Weng, Tsui-Wei and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng
              and Daniel, Luca",
  abstract = "Request PDF | Evaluating the Robustness of Neural Networks: An
              Extreme Value Theory Approach | The robustness of neural networks
              to adversarial examples has received great attention due to
              security implications. Despite various attack... | Find, read and
              cite all the research you need on ResearchGate",
  month    =  jan,
  year     =  2018,
  keywords = "LipMip citations"
}

@INCOLLECTION{Fazlyab2019-im,
  title     = "Efficient and Accurate Estimation of Lipschitz Constants for
               Deep Neural Networks",
  booktitle = "Advances in Neural Information Processing Systems 32",
  author    = "Fazlyab, Mahyar and Robey, Alexander and Hassani, Hamed and
               Morari, Manfred and Pappas, George",
  editor    = "Wallach, H and Larochelle, H and Beygelzimer, A and
               d Alche-Buc, F and Fox, E and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "11423--11434",
  year      =  2019,
  keywords  = "LipMip citations"
}

@INCOLLECTION{Virmaux2018-ti,
  title     = "Lipschitz regularity of deep neural networks: analysis and
               efficient estimation",
  booktitle = "Advances in Neural Information Processing Systems 31",
  author    = "Virmaux, Aladin and Scaman, Kevin",
  editor    = "Bengio, S and Wallach, H and Larochelle, H and Grauman, K and
               Cesa-Bianchi, N and Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "3835--3844",
  year      =  2018,
  keywords  = "LipMip citations"
}



@ARTICLE{Zico_Kolter2017-va,
  title         = "Provable defenses against adversarial examples via the
                   convex outer adversarial polytope",
  author        = "Zico Kolter, J and Wong, Eric",
  abstract      = "We propose a method to learn deep ReLU-based classifiers
                   that are provably robust against norm-bounded adversarial
                   perturbations (on the training data; for previously unseen
                   examples, the approach will be guaranteed to detect all
                   adversarial examples, though it may flag some
                   non-adversarial examples as well). The basic idea of the
                   approach is to consider a convex outer approximation of the
                   set of activations reachable through a norm-bounded
                   perturbation, and we develop a robust optimization procedure
                   that minimizes the worst case loss over this outer region
                   (via a linear program). Crucially, we show that the dual
                   problem to this linear program can be represented itself as
                   a deep network similar to the backpropagation network,
                   leading to very efficient optimization approaches that
                   produce guaranteed bounds on the robust loss. The end result
                   is that by executing a few more forward and backward passes
                   through a slightly modified version of the original network
                   (though possibly with much larger batch sizes), we can learn
                   a classifier that is provably robust to any norm-bounded
                   adversarial attack. We illustrate the approach on a toy 2D
                   robust classification task, and on a simple convolutional
                   architecture applied to MNIST, where we produce a classifier
                   that provably has less than 8.4\% test error for any
                   adversarial attack with bounded $\ell_\infty$ norm less than
                   $\epsilon = 0.1$. This represents the largest verified
                   network that we are aware of, and we discuss future
                   challenges in scaling the approach to much larger domains.",
  month         =  nov,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1711.00851"
}



@ARTICLE{Zhang2018-fl,
  title         = "Efficient Neural Network Robustness Certification with
                   General Activation Functions",
  author        = "Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh,
                   Cho-Jui and Daniel, Luca",
  abstract      = "Finding minimum distortion of adversarial examples and thus
                   certifying robustness in neural network classifiers for
                   given data points is known to be a challenging problem.
                   Nevertheless, recently it has been shown to be possible to
                   give a non-trivial certified lower bound of minimum
                   adversarial distortion, and some recent progress has been
                   made towards this direction by exploiting the piece-wise
                   linear nature of ReLU activations. However, a generic
                   robustness certification for general activation functions
                   still remains largely unexplored. To address this issue, in
                   this paper we introduce CROWN, a general framework to
                   certify robustness of neural networks with general
                   activation functions for given input data points. The
                   novelty in our algorithm consists of bounding a given
                   activation function with linear and quadratic functions,
                   hence allowing it to tackle general activation functions
                   including but not limited to four popular choices: ReLU,
                   tanh, sigmoid and arctan. In addition, we facilitate the
                   search for a tighter certified lower bound by adaptively
                   selecting appropriate surrogates for each neuron activation.
                   Experimental results show that CROWN on ReLU networks can
                   notably improve the certified lower bounds compared to the
                   current state-of-the-art algorithm Fast-Lin, while having
                   comparable computational efficiency. Furthermore, CROWN also
                   demonstrates its effectiveness and flexibility on networks
                   with general activation functions, including tanh, sigmoid
                   and arctan.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1811.00866"
}


@ARTICLE{Raghunathan2018-zu,
  title         = "Semidefinite relaxations for certifying robustness to
                   adversarial examples",
  author        = "Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy",
  abstract      = "Despite their impressive performance on diverse tasks,
                   neural networks fail catastrophically in the presence of
                   adversarial inputs---imperceptibly but adversarially
                   perturbed versions of natural inputs. We have witnessed an
                   arms race between defenders who attempt to train robust
                   networks and attackers who try to construct adversarial
                   examples. One promise of ending the arms race is developing
                   certified defenses, ones which are provably robust against
                   all attackers in some family. These certified defenses are
                   based on convex relaxations which construct an upper bound
                   on the worst case loss over all attackers in the family.
                   Previous relaxations are loose on networks that are not
                   trained against the respective relaxation. In this paper, we
                   propose a new semidefinite relaxation for certifying
                   robustness that applies to arbitrary ReLU networks. We show
                   that our proposed relaxation is tighter than previous
                   relaxations and produces meaningful robustness guarantees on
                   three different ``foreign networks'' whose training
                   objectives are agnostic to our proposed relaxation.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1811.01057"
}



@ARTICLE{He2015-ns,
  title         = "Delving Deep into Rectifiers: Surpassing {Human-Level}
                   Performance on {ImageNet} Classification",
  author        = "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
                   Jian",
  abstract      = "Rectified activation units (rectifiers) are essential for
                   state-of-the-art neural networks. In this work, we study
                   rectifier neural networks for image classification from two
                   aspects. First, we propose a Parametric Rectified Linear
                   Unit (PReLU) that generalizes the traditional rectified
                   unit. PReLU improves model fitting with nearly zero extra
                   computational cost and little overfitting risk. Second, we
                   derive a robust initialization method that particularly
                   considers the rectifier nonlinearities. This method enables
                   us to train extremely deep rectified models directly from
                   scratch and to investigate deeper or wider network
                   architectures. Based on our PReLU networks (PReLU-nets), we
                   achieve 4.94\% top-5 test error on the ImageNet 2012
                   classification dataset. This is a 26\% relative improvement
                   over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our
                   knowledge, our result is the first to surpass human-level
                   performance (5.1\%, Russakovsky et al.) on this visual
                   recognition challenge.",
  month         =  feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1502.01852"
}


@ARTICLE{Kingma2014-au,
  title         = "Adam: A Method for Stochastic Optimization",
  author        = "Kingma, Diederik P and Ba, Jimmy",
  abstract      = "We introduce Adam, an algorithm for first-order
                   gradient-based optimization of stochastic objective
                   functions, based on adaptive estimates of lower-order
                   moments. The method is straightforward to implement, is
                   computationally efficient, has little memory requirements,
                   is invariant to diagonal rescaling of the gradients, and is
                   well suited for problems that are large in terms of data
                   and/or parameters. The method is also appropriate for
                   non-stationary objectives and problems with very noisy
                   and/or sparse gradients. The hyper-parameters have intuitive
                   interpretations and typically require little tuning. Some
                   connections to related algorithms, on which Adam was
                   inspired, are discussed. We also analyze the theoretical
                   convergence properties of the algorithm and provide a regret
                   bound on the convergence rate that is comparable to the best
                   known results under the online convex optimization
                   framework. Empirical results demonstrate that Adam works
                   well in practice and compares favorably to other stochastic
                   optimization methods. Finally, we discuss AdaMax, a variant
                   of Adam based on the infinity norm.",
  month         =  dec,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1412.6980"
}

@misc{gurobi,
  author = "Gurobi Optimization, LLC",
  title = "Gurobi Optimizer Reference Manual",
  year = 2020,
  url = "http://www.gurobi.com"
}



@INCOLLECTION{Jordan2019-fv,
  title     = "Provable Certificates for Adversarial Examples: Fitting a Ball
               in the Union of Polytopes",
  booktitle = "Advances in Neural Information Processing Systems 32",
  author    = "Jordan, Matt and Lewis, Justin and Dimakis, Alexandros G",
  editor    = "Wallach, H and Larochelle, H and Beygelzimer, A and
               d Alche-Buc, F and Fox, E and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "14059--14069",
  year      =  2019
}



