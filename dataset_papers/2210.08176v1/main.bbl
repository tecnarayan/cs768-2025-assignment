\begin{thebibliography}{10}

\bibitem{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em ICML}, 2015.

\bibitem{Nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock In {\em ICLR Workshop}, 2015.

\bibitem{RealNVP}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock In {\em ICLR}, 2017.

\bibitem{GLOW}
Durk~P. Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em NeurIPS}, 2018.

\bibitem{i-ResNets}
Jens Behrmann, Will Grathwohl, Ricky T.~Q. Chen, David Duvenaud, and
  J{\"o}rn-Henrik Jacobsen.
\newblock Invertible residual networks.
\newblock In {\em ICML}, 2019.

\bibitem{ResidualFlows}
Ricky T.~Q. Chen, Jens Behrmann, David~K. Duvenaud, and J{\"o}rn-Henrik
  Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock In {\em NeurIPS}, 2019.

\bibitem{i-DenseNets}
Yura Perugachi-Diaz, Jakub Tomczak, and Sandjai Bhulai.
\newblock Invertible densenets with concatenated lipswish.
\newblock In {\em NeurIPS}, 2021.

\bibitem{CPFlow}
Chin-Wei Huang, Ricky T.~Q. Chen, Christos Tsirigotis, and Aaron Courville.
\newblock Convex potential flows: Universal probability distributions with
  optimal transport and convex optimization.
\newblock In {\em ICLR}, 2021.

\bibitem{SRFlow}
Andreas Lugmayr, Martin Danelljan, Luc~Van Gool, and Radu Timofte.
\newblock Srflow: Learning the super-resolution space with normalizing flow.
\newblock In {\em ECCV}, 2020.

\bibitem{SRFlow-DA}
Younghyun Jo, Sejong Yang, and Seon~Joo Kim.
\newblock Srflow-da: Super-resolution using normalizing flow with deep
  convolutional block.
\newblock In {\em CVPR Workshop}, 2021.

\bibitem{adFlow}
Andreas Lugmayr, Martin Danelljan, Fisher Yu, Luc Van~Gool, and Radu Timofte.
\newblock Normalizing flow as a flexible fidelity objective for photo-realistic
  super-resolution.
\newblock In {\em WACV}, 2022.

\bibitem{NoiseFlow}
Abdelrahman Abdelhamed, Marcus Brubaker, and Michael Brown.
\newblock Noise flow: Noise modeling with conditional normalizing flows.
\newblock In {\em ICCV}, 2019.

\bibitem{InvDN}
Yang Liu, Zhenyue Qin, Saeed Anwar, Pan Ji, Dongwoo Kim, Sabrina Cadwell, and
  Tom Gedeon.
\newblock Invertible denoising network: A light solution for real noise
  removal.
\newblock In {\em CVPR}, 2021.

\bibitem{Glow-TTS}
Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh Yoon.
\newblock Glow-tts: A generative flow for text-to-speech via monotonic
  alignment search.
\newblock In {\em NeurIPS}, 2020.

\bibitem{WaveGlow}
Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
\newblock Waveglow: A flow-based generative network for speech synthesis.
\newblock In {\em ICASSP}, 2019.

\bibitem{CS-Flow}
Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt.
\newblock Fully convolutional cross-scale-flows for image-based defect
  detection.
\newblock In {\em WACV}, 2022.

\bibitem{CFLOW-AD}
Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka.
\newblock Cflow-ad: Real-time unsupervised anomaly detection with localization
  via conditional normalizing flows.
\newblock In {\em WACV}, 2022.

\bibitem{albergo2019flow}
M.~S. Albergo, G.~Kanwar, and P.~E. Shanahan.
\newblock Flow-based generative models for markov chain monte carlo in lattice
  field theory.
\newblock {\em Phys. Rev. D}, 100:034515, August 2019.

\bibitem{gao2020event}
Christina Gao, Stefan H\"oche, Joshua Isaacson, Claudius Krause, and Holger
  Schulz.
\newblock Event generation with normalizing flows.
\newblock {\em Phys. Rev. D}, 101:076002, April 2020.

\bibitem{nachman2020anomaly}
Benjamin Nachman and David Shih.
\newblock Anomaly detection with density estimation.
\newblock {\em Phys. Rev. D}, 101:075042, April 2020.

\bibitem{SmoothNF}
Jonas K\"{o}hler, Andreas Kr\"{a}mer, and Frank Noe.
\newblock Smooth normalizing flows.
\newblock In {\em NeurIPS}, 2021.

\bibitem{Kobyzev2021NormalizingFA}
Ivan Kobyzev, Simon J.~D. Prince, and Marcus~A. Brubaker.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}, 43(11):3964--3979,
  2021.

\bibitem{Papamakarios2021NormalizingFF}
George Papamakarios, Eric~T. Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em J. Mach. Learn. Res.}, 22:57:1--57:64, 2021.

\bibitem{ConvExp}
Emiel Hoogeboom, Victor Garcia~Satorras, Jakub Tomczak, and Max Welling.
\newblock The convolution exponential and generalized sylvester flows.
\newblock In {\em NeurIPS}, 2020.

\bibitem{Woodbury}
You Lu and Bert Huang.
\newblock Woodbury transformations for deep generative flows.
\newblock In {\em NeurIPS}, 2020.

\bibitem{ImpFlows}
Cheng Lu, Jianfei Chen, Chongxuan Li, Qiuhao Wang, and Jun Zhu.
\newblock Implicit normalizing flows.
\newblock In {\em ICLR}, 2021.

\bibitem{SpectralNorm}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In {\em ICLR}, 2018.

\bibitem{DEQ}
Shaojie Bai, J.~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock In {\em NeurIPS}, 2019.

\bibitem{minty1962}
George~J. Minty.
\newblock Monotone (nonlinear) operators in hilbert space.
\newblock {\em Duke Math J.}, 29(3):341--346, September 1962.

\bibitem{bauschke2011convex}
Heinz~H. Bauschke and Patrick~L. Combettes.
\newblock {\em Convex analysis and monotone operator theory in Hilbert spaces}.
\newblock Springer, Cham, 2011.

\bibitem{Ryu2016APO}
Ernest~K. Ryu and Stephen Boyd.
\newblock A primer on monotone operator methods.
\newblock {\em Appl. Comput. Math.}, 15(1):3--43, 2016.

\bibitem{Teshima2020CFINN}
Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and
  Masashi Sugiyama.
\newblock Coupling-based invertible neural networks are universal
  diffeomorphism approximators.
\newblock In {\em NeurIPS}, 2020.

\bibitem{ContinuouslyIndexedFlow}
Robert Cornish, Anthony~L. Caterini, George Deligiannidis, and Arnaud Doucet.
\newblock Relaxing bijectivity constraints with continuously indexed
  normalising flows.
\newblock In {\em ICML}, 2020.

\bibitem{FlowPlusPlus}
Jonathan Ho, Xi~Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel.
\newblock Flow++: Improving flow-based generative models with variational
  dequantization nd architecture design.
\newblock In {\em ICML}, 2019.

\bibitem{Neuralsplineflows}
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.
\newblock Neural spline flows.
\newblock In {\em NeurIPS}, 2019.

\bibitem{FFJORD}
Will Grathwohl, Ricky T.~Q. Chen, Jesse Bettencourt, and David Duvenaud.
\newblock Scalable reversible generative models with free-form continuous
  dynamics.
\newblock In {\em ICLR}, 2019.

\bibitem{NAF}
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville.
\newblock Neural autoregressive flows.
\newblock In {\em ICML}, 2018.

\bibitem{IAF}
Durk~P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi~Chen, Ilya Sutskever, and
  Max Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In {\em NeurIPS}, 2016.

\bibitem{MAF}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em NeurIPS}, 2017.

\bibitem{monDEQ}
Ezra Winston and J.~Zico Kolter.
\newblock Monotone operator equilibrium networks.
\newblock In {\em NeurIPS}, 2020.

\bibitem{Pesquet2021LMM}
Jean-Christophe Pesquet, Audrey Repetti, Matthieu Terris, and Yves Wiaux.
\newblock Learning maximally monotone operators for image recovery.
\newblock {\em SIAM J. Imaging Sci.}, 14(3):1206--1237, 2021.

\bibitem{Adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em ICLR}, 2015.

\bibitem{mnist}
Li~Deng.
\newblock The mnist database of handwritten digit images for machine learning
  research.
\newblock {\em IEEE Signal Process. Mag.}, 29(6):141--142, 2012.

\bibitem{cifar-10}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{PixelRNN}
A{\"a}ron van~den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In {\em ICML}, 2016.

\bibitem{VFlow}
Jianfei Chen, Cheng Lu, Biqi Chenli, Jun Zhu, and Tian Tian.
\newblock {VF}low: More expressive generative flows with variational data
  augmentation.
\newblock In {\em ICML}, 2020.

\bibitem{AugNF}
Chin-Wei Huang, Laurent Dinh, and Aaron Courville.
\newblock Augmented normalizing flows: Bridging the gap between generative
  flows and latent variable models.
\newblock {\em arXiv:2002.07101}.

\bibitem{DenselyConnectedNF}
Matej Grci{\'c}, Ivan Grubi{\v{s}}i{\'c}, and Sini{\v{s}}a {\v{S}}egvi{\'c}.
\newblock Densely connected normalizing flows.
\newblock In {\em NeurIPS}, 2021.

\bibitem{ScoreFlow}
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.
\newblock Maximum likelihood training of score-based diffusion models.
\newblock In {\em NeurIPS}, 2021.

\bibitem{Alberti1999AGA}
Giovanni~S. Alberti and Luigi Ambrosio.
\newblock A geometrical approach to monotone functions in {$\mathbb{R}^n$}.
\newblock {\em Math. Z.}, 230:259--316, 1999.

\bibitem{Kirszbraun1934}
M.~D. Kirszbraun.
\newblock \"{U}ber die zusammenziehenden und lipschitzschen transformationen.
\newblock {\em Fund. Math.}, 22:77--108, 1934.

\end{thebibliography}
