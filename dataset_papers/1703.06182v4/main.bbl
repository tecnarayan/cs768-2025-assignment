\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amato et~al.(2009)Amato, Dibangoye, and
  Zilberstein]{amato2009incremental}
Amato, Christopher, Dibangoye, Jilles~Steeve, and Zilberstein, Shlomo.
\newblock Incremental policy generation for finite-horizon {DEC-POMDPs}.
\newblock In \emph{ICAPS}, 2009.

\bibitem[Banerjee et~al.(2012)Banerjee, Lyle, Kraemer, and
  Yellamraju]{banerjee2012sample}
Banerjee, Bikramjit, Lyle, Jeremy, Kraemer, Landon, and Yellamraju, Rajesh.
\newblock Sample bounded distributed reinforcement learning for decentralized
  {POMDPs}.
\newblock In \emph{AAAI}, 2012.

\bibitem[Barbalios \& Tzionas(2014)Barbalios and Tzionas]{barbalios2014robust}
Barbalios, Nikos and Tzionas, Panagiotis.
\newblock A robust approach for multi-agent natural resource allocation based
  on stochastic optimization algorithms.
\newblock \emph{Applied Soft Computing}, 18:\penalty0 12--24, 2014.

\bibitem[Bengio(2012)]{bengio2012practical}
Bengio, Yoshua.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  437--478.
  Springer, 2012.

\bibitem[Bernstein et~al.(2002)Bernstein, Givan, Immerman, and
  Zilberstein]{bernstein2002complexity}
Bernstein, Daniel~S, Givan, Robert, Immerman, Neil, and Zilberstein, Shlomo.
\newblock The complexity of decentralized control of markov decision processes.
\newblock \emph{Mathematics of operations research}, 27\penalty0 (4):\penalty0
  819--840, 2002.

\bibitem[Bowling \& Veloso(2002)Bowling and Veloso]{bowling2002multiagent}
Bowling, Michael and Veloso, Manuela.
\newblock Multiagent learning using a variable learning rate.
\newblock \emph{Artificial Intelligence}, 136\penalty0 (2):\penalty0 215--250,
  2002.

\bibitem[Brunskill \& Li(2013)Brunskill and Li]{brunskill2013sample}
Brunskill, Emma and Li, Lihong.
\newblock Sample complexity of multi-task reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1309.6821}, 2013.

\bibitem[Bu{\c{s}}oniu et~al.(2010)Bu{\c{s}}oniu, Babu{\v{s}}ka, and
  De~Schutter]{bucsoniu2010multi}
Bu{\c{s}}oniu, Lucian, Babu{\v{s}}ka, Robert, and De~Schutter, Bart.
\newblock Multi-agent reinforcement learning: An overview.
\newblock In \emph{Innovations in multi-agent systems and applications-1}, pp.\
   183--221. Springer, 2010.

\bibitem[Caruana(1998)]{caruana1998multitask}
Caruana, Rich.
\newblock Multitask learning.
\newblock In \emph{Learning to learn}, pp.\  95--133. Springer, 1998.

\bibitem[Claus \& Boutilier(1998)Claus and Boutilier]{claus1998dynamics}
Claus, Caroline and Boutilier, Craig.
\newblock The dynamics of reinforcement learning in cooperative multiagent
  systems.
\newblock \emph{AAAI/IAAI}, 1998:\penalty0 746--752, 1998.

\bibitem[Dutech et~al.(2001)Dutech, Buffet, and Charpillet]{Dutech01}
Dutech, Alain, Buffet, Olivier, and Charpillet, Fran\c{c}ois.
\newblock Multi-agent systems by incremental gradient reinforcement learning.
\newblock In \emph{Proc. of the International Joint Conf. on Artificial
  Intelligence}, pp.\  833--838, 2001.

\bibitem[Fern{\'a}ndez \& Veloso(2006)Fern{\'a}ndez and
  Veloso]{fernandez2006probabilistic}
Fern{\'a}ndez, Fernando and Veloso, Manuela.
\newblock Probabilistic policy reuse in a reinforcement learning agent.
\newblock In \emph{Proc. of the fifth international joint conf. on Autonomous
  agents and multiagent sys.}, pp.\  720--727. ACM, 2006.

\bibitem[Foerster et~al.(2017)Foerster, Nardelli, Farquhar, Torr, Kohli,
  Whiteson, et~al.]{foerster2017stabilising}
Foerster, Jakob, Nardelli, Nantas, Farquhar, Gregory, Torr, Philip, Kohli,
  Pushmeet, Whiteson, Shimon, et~al.
\newblock Stabilising experience replay for deep multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1702.08887}, 2017.

\bibitem[Foerster et~al.(2016)Foerster, Assael, de~Freitas, and
  Whiteson]{foerster2016learning}
Foerster, Jakob~N, Assael, Yannis~M, de~Freitas, Nando, and Whiteson, Shimon.
\newblock Learning to communicate to solve riddles with deep distributed
  recurrent {Q}-networks.
\newblock \emph{arXiv preprint arXiv:1602.02672}, 2016.

\bibitem[Fulda \& Ventura(2007)Fulda and Ventura]{fulda2007predicting}
Fulda, Nancy and Ventura, Dan.
\newblock Predicting and preventing coordination problems in cooperative
  {Q}-learning systems.
\newblock In \emph{IJCAI}, volume 2007, pp.\  780--785, 2007.

\bibitem[Gordon(1995)]{gordon1995stable}
Gordon, Geoffrey~J.
\newblock Stable function approximation in dynamic programming.
\newblock In \emph{Proc. of the twelfth international conf. on machine
  learning}, pp.\  261--268, 1995.

\bibitem[Hausknecht \& Stone(2015)Hausknecht and Stone]{hausknecht2015deep}
Hausknecht, Matthew and Stone, Peter.
\newblock Deep recurrent {Q}-learning for partially observable {MDPs}.
\newblock \emph{arXiv preprint arXiv:1507.06527}, 2015.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, Sepp and Schmidhuber, J{\"u}rgen.
\newblock Long short-term memory.
\newblock \emph{Neural comp.}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Kaelbling et~al.(1998)Kaelbling, Littman, and
  Cassandra]{kaelbling1998planning}
Kaelbling, Leslie~Pack, Littman, Michael~L, and Cassandra, Anthony~R.
\newblock Planning and acting in partially observable stochastic domains.
\newblock \emph{Artificial intelligence}, 101\penalty0 (1):\penalty0 99--134,
  1998.

\bibitem[Kapetanakis \& Kudenko(2002)Kapetanakis and
  Kudenko]{kapetanakis2002reinforcement}
Kapetanakis, Spiros and Kudenko, Daniel.
\newblock Reinforcement learning of coordination in cooperative multi-agent
  systems.
\newblock \emph{AAAI/IAAI}, 2002:\penalty0 326--331, 2002.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, Diederik and Ba, Jimmy.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lauer \& Riedmiller(2000)Lauer and Riedmiller]{lauer2000algorithm}
Lauer, Martin and Riedmiller, Martin.
\newblock An algorithm for distributed reinforcement learning in cooperative
  multi-agent systems.
\newblock In \emph{Proc. of the Seventeenth International Conf. on Machine
  Learning}. Citeseer, 2000.

\bibitem[Laurent et~al.(2011)Laurent, Matignon, Fort-Piat,
  et~al.]{laurent2011world}
Laurent, Guillaume~J, Matignon, La{\"e}titia, Fort-Piat, Le, et~al.
\newblock The world of independent learners is not markovian.
\newblock \emph{International Journal of Knowledge-based and Intelligent
  Engineering Systems}, 15\penalty0 (1):\penalty0 55--64, 2011.

\bibitem[Lin(1992)]{lin1992self}
Lin, Long-Ji.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Liu et~al.(2015)Liu, Amato, Liao, Carin, and How]{LiuIJCAI15}
Liu, Miao, Amato, Christopher, Liao, Xuejun, Carin, Lawrence, and How,
  Jonathan~P.
\newblock Stick-breaking policy learning in {Dec-POMDPs}.
\newblock In \emph{Proc. of the International Joint Conf. on Artificial
  Intelligence}, 2015.

\bibitem[Liu et~al.(2016)Liu, Amato, Anesta, Griffith, and How]{LiuAAAI16}
Liu, Miao, Amato, Christopher, Anesta, Emily, Griffith, J.~Daniel, and How,
  Jonathan~P.
\newblock Learning for decentralized control of multiagent systems in large
  partially observable stochastic environments.
\newblock In \emph{AAAI}, 2016.

\bibitem[Matignon et~al.(2007)Matignon, Laurent, and
  Le~Fort-Piat]{matignon2007hysteretic}
Matignon, La{\"e}titia, Laurent, Guillaume~J, and Le~Fort-Piat, Nadine.
\newblock Hysteretic {Q}-learning: an algorithm for decentralized reinforcement
  learning in cooperative multi-agent teams.
\newblock In \emph{IROS}, 2007.

\bibitem[Matignon et~al.(2012)Matignon, Laurent, and
  Le~Fort-Piat]{matignon2012independent}
Matignon, Laetitia, Laurent, Guillaume~J, and Le~Fort-Piat, Nadine.
\newblock Independent reinforcement learners in cooperative markov games: a
  survey regarding coordination problems.
\newblock \emph{The Knowledge Engineering Review}, 27\penalty0 (01):\penalty0
  1--31, 2012.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei~A, Veness,
  Joel, Bellemare, Marc~G, Graves, Alex, Riedmiller, Martin, Fidjeland,
  Andreas~K, Ostrovski, Georg, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Oliehoek \& Amato(2016)Oliehoek and Amato]{DecPOMDPBook16}
Oliehoek, Frans~A. and Amato, Christopher.
\newblock \emph{A Concise Introduction to Decentralized {POMDPs}}.
\newblock Springer, 2016.

\bibitem[Oliehoek et~al.(2008)Oliehoek, Spaan, Vlassis,
  et~al.]{oliehoek2008optimal}
Oliehoek, Frans~A, Spaan, Matthijs~TJ, Vlassis, Nikos~A, et~al.
\newblock Optimal and approximate q-value functions for decentralized {POMDPs}.
\newblock \emph{Journal of Artificial Intelligence Research (JAIR)},
  32:\penalty0 289--353, 2008.

\bibitem[Pan \& Yang(2010)Pan and Yang]{pan2010survey}
Pan, Sinno~Jialin and Yang, Qiang.
\newblock A survey on transfer learning.
\newblock \emph{IEEE Transactions on knowledge and data engineering},
  22\penalty0 (10):\penalty0 1345--1359, 2010.

\bibitem[Peshkin et~al.(2000)Peshkin, Kim, Meuleau, and
  Kaelbling]{peshkin2000learning}
Peshkin, Leonid, Kim, Kee-Eung, Meuleau, Nicolas, and Kaelbling, Leslie~Pack.
\newblock Learning to cooperate via policy search.
\newblock In \emph{Proc. of the Sixteenth conf. on Uncertainty in artificial
  intelligence}, pp.\  489--496. Morgan Kaufmann Publishers Inc., 2000.

\bibitem[Rusu et~al.(2015)Rusu, Colmenarejo, Gulcehre, Desjardins, Kirkpatrick,
  Pascanu, Mnih, Kavukcuoglu, and Hadsell]{rusu2015policy}
Rusu, Andrei~A, Colmenarejo, Sergio~Gomez, Gulcehre, Caglar, Desjardins,
  Guillaume, Kirkpatrick, James, Pascanu, Razvan, Mnih, Volodymyr, Kavukcuoglu,
  Koray, and Hadsell, Raia.
\newblock Policy distillation.
\newblock \emph{arXiv preprint arXiv:1511.06295}, 2015.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, Richard~S and Barto, Andrew~G.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Tan(1993)]{tan1993multi}
Tan, Ming.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In \emph{Proc. of the tenth international conf. on machine learning},
  pp.\  330--337, 1993.

\bibitem[Tanaka \& Yamamura(2003)Tanaka and Yamamura]{tanaka2003multitask}
Tanaka, Fumihide and Yamamura, Masayuki.
\newblock Multitask reinforcement learning on the distribution of mdps.
\newblock In \emph{Computational Intelligence in Robotics and Automation, 2003.
  Proceedings. 2003 IEEE International Symposium on}, volume~3, pp.\
  1108--1113. IEEE, 2003.

\bibitem[Taylor et~al.(2013)Taylor, Dusparic, Galv{\'a}n-L{\'o}pez, Clarke, and
  Cahill]{taylor2013transfer}
Taylor, Adam, Dusparic, Ivana, Galv{\'a}n-L{\'o}pez, Edgar, Clarke,
  Siobh{\'a}n, and Cahill, Vinny.
\newblock Transfer learning in multi-agent systems through parallel transfer.
\newblock In \emph{Workshop on Theoretically Grounded Transfer Learning at the
  30th International Conf. on Machine Learning (Poster)}, volume~28, pp.\ ~28.
  Omnipress, 2013.

\bibitem[Taylor \& Stone(2009)Taylor and Stone]{taylor2009transfer}
Taylor, Matthew~E and Stone, Peter.
\newblock Transfer learning for reinforcement learning domains: A survey.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Jul):\penalty0 1633--1685, 2009.

\bibitem[Torrey \& Shavlik(2009)Torrey and Shavlik]{torrey2009transfer}
Torrey, Lisa and Shavlik, Jude.
\newblock Transfer learning.
\newblock \emph{Handbook of Research on Machine Learning Applications and
  Trends: Algs., Methods, and Techniques}, 1:\penalty0 242, 2009.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, Christopher~JCH and Dayan, Peter.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Wierstra et~al.(2007)Wierstra, Foerster, Peters, and
  Schmidhuber]{wierstra2007solving}
Wierstra, Daan, Foerster, Alexander, Peters, Jan, and Schmidhuber, Juergen.
\newblock Solving deep memory {POMDPs} with recurrent policy gradients.
\newblock In \emph{International Conf. on Artificial Neural Networks}, pp.\
  697--706. Springer, 2007.

\bibitem[Wilson et~al.(2007)Wilson, Fern, Ray, and Tadepalli]{wilson2007multi}
Wilson, Aaron, Fern, Alan, Ray, Soumya, and Tadepalli, Prasad.
\newblock Multi-task reinforcement learning: a hierarchical bayesian approach.
\newblock In \emph{Proc. of the 24th international conf. on Machine learning},
  pp.\  1015--1022. ACM, 2007.

\bibitem[Wilson et~al.(2008)Wilson, Fern, Ray, and
  Tadepalli]{wilson2008learning}
Wilson, Aaron, Fern, Alan, Ray, Soumya, and Tadepalli, Prasad.
\newblock Learning and transferring roles in multi-agent reinforcement.
\newblock In \emph{Proc. AAAI-08 Workshop on Transfer Learning for Complex
  Tasks}, 2008.

\bibitem[Wu et~al.(2012)Wu, Zilberstein, and Chen]{wu2012rollout}
Wu, Feng, Zilberstein, Shlomo, and Chen, Xiaoping.
\newblock Rollout sampling policy iteration for decentralized {POMDPs}.
\newblock \emph{arXiv preprint arXiv:1203.3528}, 2012.

\bibitem[Wu et~al.(2013)Wu, Zilberstein, and Jennings]{Wu13}
Wu, Feng, Zilberstein, Shlomo, and Jennings, Nicholas~R.
\newblock Monte-carlo expectation maximization for decentralized {POMDPs}.
\newblock In \emph{Proc. of the International Joint Conf. on Artificial
  Intelligence}, pp.\  397--403, 2013.

\bibitem[Xu et~al.(2012)Xu, Zhang, Liu, and Ferrese]{xu2012multiagent}
Xu, Yinliang, Zhang, Wei, Liu, Wenxin, and Ferrese, Frank.
\newblock Multiagent-based reinforcement learning for optimal reactive power
  dispatch.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 42\penalty0 (6):\penalty0 1742--1751, 2012.

\end{thebibliography}
