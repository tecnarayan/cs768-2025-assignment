\begin{thebibliography}{10}

\bibitem{TensorFlow}
Mart{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock {\em arXiv preprint arXiv:1603.04467}, 2016.

\bibitem{AN4}
Alex Acero.
\newblock {\em Acoustical and environmental robustness in automatic speech
  recognition}, volume 201.
\newblock Springer Science \& Business Media, 2012.

\bibitem{CNTK}
Amit Agarwal, Eldar Akchurin, Chris Basoglu, Guoguo Chen, Scott Cyphers, Jasha
  Droppo, Adam Eversole, Brian Guenter, Mark Hillebrand, Ryan Hoens, et~al.
\newblock An introduction to computational networks and the computational
  network toolkit.
\newblock Technical report, Tech. Rep. MSR-TR-2014-112, August 2014., 2014.

\bibitem{full}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock {\em arXiv preprint arXiv:1610.02132}, 2016.

\bibitem{AS15}
Yossi Arjevani and Ohad Shamir.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In {\em NIPS}, 2015.

\bibitem{BBL11}
Ron Bekkerman, Mikhail Bilenko, and John Langford.
\newblock {\em Scaling up machine learning: Parallel and distributed
  approaches}.
\newblock Cambridge University Press, 2011.

\bibitem{Bubeck}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(3-4):231--357, 2015.

\bibitem{Adam}
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In {\em OSDI}, October 2014.

\bibitem{CNTK-AlexNet}
Cntk brainscript file for alexnet.
\newblock
  \url{https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Classification/AlexNet/BrainScript}.
\newblock Accessed: 2017-02-24.

\bibitem{Buckwild}
Christopher~M De~Sa, Ce~Zhang, Kunle Olukotun, and Christopher R{\'e}.
\newblock Taming the wild: A unified analysis of hogwild-style algorithms.
\newblock In {\em NIPS}, 2015.

\bibitem{DistBelief}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In {\em NIPS}, 2012.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Computer Vision and Pattern Recognition, 2009. CVPR 2009.
  IEEE Conference on}, pages 248--255. IEEE, 2009.

\bibitem{DCR15}
John~C Duchi, Sorathan Chaturapruek, and Christopher R{\'e}.
\newblock Asynchronous stochastic convex optimization.
\newblock {\em NIPS}, 2015.

\bibitem{Elias75}
Peter Elias.
\newblock Universal codeword sets and representations of the integers.
\newblock {\em IEEE transactions on information theory}, 21(2):194--203, 1975.

\bibitem{GL13}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em {SIAM} Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{IBM}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In {\em ICML}, pages 1737--1746, 2015.

\bibitem{DeepCompression}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock {\em arXiv preprint arXiv:1510.00149}, 2015.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{BNN}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4107--4115, 2016.

\bibitem{FireCaffe}
Forrest~N Iandola, Matthew~W Moskewicz, Khalid Ashraf, and Kurt Keutzer.
\newblock Firecaffe: near-linear acceleration of deep neural network training
  on compute clusters.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2592--2600, 2016.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{SVRG}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em NIPS}, 2013.

\bibitem{konevcny2017stochastic}
Jakub Kone{\v{c}}n{\`y}.
\newblock Stochastic, distributed and federated optimization for machine
  learning.
\newblock {\em arXiv preprint arXiv:1707.01155}, 2017.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{lecun1998mnist}
Yann LeCun, Corinna Cortes, and Christopher~JC Burges.
\newblock The mnist database of handwritten digits, 1998.

\bibitem{ParameterServer}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em OSDI}, 2014.

\bibitem{Rochester}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In {\em NIPS}. 2015.

\bibitem{neelakantan2015adding}
Arvind Neelakantan, Luke Vilnis, Quoc~V Le, Ilya Sutskever, Lukasz Kaiser,
  Karol Kurach, and James Martens.
\newblock Adding gradient noise improves learning for very deep networks.
\newblock {\em arXiv preprint arXiv:1511.06807}, 2015.

\bibitem{code}
Cntk implementation of qsgd.
\newblock \url{https://gitlab.com/demjangrubic/QSGD}.
\newblock Accessed: 2017-11-4.

\bibitem{Hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em NIPS}, 2011.

\bibitem{RM51}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock {\em The Annals of Mathematical Statistics}, pages 400--407, 1951.

\bibitem{DeltaSigma}
Richard Schreier and Gabor~C Temes.
\newblock {\em Understanding delta-sigma data converters}, volume~74.
\newblock IEEE Press, Piscataway, NJ, 2005.

\bibitem{OneBit}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em INTERSPEECH}, 2014.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{Strom}
Nikko Strom.
\newblock Scalable distributed {DNN} training using commodity {GPU} cloud
  computing.
\newblock In {\em INTERSPEECH}, 2015.

\bibitem{suresh2016distributed}
Ananda~Theertha Suresh, Felix~X Yu, H~Brendan McMahan, and Sanjiv Kumar.
\newblock Distributed mean estimation with limited communication.
\newblock {\em arXiv preprint arXiv:1611.00429}, 2016.

\bibitem{Chainer}
Seiya Tokui, Kenta Oono, Shohei Hido, CA~San~Mateo, and Justin Clayton.
\newblock Chainer: a next-generation open source framework for deep learning.
\newblock In {\em Proceedings of workshop on machine learning systems
  (LearningSys)}, 2015.

\bibitem{TL}
John~N Tsitsiklis and Zhi-Quan Luo.
\newblock Communication complexity of convex optimization.
\newblock {\em Journal of Complexity}, 3(3), 1987.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock {\em arXiv preprint arXiv:1705.07878}, 2017.

\bibitem{zhang2017zipml}
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji~Liu, and Ce~Zhang.
\newblock Zipml: Training linear models with end-to-end low precision, and a
  little bit of deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4035--4043, 2017.

\bibitem{ZCL15}
Sixin Zhang, Anna~E Choromanska, and Yann LeCun.
\newblock Deep learning with elastic averaging sgd.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  685--693, 2015.

\bibitem{Zhang13}
Yuchen Zhang, John Duchi, Michael~I Jordan, and Martin~J Wainwright.
\newblock Information-theoretic lower bounds for distributed statistical
  estimation with communication constraints.
\newblock In {\em NIPS}, 2013.

\bibitem{DoReFa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160}, 2016.

\end{thebibliography}
