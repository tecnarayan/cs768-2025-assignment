\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antorán et~al.(2023)Antorán, Padhy, Barbano, Nalisnick, Janz, and Hernández-Lobato]{antorán2023samplingbased}
Antorán, J., Padhy, S., Barbano, R., Nalisnick, E., Janz, D., and Hernández-Lobato, J.~M.
\newblock Sampling-based inference for large linear models, with application to linearised laplace.
\newblock \emph{arXiv preprint arXiv:2210.04994}, 2023.

\bibitem[Archambeau et~al.(2007)Archambeau, Opper, Shen, Cornford, and Shawe-Taylor]{archambeau2007variational}
Archambeau, C., Opper, M., Shen, Y., Cornford, D., and Shawe-Taylor, J.
\newblock Variational inference for diffusion processes.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Arroyo et~al.(2024)Arroyo, Cartea, Moreno-Pino, and Zohren]{arroyo2024deep}
Arroyo, A., Cartea, A., Moreno-Pino, F., and Zohren, S.
\newblock Deep attentive survival analysis in limit order books: Estimating fill probabilities with convolutional-transformers.
\newblock \emph{Quantitative Finance}, pp.\  1--23, 2024.

\bibitem[Bergna et~al.(2023)Bergna, Opolka, Liò, and Hernandez-Lobato]{bergna2023graph}
Bergna, R., Opolka, F., Liò, P., and Hernandez-Lobato, J.~M.
\newblock Graph neural stochastic differential equations.
\newblock \emph{arXiv preprint arXiv:2308.12316}, 2023.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and Wierstra]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine Learning}, volume~37, pp.\  1613--1622, 2015.

\bibitem[Calvo-Ordoñez et~al.(2023)Calvo-Ordoñez, Cheng, Huang, Zhang, Yang, Schonlieb, and Aviles-Rivero]{calvoordonez2023missing}
Calvo-Ordoñez, S., Cheng, C.-W., Huang, J., Zhang, L., Yang, G., Schonlieb, C.-B., and Aviles-Rivero, A.~I.
\newblock The missing u for efficient diffusion models.
\newblock \emph{arXiv preprint arXiv:2310.20092}, 2023.

\bibitem[Cattiaux \& L{\'e}onard(1994)Cattiaux and L{\'e}onard]{cattiaux1994minimization}
Cattiaux, P. and L{\'e}onard, C.
\newblock Minimization of the kullback information of diffusion processes.
\newblock In \emph{Annales de l'IHP Probabilit{\'e}s et statistiques}, volume~30, pp.\  83--132, 1994.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{Neural_ODEs}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~31, 2018.

\bibitem[Daxberger et~al.(2021{\natexlab{a}})Daxberger, Kristiadi, Immer, Eschenhagen, Bauer, and Hennig]{daxberger2022laplace}
Daxberger, E., Kristiadi, A., Immer, A., Eschenhagen, R., Bauer, M., and Hennig, P.
\newblock Laplace redux-effortless bayesian deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~34, pp.\  20089--20103, 2021{\natexlab{a}}.

\bibitem[Daxberger et~al.(2021{\natexlab{b}})Daxberger, Nalisnick, Allingham, Antor{\'a}n, and Hern{\'a}ndez-Lobato]{daxberger2022bayesian}
Daxberger, E., Nalisnick, E., Allingham, J.~U., Antor{\'a}n, J., and Hern{\'a}ndez-Lobato, J.~M.
\newblock Bayesian deep learning via subnetwork inference.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139, pp.\  2510--2521, 2021{\natexlab{b}}.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S.
\newblock Density estimation using real nvp.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
Dupont, E., Doucet, A., and Teh, Y.~W.
\newblock Augmented neural odes.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32, 2019.

\bibitem[Duran-Martin et~al.(2022)Duran-Martin, Kara, and Murphy]{duranmartin2021efficient}
Duran-Martin, G., Kara, A., and Murphy, K.
\newblock Efficient online bayesian inference for neural bandits.
\newblock In \emph{Proceedings of the 25th International Conference on Artificial Intelligence and Statistics}, volume 151, pp.\  6002--6021, 2022.

\bibitem[Farquhar et~al.(2020)Farquhar, Smith, and Gal]{gal_liberty_or_depth}
Farquhar, S., Smith, L., and Gal, Y.
\newblock Liberty or depth: Deep bayesian neural nets do not need complex weight posterior approximations.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  4346--4357, 2020.

\bibitem[Ghosh et~al.(2022)Ghosh, Birrell, and De~Angelis]{ghosh2022differentiable}
Ghosh, S., Birrell, P.~J., and De~Angelis, D.
\newblock Differentiable bayesian inference of sde parameters using a pathwise series expansion of brownian motion.
\newblock \emph{Proceedings of The 25th International Conference on Artificial Intelligence and Statistics}, pp.\  10982--10998, 2022.

\bibitem[Haber \& Ruthotto(2017)Haber and Ruthotto]{Haber_2017}
Haber, E. and Ruthotto, L.
\newblock Stable architectures for deep neural networks.
\newblock \emph{Inverse problems}, 34\penalty0 (1):\penalty0 014004, 2017.

\bibitem[Harrison et~al.(2024)Harrison, Willes, and Snoek]{harrison2024variational}
Harrison, J., Willes, J., and Snoek, J.
\newblock Variational bayesian last layers.
\newblock \emph{arXiv preprint arXiv:2404.11599}, 2024.

\bibitem[He et~al.(2023)He, Flamich, Guo, and Hern{\'a}ndez-Lobato]{he2023recombiner}
He, J., Flamich, G., Guo, Z., and Hern{\'a}ndez-Lobato, J.~M.
\newblock Recombiner: Robust and enhanced compression with bayesian implicit neural representations.
\newblock \emph{arXiv preprint arXiv:2309.17182}, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and Dietterich]{hendrycks2019robustness}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock \emph{Proceedings of the International Conference on Learning Representations}, 2019.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and Paisley]{hoffman13-svi}
Hoffman, M.~D., Blei, D.~M., Wang, C., and Paisley, J.
\newblock Stochastic variational inference.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0 (40):\penalty0 1303--1347, 2013.

\bibitem[Hu et~al.(2020)Hu, Yang, Zhu, and Hong]{hu2020revealing}
Hu, P., Yang, W., Zhu, Y., and Hong, L.
\newblock Revealing hidden dynamics from time-series data by odenet.
\newblock \emph{arXiv preprint arXiv:2005.04849}, 2020.

\bibitem[Immer et~al.(2021)Immer, Korzepa, and Bauer]{immer2021improving}
Immer, A., Korzepa, M., and Bauer, M.
\newblock Improving predictions of bayesian neural nets via local linearization.
\newblock In \emph{Proceedings of The 24th International Conference on Artificial Intelligence and Statistics}, volume 130, pp.\  703--711, 2021.

\bibitem[Kallenberg(2010)]{kallenberg_foundations_2010}
Kallenberg, O.
\newblock \emph{Foundations of {Modern} {Probability}}.
\newblock Probability and its {Applications}. Springer, New York, NY Berlin Heidelberg, 2. ed edition, 2010.
\newblock ISBN 9781441929495.

\bibitem[Kristiadi et~al.(2020)Kristiadi, Hein, and Hennig]{kristiadi2020bayesian}
Kristiadi, A., Hein, M., and Hennig, P.
\newblock Being bayesian, even just a bit, fixes overconfidence in relu networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119, pp.\  5436--5446, 2020.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Lampinen \& Vehtari(2001)Lampinen and Vehtari]{lampinen2001bayesian}
Lampinen, J. and Vehtari, A.
\newblock Bayesian approach for neural networks—review and case studies.
\newblock \emph{Neural Networks}, 14\penalty0 (3):\penalty0 257--274, 2001.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Leshno et~al.(1993)Leshno, Lin, Pinkus, and Schocken]{leshno93}
Leshno, M., Lin, V.~Y., Pinkus, A., and Schocken, S.
\newblock Multilayer feedforward networks with a nonpolynomial activation function can approximate any function.
\newblock \emph{Neural Networks}, 6\penalty0 (6):\penalty0 861--867, 1993.

\bibitem[Li et~al.(2024)Li, Miao, Qiu, and Zhang]{li2024training}
Li, J., Miao, Z., Qiu, Q., and Zhang, R.
\newblock Training bayesian neural networks with sparse subspace variational inference.
\newblock \emph{arXiv preprint arXiv:2402.11025}, 2024.

\bibitem[Li et~al.(2020)Li, Wong, Chen, and Duvenaud]{scalable_gradients_for_SDE}
Li, X., Wong, T.-K.~L., Chen, R. T.~Q., and Duvenaud, D.
\newblock Scalable gradients for stochastic differential equations.
\newblock In \emph{Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics}, volume 108, pp.\  3870--3882, 2020.

\bibitem[Liu et~al.(2019)Liu, Xiao, Si, Cao, Kumar, and Hsieh]{liu2019neural}
Liu, X., Xiao, T., Si, S., Cao, Q., Kumar, S., and Hsieh, C.-J.
\newblock Neural sde: Stabilizing neural ode networks with stochastic noise.
\newblock \emph{arXiv preprint arXiv:1906.02355}, 2019.

\bibitem[Mackay(1992)]{article-mackay}
Mackay, D. J.~C.
\newblock \emph{Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology, 1992.

\bibitem[Moreno-Pino \& Zohren(2022)Moreno-Pino and Zohren]{moreno2022deepvol}
Moreno-Pino, F. and Zohren, S.
\newblock Deepvol: Volatility forecasting from high-frequency data with dilated causal convolutions.
\newblock \emph{arXiv preprint arXiv:2210.04797}, 2022.

\bibitem[Neal(1995)]{Neal1995BayesianLF}
Neal, R.~M.
\newblock \emph{Bayesian Learning For Neural Networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Oksendal(2013)]{oksendal2013stochastic}
Oksendal, B.
\newblock \emph{Stochastic differential equations: an introduction with applications}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Richter et~al.(2024)Richter, Berner, and Liu]{richter2024improved}
Richter, L., Berner, J., and Liu, G.-H.
\newblock Improved sampling via learned diffusions.
\newblock \emph{Proceedings of the International Conference on Learning Representations}, 2024.

\bibitem[Roeder et~al.(2017)Roeder, Wu, and Duvenaud]{roeder2017stl}
Roeder, G., Wu, Y., and Duvenaud, D.~K.
\newblock Sticking the landing: Simple, lower-variance gradient estimators for variational inference.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~30, 2017.

\bibitem[Sharma et~al.(2023)Sharma, Farquhar, Nalisnick, and Rainforth]{do_BNN_need_fully_stochastic}
Sharma, M., Farquhar, S., Nalisnick, E., and Rainforth, T.
\newblock Do bayesian neural networks need to be fully stochastic?
\newblock In \emph{Proceedings of The 26th International Conference on Artificial Intelligence and Statistics}, volume 206, pp.\  7694--7722, 2023.

\bibitem[Titterington(2004)]{titterington2004bayesian}
Titterington, D.~M.
\newblock Bayesian methods for neural networks and related models.
\newblock \emph{Statistical Science}, 19\penalty0 (1):\penalty0 128--139, February 2004.

\bibitem[Tzen \& Raginsky(2019)Tzen and Raginsky]{tzen2019neural}
Tzen, B. and Raginsky, M.
\newblock Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit.
\newblock \emph{arXiv preprint arXiv:1905.09883}, 2019.

\bibitem[Ustyuzhaninov et~al.(2020)Ustyuzhaninov, Kazlauskaite, Kaiser, Bodin, Campbell, and Henrik~Ek]{pmlr-v124-ustyuzhaninov20a}
Ustyuzhaninov, I., Kazlauskaite, I., Kaiser, M., Bodin, E., Campbell, N., and Henrik~Ek, C.
\newblock Compositional uncertainty in deep gaussian processes.
\newblock In \emph{Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)}, volume 124, pp.\  480--489, 2020.

\bibitem[Van~Amersfoort et~al.(2020)Van~Amersfoort, Smith, Teh, and Gal]{vanamersfoort2020uncertainty}
Van~Amersfoort, J., Smith, L., Teh, Y.~W., and Gal, Y.
\newblock Uncertainty estimation using a single deep deterministic neural network.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119, pp.\  9690--9700, 2020.

\bibitem[Vanderschueren et~al.(2023)Vanderschueren, Curth, Verbeke, and van~der Schaar]{vanderschueren2023accounting}
Vanderschueren, T., Curth, A., Verbeke, W., and van~der Schaar, M.
\newblock Accounting for informative sampling when learning to forecast treatment outcomes over time.
\newblock \emph{arXiv preprint arXiv:2306.04255}, 2023.

\bibitem[Vargas et~al.(2023)Vargas, Ovsianas, Fernandes, Girolami, Lawrence, and N{\"u}sken]{vargas2023bayesian}
Vargas, F., Ovsianas, A., Fernandes, D., Girolami, M., Lawrence, N.~D., and N{\"u}sken, N.
\newblock Bayesian learning via neural schr{\"o}dinger--f{\"o}llmer flows.
\newblock \emph{Statistics and Computing}, 33\penalty0 (1):\penalty0 3, 2023.

\bibitem[Vargas et~al.(2024)Vargas, Padhy, Blessing, and Nüsken]{vargas2024transport}
Vargas, F., Padhy, S., Blessing, D., and Nüsken, N.
\newblock Transport meets variational inference: Controlled monte carlo diffusions.
\newblock \emph{Proceedings of the International Conference on Learning Representations}, 2024.

\bibitem[Xu et~al.(2022)Xu, Chen, Li, and Duvenaud]{xu2022infinitely}
Xu, W., Chen, R.~T., Li, X., and Duvenaud, D.
\newblock Infinitely deep bayesian neural networks with stochastic differential equations.
\newblock In \emph{Proceedings of The 25th International Conference on Artificial Intelligence and Statistics}, volume 151, pp.\  721--738, 2022.

\bibitem[Yarotsky(2017)]{yarotsky17}
Yarotsky, D.
\newblock Error bounds for approximations with deep relu networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.
\newblock ISSN 0893-6080.

\bibitem[Zhang et~al.(2020)Zhang, Gao, Unterman, and Arodz]{zhang20h}
Zhang, H., Gao, X., Unterman, J., and Arodz, T.
\newblock Approximation capabilities of neural {ODE}s and invertible residual networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119, pp.\  11086--11095, 2020.

\end{thebibliography}
