\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[Abdar et~al.(2021)Abdar, Pourpanah, Hussain, Rezazadegan, Liu,
  Ghavamzadeh, Fieguth, Cao, Khosravi, Acharya, et~al.]{abdar2021review}
Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li~Liu,
  Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U~Rajendra
  Acharya, et~al.
\newblock A review of uncertainty quantification in deep learning: Techniques,
  applications and challenges.
\newblock \emph{Information Fusion}, 76:\penalty0 243--297, 2021.

\bibitem[Agrawal et~al.(2019)Agrawal, Amos, Barratt, Boyd, Diamond, and
  Kolter]{agrawal2019differentiable}
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and
  J~Zico Kolter.
\newblock Differentiable convex optimization layers.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 9562--9574, 2019.

\bibitem[Amizadeh et~al.(2018)Amizadeh, Matusevych, and
  Weimer]{amizadeh2018learning}
Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer.
\newblock Learning to solve circuit-sat: An unsupervised differentiable
  approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Amos \& Kolter(2017)Amos and Kolter]{amos2017optnet}
Brandon Amos and J~Zico Kolter.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  136--145. PMLR, 2017.

\bibitem[Arakelyan et~al.(2020)Arakelyan, Daza, Minervini, and
  Cochez]{arakelyan2020complex}
Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez.
\newblock Complex query answering with neural link predictors.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Ardila et~al.(2010)Ardila, Benedetti, and Doker]{ardila2010matroid}
Federico Ardila, Carolina Benedetti, and Jeffrey Doker.
\newblock Matroid polytopes and their volumes.
\newblock \emph{Discrete \& Computational Geometry}, 43\penalty0 (4):\penalty0
  841--854, 2010.

\bibitem[Bach(2019)]{bach2019submodular}
Francis Bach.
\newblock Submodular functions: from discrete to continuous domains.
\newblock \emph{Mathematical Programming}, 175\penalty0 (1):\penalty0 419--459,
  2019.

\bibitem[Bastien et~al.(2012)Bastien, Lamblin, Pascanu, Bergstra, Goodfellow,
  Bergeron, Bouchard, Warde-Farley, and Bengio]{bastien2012theano}
Fr{\'e}d{\'e}ric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian
  Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, and Yoshua
  Bengio.
\newblock Theano: new features and speed improvements.
\newblock \emph{arXiv preprint arXiv:1211.5590}, 2012.

\bibitem[Battaglia et~al.(2018)Battaglia, Hamrick, Bapst, Sanchez-Gonzalez,
  Zambaldi, Malinowski, Tacchetti, Raposo, Santoro, Faulkner,
  et~al.]{battaglia2018relational}
Peter~W Battaglia, Jessica~B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez,
  Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam
  Santoro, Ryan Faulkner, et~al.
\newblock Relational inductive biases, deep learning, and graph networks.
\newblock \emph{arXiv preprint arXiv:1806.01261}, 2018.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Bello et~al.(2016)Bello, Pham, Le, Norouzi, and
  Bengio]{bello2016neural}
Irwan Bello, Hieu Pham, Quoc~V Le, Mohammad Norouzi, and Samy Bengio.
\newblock Neural combinatorial optimization with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.09940}, 2016.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Bengio et~al.(2021)Bengio, Lodi, and Prouvost]{bengio2021machine}
Yoshua Bengio, Andrea Lodi, and Antoine Prouvost.
\newblock Machine learning for combinatorial optimization: a methodological
  tour d’horizon.
\newblock \emph{European Journal of Operational Research}, 290\penalty0
  (2):\penalty0 405--421, 2021.

\bibitem[Bilmes(2022)]{bilmes2022submodularity}
Jeff Bilmes.
\newblock Submodularity in machine learning and artificial intelligence.
\newblock \emph{arXiv preprint arXiv:2202.00132}, 2022.

\bibitem[Calinescu et~al.(2011)Calinescu, Chekuri, P\'{a}l, and
  Vondr\'{a}k]{caliChVon11}
G.~Calinescu, C.~Chekuri, M.~P\'{a}l, and J.~Vondr\'{a}k.
\newblock Maximizing a submodular set function subject to a matroid constraint.
\newblock \emph{SIAM J. Computing}, 40\penalty0 (6), 2011.

\bibitem[Cappart et~al.(2021{\natexlab{a}})Cappart, Ch{\'e}telat, Khalil, Lodi,
  Morris, and Veli{\v{c}}kovi{\'c}]{cappart2021combinatorial}
Quentin Cappart, Didier Ch{\'e}telat, Elias Khalil, Andrea Lodi, Christopher
  Morris, and Petar Veli{\v{c}}kovi{\'c}.
\newblock Combinatorial optimization and reasoning with graph neural networks.
\newblock \emph{arXiv preprint arXiv:2102.09544}, 2021{\natexlab{a}}.

\bibitem[Cappart et~al.(2021{\natexlab{b}})Cappart, Chételat, Khalil, Lodi,
  Morris, and Veličković]{cappart}
Quentin Cappart, Didier Chételat, Elias~B. Khalil, Andrea Lodi, Christopher
  Morris, and Petar Veličković.
\newblock Combinatorial optimization and reasoning with graph neural networks.
\newblock In Zhi-Hua Zhou (ed.), \emph{Proceedings of the Thirtieth
  International Joint Conference on Artificial Intelligence, {IJCAI-21}}, pp.\
  4348--4355. International Joint Conferences on Artificial Intelligence
  Organization, 8 2021{\natexlab{b}}.
\newblock \doi{10.24963/ijcai.2021/595}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2021/595}.
\newblock Survey Track.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Cheng et~al.(2020)Cheng, Yan, and Boots]{cheng2020trajectory}
Ching-An Cheng, Xinyan Yan, and Byron Boots.
\newblock Trajectory-wise control variates for variance reduction in policy
  gradient methods.
\newblock In \emph{Conference on Robot Learning}, pp.\  1379--1394. PMLR, 2020.

\bibitem[Choquet(1954)]{choquet1954theory}
Gustave Choquet.
\newblock Theory of capacities.
\newblock In \emph{Annales de l'institut Fourier}, volume~5, pp.\  131--295,
  1954.

\bibitem[Deac et~al.(2021)Deac, Veli\v{c}kovi\'{c}, Milinkovic, Bacon, Tang,
  and Nikolic]{deacplanners2021}
Andreea-Ioana Deac, Petar Veli\v{c}kovi\'{c}, Ognjen Milinkovic, Pierre-Luc
  Bacon, Jian Tang, and Mladen Nikolic.
\newblock Neural algorithmic reasoners are implicit planners.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  15529--15542. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/82e9e7a12665240d13d0b928be28f230-Paper.pdf}.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Dudzik \& Veli{\v{c}}kovi{\'c}(2022)Dudzik and
  Veli{\v{c}}kovi{\'c}]{dudzik2022graph}
Andrew Dudzik and Petar Veli{\v{c}}kovi{\'c}.
\newblock Graph neural networks are dynamic programmers.
\newblock \emph{arXiv preprint arXiv:2203.15544}, 2022.

\bibitem[Edmonds(2003)]{edmonds2003submodular}
Jack Edmonds.
\newblock Submodular functions, matroids, and certain polyhedra.
\newblock In \emph{Combinatorial Optimization—Eureka, You Shrink!}, pp.\
  11--26. Springer, 2003.

\bibitem[El~Halabi(2018)]{el2018learning}
Marwa El~Halabi.
\newblock Learning with structured sparsity: From discrete to convex and back.
\newblock Technical report, EPFL, 2018.

\bibitem[El~Halabi \& Jegelka(2020)El~Halabi and Jegelka]{el2020optimal}
Marwa El~Halabi and Stefanie Jegelka.
\newblock Optimal approximation for unconstrained non-submodular minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3961--3972. PMLR, 2020.

\bibitem[El~Halabi et~al.(2018)El~Halabi, Bach, and
  Cevher]{el2018combinatorial}
Marwa El~Halabi, Francis Bach, and Volkan Cevher.
\newblock Combinatorial penalties: Which structures are preserved by convex
  relaxations?
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1551--1560. PMLR, 2018.

\bibitem[Falk \& Hoffman(1976)Falk and Hoffman]{falk1976successive}
James~E Falk and Karla~R Hoffman.
\newblock A successive underestimation method for concave minimization
  problems.
\newblock \emph{Mathematics of operations research}, 1\penalty0 (3):\penalty0
  251--259, 1976.

\bibitem[Fey \& Lenssen(2019)Fey and Lenssen]{fey2019fast}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with pytorch geometric.
\newblock In \emph{ICLR (Workshop on Representation Learning on Graphs and
  Manifolds)}, volume~7, 2019.

\bibitem[Goemans \& Williamson(1995)Goemans and
  Williamson]{goemans1995improved}
Michel~X Goemans and David~P Williamson.
\newblock Improved approximation algorithms for maximum cut and satisfiability
  problems using semidefinite programming.
\newblock \emph{Journal of the ACM (JACM)}, 42\penalty0 (6):\penalty0
  1115--1145, 1995.

\bibitem[Grathwohl et~al.(2018)Grathwohl, Choi, Wu, Roeder, and
  Duvenaud]{grathwohl2018backpropagation}
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Gr{\"{o}}tschel et~al.(1981)Gr{\"{o}}tschel, Lov{\'{a}}sz, and
  Schrijver]{gls81}
M.~Gr{\"{o}}tschel, L.~Lov{\'{a}}sz, and A.~Schrijver.
\newblock The ellipsoid algorithm and its consequences in combinatorial
  optimization.
\newblock \emph{Combinatorica}, 1:\penalty0 499--513, 1981.

\bibitem[Gu et~al.(2017)Gu, Lillicrap, Ghahramani, Turner, and Levine]{gu2017q}
S~Gu, T~Lillicrap, Z~Ghahramani, RE~Turner, and S~Levine.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock In \emph{5th International Conference on Learning Representations,
  ICLR 2017-Conference Track Proceedings}, 2017.

\bibitem[Guessab(2013)]{guessab2013generalized}
Allal Guessab.
\newblock Generalized barycentric coordinates and approximations of convex
  functions on arbitrary convex polytopes.
\newblock \emph{Computers \& Mathematics with Applications}, 66\penalty0
  (6):\penalty0 1120--1136, 2013.

\bibitem[{Gurobi Optimization, LLC}(2021)]{gurobi}
{Gurobi Optimization, LLC}.
\newblock {Gurobi Optimizer Reference Manual}, 2021.
\newblock URL \url{https://www.gurobi.com}.

\bibitem[Hamilton et~al.(2018)Hamilton, Bajaj, Zitnik, Jurafsky, and
  Leskovec]{hamilton2018embedding}
Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec.
\newblock Embedding logical queries on knowledge graphs.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Hormann(2014)]{hormann2014barycentric}
Kai Hormann.
\newblock Barycentric interpolation.
\newblock In \emph{Approximation Theory XIV: San Antonio 2013}, pp.\  197--218.
  Springer, 2014.

\bibitem[Iguchi et~al.(2015)Iguchi, Mixon, Peterson, and
  Villar]{iguchi2015tightness}
Takayuki Iguchi, Dustin~G Mixon, Jesse Peterson, and Soledad Villar.
\newblock On the tightness of an sdp relaxation of k-means.
\newblock \emph{arXiv preprint arXiv:1505.04778}, 2015.

\bibitem[Iyer et~al.(2014)Iyer, Jegelka, and Bilmes]{iyer2014monotone}
Rishabh Iyer, Stefanie Jegelka, and Jeff Bilmes.
\newblock Monotone closure of relaxed constraints in submodular optimization:
  Connections between minimization and maximization: Extended version.
\newblock In \emph{UAI}, 2014.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2017.

\bibitem[Karalias \& Loukas(2020)Karalias and Loukas]{karalias2020erdos}
Nikolaos Karalias and Andreas Loukas.
\newblock Erdos goes neural: an unsupervised learning framework for
  combinatorial optimization on graphs.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Kennedy \& O'Hagan(2001)Kennedy and O'Hagan]{kennedy2001bayesian}
Marc~C Kennedy and Anthony O'Hagan.
\newblock Bayesian calibration of computer models.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 63\penalty0 (3):\penalty0 425--464, 2001.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kumar(1992)]{kumar1992algorithms}
Vipin Kumar.
\newblock Algorithms for constraint-satisfaction problems: A survey.
\newblock \emph{AI magazine}, 13\penalty0 (1):\penalty0 32--32, 1992.

\bibitem[Li et~al.(2015)Li, Tarlow, Brockschmidt, and Zemel]{li2015gated}
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
\newblock Gated graph sequence neural networks.
\newblock \emph{arXiv preprint arXiv:1511.05493}, 2015.

\bibitem[Li et~al.(2020)Li, Gimeno, Kohli, and Vinyals]{li2020strong}
Yujia Li, Felix Gimeno, Pushmeet Kohli, and Oriol Vinyals.
\newblock Strong generalization and efficiency in neural programs.
\newblock \emph{arXiv preprint arXiv:2007.03629}, 2020.

\bibitem[Liu et~al.(2018)Liu, Feng, Mao, Zhou, Peng, and Liu]{liu2018action}
Hao Liu, Yihao Feng, Yi~Mao, Dengyong Zhou, Jian Peng, and Qiang Liu.
\newblock Action-dependent control variates for policy optimization via stein
  identity.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lov{\'a}sz(1983)]{lovasz1983submodular}
L{\'a}szl{\'o} Lov{\'a}sz.
\newblock Submodular functions and convexity.
\newblock In \emph{Mathematical programming the state of the art}, pp.\
  235--257. Springer, 1983.

\bibitem[Lov{\'a}sz \& Schrijver(1991)Lov{\'a}sz and
  Schrijver]{lovasz1991cones}
L{\'a}szl{\'o} Lov{\'a}sz and Alexander Schrijver.
\newblock Cones of matrices and set-functions and 0--1 optimization.
\newblock \emph{SIAM journal on optimization}, 1\penalty0 (2):\penalty0
  166--190, 1991.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2017concrete}
C~Maddison, A~Mnih, and Y~Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2017.

\bibitem[Marichal(2000)]{marichal2000axiomatic}
J-L Marichal.
\newblock An axiomatic approach of the discrete choquet integral as a tool to
  aggregate interacting criteria.
\newblock \emph{IEEE transactions on fuzzy systems}, 8\penalty0 (6):\penalty0
  800--807, 2000.

\bibitem[Mazyavkina et~al.(2021)Mazyavkina, Sviridov, Ivanov, and
  Burnaev]{mazyavkina2021reinforcement}
Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev.
\newblock Reinforcement learning for combinatorial optimization: A survey.
\newblock \emph{Computers \& Operations Research}, 134:\penalty0 105400, 2021.

\bibitem[Morris et~al.(2020)Morris, Kriege, Bause, Kersting, Mutzel, and
  Neumann]{morris2020tudataset}
Christopher Morris, Nils~M Kriege, Franka Bause, Kristian Kersting, Petra
  Mutzel, and Marion Neumann.
\newblock Tudataset: A collection of benchmark datasets for learning with
  graphs.
\newblock \emph{arXiv preprint arXiv:2007.08663}, 2020.

\bibitem[Murota(1998)]{murota1998discrete}
Kazuo Murota.
\newblock Discrete convex analysis.
\newblock \emph{Mathematical Programming}, 83\penalty0 (1):\penalty0 313--371,
  1998.

\bibitem[Niepert et~al.(2021)Niepert, Minervini, and
  Franceschi]{niepertimplicit}
Mathias Niepert, Pasquale Minervini, and Luca Franceschi.
\newblock Implicit mle: Backpropagating through discrete exponential family
  distributions.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  14567--14579. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/7a430339c10c642c4b2251756fd1b484-Paper.pdf}.

\bibitem[Obozinski \& Bach(2012)Obozinski and Bach]{obozinski2012convex}
Guillaume Obozinski and Francis Bach.
\newblock \emph{Convex Relaxation for Combinatorial Penalties}.
\newblock PhD thesis, INRIA, 2012.

\bibitem[Obozinski \& Bach(2016)Obozinski and Bach]{obozinski2016unified}
Guillaume Obozinski and Francis Bach.
\newblock A unified perspective on convex structured sparsity: Hierarchical,
  symmetric, submodular norms and beyond.
\newblock 2016.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Paulus et~al.(2021)Paulus, Rolinek, Musil, Amos, and
  Martius]{paulus21}
Anselm Paulus, Michal Rolinek, Vit Musil, Brandon Amos, and Georg Martius.
\newblock Comboptnet: Fit the right np-hard problem by learning integer
  programming constraints.
\newblock In Marina Meila and Tong Zhang (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8443--8453. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/paulus21a.html}.

\bibitem[Paulus et~al.(2020{\natexlab{a}})Paulus, Maddison, and
  Krause]{paulus2020rao}
Max~B Paulus, Chris~J Maddison, and Andreas Krause.
\newblock Rao-blackwellizing the straight-through gumbel-softmax gradient
  estimator.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Paulus et~al.(2020{\natexlab{b}})Paulus, Choi, Tarlow, Krause, and
  Maddison]{paulus2020gradient}
Max~Benedikt Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris~J
  Maddison.
\newblock Gradient estimation with stochastic softmax tricks.
\newblock In \emph{NeurIPS 2020}, 2020{\natexlab{b}}.

\bibitem[Pogan{\v{c}}i{\'c} et~al.(2019)Pogan{\v{c}}i{\'c}, Paulus, Musil,
  Martius, and Rolinek]{vlastelica2019differentiation}
Marin~Vlastelica Pogan{\v{c}}i{\'c}, Anselm Paulus, Vit Musil, Georg Martius,
  and Michal Rolinek.
\newblock Differentiation of blackbox combinatorial solvers.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ren et~al.(2019)Ren, Hu, and Leskovec]{ren2019query2box}
Hongyu Ren, Weihua Hu, and Jure Leskovec.
\newblock Query2box: Reasoning over knowledge graphs in vector space using box
  embeddings.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Schrijver et~al.(2003)]{schrijver2003combinatorial}
Alexander Schrijver et~al.
\newblock \emph{Combinatorial optimization: polyhedra and efficiency},
  volume~24.
\newblock Springer, 2003.

\bibitem[Schuetz et~al.(2022)Schuetz, Brubaker, and
  Katzgraber]{schuetz2022combinatorial}
Martin~JA Schuetz, J~Kyle Brubaker, and Helmut~G Katzgraber.
\newblock Combinatorial optimization with physics-inspired graph neural
  networks.
\newblock \emph{Nature Machine Intelligence}, 4\penalty0 (4):\penalty0
  367--377, 2022.

\bibitem[Shawe-Taylor et~al.(2004)Shawe-Taylor, Cristianini,
  et~al.]{shawe2004kernel}
John Shawe-Taylor, Nello Cristianini, et~al.
\newblock \emph{Kernel methods for pattern analysis}.
\newblock Cambridge university press, 2004.

\bibitem[Tawarmalani \& Sahinidis(2002)Tawarmalani and
  Sahinidis]{tawarmalani2002convex}
Mohit Tawarmalani and Nikolaos~V Sahinidis.
\newblock Convex extensions and envelopes of lower semi-continuous functions.
\newblock \emph{Mathematical Programming}, 93\penalty0 (2):\penalty0 247--263,
  2002.

\bibitem[Toenshoff et~al.(2021)Toenshoff, Ritzert, Wolf, and
  Grohe]{toenshoff2021graph}
Jan Toenshoff, Martin Ritzert, Hinrikus Wolf, and Martin Grohe.
\newblock Graph neural networks for maximum constraint satisfaction.
\newblock \emph{Frontiers in artificial intelligence}, 3:\penalty0 98, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2018)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Li{\`o}, and Bengio]{velivckovic2018graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Li{\`o}, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2019)Veli{\v{c}}kovi{\'c}, Ying, Padovano,
  Hadsell, and Blundell]{velivckovic2019neural}
Petar Veli{\v{c}}kovi{\'c}, Rex Ying, Matilde Padovano, Raia Hadsell, and
  Charles Blundell.
\newblock Neural execution of graph algorithms.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Veličković \& Blundell(2021)Veličković and
  Blundell]{VELICKOVIC2021100273}
Petar Veličković and Charles Blundell.
\newblock Neural algorithmic reasoning.
\newblock \emph{Patterns}, 2\penalty0 (7):\penalty0 100273, 2021.
\newblock ISSN 2666-3899.

\bibitem[Vondr\'ak(2008)]{vondrak08}
J.~Vondr\'ak.
\newblock Optimal approximation for the submodular welfare problem in the value
  oracle model.
\newblock In \emph{Symposium on Theory of Computing (STOC)}, 2008.

\bibitem[Wang et~al.(2019)Wang, Donti, Wilder, and Kolter]{wang2019satnet}
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter.
\newblock Satnet: Bridging deep learning and logical reasoning using a
  differentiable satisfiability solver.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6545--6554. PMLR, 2019.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Wu et~al.(2018)Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade, Mordatch,
  and Abbeel]{wu2018variance}
Cathy Wu, Aravind Rajeswaran, Yan Duan, Vikash Kumar, Alexandre~M Bayen, Sham
  Kakade, Igor Mordatch, and Pieter Abbeel.
\newblock Variance reduction for policy gradient with action-dependent
  factorized baselines.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Xu et~al.(2020)Xu, Hui, Fu, and Zhang]{xu2020tilingnn}
Hao Xu, Ka-Hei Hui, Chi-Wing Fu, and Hao Zhang.
\newblock Tilingnn: learning to tile with self-supervised graph neural network.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 39\penalty0 (4):\penalty0
  129--1, 2020.

\bibitem[Xu et~al.(2019)Xu, Li, Zhang, Du, Kawarabayashi, and
  Jegelka]{xu2019can}
Keyulu Xu, Jingling Li, Mozhi Zhang, Simon~S Du, Ken-ichi Kawarabayashi, and
  Stefanie Jegelka.
\newblock What can neural networks reason about?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yan et~al.(2020)Yan, Swersky, Koutra, Ranganathan, and
  Hashemi]{yan2020neural}
Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad
  Hashemi.
\newblock Neural execution engines: Learning to execute subroutines.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Yu et~al.(2015)Yu, Wang, and Samworth]{yu2015useful}
Yi~Yu, Tengyao Wang, and Richard~J Samworth.
\newblock A useful variant of the davis--kahan theorem for statisticians.
\newblock \emph{Biometrika}, 102\penalty0 (2):\penalty0 315--323, 2015.

\bibitem[Yujia et~al.(2016)Yujia, Daniel, Marc, Richard,
  et~al.]{yujia2016gated}
Li~Yujia, Tarlow Daniel, Brockschmidt Marc, Zemel Richard, et~al.
\newblock Gated graph sequence neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\end{thebibliography}
