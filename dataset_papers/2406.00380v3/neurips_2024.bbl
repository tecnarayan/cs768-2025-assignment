\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[OpenAI(2023{\natexlab{a}})]{GPT-4}
OpenAI.
\newblock Gpt-4, 2023{\natexlab{a}}.
\newblock \url{https://openai.com/gpt-4}.

\bibitem[Meta(2023{\natexlab{a}})]{LLAMA3}
Meta.
\newblock Llama 3, 2023{\natexlab{a}}.
\newblock \url{https://llama.meta.com/llama3}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Huang, Yu, Zhang, Wu, Cao, Dai, Zhao, Li, Shu, et~al.]{liu2023deid}
Zhengliang Liu, Yue Huang, Xiaowei Yu, Lu~Zhang, Zihao Wu, Chao Cao, Haixing Dai, Lin Zhao, Yiwei Li, Peng Shu, et~al.
\newblock Deid-gpt: Zero-shot medical text de-identification by gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.11032}, 2023{\natexlab{a}}.

\bibitem[Kasneci et~al.(2023)Kasneci, Se{\ss}ler, K{\"u}chemann, Bannert, Dementieva, Fischer, Gasser, Groh, G{\"u}nnemann, H{\"u}llermeier, et~al.]{kasneci2023chatgpt}
Enkelejda Kasneci, Kathrin Se{\ss}ler, Stefan K{\"u}chemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G{\"u}nnemann, Eyke H{\"u}llermeier, et~al.
\newblock Chatgpt for good? on opportunities and challenges of large language models for education.
\newblock \emph{Learning and individual differences}, 103:\penalty0 102274, 2023.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Huang, Wu, Tang, Chen, Bai, He, Wang, Zhou, Li, et~al.]{chen2024gui}
Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, et~al.
\newblock Gui-world: A dataset for gui-oriented multimodal llm-based agents.
\newblock \emph{arXiv preprint arXiv:2406.10819}, 2024{\natexlab{a}}.

\bibitem[Wu et~al.(2024{\natexlab{a}})Wu, Huang, Gao, Chen, Zhang, Wan, Zhou, Zhang, Gao, Xiao, et~al.]{wu2024unigen}
Siyuan Wu, Yue Huang, Chujie Gao, Dongping Chen, Qihui Zhang, Yao Wan, Tianyi Zhou, Xiangliang Zhang, Jianfeng Gao, Chaowei Xiao, et~al.
\newblock Unigen: A unified framework for textual dataset generation using large language models.
\newblock \emph{arXiv preprint arXiv:2406.18966}, 2024{\natexlab{a}}.

\bibitem[Ji et~al.(2024)Ji, Qiu, Chen, Zhang, Lou, Wang, Duan, He, Zhou, Zhang, Zeng, Ng, Dai, Pan, O'Gara, Lei, Xu, Tse, Fu, McAleer, Yang, Wang, Zhu, Guo, and Gao]{ji2024alignmentsurvey}
Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan~Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, and Wen Gao.
\newblock Ai alignment: A comprehensive survey, 2024.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Zhong, Li, Mi, Zeng, Huang, Shang, Jiang, and Liu]{wang2023aligning}
Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu.
\newblock Aligning large language models with human: A survey, 2023{\natexlab{a}}.

\bibitem[Miyai et~al.(2024)Miyai, Yang, Zhang, Ming, Yu, Irie, Li, Li, Liu, and Aizawa]{miyai2024unsolvable}
Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go~Irie, Yixuan Li, Hai Li, Ziwei Liu, and Kiyoharu Aizawa.
\newblock Unsolvable problem detection: Evaluating trustworthiness of vision language models, 2024.

\bibitem[Deng et~al.(2024)Deng, Zhao, Li, Ng, and Chua]{deng2024gotcha}
Yang Deng, Yong Zhao, Moxin Li, See-Kiong Ng, and Tat-Seng Chua.
\newblock Gotcha! don't trick me with unanswerable questions! self-aligning large language models for responding to unknown questions, 2024.

\bibitem[Yin et~al.(2023)Yin, Sun, Guo, Wu, Qiu, and Huang]{yin2023large}
Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang.
\newblock Do large language models know what they don't know?, 2023.

\bibitem[Yang et~al.(2023)Yang, Chern, Qiu, Neubig, and Liu]{yang2023alignment}
Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu.
\newblock Alignment for honesty, 2023.

\bibitem[Rimsky(2024)]{honest_sycophancy}
Nina Rimsky.
\newblock Reducing sycophancy and improving honesty via activation steering, 2024.
\newblock \url{https://www.alignmentforum.org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation}.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones, Joseph, Mann, DasSarma, Elhage, Hatfield-Dodds, Hernandez, Kernion, Ndousse, Olsson, Amodei, Brown, Clark, McCandlish, Olah, and Kaplan]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan.
\newblock A general language assistant as a laboratory for alignment, 2021.

\bibitem[Zheng et~al.(2024{\natexlab{a}})Zheng, Yin, Zhou, Meng, Zhou, Chang, Huang, and Peng]{zheng2024promptdriven}
Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng.
\newblock On prompt-driven safeguarding for large language models, 2024{\natexlab{a}}.

\bibitem[Abdi and Williams(2010)]{abdi2010principal}
Herv{\'e} Abdi and Lynne~J Williams.
\newblock Principal component analysis.
\newblock \emph{Wiley interdisciplinary reviews: computational statistics}, 2\penalty0 (4):\penalty0 433--459, 2010.

\bibitem[Liu et~al.(2024)Liu, Sumers, Dasgupta, and Griffiths]{liu2024large}
Ryan Liu, Theodore~R Sumers, Ishita Dasgupta, and Thomas~L Griffiths.
\newblock How do large language models navigate conflicts between honesty and helpfulness?
\newblock \emph{arXiv preprint arXiv:2402.07282}, 2024.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{curri}
Yoshua Bengio, J\'{e}r\^{o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th Annual International Conference on Machine Learning}, ICML '09, page 41–48, New York, NY, USA, 2009. Association for Computing Machinery.
\newblock ISBN 9781605585161.
\newblock \doi{10.1145/1553374.1553380}.
\newblock URL \url{https://doi.org/10.1145/1553374.1553380}.

\bibitem[Huang et~al.(2023{\natexlab{a}})Huang, Shi, Li, Fan, Wu, Zhang, Liu, Zhou, Wan, Gong, et~al.]{Yue2023metatool}
Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil~Zhenqiang Gong, et~al.
\newblock Metatool benchmark for large language models: Deciding whether to use tools and which to use.
\newblock \emph{arXiv preprint arXiv:2310.03128}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2024)Li, Huang, Lin, Wu, Wan, and Sun]{li2024i}
Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, and Lichao Sun.
\newblock I think, therefore i am: Benchmarking awareness of large language models using awarebench, 2024.

\bibitem[Sharma et~al.(2023)Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston, Kravec, Maxwell, McCandlish, Ndousse, Rausch, Schiefer, Yan, Zhang, and Perez]{sharma2023understanding}
Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel~R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott~R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da~Yan, Miranda Zhang, and Ethan Perez.
\newblock Towards understanding sycophancy in language models, 2023.

\bibitem[Wei et~al.(2024)Wei, Huang, Lu, Zhou, and Le]{wei2024simple}
Jerry Wei, Da~Huang, Yifeng Lu, Denny Zhou, and Quoc~V. Le.
\newblock Simple synthetic data reduces sycophancy in large language models, 2024.

\bibitem[Xu et~al.(2024)Xu, Lin, Yang, Zhang, Shi, Zhang, Fang, Xu, and Qiu]{xu2024earth}
Rongwu Xu, Brian~S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, and Han Qiu.
\newblock The earth is flat because...: Investigating llms' belief towards misinformation via persuasive conversation, 2024.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Yu, Zhang, Xu, Lei, Lai, Gu, Ding, Men, Yang, Zhang, Deng, Zeng, Du, Zhang, Shen, Zhang, Su, Sun, Huang, Dong, and Tang]{liu2023agentbench}
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu~Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu~Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang.
\newblock Agentbench: Evaluating llms as agents, 2023{\natexlab{b}}.

\bibitem[Zhuang et~al.(2024)Zhuang, Yu, Wang, Sun, and Zhang]{zhuang2024toolqa}
Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang.
\newblock Toolqa: A dataset for llm question answering with external tools.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9459--9474, 2020.

\bibitem[Kim et~al.(2024{\natexlab{a}})Kim, Kim, Park, Kim, Park, Yoo, Lee, and Kim]{kim2024aligning}
Hyuhng~Joon Kim, Youna Kim, Cheonbok Park, Junyeob Kim, Choonghyun Park, Kang~Min Yoo, Sang-goo Lee, and Taeuk Kim.
\newblock Aligning language models to explicitly handle ambiguity.
\newblock \emph{arXiv preprint arXiv:2404.11972}, 2024{\natexlab{a}}.

\bibitem[Rissling et~al.(2013)Rissling, Park, Young, Rissling, Sugar, Sprock, Mathias, Pela, Sharp, Braff, et~al.]{rissling2013demand}
Anthony~J Rissling, Sung-Hyouk Park, Jared~W Young, Michelle~B Rissling, Catherine~A Sugar, Joyce Sprock, Daniel~J Mathias, Marlena Pela, Richard~F Sharp, David~L Braff, et~al.
\newblock Demand and modality of directed attention modulate “pre-attentive” sensory processes in schizophrenia patients and nonpsychiatric controls.
\newblock \emph{Schizophrenia research}, 146\penalty0 (1-3):\penalty0 326--335, 2013.

\bibitem[Zhang et~al.(2024)Zhang, Yu, Li, Dong, Su, Chu, and Yu]{zhang2024mm}
Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu.
\newblock Mm-llms: Recent advances in multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2401.13601}, 2024.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}, 2023.

\bibitem[Mahowald et~al.(2024)Mahowald, Ivanova, Blank, Kanwisher, Tenenbaum, and Fedorenko]{mahowald2024dissociating}
Kyle Mahowald, Anna~A Ivanova, Idan~A Blank, Nancy Kanwisher, Joshua~B Tenenbaum, and Evelina Fedorenko.
\newblock Dissociating language and thought in large language models.
\newblock \emph{Trends in Cognitive Sciences}, 2024.

\bibitem[Lurz(2009)]{lurz2009philosophy}
Robert~W Lurz.
\newblock \emph{The philosophy of animal minds}.
\newblock Cambridge University Press, 2009.

\bibitem[Berglund et~al.(2023)Berglund, Stickland, Balesni, Kaufmann, Tong, Korbak, Kokotajlo, and Evans]{berglund2023taken}
Lukas Berglund, Asa~Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, and Owain Evans.
\newblock Taken out of context: On measuring situational awareness in llms.
\newblock \emph{arXiv preprint arXiv:2309.00667}, 2023.

\bibitem[Huang et~al.(2024{\natexlab{a}})Huang, Sun, Wang, Wu, Zhang, Gao, Huang, Lyu, Zhang, Li, et~al.]{sun2024trustllm}
Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et~al.
\newblock Trustllm: Trustworthiness in large language models.
\newblock \emph{arXiv preprint arXiv:2401.05561}, 2024{\natexlab{a}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Dong et~al.(2023)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, Li, and Sui]{dong2023ICLsurvey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, Lei Li, and Zhifang Sui.
\newblock A survey on in-context learning, 2023.

\bibitem[OpenAI(2024{\natexlab{a}})]{text-embedding-ada-002}
OpenAI.
\newblock text-embedding-ada-002, 2024{\natexlab{a}}.
\newblock \url{https://platform.openai.com/docs/guides/embeddings}.

\bibitem[Ling et~al.(2024)Ling, Zhao, Zhang, Cheng, Liu, Sun, Oishi, Osaki, Matsuda, Ji, Bai, Zhao, and Chen]{ling2024uncertainty}
Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, and Haifeng Chen.
\newblock Uncertainty quantification for in-context learning of large language models, 2024.

\bibitem[Xiao et~al.(2022)Xiao, Liang, Bhatt, Neiswanger, Salakhutdinov, and Morency]{xiao2022uncertainty}
Yuxin Xiao, Paul~Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency.
\newblock Uncertainty quantification with pre-trained language models: A large-scale empirical analysis, 2022.

\bibitem[Lyu et~al.(2024)Lyu, Shridhar, Malaviya, Zhang, Elazar, Tandon, Apidianaki, Sachan, and Callison-Burch]{lyu2024calibrating}
Qing Lyu, Kumar Shridhar, Chaitanya Malaviya, Li~Zhang, Yanai Elazar, Niket Tandon, Marianna Apidianaki, Mrinmaya Sachan, and Chris Callison-Burch.
\newblock Calibrating large language models with sample consistency, 2024.

\bibitem[Lin et~al.(2023)Lin, Trivedi, and Sun]{lin2023generating}
Zhen Lin, Shubhendu Trivedi, and Jimeng Sun.
\newblock Generating with confidence: Uncertainty quantification for black-box large language models, 2023.

\bibitem[Xiong et~al.(2024)Xiong, Hu, Lu, Li, Fu, He, and Hooi]{xiong2024llms}
Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi.
\newblock Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms, 2024.

\bibitem[Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan]{sun2023principledriven}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision, 2023.

\bibitem[Lee et~al.(2023)Lee, Phatale, Mansoor, Mesnard, Ferret, Lu, Bishop, Hall, Carbune, Rastogi, and Prakash]{lee2023rlaif}
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.
\newblock Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023.

\bibitem[Petridis et~al.(2023)Petridis, Wedin, Wexler, Donsbach, Pushkarna, Goyal, Cai, and Terry]{petridis2023constitutionmaker}
Savvas Petridis, Ben Wedin, James Wexler, Aaron Donsbach, Mahima Pushkarna, Nitesh Goyal, Carrie~J. Cai, and Michael Terry.
\newblock Constitutionmaker: Interactively critiquing large language models by converting feedback into principles, 2023.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D. Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model, 2023.

\bibitem[OpenAI(2023{\natexlab{b}})]{ChatGPT}
OpenAI.
\newblock Chatgpt, 2023{\natexlab{b}}.
\newblock \url{https://openai.com/product/chatgpt}.

\bibitem[Ope(2024)]{OpenAI}
Openai, 2024.
\newblock \url{https://openai.com/}.

\bibitem[Meta(2023{\natexlab{b}})]{LLAMA2}
Meta.
\newblock Llama 2, 2023{\natexlab{b}}.
\newblock \url{https://llama.meta.com/llama2}.

\bibitem[Meta(2024)]{metaailab}
Meta.
\newblock Ai at meta, 2024.
\newblock \url{https://ai.meta.com}.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts, 2024.

\bibitem[OpenAI(2024{\natexlab{b}})]{MistralAI}
OpenAI.
\newblock Mistral ai, 2024{\natexlab{b}}.
\newblock \url{https://mistral.ai/company/}.

\bibitem[Anthropic(2023)]{Claude}
Anthropic.
\newblock Claude, 2023.
\newblock \url{https://www.anthropic.com/claude}.

\bibitem[Ant(2024)]{Anthropic}
Anthropic, 2024.
\newblock \url{https://www.anthropic.com/}.

\bibitem[Zou et~al.(2023)Zou, Wang, Carlini, Nasr, Kolter, and Fredrikson]{zou2023universal}
Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric~P. Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Lei, Wang, Huang, Feng, Wen, Cheng, Ke, Xu, Tam, et~al.]{liu2023alignbench}
Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng~Lam Tam, et~al.
\newblock Alignbench: Benchmarking chinese alignment of large language models.
\newblock \emph{arXiv preprint arXiv:2311.18743}, 2023{\natexlab{c}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Chen, Zhang, Liu, Wang, Zhou, Zhang, Zhou, Wan, and Sun]{chen2024mllmasajudge}
Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun.
\newblock Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark, 2024{\natexlab{b}}.

\bibitem[Ke et~al.(2023)Ke, Wen, Feng, Liu, Lei, Cheng, Wang, Zeng, Dong, Wang, Tang, and Huang]{ke2023critiquellm}
Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang.
\newblock Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation, 2023.

\bibitem[Kim et~al.(2024{\natexlab{b}})Kim, Suk, Longpre, Lin, Shin, Welleck, Neubig, Lee, Lee, and Seo]{kim2024prometheus}
Seungone Kim, Juyoung Suk, Shayne Longpre, Bill~Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo.
\newblock Prometheus 2: An open source language model specialized in evaluating other language models, 2024{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Zhu et~al.(2018)Zhu, Lu, Zheng, Guo, Zhang, Wang, and Yu]{zhu2018texygen}
Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu.
\newblock Texygen: A benchmarking platform for text generation models, 2018.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Zheng et~al.(2024{\natexlab{b}})Zheng, Zhang, Zhang, Ye, Luo, and Ma]{zheng2024llamafactory}
Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma.
\newblock Llamafactory: Unified efficient fine-tuning of 100+ language models.
\newblock \emph{arXiv preprint arXiv:2403.13372}, 2024{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/2403.13372}.

\bibitem[Ye et~al.(2024)Ye, Wang, Huang, Chen, Zhang, Moniz, Gao, Geyer, Huang, Chen, et~al.]{ye2024justice}
Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et~al.
\newblock Justice or prejudice? quantifying biases in llm-as-a-judge.
\newblock \emph{arXiv preprint arXiv:2410.02736}, 2024.

\bibitem[Evans et~al.(2021)Evans, Cotton-Barratt, Finnveden, Bales, Balwit, Wills, Righetti, and Saunders]{evans2021truthful}
Owain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills, Luca Righetti, and William Saunders.
\newblock Truthful ai: Developing and governing ai that does not lie.
\newblock \emph{arXiv preprint arXiv:2110.06674}, 2021.

\bibitem[Park et~al.(2023)Park, Goldstein, O'Gara, Chen, and Hendrycks]{park2023ai}
Peter~S Park, Simon Goldstein, Aidan O'Gara, Michael Chen, and Dan Hendrycks.
\newblock Ai deception: A survey of examples, risks, and potential solutions.
\newblock \emph{arXiv preprint arXiv:2308.14752}, 2023.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Cheng, Zhao, Nie, and Wen]{li2023halueval}
Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
\newblock Halueval: A large-scale hallucination evaluation benchmark for large language models.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023{\natexlab{a}}.

\bibitem[Qin et~al.(2023)Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, et~al.]{qin2023toolllm}
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et~al.
\newblock Toolllm: Facilitating large language models to master 16000+ real-world apis.
\newblock \emph{arXiv preprint arXiv:2307.16789}, 2023.

\bibitem[Tang et~al.(2023)Tang, Deng, Lin, Han, Liang, and Sun]{Tang_Deng_Lin_Han_Liang_Sun_2023}
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le~Sun.
\newblock Toolalpaca: Generalized tool learning for language models with 3000 simulated cases.
\newblock \emph{arXiv preprint arXiv:2306.05301}, 2023.

\bibitem[Yang et~al.(2024)Yang, Song, Li, Zhao, Ge, Li, and Shan]{yang2024gpt4tools}
Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan.
\newblock Gpt4tools: Teaching large language model to use tools via self-instruction.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Cheng et~al.(2024)Cheng, Sun, Liu, Zhang, Yin, Li, Li, He, Chen, and Qiu]{cheng2024ai}
Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu.
\newblock Can ai assistants know what they don't know?, 2024.

\bibitem[Shen et~al.(2023)Shen, Jin, Huang, Liu, Dong, Guo, Wu, Liu, and Xiong]{shen2023large}
Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong.
\newblock Large language model alignment: A survey, 2023.

\bibitem[Wolf et~al.(2023)Wolf, Wies, Levine, and Shashua]{wolf2023fundamental}
Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock Fundamental limitations of alignment in large language models.
\newblock \emph{arXiv preprint arXiv:2304.11082}, 2023.

\bibitem[Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, et~al.]{zhou2024lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms, 2017.

\bibitem[Cheng et~al.(2023)Cheng, Liu, Zheng, Ke, Wang, Dong, Tang, and Huang]{cheng2023blackbox}
Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang.
\newblock Black-box prompt optimization: Aligning large language models without model training, 2023.

\bibitem[Huang et~al.(2024{\natexlab{b}})Huang, Hu, and Liu]{huang2024vaccine}
Tiansheng Huang, Sihao Hu, and Ling Liu.
\newblock Vaccine: Perturbation-aware alignment for large language model, 2024{\natexlab{b}}.

\bibitem[Lai et~al.(2024)Lai, Wang, Liu, Huang, and Wei]{lai2024alarm}
Yuhang Lai, Siyuan Wang, Shujun Liu, Xuanjing Huang, and Zhongyu Wei.
\newblock Alarm: Align language models via hierarchical rewards modeling, 2024.

\bibitem[Sun et~al.(2024)Sun, Yu, Shen, Liu, Yang, Welleck, and Gan]{sun2024easytohard}
Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan.
\newblock Easy-to-hard generalization: Scalable alignment beyond human supervision, 2024.

\bibitem[Liu et~al.(2023{\natexlab{d}})Liu, Yao, Ton, Zhang, Cheng, Klochkov, Taufiq, and Li]{liu2023trustworthy}
Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo~Hao Cheng, Yegor Klochkov, Muhammad~Faaiz Taufiq, and Hang Li.
\newblock Trustworthy llms: a survey and guideline for evaluating large language models' alignment.
\newblock \emph{arXiv preprint arXiv:2308.05374}, 2023{\natexlab{d}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Chen, Pei, Xie, Kang, Zhang, Xu, Xiong, Dutta, Schaeffer, et~al.]{wang2023decodingtrust}
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et~al.
\newblock Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.
\newblock \emph{arXiv preprint arXiv:2306.11698}, 2023{\natexlab{b}}.

\bibitem[Huang et~al.(2023{\natexlab{b}})Huang, Zhang, Sun, et~al.]{huang2023trustgpt}
Yue Huang, Qihui Zhang, Lichao Sun, et~al.
\newblock Trustgpt: A benchmark for trustworthy and responsible large language models.
\newblock \emph{arXiv preprint arXiv:2306.11507}, 2023{\natexlab{b}}.

\bibitem[Shi et~al.(2024)Shi, Yuan, Liu, Huang, Zhou, Sun, and Gong]{shi2024optimization}
Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil~Zhenqiang Gong.
\newblock Optimization-based prompt injection attack to llm-as-a-judge.
\newblock \emph{arXiv preprint arXiv:2403.17710}, 2024.

\bibitem[Tonmoy et~al.(2024)Tonmoy, Zaman, Jain, Rani, Rawte, Chadha, and Das]{tonmoy2024comprehensive}
S.~M Towhidul~Islam Tonmoy, S~M~Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das.
\newblock A comprehensive survey of hallucination mitigation techniques in large language models, 2024.

\bibitem[Huang et~al.(2023{\natexlab{c}})Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng, Qin, and Liu]{huang2023survey}
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu.
\newblock A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, 2023{\natexlab{c}}.

\bibitem[Huang and Sun(2023)]{huang2023harnessing}
Yue Huang and Lichao Sun.
\newblock Harnessing the power of chatgpt in fake news: An in-depth exploration in generation, detection and explanation.
\newblock \emph{arXiv preprint arXiv:2310.05046}, 2023.

\bibitem[Wei et~al.(2023)Wei, Haghtalab, and Steinhardt]{wei2023jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does llm safety training fail?, 2023.

\bibitem[Huang et~al.(2024{\natexlab{c}})Huang, Tang, Chen, Tang, Wan, Sun, and Zhang]{huang2024obscureprompt}
Yue Huang, Jingyu Tang, Dongping Chen, Bingda Tang, Yao Wan, Lichao Sun, and Xiangliang Zhang.
\newblock Obscureprompt: Jailbreaking large language models via obscure input.
\newblock \emph{arXiv preprint arXiv:2406.13662}, 2024{\natexlab{c}}.

\bibitem[Wu et~al.(2024{\natexlab{b}})Wu, Huang, Liu, Li, Zhou, and Sun]{wu2024can}
Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, and Lichao Sun.
\newblock Can large language models automatically jailbreak gpt-4v?
\newblock \emph{arXiv preprint arXiv:2407.16686}, 2024{\natexlab{b}}.

\bibitem[Zhang et~al.(2023)Zhang, Lei, Wu, Sun, Huang, Long, Liu, Lei, Tang, and Huang]{zhang2023safetybench}
Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang.
\newblock Safetybench: Evaluating the safety of large language models with multiple choice questions, 2023.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Li, Han, Nakov, and Baldwin]{wang2023donotanswer}
Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin.
\newblock Do-not-answer: A dataset for evaluating safeguards in llms, 2023{\natexlab{c}}.

\bibitem[Zhu et~al.(2023)Zhu, Wang, Zhou, Wang, Chen, Wang, Yang, Ye, Zhang, Gong, and Xie]{zhu2023promptbench}
Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil~Zhenqiang Gong, and Xing Xie.
\newblock Promptbench: Towards evaluating the robustness of large language models on adversarial prompts, 2023.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Chen, Luo, Kang, Zhang, Hu, Chan, and Song]{li2023privacy}
Haoran Li, Yulin Chen, Jinglong Luo, Yan Kang, Xiaojin Zhang, Qi~Hu, Chunkit Chan, and Yangqiu Song.
\newblock Privacy in large language models: Attacks, defenses and future directions, 2023{\natexlab{b}}.

\bibitem[Ranaldi and Pucci(2023)]{ranaldi2023large}
Leonardo Ranaldi and Giulia Pucci.
\newblock When large language models contradict humans? large language models' sycophantic behaviour, 2023.

\end{thebibliography}
