@article{diskin2021distributed,
  title={Distributed deep learning in open collaborations},
  author={Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry V and Kashirin, Maxim and Borzunov, Alexander and Villanova del Moral, Albert and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={7879--7897},
  year={2021}
}


@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@misc{baines2021fairscale,
  title={Fairscale: A general purpose modular pytorch library for high performance and large scale training},
  author={Baines, Mandeep and Bhosale, Shruti and Caggiano, Vittorio and Goyal, Naman and Goyal, Siddharth and Ott, Myle and Lefaudeux, Benjamin and Liptchinsky, Vitaliy and Rabbat, Mike and Sheiffer, Sam and others},
  year={2021}
}

@article{zhang2022opt,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{shirts2000screen,
  title={Screen savers of the world unite!},
  author={Shirts, Michael and Pande, Vijay S},
  journal={Science},
  volume={290},
  number={5498},
  pages={1903--1904},
  year={2000},
  publisher={American Association for the Advancement of Science}
}

@article{khan2021transformers,
  title={Transformers in vision: A survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM Computing Surveys (CSUR)},
  year={2021},
  publisher={ACM New York, NY}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{velivckovic2017graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}


@article{edwin2020open,
  title={Open science approaches to COVID-19},
  author={Edwin, G Tse and Klug, Dana M and Todd, Matthew H},
  journal={F1000Research},
  volume={9},
  year={2020},
  publisher={Faculty of 1000 Ltd}
}

@article{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{koloskova2019decentralized,
  title={Decentralized stochastic optimization and gossip algorithms with compressed communication},
  author={Koloskova, Anastasia and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3478--3487},
  year={2019},
  organization={PMLR}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized sgd with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{li2020pytorch,
  title={PyTorch distributed: experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={12},
  pages={3005--3018},
  year={2020},
  publisher={VLDB Endowment}
}

@inproceedings{narayanan2019pipedream,
  title={PipeDream: generalized pipeline parallelism for DNN training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}

@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{chan2007collective,
  title={Collective communication: theory, practice, and experience},
  author={Chan, Ernie and Heimlich, Marcel and Purkayastha, Avi and Van De Geijn, Robert},
  journal={Concurrency and Computation: Practice and Experience},
  volume={19},
  number={13},
  pages={1749--1783},
  year={2007},
  publisher={Wiley Online Library}
}

@misc{nccl,
  title = {NCCL},
  howpublished = {\url{https://developer.nvidia.com/nccl}}
}


@misc{fluidstak,
    title = {FluidStack},
    howpublished = {\url{https://www.fluidstack.io/}}
}

@misc{strongswan,
  title = {StrongSwan VPN},
  howpublished = {\url{https://www.strongswan.org/}}
}


@inproceedings{narayanan2021memory,
  title={Memory-efficient pipeline-parallel dnn training},
  author={Narayanan, Deepak and Phanishayee, Amar and Shi, Kaiyu and Chen, Xie and Zaharia, Matei},
  booktitle={International Conference on Machine Learning},
  pages={7937--7947},
  year={2021},
  organization={PMLR}
}

@inproceedings{ren2021zero,
  title={$\{$ZeRO-Offload$\}$: Democratizing $\{$Billion-Scale$\}$ Model Training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={551--564},
  year={2021}
}


@misc{gpu_increase,
  title={Q4â€™21 sees a nominal rise in GPU and PC shipments quarter-to-quarter},
  howpublished = {\url{https://www.jonpeddie.com/press-releases/q421-sees-a-nominal-rise-in-gpu-and-pc-shipments-quarter-to-quarter}}
}

@article{ryabinin2020towards,
  title={Towards crowdsourced training of large neural networks using decentralized mixture-of-experts},
  author={Ryabinin, Max and Gusev, Anton},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3659--3672},
  year={2020}
}

@misc{ttt2021,
  title={Training Transformers Together},
  howpublished = {\url{https://huggingface.co/training-transformers-together}}
}


@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{li2018pipe,
  title={Pipe-SGD: a decentralized pipelined SGD framework for distributed deep net training},
  author={Li, Youjie and Yu, Mingchao and Li, Songze and Avestimehr, Salman and Kim, Nam Sung and Schwing, Alexander},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={8056--8067},
  year={2018}
}

@inproceedings{lian2018asynchronous,
  title={Asynchronous decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Wei and Zhang, Ce and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={3043--3052},
  year={2018},
  organization={PMLR}
}

@inproceedings{tang2018communication,
  title={Communication compression for decentralized training},
  author={Tang, Hanlin and Gan, Shaoduo and Zhang, Ce and Zhang, Tong and Liu, Ji},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={7663--7673},
  year={2018}
}

@inproceedings{tang2018d,
  title={D2: Decentralized training over decentralized data},
  author={Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={4848--4856},
  year={2018},
  organization={PMLR}
}

@inproceedings{athlur2022varuna,
  title={Varuna: scalable, low-cost training of massive deep learning models},
  author={Athlur, Sanjith and Saran, Nitika and Sivathanu, Muthian and Ramjee, Ramachandran and Kwatra, Nipun},
  booktitle={Proceedings of the Seventeenth European Conference on Computer Systems},
  pages={472--487},
  year={2022}
}

@inproceedings{anderson2004boinc,
  title={Boinc: A system for public-resource computing and storage},
  author={Anderson, David P},
  booktitle={Fifth IEEE/ACM international workshop on grid computing},
  pages={4--10},
  year={2004},
  organization={IEEE}
}


@misc{tc,
  title={Linux Traffic Control},
  howpublished = {\url{https://linux.die.net/man/8/tc}}
}

@article{DBLP:journals/corr/abs-2108-07258,
  author    = {Rishi Bommasani and
               Drew A. Hudson and
               Ehsan Adeli and
               Russ Altman and
               Simran Arora and
               Sydney von Arx and
               Michael S. Bernstein and
               Jeannette Bohg and
               Antoine Bosselut and
               Emma Brunskill and
               Erik Brynjolfsson and
               Shyamal Buch and
               Dallas Card and
               Rodrigo Castellon and
               Niladri S. Chatterji and
               Annie S. Chen and
               Kathleen Creel and
               Jared Quincy Davis and
               Dorottya Demszky and
               Chris Donahue and
               Moussa Doumbouya and
               Esin Durmus and
               Stefano Ermon and
               John Etchemendy and
               Kawin Ethayarajh and
               Li Fei{-}Fei and
               Chelsea Finn and
               Trevor Gale and
               Lauren Gillespie and
               Karan Goel and
               Noah D. Goodman and
               Shelby Grossman and
               Neel Guha and
               Tatsunori Hashimoto and
               Peter Henderson and
               John Hewitt and
               Daniel E. Ho and
               Jenny Hong and
               Kyle Hsu and
               Jing Huang and
               Thomas Icard and
               Saahil Jain and
               Dan Jurafsky and
               Pratyusha Kalluri and
               Siddharth Karamcheti and
               Geoff Keeling and
               Fereshte Khani and
               Omar Khattab and
               Pang Wei Koh and
               Mark S. Krass and
               Ranjay Krishna and
               Rohith Kuditipudi and
               et al.},
  title     = {On the Opportunities and Risks of Foundation Models},
  journal   = {CoRR},
  volume    = {abs/2108.07258},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.07258},
  eprinttype = {arXiv},
  eprint    = {2108.07258},
  timestamp = {Tue, 04 Jan 2022 14:40:20 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-07258.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{tarnawski2021piper,
  title={Piper: Multidimensional Planner for DNN Parallelization},
  author={Tarnawski, Jakub M and Narayanan, Deepak and Phanishayee, Amar},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{fan2021dapple,
  title={DAPPLE: A pipelined data parallel approach for training large models},
  author={Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={431--445},
  year={2021}
}

@inproceedings{park2020hetpipe,
  title={$\{$HetPipe$\}$: Enabling Large $\{$DNN$\}$ Training on (Whimpy) Heterogeneous $\{$GPU$\}$ Clusters through Integration of Pipelined Model Parallelism and Data Parallelism},
  author={Park, Jay H and Yun, Gyeongchan and Chang, M Yi and Nguyen, Nguyen T and Lee, Seungmin and Choi, Jaesik and Noh, Sam H and Choi, Young-ri},
  booktitle={2020 USENIX Annual Technical Conference (USENIX ATC 20)},
  pages={307--321},
  year={2020}
}

@article{tarnawski2020efficient,
  title={Efficient algorithms for device placement of dnn graph operators},
  author={Tarnawski, Jakub M and Phanishayee, Amar and Devanur, Nikhil and Mahajan, Divya and Nina Paravecino, Fanny},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15451--15463},
  year={2020}
}

@misc{iperf,
  title={iPerf - The ultimate speed test tool for TCP, UDP and SCTP},
  howpublished = {\url{https://iperf.fr/}}
}

@misc{vast,
  title={GPU Sharing Economy},
  howpublished = {\url{https://vast.ai/}}
}

@misc{economic_mining,
  title={GPU Economics Cost Analysis},
  howpublished = {\url{https://venturebeat.com/2018/02/25/the-real-cost-of-mining-ethereum/}}
}




@misc{minwpm,
  title={Minimal Weight Perfect Matching Problem},
  howpublished = {\url{https://math.mit.edu/~goemans/18453S17/matching-notes.pdf}}
}

@inproceedings{jiang2020unified,
  title={A Unified Architecture for Accelerating Distributed $\{$DNN$\}$ Training in Heterogeneous $\{$GPU/CPU$\}$ Clusters},
  author={Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
  booktitle={14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages={463--479},
  year={2020}
}

@inproceedings{kang2000hybrid,
  title={A hybrid genetic algorithm for multiway graph partitioning},
  author={Kang, So-Jin and Moon, Byung-Ro},
  booktitle={Proceedings of the 2nd Annual Conference on Genetic and Evolutionary Computation},
  pages={159--166},
  year={2000},
  organization={Citeseer}
}

@article{xu2021gspmd,
  title={GSPMD: general and scalable parallelization for ML computation graphs},
  author={Xu, Yuanzhong and Lee, HyoukJoong and Chen, Dehao and Hechtman, Blake and Huang, Yanping and Joshi, Rahul and Krikun, Maxim and Lepikhin, Dmitry and Ly, Andy and Maggioni, Marcello and others},
  journal={arXiv preprint arXiv:2105.04663},
  year={2021}
}

@article{zheng2022alpa,
  title={Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2201.12023},
  year={2022}
}

@article{karakus2021amazon,
  title={Amazon SageMaker Model Parallelism: A General and Flexible Framework for Large Model Training},
  author={Karakus, Can and Huilgol, Rahul and Wu, Fei and Subramanian, Anirudh and Daniel, Cade and Cavdar, Derya and Xu, Teng and Chen, Haohan and Rahnama, Arash and Quintela, Luis},
  journal={arXiv preprint arXiv:2111.05972},
  year={2021}
}

@article{ryabinin2021swarm,
  title={SWARM Parallelism: Training Large Models Can Be Surprisingly Communication-Efficient},
    author={Max Ryabinin and Tim Dettmers and Michael Diskin and Alexander Borzunov},
    year={2023},
    eprint={2301.11913},
    archivePrefix={arXiv},
    primaryClass={cs.DC}
}

@misc{folding_stats,
  title = {{OS Statistics}},
  howpublished = "\url{https://stats.foldingathome.org/os}",
  year = {2022}, 
  note = "[Online; accessed 15-May-2022]"
}

@article{goemans2009lecture,
  title={Lecture notes on bipartite matching},
  author={Goemans, Michel X},
  journal={Massachusetts Institute of Technology},
  year={2009}
}

@book{garey1979computers,
  title={Computers and intractability},
  author={Garey, Michael R and Johnson, David S},
  volume={174},
  year={1979},
  publisher={freeman San Francisco}
}

@article{el2006hybrid,
  title={Hybrid Genetic Algorithms: A Review.},
  author={El-Mihoub, Tarek A and Hopgood, Adrian A and Nolle, Lars and Battersby, Alan},
  journal={Eng. Lett.},
  volume={13},
  number={2},
  pages={124--137},
  year={2006}
}

@article{glam,
  author    = {Nan Du and
               Yanping Huang and
               Andrew M. Dai and
               Simon Tong and
               Dmitry Lepikhin and
               Yuanzhong Xu and
               Maxim Krikun and
               Yanqi Zhou and
               Adams Wei Yu and
               Orhan Firat and
               Barret Zoph and
               Liam Fedus and
               Maarten Bosma and
               Zongwei Zhou and
               Tao Wang and
               Yu Emma Wang and
               Kellie Webster and
               Marie Pellat and
               Kevin Robinson and
               Kathy Meier{-}Hellstern and
               Toju Duke and
               Lucas Dixon and
               Kun Zhang and
               Quoc V. Le and
               Yonghui Wu and
               Zhifeng Chen and
               Claire Cui},
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  journal   = {CoRR},
  volume    = {abs/2112.06905},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.06905},
  eprinttype = {arXiv},
  eprint    = {2112.06905},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-06905.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{li2021terapipe,
  title={Terapipe: Token-level pipeline parallelism for training large-scale language models},
  author={Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and Zhang, Hao and Song, Dawn and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={6543--6552},
  year={2021},
  organization={PMLR}
}

@article{papadimitriou1977euclidean,
  title={The Euclidean travelling salesman problem is NP-complete},
  author={Papadimitriou, Christos H},
  journal={Theoretical computer science},
  volume={4},
  number={3},
  pages={237--244},
  year={1977},
  publisher={Elsevier}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@article{andreev2006balanced,
  title={Balanced graph partitioning},
  author={Andreev, Konstantin and Racke, Harald},
  journal={Theory of Computing Systems},
  volume={39},
  number={6},
  pages={929--939},
  year={2006},
  publisher={Springer}
}

@inproceedings{sanders2013think,
  title={Think locally, act globally: Highly balanced graph partitioning},
  author={Sanders, Peter and Schulz, Christian},
  booktitle={International Symposium on Experimental Algorithms},
  pages={164--175},
  year={2013},
  organization={Springer}
}

@article{bulucc2016recent,
  title={Recent advances in graph partitioning},
  author={Bulu{\c{c}}, Ayd{\i}n and Meyerhenke, Henning and Safro, Ilya and Sanders, Peter and Schulz, Christian},
  journal={Algorithm engineering},
  pages={117--158},
  year={2016},
  publisher={Springer}
}

@article{bui1996genetic,
  title={Genetic algorithm and graph partitioning},
  author={Bui, Thang Nguyen and Moon, Byung Ro},
  journal={IEEE Transactions on computers},
  volume={45},
  number={7},
  pages={841--855},
  year={1996},
  publisher={IEEE}
}

@article{chasins2022sky,
  title={The Sky Above The Clouds},
  author={Chasins, Sarah and Cheung, Alvin and Crooks, Natacha and Ghodsi, Ali and Goldberg, Ken and Gonzalez, Joseph E and Hellerstein, Joseph M and Jordan, Michael I and Joseph, Anthony D and Mahoney, Michael W and others},
  journal={arXiv preprint arXiv:2205.07147},
  year={2022}
}

@inproceedings{stoica2021cloud,
  title={From cloud computing to sky computing},
  author={Stoica, Ion and Shenker, Scott},
  booktitle={Proceedings of the Workshop on Hot Topics in Operating Systems},
  pages={26--32},
  year={2021}
}


@inproceedings{zhang2021simple,
  title={Simple and Automatic Distributed Machine Learning on Ray},
  author={Zhang, Hao and Li, Zhuohan and Zheng, Lianmin and Stoica, Ion},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={4094--4095},
  year={2021}
}


@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={561--577},
  year={2018}
}

@article{wang2021wavelet,
  title={Wavelet: Efficient DNN Training with Tick-Tock Scheduling},
  author={Wang, Guanhua and Wang, Kehan and Jiang, Kenan and Li, Xiangjun and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={696--710},
  year={2021}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@article{shyalika2020reinforcement,
  title={Reinforcement learning in dynamic task scheduling: A review},
  author={Shyalika, Chathurangi and Silva, Thushari and Karunananda, Asoka},
  journal={SN Computer Science},
  volume={1},
  number={6},
  pages={1--17},
  year={2020},
  publisher={Springer}
}