\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2019{\natexlab{a}})Aljundi, Kelchtermans, and
  Tuytelaars]{aljundi2019task}
Aljundi, R., Kelchtermans, K., and Tuytelaars, T.
\newblock Task-free continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11254--11263, 2019{\natexlab{a}}.

\bibitem[Aljundi et~al.(2019{\natexlab{b}})Aljundi, Lin, Goujaud, and
  Bengio]{aljundi2019gradient}
Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y.
\newblock Gradient based sample selection for online continual learning.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Arani et~al.(2022)Arani, Sarfraz, and
  Zonooz]{DBLP:conf/iclr/AraniSZ22}
Arani, E., Sarfraz, F., and Zonooz, B.
\newblock Learning fast, learning slow: {A} general continual learning method
  based on complementary learning system.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.

\bibitem[Babanezhad~Harikandeh et~al.(2015)Babanezhad~Harikandeh, Ahmed,
  Virani, Schmidt, Kone{\v{c}}n{\`y}, and Sallinen]{babanezhad2015stopwasting}
Babanezhad~Harikandeh, R., Ahmed, M.~O., Virani, A., Schmidt, M.,
  Kone{\v{c}}n{\`y}, J., and Sallinen, S.
\newblock Stopwasting my gradients: Practical svrg.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Bai et~al.(2021)Bai, Wang, Liu, Liu, Song, Sebe, and
  Kim]{bai2021explainable}
Bai, X., Wang, X., Liu, X., Liu, Q., Song, J., Sebe, N., and Kim, B.
\newblock Explainable deep learning for efficient and robust pattern
  recognition: A survey of recent developments.
\newblock \emph{Pattern Recognition}, 120:\penalty0 108102, 2021.

\bibitem[Boschini et~al.(2022)Boschini, Bonicelli, Buzzega, Porrello, and
  Calderara]{boschini2022class}
Boschini, M., Bonicelli, L., Buzzega, P., Porrello, A., and Calderara, S.
\newblock Class-incremental continual learning into the extended der-verse.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 45\penalty0 (5):\penalty0 5497--5512, 2022.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Bottou et~al.(1991)]{bottou1991stochastic}
Bottou, L. et~al.
\newblock Stochastic gradient learning in neural networks.
\newblock \emph{Proceedings of Neuro-N{\i}mes}, 91\penalty0 (8):\penalty0 12,
  1991.

\bibitem[Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and
  Calderara]{buzzega2020dark}
Buzzega, P., Boschini, M., Porrello, A., Abati, D., and Calderara, S.
\newblock Dark experience for general continual learning: a strong, simple
  baseline.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 15920--15930, 2020.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Dokania, Ajanthan, and
  Torr]{chaudhry2018riemannian}
Chaudhry, A., Dokania, P.~K., Ajanthan, T., and Torr, P.~H.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  532--547, 2018.

\bibitem[Chaudhry et~al.(2019{\natexlab{a}})Chaudhry, Ranzato, Rohrbach, and
  Elhoseiny]{DBLP:conf/iclr/ChaudhryRRE19}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with {A-GEM}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net,
  2019{\natexlab{a}}.

\bibitem[Chaudhry et~al.(2019{\natexlab{b}})Chaudhry, Rohrbach, Elhoseiny,
  Ajanthan, Dokania, Torr, and Ranzato]{chaudhry2019tiny}
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.~K., Torr,
  P.~H., and Ranzato, M.
\newblock On tiny episodic memories in continual learning.
\newblock \emph{arXiv preprint arXiv:1902.10486}, 2019{\natexlab{b}}.

\bibitem[Chaudhry et~al.(2021)Chaudhry, Gordo, Dokania, Torr, and
  Lopez-Paz]{chaudhry2021using}
Chaudhry, A., Gordo, A., Dokania, P., Torr, P., and Lopez-Paz, D.
\newblock Using hindsight to anchor past knowledge in continual learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~35, pp.\  6993--7001, 2021.

\bibitem[De~Lange et~al.(2021)De~Lange, Aljundi, Masana, Parisot, Jia,
  Leonardis, Slabaugh, and Tuytelaars]{de2021continual}
De~Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A.,
  Slabaugh, G., and Tuytelaars, T.
\newblock A continual learning survey: Defying forgetting in classification
  tasks.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 44\penalty0 (7):\penalty0 3366--3385, 2021.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Douillard et~al.(2020)Douillard, Cord, Ollion, Robert, and
  Valle]{douillard2020podnet}
Douillard, A., Cord, M., Ollion, C., Robert, T., and Valle, E.
\newblock Podnet: Pooled outputs distillation for small-tasks incremental
  learning.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XX 16}, pp.\  86--102.
  Springer, 2020.

\bibitem[Farajtabar et~al.(2020)Farajtabar, Azizan, Mott, and
  Li]{farajtabar2020orthogonal}
Farajtabar, M., Azizan, N., Mott, A., and Li, A.
\newblock Orthogonal gradient descent for continual learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3762--3773. PMLR, 2020.

\bibitem[Farquhar \& Gal(2018)Farquhar and Gal]{farquhar2018towards}
Farquhar, S. and Gal, Y.
\newblock Towards robust evaluations of continual learning.
\newblock \emph{arXiv preprint arXiv:1805.09733}, 2018.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and
  Sidford]{frostig2015competing}
Frostig, R., Ge, R., Kakade, S.~M., and Sidford, A.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In \emph{Conference on learning theory}, pp.\  728--763. PMLR, 2015.

\bibitem[Gao \& Liu(2023)Gao and Liu]{DBLP:conf/icml/GaoL23a}
Gao, R. and Liu, W.
\newblock {DDGR:} continual learning with deep diffusion-based generative
  replay.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{International Conference on Machine Learning,
  {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10744--10763. {PMLR},
  2023.

\bibitem[Ghunaim et~al.(2023)Ghunaim, Bibi, Alhamoud, Alfarra, Hammoud, Prabhu,
  Torr, and Ghanem]{DBLP:conf/cvpr/GhunaimBAAHPTG23}
Ghunaim, Y., Bibi, A., Alhamoud, K., Alfarra, M., Hammoud, H. A. A.~K., Prabhu,
  A., Torr, P. H.~S., and Ghanem, B.
\newblock Real-time evaluation in online continual learning: {A} new hope.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2023, Vancouver, BC, Canada, June 17-24, 2023}, pp.\
  11888--11897. {IEEE}, 2023.

\bibitem[Goodfellow et~al.(2014{\natexlab{a}})Goodfellow, Pouget-Abadie, Mirza,
  Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27,
  2014{\natexlab{a}}.

\bibitem[Goodfellow et~al.(2014{\natexlab{b}})Goodfellow, Mirza, Da, Courville,
  and Bengio]{DBLP:journals/corr/GoodfellowMDCB13}
Goodfellow, I.~J., Mirza, M., Da, X., Courville, A.~C., and Bengio, Y.
\newblock An empirical investigation of catastrophic forgeting in
  gradient-based neural networks.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014{\natexlab{b}}.

\bibitem[Gower et~al.(2020)Gower, Schmidt, Bach, and
  Richt{\'a}rik]{gower2020variance}
Gower, R.~M., Schmidt, M., Bach, F., and Richt{\'a}rik, P.
\newblock Variance-reduced methods for machine learning.
\newblock \emph{Proceedings of the IEEE}, 108\penalty0 (11):\penalty0
  1968--1983, 2020.

\bibitem[Guo et~al.(2022)Guo, Hu, Zhao, and Liu]{guo2022adaptive}
Guo, Y., Hu, W., Zhao, D., and Liu, B.
\newblock Adaptive orthogonal projection for batch and online continual
  learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  6783--6791, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Hou et~al.(2019)Hou, Pan, Loy, Wang, and Lin]{hou2019learning}
Hou, S., Pan, X., Loy, C.~C., Wang, Z., and Lin, D.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  831--839, 2019.

\bibitem[Hsu et~al.(2018)Hsu, Liu, Ramasamy, and Kira]{hsu2018re}
Hsu, Y.-C., Liu, Y.-C., Ramasamy, A., and Kira, Z.
\newblock Re-evaluating continual learning scenarios: A categorization and case
  for strong baselines.
\newblock \emph{arXiv preprint arXiv:1810.12488}, 2018.

\bibitem[Hu et~al.(2021)Hu, Tang, Miao, Hua, and Zhang]{hu2021distilling}
Hu, X., Tang, K., Miao, C., Hua, X.-S., and Zhang, H.
\newblock Distilling causal effect of data in class-incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on Computer Vision
  and Pattern Recognition}, pp.\  3957--3966, 2021.

\bibitem[Jin et~al.(2019)Jin, Lin, and Zhang]{jin2019towards}
Jin, H., Lin, D., and Zhang, Z.
\newblock Towards better generalization: Bp-svrg in training deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1908.06395}, 2019.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Kim et~al.(2023)Kim, Noci, Orvieto, and
  Hofmann]{DBLP:conf/cvpr/KimNOH23}
Kim, S., Noci, L., Orvieto, A., and Hofmann, T.
\newblock Achieving a better stability-plasticity trade-off via auxiliary
  networks in continual learning.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2023, Vancouver, BC, Canada, June 17-24, 2023}, pp.\
  11930--11939. {IEEE}, 2023.
\newblock \doi{10.1109/CVPR52729.2023.01148}.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{lei2017non}
Lei, L., Ju, C., Chen, J., and Jordan, M.~I.
\newblock Non-convex finite-sum optimization via scsg methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Li \& Hoiem(2017)Li and Hoiem]{li2017learning}
Li, Z. and Hoiem, D.
\newblock Learning without forgetting.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (12):\penalty0 2935--2947, 2017.

\bibitem[Liu \& Liu(2022)Liu and Liu]{DBLP:conf/iclr/LiuL22}
Liu, H. and Liu, H.
\newblock Continual learning with recursive gradient optimization.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  10012--10022, 2021.

\bibitem[Lopez-Paz \& Ranzato(2017)Lopez-Paz and Ranzato]{lopez2017gradient}
Lopez-Paz, D. and Ranzato, M.
\newblock Gradient episodic memory for continual learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Mai et~al.(2022)Mai, Li, Jeong, Quispe, Kim, and
  Sanner]{mai2022online}
Mai, Z., Li, R., Jeong, J., Quispe, D., Kim, H., and Sanner, S.
\newblock Online continual learning in image classification: An empirical
  survey.
\newblock \emph{Neurocomputing}, 469:\penalty0 28--51, 2022.

\bibitem[McCloskey \& Cohen(1989)McCloskey and
  Cohen]{mccloskey1989catastrophic}
McCloskey, M. and Cohen, N.~J.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pp.\
  109--165. Elsevier, 1989.

\bibitem[Mirzadeh et~al.(2020)Mirzadeh, Farajtabar, Gorur, Pascanu, and
  Ghasemzadeh]{mirzadeh2020linear}
Mirzadeh, S.~I., Farajtabar, M., Gorur, D., Pascanu, R., and Ghasemzadeh, H.
\newblock Linear mode connectivity in multitask and continual learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and
  Wermter]{parisi2019continual}
Parisi, G.~I., Kemker, R., Part, J.~L., Kanan, C., and Wermter, S.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural networks}, 113:\penalty0 54--71, 2019.

\bibitem[Prabhu et~al.(2023)Prabhu, Hammoud, Dokania, Torr, Lim, Ghanem, and
  Bibi]{DBLP:conf/cvpr/PrabhuHDTLGB23}
Prabhu, A., Hammoud, H. A. A.~K., Dokania, P.~K., Torr, P. H.~S., Lim, S.,
  Ghanem, B., and Bibi, A.
\newblock Computationally budgeted continual learning: What does matter?
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2023, Vancouver, BC, Canada, June 17-24, 2023}, pp.\
  3698--3707. {IEEE}, 2023.
\newblock \doi{10.1109/CVPR52729.2023.00360}.

\bibitem[Ratcliff(1990)]{ratcliff1990connectionist}
Ratcliff, R.
\newblock Connectionist models of recognition memory: constraints imposed by
  learning and forgetting functions.
\newblock \emph{Psychological review}, 97\penalty0 (2):\penalty0 285, 1990.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and
  Lampert]{rebuffi2017icarl}
Rebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C.~H.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pp.\  2001--2010, 2017.

\bibitem[Riemer et~al.(2019)Riemer, Cases, Ajemian, Liu, Rish, Tu, and
  Tesauro]{DBLP:conf/iclr/RiemerCALRTT19}
Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., and Tesauro, G.
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[Shin et~al.(2017)Shin, Lee, Kim, and Kim]{shin2017continual}
Shin, H., Lee, J.~K., Kim, J., and Kim, J.
\newblock Continual learning with deep generative replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Tiwari et~al.(2022)Tiwari, Killamsetty, Iyer, and
  Shenoy]{tiwari2022gcr}
Tiwari, R., Killamsetty, K., Iyer, R., and Shenoy, P.
\newblock Gcr: Gradient coreset based replay buffer selection for continual
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  99--108, 2022.

\bibitem[Van~de Ven \& Tolias(2019)Van~de Ven and Tolias]{van2019three}
Van~de Ven, G.~M. and Tolias, A.~S.
\newblock Three scenarios for continual learning.
\newblock \emph{arXiv preprint arXiv:1904.07734}, 2019.

\bibitem[Vitter(1985)]{vitter1985random}
Vitter, J.~S.
\newblock Random sampling with a reservoir.
\newblock \emph{ACM Transactions on Mathematical Software (TOMS)}, 11\penalty0
  (1):\penalty0 37--57, 1985.

\bibitem[Wang et~al.(2023)Wang, Zhang, Su, and Zhu]{wang2023comprehensive}
Wang, L., Zhang, X., Su, H., and Zhu, J.
\newblock A comprehensive survey of continual learning: Theory, method and
  application.
\newblock \emph{arXiv preprint arXiv:2302.00487}, 2023.

\bibitem[Wu et~al.(2018)Wu, Herranz, Liu, Van De~Weijer, Raducanu,
  et~al.]{wu2018memory}
Wu, C., Herranz, L., Liu, X., Van De~Weijer, J., Raducanu, B., et~al.
\newblock Memory replay gans: Learning to generate new categories without
  forgetting.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Xu \& Zhu(2018)Xu and Zhu]{xu2018reinforced}
Xu, J. and Zhu, Z.
\newblock Reinforced continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Yan et~al.(2021)Yan, Xie, and He]{yan2021dynamically}
Yan, S., Xie, J., and He, X.
\newblock Der: Dynamically expandable representation for class incremental
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  3014--3023, 2021.

\bibitem[Yu et~al.(2023)Yu, Hu, Hong, Liu, Weller, and
  Liu]{DBLP:journals/tmlr/YuHHLWL23}
Yu, L., Hu, T., Hong, L., Liu, Z., Weller, A., and Liu, W.
\newblock Continual learning by modeling intra-class variation.
\newblock \emph{Trans. Mach. Learn. Res.}, 2023, 2023.

\bibitem[Zhu et~al.(2016)Zhu, Yao, and Bai]{DBLP:journals/fcsc/ZhuYB16}
Zhu, Y., Yao, C., and Bai, X.
\newblock Scene text detection and recognition: recent advances and future
  trends.
\newblock \emph{Frontiers Comput. Sci.}, 10\penalty0 (1):\penalty0 19--36,
  2016.

\end{thebibliography}
