\begin{thebibliography}{10}

\bibitem{hinton2012improving}
Geoffrey~E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em arXiv preprint arXiv:1207.0580}, 2012.

\bibitem{van2013learning}
Laurens van~der Maaten, Minmin Chen, Stephen Tyree, and Kilian~Q Weinberger.
\newblock Learning with marginalized corrupted features.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2013.

\bibitem{wang2013fast}
Sida~I Wang and Christopher~D Manning.
\newblock Fast dropout training.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2013.

\bibitem{abu1990learning}
Yaser~S Abu-Mostafa.
\newblock Learning from hints in neural networks.
\newblock {\em Journal of Complexity}, 6(2):192--198, 1990.

\bibitem{scholkopf1997improving}
Chris~J.C. Burges and Bernhard Sch√∂lkopf.
\newblock Improving the accuracy and speed of support vector machines.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  375--381, 1997.

\bibitem{simard2000transformation}
Patrice~Y Simard, Yann~A Le~Cun, John~S Denker, and Bernard Victorri.
\newblock Transformation invariance in pattern recognition: Tangent distance
  and propagation.
\newblock {\em International Journal of Imaging Systems and Technology},
  11(3):181--197, 2000.

\bibitem{rifai2011manifold}
Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller.
\newblock The manifold tangent classifier.
\newblock {\em Advances in Neural Information Processing Systems},
  24:2294--2302, 2011.

\bibitem{matsuoka1992noise}
Kiyotoshi Matsuoka.
\newblock Noise injection into inputs in back-propagation learning.
\newblock {\em Systems, Man and Cybernetics, IEEE Transactions on},
  22(3):436--440, 1992.

\bibitem{bishop1995training}
Chris~M Bishop.
\newblock Training with noise is equivalent to {T}ikhonov regularization.
\newblock {\em Neural computation}, 7(1):108--116, 1995.

\bibitem{rifai2011adding}
Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent.
\newblock Adding noise to the input of a model trained with a regularized
  objective.
\newblock {\em arXiv preprint arXiv:1104.3250}, 2011.

\bibitem{duchi2010adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12:2121--2159, 2010.

\bibitem{maas2011learning}
Andrew~L Maas, Raymond~E Daly, Peter~T Pham, Dan Huang, Andrew~Y Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for
  Computational Linguistics}, pages 142--150. Association for Computational
  Linguistics, 2011.

\bibitem{wang2013feature}
Sida~I Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher~D
  Manning.
\newblock Feature noising for log-linear structured prediction.
\newblock In {\em Empirical Methods in Natural Language Processing}, 2013.

\bibitem{goodfellow2013maxout}
Ian~J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua
  Bengio.
\newblock Maxout networks.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2013.

\bibitem{wang2012baselines}
Sida Wang and Christopher~D Manning.
\newblock Baselines and bigrams: Simple, good sentiment and topic
  classification.
\newblock In {\em Proceedings of the 50th Annual Meeting of the Association for
  Computational Linguistics}, pages 90--94. Association for Computational
  Linguistics, 2012.

\bibitem{friedman2010regularization}
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock {\em Journal of Statistical Software}, 33(1):1, 2010.

\bibitem{lehmann1998theory}
Erich~Leo Lehmann and George Casella.
\newblock {\em Theory of Point Estimation}.
\newblock Springer, 1998.

\bibitem{crammer2009adaptive}
Koby Crammer, Alex Kulesza, Mark Dredze, et~al.
\newblock Adaptive regularization of weight vectors.
\newblock {\em Advances in Neural Information Processing Systems}, 22:414--422,
  2009.

\bibitem{su2011semi}
Jiang Su, Jelber~Sayyad Shirab, and Stan Matwin.
\newblock Large scale text classification using semi-supervised multinomial
  naive {B}ayes.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, 2011.

\bibitem{nigam2000semi}
Kamal Nigam, Andrew~Kachites McCallum, Sebastian Thrun, and Tom Mitchell.
\newblock Text classification from labeled and unlabeled documents using {EM}.
\newblock {\em Machine Learning}, 39(2-3):103--134, May 2000.

\bibitem{bouchard04tradeoff}
G.~Bouchard and B.~Triggs.
\newblock The trade-off between generative and discriminative classifiers.
\newblock In {\em International Conference on Computational Statistics}, pages
  721--728, 2004.

\bibitem{raina04hybrid}
R.~Raina, Y.~Shen, A.~Ng, and A.~McCallum.
\newblock Classification with hybrid generative/discriminative models.
\newblock In {\em Advances in Neural Information Processing Systems},
  Cambridge, MA, 2004. MIT Press.

\bibitem{suzuki07hybrid}
J.~Suzuki, A.~Fujino, and H.~Isozaki.
\newblock Semi-supervised structured output learning based on a hybrid
  generative and discriminative approach.
\newblock In {\em Empirical Methods in Natural Language Processing and
  Computational Natural Language Learning}, 2007.

\bibitem{grandvalet05entropy}
Y.~Grandvalet and Y.~Bengio.
\newblock Entropy regularization.
\newblock In {\em Semi-Supervised Learning}, United Kingdom, 2005. Springer.

\bibitem{joachims99transductive}
Thorsten Joachims.
\newblock Transductive inference for text classification using support vector
  machines.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning}, pages 200--209, 1999.

\end{thebibliography}
