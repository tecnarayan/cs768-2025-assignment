\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DOSW11}

\bibitem[ABL17]{ABL17}
P.~Awasthi, M.~F. Balcan, and P.~M. Long.
\newblock The power of localization for efficiently learning linear separators
  with noise.
\newblock {\em J. {ACM}}, 63(6):50:1--50:27, 2017.

\bibitem[ABSS97]{ABSS97}
S.~Arora, L.~Babai, J.~Stern, and Z.~Sweedyk.
\newblock The hardness of approximate optima in lattices, codes, and systems of
  linear equations.
\newblock {\em J. Comput. Syst. Sci.}, 54(2):317--331, 1997.

\bibitem[ALM{\etalchar{+}}98]{AroraLMSS98}
S.~Arora, C.~Lund, R.~Motwani, M.~Sudan, and M.~Szegedy.
\newblock Proof verification and the hardness of approximation problems.
\newblock {\em J. {ACM}}, 45(3):501--555, 1998.

\bibitem[AS98]{AroraS98}
S.~Arora and S.~Safra.
\newblock Probabilistic checking of proofs: {A} new characterization of {NP}.
\newblock {\em J. {ACM}}, 45(1):70--122, 1998.

\bibitem[BGLR94]{BGLR94}
M.~Bellare, S.~Goldwasser, C.~Lund, and A.~Russell.
\newblock Efficient probabilistic checkable proofs and applications to
  approximation.
\newblock In {\em Proceedings of the Twenty-Sixth Annual {ACM} Symposium on
  Theory of Computing}, page 820, 1994.

\bibitem[BGS18]{BhattacharyyaGS18}
A.~Bhattacharyya, S.~Ghoshal, and R.~Saket.
\newblock Hardness of learning noisy halfspaces using polynomial thresholds.
\newblock In {\em Conference On Learning Theory, {COLT} 2018}, pages 876--917,
  2018.

\bibitem[BLPR19]{BubeckLPR19}
S.~Bubeck, Y.~T. Lee, E.~Price, and I.~P. Razenshteyn.
\newblock Adversarial examples from computational constraints.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, pages 831--840, 2019.

\bibitem[BM02]{BartlettM02}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3:463--482, 2002.

\bibitem[BS00]{BenDavidS00}
S.~Ben{-}David and H.~Ulrich Simon.
\newblock Efficient learning of linear perceptrons.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)
  2000}, pages 189--195, 2000.

\bibitem[BS12]{BirnbaumS12}
A.~Birnbaum and S.~Shalev{-}Shwartz.
\newblock Learning halfspaces with the zero-one loss: Time-accuracy tradeoffs.
\newblock In {\em Advances in Neural Information Processing Systems 25: NIPS
  2012}, pages 935--943, 2012.

\bibitem[Cho61]{Chow:61}
C.K. Chow.
\newblock {On the characterization of threshold functions}.
\newblock In {\em Proceedings of the Symposium on Switching Circuit Theory and
  Logical Design (FOCS)}, pages 34--38, 1961.

\bibitem[Dan16]{Daniely16}
A.~Daniely.
\newblock Complexity theoretic limitations on learning halfspaces.
\newblock In {\em Proceedings of the 48th Annual Symposium on Theory of
  Computing, {STOC} 2016}, pages 105--117, 2016.

\bibitem[DDFS14]{DeDFS14}
A.~De, I.~Diakonikolas, V.~Feldman, and R.~A. Servedio.
\newblock Nearly optimal solutions for the chow parameters problem and
  low-weight approximation of halfspaces.
\newblock {\em J. {ACM}}, 61(2):11:1--11:36, 2014.

\bibitem[DF95]{DowneyF95}
R.~G. Downey and M.~R. Fellows.
\newblock Fixed-parameter tractability and completeness {II:} on completeness
  for {W[1]}.
\newblock {\em Theor. Comput. Sci.}, 141(1{\&}2):109--131, 1995.

\bibitem[DF13]{DowneyF13}
R.~G. Downey and M.~R. Fellows.
\newblock {\em Fundamentals of Parameterized Complexity}.
\newblock Texts in Computer Science. Springer, 2013.

\bibitem[DHK15]{DHK15}
I.~Dinur, P.~Harsha, and G.~Kindler.
\newblock Polynomially low error {PCP}s with polyloglog n queries via modular
  composition.
\newblock In {\em Proceedings of the Forty-Seventh Annual {ACM} on Symposium on
  Theory of Computing, {STOC} 2015}, pages 267--276, 2015.

\bibitem[Din07]{Dinur07}
I.~Dinur.
\newblock The {PCP} theorem by gap amplification.
\newblock {\em J. {ACM}}, 54(3):12, 2007.

\bibitem[DK19]{DK19-degd}
I.~Diakonikolas and D.~M. Kane.
\newblock Degree-d chow parameters robustly determine degree-d ptfs (and
  algorithmic applications).
\newblock In {\em Proceedings of the 51st ACM Symposium on Theory of Computing,
  {STOC} 2019}, pages 804--815, 2019.

\bibitem[DKK{\etalchar{+}}16]{DKKLMS16}
I.~Diakonikolas, G.~Kamath, D.~M. Kane, J.~Li, A.~Moitra, and A.~Stewart.
\newblock Robust estimators in high dimensions without the computational
  intractability.
\newblock In {\em Proceedings of FOCS'16}, pages 655--664, 2016.

\bibitem[DKK{\etalchar{+}}17]{DKK+17}
I.~Diakonikolas, G.~Kamath, D.~M. Kane, J.~Li, A.~Moitra, and A.~Stewart.
\newblock Being robust (in high dimensions) can be practical.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017}, pages 999--1008, 2017.

\bibitem[DKK{\etalchar{+}}18]{DKKLMS18-soda}
I.~Diakonikolas, G.~Kamath, D.~M. Kane, J.~Li, A.~Moitra, and A.~Stewart.
\newblock Robustly learning a gaussian: Getting optimal error, efficiently.
\newblock In {\em Proceedings of the Twenty-Ninth Annual {ACM-SIAM} Symposium
  on Discrete Algorithms, {SODA} 2018}, pages 2683--2702, 2018.

\bibitem[DKK{\etalchar{+}}19]{DKK+19-sever}
I.~Diakonikolas, G.~Kamath, D.~Kane, J.~Li, J.~Steinhardt, and A.~Stewart.
\newblock Sever: {A} robust meta-algorithm for stochastic optimization.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, pages 1596--1606, 2019.

\bibitem[DKS18]{DKS18a}
I.~Diakonikolas, D.~M. Kane, and A.~Stewart.
\newblock Learning geometric concepts with nasty noise.
\newblock In {\em Proceedings of the 50th Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC} 2018}, pages 1061--1073, 2018.

\bibitem[DKS19]{DKS19}
I.~Diakonikolas, W.~Kong, and A.~Stewart.
\newblock Efficient algorithms and lower bounds for robust linear regression.
\newblock In {\em Proceedings of the Thirtieth Annual {ACM-SIAM} Symposium on
  Discrete Algorithms, {SODA} 2019}, pages 2745--2754, 2019.

\bibitem[DL01]{DL:01}
L.~Devroye and G.~Lugosi.
\newblock {\em Combinatorial methods in density estimation}.
\newblock Springer Series in Statistics, Springer, 2001.

\bibitem[DLS14]{DanielyLS14b}
A.~Daniely, N.~Linial, and S.~Shalev{-}Shwartz.
\newblock The complexity of learning halfspaces using generalized linear
  methods.
\newblock In {\em Proceedings of The 27th Conference on Learning Theory, {COLT}
  2014}, pages 244--286, 2014.

\bibitem[DNV19]{DegwekarNV19}
A.~Degwekar, P.~Nakkiran, and V.~Vaikuntanathan.
\newblock Computational limitations in robust classification and win-win
  results.
\newblock In {\em Conference on Learning Theory, {COLT} 2019}, pages 994--1028,
  2019.

\bibitem[DOSW11]{DOSW:11}
I.~Diakonikolas, R.~O'Donnell, R.~Servedio, and Y.~Wu.
\newblock Hardness results for agnostically learning low-degree polynomial
  threshold functions.
\newblock In {\em SODA}, pages 1590--1606, 2011.

\bibitem[DS09]{DiakonikolasServedio:09}
I.~Diakonikolas and R.~Servedio.
\newblock Improved approximation of linear threshold functions.
\newblock In {\em Proc.\ 24th Annual IEEE Conference on Computational
  Complexity (CCC)}, pages 161--172, 2009.

\bibitem[FGKP06]{FGK+:06short}
V.~Feldman, P.~Gopalan, S.~Khot, and A.~Ponnuswami.
\newblock New results for learning noisy parities and halfspaces.
\newblock In {\em Proc. FOCS}, pages 563--576, 2006.

\bibitem[FS97]{FreundSchapire:97}
Y.~Freund and R.~Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock {\em Journal of Computer and System Sciences}, 55(1):119--139, 1997.

\bibitem[Gol06]{Goldberg:06b}
P.~Goldberg.
\newblock {A Bound on the Precision Required to Estimate a Boolean Perceptron
  from its Average Satisfying Assignment}.
\newblock {\em SIAM Journal on Discrete Mathematics}, 20:328--343, 2006.

\bibitem[GR06]{GR:06}
V.~Guruswami and P.~Raghavendra.
\newblock {Hardness of learning halfspaces with noise}.
\newblock In {\em Proc.\ 47th IEEE Symposium on Foundations of Computer Science
  (FOCS)}, pages 543--552. IEEE Computer Society, 2006.

\bibitem[Hau92]{Haussler:92}
D.~Haussler.
\newblock {Decision theoretic generalizations of the PAC model for neural net
  and other learning applications}.
\newblock {\em Information and Computation}, 100:78--150, 1992.

\bibitem[IP01]{ImpagliazzoP01}
R.~Impagliazzo and R.~Paturi.
\newblock On the complexity of k-sat.
\newblock {\em J. Comput. Syst. Sci.}, 62(2):367--375, 2001.

\bibitem[IPZ01]{ImpagliazzoPZ01}
R.~Impagliazzo, R.~Paturi, and F.~Zane.
\newblock Which problems have strongly exponential complexity?
\newblock {\em J. Comput. Syst. Sci.}, 63(4):512--530, 2001.

\bibitem[JL84]{JohnsonLindenstrauss:84}
W.~Johnson and J.~Lindenstrauss.
\newblock {Extensions of Lipshitz mapping into Hilbert space}.
\newblock {\em Contemporary Mathematics}, 26:189--206, 1984.

\bibitem[KKM18]{KlivansKM18}
A.~R. Klivans, P.~K. Kothari, and R.~Meka.
\newblock Efficient algorithms for outlier-robust regression.
\newblock In {\em Conference On Learning Theory, {COLT} 2018}, pages
  1420--1430, 2018.

\bibitem[KL93]{keali93}
M.~J. Kearns and M.~Li.
\newblock Learning in the presence of malicious errors.
\newblock {\em SIAM Journal on Computing}, 22(4):807--837, 1993.

\bibitem[KLS09]{KLS09}
A.~Klivans, P.~Long, and R.~Servedio.
\newblock Learning halfspaces with malicious noise.
\newblock To appear in \emph{Proc.\ 17th Internat. Colloq. on Algorithms,
  Languages and Programming (ICALP)}, 2009.

\bibitem[KS04]{KlivansServedio:04coltmargin}
A.~Klivans and R.~Servedio.
\newblock Learning intersections of halfspaces with a margin.
\newblock In {\em Proceedings of the 17th Annual Conference on Learning
  Theory,}, pages 348--362, 2004.

\bibitem[KSS94]{KSS:94}
M.~Kearns, R.~Schapire, and L.~Sellie.
\newblock {Toward Efficient Agnostic Learning}.
\newblock {\em Machine Learning}, 17(2/3):115--141, 1994.

\bibitem[LMS11]{LokshtanovMS11}
D.~Lokshtanov, D.~Marx, and S.~Saurabh.
\newblock Lower bounds based on the exponential time hypothesis.
\newblock {\em Bulletin of the {EATCS}}, 105:41--72, 2011.

\bibitem[LRV16]{LaiRV16}
K.~A. Lai, A.~B. Rao, and S.~Vempala.
\newblock Agnostic estimation of mean and covariance.
\newblock In {\em Proceedings of FOCS'16}, 2016.

\bibitem[LS11]{LS:11malicious}
P.~Long and R.~Servedio.
\newblock Learning large-margin halfspaces with more malicious noise.
\newblock {\em NIPS}, 2011.

\bibitem[McA03]{McAllester03}
D.~A. McAllester.
\newblock Simplified {PAC}-bayesian margin bounds.
\newblock In {\em 16th Annual Conference on Computational Learning Theory},
  pages 203--215, 2003.

\bibitem[MHS19]{MontasserHS19}
O.~Montasser, S.~Hanneke, and N.~Srebro.
\newblock {VC} classes are adversarially robustly learnable, but only
  improperly.
\newblock In {\em Conference on Learning Theory, {COLT} 2019}, pages
  2512--2530, 2019.

\bibitem[MR10]{MR10}
D.~Moshkovitz and R.~Raz.
\newblock Two-query {PCP} with subconstant error.
\newblock {\em J. {ACM}}, 57(5):29:1--29:29, 2010.

\bibitem[Nak19]{Nakkiran19}
P.~Nakkiran.
\newblock Adversarial robustness may be at odds with simplicity.
\newblock {\em CoRR}, abs/1901.00532, 2019.

\bibitem[OS11]{OS11:chow}
R.~O'Donnell and R.~Servedio.
\newblock {The Chow Parameters Problem}.
\newblock {\em SIAM J. on Comput.}, 40(1):165--199, 2011.

\bibitem[Ros58]{Rosenblatt:58}
F.~Rosenblatt.
\newblock The {P}erceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological Review}, 65:386--407, 1958.

\bibitem[Ser01]{Servedio:01lnh}
R.~Servedio.
\newblock Smooth boosting and learning with malicious noise.
\newblock In {\em Proceedings of the Fourteenth Annual Conference on
  Computational Learning Theory}, pages 473--489, 2001.

\bibitem[SSS09]{SSS09}
S.~Shalev{-}Shwartz, O.~Shamir, and K.~Sridharan.
\newblock Agnostically learning halfspaces with margin errors.
\newblock In {\em Technical report, Toyota Technological Institute}, 2009.

\bibitem[SSS10]{SSS10}
S.~Shalev{-}Shwartz, O.~Shamir, and K.~Sridharan.
\newblock Learning kernel-based halfspaces with the zero-one loss.
\newblock In {\em The 23rd Conference on Learning Theory, COLT 2010}, pages
  441--450, 2010.

\bibitem[TTV08]{TTV:09short}
L.~Trevisan, M.~Tulsiani, and S.~Vadhan.
\newblock {Regularity, Boosting and Efficiently Simulating every High Entropy
  Distribution}.
\newblock Technical Report 103, ECCC, 2008.
\newblock {Conference version in Proc. CCC 2009}.

\bibitem[Val85]{Valiant:85short}
L.~Valiant.
\newblock Learning disjunctions of conjunctions.
\newblock In {\em Proc. 9th IJCAI}, pages 560--566, 1985.

\bibitem[Vap98]{Vapnik:98}
V.~Vapnik.
\newblock {\em Statistical Learning Theory}.
\newblock Wiley-Interscience, New York, 1998.

\end{thebibliography}
