\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[pyb()]{pybullet}
Pybullet gymperium.
\newblock URL \url{https://github.com/benelot/pybullet-gym}.

\bibitem[Bertsekas(1995)]{1995:dpoc}
D.~P. Bertsekas.
\newblock \emph{Dynamic programming and optimal control}, volume~1.
\newblock Athena scientific Belmont, MA, 1995.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{2016:gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Degris et~al.(2012)Degris, White, and Sutton]{2012:opac}
T.~Degris, M.~White, and R.~S. Sutton.
\newblock Off-policy actor-critic.
\newblock \emph{arXiv preprint arXiv:1205.4839}, 2012.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{2016:benchmarking}
Y.~Duan, X.~Chen, R.~Houthooft, J.~Schulman, and P.~Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning}, pages
  1329--1338, 2016.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Van~Hoof, and Meger]{2018:td3}
S.~Fujimoto, H.~Van~Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{arXiv preprint arXiv:1802.09477}, 2018.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Sutskever, and Levine]{2016:naf}
S.~Gu, T.~Lillicrap, I.~Sutskever, and S.~Levine.
\newblock Continuous deep q-learning with model-based acceleration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2829--2838, 2016.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{2018:sac}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1861--1870, 2018.

\bibitem[Kakade(2002)]{2002:natural_pg}
S.~M. Kakade.
\newblock A natural policy gradient.
\newblock In \emph{Advances in neural information processing systems}, pages
  1531--1538, 2002.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{2015:ddpg}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Marbach and Tsitsiklis(2001)]{2001:continual_pg}
P.~Marbach and J.~N. Tsitsiklis.
\newblock Simulation-based optimization of markov reward processes.
\newblock \emph{IEEE Transactions on Automatic Control}, 46\penalty0
  (2):\penalty0 191--209, 2001.

\bibitem[Meyn and Tweedie(2012)]{2012:markov_chain}
S.~P. Meyn and R.~L. Tweedie.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{2015:dqn}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{2016:a3c}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem[Pardo et~al.(2017)Pardo, Tavakoli, Levdik, and
  Kormushev]{2017:time_limit}
F.~Pardo, A.~Tavakoli, V.~Levdik, and P.~Kormushev.
\newblock Time limits in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1712.00378}, 2017.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{2015:trpo}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017{\natexlab{a}})Schulman, Chen, and
  Abbeel]{2017:equivalence}
J.~Schulman, X.~Chen, and P.~Abbeel.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017{\natexlab{a}}.

\bibitem[Schulman et~al.(2017{\natexlab{b}})Schulman, Wolski, Dhariwal,
  Radford, and Klimov]{2017:ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017{\natexlab{b}}.

\bibitem[Serfozo(2009)]{2009:markov_chain}
R.~Serfozo.
\newblock \emph{Basics of applied stochastic processes}.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{2014:dpg}
D.~Silver, G.~Lever, N.~Heess, T.~Degris, D.~Wierstra, and M.~Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{ICML}, 2014.

\bibitem[Silver et~al.(2017)Silver, van Hasselt, Hessel, Schaul, Guez, Harley,
  Dulac-Arnold, Reichert, Rabinowitz, Barreto, et~al.]{2017:predictron}
D.~Silver, H.~van Hasselt, M.~Hessel, T.~Schaul, A.~Guez, T.~Harley,
  G.~Dulac-Arnold, D.~Reichert, N.~Rabinowitz, A.~Barreto, et~al.
\newblock The predictron: End-to-end learning and planning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3191--3199. JMLR. org, 2017.

\bibitem[Sutton et~al.(2014)Sutton, Mahmood, Precup, and Hasselt]{2014:qlambda}
R.~Sutton, A.~R. Mahmood, D.~Precup, and H.~Hasselt.
\newblock A new q (lambda) with interim forward view and monte carlo
  equivalence.
\newblock In \emph{International Conference on Machine Learning}, pages
  568--576, 2014.

\bibitem[Sutton(1995)]{1995:td_models}
R.~S. Sutton.
\newblock Td models: modeling the world at a mixture of time scales.
\newblock In \emph{Proceedings of the Twelfth International Conference on
  International Conference on Machine Learning}, pages 531--539. Morgan
  Kaufmann Publishers Inc., 1995.

\bibitem[Sutton and Barto(2018)]{2018:RL}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and Mansour]{2000:pg}
R.~S. Sutton, D.~A. McAllester, S.~P. Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{2011:horde}
R.~S. Sutton, J.~Modayil, M.~Delp, T.~Degris, P.~M. Pilarski, A.~White, and
  D.~Precup.
\newblock Horde: a scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pages 761--768, 2011.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{2016:emphatic}
R.~S. Sutton, A.~R. Mahmood, and M.~White.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2603--2631, 2016.

\bibitem[Thomas(2014)]{2014:thomas}
P.~Thomas.
\newblock Bias in natural actor-critic algorithms.
\newblock In \emph{International conference on machine learning}, pages
  441--448, 2014.

\bibitem[Tsitsiklis and Van~Roy(2002)]{2002:tsitsiklis}
J.~N. Tsitsiklis and B.~Van~Roy.
\newblock On average versus discounted reward temporal-difference learning.
\newblock \emph{Machine Learning}, 49\penalty0 (2-3):\penalty0 179--191, 2002.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{2016:ddqn}
H.~Van~Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, volume~2, page~5. Phoenix, AZ, 2016.

\bibitem[van Hasselt(2011)]{2011:insights}
H.~P. van Hasselt.
\newblock \emph{Insights in reinforcement rearning: formal analysis and
  empirical evaluation of temporal-difference learning algorithms}.
\newblock Utrecht University, 2011.

\bibitem[Watkins(1989)]{1989:qlearning}
C.~Watkins.
\newblock Learning from delayed rewards.
\newblock \emph{PhD thesis, King's College, University of Cambridge}, 1989.

\bibitem[White(2017)]{2017:unifying}
M.~White.
\newblock Unifying task specification in reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3742--3750. JMLR. org, 2017.

\bibitem[Williams(1992)]{1992:reinforce}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Yu(2015)]{2015:YU}
H.~Yu.
\newblock On convergence of emphatic temporal-difference learning.
\newblock In \emph{Conference on Learning Theory}, pages 1724--1751, 2015.

\bibitem[Zhang et~al.(2019)Zhang, Boehmer, and Whiteson]{2019:generalized}
S.~Zhang, W.~Boehmer, and S.~Whiteson.
\newblock Generalized off-policy actor-critic.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1999--2009, 2019.

\end{thebibliography}
