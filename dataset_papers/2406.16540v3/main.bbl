\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Geirhos et~al.(2018)Geirhos, Temme, Rauber, Sch{\"u}tt, Bethge, and
  Wichmann]{geirhos2018generalisation}
Robert Geirhos, Carlos~RM Temme, Jonas Rauber, Heiko~H Sch{\"u}tt, Matthias
  Bethge, and Felix~A Wichmann.
\newblock Generalisation in humans and deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{hendrycks2019augmix}
Dan Hendrycks, Norman Mu, Ekin~D Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock \emph{arXiv preprint arXiv:1912.02781}, 2019.

\bibitem[Lopes et~al.(2019)Lopes, Yin, Poole, Gilmer, and
  Cubuk]{lopes2019improving}
Raphael~Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin~D Cubuk.
\newblock Improving robustness without sacrificing accuracy with patch gaussian
  augmentation.
\newblock \emph{arXiv preprint arXiv:1906.02611}, 2019.

\bibitem[Mintun et~al.(2021)Mintun, Kirillov, and Xie]{mintun2021on}
Eric Mintun, Alexander Kirillov, and Saining Xie.
\newblock On interaction between augmentations and corruptions in natural
  corruption robustness.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=LOHyqjfyra}.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019canyou}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D.~Sculley, Sebastian
  Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model\textquotesingle s uncertainty? {E}valuating
  predictive uncertainty under dataset shift.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf}.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{dusenberry20a}
Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek,
  Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran.
\newblock Efficient and scalable {B}ayesian neural nets with rank-1 factors.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 2782--2792. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/dusenberry20a.html}.

\bibitem[Trinh et~al.(2022)Trinh, Heinonen, Acerbi, and Kaski]{trinh22a}
Trung Trinh, Markus Heinonen, Luigi Acerbi, and Samuel Kaski.
\newblock Tackling covariate shift with node-based {B}ayesian neural networks.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 21751--21775. PMLR,
  17--23 Jul 2022.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In~Kwon Choi.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5905--5914. PMLR, 2021.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE conference on Computer Vision and Pattern Recognition},
  2016{\natexlab{a}}.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Chen et~al.(2022)Chen, Hsieh, and Gong]{chen2022when}
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong.
\newblock When vision transformers outperform resnets without pre-training or
  strong data augmentations.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=LtKcMgGOeLt}.

\bibitem[Beyer et~al.(2022)Beyer, Zhai, and Kolesnikov]{beyer2022better}
Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov.
\newblock Better plain vit baselines for imagenet-1k.
\newblock \emph{arXiv preprint arXiv:2205.01580}, 2022.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N. Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=r1Ddp1-Rb}.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition workshops}, pages 702--703, 2020.

\bibitem[Rusak et~al.(2020)Rusak, Schott, Zimmermann, Bitterwolf, Bringmann,
  Bethge, and Brendel]{rusak2020simple}
Evgenia Rusak, Lukas Schott, Roland~S Zimmermann, Julian Bitterwolf, Oliver
  Bringmann, Matthias Bethge, and Wieland Brendel.
\newblock A simple way to make neural networks robust against diverse image
  corruptions.
\newblock \emph{arXiv preprint arXiv:2001.06057}, 2020.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017on}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem[Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{Jiang2020Fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgIPJBFvH}.

\bibitem[Krizhevsky(2009)]{krizhevsky2009cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Le and Yang(2015)]{Le2015TinyIV}
Ya~Le and Xuan~S. Yang.
\newblock Tiny {ImageNet} visual recognition challenge.
\newblock 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 248--255, 2009.
\newblock \doi{10.1109/CVPR.2009.5206848}.

\bibitem[Zhang et~al.(2024)Zhang, Pan, Kim, Kweon, and Mao]{imagenet-d}
Chenshuang Zhang, Fei Pan, Junmo Kim, In~So Kweon, and Chengzhi Mao.
\newblock Imagenet-d: Benchmarking neural network robustness on diffusion
  synthetic object.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 21752--21762, June 2024.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Zhao, Basart, Steinhardt, and
  Song]{imagenet-a}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 15262--15271, June 2021.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{imagenet-sketch}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/3eefceb8087e964f89c2d59e8a249915-Paper.pdf}.

\bibitem[Salvador and Oberman(2022)]{imagenet-cartoon-drawing}
Tiago Salvador and Adam~M Oberman.
\newblock Imagenet-cartoon and imagenet-drawing: two domain shift datasets for
  imagenet.
\newblock In \emph{ICML 2022 Shift Happens Workshop}, 2022.
\newblock URL \url{https://openreview.net/forum?id=YlAUXhjwaQt}.

\bibitem[Taesiri et~al.(2023)Taesiri, Nguyen, Habchi, Bezemer, and
  Nguyen]{taesiri2023zoom}
Mohammad~Reza Taesiri, Giang Nguyen, Sarra Habchi, Cor-Paul Bezemer, and Anh
  Nguyen.
\newblock Imagenet-hard: The hardest images remaining from a study of the power
  of zoom and spatial biases in image classification.
\newblock 2023.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and Szegedy]{fgsm}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, 2016{\natexlab{b}}.

\bibitem[Michaelis et~al.(2019)Michaelis, Mitzkus, Geirhos, Rusak, Bringmann,
  Ecker, Bethge, and Brendel]{michaelis2019dragon}
Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver
  Bringmann, Alexander~S. Ecker, Matthias Bethge, and Wieland Brendel.
\newblock Benchmarking robustness in object detection: Autonomous driving when
  winter is coming.
\newblock \emph{arXiv preprint arXiv:1907.07484}, 2019.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava14a}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Wang, Yu, Shi, Jiang, Tay, Feng, and
  Yan]{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang,
  Francis~EH Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 558--567, 2021.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Le~Cun, and
  Fergus]{pmlr-v28-wan13}
Li~Wan, Matthew Zeiler, Sixin Zhang, Yann Le~Cun, and Rob Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In Sanjoy Dasgupta and David McAllester, editors, \emph{Proceedings
  of the 30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pages 1058--1066, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/wan13.html}.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Durk~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Graves(2011)]{graves2011}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In J.~Shawe-Taylor, R.~Zemel, P.~Bartlett, F.~Pereira, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~24. Curran Associates, Inc., 2011.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf}.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{International conference on machine learning}, pages
  1613--1622. PMLR, 2015.

\bibitem[Gal and Ghahramani(2016)]{pmlr-v48-gal16}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In Maria~Florina Balcan and Kilian~Q. Weinberger, editors,
  \emph{Proceedings of The 33rd International Conference on Machine Learning},
  volume~48 of \emph{Proceedings of Machine Learning Research}, pages
  1050--1059, New York, New York, USA, 20--22 Jun 2016. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v48/gal16.html}.

\bibitem[Louizos and Welling(2017)]{louizos2017multiplicative}
Christos Louizos and Max Welling.
\newblock Multiplicative normalizing flows for variational bayesian neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2218--2227. PMLR, 2017.

\bibitem[Izmailov et~al.(2021)Izmailov, Nicholson, Lotfi, and
  Wilson]{izmailov2021dangers}
Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and Andrew~G Wilson.
\newblock Dangers of bayesian model averaging under covariate shift.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 3309--3322, 2021.

\bibitem[Ioffe and Szegedy(2015)]{batchnorm2015}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proceedings of the 32nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, page
  448–456. JMLR.org, 2015.

\bibitem[Li et~al.(2016)Li, Wang, Shi, Liu, and Hou]{li2016revisiting}
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou.
\newblock Revisiting batch normalization for practical domain adaptation.
\newblock \emph{arXiv preprint arXiv:1603.04779}, 2016.

\bibitem[Nado et~al.(2020)Nado, Padhy, Sculley, D'Amour, Lakshminarayanan, and
  Snoek]{nado2020evaluating}
Zachary Nado, Shreyas Padhy, D~Sculley, Alexander D'Amour, Balaji
  Lakshminarayanan, and Jasper Snoek.
\newblock Evaluating prediction-time batch normalization for robustness under
  covariate shift.
\newblock \emph{arXiv preprint arXiv:2006.10963}, 2020.

\bibitem[Schneider et~al.(2020)Schneider, Rusak, Eck, Bringmann, Brendel, and
  Bethge]{schneider2020improving}
Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel,
  and Matthias Bethge.
\newblock Improving robustness against common corruptions by covariate shift
  adaptation.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 11539--11551, 2020.

\bibitem[Benz et~al.(2021)Benz, Zhang, Karjauv, and Kweon]{benz2021revisiting}
Philipp Benz, Chaoning Zhang, Adil Karjauv, and In~So Kweon.
\newblock Revisiting batch normalization for improving corruption robustness.
\newblock In \emph{Proceedings of the IEEE/CVF winter conference on
  applications of computer vision}, pages 494--503, 2021.

\bibitem[B{\"o}hning and Lindsay(1988)]{bohning1988monotonicity}
Dankmar B{\"o}hning and Bruce~G Lindsay.
\newblock Monotonicity of quadratic-approximation algorithms.
\newblock \emph{Annals of the Institute of Statistical Mathematics},
  40\penalty0 (4):\penalty0 641--663, 1988.

\end{thebibliography}
