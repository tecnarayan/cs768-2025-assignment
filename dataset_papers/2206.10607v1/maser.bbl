\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(2016)]{Amaribook}
Amari, S.-i.
\newblock \emph{Information geometry and its applications}, volume 194.
\newblock Springer, 2016.

\bibitem[Andriotis \& Papakonstantinou(2019)Andriotis and
  Papakonstantinou]{2019Andriotis}
Andriotis, C. and Papakonstantinou, K.
\newblock Managing engineering systems with large state and action spaces
  through deep reinforcement learning.
\newblock \emph{Reliability Engineering \& System Safety}, 191:\penalty0
  106483, 2019.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{HER}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:1707.01495}, 2017.

\bibitem[Chane-Sane et~al.(2021)Chane-Sane, Schmid, and
  Laptev]{imagined_subgoal}
Chane-Sane, E., Schmid, C., and Laptev, I.
\newblock Goal-conditioned reinforcement learning with imagined subgoals.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1430--1440. PMLR, 2021.

\bibitem[Christianos et~al.(2020)Christianos, Sch{\"a}fer, and Albrecht]{seac}
Christianos, F., Sch{\"a}fer, L., and Albrecht, S.~V.
\newblock Shared experience actor-critic for multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2006.07169}, 2020.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and Bengio]{gru}
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Du et~al.(2019)Du, Han, Fang, Liu, Dai, and Tao]{LIIR}
Du, Y., Han, L., Fang, M., Liu, J., Dai, T., and Tao, D.
\newblock Liir: Learning individual intrinsic reward in multi-agent
  reinforcement learning.
\newblock 2019.

\bibitem[Foerster et~al.(2018{\natexlab{a}})Foerster, Farquhar, Afouras,
  Nardelli, and Whiteson]{COMA}
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018{\natexlab{a}}.

\bibitem[Foerster et~al.(2018{\natexlab{b}})Foerster, Farquhar, Afouras,
  Nardelli, and Whiteson]{2018Foerster}
Foerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018{\natexlab{b}}.

\bibitem[Ghosh et~al.(2018)Ghosh, Gupta, and Levine]{ARC}
Ghosh, D., Gupta, A., and Levine, S.
\newblock Learning actionable representations with goal-conditioned policies.
\newblock \emph{arXiv preprint arXiv:1811.07819}, 2018.

\bibitem[Hausknecht \& Stone(2015)Hausknecht and Stone]{drqn}
Hausknecht, M. and Stone, P.
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock In \emph{2015 aaai fall symposium series}, 2015.

\bibitem[Huang et~al.(2019)Huang, Liu, and Su]{mapping_state}
Huang, Z., Liu, F., and Su, H.
\newblock Mapping state space using landmarks for universal goal reaching.
\newblock \emph{arXiv preprint arXiv:1908.05451}, 2019.

\bibitem[Jaques et~al.(2018)Jaques, Lazaridou, Hughes, Gulcehre, Ortega,
  Strouse, Leibo, and De~Freitas]{2019Jaques}
Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Ortega, P.~A., Strouse,
  D., Leibo, J.~Z., and De~Freitas, N.
\newblock Social influence as intrinsic motivation for multi-agent deep
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1810.08647}, 2018.

\bibitem[Kim et~al.(2020{\natexlab{a}})Kim, Jung, Cho, and Sung]{MMI}
Kim, W., Jung, W., Cho, M., and Sung, Y.
\newblock A maximum mutual information framework for multi-agent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2006.02732}, 2020{\natexlab{a}}.

\bibitem[Kim et~al.(2020{\natexlab{b}})Kim, Park, and Sung]{MARL-IS}
Kim, W., Park, J., and Sung, Y.
\newblock Communication in multi-agent reinforcement learning: Intention
  sharing.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{hdqn}
Kulkarni, T.~D., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 3675--3683, 2016.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Li et~al.(2019)Li, Qin, Jiao, Yang, Wang, Wang, Wu, and Ye]{2019Li}
Li, M., Qin, Z., Jiao, Y., Yang, Y., Wang, J., Wang, C., Wu, G., and Ye, J.
\newblock Efficient ridesharing order dispatching with mean field multi-agent
  reinforcement learning.
\newblock In \emph{The World Wide Web Conference}, pp.\  983--994, 2019.

\bibitem[Liu et~al.(2021)Liu, Jain, Yeh, and Schwing]{cmae}
Liu, I.-J., Jain, U., Yeh, R.~A., and Schwing, A.
\newblock Cooperative exploration for multi-agent deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6826--6836. PMLR, 2021.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and Mordatch]{MADDPG}
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{arXiv preprint arXiv:1706.02275}, 2017.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and Whiteson]{maven}
Mahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S.
\newblock Maven: Multi-agent variational exploration.
\newblock \emph{arXiv preprint arXiv:1910.07483}, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Oliehoek(2012)]{Dec-POMDP}
Oliehoek, F.~A.
\newblock Decentralized pomdps.
\newblock In \emph{Reinforcement Learning}, pp.\  471--503. Springer, 2012.

\bibitem[OroojlooyJadid \& Hajinezhad(2019)OroojlooyJadid and
  Hajinezhad]{2019Oroojlooyjadid}
OroojlooyJadid, A. and Hajinezhad, D.
\newblock A review of cooperative multi-agent deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1908.03963}, 2019.

\bibitem[Rashid et~al.(2018{\natexlab{a}})Rashid, Samvelyan, De~Witt, Farquhar,
  Foerster, and Whiteson]{2018rashid}
Rashid, T., Samvelyan, M., De~Witt, C.~S., Farquhar, G., Foerster, J., and
  Whiteson, S.
\newblock Qmix: monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.11485}, 2018{\natexlab{a}}.

\bibitem[Rashid et~al.(2018{\natexlab{b}})Rashid, Samvelyan, Schroeder,
  Farquhar, Foerster, and Whiteson]{QMIX}
Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., and
  Whiteson, S.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4295--4304. PMLR, 2018{\natexlab{b}}.

\bibitem[Ryu et~al.(2020)Ryu, Shin, and Park]{remax}
Ryu, H., Shin, H., and Park, J.
\newblock Remax: Relational representation for multi-agent exploration.
\newblock \emph{arXiv preprint arXiv:2008.05214}, 2020.

\bibitem[Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi]{QTRAN}
Son, K., Kim, D., Kang, W.~J., Hostallero, D.~E., and Yi, Y.
\newblock Qtran: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5887--5896. PMLR, 2019.

\bibitem[Sunehag et~al.(2017)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, et~al.]{VDN}
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg,
  M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning.
\newblock \emph{arXiv preprint arXiv:1706.05296}, 2017.

\bibitem[Tan(1993)]{1993Tan}
Tan, M.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In \emph{Proceedings of the tenth international conference on machine
  learning}, pp.\  330--337, 1993.

\bibitem[Tang et~al.(2018)Tang, Hao, Lv, Chen, Zhang, Jia, Ren, Zheng, Meng,
  Fan, et~al.]{hdmarl}
Tang, H., Hao, J., Lv, T., Chen, Y., Zhang, Z., Jia, H., Ren, C., Zheng, Y.,
  Meng, Z., Fan, C., et~al.
\newblock Hierarchical deep multiagent reinforcement learning with temporal
  abstraction.
\newblock \emph{arXiv preprint arXiv:1809.09332}, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Dong, Lesser, and Zhang]{roma}
Wang, T., Dong, H., Lesser, V., and Zhang, C.
\newblock Roma: Multi-agent reinforcement learning with emergent roles.
\newblock \emph{arXiv preprint arXiv:2003.08039}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Gupta, Mahajan, Peng, Whiteson,
  and Zhang]{rode}
Wang, T., Gupta, T., Mahajan, A., Peng, B., Whiteson, S., and Zhang, C.
\newblock Rode: Learning roles to decompose multi-agent tasks.
\newblock \emph{arXiv preprint arXiv:2010.01523}, 2020{\natexlab{b}}.

\bibitem[Wu et~al.(2021)Wu, Li, Zhang, Wang, and Zhang]{GIIR}
Wu, H., Li, H., Zhang, J., Wang, Z., and Zhang, J.
\newblock Generating individual intrinsic reward for cooperative multiagent
  reinforcement learning.
\newblock \emph{International Journal of Advanced Robotic Systems}, 18\penalty0
  (5):\penalty0 17298814211044946, 2021.

\bibitem[Wu et~al.(2018)Wu, Tucker, and Nachum]{laplacian}
Wu, Y., Tucker, G., and Nachum, O.
\newblock The laplacian in rl: Learning representations with efficient
  approximations.
\newblock \emph{arXiv preprint arXiv:1810.04586}, 2018.

\bibitem[Yang et~al.(2019)Yang, Borovikov, and Zha]{hsd}
Yang, J., Borovikov, I., and Zha, H.
\newblock Hierarchical cooperative multi-agent reinforcement learning with
  skill discovery.
\newblock \emph{arXiv preprint arXiv:1912.03558}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Guo, Tan, Hu, and Chen]{generate_subgoal}
Zhang, T., Guo, S., Tan, T., Hu, X., and Chen, F.
\newblock Generating adjacency-constrained subgoals in hierarchical
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.11485}, 2020.

\end{thebibliography}
