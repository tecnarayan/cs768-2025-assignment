\begin{thebibliography}{10}

\bibitem{afsar2022reinforcement}
M~Mehdi Afsar, Trafford Crump, and Behrouz Far.
\newblock Reinforcement learning based recommender systems: A survey.
\newblock {\em ACM Computing Surveys}, 55(7):1--38, 2022.

\bibitem{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  104--114. PMLR, 2020.

\bibitem{aigner1976estimation}
Dennis~J Aigner, Takeshi Amemiya, and Dale~J Poirier.
\newblock On the estimation of production frontiers: maximum likelihood
  estimation of the parameters of a discontinuous density function.
\newblock {\em International economic review}, pages 377--396, 1976.

\bibitem{ajay2022conditional}
Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit
  Agrawal.
\newblock Is conditional generative modeling all you need for decision-making?
\newblock {\em arXiv preprint arXiv:2211.15657}, 2022.

\bibitem{argenson2020model}
Arthur Argenson and Gabriel Dulac-Arnold.
\newblock Model-based offline planning.
\newblock {\em arXiv preprint arXiv:2008.05556}, 2020.

\bibitem{badia2020agent57}
Adri{\`a}~Puigdom{\`e}nech Badia, Bilal Piot, Steven Kapturowski, Pablo
  Sprechmann, Alex Vitvitskyi, Zhaohan~Daniel Guo, and Charles Blundell.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock In {\em International conference on machine learning}, pages
  507--517. PMLR, 2020.

\bibitem{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{char2022bats}
Ian Char, Viraj Mehta, Adam Villaflor, John~M Dolan, and Jeff Schneider.
\newblock Bats: Best action trajectory stitching.
\newblock {\em arXiv preprint arXiv:2204.12026}, 2022.

\bibitem{chen2022offline}
Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu.
\newblock Offline reinforcement learning via high-fidelity generative behavior
  modeling.
\newblock {\em arXiv preprint arXiv:2209.14548}, 2022.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems},
  34:15084--15097, 2021.

\bibitem{chen2019generative}
Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, and Le~Song.
\newblock Generative adversarial user model for reinforcement learning based
  recommendation system.
\newblock In {\em International Conference on Machine Learning}, pages
  1052--1061. PMLR, 2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International conference on machine learning}, pages
  2052--2062. PMLR, 2019.

\bibitem{gulcehre2020rl}
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio G{\'o}mez,
  Konrad Zolna, Rishabh Agarwal, Josh~S Merel, Daniel~J Mankowitz, Cosmin
  Paduraru, et~al.
\newblock Rl unplugged: A suite of benchmarks for offline reinforcement
  learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7248--7259, 2020.

\bibitem{hensen2023implicit}
Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien~Kuba,
  and Sergey Levine.
\newblock Idql: Implicit q-learning as an actor-critic method with diffusion
  policies.
\newblock {\em arXiv preprint arXiv:2304.10573}, 2023.

\bibitem{hepburn2022model}
Charles~A Hepburn and Giovanni Montana.
\newblock Model-based trajectory stitching for improved offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2211.11603}, 2022.

\bibitem{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock {\em arXiv preprint arXiv:1904.09751}, 2019.

\bibitem{janner2022planning}
Michael Janner, Yilun Du, Joshua~B Tenenbaum, and Sergey Levine.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock {\em arXiv preprint arXiv:2205.09991}, 2022.

\bibitem{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock {\em Advances in neural information processing systems},
  34:1273--1286, 2021.

\bibitem{kaiser2019model}
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy~H
  Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski,
  Sergey Levine, et~al.
\newblock Model-based reinforcement learning for atari.
\newblock {\em arXiv preprint arXiv:1903.00374}, 2019.

\bibitem{kappen2012optimal}
Hilbert~J Kappen, Vicen{\c{c}} G{\'o}mez, and Manfred Opper.
\newblock Optimal control as a graphical model inference problem.
\newblock {\em Machine learning}, 87:159--182, 2012.

\bibitem{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  33:21810--21823, 2020.

\bibitem{kostrikov2021offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock {\em arXiv preprint arXiv:2110.06169}, 2021.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1179--1191, 2020.

\bibitem{lee2021representation}
Byung-Jun Lee, Jongmin Lee, and Kee-Eung Kim.
\newblock Representation balancing offline model-based reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{lee2022multi}
Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie
  Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, et~al.
\newblock Multi-game decision transformers.
\newblock {\em arXiv preprint arXiv:2205.15241}, 2022.

\bibitem{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem{liang2023adaptdiffuser}
Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo.
\newblock Adaptdiffuser: Diffusion models as adaptive self-evolving planners.
\newblock {\em arXiv preprint arXiv:2302.01877}, 2023.

\bibitem{liu2021offline}
Ran Liu, Joseph~L Greenstein, James~C Fackler, Jules Bergmann, Melania~M
  Bembea, and Raimond~L Winslow.
\newblock Offline reinforcement learning with uncertainty for treatment
  strategies in sepsis.
\newblock {\em arXiv preprint arXiv:2107.04491}, 2021.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 10012--10022, 2021.

\bibitem{mitchell2021offline}
Eric Mitchell, Rafael Rafailov, Xue~Bin Peng, Sergey Levine, and Chelsea Finn.
\newblock Offline meta-reinforcement learning with advantage weighting.
\newblock In {\em International Conference on Machine Learning}, pages
  7780--7791. PMLR, 2021.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{nair2020awac}
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine.
\newblock Awac: Accelerating online reinforcement learning with offline
  datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem{newey1987asymmetric}
Whitney~K Newey and James~L Powell.
\newblock Asymmetric least squares estimation and testing.
\newblock {\em Econometrica: Journal of the Econometric Society}, pages
  819--847, 1987.

\bibitem{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1910.00177}, 2019.

\bibitem{plappert2018multi}
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker,
  Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder,
  et~al.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock {\em arXiv preprint arXiv:1802.09464}, 2018.

\bibitem{polydoros2017survey}
Athanasios~S Polydoros and Lazaros Nalpantidis.
\newblock Survey of model-based reinforcement learning: Applications on
  robotics.
\newblock {\em Journal of Intelligent \& Robotic Systems}, 86(2):153--173,
  2017.

\bibitem{puterman1990markov}
Martin~L Puterman.
\newblock Markov decision processes.
\newblock {\em Handbooks in operations research and management science},
  2:331--434, 1990.

\bibitem{shachter1988probabilistic}
Ross~D Shachter.
\newblock Probabilistic inference and influence diagrams.
\newblock {\em Operations research}, 36(4):589--604, 1988.

\bibitem{silver2018general}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock {\em Science}, 362(6419):1140--1144, 2018.

\bibitem{sinha2022s4rl}
Samarth Sinha, Ajay Mandlekar, and Animesh Garg.
\newblock S4rl: Surprisingly simple self-supervision for offline reinforcement
  learning in robotics.
\newblock In {\em Conference on Robot Learning}, pages 907--917. PMLR, 2022.

\bibitem{soleymani2020financial}
Farzan Soleymani and Eric Paquet.
\newblock Financial portfolio optimization with online deep reinforcement
  learning and restricted stacked autoencoderâ€”deepbreath.
\newblock {\em Expert Systems with Applications}, 156:113456, 2020.

\bibitem{todorov2006linearly}
Emanuel Todorov.
\newblock Linearly-solvable markov decision problems.
\newblock {\em Advances in neural information processing systems}, 19, 2006.

\bibitem{toussaint2009robot}
Marc Toussaint.
\newblock Robot trajectory optimization using approximate inference.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 1049--1056, 2009.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2022diffusion}
Zhendong Wang, Jonathan~J Hunt, and Mingyuan Zhou.
\newblock Diffusion policies as an expressive policy class for offline
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2208.06193}, 2022.

\bibitem{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh~S Merel, Jost~Tobias
  Springenberg, Scott~E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7768--7778, 2020.

\bibitem{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 conference on empirical methods in
  natural language processing: system demonstrations}, pages 38--45, 2020.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{wu2023learning}
Yueh-Hua Wu, Jiashun Wang, and Xiaolong Wang.
\newblock Learning generalizable dexterous manipulation from human grasp
  affordance.
\newblock In {\em Conference on Robot Learning}, pages 618--629. PMLR, 2023.

\bibitem{xiao2021general}
Teng Xiao and Donglin Wang.
\newblock A general offline reinforcement learning framework for interactive
  recommendation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem{xu2022policy}
Haoran Xu, Li~Jiang, Li~Jianxiong, and Xianyuan Zhan.
\newblock A policy-guided imitation approach for offline reinforcement
  learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:4085--4098, 2022.

\bibitem{xu2023hyper}
Mengdi Xu, Yuchen Lu, Yikang Shen, Shun Zhang, Ding Zhao, and Chuang Gan.
\newblock Hyper-decision transformer for efficient online policy adaptation.
\newblock {\em arXiv preprint arXiv:2304.08487}, 2023.

\bibitem{xu2022prompting}
Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and
  Chuang Gan.
\newblock Prompting decision transformer for few-shot policy generalization.
\newblock In {\em international conference on machine learning}, pages
  24631--24645. PMLR, 2022.

\bibitem{yamagata2022q}
Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez.
\newblock Q-learning decision transformer: Leveraging dynamic programming for
  conditional sequence modelling in offline rl.
\newblock {\em arXiv preprint arXiv:2209.03993}, 2022.

\bibitem{yarats2021mastering}
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto.
\newblock Mastering visual continuous control: Improved data-augmented
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2107.09645}, 2021.

\end{thebibliography}
