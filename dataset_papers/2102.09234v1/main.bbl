\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Allen-Zhu, Z.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  1200--1205. ACM, 2017.

\bibitem[Bazerque \& Giannakis(2009)Bazerque and
  Giannakis]{bazerque2009distributed}
Bazerque, J.~A. and Giannakis, G.~B.
\newblock Distributed spectrum sensing for cognitive radio networks by
  exploiting sparsity.
\newblock \emph{IEEE Transactions on Signal Processing}, 58\penalty0
  (3):\penalty0 1847--1862, 2009.

\bibitem[Beck et~al.(2014)Beck, Nedi{\'c}, Ozdaglar, and Teboulle]{beck20141}
Beck, A., Nedi{\'c}, A., Ozdaglar, A., and Teboulle, M.
\newblock An $ o (1/k) $ gradient method for network resource allocation
  problems.
\newblock \emph{IEEE Transactions on Control of Network Systems}, 1\penalty0
  (1):\penalty0 64--73, 2014.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Horv\'{a}th, Richt\'{a}rik, and
  Safaryan]{biased2020}
Beznosikov, A., Horv\'{a}th, S., Richt\'{a}rik, P., and Safaryan, M.
\newblock On biased compression for distributed learning.
\newblock \emph{arXiv:2002.12410}, 2020.

\bibitem[Chang \& Lin(2011)Chang and Lin]{chang2011libsvm}
Chang, C.-C. and Lin, C.-J.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Gan et~al.(2012)Gan, Topcu, and Low]{gan2012optimal}
Gan, L., Topcu, U., and Low, S.~H.
\newblock Optimal decentralized protocol for electric vehicle charging.
\newblock \emph{IEEE Transactions on Power Systems}, 28\penalty0 (2):\penalty0
  940--951, 2012.

\bibitem[Giselsson et~al.(2013)Giselsson, Doan, Keviczky, De~Schutter, and
  Rantzer]{giselsson2013accelerated}
Giselsson, P., Doan, M.~D., Keviczky, T., De~Schutter, B., and Rantzer, A.
\newblock Accelerated gradient methods and dual decomposition in distributed
  model predictive control.
\newblock \emph{Automatica}, 49\penalty0 (3):\penalty0 829--833, 2013.

\bibitem[Gorbunov et~al.(2020{\natexlab{a}})Gorbunov, Kovalev, Makarenko, and
  Richt{\'a}rik]{gorbunov2020linearly}
Gorbunov, E., Kovalev, D., Makarenko, D., and Richt{\'a}rik, P.
\newblock Linearly converging error compensated {SGD}.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{a}}.

\bibitem[Gorbunov et~al.(2020{\natexlab{b}})Gorbunov, Rogozin, Beznosikov,
  Dvinskikh, and Gasnikov]{Gorbunov-Decentralized-Survey2020}
Gorbunov, E., Rogozin, A., Beznosikov, A., Dvinskikh, D., and Gasnikov, A.
\newblock Recent theoretical advances in decentralized distributed convex
  optimization.
\newblock \emph{arXiv preprint arXiv:2011.13259}, 2020{\natexlab{b}}.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
Karimireddy, S.~P., Rebjock, Q., Stich, S.~U., and Jaggi, M.
\newblock Error feedback fixes {S}ign{SGD} and other gradient compression
  schemes.
\newblock \emph{arXiv preprint arXiv:1901.09847}, 2019.

\bibitem[Kolar et~al.(2010)Kolar, Song, Ahmed, and Xing]{Kolar2010}
Kolar, M., Song, L., Ahmed, A., and Xing, E.~P.
\newblock Estimating time-varying networks.
\newblock \emph{The Annals of Applied Statistics}, 4\penalty0 (1):\penalty0
  94--123, 2010.

\bibitem[Kone\v{c}n\'{y} et~al.(2016)Kone\v{c}n\'{y}, McMahan, Yu,
  Richt\'{a}rik, Suresh, and Bacon]{FEDLEARN}
Kone\v{c}n\'{y}, J., McMahan, H.~B., Yu, F., Richt\'{a}rik, P., Suresh, A.~T.,
  and Bacon, D.
\newblock Federated learning: strategies for improving communication
  efficiency.
\newblock In \emph{NIPS Private Multi-Party Machine Learning Workshop}, 2016.

\bibitem[Kovalev et~al.(2020{\natexlab{a}})Kovalev, Horv\'{a}th, and
  Richt\'{a}rik]{L-SVRG}
Kovalev, D., Horv\'{a}th, S., and Richt\'{a}rik, P.
\newblock Donâ€™t jump through hoops and remove those loops: {SVRG} and
  {K}atyusha are better without the outer loop.
\newblock In \emph{Proceedings of the 31st International Conference on
  Algorithmic Learning Theory}, 2020{\natexlab{a}}.

\bibitem[Kovalev et~al.(2020{\natexlab{b}})Kovalev, Salim, and
  Richt{\'a}rik]{kovalev2020optimal}
Kovalev, D., Salim, A., and Richt{\'a}rik, P.
\newblock Optimal and practical algorithms for smooth and strongly convex
  decentralized optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Kovalev et~al.(2021)Kovalev, Koloskova, Jaggi, Richt\'{a}rik, and
  Stich]{D-DIANA}
Kovalev, D., Koloskova, A., Jaggi, M., Richt\'{a}rik, P., and Stich, S.
\newblock A linearly convergent algorithm for decentralized optimization:
  Sending less bits for free!
\newblock In \emph{The 24th International Conference on Artificial Intelligence
  and Statistics (AISTATS 2021)}, 2021.

\bibitem[Li et~al.(2018)Li, Fang, Yin, and Lin]{li2018sharp}
Li, H., Fang, C., Yin, W., and Lin, Z.
\newblock A sharp convergence rate analysis for distributed accelerated
  gradient methods.
\newblock \emph{arXiv preprint arXiv:1810.01053}, 2018.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{FL_survey_2020}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: challenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  50--60, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Kovalev, Qian, and
  Richt\'{a}rik]{ADIANA}
Li, Z., Kovalev, D., Qian, X., and Richt\'{a}rik, P.
\newblock Acceleration for compressed gradient descent in distributed and
  federated optimization.
\newblock In \emph{International Conference on Machine Learing},
  2020{\natexlab{b}}.

\bibitem[Maros \& Jald{\'e}n(2018)Maros and Jald{\'e}n]{maros2018panda}
Maros, M. and Jald{\'e}n, J.
\newblock Panda: A dual linearly converging method for distributed optimization
  over time-varying undirected graphs.
\newblock In \emph{2018 IEEE Conference on Decision and Control (CDC)}, pp.\
  6520--6525. IEEE, 2018.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and Ag\"{u}era~y
  Arcas]{FL2017-AISTATS}
McMahan, H.~B., Moore, E., Ramage, D., Hampson, S., and Ag\"{u}era~y Arcas, B.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Morris et~al.(2020)Morris, Kriege, Bause, Kersting, Mutzel, and
  Neumann]{morris2020tudataset}
Morris, C., Kriege, N.~M., Bause, F., Kersting, K., Mutzel, P., and Neumann, M.
\newblock Tudataset: A collection of benchmark datasets for learning with
  graphs.
\newblock In \emph{ICML 2020 Workshop on Graph Representation Learning and
  Beyond (GRL+ 2020)}, 2020.
\newblock URL \url{www.graphlearning.io}.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Nedic, A., Olshevsky, A., and Shi, W.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Pu et~al.(2020)Pu, Shi, Xu, and Nedic]{pu2020push}
Pu, S., Shi, W., Xu, J., and Nedic, A.
\newblock Push-pull gradient methods for distributed optimization in networks.
\newblock \emph{IEEE Transactions on Automatic Control}, 2020.

\bibitem[Qian et~al.(2020)Qian, Richt\'{a}rik, and Zhang]{EC-Katyusha}
Qian, X., Richt\'{a}rik, P., and Zhang, T.
\newblock Error compensated distributed {SGD} can be accelerated.
\newblock \emph{arXiv preprint arXiv:2010.00091}, 2020.

\bibitem[Qu \& Li(2019)Qu and Li]{qu2019accelerated}
Qu, G. and Li, N.
\newblock Accelerated distributed nesterov gradient descent.
\newblock \emph{IEEE Transactions on Automatic Control}, 2019.

\bibitem[Rabbat \& Nowak(2004)Rabbat and Nowak]{rabbat2004distributed}
Rabbat, M. and Nowak, R.
\newblock Distributed optimization in sensor networks.
\newblock In \emph{Proceedings of the 3rd international symposium on
  Information processing in sensor networks}, pp.\  20--27, 2004.

\bibitem[Rockafellar(1970)]{rockafellar1970convex}
Rockafellar, R.~T.
\newblock \emph{Convex analysis}, volume~36.
\newblock Princeton university press, 1970.

\bibitem[Rogozin et~al.(2019)Rogozin, Uribe, Gasnikov, Malkovskii, and
  Nedich]{rogozin2019optimal}
Rogozin, A., Uribe, C., Gasnikov, A., Malkovskii, N., and Nedich, A.
\newblock Optimal distributed convex optimization on slowly time-varying
  graphs.
\newblock \emph{IEEE Transactions on Control of Network Systems}, 2019.

\bibitem[Rogozin et~al.(2020)Rogozin, Lukoshkin, Gasnikov, Kovalev, and
  Shulgin]{rogozin2020towards}
Rogozin, A., Lukoshkin, V., Gasnikov, A., Kovalev, D., and Shulgin, E.
\newblock Towards accelerated rates for distributed optimization over
  time-varying networks.
\newblock \emph{arXiv preprint arXiv:2009.11069}, 2020.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
Scaman, K., Bach, F., Bubeck, S., Lee, Y.~T., and Massouli{\'e}, L.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock \emph{arXiv preprint arXiv:1702.08704}, 2017.

\bibitem[Stich \& Karimireddy(2019)Stich and Karimireddy]{stich2019error}
Stich, S.~U. and Karimireddy, S.~P.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock \emph{arXiv preprint arXiv:1909.05350}, 2019.

\bibitem[Ye et~al.(2020)Ye, Luo, Zhou, and Zhang]{ye2020multi}
Ye, H., Luo, L., Zhou, Z., and Zhang, T.
\newblock Multi-consensus decentralized accelerated gradient descent.
\newblock \emph{arXiv preprint arXiv:2005.00797}, 2020.

\bibitem[{Zadeh}(1961)]{Zadeh1961}
{Zadeh}, L.~A.
\newblock Time-varying networks, i.
\newblock \emph{Proceedings of the IRE}, 49\penalty0 (10):\penalty0 1488--1503,
  1961.

\end{thebibliography}
