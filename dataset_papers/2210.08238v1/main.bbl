\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Almirall et~al.(2012)Almirall, Compton, Gunlicks-Stoessel, Duan, and
  Murphy]{almirall2012designing}
Daniel Almirall, Scott~N Compton, Meredith Gunlicks-Stoessel, Naihua Duan, and
  Susan~A Murphy.
\newblock Designing a pilot sequential multiple assignment randomized trial for
  developing an adaptive treatment strategy.
\newblock \emph{Statistics in medicine}, 31\penalty0 (17):\penalty0 1887--1902,
  2012.

\bibitem[Almirall et~al.(2014)Almirall, Nahum-Shani, Sherwood, and
  Murphy]{almirall2014introduction}
Daniel Almirall, Inbal Nahum-Shani, Nancy~E Sherwood, and Susan~A Murphy.
\newblock Introduction to smart designs for the development of adaptive
  interventions: with application to weight loss research.
\newblock \emph{Translational behavioral medicine}, 4\penalty0 (3):\penalty0
  260--274, 2014.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, and Hilbert~J Kappen.
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement
  learning with a generative model.
\newblock \emph{Machine learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Bai et~al.(2019)Bai, Xie, Jiang, and Wang]{bai2019provably}
Yu~Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang.
\newblock Provably efficient q-learning with low switching cost.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8004--8013, 2019.

\bibitem[Bartlett and Tewari(2009)]{bartlett2009regal}
Peter~L Bartlett and Ambuj Tewari.
\newblock Regal: a regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In \emph{Proceedings of the 25th Conference on Uncertainty in
  Artificial Intelligence (UAI 2009))}, 2009.

\bibitem[Cesa-Bianchi et~al.(2013)Cesa-Bianchi, Dekel, and
  Shamir]{cesa2013online}
Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir.
\newblock Online learning with switching costs and other adaptive adversaries.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1160--1168, 2013.

\bibitem[Cohen et~al.(2021)Cohen, Lee, and Song]{cohen2021solving}
Michael~B Cohen, Yin~Tat Lee, and Zhao Song.
\newblock Solving linear programs in the current matrix multiplication time.
\newblock \emph{Journal of the ACM (JACM)}, 68\penalty0 (1):\penalty0 1--39,
  2021.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research},
  pages 1507--1516, Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem[Dong et~al.(2020)Dong, Li, Zhang, and Zhou]{dong2020multinomial}
Kefan Dong, Yingkai Li, Qin Zhang, and Yuan Zhou.
\newblock Multinomial logit bandit with low switching cost.
\newblock In \emph{International Conference on Machine Learning}, pages
  2607--2615. PMLR, 2020.

\bibitem[Gao et~al.(2021)Gao, Xie, Du, and Yang]{gao2021provably}
Minbo Gao, Tianle Xie, Simon~S Du, and Lin~F Yang.
\newblock A provably efficient algorithm for linear markov decision process
  with low switching cost.
\newblock \emph{arXiv preprint arXiv:2101.00494}, 2021.

\bibitem[Gao et~al.(2019)Gao, Han, Ren, and Zhou]{gao2019batched}
Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou.
\newblock Batched multi-armed bandits problem.
\newblock \emph{arXiv preprint arXiv:1904.01763}, 2019.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Kakade(2003)]{kakade2003sample}
Sham~M Kakade.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[Krishnan et~al.(2018)Krishnan, Yang, Goldberg, Hellerstein, and
  Stoica]{krishnan2018learning}
Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion
  Stoica.
\newblock Learning to optimize join queries with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1808.03196}, 2018.

\bibitem[Lei et~al.(2012)Lei, Nahum-Shani, Lynch, Oslin, and
  Murphy]{lei2012smart}
Huitan Lei, Inbal Nahum-Shani, Kevin Lynch, David Oslin, and Susan~A Murphy.
\newblock A" smart" design for building individualized treatment sequences.
\newblock \emph{Annual review of clinical psychology}, 8:\penalty0 21--48,
  2012.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li2020breaking}
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock \emph{arXiv preprint arXiv:2005.12900}, 2020.

\bibitem[Mirhoseini et~al.(2017)Mirhoseini, Pham, Le, Steiner, Larsen, Zhou,
  Kumar, Norouzi, Bengio, and Dean]{mirhoseini2017device}
Azalia Mirhoseini, Hieu Pham, Quoc~V Le, Benoit Steiner, Rasmus Larsen, Yuefeng
  Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean.
\newblock Device placement optimization with reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2430--2439. PMLR, 2017.

\bibitem[Perchet et~al.(2016)Perchet, Rigollet, Chassang, Snowberg,
  et~al.]{perchet2016batched}
Vianney Perchet, Philippe Rigollet, Sylvain Chassang, Erik Snowberg, et~al.
\newblock Batched bandit problems.
\newblock \emph{Annals of Statistics}, 44\penalty0 (2):\penalty0 660--681,
  2016.

\bibitem[Qiao et~al.(2022)Qiao, Yin, Min, and Wang]{qiao2022sample}
Dan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang.
\newblock Sample-efficient reinforcement learning with loglog (t) switching
  cost.
\newblock \emph{arXiv preprint arXiv:2202.06385}, 2022.

\bibitem[Ruan et~al.(2020)Ruan, Yang, and Zhou]{ruan2020linear}
Yufei Ruan, Jiaqi Yang, and Yuan Zhou.
\newblock Linear bandits with limited adaptivity and learning distributional
  optimal design.
\newblock \emph{arXiv preprint arXiv:2007.01980}, 2020.

\bibitem[Simchi-Levi and Xu(2019)]{simchi2019phase}
David Simchi-Levi and Yunzong Xu.
\newblock Phase transitions and cyclic phenomena in bandits with switching
  constraints.
\newblock \emph{Available at SSRN 3380783}, 2019.

\bibitem[Yu et~al.(2019)Yu, Yang, Kolar, and Wang]{yu2019convergent}
Ming Yu, Zhuoran Yang, Mladen Kolar, and Zhaoran Wang.
\newblock Convergent policy optimization for safe reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.12156}, 2019.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312, 2019.

\bibitem[Zhang and Ji(2019)]{zhang2019regret}
Zihan Zhang and Xiangyang Ji.
\newblock Regret minimization for reinforcement learning by evaluating the
  optimal bias function.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2823--2832, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Ji]{zhang2020almost}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock \emph{arXiv preprint arXiv:2004.10019}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Ji, and Du]{zhang2021reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \emph{Conference on Learning Theory}, pages 4528--4531. PMLR,
  2021.

\end{thebibliography}
