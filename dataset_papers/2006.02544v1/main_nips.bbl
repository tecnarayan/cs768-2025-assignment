\begin{thebibliography}{10}

\bibitem{barber2019limits}
R.~F. Barber, E.~J. Cand{\`e}s, A.~Ramdas, and R.~J. Tibshirani.
\newblock The limits of distribution-free conditional predictive inference.
\newblock {\em arXiv preprint arXiv:1903.04684}, 2019.

\bibitem{cauchois2020knowing}
M.~Cauchois, S.~Gupta, and J.~Duchi.
\newblock Knowing what you know: valid confidence sets in multiclass and
  multilabel prediction.
\newblock {\em arXiv preprint arXiv:2004.10181}, 2020.

\bibitem{chernozhukov2019distributional}
V.~Chernozhukov, K.~W{\"u}thrich, and Y.~Zhu.
\newblock Distributional conformal prediction.
\newblock {\em arXiv preprint arXiv:1909.07889}, 2019.

\bibitem{barber2019predictive}
R.~Foygel~Barber, E.~J. Cand{\`e}s, A.~Ramdas, and R.~J. Tibshirani.
\newblock Predictive inference with the jackknife+.
\newblock {\em arXiv preprint arXiv:1905.02928}, 2019.

\bibitem{guan2019conformal}
L.~Guan.
\newblock Conformal prediction with localization.
\newblock {\em arXiv preprint arXiv:1908.08558}, 2019.

\bibitem{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1321--1330. JMLR. org, 2017.

\bibitem{hechtlinger2018cautious}
Y.~Hechtlinger, B.~P{\'o}czos, and L.~Wasserman.
\newblock Cautious deep learning.
\newblock {\em arXiv preprint arXiv:1805.09460}, 2018.

\bibitem{izbicki2019distribution}
R.~Izbicki, G.~T. Shimizu, and R.~B. Stern.
\newblock Distribution-free conditional predictive bands using density
  estimators.
\newblock {\em arXiv preprint arXiv:1910.05575}, 2019.

\bibitem{kivaranovic2019adaptive}
D.~Kivaranovic, K.~D. Johnson, and H.~Leeb.
\newblock Adaptive, distribution-free prediction intervals for deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1905.10634}, 2019.

\bibitem{kuchibhotla2019nested}
A.~K. Kuchibhotla and A.~K. Ramdas.
\newblock Nested conformal prediction and the generalized jackknife+.
\newblock {\em arXiv preprint arXiv:1910.10562}, 2019.

\bibitem{kumar2019verified}
A.~Kumar, P.~S. Liang, and T.~Ma.
\newblock Verified uncertainty calibration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3787--3798, 2019.

\bibitem{lei2018distribution}
J.~Lei, M.~G'Sell, A.~Rinaldo, R.~J. Tibshirani, and L.~Wasserman.
\newblock Distribution-free predictive inference for regression.
\newblock {\em Journal of the American Statistical Association},
  113(523):1094--1111, 2018.

\bibitem{lei2014distribution}
J.~Lei and L.~Wasserman.
\newblock Distribution-free prediction bands for non-parametric regression.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 76(1):71--96, 2014.

\bibitem{meinshausen2006quantile}
N.~Meinshausen.
\newblock Quantile regression forests.
\newblock {\em Journal of Machine Learning Research}, 7:983--999, 2006.

\bibitem{neumann2018relaxed}
L.~Neumann, A.~Zisserman, and A.~Vedaldi.
\newblock Relaxed softmax: Efficient confidence auto-calibration for safe
  pedestrian detection.
\newblock 2018.

\bibitem{platt1999probabilistic}
J.~Platt.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock {\em Advances in large margin classifiers}, 10(3):61--74, 1999.

\bibitem{Romano2020With}
Y.~Romano, R.~F. Barber, C.~Sabatti, and E.~Cand√®s.
\newblock With malice toward none: Assessing uncertainty via equalized
  coverage.
\newblock {\em Harvard Data Science Review}, 4 2020.
\newblock https://hdsr.mitpress.mit.edu/pub/qedrwcz3.

\bibitem{romano2019conformalized}
Y.~Romano, E.~Patterson, and E.~J. Cand{\`e}s.
\newblock Conformalized quantile regression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3538--3548, 2019.

\bibitem{sadinle2019least}
M.~Sadinle, J.~Lei, and L.~Wasserman.
\newblock Least ambiguous set-valued classifiers with bounded error levels.
\newblock {\em Journal of the American Statistical Association},
  114(525):223--234, 2019.

\bibitem{sesia2019comparison}
M.~Sesia and E.~J. Cand{\`e}s.
\newblock A comparison of some conformal quantile regression methods.
\newblock {\em Stat}, 9(1):e261, 2020.

\bibitem{taylor2000quantile}
J.~W. Taylor.
\newblock A quantile regression neural network approach to estimating the
  conditional density of multiperiod returns.
\newblock {\em Journal of Forecasting}, 19(4):299--311, 2000.

\bibitem{vaicenavicius2019evaluating}
J.~Vaicenavicius, D.~Widmann, C.~Andersson, F.~Lindsten, J.~Roll, and T.~B.
  Sch{\"o}n.
\newblock Evaluating model calibration in classification.
\newblock {\em arXiv preprint arXiv:1902.06977}, 2019.

\bibitem{vovk2012conditional}
V.~Vovk.
\newblock Conditional validity of inductive conformal predictors.
\newblock In {\em Asian conference on machine learning}, pages 475--490, 2012.

\bibitem{vovk2005algorithmic}
V.~Vovk, A.~Gammerman, and G.~Shafer.
\newblock {\em Algorithmic learning in a random world}.
\newblock Springer, 2005.

\bibitem{vovk2003mondrian}
V.~Vovk, D.~Lindsay, I.~Nouretdinov, and A.~Gammerman.
\newblock Mondrian confidence machine.
\newblock Technical report, Royal Holloway, University of London, 2003.
\newblock On-line Compression Modelling project.

\bibitem{vovk2009line}
V.~Vovk, I.~Nouretdinov, and A.~Gammerman.
\newblock On-line predictive linear regression.
\newblock {\em The Annals of Statistics}, 37(3):1566--1590, 2009.

\bibitem{zadrozny2001obtaining}
B.~Zadrozny and C.~Elkan.
\newblock Obtaining calibrated probability estimates from decision trees and
  naive bayesian classifiers.
\newblock In {\em Icml}, volume~1, pages 609--616. Citeseer, 2001.

\bibitem{zadrozny2002transforming}
B.~Zadrozny and C.~Elkan.
\newblock Transforming classifier scores into accurate multiclass probability
  estimates.
\newblock In {\em Proceedings of the eighth ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 694--699, 2002.

\end{thebibliography}
