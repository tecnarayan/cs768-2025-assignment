@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={ICCV},
  year={2015}
}

@inproceedings{shankar2020evaluating,
  title={Evaluating machine accuracy on imagenet},
  author={Shankar, Vaishaal and Roelofs, Rebecca and Mania, Horia and Fang, Alex and Recht, Benjamin and Schmidt, Ludwig},
  booktitle={ICML},
  year={2020}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT press Cambridge}
}

@inproceedings{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{jetley2018friends,
  title={With friends like these, who needs adversaries?},
  author={Jetley, Saumya and Lord, Nicholas and Torr, Philip},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{maini2020adversarial,
  title={Adversarial robustness against the union of multiple perturbation models},
  author={Maini, Pratyush and Wong, Eric and Kolter, Zico},
  booktitle={ICML},
  year={2020},
}

@inproceedings{uesato2018adversarial,
  title={Adversarial Risk and the Dangers of Evaluating Against Weak Attacks},
  author={Uesato, Jonathan and O’Donoghue, Brendan and Kohli, Pushmeet and Oord, Aaron},
  booktitle={ICML},
  year={2018}
}

@inproceedings{su2018robustness,
  title={Is Robustness the Cost of Accuracy?--A Comprehensive Study on the Robustness of 18 Deep Image Classification Models},
  author={Su, Dong and Zhang, Huan and Chen, Hongge and Yi, Jinfeng and Chen, Pin-Yu and Gao, Yupeng},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{tsipras2018robustness,
  title={Robustness May Be at Odds with Accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{yang2020closer,
  title={A closer look at accuracy vs. robustness},
  author={Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Russ R and Chaudhuri, Kamalika},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{kumar2020adversarial,
  title={Adversarial machine learning-industry perspectives},
  author={Kumar, Ram Shankar Siva and Nystr{\"o}m, Magnus and Lambert, John and Marshall, Andrew and Goertzel, Mario and Comissoneru, Andi and Swann, Matt and Xia, Sharon},
  booktitle={IEEE Security and Privacy Workshops (SPW)},
  year={2020},
}

@inproceedings{newsome2006paragraph,
  title={Paragraph: Thwarting signature learning by training maliciously},
  author={Newsome, James and Karp, Brad and Song, Dawn},
  booktitle={International Workshop on Recent Advances in Intrusion Detection},
  year={2006},
}

@article{barreno2010security,
  title={The security of machine learning},
  author={Barreno, Marco and Nelson, Blaine and Joseph, Anthony D and Tygar, J Doug},
  journal={Machine Learning},
  volume={81},
  number={2},
  pages={121--148},
  year={2010},
  publisher={Springer}
}

@article{biggio2018wild,
  title={Wild patterns: Ten years after the rise of adversarial machine learning},
  author={Biggio, Battista and Roli, Fabio},
  journal={Pattern Recognition},
  volume={84},
  pages={317--331},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{feng2019learning,
  title={Learning to confuse: generating training time adversarial data with auto-encoder},
  author={Feng, Ji and Cai, Qi-Zhi and Zhou, Zhi-Hua},
  booktitle={NeurIPS},
  year={2019}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{sinha2018certifiable,
title={Certifiable Distributional Robustness with Principled Adversarial Training},
author={Aman Sinha and Hongseok Namkoong and John Duchi},
booktitle={ICLR},
year={2018},
}

@inproceedings{staib2017distributionally,
  title={Distributionally robust deep learning as a generalization of adversarial training},
  author={Staib, Matthew and Jegelka, Stefanie},
  booktitle={NeurIPS workshop on Machine Learning and Computer Security},
  year={2017}
}

@InProceedings{zhu2020learning,
  title={Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization},
  author={Zhu, Sicheng and Zhang, Xiao and Evans, David},
  booktitle={ICML},
  year={2020}
}

@article{xu2020robust,
  title={To be Robust or to be Fair: Towards Fairness in Adversarial Training},
  author={Xu, Han and Liu, Xiaorui and Li, Yaxin and Tang, Jiliang},
  journal={arXiv preprint arXiv:2010.06121},
  year={2020}
}

@article{dobriban2020provable,
  title={Provable tradeoffs in adversarially robust classification},
  author={Dobriban, Edgar and Hassani, Hamed and Hong, David and Robey, Alexander},
  journal={arXiv preprint arXiv:2006.05161},
  year={2020}
}

@article{nakkiran2019a,
  author = {Nakkiran, Preetum},
  title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Examples are Just Bugs, Too},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/advex-bugs-discussion/response-5},
  doi = {10.23915/distill.00019.5}
}

@inproceedings{salman2021unadversarial,
  title={Unadversarial Examples: Designing Objects for Robust Vision},
  author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Vemprala, Sai and Madry, Aleksander and Kapoor, Ashish},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{tao2020false,
  title={With False Friends Like These, Who Can Notice Mistakes?},
  author={Lue Tao and Lei Feng and Jinfeng Yi and Songcan Chen},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{moosavi2017universal,
  title={Universal adversarial perturbations},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
  booktitle={CVPR},
  year={2017}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@inproceedings{shah2020pitfalls,
  title={The pitfalls of simplicity bias in neural networks},
  author={Shah, Harshay and Tamuly, Kaustav and Raghunathan, Aditi and Jain, Prateek and Netrapalli, Praneeth},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{gidaris2018unsupervised,
  title={Unsupervised representation learning by predicting image rotations},
  author={Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={ICLR},
  year={2015}
}

@inproceedings{pezeshki2020gradient,
  title={Gradient Starvation: A Learning Proclivity in Neural Networks},
  author={Pezeshki, Mohammad and Kaba, S{\'e}kou-Oumar and Bengio, Yoshua and Courville, Aaron and Precup, Doina and Lajoie, Guillaume},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{hermann2020shapes,
  title={What shapes feature representations? Exploring datasets, architectures, and training},
  author={Hermann, Katherine L and Lampinen, Andrew K},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{biggio2013evasion,
  title={Evasion attacks against machine learning at test time},
  author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and {\v{S}}rndi{\'c}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
  booktitle={ECML-KDD},
  year={2013},
}

@inproceedings{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  booktitle={ICLR},
  year={2014}
}

@inproceedings{cohen2019certified,
  title={Certified Adversarial Robustness via Randomized Smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={ICML},
  year={2019}
}

@inproceedings{salman2019provably,
  title={Provably robust deep learning via adversarially trained smoothed classifiers},
  author={Salman, Hadi and Li, Jerry and Razenshteyn, Ilya and Zhang, Pengchuan and Zhang, Huan and Bubeck, Sebastien and Yang, Greg},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{xie2020adversarial,
  title={Adversarial examples improve image recognition},
  author={Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan L and Le, Quoc V},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{lin2020dual,
  title={Dual Manifold Adversarial Robustness: Defense against Lp and non-Lp Adversarial Attacks},
  author={Lin, Wei-An and Lau, Chun Pong and Levine, Alexander and Chellappa, Rama and Feizi, Soheil},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{schwarzschild2021just,
  title={Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks},
  author={Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom},
  booktitle={ICML},
  year={2021},
}

@inproceedings{nelson2008exploiting,
  title={Exploiting machine learning to subvert your spam filter},
  author={Nelson, Blaine and Barreno, Marco and Chi, Fuching Jack and Joseph, Anthony D and Rubinstein, Benjamin IP and Saini, Udam and Sutton, Charles and Tygar, JD and Xia, Kai},
  booktitle={Usenix Workshop on Large-Scale Exploits and Emergent Threats},
  year={2008}
}

@inproceedings{biggio2011support,
  title={Support vector machines under adversarial label noise},
  author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  booktitle={ACML},
  year={2011}
}

@inproceedings{biggio2012poisoning,
  title={Poisoning attacks against support vector machines},
  author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  booktitle={ICML},
  year={2012}
}

@inproceedings{xiao2015feature,
  title={Is feature selection secure against training data poisoning?},
  author={Xiao, Huang and Biggio, Battista and Brown, Gavin and Fumera, Giorgio and Eckert, Claudia and Roli, Fabio},
  booktitle={ICML},
  year={2015}
}

@inproceedings{zhao2017efficient,
  title={Efficient label contamination attacks against black-box learning models},
  author={Zhao, Mengchen and An, Bo and Gao, Wei and Zhang, Teng},
  booktitle={IJCAI},
  year={2017}
}

@inproceedings{shafahi2018poison,
  title={Poison frogs! targeted clean-label poisoning attacks on neural networks},
  author={Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{huang2020metapoison,
  title={MetaPoison: Practical General-purpose Clean-label Data Poisoning},
  author={Huang, W Ronny and Geiping, Jonas and Fowl, Liam and Taylor, Gavin and Goldstein, Tom},
  booktitle={NeurIPS},
  year={2020}
}

@article{geiping2020witches,
  title={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},
  author={Geiping, Jonas and Fowl, Liam and Huang, W Ronny and Czaja, Wojciech and Taylor, Gavin and Moeller, Michael and Goldstein, Tom},
  journal={arXiv preprint arXiv:2009.02276},
  year={2020}
}

@inproceedings{zhu2019transferable,
  title={Transferable Clean-Label Poisoning Attacks on Deep Neural Nets},
  author={Zhu, Chen and Huang, W Ronny and Li, Hengduo and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={ICML},
  year={2019}
}

@inproceedings{chacon2019deep,
  title={Deep learning poison data attack detection},
  author={Chacon, Henry and Silva, Samuel and Rad, Paul},
  booktitle={ICTAI},
  year={2019},
}

@article{koh2018stronger,
  title={Stronger data poisoning attacks break data sanitization defenses},
  author={Koh, Pang Wei and Steinhardt, Jacob and Liang, Percy},
  journal={arXiv preprint arXiv:1811.00741},
  year={2018}
}

@article{farokhi2020regularization,
  title={Regularization Helps with Mitigating Poisoning Attacks: Distributionally-Robust Machine Learning Using the Wasserstein Distance},
  author={Farokhi, Farhad},
  journal={arXiv preprint arXiv:2001.10655},
  year={2020}
}

@article{gama2014survey,
  title={A survey on concept drift adaptation},
  author={Gama, Jo{\~a}o and {\v{Z}}liobait{\.e}, Indr{\.e} and Bifet, Albert and Pechenizkiy, Mykola and Bouchachia, Abdelhamid},
  journal={ACM computing surveys (CSUR)},
  volume={46},
  number={4},
  pages={1--37},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={IJCV},
  year={2015},
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={NeurIPS},
  year={2012}
}

@inproceedings{graves2013speech,
  title={Speech recognition with deep recurrent neural networks},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={ICASSP},
  year={2013},
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{elsayed2018adversarial,
  title={Adversarial examples that fool both computer vision and time-limited humans},
  author={Elsayed, Gamaleldin F and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alex and Goodfellow, Ian and Sohl-Dickstein, Jascha},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{kurakin2016adversarial,
  title={Adversarial machine learning at scale},
  author={Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{salman2020adversarially,
  title={Do adversarially robust imagenet models transfer better?},
  author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={ICML},
  year={2020},
}

@article{jing2020self,
  title={Self-supervised visual feature learning with deep neural networks: A survey},
  author={Jing, Longlong and Tian, Yingli},
  journal={TPAMI},
  year={2020},
}

@inproceedings{lechner2021adversarial,
  title={Adversarial Training is Not Ready for Robot Learning},
  author={Lechner, Mathias and Hasani, Ramin and Grosu, Radu and Rus, Daniela and Henzinger, Thomas A},
  booktitle={ICRA},
  year={2021}
}


@inproceedings{kandel2011wrangler,
  title={Wrangler: Interactive visual specification of data transformation scripts},
  author={Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
  booktitle={CHI Conference on Human Factors in Computing Systems (CHI)},
  year={2011}
}

@inproceedings{tran2018spectral,
  title={Spectral signatures in backdoor attacks},
  author={Tran, Brandon and Li, Jerry and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{dietterich2002machine,
  title={Machine learning for sequential data: A review},
  author={Dietterich, Thomas G},
  booktitle={Joint IAPR international workshops on statistical techniques in pattern recognition (SPR) and structural and syntactic pattern recognition (SSPR)},
  year={2002},
}

@inproceedings{huang2021unlearnable,
  title={Unlearnable Examples: Making Personal Data Unexploitable},
  author={Huang, Hanxun and Ma, Xingjun and Erfani, Sarah Monazam and Bailey, James and Wang, Yisen},
  booktitle={ICLR},
  year={2021}
}

@article{liang2020does,
  title={Does Adversarial Transferability Indicate Knowledge Transferability?},
  author={Liang, Kaizhao and Zhang, Jacky Y and Koyejo, Oluwasanmi and Li, Bo},
  journal={arXiv preprint arXiv:2006.14512},
  year={2020}
}

@inproceedings{pang2020bag,
  title={Bag of tricks for adversarial training},
  author={Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{wong2018provable,
  title={Provable defenses against adversarial examples via the convex outer adversarial polytope},
  author={Wong, Eric and Kolter, Zico},
  booktitle={ICML},
  year={2018},
}

@inproceedings{zhang2020geometry,
  title={Geometry-aware instance-reweighted adversarial training},
  author={Zhang, Jingfeng and Zhu, Jianing and Niu, Gang and Han, Bo and Sugiyama, Masashi and Kankanhalli, Mohan},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{cai2018curriculum,
  title={Curriculum adversarial training},
  author={Cai, Qi-Zhi and Liu, Chang and Song, Dawn},
  booktitle={IJCAI},
  year={2018}
}

@inproceedings{wang2019convergence,
  title={On the Convergence and Robustness of Adversarial Training.},
  author={Wang, Yisen and Ma, Xingjun and Bailey, James and Yi, Jinfeng and Zhou, Bowen and Gu, Quanquan},
  booktitle={ICML},
  year={2019}
}

@inproceedings{zhang2020attacks,
  title={Attacks which do not kill training make adversarial learning stronger},
  author={Zhang, Jingfeng and Xu, Xilie and Han, Bo and Niu, Gang and Cui, Lizhen and Sugiyama, Masashi and Kankanhalli, Mohan},
  booktitle={ICML},
  year={2020},
}

@inproceedings{croce2021robustbench,
title={RobustBench: a standardized adversarial robustness benchmark},
author={Francesco Croce and Maksym Andriushchenko and Vikash Sehwag and Edoardo Debenedetti and Nicolas Flammarion and Mung Chiang and Prateek Mittal and Matthias Hein},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
year={2021},
url={https://openreview.net/forum?id=SSKZPJCt7B}
}

@inproceedings{tian2021analysis,
  title={Analysis and Applications of Class-wise Robustness in Adversarial Training},
  author={Tian, Qi and Kuang, Kun and Jiang, Kelu and Wu, Fei and Wang, Yisen},
  booktitle={KDD},
  year={2021}
}

@article{goldblum2020dataset,
  title={Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses},
  author={Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Madry, Aleksander and Li, Bo and Goldstein, Tom},
  journal={arXiv preprint arXiv:2012.10544},
  year={2020}
}

@article{shen2019tensorclog,
  title={TensorClog: An imperceptible poisoning attack on deep neural network applications},
  author={Shen, Juncheng and Zhu, Xiaolei and Ma, De},
  journal={IEEE Access},
  volume={7},
  pages={41498--41506},
  year={2019},
  publisher={IEEE}
}

@inproceedings{globerson2006nightmare,
  title={Nightmare at test time: robust learning by feature deletion},
  author={Globerson, Amir and Roweis, Sam},
  booktitle={ICML},
  year={2006}
}

@inproceedings{mei2015using,
  title={Using machine teaching to identify optimal training-set attacks on machine learners},
  author={Mei, Shike and Zhu, Xiaojin},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2015}
}

@InProceedings{pmlr-v139-liang21b,
  title = 	 {Uncovering the Connections Between Adversarial Transferability and Knowledge Transferability},
  author =       {Liang, Kaizhao and Zhang, Jacky Y and Wang, Boxin and Yang, Zhuolin and Koyejo, Sanmi and Li, Bo},
  booktitle={ICML},
  year = 	 {2021},
}

@inproceedings{utrera2021adversariallytrained,
title={Adversarially-Trained Deep Nets Transfer Better: Illustration on Image Classification},
author={Francisco Utrera and Evan Kravitz and N. Benjamin Erichson and Rajiv Khanna and Michael W. Mahoney},
booktitle={ICLR},
year={2021},
}

@inproceedings{deng2021adversarial,
  title={Adversarial Training Helps Transfer Learning via Better Representations},
  author={Deng, Zhun and Zhang, Linjun and Vodrahalli, Kailas and Kawaguchi, Kenji and Zou, James},
  booktitle={NeurIPS},
  year={2021}
}


@InProceedings{pmlr-v139-yi21a,
  title = 	 {Improved OOD Generalization via Adversarial Training and Pretraing},
  author =       {Yi, Mingyang and Hou, Lu and Sun, Jiacheng and Shang, Lifeng and Jiang, Xin and Liu, Qun and Ma, Zhiming},
  booktitle={ICML},
  year = 	 {2021},
}

@inproceedings{barreno2006can,
  title={Can machine learning be secure?},
  author={Barreno, Marco and Nelson, Blaine and Sears, Russell and Joseph, Anthony D and Tygar, J Doug},
  booktitle={Proceedings of the 2006 ACM Symposium on Information, computer and communications security},
  year={2006}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={ICML},
  year={2017},
}

@inproceedings{pang2021accumulative,
  title={Accumulative Poisoning Attacks on Real-time Data},
  author={Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
  booktitle={NeurIPS},
  year={2021}
}

@article{carlini2021poisoning,
  title={Poisoning and Backdooring Contrastive Learning},
  author={Carlini, Nicholas and Terzis, Andreas},
  journal={arXiv preprint arXiv:2106.09667},
  year={2021}
}

@article{saha2021backdoor,
  title={Backdoor Attacks on Self-Supervised Learning},
  author={Saha, Aniruddha and Tejankar, Ajinkya and Koohpayegani, Soroush Abbasi and Pirsiavash, Hamed},
  journal={arXiv preprint arXiv:2105.10123},
  year={2021}
}

@inproceedings{mehra2021effectiveness,
  title={Understanding the Limits of Unsupervised Domain Adaptation via Data Poisoning},
  author={Mehra, Akshay and Kailkhura, Bhavya and Chen, Pin-Yu and Hamm, Jihun},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{liu2019unified,
  title={A unified framework for data poisoning attack to graph-based semi-supervised learning},
  author={Liu, Xuanqing and Si, Si and Zhu, Xiaojin and Li, Yang and Hsieh, Cho-Jui},
  booktitle={NeurIPS},
  year={2019}
}

@article{franci2020effective,
  title={Effective and Efficient Data Poisoning in Semi-Supervised Learning},
  author={Franci, Adriano and Cordy, Maxime and Gubri, Martin and Papadakis, Mike and Traon, Yves Le},
  journal={arXiv preprint arXiv:2012.07381},
  year={2020}
}

@inproceedings {274598,
author = {Nicholas Carlini},
title = {Poisoning the Unlabeled Dataset of Semi-Supervised Learning},
booktitle = {{USENIX} Security Symposium},
year = {2021},
}

@inproceedings{anonymous2022adversarially,
title={Adversarially Robust Models may not Transfer Better: Sufficient Conditions for Domain Transferability from the View of Regularization},
author={Anonymous},
booktitle={Submitted to The Tenth ICLR},
year={2022},
url={https://openreview.net/forum?id=_ixHFNR-FZ},
note={under review}
}

@article{aghakhani2020bullseye,
  title={Bullseye polytope: A scalable clean-label poisoning attack with improved transferability},
  author={Aghakhani, Hojjat and Meng, Dongyu and Wang, Yu-Xiang and Kruegel, Christopher and Vigna, Giovanni},
  journal={arXiv preprint arXiv:2005.00191},
  year={2020}
}

@article{gu2019badnets,
  title={Badnets: Evaluating backdooring attacks on deep neural networks},
  author={Gu, Tianyu and Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
  journal={IEEE Access},
  volume={7},
  pages={47230--47244},
  year={2019},
}

@article{chen2017targeted,
  title={Targeted backdoor attacks on deep learning systems using data poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  journal={arXiv preprint arXiv:1712.05526},
  year={2017}
}

@inproceedings{Trojannn,
  author    = {Yingqi Liu and
               Shiqing Ma and
               Yousra Aafer and
               Wen-Chuan Lee and
               Juan Zhai and
               Weihang Wang and
               Xiangyu Zhang},
  title     = {Trojaning Attack on Neural Networks},
  booktitle = {Annual Network and Distributed System Security Symposium (NDSS)},
  year      = {2018},
}

@inproceedings{nguyen2020input,
  title={Input-Aware Dynamic Backdoor Attack},
  author={Nguyen, Tuan Anh and Tran, Anh},
  booktitle={NeurIPS},
  year={2020}
}

@article{turner2019label,
  title={Label-consistent backdoor attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  journal={arXiv preprint arXiv:1912.02771},
  year={2019}
}

@inproceedings{liu2020reflection,
  title={Reflection backdoor: A natural backdoor attack on deep neural networks},
  author={Liu, Yunfei and Ma, Xingjun and Bailey, James and Lu, Feng},
  booktitle={ECCV},
  year={2020},
}

@inproceedings{nguyen2021wanet,
title={WaNet - Imperceptible Warping-based Backdoor Attack},
author={Tuan Anh Nguyen and Anh Tuan Tran},
booktitle={ICLR},
year={2021},
}

@inproceedings{pang2020tale,
  title={A tale of evil twins: Adversarial inputs versus poisoned models},
  author={Pang, Ren and Shen, Hua and Zhang, Xinyang and Ji, Shouling and Vorobeychik, Yevgeniy and Luo, Xiapu and Liu, Alex and Wang, Ting},
  booktitle={CCS},
  year={2020}
}

@inproceedings{steinhardt2017certified,
  title={Certified defenses for data poisoning attacks},
  author={Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{diakonikolas2019sever,
  title={Sever: A robust meta-algorithm for stochastic optimization},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Steinhardt, Jacob and Stewart, Alistair},
  booktitle={ICML},
  year={2019},
}

@inproceedings{peri2020deep,
author="Peri, Neehar
and Gupta, Neal
and Huang, W. Ronny
and Fowl, Liam
and Zhu, Chen
and Feizi, Soheil
and Goldstein, Tom
and Dickerson, John P.",
editor="Bartoli, Adrien
and Fusiello, Andrea",
title="Deep k-NN Defense Against Clean-Label Data Poisoning Attacks",
booktitle="ECCV Workshop",
year="2020",
}

@article{chen2018detecting,
  title={Detecting backdoor attacks on deep neural networks by activation clustering},
  author={Chen, Bryant and Carvalho, Wilka and Baracaldo, Nathalie and Ludwig, Heiko and Edwards, Benjamin and Lee, Taesung and Molloy, Ian and Srivastava, Biplav},
  journal={arXiv preprint arXiv:1811.03728},
  year={2018}
}

@inproceedings{gao2019strip,
  title={Strip: A defence against trojan attacks on deep neural networks},
  author={Gao, Yansong and Xu, Change and Wang, Derui and Chen, Shiping and Ranasinghe, Damith C and Nepal, Surya},
  booktitle={Annual Computer Security Applications Conference},
  year={2019}
}

@inproceedings{pmlr-v139-hayase21a,
  title = 	 {SPECTRE: defending against backdoor attacks using robust statistics},
  author =       {Hayase, Jonathan and Kong, Weihao and Somani, Raghav and Oh, Sewoong},
  booktitle = 	 {ICML},
  year = 	 {2021},
}

@inproceedings{levine2021deep,
title={Deep Partition Aggregation: Provable Defenses against General Poisoning Attacks},
author={Alexander Levine and Soheil Feizi},
booktitle={ICLR},
year={2021},
}

@inproceedings{rosenfeld2020certified,
  title={Certified robustness to label-flipping attacks via randomized smoothing},
  author={Rosenfeld, Elan and Winston, Ezra and Ravikumar, Pradeep and Kolter, Zico},
  booktitle={ICML},
  year={2020},
}

@article{weber2020rab,
  title={Rab: Provable robustness against backdoor attacks},
  author={Weber, Maurice and Xu, Xiaojun and Karla{\v{s}}, Bojan and Zhang, Ce and Li, Bo},
  journal={arXiv preprint arXiv:2003.08904},
  year={2020}
}

@inproceedings{ma2019data,
  title={Data Poisoning against Differentially-Private Learners: Attacks and Defenses},
  author={Ma, Yuzhe and Zhu, Xiaojin Zhu and Hsu, Justin},
  booktitle={IJCAI},
  year={2019}
}

@inproceedings{borgnia2021strong,
  title={Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff},
  author={Borgnia, Eitan and Cherepanova, Valeriia and Fowl, Liam and Ghiasi, Amin and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Gupta, Arjun},
  booktitle={ICASSP},
  year={2021},
}

@inproceedings{Chen2021REFIT,
author = {Chen, Xinyun and Wang, Wenxiao and Bender, Chris and Ding, Yiming and Jia, Ruoxi and Li, Bo and Song, Dawn},
title = {REFIT: A Unified Watermark Removal Framework For Deep Learning Systems With Limited Data},
year = {2021},
booktitle = {ACM Asia Conference on Computer and Communications Security},
}

@inproceedings{li2021neural,
title={Neural Attention Distillation: Erasing Backdoor Triggers from Deep Neural Networks},
author={Yige Li and Xixiang Lyu and Nodens Koren and Lingjuan Lyu and Bo Li and Xingjun Ma},
booktitle={ICLR},
year={2021},
}

@inproceedings{wu2020adversarial,
 title = {Adversarial Weight Perturbation Helps Robust Generalization},
 author = {Wu, Dongxian and Xia, Shu-Tao and Wang, Yisen},
 booktitle = {NeurIPS},
 year = {2020}
}

@inproceedings{wu2021Adversarial,
 title = {Adversarial Neuron Pruning Purifies Backdoored Deep Models},
 author = {Wu, Dongxian and Wang, Yisen},
 booktitle = {NeurIPS},
 year = {2021}
}

@inproceedings{shokri2020bypassing,
  title={Bypassing backdoor detection algorithms in deep learning},
  author={Shokri, Reza and others},
  booktitle={EuroS\&P},
  year={2020},
}

@article{veldanda2020evaluating,
  title={On Evaluating Neural Network Backdoor Defenses},
  author={Veldanda, Akshaj and Garg, Siddharth},
  journal={arXiv preprint arXiv:2010.12186},
  year={2020}
}

@article{hong2020effectiveness,
  title={On the effectiveness of mitigating data poisoning attacks with gradient shaping},
  author={Hong, Sanghyun and Chandrasekaran, Varun and Kaya, Yi{\u{g}}itcan and Dumitra{\c{s}}, Tudor and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2002.11497},
  year={2020}
}

@InProceedings{pmlr-v134-blum21a,
  title = 	 {Robust learning under clean-label attack},
  author =       {Blum, Avrim and Hanneke, Steve and Qian, Jian and Shao, Han},
  booktitle = 	 {COLT},
  year = 	 {2021},
}

@InProceedings{pmlr-v139-wang21r,
  title = 	 {Robust Learning for Data Poisoning Attacks},
  author =       {Wang, Yunjuan and Mianjy, Poorya and Arora, Raman},
  booktitle = 	 {ICML},
  year = 	 {2021},
}

@InProceedings{gao2021learning,
  title={Learning and Certification under Instance-targeted Poisoning},
  author={Gao, Ji and Karbasi, Amin and Mahmoody, Mohammad},
  booktitle = 	 {UAI},
  year={2021}
}

@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={IEEE symposium on security and privacy (SP)},
  year={2016},
}

@inproceedings{athalye2018obfuscated,
  title={Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  booktitle={ICML},
  year={2018},
}

@inproceedings{shan2020fawkes,
  title={Fawkes: Protecting privacy against unauthorized deep learning models},
  author={Shan, Shawn and Wenger, Emily and Zhang, Jiayun and Li, Huiying and Zheng, Haitao and Zhao, Ben Y},
booktitle = {{USENIX} Security Symposium},
  year={2020}
}

@inproceedings{cherepanova2021lowkey,
title={LowKey: Leveraging Adversarial Attacks to Protect Social Media Users from Facial Recognition},
author={Valeriia Cherepanova and Micah Goldblum and Harrison Foley and Shiyuan Duan and John P Dickerson and Gavin Taylor and Tom Goldstein},
booktitle={ICLR},
year={2021},
}

@inproceedings{radiya2021data,
  title={Data Poisoning Won’t Save You From Facial Recognition},
  author={Radiya-Dixit, Evani and Tramer, Florian},
  booktitle={ICML 2021 Workshop on Adversarial Machine Learning},
  year={2021}
}

@inproceedings{schmidt2018adversarially,
  title={Adversarially Robust Generalization Requires More Data},
  author={Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{rice2020overfitting,
  title={Overfitting in adversarially robust deep learning},
  author={Rice, Leslie and Wong, Eric and Kolter, Zico},
  booktitle={ICML},
  year={2020},
}

@inproceedings{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael},
  booktitle={ICML},
  year={2019},
}

@inproceedings{wang2019improving,
  title={Improving adversarial robustness requires revisiting misclassified examples},
  author={Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{gao2021maximum,
  title={Maximum mean discrepancy test is aware of adversarial attacks},
  author={Gao, Ruize and Liu, Feng and Zhang, Jingfeng and Han, Bo and Liu, Tongliang and Niu, Gang and Sugiyama, Masashi},
  booktitle={ICML},
  year={2021},
}

@inproceedings{tack2021consistency,
title={Consistency Regularization for Adversarial Robustness},
author={Jihoon Tack and Sihyun Yu and Jongheon Jeong and Minseon Kim and Sung Ju Hwang and Jinwoo Shin},
booktitle={ICML 2021 Workshop on Adversarial Machine Learning},
year={2021},
}

@inproceedings{du2021learning,
  title={Learning diverse-structured networks for adversarial robustness},
  author={Du, Xuefeng and Zhang, Jingfeng and Han, Bo and Liu, Tongliang and Rong, Yu and Niu, Gang and Huang, Junzhou and Sugiyama, Masashi},
  booktitle={ICML},
  year={2021}
}

@inproceedings{huang2021exploring,
  title={Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks},
  author={Huang, Hanxun and Wang, Yisen and Erfani, Sarah Monazam and Gu, Quanquan and Bailey, James and Ma, Xingjun},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{wang2021probabilistic,
  title={Probabilistic Margins for Instance Reweighting in Adversarial Training},
  author={Wang, Qizhou and Liu, Feng and Han, Bo and Liu, Tongliang and Gong, Chen and Niu, Gang and Zhou, Mingyuan and Sugiyama, Masashi},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{dong2020adversarial,
  title={Adversarial Distributional Training for Robust Deep Learning},
  author={Dong, Yinpeng and Deng, Zhijie and Pang, Tianyu and Zhu, Jun and Su, Hang},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{gowal2021improving,
      title={Improving Robustness using Generated Data}, 
      author={Sven Gowal and Sylvestre-Alvise Rebuffi and Olivia Wiles and Florian Stimberg and Dan Andrei Calian and Timothy Mann},
  booktitle={NeurIPS},
      year={2021},
}

@inproceedings{dong2021black,
  title={Black-box Detection of Backdoor Attacks with Limited Information and Data},
  author={Dong, Yinpeng and Yang, Xiao and Deng, Zhijie and Pang, Tianyu and Xiao, Zihao and Su, Hang and Zhu, Jun},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{weng2020trade,
  title={On the Trade-off between Adversarial and Backdoor Robustness},
  author={Weng, Cheng-Hsin and Lee, Yan-Ting and Wu, Shan-Hung Brandon},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{zhang2019interpreting,
  title={Interpreting adversarially trained convolutional neural networks},
  author={Zhang, Tianyuan and Zhu, Zhanxing},
  booktitle={ICML},
  year={2019},
}

@article{noack2021empirical,
  title={An Empirical Study on the Relation Between Network Interpretability and Adversarial Robustness},
  author={Noack, Adam and Ahern, Isaac and Dou, Dejing and Li, Boyang},
  journal={SN Computer Science},
  year={2021},
}

@inproceedings{terzi2021adversarial,
  title={Adversarial Training Reduces Information and Improves Transferability},
  author={Terzi, Matteo and Achille, Alessandro and Maggipinto, Marco and Susto, Gian Antonio},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{santurkar2019image,
  title={Image Synthesis with a Single (Robust) Classifier},
  author={Santurkar, Shibani and Ilyas, Andrew and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  booktitle={NeurIPS},
  year={2019}
}

@article{kireev2021effectiveness,
  title={On the effectiveness of adversarial training against common corruptions},
  author={Kireev, Klim and Andriushchenko, Maksym and Flammarion, Nicolas},
  journal={ICLR RobustML Workshop},
  year={2021}
}

@article{zhu2021understanding,
  title={Understanding the Interaction of Adversarial Training with Noisy Labels},
  author={Zhu, Jianing and Zhang, Jingfeng and Han, Bo and Liu, Tongliang and Niu, Gang and Yang, Hongxia and Kankanhalli, Mohan and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2102.03482},
  year={2021}
}

@inproceedings{goyal2020drocc,
  title={DROCC: Deep robust one-class classification},
  author={Goyal, Sachin and Raghunathan, Aditi and Jain, Moksh and Simhadri, Harsha Vardhan and Jain, Prateek},
  booktitle={ICML},
  year={2020},
}

@article{salehi2021arae,
  title={Arae: Adversarially robust training of autoencoders improves novelty detection},
  author={Salehi, Mohammadreza and Arya, Atrin and Pajoum, Barbod and Otoofi, Mohammad and Shaeiri, Amirreza and Rohban, Mohammad Hossein and Rabiee, Hamid R},
  journal={Neural Networks},
  year={2021},
}

@inproceedings{bai2021clustering,
 title = {Clustering Effect of Adversarial Robust Models},
 author = {Bai, Yang and Yan, Xin and Jiang, Yong and Xia, Shu-Tao and Wang, Yisen},
 booktitle = {NeurIPS},
 year = {2021}
}

@article{fowl2021preventing,
  title={Preventing unauthorized use of proprietary data: Poisoning for secure dataset release},
  author={Fowl, Liam and Chiang, Ping-yeh and Goldblum, Micah and Geiping, Jonas and Bansal, Arpit and Czaja, Wojtek and Goldstein, Tom},
  journal={arXiv preprint arXiv:2103.02683},
  year={2021}
}

@inproceedings{evtimov2021disrupting,
  title={Disrupting Model Training with Adversarial Shortcuts},
  author={Evtimov, Ivan and Covert, Ian and Kusupati, Aditya and Kohno, Tadayoshi},
  booktitle={ICML Workshop},
  year={2021}
}

@InProceedings{pmlr-v139-yuan21b,
  title = 	 {Neural Tangent Generalization Attacks},
  author =       {Yuan, Chia-Hung and Wu, Shan-Hung},
  booktitle = 	 {ICML},
  year = 	 {2021},
}

@inproceedings{fowl2021adversarial,
  title={Adversarial Examples Make Strong Poisons},
  author={Fowl, Liam and Goldblum, Micah and Chiang, Ping-yeh and Geiping, Jonas and Czaja, Wojtek and Goldstein, Tom},
 booktitle = {NeurIPS},
  year={2021}
}

@inproceedings{zhang2021towards,
  title={Towards Certifying L-infinity Robustness using Neural Networks with L-inf-dist Neurons},
  author={Zhang, Bohang and Cai, Tianle and Lu, Zhou and He, Di and Wang, Liwei},
  booktitle={ICML},
  year={2021},
}

@article{zhao2021deep,
  title={What Do Deep Nets Learn? Class-wise Patterns Revealed in the Input Space},
  author={Zhao, Shihao and Ma, Xingjun and Wang, Yisen and Bailey, James and Li, Bo and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2101.06898},
  year={2021}
}

@article{dong2021exploring,
  title={Exploring Memorization in Adversarial Training},
  author={Dong, Yinpeng and Xu, Ke and Yang, Xiao and Pang, Tianyu and Deng, Zhijie and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2106.01606},
  year={2021}
}

@inproceedings{li2021anti,
  title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data},
  author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},
  booktitle={NeurIPS},
  year={2021}
}

@article{geiping2021doesn,
  title={What Doesn't Kill You Makes You Robust (er): Adversarial Training against Poisons and Backdoors},
  author={Geiping, Jonas and Fowl, Liam and Somepalli, Gowthami and Goldblum, Micah and Moeller, Michael and Goldstein, Tom},
  journal={arXiv preprint arXiv:2102.13624},
  year={2021}
}

@inproceedings{munoz2017towards,
  title={Towards poisoning of deep learning algorithms with back-gradient optimization},
  author={Mu{\~n}oz-Gonz{\'a}lez, Luis and Biggio, Battista and Demontis, Ambra and Paudice, Andrea and Wongrassamee, Vasin and Lupu, Emil C and Roli, Fabio},
  booktitle={ACM Workshop on Artificial Intelligence and Security},
  year={2017}
}

