\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Per(2022)]{Perplexity}
Perplexity of fixed-length models, 2022.
\newblock URL \url{https://huggingface.co/docs/transformers/perplexity}.

\bibitem[Chen et~al.(2021)Chen, Cheng, Wang, Gan, Wang, and
  Liu]{Chen2021earlybert}
Chen, X., Cheng, Y., Wang, S., Gan, Z., Wang, Z., and Liu, J.
\newblock {E}arly{BERT}: Efficient {BERT} training via early-bird lottery
  tickets.
\newblock In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.),
  \emph{Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  2195--2207,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.171}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.171}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{Dong2017}
Dong, X., Chen, S., and Pan, S.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Fan et~al.(2020)Fan, Grave, and Joulin]{Fan2020}
Fan, A., Grave, E., and Joulin, A.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SylO2yStDr}.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{pmlr-v202-frantar23a}
Frantar, E. and Alistarh, D.
\newblock {S}parse{GPT}: Massive language models can be accurately pruned in
  one-shot.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
  Learning Research}, pp.\  10323--10337. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/frantar23a.html}.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{Guo2016}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, NIPS'16, pp.\  1387–1395, Red Hook, NY,
  USA, 2016. Curran Associates Inc.
\newblock ISBN 9781510838819.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{Han2015}
Han, S., Pool, J., Tran, J., and Dally, W.~J.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems - Volume 1}, NIPS'15, pp.\  1135–1143,
  Cambridge, MA, USA, 2015. MIT Press.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{Han2016}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem[Hassibi \& Stork(1992)Hassibi and Stork]{OBS1992}
Hassibi, B. and Stork, D.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock In Hanson, S., Cowan, J., and Giles, C. (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~5. Morgan-Kaufmann, 1992.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{He2017}
He, Y., Zhang, X., and Sun, J.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{2017 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  1398--1406, Los Alamitos, CA, USA, oct 2017. IEEE Computer
  Society.
\newblock \doi{10.1109/ICCV.2017.155}.
\newblock URL \url{https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.155}.

\bibitem[He et~al.(2018{\natexlab{a}})He, Kang, Dong, Fu, and Yang]{He2018a}
He, Y., Kang, G., Dong, X., Fu, Y., and Yang, Y.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, IJCAI'18, pp.\  2234–2240. AAAI Press,
  2018{\natexlab{a}}.
\newblock ISBN 9780999241127.

\bibitem[He et~al.(2018{\natexlab{b}})He, Liu, Wang, Hu, and
  Yang]{He2018geometric}
He, Y., Liu, P., Wang, Z., Hu, Z., and Yang, Y.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  4335--4344, 2018{\natexlab{b}}.

\bibitem[Hou et~al.(2020)Hou, Huang, Shang, Jiang, Chen, and Liu]{Hou2020}
Hou, L., Huang, Z., Shang, L., Jiang, X., Chen, X., and Liu, Q.
\newblock Dynabert: Dynamic bert with adaptive width and depth.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kurtić et~al.(2023)Kurtić, Frantar, and Alistarh]{ziplm}
Kurtić, E., Frantar, E., and Alistarh, D.
\newblock Ziplm: Inference-aware structured pruning of language models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=d8j3lsBWpV}.

\bibitem[Kwon et~al.(2022)Kwon, Kim, Mahoney, Hassoun, Keutzer, and
  Gholami]{kwon2022a}
Kwon, W., Kim, S., Mahoney, M.~W., Hassoun, J., Keutzer, K., and Gholami, A.
\newblock A fast post-training pruning framework for transformers.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=0GRBKLBjJE}.

\bibitem[Lebedev \& Lempitsky(2016)Lebedev and Lempitsky]{Lebedev2016}
Lebedev, V. and Lempitsky, V.
\newblock Fast convnets using group-wise brain damage.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  2554--2564, 2016.
\newblock \doi{10.1109/CVPR.2016.280}.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{OBD}
LeCun, Y., Denker, J., and Solla, S.
\newblock Optimal brain damage.
\newblock In Touretzky, D. (ed.), \emph{Advances in Neural Information
  Processing Systems}, volume~2. Morgan-Kaufmann, 1989.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{Li2017}
Li, H., Kadav, A., Durdanovic, I., Samet, H., and Graf, H.~P.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=rJqFGTslg}.

\bibitem[Lin et~al.(2020)Lin, Ji, Wang, Zhang, Zhang, Tian, and
  Shao]{Lin2020cvpr}
Lin, M., Ji, R., Wang, Y., Zhang, Y., Zhang, B., Tian, Y., and Shao, L.
\newblock Hrank: Filter pruning using high-rank feature map.
\newblock In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  1526--1535, Los Alamitos, CA, USA, jun 2020. IEEE
  Computer Society.
\newblock \doi{10.1109/CVPR42600.2020.00160}.
\newblock URL
  \url{https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00160}.

\bibitem[Liu et~al.(2021)Liu, Zhang, Kuang, Zhou, Xue, Wang, Chen, Yang, Liao,
  and Zhang]{liu21ab}
Liu, L., Zhang, S., Kuang, Z., Zhou, A., Xue, J.-H., Wang, X., Chen, Y., Yang,
  W., Liao, Q., and Zhang, W.
\newblock Group fisher pruning for practical network compression.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  7021--7032. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/liu21ab.html}.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{Liu2017}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{2017 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  2755--2763, 2017.
\newblock \doi{10.1109/ICCV.2017.298}.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{Luo17}
Luo, J., Wu, J., and Lin, W.
\newblock Thinet: {A} filter level pruning method for deep neural network
  compression.
\newblock In \emph{{IEEE} International Conference on Computer Vision, {ICCV}
  2017, Venice, Italy, October 22-29, 2017}, pp.\  5068--5076. {IEEE} Computer
  Society, 2017.
\newblock \doi{10.1109/ICCV.2017.541}.

\bibitem[Luo \& Wu(2020)Luo and Wu]{Luo2020}
Luo, J.-H. and Wu, J.
\newblock Neural network pruning with residual-connections and limited-data.
\newblock In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  1455--1464, 2020.
\newblock \doi{10.1109/CVPR42600.2020.00153}.

\bibitem[Malladi et~al.(2023)Malladi, Gao, Nichani, Damian, Lee, Chen, and
  Arora]{Malladi2023}
Malladi, S., Gao, T., Nichani, E., Damian, A., Lee, J., Chen, D., and Arora, S.
\newblock Fine-tuning language models with just forward passes.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Vota6rFhBQ}.

\bibitem[Marcus et~al.(1994)Marcus, Kim, Marcinkiewicz, MacIntyre, Bies,
  Ferguson, Katz, and Schasberger]{Marcus1994}
Marcus, M., Kim, G., Marcinkiewicz, M.~A., MacIntyre, R., Bies, A., Ferguson,
  M., Katz, K., and Schasberger, B.
\newblock The penn treebank: Annotating predicate argument structure.
\newblock In \emph{Proceedings of the Workshop on Human Language Technology},
  HLT '94, pp.\  114–119, USA, 1994. Association for Computational
  Linguistics.
\newblock ISBN 1558603573.
\newblock \doi{10.3115/1075812.1075835}.
\newblock URL \url{https://doi.org/10.3115/1075812.1075835}.

\bibitem[McCarley(2019)]{McCarley2019}
McCarley, J.~S.
\newblock Pruning a bert-based question answering model.
\newblock \emph{ArXiv}, abs/1910.06360, 2019.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:204575977}.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2017pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Byj72udxe}.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{Michel2019}
Michel, P., Levy, O., and Neubig, G.
\newblock Are sixteen heads really better than one?
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Molchanov et~al.(2017)Molchanov, Tyree, Karras, Aila, and
  Kautz]{Molchanov2017}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=SJGCiw5gl}.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989using}
Mozer, M.~C. and Smolensky, P.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16, 1989.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{Raffel2019}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21\penalty0 (1), jan 2020.
\newblock ISSN 1532-4435.

\bibitem[Sajjad et~al.(2020)Sajjad, Dalvi, Durrani, and Nakov]{Sajjad2020}
Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P.
\newblock On the effect of dropping layers of pre-trained transformer models.
\newblock \emph{Comput. Speech Lang.}, 77:\penalty0 101429, 2020.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:251005814}.

\bibitem[Sui et~al.(2021)Sui, Yin, Xie, Phan, Zonouz, and Yuan]{Sui2021}
Sui, Y., Yin, M., Xie, Y., Phan, H., Zonouz, S.~A., and Yuan, B.
\newblock {CHIP}: {CH}annel independence-based pruning for compact neural
  networks.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=EmeWbcWORRg}.

\bibitem[Tang et~al.(2020)Tang, Wang, Xu, Tao, Xu, Xu, and Xu]{Tang2020}
Tang, Y., Wang, Y., Xu, Y., Tao, D., Xu, C., Xu, C., and Xu, C.
\newblock Scop: Scientific control for reliable neural network pruning.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{Voita2019}
Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock In Korhonen, A., Traum, D., and M{\`a}rquez, L. (eds.),
  \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pp.\  5797--5808, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1580}.
\newblock URL \url{https://aclanthology.org/P19-1580}.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{Wen2016}
Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, NIPS'16, pp.\  2082–2090, Red Hook, NY,
  USA, 2016. Curran Associates Inc.
\newblock ISBN 9781510838819.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh,
  et~al.]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In Liu, Q. and Schlangen, D. (eds.), \emph{Proceedings of the 2020
  Conference on Empirical Methods in Natural Language Processing: System
  Demonstrations}, pp.\  38--45, Online, October 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-demos.6}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-demos.6}.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{Xiao2023}
Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S.
\newblock {S}mooth{Q}uant: Accurate and efficient post-training quantization
  for large language models.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, 2023.

\bibitem[Yao et~al.(2022)Yao, Yazdani~Aminabadi, Zhang, Wu, Li, and
  He]{Yao2022}
Yao, Z., Yazdani~Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  27168--27183. Curran Associates, Inc., 2022.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{Yu2018}
Yu, R., Li, A., Chen, C., Lai, J., Morariu, V.~I., Han, X., Gao, M., Lin, C.,
  and Davis, L.~S.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  9194--9203, Los Alamitos, CA, USA, jun 2018. IEEE
  Computer Society.
\newblock \doi{10.1109/CVPR.2018.00958}.
\newblock URL
  \url{https://doi.ieeecomputersociety.org/10.1109/CVPR.2018.00958}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,
  Diab, M., Li, X., Lin, X.~V., et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2015)Zhang, Zou, He, and Sun]{zhang2015accelerating}
Zhang, X., Zou, J., He, K., and Sun, J.
\newblock Accelerating very deep convolutional networks for classification and
  detection.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (10):\penalty0 1943--1955, 2015.

\bibitem[Zhou et~al.(2016)Zhou, Alvarez, and Porikli]{Zhou2016}
Zhou, H., Alvarez, J.~M., and Porikli, F.
\newblock Less is more: Towards compact cnns.
\newblock In Leibe, B., Matas, J., Sebe, N., and Welling, M. (eds.),
  \emph{Computer Vision -- ECCV 2016}, pp.\  662--677, Cham, 2016. Springer
  International Publishing.

\end{thebibliography}
