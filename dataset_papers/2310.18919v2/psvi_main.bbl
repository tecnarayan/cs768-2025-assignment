\begin{thebibliography}{}

\bibitem[Abbasi-Yadkori et~al., 2011]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C. (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock {\em Advances in neural information processing systems}, 24.

\bibitem[Abeille and Lazaric, 2017]{abeille2017linear}
Abeille, M. and Lazaric, A. (2017).
\newblock Linear thompson sampling revisited.
\newblock In {\em Artificial Intelligence and Statistics}, pages 176--184.
  PMLR.

\bibitem[Abramowitz and Stegun, 1964]{abramowitz1964handbook}
Abramowitz, M. and Stegun, I.~A. (1964).
\newblock {\em Handbook of Mathematical Functions with Formulas, Graphs, and
  Mathematical Tables}, volume~55.
\newblock US Government Printing Office.

\bibitem[Agarwal and Aggarwal, 2021]{agarwal2021blind}
Agarwal, M. and Aggarwal, V. (2021).
\newblock Blind decision making: Reinforcement learning with delayed
  observations.
\newblock {\em Pattern Recognition Letters}, 150:176--182.

\bibitem[Agrawal et~al., 2021]{agrawal2021improved}
Agrawal, P., Chen, J., and Jiang, N. (2021).
\newblock Improved worst-case regret bounds for randomized least-squares value
  iteration.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 6566--6573.

\bibitem[Agrawal and Goyal, 2013]{agrawal2013thompson}
Agrawal, S. and Goyal, N. (2013).
\newblock Thompson sampling for contextual bandits with linear payoffs.
\newblock In {\em International conference on machine learning}, pages
  127--135. PMLR.

\bibitem[Agrawal and Jia, 2017]{agrawal2017optimistic}
Agrawal, S. and Jia, R. (2017).
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Auer et~al., 2008]{auer2008near}
Auer, P., Jaksch, T., and Ortner, R. (2008).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 21.

\bibitem[Ayoub et~al., 2020]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L. (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In {\em International Conference on Machine Learning}, pages
  463--474. PMLR.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  263--272. PMLR.

\bibitem[Black et~al., 2023]{black2023training}
Black, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S. (2023).
\newblock Training diffusion models with reinforcement learning.
\newblock {\em arXiv preprint arXiv:2305.13301}.

\bibitem[Bouteiller et~al., 2020]{bouteiller2020reinforcement}
Bouteiller, Y., Ramstedt, S., Beltrame, G., Pal, C., and Binas, J. (2020).
\newblock Reinforcement learning with random delays.
\newblock In {\em International conference on learning representations}.

\bibitem[Cai et~al., 2020]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z. (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1283--1294. PMLR.

\bibitem[Chapelle and Li, 2011]{chapelle2011empirical}
Chapelle, O. and Li, L. (2011).
\newblock An empirical evaluation of thompson sampling.
\newblock {\em Advances in neural information processing systems}, 24.

\bibitem[Chen et~al., 2020]{chen2020delay}
Chen, B., Xu, M., Liu, Z., Li, L., and Zhao, D. (2020).
\newblock Delay-aware multi-agent reinforcement learning for cooperative and
  competitive environments.
\newblock {\em arXiv preprint arXiv:2005.05441}.

\bibitem[Chen et~al., 2023]{chen2023efficient}
Chen, M., Bai, Y., Poor, H.~V., and Wang, M. (2023).
\newblock Efficient rl with impaired observability: Learning to act with
  delayed and missing state observations.
\newblock {\em arXiv preprint arXiv:2306.01243}.

\bibitem[Derman et~al., 2020]{derman2020acting}
Derman, E., Dalal, G., and Mannor, S. (2020).
\newblock Acting in delayed environments with non-stationary markov policies.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Dulac-Arnold et~al., 2019]{dulac2019challenges}
Dulac-Arnold, G., Mankowitz, D., and Hester, T. (2019).
\newblock Challenges of real-world reinforcement learning.
\newblock {\em arXiv preprint arXiv:1904.12901}.

\bibitem[Fan and Ming, 2021]{fan2021model}
Fan, Y. and Ming, Y. (2021).
\newblock Model-based reinforcement learning for continuous control with
  posterior sampling.
\newblock In {\em International Conference on Machine Learning}, pages
  3078--3087. PMLR.

\bibitem[Fellows et~al., 2019]{fellows2019virel}
Fellows, M., Mahajan, A., Rudner, T.~G., and Whiteson, S. (2019).
\newblock Virel: A variational inference framework for reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Fujimoto et~al., 2018]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D. (2018).
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages
  1587--1596. PMLR.

\bibitem[Gael et~al., 2020]{gael2020stochastic}
Gael, M.~A., Vernade, C., Carpentier, A., and Valko, M. (2020).
\newblock Stochastic bandits with arm-dependent delays.
\newblock In {\em International Conference on Machine Learning}, pages
  3348--3356. PMLR.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages
  1861--1870. PMLR.

\bibitem[Hamidi and Bayati, 2020]{hamidi2020worst}
Hamidi, N. and Bayati, M. (2020).
\newblock On frequentist regret of linear thompson sampling.
\newblock {\em arXiv preprint arXiv:2006.06790}.

\bibitem[Han et~al., 2022]{han2022off}
Han, B., Ren, Z., Wu, Z., Zhou, Y., and Peng, J. (2022).
\newblock Off-policy reinforcement learning with delayed rewards.
\newblock In {\em International Conference on Machine Learning}, pages
  8280--8303. PMLR.

\bibitem[Hasselt, 2010]{hasselt2010double}
Hasselt, H. (2010).
\newblock Double q-learning.
\newblock {\em Advances in neural information processing systems}, 23.

\bibitem[He et~al., 2023]{he2023nearly}
He, J., Zhao, H., Zhou, D., and Gu, Q. (2023).
\newblock Nearly minimax optimal reinforcement learning for linear markov
  decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  12790--12822. PMLR.

\bibitem[Howson et~al., 2023a]{howson2023delayed}
Howson, B., Pike-Burke, C., and Filippi, S. (2023a).
\newblock Delayed feedback in generalised linear bandits revisited.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 6095--6119. PMLR.

\bibitem[Howson et~al., 2023b]{howson2023optimism}
Howson, B., Pike-Burke, C., and Filippi, S. (2023b).
\newblock Optimism and delays in episodic reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 6061--6094. PMLR.

\bibitem[Hsu et~al., 2012]{hsu2012tail}
Hsu, D., Kakade, S., and Zhang, T. (2012).
\newblock A tail inequality for quadratic forms of subgaussian random vectors.

\bibitem[Ishfaq et~al., 2021]{ishfaq2021randomized}
Ishfaq, H., Cui, Q., Nguyen, V., Ayoub, A., Yang, Z., Wang, Z., Precup, D., and
  Yang, L. (2021).
\newblock Randomized exploration in reinforcement learning with general value
  function approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  4607--4616. PMLR.

\bibitem[Ishfaq et~al., 2023]{ishfaq2023provable}
Ishfaq, H., Lan, Q., Xu, P., Mahmood, A.~R., Precup, D., Anandkumar, A., and
  Azizzadenesheli, K. (2023).
\newblock Provable and practical: Efficient exploration in reinforcement
  learning via langevin monte carlo.
\newblock {\em arXiv preprint arXiv:2305.18246}.

\bibitem[Ito et~al., 2020]{ito2020delay}
Ito, S., Hatano, D., Sumita, H., Takemura, K., Fukunaga, T., Kakimura, N., and
  Kawarabayashi, K.-I. (2020).
\newblock Delay and cooperation in nonstochastic linear bandits.
\newblock {\em Advances in Neural Information Processing Systems},
  33:4872--4883.

\bibitem[Jiang et~al., 2017]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
  (2017).
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In {\em International Conference on Machine Learning}, pages
  1704--1713. PMLR.

\bibitem[Jin et~al., 2021]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021).
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em Advances in neural information processing systems},
  34:13406--13418.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Jin et~al., 2022]{jin2022near}
Jin, T., Lancewicki, T., Luo, H., Mansour, Y., and Rosenberg, A. (2022).
\newblock Near-optimal regret for adversarial mdp with delayed bandit feedback.
\newblock {\em arXiv preprint arXiv:2201.13172}.

\bibitem[Jumper et~al., 2021]{jumper2021highly}
Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O.,
  Tunyasuvunakool, K., Bates, R., {\v{Z}}{\'\i}dek, A., Potapenko, A., et~al.
  (2021).
\newblock Highly accurate protein structure prediction with alphafold.
\newblock {\em Nature}, 596(7873):583--589.

\bibitem[Karbasi et~al., 2023]{karbasi2023langevin}
Karbasi, A., Kuang, N.~L., Ma, Y., and Mitra, S. (2023).
\newblock Langevin thompson sampling with logarithmic communication: bandits
  and reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  15828--15860. PMLR.

\bibitem[Kebria et~al., 2019]{kebria2019robust}
Kebria, P.~M., Khosravi, A., Nahavandi, S., Shi, P., and Alizadehsani, R.
  (2019).
\newblock Robust adaptive control scheme for teleoperation systems with delay
  and uncertainties.
\newblock {\em IEEE transactions on cybernetics}, 50(7):3243--3253.

\bibitem[Lancewicki et~al., 2022]{lancewicki2022learning}
Lancewicki, T., Rosenberg, A., and Mansour, Y. (2022).
\newblock Learning adversarial markov decision processes with delayed feedback.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 7281--7289.

\bibitem[Lee et~al., 2023]{lee2023aligning}
Lee, K., Liu, H., Ryu, M., Watkins, O., Du, Y., Boutilier, C., Abbeel, P.,
  Ghavamzadeh, M., and Gu, S.~S. (2023).
\newblock Aligning text-to-image models using human feedback.
\newblock {\em arXiv preprint arXiv:2302.12192}.

\bibitem[Li et~al., 2021]{li2021hyperdqn}
Li, Z., Li, Y., Zhang, Y., Zhang, T., and Luo, Z.-Q. (2021).
\newblock Hyperdqn: A randomized exploration method for deep reinforcement
  learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Mazumdar et~al., 2020]{mazumdar2020approximate}
Mazumdar, E., Pacchiano, A., Ma, Y., Jordan, M., and Bartlett, P. (2020).
\newblock On approximate thompson sampling with langevin algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  6797--6807. PMLR.

\bibitem[Min et~al., 2021]{min2021variance}
Min, Y., Wang, T., Zhou, D., and Gu, Q. (2021).
\newblock Variance-aware off-policy evaluation with linear function
  approximation.
\newblock {\em Advances in neural information processing systems},
  34:7598--7610.

\bibitem[Mondal and Aggarwal, 2023]{mondal2023reinforcement}
Mondal, W.~U. and Aggarwal, V. (2023).
\newblock Reinforcement learning with delayed, composite, and partially
  anonymous reward.
\newblock {\em arXiv preprint arXiv:2305.02527}.

\bibitem[Nguyen-Tang et~al., 2022]{nguyen2022instance}
Nguyen-Tang, T., Yin, M., Gupta, S., Venkatesh, S., and Arora, R. (2022).
\newblock On instance-dependent bounds for offline reinforcement learning with
  linear function approximation.
\newblock {\em arXiv preprint arXiv:2211.13208}.

\bibitem[Osband et~al., 2016a]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B. (2016a).
\newblock Deep exploration via bootstrapped dqn.
\newblock {\em Advances in neural information processing systems}, 29.

\bibitem[Osband et~al., 2019]{osband2019deep}
Osband, I., Van~Roy, B., Russo, D.~J., Wen, Z., et~al. (2019).
\newblock Deep exploration via randomized value functions.
\newblock {\em J. Mach. Learn. Res.}, 20(124):1--62.

\bibitem[Osband et~al., 2016b]{osband2016generalization}
Osband, I., Van~Roy, B., and Wen, Z. (2016b).
\newblock Generalization and exploration via randomized value functions.
\newblock In {\em International Conference on Machine Learning}, pages
  2377--2386. PMLR.

\bibitem[Ouyang et~al., 2022]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
  C., Agarwal, S., Slama, K., Ray, A., et~al. (2022).
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744.

\bibitem[Padalkar et~al., 2023]{padalkar2023open}
Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A.,
  Khazatsky, A., Rai, A., Singh, A., Brohan, A., et~al. (2023).
\newblock Open x-embodiment: Robotic learning datasets and rt-x models.
\newblock {\em arXiv preprint arXiv:2310.08864}.

\bibitem[Riquelme et~al., 2018]{riquelme2018deep}
Riquelme, C., Tucker, G., and Snoek, J. (2018).
\newblock Deep bayesian bandits showdown: An empirical comparison of bayesian
  deep networks for thompson sampling.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Russo, 2019]{russo2019worst}
Russo, D. (2019).
\newblock Worst-case regret bounds for exploration via randomized value
  functions.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Russo and Van~Roy, 2013]{russo2013eluder}
Russo, D. and Van~Roy, B. (2013).
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock {\em Advances in Neural Information Processing Systems}, 26.

\bibitem[Strehl and Littman, 2008]{strehl2008analysis}
Strehl, A.~L. and Littman, M.~L. (2008).
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock {\em Journal of Computer and System Sciences}, 74(8):1309--1331.

\bibitem[Tang et~al., 2021]{tang2021bandit}
Tang, W., Ho, C.-J., and Liu, Y. (2021).
\newblock Bandit learning with delayed impact of actions.
\newblock {\em Advances in Neural Information Processing Systems},
  34:26804--26817.

\bibitem[Thompson, 1933]{thompson1933likelihood}
Thompson, W.~R. (1933).
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 25(3-4):285--294.

\bibitem[Thune et~al., 2019]{thune2019nonstochastic}
Thune, T.~S., Cesa-Bianchi, N., and Seldin, Y. (2019).
\newblock Nonstochastic multiarmed bandits with unrestricted delays.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Touvron et~al., 2023]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al. (2023).
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}.

\bibitem[Vernade et~al., 2020]{vernade2020linear}
Vernade, C., Carpentier, A., Lattimore, T., Zappella, G., Ermis, B., and
  Brueckner, M. (2020).
\newblock Linear bandits with stochastic delayed feedback.
\newblock In {\em International Conference on Machine Learning}, pages
  9712--9721. PMLR.

\bibitem[Wang et~al., 2020]{wang2020reinforcement}
Wang, R., Salakhutdinov, R.~R., and Yang, L. (2020).
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6123--6135.

\bibitem[Wang et~al., 2019]{wang2019optimism}
Wang, Y., Wang, R., Du, S.~S., and Krishnamurthy, A. (2019).
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock {\em arXiv preprint arXiv:1912.04136}.

\bibitem[Wang et~al., 2023]{wang2023aligning}
Wang, Y., Zhong, W., Li, L., Mi, F., Zeng, X., Huang, W., Shang, L., Jiang, X.,
  and Liu, Q. (2023).
\newblock Aligning large language models with human: A survey.
\newblock {\em arXiv preprint arXiv:2307.12966}.

\bibitem[Welling and Teh, 2011]{welling2011bayesian}
Welling, M. and Teh, Y.~W. (2011).
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688.

\bibitem[Xu et~al., 2022]{xu2022langevin}
Xu, P., Zheng, H., Mazumdar, E.~V., Azizzadenesheli, K., and Anandkumar, A.
  (2022).
\newblock Langevin monte carlo for contextual bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  24830--24850. PMLR.

\bibitem[Yang and Wang, 2019]{yang2019sample}
Yang, L. and Wang, M. (2019).
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In {\em International Conference on Machine Learning}, pages
  6995--7004. PMLR.

\bibitem[Yang and Wang, 2020]{yang2020reinforcement}
Yang, L. and Wang, M. (2020).
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In {\em International Conference on Machine Learning}, pages
  10746--10756. PMLR.

\bibitem[Yang et~al., 2023]{yang2023reduction}
Yang, Y., Zhong, H., Wu, T., Liu, B., Wang, L., and Du, S.~S. (2023).
\newblock A reduction-based framework for sequential decision making with
  delayed feedback.
\newblock {\em arXiv preprint arXiv:2302.01477}.

\bibitem[Yin et~al., 2021]{yin2021near}
Yin, M., Bai, Y., and Wang, Y.-X. (2021).
\newblock Near-optimal provable uniform convergence in offline policy
  evaluation for reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1567--1575. PMLR.

\bibitem[Yin et~al., 2022]{yin2022near}
Yin, M., Duan, Y., Wang, M., and Wang, Y.-X. (2022).
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Yin et~al., 2023]{yin2023offline}
Yin, M., Wang, M., and Wang, Y.-X. (2023).
\newblock Offline reinforcement learning with differentiable function
  approximation is provably efficient.
\newblock {\em International Conference on Learning Representations}.

\bibitem[Zanette et~al., 2020a]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
  (2020a).
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1954--1964. PMLR.

\bibitem[Zanette et~al., 2020b]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020b).
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In {\em International Conference on Machine Learning}, pages
  10978--10989. PMLR.

\bibitem[Zhang, 2022]{zhang2022feel}
Zhang, T. (2022).
\newblock Feel-good thompson sampling for contextual bandits and reinforcement
  learning.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 4(2):834--857.

\bibitem[Zhou et~al., 2021a]{zhou2021nearly}
Zhou, D., Gu, Q., and Szepesvari, C. (2021a).
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In {\em Conference on Learning Theory}, pages 4532--4576. PMLR.

\bibitem[Zhou et~al., 2021b]{zhou2021provably}
Zhou, D., He, J., and Gu, Q. (2021b).
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In {\em International Conference on Machine Learning}, pages
  12793--12802. PMLR.

\bibitem[Zhou et~al., 2019]{zhou2019learning}
Zhou, Z., Xu, R., and Blanchet, J. (2019).
\newblock Learning in generalized linear contextual bandits with stochastic
  delays.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Zimmert and Seldin, 2020]{zimmert2020optimal}
Zimmert, J. and Seldin, Y. (2020).
\newblock An optimal algorithm for adversarial bandits with arbitrary delays.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3285--3294. PMLR.

\end{thebibliography}
