\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Mart\'{\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dandelion Man\'{e}, Rajat Monga, Sherry Moore, Derek Murray, Chris
  Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
  Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\'{e}gas,
  Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
  Xiaoqiang Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Agarwal and Duchi(2012)]{agarwal2012distributed}
Alekh Agarwal and John~C Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{2012 IEEE 51st IEEE Conference on Decision and Control
  (CDC)}, pages 5451--5452. IEEE, 2012.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry~Z Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 1707--1718, 2017.

\bibitem[Arjevani et~al.(2020)Arjevani, Shamir, and Srebro]{arjevani2020tight}
Yossi Arjevani, Ohad Shamir, and Nathan Srebro.
\newblock A tight convergence analysis for stochastic gradient descent with
  delayed updates.
\newblock In \emph{Algorithmic Learning Theory}, pages 111--132. PMLR, 2020.

\bibitem[Aviv et~al.(2021)Aviv, Hakimi, Schuster, and Levy]{pmlr-v139-aviv21a}
Rotem~Zamir Aviv, Ido Hakimi, Assaf Schuster, and Kfir~Yehuda Levy.
\newblock Asynchronous distributed learning : Adapting to gradient delays
  without prior knowledge.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 436--445. PMLR, 18--24
  Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/aviv21a.html}.

\bibitem[Bertsekas and Tsitsiklis(1997)]{bertsekas1997parallel}
D.P. Bertsekas and J.N. Tsitsiklis.
\newblock \emph{Parallel and Distributed Computation: Numerical Methods}.
\newblock Athena Scientific, 1997.

\bibitem[Chaturapruek et~al.(2015)Chaturapruek, Duchi, and
  R{\'e}]{chaturapruek2015asynchronous}
Sorathan Chaturapruek, John~C Duchi, and Christopher R{\'e}.
\newblock Asynchronous stochastic convex optimization: the noise is in the
  noise and sgd don't care.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 1531--1539, 2015.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and
  Sridharan]{cotter2011better}
Andrew Cotter, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In \emph{Proceedings of the 24th International Conference on Neural
  Information Processing Systems}, pages 1647--1655, 2011.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (1), 2012.

\bibitem[Duchi et~al.(2012)Duchi, Bartlett, and
  Wainwright]{duchi2012randomized}
John~C Duchi, Peter~L Bartlett, and Martin~J Wainwright.
\newblock Randomized smoothing for stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  674--701, 2012.

\bibitem[Feyzmahdavian et~al.(2016)Feyzmahdavian, Aytekin, and
  Johansson]{feyzmahdavian2016asynchronous}
Hamid~Reza Feyzmahdavian, Arda Aytekin, and Mikael Johansson.
\newblock An asynchronous mini-batch algorithm for regularized stochastic
  optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 61\penalty0
  (12):\penalty0 3740--3754, 2016.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock In \emph{International Conference on Machine Learning}, pages
  3252--3261. PMLR, 2019.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Computer Science Department, University of Toronto,
  April 2009.

\bibitem[Lan(2012)]{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1):\penalty0 365--397,
  2012.

\bibitem[Leblond et~al.(2018)Leblond, Pedregosa, and
  Lacoste-Julien]{leblond2018improved}
R{\'e}mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock Improved asynchronous parallel optimization analysis for stochastic
  incremental methods.
\newblock \emph{Journal of Machine Learning Research}, 19:\penalty0 1--68,
  2018.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 2}, pages 2737--2745, 2015.

\bibitem[Mania et~al.(2017)Mania, Pan, Papailiopoulos, Recht, Ramchandran, and
  Jordan]{mania2017perturbed}
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan
  Ramchandran, and Michael~I Jordan.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2202--2229, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Nedi{\'c} et~al.(2001)Nedi{\'c}, Bertsekas, and
  Borkar]{nedic2001distributed}
Angelia Nedi{\'c}, Dimitri~P Bertsekas, and Vivek~S Borkar.
\newblock Distributed asynchronous incremental subgradient methods.
\newblock \emph{Studies in Computational Mathematics}, 8\penalty0 (C):\penalty0
  381--407, 2001.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov et~al.(2018)]{nesterov2018lectures}
Yurii Nesterov et~al.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Niu et~al.(2011)Niu, Recht, Re, and Wright]{niu2011hogwild}
Feng Niu, Benjamin Recht, Christopher Re, and Stephen~J Wright.
\newblock Hogwild! a lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Proceedings of the 24th International Conference on Neural
  Information Processing Systems}, pages 693--701, 2011.

\bibitem[Reddi et~al.(2015)Reddi, Hefny, Sra, P{\"o}czos, and
  Smola]{reddi2015variance}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnab{\'a}s P{\"o}czos, and Alex
  Smola.
\newblock On variance reduction in stochastic gradient descent and its
  asynchronous variants.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 2}, pages 2647--2655, 2015.

\bibitem[Stich and Karimireddy(2020)]{stich2020error}
Sebastian~U Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed updates.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--36,
  2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, Mcmahan,
  Shamir, and Srebro]{woodworth2020local}
Blake Woodworth, Kumar~Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins,
  Brendan Mcmahan, Ohad Shamir, and Nathan Srebro.
\newblock Is local sgd better than minibatch sgd?
\newblock In \emph{International Conference on Machine Learning}, pages
  10334--10343. PMLR, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Shang, and Cheng]{zhou2018simple}
Kaiwen Zhou, Fanhua Shang, and James Cheng.
\newblock A simple stochastic variance reduced algorithm with fast convergence
  rates.
\newblock In \emph{International Conference on Machine Learning}, pages
  5980--5989. PMLR, 2018.

\end{thebibliography}
