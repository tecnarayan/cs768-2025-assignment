\begin{thebibliography}{10}

\bibitem{chung2015gated}
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio.
\newblock Gated feedback recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1502.02367}, 2015.

\bibitem{kalchbrenner2015grid}
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves.
\newblock Grid long short-term memory.
\newblock {\em arXiv preprint arXiv:1507.01526}, 2015.

\bibitem{zhang2016architectural}
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan
  Salakhutdinov, and Yoshua Bengio.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1602.08210}, 2016.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1406.1078}, 2014.

\bibitem{goudreau1994first}
Mark~W Goudreau, C~Lee Giles, Srimat~T Chakradhar, and D~Chen.
\newblock First-order versus second-order single-layer recurrent neural
  networks.
\newblock {\em Neural Networks, IEEE Transactions on}, 5(3):511--513, 1994.

\bibitem{Cooijmans2016}
Tim Cooijmans, Nicolas Ballas, C{\'{e}}sar Laurent, and Aaron Courville.
\newblock Recurrent batch normalization.
\newblock {\em http://arxiv.org/pdf/1603.09025v4.pdf}, 2016.

\bibitem{baum1967inequality}
LE~Baum and JA~Eagon.
\newblock An inequality with application to statistical estimation for
  probabilistic functions of markov processes and to a model for ecology.
\newblock {\em Bulletin of the American Mathematical Society}, 73:360--363,
  1967.

\bibitem{sutskever2011generating}
Ilya Sutskever, James Martens, and Geoffrey~E Hinton.
\newblock Generating text with recurrent neural networks.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 1017--1024, 2011.

\bibitem{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em arXiv preprint arXiv:1512.03385}, 2015.

\bibitem{marcus1993building}
Mitchell~P Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock {\em Computational linguistics}, 19(2):313--330, 1993.

\bibitem{mikolov2012subword}
Tom{\'a}{\v{s}} Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, and Stefan
  Kombrink.
\newblock Subword language modeling with neural networks.
\newblock {\em preprint, (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf)},
  2012.

\bibitem{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{krueger2015regularizing}
David Krueger and Roland Memisevic.
\newblock Regularizing rnns by stabilizing activations.
\newblock {\em arXiv preprint arXiv:1511.08400}, 2015.

\bibitem{hannun2014first}
Awni~Y Hannun, Andrew~L Maas, Daniel Jurafsky, and Andrew~Y Ng.
\newblock First-pass large vocabulary continuous speech recognition using
  bi-directional recurrent dnns.
\newblock {\em arXiv preprint arXiv:1408.2873}, 2014.

\bibitem{bahdanau2015end}
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua
  Bengio.
\newblock End-to-end attention-based large vocabulary speech recognition.
\newblock {\em arXiv preprint arXiv:1508.04395}, 2015.

\bibitem{graves2014towards}
Alex Graves and Navdeep Jaitly.
\newblock Towards end-to-end speech recognition with recurrent neural networks.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pages 1764--1772, 2014.

\bibitem{miao2015eesen}
Yajie Miao, Mohammad Gowayyed, and Florian Metze.
\newblock Eesen: End-to-end speech recognition using deep rnn models and
  wfst-based decoding.
\newblock {\em arXiv preprint arXiv:1507.08240}, 2015.

\bibitem{pachitariu2013regularization}
Marius Pachitariu and Maneesh Sahani.
\newblock Regularization and nonlinearities for neural language models: when
  are they needed?
\newblock {\em arXiv preprint arXiv:1301.5650}, 2013.

\bibitem{graves2013generating}
Alex Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1308.0850}, 2013.

\bibitem{graves2006connectionist}
Alex Graves, Santiago Fern{\'a}ndez, Faustino Gomez, and J{\"u}rgen
  Schmidhuber.
\newblock Connectionist temporal classification: labelling unsegmented sequence
  data with recurrent neural networks.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 369--376. ACM, 2006.

\bibitem{mohri2002weighted}
Mehryar Mohri, Fernando Pereira, and Michael Riley.
\newblock Weighted finite-state transducers in speech recognition.
\newblock {\em Computer Speech \& Language}, 16(1):69--88, 2002.

\bibitem{kiros2015skip}
Ryan Kiros, Yukun Zhu, Ruslan~R Salakhutdinov, Richard Zemel, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler.
\newblock Skip-thought vectors.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3276--3284, 2015.

\bibitem{hermann2015teaching}
Karl~Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will
  Kay, Mustafa Suleyman, and Phil Blunsom.
\newblock Teaching machines to read and comprehend.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1684--1692, 2015.

\bibitem{857908}
T.~Wessels and C.~W. Omlin.
\newblock Refining hidden markov models with recurrent neural networks.
\newblock In {\em Neural Networks, 2000. IJCNN 2000, Proceedings of the
  IEEE-INNS-ENNS International Joint Conference on}, volume~2, pages 271--276
  vol.2, 2000.

\bibitem{journals/corr/RasmusVHBR15}
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko.
\newblock Semi-supervised learning with ladder network.
\newblock {\em arXiv preprint arXiv:1507.02672}, 2015.

\bibitem{citeulike:13937173}
Mohammad Pezeshki, Linxi Fan, Philemon Brakel, Aaron Courville, and Yoshua
  Bengio.
\newblock {Deconstructing the ladder network architecture.}
\newblock {\em arXiv preprint arXiv:1511.06430}, 2015.

\bibitem{yang-etal-2014}
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li~Deng.
\newblock {Embedding entities and relations for learning and inference in
  knowledge bases}.
\newblock {\em arXiv preprint arXiv:1412.6575}, 2014.

\bibitem{Bastien-Theano-2016}
Rami Al{-}Rfou, Guillaume Alain, Amjad Almahairi, and et~al.
\newblock Theano: {A} python framework for fast computation of mathematical
  expressions, 2016.

\bibitem{chollet2015}
Fran√ßois Chollet.
\newblock Keras.
\newblock GitHub repository: \url{https://github.com/fchollet/keras}, 2015.

\end{thebibliography}
