\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2019)]{zhu2019}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Allen{-}Zhu and Li(2020)]{zhu2020}
Zeyuan Allen{-}Zhu and Yuanzhi Li.
\newblock Backward feature correction: How deep learning performs deep
  learning.
\newblock 2020.
\newblock URL \url{https://arxiv.org/abs/2001.04413}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{zhu2019b}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and Song]{zhu2019c}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, Proceedings
  of Machine Learning Research, pages 242--252, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019CNTK}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2019{\natexlab{b}}.

\bibitem[Bai and Lee(2020)]{bai2020}
Yu~Bai and Jason~D. Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Bai et~al.(2020)Bai, Krause, Wang, Xiong, and
  Socher]{bai2020taylorized}
Yu~Bai, Ben Krause, Huan Wang, Caiming Xiong, and Richard Socher.
\newblock Taylorized training: Towards better approximation of neural network
  training at finite width, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.04010.abs}.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and Rakhlin]{bartlett2021}
Peter~L. Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: A statistical viewpoint.
\newblock \emph{Acta Numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Cao and Gu(2019)]{cao2019}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Bai, Lee, Zhao, Wang, Xiong, and
  Socher]{chen2020}
Minshuo Chen, Yu~Bai, Jason~D. Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and
  Richard Socher.
\newblock Towards understanding hierarchical learning: Benefits of neural
  representations.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{a}}.

\bibitem[Chen et~al.(2021)Chen, Chi, Fan, and Ma]{spectral_methods}
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma.
\newblock Spectral methods for data science: A statistical perspective.
\newblock \emph{Foundations and Trends® in Machine Learning}, 14\penalty0
  (5):\penalty0 566--806, 2021.
\newblock ISSN 1935-8237.
\newblock \doi{10.1561/2200000079}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Cao, Gu, and
  Zhang]{chen2020generalized}
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang.
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{b}}.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2018}
Lénaïc Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Daniely and Malach(2020)]{daniely2020}
Amit Daniely and Eran Malach.
\newblock Learning parities with neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Davis and Kahan(1970)]{daviskahan}
Chandler Davis and W.~M. Kahan.
\newblock The rotation of eigenvectors by a perturbation. iii.
\newblock \emph{SIAM Journal on Numerical Analysis}, 7:\penalty0 1--46, 1970.

\bibitem[Du and Lee(2018)]{du2018quad}
Simon~S. Du and Jason~D. Lee.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, pages 1329--1338, 2018.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{du2018b}
Simon~S. Du, Jason~D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{du2019}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019{\natexlab{b}}.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Ge et~al.(2017)Ge, Lee, and Ma]{ge2017}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In \emph{International Conference on Learning Representations}.
  arXiv, 2017.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019b}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2020}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Ghorbani et~al.(2021)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{montanari2021}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{The Annals of Statistics}, 49:\penalty0 1029–1054, 2021.

\bibitem[Ghosh et~al.(2022)Ghosh, Mei, and Yu]{ghosh2022}
Nikhil Ghosh, Song Mei, and Bin Yu.
\newblock The three stages of learning dynamics in high-dimensional kernel
  methods.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Hu et~al.(2020)Hu, Xiao, Adlam, and Pennington]{hu2020}
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Huang and Yau(2020)]{huang2020}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, pages 4542--4551, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018}
Arthur Jacot, Franck Gabriel, and Clément Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 8571 -- 8580, 2018.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and Jordan]{jin2017}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M. Kakade, and Michael~I. Jordan.
\newblock How to escape saddle points efficiently.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Jin et~al.(2021)Jin, Netrapalli, Ge, Kakade, and Jordan]{jin2019}
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham~M. Kakade, and Michael~I. Jordan.
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock \emph{Journal of the ACM}, 68, 2021.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{leewide2019}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{leefinite2020}
Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao,
  Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016}
Jason~D. Lee, Max Simchowitz, Michael~I. Jordan, and Benjamin Recht.
\newblock In \emph{29th Annual Conference on Learning Theory}, Proceedings of
  Machine Learning Research, pages 1246--1257, 2016.

\bibitem[Li and Liang(2018)]{li2018}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Li et~al.(2020)Li, Ma, and Zhang]{li2020}
Yuanzhi Li, Tengyu Ma, and Hongyang~R. Zhang.
\newblock Learning over-parametrized two-layer neural networks beyond ntk.
\newblock In \emph{Proceedings of Thirty Third Conference on Learning Theory},
  Proceedings of Machine Learning Research, pages 2613--2682, 2020.

\bibitem[Malach et~al.(2021)Malach, Kamath, Abbe, and Srebro]{malach2021}
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro.
\newblock Quantifying the benefit of using differentiable learning over tangent
  kernels.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, pages 7379--7389, 2021.

\bibitem[Mei et~al.(2022)Mei, Misiakiewicz, and Montanari]{mei2022}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Generalization error of random features and kernel methods:
  hypercontractivity and kernel matrix concentration.
\newblock \emph{Applied and Computational Harmonic Analysis, Special Issue on
  Harmonic Analysis and Machine Learning}, 59:\penalty0 3--84, 2022.

\bibitem[Montanari and Zhong(2020)]{montanari2020}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training, 2020.
\newblock URL \url{https://arxiv.org/abs/2007.12826}.

\bibitem[Novak et~al.(2020)Novak, Xiao, Hron, Lee, Alemi, Sohl-Dickstein, and
  Schoenholz]{neuraltangents2020}
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander~A. Alemi, Jascha
  Sohl-Dickstein, and Samuel~S. Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://github.com/google/neural-tangents}.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D. Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65:\penalty0
  742--769, 2018.

\bibitem[Vershynin(2018)]{vershynin}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge University Press, 2018.

\bibitem[Wainwright(2019)]{wainwright}
Martin~J. Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wei et~al.(2018)Wei, Lee, Liu, and Ma]{regmatters2018}
Colin Wei, Jason~D. Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets v.s. their induced kernel, 2018.
\newblock URL \url{https://arxiv.org/abs/1810.05369}.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020}
Blake Woodworth, Suriya Gunasekar, Jason~D. Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory}, Proceedings of Machine
  Learning Research, pages 3635--3673, 2020.

\bibitem[Yehudai and Shamir(2019)]{yehudai2019}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks, 2018.
\newblock URL \url{https://arxiv.org/abs/1811.08888}.

\end{thebibliography}
