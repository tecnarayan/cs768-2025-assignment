\begin{thebibliography}{}

\bibitem[Arjevani et~al., 2020]{arjevani2020tight}
Arjevani, Y., Shamir, O., and Srebro, N. (2020).
\newblock A tight convergence analysis for stochastic gradient descent with
  delayed updates.
\newblock In {\em Algorithmic Learning Theory}, pages 111--132. PMLR.

\bibitem[Armijo, 1966]{armijo1966minimization}
Armijo, L. (1966).
\newblock Minimization of functions having {L}ipschitz continuous first partial
  derivatives.
\newblock {\em Pacific Journal of mathematics}.

\bibitem[Aybat et~al., 2019]{aybat2019universally}
Aybat, N.~S., Fallah, A., Gurbuzbalaban, M., and Ozdaglar, A. (2019).
\newblock A universally optimal multistage accelerated stochastic gradient
  method.
\newblock {\em Advances in neural information processing systems},
  32:8525--8536.

\bibitem[Bengio, 2015]{bengio2015rmsprop}
Bengio, Y. (2015).
\newblock Rmsprop and equilibrated adaptive learning rates for nonconvex
  optimization.
\newblock {\em corr abs/1502.04390}.

\bibitem[Berrada et~al., 2020]{berrada2020training}
Berrada, L., Zisserman, A., and Kumar, M.~P. (2020).
\newblock Training neural networks for and by interpolation.
\newblock In {\em International Conference on Machine Learning}, pages
  799--809. PMLR.

\bibitem[Bottou et~al., 2018]{bottou2018optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J. (2018).
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM Review}, 60(2):223--311.

\bibitem[Chang and Lin, 2011]{libsvm}
Chang, C.-C. and Lin, C.-J. (2011).
\newblock {LIBSVM}: A library for support vector machines.
\newblock {\em ACM Transactions on Intelligent Systems and Technology},
  2(3):1--27.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Cohen et~al., 2018]{cohen2018acceleration}
Cohen, M., Diakonikolas, J., and Orecchia, L. (2018).
\newblock On acceleration with noise-corrupted gradients.
\newblock In {\em International Conference on Machine Learning}, pages
  1019--1028. PMLR.

\bibitem[Duchi et~al., 2011]{duchi2011adaptive}
Duchi, J.~C., Hazan, E., and Singer, Y. (2011).
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em The Journal of Machine Learning Research}, 12:2121--2159.

\bibitem[Ghadimi and Lan, 2012]{ghadimi2012optimal}
Ghadimi, S. and Lan, G. (2012).
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock {\em SIAM Journal on Optimization}, 22(4):1469--1492.

\bibitem[Ghadimi and Lan, 2013]{ghadimi2013optimal}
Ghadimi, S. and Lan, G. (2013).
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization, ii: shrinking procedures and optimal
  algorithms.
\newblock {\em SIAM Journal on Optimization}, 23(4):2061--2089.

\bibitem[Gower et~al., 2021]{gower2021sgd}
Gower, R., Sebbouh, O., and Loizou, N. (2021).
\newblock Sgd for structured nonconvex functions: Learning rates, minibatching
  and interpolation.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1315--1323. PMLR.

\bibitem[Gower et~al., 2019]{gower2019sgd}
Gower, R.~M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E., and
  Richt{\'a}rik, P. (2019).
\newblock {SGD}: General analysis and improved rates.
\newblock In {\em ICML}.

\bibitem[Hinder et~al., 2020]{hinder2020near}
Hinder, O., Sidford, A., and Sohoni, N. (2020).
\newblock Near-optimal methods for minimizing star-convex functions and beyond.
\newblock In {\em Conference on Learning Theory}, pages 1894--1938. PMLR.

\bibitem[Jain et~al., 2018]{jain2018accelerating}
Jain, P., Kakade, S.~M., Kidambi, R., Netrapalli, P., and Sidford, A. (2018).
\newblock Accelerating stochastic gradient descent for least squares
  regression.
\newblock In {\em Conference On Learning Theory}, pages 545--604. PMLR.

\bibitem[Karimi et~al., 2016]{karimi2016linear}
Karimi, H., Nutini, J., and Schmidt, M. (2016).
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer.

\bibitem[Khaled and Richt{\'a}rik, 2020]{khaled2020better}
Khaled, A. and Richt{\'a}rik, P. (2020).
\newblock Better theory for sgd in the nonconvex world.
\newblock {\em arXiv preprint arXiv:2002.03329}.

\bibitem[Kingma and Ba, 2015]{kingma2014adam}
Kingma, D. and Ba, J. (2015).
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em {ICLR}}.

\bibitem[Kleinberg et~al., 2018]{kleinberg2018alternative}
Kleinberg, B., Li, Y., and Yuan, Y. (2018).
\newblock An alternative view: When does sgd escape local minima?
\newblock In {\em International Conference on Machine Learning}, pages
  2698--2707. PMLR.

\bibitem[Kulunchakov and Mairal, 2019]{kulunchakov2019estimate}
Kulunchakov, A. and Mairal, J. (2019).
\newblock Estimate sequences for variance-reduced stochastic composite
  optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  3541--3550. PMLR.

\bibitem[Levy et~al., 2018]{levy2018online}
Levy, K.~Y., Yurtsever, A., and Cevher, V. (2018).
\newblock Online adaptive methods, universality and acceleration.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}.

\bibitem[Li and Orabona, 2019]{li2018convergence}
Li, X. and Orabona, F. (2019).
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In {\em AISTATS}.

\bibitem[Li et~al., 2020]{li2020second}
Li, X., Zhuang, Z., and Orabona, F. (2020).
\newblock A second look at exponential and cosine step sizes: Simplicity,
  convergence, and performance.
\newblock {\em arXiv preprint arXiv:2002.05273}.

\bibitem[Liu and Belkin, 2018]{liu2018accelerating}
Liu, C. and Belkin, M. (2018).
\newblock Accelerating sgd with momentum for over-parameterized learning.
\newblock {\em arXiv preprint arXiv:1810.13395}.

\bibitem[Lohr, 2019]{lohr2019sampling}
Lohr, S.~L. (2019).
\newblock {\em {Sampling: Design and Analysis: Design and Analysis}}.
\newblock Chapman and Hall/CRC.

\bibitem[Loizou et~al., 2021]{loizou2021stochastic}
Loizou, N., Vaswani, S., Laradji, I.~H., and Lacoste-Julien, S. (2021).
\newblock Stochastic polyak step-size for sgd: An adaptive learning rate for
  fast convergence.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1306--1314. PMLR.

\bibitem[Lucas et~al., 2021]{lucas2021analyzing}
Lucas, J., Bae, J., Zhang, M.~R., Fort, S., Zemel, R., and Grosse, R. (2021).
\newblock Analyzing monotonic linear interpolation in neural network loss
  landscapes.
\newblock {\em arXiv preprint arXiv:2104.11044}.

\bibitem[Mishkin, 2020]{mishkin2020interpolation}
Mishkin, A. (2020).
\newblock {\em Interpolation, growth conditions, and stochastic gradient
  descent}.
\newblock PhD thesis, University of British Columbia.

\bibitem[Moulines and Bach, 2011]{moulines2011non}
Moulines, E. and Bach, F. (2011).
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock {\em Advances in neural information processing systems}, 24:451--459.

\bibitem[Nemirovski and Yudin, 1983]{NemYudin1983book}
Nemirovski, A. and Yudin, D.~B. (1983).
\newblock {\em Problem complexity and method efficiency in optimization}.
\newblock Wiley Interscience.

\bibitem[Nesterov, 2004]{nesterov2013introductory}
Nesterov, Y. (2004).
\newblock {\em Introductory lectures on convex optimization: A basic course}.
\newblock Springer Science \& Business Media.

\bibitem[Nguyen et~al., 2018]{nguyen2018tight}
Nguyen, P.~H., Nguyen, L.~M., and van Dijk, M. (2018).
\newblock Tight dimension independent lower bound on the expected convergence
  rate for diminishing step sizes in sgd.
\newblock {\em arXiv preprint arXiv:1810.04723}.

\bibitem[Nocedal and Wright, 2006]{nocedal2006numerical}
Nocedal, J. and Wright, S. (2006).
\newblock {\em Numerical optimization}.
\newblock Springer Science \& Business Media.

\bibitem[Robbins and Monro, 1951]{robbins1951stochastic}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em The Annals of Mathematical Statistics}, pages 400--407.

\bibitem[Schmidt and Roux, 2013]{schmidt2013fast}
Schmidt, M. and Roux, N.~L. (2013).
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition.
\newblock {\em arXiv preprint arXiv:1308.6370}.

\bibitem[Stich, 2019]{stich2019unified}
Stich, S.~U. (2019).
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock {\em arXiv preprint arXiv:1907.04232}.

\bibitem[Vaswani et~al., 2019a]{vaswani2019fast}
Vaswani, S., Bach, F., and Schmidt, M. (2019a).
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1195--1204. PMLR.

\bibitem[Vaswani et~al., 2020]{vaswani2020adaptive}
Vaswani, S., Laradji, I.~H., Kunstner, F., Meng, S.~Y., Schmidt, M., and
  Lacoste-Julien, S. (2020).
\newblock Adaptive gradient methods converge faster with over-parameterization
  (and you can do a line-search).

\bibitem[Vaswani et~al., 2019b]{vaswani2019painless}
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., and
  Lacoste-Julien, S. (2019b).
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock {\em Advances in neural information processing systems},
  32:3732--3745.

\bibitem[Zhang and Zhou, 2019]{zhang2019stochastic}
Zhang, L. and Zhou, Z.-H. (2019).
\newblock Stochastic approximation of smooth and strongly convex functions:
  Beyond the $ o (1/t) $ convergence rate.
\newblock In {\em Conference on Learning Theory}, pages 3160--3179. PMLR.

\end{thebibliography}
