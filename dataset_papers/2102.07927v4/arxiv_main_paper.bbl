\begin{thebibliography}{10}

\bibitem{Alemi2018FixingAB}
A.~A. Alemi, B.~Poole, I.~S. Fischer, J.~V. Dillon, R.~A. Saurous, and K.~P.
  Murphy.
\newblock Fixing a broken elbo.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{arenz2020trust}
O.~Arenz, M.~Zhong, and G.~Neumann.
\newblock Trust-region variational inference with {G}aussian mixture models.
\newblock {\em Journal of Machine Learning Research}, 21(163):1--60, 2020.

\bibitem{asuncion2007uci}
A.~Asuncion and D.~Newman.
\newblock {UCI} machine learning repository, 2007.

\bibitem{bartlett2017spectrally}
P.~Bartlett, D.~J. Foster, and M.~Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock {\em arXiv preprint arXiv:1706.08498}, 2017.

\bibitem{berg2018sylvester}
R.~v.~d. Berg, L.~Hasenclever, J.~M. Tomczak, and M.~Welling.
\newblock Sylvester normalizing flows for variational inference.
\newblock {\em arXiv preprint arXiv:1803.05649}, 2018.

\bibitem{bischof1994orthogonal}
C.~H. Bischof and X.~Sun.
\newblock On orthogonal block elimination.
\newblock {\em Preprint MCS-P450-0794, Mathematics and Computer Science
  Division, Argonne National Laboratory}, 1994.

\bibitem{blanchet2020machine}
J.~Blanchet, Y.~Kang, J.~L.~M. Olea, V.~A. Nguyen, and X.~Zhang.
\newblock Machine learning's dropout training is distributionally robust
  optimal.
\newblock {\em arXiv preprint arXiv:2009.06111}, 2020.

\bibitem{blundell2015weight}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622. PMLR, 2015.

\bibitem{burger2003analysis}
M.~Burger and A.~Neubauer.
\newblock Analysis of tikhonov regularization for function approximation by
  neural networks.
\newblock {\em Neural Networks}, 16(1):79--90, 2003.

\bibitem{camuto2020explicit}
A.~Camuto, M.~Willetts, U.~{\c{S}}im{\c{s}}ekli, S.~Roberts, and C.~Holmes.
\newblock Explicit regularisation in {G}aussian noise injections.
\newblock {\em arXiv preprint arXiv:2007.07368}, 2020.

\bibitem{coates2011analysis}
A.~Coates, A.~Ng, and H.~Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In {\em Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 215--223. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem{cui2020informative}
T.~Cui, A.~Havulinna, P.~Marttinen, and S.~Kaski.
\newblock Informative {G}aussian scale mixture priors for {B}ayesian neural
  networks.
\newblock {\em arXiv preprint arXiv:2002.10243}, 2020.

\bibitem{daxberger2020expressive}
E.~Daxberger, E.~Nalisnick, J.~U. Allingham, J.~Antor{\'a}n, and J.~M.
  Hern{\'a}ndez-Lobato.
\newblock Expressive yet tractable bayesian deep learning via subnetwork
  inference.
\newblock {\em arXiv preprint arXiv:2010.14689}, 2020.

\bibitem{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of machine learning research}, 12(7), 2011.

\bibitem{dusenberry2020efficient}
M.~Dusenberry, G.~Jerfel, Y.~Wen, Y.~Ma, J.~Snoek, K.~Heller,
  B.~Lakshminarayanan, and D.~Tran.
\newblock Efficient and scalable {B}ayesian neural nets with rank-1 factors.
\newblock In {\em International conference on machine learning}, pages
  2782--2792. PMLR, 2020.

\bibitem{efron2012large}
B.~Efron.
\newblock {\em Large-scale inference: empirical Bayes methods for estimation,
  testing, and prediction}, volume~1.
\newblock Cambridge University Press, 2012.

\bibitem{foong2019expressiveness}
A.~Y. Foong, D.~R. Burt, Y.~Li, and R.~E. Turner.
\newblock On the expressiveness of approximate inference in {B}ayesian neural
  networks.
\newblock {\em arXiv preprint arXiv:1909.00719}, 2019.

\bibitem{gal2016uncertainty}
Y.~Gal.
\newblock Uncertainty in deep learning.
\newblock {\em University of Cambridge}, 1(3), 2016.

\bibitem{gal2015bayesian}
Y.~Gal and Z.~Ghahramani.
\newblock {B}ayesian convolutional neural networks with bernoulli approximate
  variational inference.
\newblock {\em arXiv preprint arXiv:1506.02158}, 2015.

\bibitem{gal2016dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059, 2016.

\bibitem{ghosh2018structured}
S.~Ghosh, J.~Yao, and F.~Doshi-Velez.
\newblock Structured variational learning of bayesian neural networks with
  horseshoe priors.
\newblock In {\em International Conference on Machine Learning}, pages
  1744--1753. PMLR, 2018.

\bibitem{graves2011practical}
A.~Graves.
\newblock Practical variational inference for neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2348--2356, 2011.

\bibitem{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1321--1330. PMLR, 2017.

\bibitem{gupta2018matrix}
A.~K. Gupta and D.~K. Nagar.
\newblock {\em Matrix variate distributions}, volume 104.
\newblock CRC Press, 2018.

\bibitem{gupta2017unified}
V.~Gupta, T.~Koren, and Y.~Singer.
\newblock A unified approach to adaptive regularization in online and
  stochastic optimization.
\newblock {\em arXiv preprint arXiv:1706.06569}, 2017.

\bibitem{helmbold2015inductive}
D.~P. Helmbold and P.~M. Long.
\newblock On the inductive bias of dropout.
\newblock {\em The Journal of Machine Learning Research}, 16(1):3403--3454,
  2015.

\bibitem{helmbold2017surprising}
D.~P. Helmbold and P.~M. Long.
\newblock Surprising properties of dropout in deep networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):7284--7311,
  2017.

\bibitem{hernandez2015probabilistic}
J.~M. Hern{\'a}ndez-Lobato and R.~Adams.
\newblock Probabilistic backpropagation for scalable learning of {B}ayesian
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1869, 2015.

\bibitem{hinton2012improving}
G.~E. Hinton, N.~Srivastava, A.~Krizhevsky, I.~Sutskever, and R.~R.
  Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em arXiv preprint arXiv:1207.0580}, 2012.

\bibitem{hinton1993keeping}
G.~E. Hinton and D.~Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In {\em Proceedings of the sixth annual conference on Computational
  learning theory}, pages 5--13, 1993.

\bibitem{hron2018variational}
J.~Hron, A.~Matthews, and Z.~Ghahramani.
\newblock Variational {B}ayesian dropout: pitfalls and fixes.
\newblock In {\em International Conference on Machine Learning}, pages
  2019--2028. PMLR, 2018.

\bibitem{izmailov2020subspace}
P.~Izmailov, W.~J. Maddox, P.~Kirichenko, T.~Garipov, D.~Vetrov, and A.~G.
  Wilson.
\newblock Subspace inference for bayesian deep learning.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1169--1179.
  PMLR, 2020.

\bibitem{jordan1999introduction}
M.~I. Jordan, Z.~Ghahramani, T.~S. Jaakkola, and L.~K. Saul.
\newblock An introduction to variational methods for graphical models.
\newblock {\em Machine learning}, 37(2):183--233, 1999.

\bibitem{keskar2017improving}
N.~S. Keskar and R.~Socher.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock {\em arXiv preprint arXiv:1712.07628}, 2017.

\bibitem{khan2018fast}
M.~Khan, D.~Nielsen, V.~Tangkaratt, W.~Lin, Y.~Gal, and A.~Srivastava.
\newblock Fast and scalable {B}ayesian deep learning by weight-perturbation in
  adam.
\newblock In {\em International Conference on Machine Learning}, pages
  2611--2620. PMLR, 2018.

\bibitem{kharitonov2018variational}
V.~Kharitonov, D.~Molchanov, and D.~Vetrov.
\newblock Variational dropout via empirical {B}ayes.
\newblock {\em arXiv preprint arXiv:1811.00596}, 2018.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2016improved}
D.~P. Kingma, T.~Salimans, R.~Jozefowicz, X.~Chen, I.~Sutskever, and
  M.~Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock {\em Advances in neural information processing systems},
  29:4743--4751, 2016.

\bibitem{kingma2015variational}
D.~P. Kingma, T.~Salimans, and M.~Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In {\em Advances in neural information processing systems}, pages
  2575--2583, 2015.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{koller2009probabilistic}
D.~Koller and N.~Friedman.
\newblock {\em Probabilistic graphical models: principles and techniques}.
\newblock MIT press, 2009.

\bibitem{krishnan2020specifying}
R.~Krishnan, M.~Subedar, and O.~Tickoo.
\newblock Specifying weight priors in bayesian deep neural networks with
  empirical bayes.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4477--4484, 2020.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lakshminarayanan2016simple}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock {\em arXiv preprint arXiv:1612.01474}, 2016.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{louizos2017bayesian}
C.~Louizos, K.~Ullrich, and M.~Welling.
\newblock Bayesian compression for deep learning.
\newblock {\em arXiv preprint arXiv:1705.08665}, 2017.

\bibitem{louizos2016structured}
C.~Louizos and M.~Welling.
\newblock Structured and efficient variational deep learning with matrix
  {G}aussian posteriors.
\newblock In {\em International Conference on Machine Learning}, pages
  1708--1716, 2016.

\bibitem{louizos2017multiplicative}
C.~Louizos and M.~Welling.
\newblock Multiplicative normalizing flows for variational {B}ayesian neural
  networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2218--2227. JMLR. org, 2017.

\bibitem{mackay1992practical}
D.~J. MacKay.
\newblock A practical {B}ayesian framework for backpropagation networks.
\newblock {\em Neural computation}, 4(3):448--472, 1992.

\bibitem{maddox2019simple}
W.~J. Maddox, P.~Izmailov, T.~Garipov, D.~P. Vetrov, and A.~G. Wilson.
\newblock A simple baseline for {B}ayesian uncertainty in deep learning.
\newblock {\em Advances in Neural Information Processing Systems},
  32:13153--13164, 2019.

\bibitem{maeda2014bayesian}
S.-i. Maeda.
\newblock A {B}ayesian encourages dropout.
\newblock {\em arXiv preprint arXiv:1412.7003}, 2014.

\bibitem{mcallester2013pac}
D.~McAllester.
\newblock A pac-{B}ayesian tutorial with a dropout bound.
\newblock {\em arXiv preprint arXiv:1307.2118}, 2013.

\bibitem{mianjy2019dropout}
P.~Mianjy and R.~Arora.
\newblock On dropout and nuclear norm regularization.
\newblock In {\em International Conference on Machine Learning}, pages
  4575--4584. PMLR, 2019.

\bibitem{mianjy2020convergence}
P.~Mianjy and R.~Arora.
\newblock On convergence and generalization of dropout training.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{mianjy2018implicit}
P.~Mianjy, R.~Arora, and R.~Vidal.
\newblock On the implicit bias of dropout.
\newblock In {\em International Conference on Machine Learning}, pages
  3537--3545, 2018.

\bibitem{mishkin2018slang}
A.~Mishkin, F.~Kunstner, D.~Nielsen, M.~Schmidt, and M.~E. Khan.
\newblock Slang: Fast structured covariance approximations for {B}ayesian deep
  learning with natural gradient.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6245--6255, 2018.

\bibitem{molchanov2017variational}
D.~Molchanov, A.~Ashukha, and D.~Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2498--2507. PMLR, 2017.

\bibitem{molchanov2019doubly}
D.~Molchanov, V.~Kharitonov, A.~Sobolev, and D.~Vetrov.
\newblock Doubly semi-implicit variational inference.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2593--2602, 2019.

\bibitem{mou2018dropout}
W.~Mou, Y.~Zhou, J.~Gao, and L.~Wang.
\newblock Dropout training, data-dependent regularization, and generalization
  bounds.
\newblock In {\em International Conference on Machine Learning}, pages
  3645--3653, 2018.

\bibitem{mukhoti2018importance}
J.~Mukhoti, P.~Stenetorp, and Y.~Gal.
\newblock On the importance of strong baselines in {B}ayesian deep learning.
\newblock {\em arXiv preprint arXiv:1811.09385}, 2018.

\bibitem{naeini2015obtaining}
M.~P. Naeini, G.~Cooper, and M.~Hauskrecht.
\newblock Obtaining well calibrated probabilities using {B}ayesian binning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~29, 2015.

\bibitem{nalisnick2019dropout}
E.~Nalisnick, J.~M. Hern{\'a}ndez-Lobato, and P.~Smyth.
\newblock Dropout as a structured shrinkage prior.
\newblock In {\em International Conference on Machine Learning}, pages
  4712--4722. PMLR, 2019.

\bibitem{neal1995bayesian}
R.~M. Neal.
\newblock {\em {B}ayesian Learning for Neural Networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{neyshabur2017pac}
B.~Neyshabur, S.~Bhojanapalli, and N.~Srebro.
\newblock A pac-{B}ayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock {\em arXiv preprint arXiv:1707.09564}, 2017.

\bibitem{oh2019radial}
C.~Oh, K.~Adamczewski, and M.~Park.
\newblock Radial and directional posteriors for {B}ayesian neural networks.
\newblock {\em arXiv preprint arXiv:1902.02603}, 2019.

\bibitem{ong2018gaussian}
V.~M.-H. Ong, D.~J. Nott, and M.~S. Smith.
\newblock {G}aussian variational approximation with a factor covariance
  structure.
\newblock {\em Journal of Computational and Graphical Statistics},
  27(3):465--478, 2018.

\bibitem{osawa2019practical}
K.~Osawa, S.~Swaroop, A.~Jain, R.~Eschenhagen, R.~E. Turner, R.~Yokota, and
  M.~E. Khan.
\newblock Practical deep learning with {B}ayesian principles.
\newblock {\em arXiv preprint arXiv:1906.02506}, 2019.

\bibitem{ranganath2016hierarchical}
R.~Ranganath, D.~Tran, and D.~Blei.
\newblock Hierarchical variational models.
\newblock In {\em International Conference on Machine Learning}, pages
  324--333, 2016.

\bibitem{rezende2015variational}
D.~Rezende and S.~Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages
  1530--1538. PMLR, 2015.

\bibitem{ritter2018scalable}
H.~Ritter, A.~Botev, and D.~Barber.
\newblock A scalable laplace approximation for neural networks.
\newblock In {\em 6th International Conference on Learning Representations,
  ICLR 2018-Conference Track Proceedings}, volume~6. International Conference
  on Representation Learning, 2018.

\bibitem{rossi2019walsh}
S.~Rossi, S.~Marmin, and M.~Filippone.
\newblock Walsh-hadamard variational inference for {B}ayesian deep learning.
\newblock {\em arXiv preprint arXiv:1905.11248}, 2019.

\bibitem{sagun2017empirical}
L.~Sagun, U.~Evci, V.~U. Guney, Y.~Dauphin, and L.~Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em arXiv preprint arXiv:1706.04454}, 2017.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{sun2017learning}
S.~Sun, C.~Chen, and L.~Carin.
\newblock Learning structured weight uncertainty in {B}ayesian neural networks.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1283--1292,
  2017.

\bibitem{sun1995basis}
X.~Sun and C.~Bischof.
\newblock A basis-kernel representation of orthogonal matrices.
\newblock {\em SIAM journal on matrix analysis and applications},
  16(4):1184--1196, 1995.

\bibitem{swiatkowski2020k}
J.~Swiatkowski, K.~Roth, B.~Veeling, L.~Tran, J.~Dillon, J.~Snoek, S.~Mandt,
  T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock The k-tied normal distribution: A compact parameterization of
  {G}aussian mean field posteriors in {B}ayesian neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  9289--9299. PMLR, 2020.

\bibitem{teye2018bayesian}
M.~Teye, H.~Azizpour, and K.~Smith.
\newblock Bayesian uncertainty estimation for batch normalized deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  4907--4916. PMLR, 2018.

\bibitem{tomczak2017improving}
J.~M. Tomczak and M.~Welling.
\newblock Improving variational auto-encoders using convex combination linear
  inverse autoregressive flow.
\newblock {\em arXiv preprint arXiv:1706.02326}, 2017.

\bibitem{tomczak2020efficient}
M.~Tomczak, S.~Swaroop, and R.~Turner.
\newblock Efficient low rank {G}aussian variational inference for neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{wager2013dropout}
S.~Wager, S.~Wang, and P.~S. Liang.
\newblock Dropout training as adaptive regularization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  351--359, 2013.

\bibitem{wang2013fast}
S.~Wang and C.~Manning.
\newblock Fast dropout training.
\newblock In {\em international conference on machine learning}, pages
  118--126. PMLR, 2013.

\bibitem{wei2020implicit}
C.~Wei, S.~Kakade, and T.~Ma.
\newblock The implicit and explicit regularization effects of dropout.
\newblock In {\em International Conference on Machine Learning}, pages
  10181--10192. PMLR, 2020.

\bibitem{wenzel2020good}
F.~Wenzel, K.~Roth, B.~S. Veeling, J.~{\'S}wi{\k{a}}tkowski, L.~Tran, S.~Mandt,
  J.~Snoek, T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock {\em arXiv preprint arXiv:2002.02405}, 2020.

\bibitem{wilson2017marginal}
A.~C. Wilson, R.~Roelofs, M.~Stern, N.~Srebro, and B.~Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock {\em arXiv preprint arXiv:1705.08292}, 2017.

\bibitem{wilson2020bayesian}
A.~G. Wilson and P.~Izmailov.
\newblock {B}ayesian deep learning and a probabilistic perspective of
  generalization.
\newblock {\em arXiv preprint arXiv:2002.08791}, 2020.

\bibitem{wu2018deterministic}
A.~Wu, S.~Nowozin, E.~Meeds, R.~E. Turner, J.~M. Hernandez-Lobato, and A.~L.
  Gaunt.
\newblock Deterministic variational inference for robust bayesian neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.03958}, 2018.

\bibitem{yin2018semi}
M.~Yin and M.~Zhou.
\newblock Semi-implicit variational inference.
\newblock In {\em International Conference on Machine Learning}, pages
  5660--5669. PMLR, 2018.

\bibitem{zhang2018noisy}
G.~Zhang, S.~Sun, D.~Duvenaud, and R.~Grosse.
\newblock Noisy natural gradient as variational inference.
\newblock In {\em International Conference on Machine Learning}, pages
  5852--5861. PMLR, 2018.

\bibitem{zhang2018stabilizing}
J.~Zhang, Q.~Lei, and I.~Dhillon.
\newblock Stabilizing gradients for deep neural networks via efficient svd
  parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  5806--5814. PMLR, 2018.

\bibitem{zhao2019learning}
H.~Zhao, Y.-H.~H. Tsai, R.~Salakhutdinov, and G.~J. Gordon.
\newblock Learning neural networks with adaptive regularization.
\newblock {\em arXiv preprint arXiv:1907.06288}, 2019.

\bibitem{zhao2019infovae}
S.~Zhao, J.~Song, and S.~Ermon.
\newblock Infovae: Balancing learning and inference in variational
  autoencoders.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pages 5885--5892, 2019.

\bibitem{zhou2020towards}
P.~Zhou, J.~Feng, C.~Ma, C.~Xiong, S.~Hoi, et~al.
\newblock Towards theoretically understanding why sgd generalizes better than
  adam in deep learning.
\newblock {\em arXiv preprint arXiv:2010.05627}, 2020.

\bibitem{zobay2014variational}
O.~Zobay et~al.
\newblock Variational {B}ayesian inference with {G}aussian-mixture
  approximations.
\newblock {\em Electronic Journal of Statistics}, 8(1):355--389, 2014.

\end{thebibliography}
