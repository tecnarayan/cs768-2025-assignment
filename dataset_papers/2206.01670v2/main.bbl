\begin{thebibliography}{10}

\bibitem{xu2016msr}
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
\newblock Msr-vtt: A large video description dataset for bridging video and
  language.
\newblock In {\em CVPR}, pages 5288--5296, 2016.

\bibitem{patrick2020support}
Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann,
  Joao Henriques, and Andrea Vedaldi.
\newblock Support-set bottlenecks for video-text representation learning.
\newblock In {\em ICLR}, 2020.

\bibitem{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In {\em ICCV}, pages 1728--1738, 2021.

\bibitem{msrvttqamsvdqa}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In {\em MM}, pages 1645--1653, 2017.

\bibitem{yu2018joint}
Youngjae Yu, Jongseok Kim, and Gunhee Kim.
\newblock A joint sequence fusion model for video question answering and
  retrieval.
\newblock In {\em ECCV}, pages 471--487, 2018.

\bibitem{zhu2020actbert}
Linchao Zhu and Yi~Yang.
\newblock Actbert: Learning global-local video-text representations.
\newblock In {\em CVPR}, pages 8746--8755, 2020.

\bibitem{krishna2017dense}
Ranjay Krishna, Kenji Hata, Frederic Ren, Li~Fei-Fei, and Juan Carlos~Niebles.
\newblock Dense-captioning events in videos.
\newblock In {\em ICCV}, pages 706--715, 2017.

\bibitem{wang2018reconstruction}
Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu.
\newblock Reconstruction network for video captioning.
\newblock In {\em CVPR}, pages 7622--7631, 2018.

\bibitem{zhou2018end}
Luowei Zhou, Yingbo Zhou, Jason~J Corso, Richard Socher, and Caiming Xiong.
\newblock End-to-end dense video captioning with masked transformer.
\newblock In {\em CVPR}, pages 8739--8748, 2018.

\bibitem{miech2019howto100m}
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan
  Laptev, and Josef Sivic.
\newblock Howto100m: Learning a text-video embedding by watching hundred
  million narrated video clips.
\newblock In {\em ICCV}, pages 2630--2640, 2019.

\bibitem{caba2015activitynet}
Fabian Caba~Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos~Niebles.
\newblock Activitynet: A large-scale video benchmark for human activity
  understanding.
\newblock In {\em CVPR}, pages 961--970, 2015.

\bibitem{abu2018will}
Yazan Abu~Farha, Alexander Richard, and Juergen Gall.
\newblock When will you do what?-anticipating temporal occurrences of
  activities.
\newblock In {\em CVPR}, pages 5343--5352, 2018.

\bibitem{ma2002user}
Yu-Fei Ma, Lie Lu, Hong-Jiang Zhang, and Mingjing Li.
\newblock A user attention model for video summarization.
\newblock In {\em MM}, pages 533--542, 2002.

\bibitem{kazakos2019epic}
Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen.
\newblock Epic-fusion: Audio-visual temporal binding for egocentric action
  recognition.
\newblock In {\em ICCV}, pages 5492--5501, 2019.

\bibitem{grauman2021ego4d}
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino
  Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu,
  et~al.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video.
\newblock In {\em CVPR}, pages 18995--19012, 2022.

\bibitem{zhou2018towards}
Luowei Zhou, Chenliang Xu, and Jason~J Corso.
\newblock Towards automatic learning of procedures from web instructional
  videos.
\newblock In {\em AAAI}, 2018.

\bibitem{sigurdsson2018charades}
Gunnar~A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek
  Alahari.
\newblock Charades-ego: A large-scale dataset of paired third and first person
  videos.
\newblock {\em arXiv preprint arXiv:1804.09626}, 2018.

\bibitem{lee2012discovering}
Yong~Jae Lee, Joydeep Ghosh, and Kristen Grauman.
\newblock Discovering important people and objects for egocentric video
  summarization.
\newblock In {\em CVPR}, pages 1346--1353. IEEE, 2012.

\bibitem{fathi2012social}
Alircza Fathi, Jessica~K Hodgins, and James~M Rehg.
\newblock Social interactions: A first-person perspective.
\newblock In {\em CVPR}, pages 1226--1233. IEEE, 2012.

\bibitem{damen2022rescaling}
Dima Damen, Hazel Doughty, Giovanni~Maria Farinella, Antonino Furnari,
  Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett,
  Will Price, et~al.
\newblock Rescaling egocentric vision: Collection, pipeline and challenges for
  epic-kitchens-100.
\newblock {\em IJCV}, 130(1):33--55, 2022.

\bibitem{anne2017localizing}
Lisa Anne~Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
  and Bryan Russell.
\newblock Localizing moments in video with natural language.
\newblock In {\em ICCV}, pages 5803--5812, 2017.

\bibitem{chen2017sca}
Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu, and
  Tat-Seng Chua.
\newblock Sca-cnn: Spatial and channel-wise attention in convolutional networks
  for image captioning.
\newblock In {\em CVPR}, pages 5659--5667, 2017.

\bibitem{miech2020end}
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic,
  and Andrew Zisserman.
\newblock End-to-end learning of visual representations from uncurated
  instructional videos.
\newblock In {\em CVPR}, pages 9879--9889, 2020.

\bibitem{lei2021less}
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara~L Berg, Mohit Bansal, and
  Jingjing Liu.
\newblock Less is more: Clipbert for video-and-language learning via sparse
  sampling.
\newblock In {\em CVPR}, pages 7331--7341, 2021.

\bibitem{Sun_2019_ICCV}
Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid.
\newblock Videobert: A joint model for video and language representation
  learning.
\newblock In {\em ICCV}, October 2019.

\bibitem{wang2022object}
Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu
  Qie, and Mike~Zheng Shou.
\newblock Object-aware video-language pre-training for retrieval.
\newblock In {\em CVPR}, pages 3313--3322, 2022.

\bibitem{escorcia2019temporal}
Victor Escorcia, Mattia Soldan, Josef Sivic, Bernard Ghanem, and Bryan Russell.
\newblock Temporal localization of moments in video collections with natural
  language.
\newblock {\em arXiv preprint arXiv:1907.12763}, 2019.

\bibitem{miech2021thinking}
Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew
  Zisserman.
\newblock Thinking fast and slow: Efficient text-to-visual retrieval with
  transformers.
\newblock In {\em CVPR}, pages 9826--9836, 2021.

\bibitem{infonce}
Aaron Van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv e-prints}, pages arXiv--1807, 2018.

\bibitem{wong2022assistq}
Benita Wong, Joya Chen, You Wu, Stan~Weixian Lei, Dongxing Mao, Difei Gao, and
  Mike~Zheng Shou.
\newblock Assistq: Affordance-centric question-driven task completion for
  egocentric assistant.
\newblock In {\em ECCV}, 2022.

\bibitem{li2015delving}
Yin Li, Zhefan Ye, and James~M Rehg.
\newblock Delving into egocentric actions.
\newblock In {\em CVPR}, pages 287--295, 2015.

\bibitem{timesformer}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In {\em ICML}, volume~2, page~4, 2021.

\bibitem{distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{slowfast}
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
\newblock Slowfast networks for video recognition.
\newblock In {\em ICCV}, pages 6202--6211, 2019.

\bibitem{liu2022video}
Ze~Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
\newblock Video swin transformer.
\newblock In {\em CVPR}, pages 3202--3211, 2022.

\bibitem{mil_nce}
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic,
  and Andrew Zisserman.
\newblock End-to-end learning of visual representations from uncurated
  instructional videos.
\newblock In {\em CVPR}, pages 9879--9889, 2020.

\bibitem{wray2021semantic}
Michael Wray, Hazel Doughty, and Dima Damen.
\newblock On semantic similarity in video retrieval.
\newblock In {\em CVPR}, 2021.

\bibitem{hendricks2018localizing}
Lisa~Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
  and Bryan Russell.
\newblock Localizing moments in video with temporal language.
\newblock In {\em EMNLP}, 2018.

\bibitem{Gao_2017_ICCV}
{Gao Jiyang, Sun Chen, Yang Zhenheng, Nevatia, Ram}.
\newblock {TALL: Temporal Activity Localization via Language Query}.
\newblock In {\em ICCV}, 2017.

\bibitem{soldan2021vlg}
Mattia Soldan, Mengmeng Xu, Sisi Qu, Jesper Tegner, and Bernard Ghanem.
\newblock Vlg-net: Video-language graph matching network for video grounding.
\newblock In {\em ICCV}, pages 3224--3234, 2021.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{xie2018rethinking}
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
\newblock Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs
  in video classification.
\newblock In {\em ECCV}, pages 305--321, 2018.

\bibitem{wray2019fine}
Michael Wray, Diane Larlus, Gabriela Csurka, and Dima Damen.
\newblock Fine-grained action retrieval through multiple parts-of-speech
  embeddings.
\newblock In {\em ICCV}, pages 450--459, 2019.

\bibitem{zhang2020learning}
Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo.
\newblock Learning 2d temporal adjacent networks for moment localization with
  natural language.
\newblock In {\em AAAI}, volume~34, pages 12870--12877, 2020.

\bibitem{zhang2020span}
Hao Zhang, Aixin Sun, Wei Jing, and Joey~Tianyi Zhou.
\newblock Span-based localizing network for natural language video
  localization.
\newblock In {\em ACL}, pages 6543--6554, 2020.

\bibitem{sigurdsson2018actor}
Gunnar~A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek
  Alahari.
\newblock Actor and observer: Joint modeling of first and third-person videos.
\newblock In {\em CVPR}, pages 7396--7404, 2018.

\bibitem{choi2020unsupervised}
Jinwoo Choi, Gaurav Sharma, Manmohan Chandraker, and Jia-Bin Huang.
\newblock Unsupervised and semi-supervised domain adaptation for action
  recognition from drones.
\newblock In {\em WACV}, pages 1717--1726, 2020.

\bibitem{li2021ego}
Yanghao Li, Tushar Nagarajan, Bo~Xiong, and Kristen Grauman.
\newblock Ego-exo: Transferring visual representations from third-person to
  first-person videos.
\newblock In {\em CVPR}, pages 6943--6953, 2021.

\bibitem{zhao2021video}
Chen Zhao, Ali~K Thabet, and Bernard Ghanem.
\newblock Video self-stitching graph network for temporal action localization.
\newblock In {\em ICCV}, pages 13658--13667, 2021.

\bibitem{graves2005bidirectional}
Alex Graves, Santiago Fern{\'a}ndez, and J{\"u}rgen Schmidhuber.
\newblock Bidirectional lstm networks for improved phoneme classification and
  recognition.
\newblock In {\em ICANN}, pages 799--804. Springer, 2005.

\bibitem{carreira2017quo}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In {\em CVPR}, pages 6299--6308, 2017.

\bibitem{lei2021detecting}
Jie Lei, Tamara~L Berg, and Mohit Bansal.
\newblock Detecting moments and highlights in videos via natural language
  queries.
\newblock {\em NeurIPS}, 34:11846--11858, 2021.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2020.

\end{thebibliography}
