\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2020)Agrawal, Sheldon, and Domke]{agrawal2020advances}
Agrawal, A., Sheldon, D.~R., and Domke, J.
\newblock Advances in black-box {VI:} normalizing flows, importance weighting,
  and optimization.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Albergo et~al.(2021)Albergo, Boyda, Hackett, Kanwar, Cranmer,
  Racani{\`e}re, Rezende, and Shanahan]{albergo2021introduction}
Albergo, M.~S., Boyda, D., Hackett, D.~C., Kanwar, G., Cranmer, K.,
  Racani{\`e}re, S., Rezende, D.~J., and Shanahan, P.~E.
\newblock Introduction to normalizing flows for lattice field theory.
\newblock \emph{arXiv preprint arXiv:2101.08176}, 2021.

\bibitem[Bauer \& Mnih(2021)Bauer and Mnih]{bauer2021generalized}
Bauer, M. and Mnih, A.
\newblock Generalized doubly-reparameterized gradient estimators.
\newblock In \emph{Third Symposium on Advances in Approximate Bayesian
  Inference}, 2021.

\bibitem[Bornschein \& Bengio(2015)Bornschein and
  Bengio]{bornschein2014reweighted}
Bornschein, J. and Bengio, Y.
\newblock Reweighted wake-sleep.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Burda et~al.(2016)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Burda, Y., Grosse, R.~B., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{4th International
  Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico,
  May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Chen, T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.
\newblock Neural ordinary differential equations.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pp.\  6572--6583, 2018.

\bibitem[de~Haan et~al.(2021)de~Haan, Rainone, Cheng, and
  Bondesan]{haan2021scaling}
de~Haan, P., Rainone, C., Cheng, M. C.~N., and Bondesan, R.
\newblock Scaling up machine learning for quantum field theory with equivariant
  continuous flows.
\newblock \emph{CoRR}, abs/2110.02673, 2021.

\bibitem[Del~Debbio et~al.(2021)Del~Debbio, Rossney, and
  Wilson]{del2021machine}
Del~Debbio, L., Rossney, J.~M., and Wilson, M.
\newblock Machine learning trivializing maps: A first step towards
  understanding how flow-based samplers scale up.
\newblock \emph{arXiv preprint arXiv:2112.15532}, 2021.

\bibitem[Finke \& Thiery(2019)Finke and Thiery]{finke2019importanceweighted}
Finke, A. and Thiery, A.~H.
\newblock On importance-weighted autoencoders, 2019.

\bibitem[Geffner \& Domke(2020)Geffner and Domke]{geffner2020difficulty}
Geffner, T. and Domke, J.
\newblock On the difficulty of unbiased alpha divergence minimization.
\newblock \emph{arXiv preprint arXiv:2010.09541}, 2020.

\bibitem[Geffner \& Domke(2021)Geffner and Domke]{geffner2021empirical}
Geffner, T. and Domke, J.
\newblock Empirical evaluation of biased methods for alpha divergence
  minimization.
\newblock \emph{arXiv preprint arXiv:2105.06587}, 2021.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Chen, Bettencourt, and
  Duvenaud]{grathwohl2018scalable}
Grathwohl, W., Chen, R. T.~Q., Bettencourt, J., and Duvenaud, D.
\newblock Scalable reversible generative models with free-form continuous
  dynamics.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kanwar(2021)]{kanwar2021machine}
Kanwar, G.
\newblock Machine learning and variational algorithms for lattice field theory.
\newblock \emph{arXiv preprint arXiv:2106.01975}, 2021.

\bibitem[Kanwar et~al.(2020)Kanwar, Albergo, Boyda, Cranmer, Hackett,
  Racaniere, Rezende, and Shanahan]{kanwar2020equivariant}
Kanwar, G., Albergo, M.~S., Boyda, D., Cranmer, K., Hackett, D.~C., Racaniere,
  S., Rezende, D.~J., and Shanahan, P.~E.
\newblock Equivariant flow-based sampling for lattice gauge theory.
\newblock \emph{Physical Review Letters}, 125\penalty0 (12):\penalty0 121601,
  2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
Kingma, D.~P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and
  Welling, M.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4743--4751, 2016.

\bibitem[K{\"{o}}hler et~al.(2020)K{\"{o}}hler, Klein, and
  No{\'{e}}]{kohler2020equivariant}
K{\"{o}}hler, J., Klein, L., and No{\'{e}}, F.
\newblock Equivariant flows: Exact likelihood generative learning for symmetric
  densities.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5361--5370. {PMLR},
  2020.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, McWilliams, Rousselle, Gross, and
  Nov{\'a}k]{muller2019neural}
M{\"u}ller, T., McWilliams, B., Rousselle, F., Gross, M., and Nov{\'a}k, J.
\newblock Neural importance sampling.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 38\penalty0 (5):\penalty0
  1--19, 2019.

\bibitem[Nicoli et~al.(2020)Nicoli, Nakajima, Strodthoff, Samek, M{\"u}ller,
  and Kessel]{nicoli2020asymptotically}
Nicoli, K.~A., Nakajima, S., Strodthoff, N., Samek, W., M{\"u}ller, K.-R., and
  Kessel, P.
\newblock Asymptotically unbiased estimation of physical observables with
  neural samplers.
\newblock \emph{Physical Review E}, 101\penalty0 (2):\penalty0 023304, 2020.

\bibitem[Nicoli et~al.(2021)Nicoli, Anders, Funcke, Hartung, Jansen, Kessel,
  Nakajima, and Stornati]{nicoli2021estimation}
Nicoli, K.~A., Anders, C.~J., Funcke, L., Hartung, T., Jansen, K., Kessel, P.,
  Nakajima, S., and Stornati, P.
\newblock Estimation of thermodynamic observables in lattice field theories
  with deep generative models.
\newblock \emph{Phys. Rev. Lett.}, 126:\penalty0 032001, 2021.

\bibitem[No{\'e} et~al.(2019)No{\'e}, Olsson, K{\"o}hler, and
  Wu]{noe2019boltzmann}
No{\'e}, F., Olsson, S., K{\"o}hler, J., and Wu, H.
\newblock Boltzmann generators: Sampling equilibrium states of many-body
  systems with deep learning.
\newblock \emph{Science}, 365\penalty0 (6457), 2019.

\bibitem[Nowozin(2018)]{nowozin2018debiasing}
Nowozin, S.
\newblock Debiasing evidence approximations: On importance-weighted
  autoencoders and jackknife variational inference.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.

\bibitem[Pontryagin(1987)]{pontryagin1987mathematical}
Pontryagin, L.~S.
\newblock \emph{Mathematical theory of optimal processes}.
\newblock CRC press, 1987.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the 31th International Conference on Machine
  Learning, {ICML} 2014, Beijing, China, 21-26 June 2014}, volume~32 of
  \emph{{JMLR} Workshop and Conference Proceedings}, pp.\  1278--1286.
  JMLR.org, 2014.

\bibitem[Roeder et~al.(2017)Roeder, Wu, and Duvenaud]{roeder2017sticking}
Roeder, G., Wu, Y., and Duvenaud, D.
\newblock Sticking the landing: Simple, lower-variance gradient estimators for
  variational inference.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\
  6925--6934, 2017.

\bibitem[Tucker et~al.(2019)Tucker, Lawson, Gu, and Maddison]{tucker2018doubly}
Tucker, G., Lawson, D., Gu, S., and Maddison, C.~J.
\newblock Doubly reparameterized gradient estimators for monte carlo
  objectives.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[van~den Berg et~al.(2018)van~den Berg, Hasenclever, Tomczak, and
  Welling]{vdberg2018sylvester}
van~den Berg, R., Hasenclever, L., Tomczak, J.~M., and Welling, M.
\newblock Sylvester normalizing flows for variational inference.
\newblock In Globerson, A. and Silva, R. (eds.), \emph{Proceedings of the
  Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, {UAI}
  2018, Monterey, California, USA, August 6-10, 2018}, pp.\  393--402. {AUAI}
  Press, 2018.

\bibitem[Wu et~al.(2019)Wu, Wang, and Zhang]{wu2019solving}
Wu, D., Wang, L., and Zhang, P.
\newblock Solving statistical mechanics using variational autoregressive
  networks.
\newblock \emph{Physical review letters}, 122\penalty0 (8):\penalty0 080602,
  2019.

\end{thebibliography}
