@misc{lichess_puzzles,
  title  = {Lichess tactics puzzles dataset},
  author = {{Lichess team}},
  url    = {https://database.lichess.org/#puzzles}
}

@misc{leela,
title = {{Leela Chess Zero}},
author = {{Leela Chess Zero team}},
url = {https://lczero.org/}
}

@misc{leela_vs_gdm,
title = {How well do {Lc0} networks compare to the greatest transformer network from {DeepMind}?},
author = {lepned},
year = {2024},
url = {https://lczero.org/blog/2024/02/how-well-do-lc0-networks-compare-to-the-greatest-transformer-network-from-deepmind/}
}

@article{alphazero,
author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
title = {A general reinforcement learning algorithm that masters chess, shogi, and {Go} through self-play},
journal = {Science},
year = {2018},
}

@misc{ruoss2024grandmasterlevel,
  title={Grandmaster-Level Chess Without Search}, 
  author={Anian Ruoss and Grégoire Delétang and Sourabh Medapati and Jordi Grau-Moya and Li Kevin Wenliang and Elliot Catt and John Reid and Tim Genewein},
  year={2024},
  eprint={2402.04494},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{futurelens,
    title={{Future Lens}: Anticipating Subsequent Tokens from a Single Hidden State},
    author={Pal, Koyena and Sun, Jiuding and Yuan, Andrew and Wallace, Byron C and Bau, David},
    journal={Conference on Computational Natural Language Learning (CoNLL)},
    year={2023}
}

@misc{brinkmann2024mechanistic,
  title={A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task}, 
  author={Jannik Brinkmann and Abhay Sheshadri and Victor Levoso and Paul Swoboda and Christian Bartelt},
  year={2024},
  eprint={2402.11917},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{oswald2023transformers,
author = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo\~{a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
title = {Transformers learn in-context by gradient descent},
year = {2023},
journal = {ICML},
}


@article{akyurek2023what,
title={What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
journal={ICLR},
year={2023},
}

@misc{oswald2023uncovering,
  title={Uncovering mesa-optimization algorithms in Transformers}, 
  author={Johannes von Oswald and Eyvind Niklasson and Maximilian Schlegel and Seijin Kobayashi and Nicolas Zucchet and Nino Scherrer and Nolan Miller and Mark Sandler and Blaise Agüera y Arcas and Max Vladymyrov and Razvan Pascanu and João Sacramento},
  year={2023},
  eprint={2309.05858},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{hubinger2019risks,
  title={Risks from Learned Optimization in Advanced Machine Learning Systems}, 
  author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
  year={2019},
  eprint={1906.01820},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}

@article{mcgrath2022,
author = {Thomas McGrath  and Andrei Kapishnikov  and Nenad Tomašev  and Adam Pearce  and Martin Wattenberg  and Demis Hassabis  and Been Kim  and Ulrich Paquet  and Vladimir Kramnik },
title = {Acquisition of chess knowledge in {AlphaZero}},
journal = {Proceedings of the National Academy of Sciences},
year = {2022},
}

@misc{schut2023bridging,
  title={Bridging the Human-{AI} Knowledge Gap: Concept Discovery and Transfer in {AlphaZero}}, 
  author={Lisa Schut and Nenad Tomasev and Tom McGrath and Demis Hassabis and Ulrich Paquet and Been Kim},
  year={2023},
  eprint={2310.16410},
  archivePrefix={arXiv},
  primaryClass={cs.AI}
}

@article{
    othellogpt,
    title={Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task},
    author={Kenneth Li and Aspen K Hopkins and David Bau and Fernanda Vi{\'e}gas and Hanspeter Pfister and Martin Wattenberg},
    journal={ICLR},
    year={2023},
}

@misc{othellogpt_neel,
      title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models}, 
      author={Neel Nanda and Andrew Lee and Martin Wattenberg},
      year={2023},
      eprint={2309.00941},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{karvonen2024emergent,
      title={Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models}, 
      author={Adam Karvonen},
      year={2024},
      eprint={2403.15498},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{vig2020,
 author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
 journal = {NeurIPS},
 title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
 year = {2020}
}

@article{meng2022locating,
  title={Locating and editing factual associations in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={NeurIPS},
  year={2022}
}

@article{geiger2021,
 author = {Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
 journal = {NeurIPS},
 title = {Causal Abstractions of Neural Networks},
 year = {2021}
}

@article{veit2016,
 author = {Veit, Andreas and Wilber, Michael J and Belongie, Serge},
 journal = {NeurIPS},
 title = {Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
 year = {2016}
}

@misc{logitlens,
title = {Interpreting {GPT}: the logit lens},
url = {https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
author = {nostalgebraist},
year = {2020}
}

@misc{tunedlens,
      title={Eliciting Latent Predictions from Transformers with the Tuned Lens}, 
      author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
      year={2023},
      eprint={2303.08112},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{din2023jump,
      title={Jump to Conclusions: Short-Cutting Transformers With Linear Transformations}, 
      author={Alexander Yom Din and Taelin Karidi and Leshem Choshen and Mor Geva},
      year={2023},
      eprint={2303.09435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vaswani2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L{}ukasz and Polosukhin, Illia},
 journal = {NeurIPS},
 title = {Attention is All you Need},
 year = {2017}
}

@article{tcec,
  title={The 20th {Top} {Chess} {Engine} {Championship}, {TCEC20}.},
  author={Haworth, Guy and Hernandez, Nelson},
  journal={J. Int. Comput. Games Assoc.},
  year={2021},
  url={https://tcec-chess.com/articles/TCEC_20.pdf}
}

@misc{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  year={2019},
  url={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@article{hewitt2019designing,
author =      "Hewitt, John and Liang, Percy",
title =       "Designing and Interpreting Probes with Control Tasks",
journal =   "EMNLP",
year =        "2019",
}

@misc{heimersheim2024use,
      title={How to use and interpret activation patching}, 
      author={Stefan Heimersheim and Neel Nanda},
      year={2024},
      eprint={2404.15255},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{
zhang2024towards,
title={Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
author={Fred Zhang and Neel Nanda},
journal={ICLR},
year={2024},
}

@software{nnsight,
author = {Jaden Fiotto-Kaufman},
license = {MIT},
title = {{nnsight}: The package for interpreting and manipulating the internals of deep learned models.
},
url = {https://github.com/JadenFiotto-Kaufman/nnsight}
}

@software{iceberg,
    author = {{IceBerg Contributors}},
    license = {MIT},
    title = {{IceBerg} – Compositional Graphics \& Diagramming},
    url = {https://github.com/revalo/iceberg},
    year = {2023}
}

@software{python_chess,
    url = {https://github.com/niklasf/python-chess},
    author = {Niklas Fiekas},
    title = {{python-chess}},
    license = {GLP-3.0}
}

@software{onnx2torch,
  title={onnx2torch},
  author={{ENOT developers} and Kalgin, Igor and Yanchenko, Arseny and Ivanov, Pyoter and Goncharenko, Alexander},
  year={2021},
  url={https://github.com/ENOT-AutoDL/onnx2torch}
}

@article{
    rogozhnikov2022einops,
    title={Einops: Clear and Reliable Tensor Manipulations with {Einstein}-like Notation},
    author={Alex Rogozhnikov},
    journal={ICLR},
    year={2022},
}

@article{pytorch,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, CK and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Suo, Michael and Tillet, Phil and Wang, Eikan and Wang, Xiaodong and Wen, William and Zhang, Shunting and Zhao, Xu and Zhou, Keren and Zou, Richard and Mathews, Ajit and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
journal = {ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
title = {{PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation}},
url = {https://pytorch.org/assets/pytorch2-2.pdf},
year = {2024}
}

@software{lczero_tools,
    url = {https://github.com/so-much-meta/lczero_tools},
    author = {Trevor Graffa},
    license = {GPL-3.0},
    title = {lczero\textunderscore{}tools}
}

@article{matplotlib,
author = {Hunter, John D.},
journal = {Computing in Science \& Engineering},
title = {{Matplotlib}: A {2D} graphics environment},
year = {2007}
}

@article{
lee2023supervised,
title={Supervised Pretraining Can Learn In-Context Reinforcement Learning},
author={Jonathan Lee and Annie Xie and Aldo Pacchiano and Yash Chandak and Chelsea Finn and Ofir Nachum and Emma Brunskill},
journal={NeurIPS},
year={2023},
}

@misc{duan2016rl2,
      title={{RL$^2$:} Fast Reinforcement Learning via Slow Reinforcement Learning}, 
      author={Yan Duan and John Schulman and Xi Chen and Peter L. Bartlett and Ilya Sutskever and Pieter Abbeel},
      year={2016},
      eprint={1611.02779},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{
laskin2023incontext,
title={In-context Reinforcement Learning with Algorithm Distillation},
author={Michael Laskin and Luyu Wang and Junhyuk Oh and Emilio Parisotto and Stephen Spencer and Richie Steigerwald and DJ Strouse and Steven Stenberg Hansen and Angelos Filos and Ethan Brooks and maxime gazeau and Himanshu Sahni and Satinder Singh and Volodymyr Mnih},
journal={ICLR},
year={2023},
}

@misc{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  eprint={1611.05763},
  year={2016}
}

@article{hochreiter2001,
author="Hochreiter, Sepp
and Younger, A. Steven
and Conwell, Peter R.",
title="Learning to Learn Using Gradient Descent",
journal="ICANN",
year="2001",
}

