@article{pvtv2,
  title={Pvtv2: Improved baselines with pyramid vision transformer},
  author={Wang, Wenhai and Xie, Enze and Li, Xiang and Fan, Deng-Ping and Song, Kaitao and Liang, Ding and Lu, Tong and Luo, Ping and Shao, Ling},
  journal={Computational Visual Media},
  volume={8},
  number={3},
  pages={1--10},
  year={2022},
  publisher={Springer}
}

@inproceedings{attention,
title={Attention is all you need},
author={Ashish Vaswani and Noam Shazeer and Niki Parmar and others},
booktitle={NeurIPS},
year={2017}
}

@inproceedings{EdgeNeXt,
      title={EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications},
      author={Muhammad Maaz and Abdelrahman Shaker and Hisham Cholakkal and Salman Khan and Syed Waqas Zamir and Rao Muhammad Anwer and Fahad Shahbaz Khan},
      booktitle={CADL},
      year={2022},
}



@article{conv2former,
  title={Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition},
  author={Hou, Qibin and Lu, Cheng-Ze and Cheng, Ming-Ming and Feng, Jiashi},
  journal={arXiv preprint arXiv:2211.11943},
  year={2022}
}

@article{Moganet,
title={Efficient Multi-order Gated Aggregation Network},
author={Siyuan Li and Zedong Wang and Zicheng Liu and others},
journal={arXiv preprint arXiv:2211.03295},
year={2022}

}

@article{hornet,
  title={HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions},
  author={Rao, Yongming and Zhao, Wenliang and Tang, Yansong and Zhou, Jie and Lim, Ser-Lam and Lu, Jiwen},
  journal={NeurIPS},
  year={2022}
}

@article{focalnet,
      title={Focal Modulation Networks}, 
      author={Jianwei Yang and Chunyuan Li and Xiyang Dai and Jianfeng Gao},
      journal={NeurIPS},
      year={2022}
}

@inproceedings{mobilevit,
     title={MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer},
     author={Sachin Mehta and Mohammad Rastegari},
     booktitle={ICLR},
     year={2022}
}

@inproceedings{edgevit,
  title={EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers},
  author={Pan, Junting and Bulat, Adrian and Tan, Fuwen and others},
  booktitle={ECCV},
  year={2022}
}
@inproceedings{uniformer,
title={ Uniformer: Unified transformer for efficient spatial-temporal representation learning},
author={Li, Kunchang and Wang, Yali and Gao, Peng and others},
booktitle={ICLR},
year={2022}
}

@inproceedings{LVT,
title={Lite Vision Transformer with Enhanced Self-Attention},
author={Yang, Chenglin and Wang, Yilin and Zhang, Jianming and others},
booktitle={CVPR},
year={2022}
}

@inproceedings{ortho,
title={Orthogonal Transformer: An Efficient Vision Transformer Backbone with Token Orthogonalization},
author={Huang, Huaibo and Zhou, Xiaoqiang and He, Ran},
booktitle={NeurIPS},
year={2022}
}

@inproceedings{quadtree,
title={ Quadtree attention for vision transformers},
author={Tang, Shitao and Zhang, Jiahui and Zhu, Siyu and others},
booktitle={ICLR},
year={2022}
}

@inproceedings{t2t,
title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
author={Yuan, Li and Chen, Yunpeng and Wang, Tao and others},
booktitle={ICCV},
year={2021}
}

@inproceedings{parcnet,
  title={ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer},
  author={Zhang, Haokui and Hu, Wenze and Wang, Xiaoyu},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{tnt,
title={Transformer in Transformer},
author={Han, Kai and Xiao, An and Wu, Enhua and others},
booktitle={NeurIPS},
year={2021}
}

@inproceedings{SASA,
title={Stand-Alone Self-Attention in Vision Models},
author={Prajit Ramachandran and Niki Parmar and Ashish Vaswani and Irwan Bello and Anselm Levskaya and Jonathon Shlens},
booktitle={NeurIPS},
year={2019}
}

@inproceedings{regionvit,
    title={{RegionViT: Regional-to-Local Attention for Vision Transformers}},
    author={Chun-Fu (Richard) Chen and Rameswar Panda and Quanfu Fan},
    booktitle={ICLR},
    year={2022}
}

@inproceedings{wavevit,
    title     = {Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning},
    author    = {Yao, Ting and Pan, Yingwei and Li, Yehao and Ngo, Chong-Wah and Mei, Tao},
    booktitle = {Proceedings of the European conference on computer vision (ECCV)},
    year      = {2022},
}

@article{ScalableViT,
  title={ScalableViT: Rethinking the context-oriented generalization of vision transformer},
  author={Yang, Rui and Ma, Hailong and Wu, Jie and Tang, Yansong and Xiao, Xuefeng and Zheng, Min and Li, Xiu},
  journal={ECCV},
  year={2022}
}

@article{cmt,
  title={CMT: Convolutional neural networks meet vision transformers},
  author={Guo, Jianyuan and Han, Kai and Wu, Han and Xu, Chang and Tang, Yehui and Xu, Chunjing and Wang, Yunhe},
  journal={CVPR},
  year={2022}
}

@inproceedings{deit,
title={Training data-efficient image transformers \& distillation through attention},
author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and others},
booktitle={ICML},
year={2021}
}

@inproceedings{ceit,
title={Incorporating convolution designs into visual transformers},
author={Yuan, Kun and Guo, Shaopeng and Liu, Ziwei and others},
booktitle={ICCV},
year={2021}
}

@inproceedings{rest,
  title={ResT: An Efficient Transformer for Visual Recognition},
  author={Qinglong Zhang and Yu-bin Yang},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{xcit,
  title={XCiT: Cross-Covariance Image Transformers},
  author={El-Nouby, Alaaeldin and Touvron, Hugo and Caron, Mathilde and others},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{pervit,
title={Peripheral Vision Transformer},
author={Min, Juhong and Zhao, Yucheng and Luo, Chong and others},
booktitle={NeurIPS},
year={2022}
}

@inproceedings{dfvit,
author={Gao, Li and Nie, Dong and Li, Bo and Ren, Xiaofeng},
title={Doubly-Fused ViT: Fuse Information from Vision Transformer Doubly with Local Representation},
booktitle={ECCV},
year={2022}
}

@inproceedings{mobileformer,
title={Mobile-Former: Bridging MobileNet and Transformer},
author={Chen, Yinpeng and Dai, Xiyang and Chen, Dongdong and others},
booktitle={CVPR},
year={2022}
}

@inproceedings{sit,
title={Self-slimmed Vision Transformer},
author={Zong, Zhuofan and Li, Kunchang and Song, Guanglu and others},
booktitle={ECCV},
year={2022}
}

@inproceedings{vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Dosovitskiy, Alexey and Beyer,  Lucas and  Kolesnikov, Alexander and others},
booktitle={ICLR},
year={2021}
}

@inproceedings{SwinTransformer,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}

@article{pvt,
      title={Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions}, 
      author={Wenhai Wang and Enze Xie and Xiang Li and Deng-Ping Fan and Kaitao Song and Ding Liang and Tong Lu and Ping Luo and Ling Shao},
      journal={arXiv preprint arXiv:2103.15808},
      year={2021}
}

@article{cvt,
  title={Cvt: Introducing convolutions to vision transformers},
  author={Wu, Haiping and Xiao, Bin and Codella, Noel and Liu, Mengchen and Dai, Xiyang and Yuan, Lu and Zhang, Lei},
  journal={arXiv preprint arXiv:2103.15808},
  year={2021}
}

@inproceedings{dat,
    author    = {Xia, Zhuofan and Pan, Xuran and Song, Shiji and Li, Li Erran and Huang, Gao},
    title     = {Vision Transformer With Deformable Attention},
    booktitle = {CVPR},
    year      = {2022}
}

@inproceedings{maxvit,
title={MaxViT: Multi-Axis Vision Transformer},
author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and others},
booktitle={ECCV},
year={2022}
}

@inproceedings{imagenet,
title={Imagenet: A large-scale hierarchical image database},
author={Deng, Jia and Dong, Wei and Socher, Richard and others},
booktitle={CVPR},
year={2009}
}

@inproceedings{coco,
title={Microsoft coco: Common objects in context},
author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and others},
booktitle={ECCV},
year={2014}
}

@inproceedings{ade20k,
title={Scene parsing through ade20k dataset},
author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and others},
booktitle={CVPR},
year={2017}
}

@inproceedings{mobilenet,
title={Searching for MobileNetV3},
author={Howard, Andrew and Sandler, Mark and Chu, Grace and others},
booktitle={ICCV},
year={2019}
}

@article{cait,
  author    = {Hugo Touvron and
               Matthieu Cord and
               Alexandre Sablayrolles and
               Gabriel Synnaeve and
               Herv{\'{e}} J{\'{e}}gou},
  title     = {Going deeper with Image Transformers},
  journal   = {CoRR},
  volume    = {abs/2103.17239},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.17239},
  eprinttype = {arXiv},
  eprint    = {2103.17239},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-17239.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{localvit,
  title={LocalViT: Bringing Locality to Vision Transformers},
  author={Li, Yawei and Zhang, Kai and Cao, Jiezhang and Timofte, Radu and Van Gool, Luc},
  journal={arXiv preprint arXiv:2104.05707},
  year={2021}
}

@inproceedings{hivit,
  title={HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer},
  author={Zhang, Xiaosong and Tian, Yunjie and Xie, Lingxi and Huang, Wei and Dai, Qi and Ye, Qixiang and Tian, Qi},
  booktitle={ICLR},
  year={2023},
}



@inproceedings{crossformer,
  title = {CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention},
  author = {Wang, Wenxiao and Yao, Lu and Chen, Long and Lin, Binbin and Cai, Deng and He, Xiaofei and Liu, Wei},
  booktitle = {ICLR},
  year = {2022}
}

@inproceedings{davit,
title={DaViT: Dual Attention Vision Transformers},
author={Ding, Mingyu and Xiao, Bin and Codella, Noel and others},
booktitle={ECCV},
year={2022}
}

@inproceedings{cswin,
title={CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows},
author={Dong, Xiaoyi and Bao, Jianmin and Chen, Dongdong and others},
booktitle={CVPR},
year={2022}
}

@inproceedings{qna,
title={Learned Queries for Efficient Local Attention},
author={Arar, Moab and Shamir, Ariel and Amit H. Bermano and others},
booktitle={CVPR},
year={2022}
}

@inproceedings{replknet,
    title={Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs},
    author={Ding, Xiaohan and Zhang, Xiangyu and Zhou, Yizhuang and Han, Jungong and Ding, Guiguang and Sun, Jian},
    journal={CVPR},
    year={2022}
    }

@inproceedings{shunted,
title={Shunted Self-Attention via Multi-Scale Token Aggregation },
author={Sucheng Ren and Daquan Zhou and Shengfeng He and Jiashi Feng and Xinchao Wang},
booktitle={CVPR},
year={2022}
}

@misc{focal,
    title={Focal Self-Attention for Local-Global Interactions in Vision Transformers}, 
    author={Jianwei Yang and Chunyuan Li and Pengchuan Zhang and Xiyang Dai and Bin Xiao and Lu Yuan and Jianfeng Gao},
    year={2021},
    eprint={2107.00641},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{litv2,
  title={Fast Vision Transformers with HiLo Attention},
  author={Pan, Zizheng and Cai, Jianfei and Zhuang, Bohan},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{litv1,
  title={Less is More: Pay Less Attention in Vision Transformers},
  author={Pan, Zizheng and Zhuang, Bohan and He, Haoyu and Liu, Jing and Cai, Jianfei},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{twins,
	title={Twins: Revisiting the Design of Spatial Attention in Vision Transformers},
	author={Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen},
	booktitle={NeurIPS},
	year={2021}
}

@inproceedings{dynamicvit,
  title={DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification},
  author={Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{evovit,
  title={Evo-vit: Slow-fast token evolution for dynamic vision transformer},
  author={Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing},
  booktitle={AAAI},
  year={2022}
}

@inproceedings{unkown,
title={IA-RED$^2$: Interpretability-Aware Redundancy Reduction for Vision Transformers},
author={Bowen Pan and Rameswar Panda and Yifan Jiang and others},
booktitle={NeurIPS},
year={2021}
}

@inproceedings{resnet,
title={Deep Residual Learning for Image Recognition},
author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Jian, Sun},
booktitle={CVPR},
year={2016}
}

@inproceedings{vgg,
title={Very Deep Convolutional Networks for Large-scale Image Recognition},
author={Simonyan, Karen and Zisserman, Andrew},
booktitle={ICLR},
year={2015}
}

@inproceedings{huang2017densely,
  title={Densely Connected Convolutional Networks},
  author={Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q },
  booktitle={CVPR},
  year={2017}
}

@article{yuan2022volo,
  title={Volo: Vision outlooker for visual recognition},
  author={Yuan, Li and Hou, Qibin and Jiang, Zihang and Feng, Jiashi and Yan, Shuicheng},
  journal={TPAMI},
  year={2022},
  publisher={IEEE}
}

@article{lightvit,
  title = {LightViT: Towards Light-Weight Convolution-Free Vision Transformers},
  author = {Huang, Tao and Huang, Lang and You, Shan and Wang, Fei and Qian, Chen and Xu, Chang},
  journal = {arXiv preprint arXiv:2207.05557},
  year = {2022}
}

@inproceedings{levit,
title={LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference},
author={Graham, Benjamin and El-Nouby, Alaaeldin and Touvron, Hugo and Stock, Pierre},
booktitle={ICCV},
year={2021}
}

@inproceedings{googlenet,
title={Going deeper with convolutions},
author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and others},
booktitle={CVPR},
year={2015}
}

@inproceedings{nin,
title={Network In Network},
author={Lin M and Chen Q and Yan S},
booktitle={ICLR},
year={2014}
}

@inproceedings{involution,
    author = {Li, Duo and Hu, Jie and Wang, Changhu and Li, Xiangtai and She, Qi and Zhu, Lei and Zhang, Tong and Chen, Qifeng},
    title = {Involution: Inverting the Inherence of Convolution for Visual Recognition},
    booktitle = {CVPR},
    year = {2021}
}

@inproceedings{mixformer,
author={Chen, Qiang and Wu, Qiman and Wang, Jian and others},
title={MixFormer: Mixing Features across Windows and Dimensions},
booktitle={CVPR},
year={2022}
}

@inproceedings{conformer,
title={Conformer: Local Features Coupling Global Representations for Visual Recognition},
author={Peng, Zhiliang and Huang, Wei and Gu, Shanzhi and others},
booktitle={ICCV},
year={2021}
}

@inproceedings{CPVT,
	title={Conditional Positional Encodings for Vision Transformers},
	author={Xiangxiang Chu and Zhi Tian and Bo Zhang and Xinlong Wang and Chunhua Shen},
	booktitle={ICLR},
	year={2023}
}

@inproceedings{ViL,
  title={Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding},
  author={Zhang, Pengchuan and Dai, Xiyang and Yang, Jianwei and Xiao, Bin and Yuan, Lu and Zhang, Lei and Gao, Jianfeng},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{dynamicconv,
title={Dynamic Convolution: Attention over Convolution Kernels},
author={Chen, Yinpeng and Dai, Xiyang and Liu, Mengchen and others},
booktitle={CVPR},
year={2020}
}

@inproceedings{coat,
    author    = {Xu, Weijian and Xu, Yifan and Chang, Tyler and Tu, Zhuowen},
    title     = {Co-Scale Conv-Attentional Image Transformers},
    booktitle = {ICCV},
    year      = {2021}
}

@inproceedings{mpvit,
      title={MPViT: Multi-Path Vision Transformer for Dense Prediction}, 
      author={Youngwan Lee and Jonghee Kim and Jeffrey Willette and Sung Ju Hwang},
      booktitle={CVPR},
      year={2022}
}

@inproceedings{poolformer,
  title={Metaformer is actually what you need for vision},
  author={Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{early,
title={Early convolutions help transformers see better},
author={Tete Xiao and Mannat Singh and Eric Mintun and others},
booktitle={NeurIPS},
year={2021}
}

@inproceedings{adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={ICLR},
year={2019}
}

@inproceedings{fpn,
title={Feature Pyramid Networks for Object Detection},
author={Lin, Tsung{-}Yi and Doll{\'{a}}r, Piotr and Girshick, Ross},
booktitle={CVPR},
year={2017}
}

@inproceedings{retinanet,
title={ Focal loss for dense object detection},
author={Tsung{-}Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He andPiotr Doll{\'{a}}r},
booktitle={ICCV},
year={2017}
}

@inproceedings{maskrcnn,
title={Mask R-CNN},
author={Kaiming He and Georgia Gkioxari and Piotr Doll{\'{a}}r and Ross B. Girshick},
booktitle={ICCV},
year={2017}
}

@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and others},
  journal= {arXiv preprint arXiv:1906.07155},
  year={2019}
}

@misc{mmsegmentation,
  title={Mmsegmentation, an open source semantic segmentation toolbox},
  author={Contributors, MMSegmentation},
  year={2020}
}

@inproceedings{semanticfpn,
title={Panoptic feature pyramid networks},
author={Alexander Kirillov and Ross Girshick and Kaiming He and Piotr Doll{\'{a}}r},
booktitle={CVPR},
year={2019}
}

@inproceedings{inceptionformer,
title={Inception Transformer},
author={Chenyang Si and Weihao Yu and Pan Zhou and Yichen Zhou and Xinchao Wang and Shuicheng YAN},
booktitle={NeurIPS},
year={2022}
}

@inproceedings{convnext,
title={A convnet for the 2020s},
author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and others},
booktitle={CVPR},
year={2022}
}

@inproceedings{pit,
title={Rethinking spatial dimensions of vision transformers},
author={Byeongho Heo and Sangdoo Yun and Dongyoon Han and others},
booktitle={ICCV},
year={2021}
}

@inproceedings{efficientformer,
title={EfficientFormer: Vision Transformers at MobileNet Speed},
author={Li, Yanyu and Yuan, Geng and Wen, Yang and others},
booktitle={NeurIPS},
year={2022}
}

@inproceedings{randomaugment,
title={Randaugment: Practical automated data augmentation with a reduced search space},
author={Ekin D Cubuk and Barret Zoph and Jonathon Shlens and others},
booktitle={CVPRW},
year={2020}
}

@inproceedings{mixup,
title={ mixup: Beyond empirical risk minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N Dauphin and others},
booktitle={ICLR},
year={2018}
}

@inproceedings{cutmix,
title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
author={Sangdoo Yun and Dongyoon Han and Seong Joon Oh and others},
booktitle={ICCV},
year={2019}
}

@inproceedings{randera,
title={Random erasing data augmentation},
author={Zhun Zhong and Liang Zheng and Guoliang Kang and others},
booktitle={AAAI},
year={2020}
}

@inproceedings{droppath,
title={Deep networks with stochastic depth},
author={Gao Huang and Yu Sun and Zhuang Liu},
booktitle={ECCV},
year={2016}
}

@inproceedings{mobilenetv2,
title={ Mobilenetv2: Inverted residuals and linear bottlenecks},
author={Sandler, M. and Howard, A. and Zhu, M. and others},
booktitle={CVPR},
year={2018}
}

@inproceedings{shufflenetv2,
title={Shufflenet v2: Practical guidelines for efficient cnn architecture design},
author={Ningning Ma and Xiangyu Zhang and Hai-Tao Zheng and others},
booktitle={ECCV},
year={2018}
}

@InProceedings{regnety,
  title = {Designing Network Design Spaces},
  author = {Ilija Radosavovic and Raj Prateek Kosaraju and Ross Girshick and Kaiming He and Piotr Doll{\'a}r},
  booktitle = {CVPR},
  year = {2020}
}

@article{VAN,
  title={Visual Attention Network},
  author={Guo, Meng-Hao and Lu, Cheng-Ze and Liu, Zheng-Ning and Cheng, Ming-Ming and Hu, Shi-Min},
  journal={arXiv preprint arXiv:2202.09741},
  year={2022}
}

@inproceedings{FAN,
title={Understanding the robustness in vision transformers},
author={Daquan Zhou and Zhiding Yu and Enze Xie and others},
booktitle={ICML},
year={2022}
}

@inproceedings{efficientnet,
title={Efficientnet: Rethinking model scaling for convolutional neural networks},
author={Mingxing Tan and Quoc Le},
booktitle={ICML},
year={2019}
}

@inproceedings{wave-mlp,
  title={An Image Patch is a Wave: Phase-Aware Vision MLP},
  author={Tang, Yehui and Han, Kai and Guo, Jianyuan and Xu, Chang and Li, Yanxi and Xu, Chao and Wang, Yunhe},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{upernet,
  title={Unified Perceptual Parsing for Scene Understanding},
  author={Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
  booktitle={ECCV},
  year={2018},
}

@article{shuffletrans,
 title={Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer},
 author={Huang, Zilong and Ben, Youcheng and Luo, Guozhong and Cheng, Pei and Yu, Gang and Fu, Bin},
 journal={arXiv preprint arXiv:2106.03650},
 year={2021}
}
