\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{OSDI}, 2016.

\bibitem[Agrawal et~al.(2020)Agrawal, Sheldon, and
  Domke]{Agrawal2020_advances_blackbox_vi}
Agrawal, A., Sheldon, D.~R., and Domke, J.
\newblock Advances in black-box {VI:} normalizing flows, importance weighting,
  and optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{Blei2017_vi_review}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 2017.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, and Wanderman-Milne]{jax2018}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., and Wanderman-Milne, S.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem[Burda et~al.(2016)Burda, Grosse, and Salakhutdinov]{Burda2015_iwae}
Burda, Y., Grosse, R.~B., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock In \emph{Proceedings of the 4th International Conference on Learning
  Representations}, 2016.

\bibitem[Dillon et~al.(2017)Dillon, Langmore, Tran, Brevdo, Vasudevan, Moore,
  Patton, Alemi, Hoffman, and Saurous]{Dillon2017_tfp_distributions}
Dillon, J.~V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,
  Patton, B., Alemi, A., Hoffman, M.~D., and Saurous, R.~A.
\newblock Tensorflow distributions.
\newblock \emph{arXiv preprint arXiv:1711.10604}, 2017.

\bibitem[Geffner \& Domke(2020)Geffner and Domke]{geffner2020approximation}
Geffner, T. and Domke, J.
\newblock Approximation based variance reduction for reparameterization
  gradients.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del Rio, Wiebe, Peterson, G{\'e}rard-Marchant, Sheppard,
  Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{Harris2020_numpy}
Harris, C.~R., Millman, K.~J., van~der Walt, S.~J., Gommers, R., Virtanen, P.,
  Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.~J., Kern, R.,
  Picus, M., Hoyer, S., van Kerkwijk, M.~H., Brett, M., Haldane, A., del Rio,
  J.~F., Wiebe, M., Peterson, P., G{\'e}rard-Marchant, P., Sheppard, K., Reddy,
  T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T.~E.
\newblock Array programming with numpy.
\newblock \emph{Nature}, Sep 2020.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and Babuschkin]{haiku2020}
Hennigan, T., Cai, T., Norman, T., and Babuschkin, I.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{Jordon1999_variational_methods}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K.
\newblock \emph{An Introduction to Variational Methods for Graphical Models}.
\newblock MIT Press, Cambridge, MA, USA, 1999.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma_adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proceedings of the 3rd International Conference on Learning
  Representations}, 2015.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{Kingma_vae}
Kingma, D. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In \emph{Proceedings of the 2nd International Conference on Learning
  Representations}, 2014.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and
  Tenenbaum]{Lake_Omniglot_2015}
Lake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B.
\newblock {Human-level concept learning through probabilistic program
  induction}.
\newblock \emph{Science}, 2015.

\bibitem[LeCun \& Cortes(2010)LeCun and Cortes]{lecun2010mnist}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Miller et~al.(2017)Miller, Foti, D'Amour, and
  Adams]{miller2017reducing}
Miller, A., Foti, N., D'Amour, A., and Adams, R.~P.
\newblock Reducing reparameterization gradient variance.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and
  Mnih]{Mohamed2020_gradient_estimation_ml}
Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A.
\newblock Monte carlo gradient estimation in machine learning.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\bibitem[Rainforth et~al.(2018)Rainforth, Kosiorek, Le, Maddison, Igl, Wood,
  and Teh]{Rainforth2018_bound}
Rainforth, T., Kosiorek, A., Le, T.~A., Maddison, C., Igl, M., Wood, F., and
  Teh, Y.~W.
\newblock Tighter variational bounds are not necessarily better.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{Rezende2015_flows}
Rezende, D.~J. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and Wierstra]{Rezende2014_vae}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, volume~32, 2014.

\bibitem[Roeder et~al.(2017)Roeder, Wu, and Duvenaud]{Roeder2017_stl}
Roeder, G., Wu, Y., and Duvenaud, D.
\newblock Sticking the landing: Simple, lower-variance gradient estimators for
  variational inference.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, 2017.

\bibitem[Ruiz et~al.(2016)Ruiz, Titsias, and Blei]{Ruiz2016}
Ruiz, F. J.~R., Titsias, M.~K., and Blei, D.~M.
\newblock Overdispersed black-box variational inference.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2016.

\bibitem[Tucker et~al.(2019)Tucker, Lawson, Gu, and Maddison]{Tucker2019_dregs}
Tucker, G., Lawson, D., Gu, S., and Maddison, C.~J.
\newblock Doubly reparameterized gradient estimators for monte carlo
  objectives.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations}, 2019.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 1992.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{FashionMNIST}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\end{thebibliography}
