\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abel et~al.(2021)Abel, Dabney, Harutyunyan, Ho, Littman, Precup, and
  Singh]{abel2021expressivity}
Abel, D., Dabney, W., Harutyunyan, A., Ho, M.~K., Littman, M., Precup, D., and
  Singh, S.
\newblock On the expressivity of markov reward.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Antos et~al.(2007)Antos, Munos, and Szepesv{\'a}ri]{antos2007fitted}
Antos, A., Munos, R., and Szepesv{\'a}ri, C.
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock 2007.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Antos, A., Szepesv{\'a}ri, C., and Munos, R.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Dabney et~al.(2020)Dabney, Ostrovski, and
  Barreto]{dabney2020temporally}
Dabney, W., Ostrovski, G., and Barreto, A.
\newblock Temporally-extended epsilon-greedy exploration.
\newblock \emph{arXiv preprint arXiv:2006.01782}, 2020.

\bibitem[Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{dann2018oracle}
Dann, C., Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and
  Schapire, R.~E.
\newblock On oracle-efficient pac rl with rich observations.
\newblock \emph{arXiv preprint arXiv:1803.00606}, 2018.

\bibitem[Dann et~al.(2021{\natexlab{a}})Dann, Marinov, Mohri, and
  Zimmert]{dann2021beyond}
Dann, C., Marinov, T.~V., Mohri, M., and Zimmert, J.
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds
  for episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Dann et~al.(2021{\natexlab{b}})Dann, Mohri, Zhang, and
  Zimmert]{dann2021provably}
Dann, C., Mohri, M., Zhang, T., and Zimmert, J.
\newblock A provably efficient model-free posterior sampling method for
  episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Devidze et~al.(2021)Devidze, Radanovic, Kamalaruban, and
  Singla]{devidze2021explicable}
Devidze, R., Radanovic, G., Kamalaruban, P., and Singla, A.
\newblock Explicable reward design for reinforcement learning agents.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Du, S.~S., Kakade, S.~M., Lee, J.~D., Lovett, S., Mahajan, G., Sun, W., and
  Wang, R.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock \emph{arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 503--556,
  2005.

\bibitem[Even-Dar \& Mansour(2003)Even-Dar and Mansour]{even2003learning}
Even-Dar, E. and Mansour, Y.
\newblock Learning rates for {Q}-learning.
\newblock \emph{Journal of machine learning Research}, 5\penalty0 (1), 2003.

\bibitem[Foster et~al.(2020)Foster, Rakhlin, Simchi-Levi, and
  Xu]{foster2020instance}
Foster, D.~J., Rakhlin, A., Simchi-Levi, D., and Xu, Y.
\newblock Instance-dependent complexity of contextual bandits and reinforcement
  learning: A disagreement-based perspective.
\newblock \emph{arXiv preprint arXiv:2010.03104}, 2020.

\bibitem[Grzes(2017)]{grzes2017reward}
Grzes, M.
\newblock Reward shaping in episodic reinforcement learning.
\newblock 2017.

\bibitem[Howard et~al.(2021)Howard, Ramdas, McAuliffe, and
  Sekhon]{howard2021time}
Howard, S.~R., Ramdas, A., McAuliffe, J., and Sekhon, J.
\newblock Time-uniform, nonparametric, nonasymptotic confidence sequences.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (2):\penalty0
  1055--1080, 2021.

\bibitem[Hu et~al.(2020)Hu, Wang, Jia, Wang, Chen, Hao, Wu, and
  Fan]{hu2020learning}
Hu, Y., Wang, W., Jia, H., Wang, Y., Chen, Y., Hao, J., Wu, F., and Fan, C.
\newblock Learning to utilize shaping rewards: A new approach of reward
  shaping.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15931--15941, 2020.

\bibitem[Icarte et~al.(2022)Icarte, Klassen, Valenzano, and
  McIlraith]{icarte2022reward}
Icarte, R.~T., Klassen, T.~Q., Valenzano, R., and McIlraith, S.~A.
\newblock Reward machines: Exploiting reward function structure in
  reinforcement learning.
\newblock \emph{Journal of Artificial Intelligence Research}, 73:\penalty0
  173--208, 2022.

\bibitem[Jiang et~al.(2016)Jiang, Singh, and Tewari]{jiang2016structural}
Jiang, N., Singh, S.~P., and Tewari, A.
\newblock On structural properties of mdps that bound loss due to shallow
  planning.
\newblock In \emph{IJCAI}, pp.\  1640--1647, 2016.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1704--1713. JMLR. org, 2017.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143, 2020.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S.
\newblock Bellman {E}luder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and
  Szepesv{\'a}ri]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Laud \& DeJong(2003)Laud and DeJong]{laud2003influence}
Laud, A. and DeJong, G.
\newblock The influence of reward on the speed of reinforcement learning: An
  analysis of shaping.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pp.\  440--447, 2003.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu \& Brunskill(2018)Liu and Brunskill]{liu2018simple}
Liu, Y. and Brunskill, E.
\newblock When simple exploration is sample efficient: Identifying sufficient
  conditions for random exploration to yield pac rl algorithms.
\newblock \emph{arXiv preprint arXiv:1805.09045}, 2018.

\bibitem[Mania et~al.(2019)Mania, Tu, and Recht]{mania2019certainty}
Mania, H., Tu, S., and Recht, B.
\newblock Certainty equivalence is efficient for linear quadratic control.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, pp.\  10154--10164, 2019.

\bibitem[Marthi(2007)]{marthi2007automatic}
Marthi, B.
\newblock Automatic shaping and decomposition of reward functions.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  learning}, pp.\  601--608, 2007.

\bibitem[Mataric(1994)]{mataric1994reward}
Mataric, M.~J.
\newblock Reward functions for accelerated learning.
\newblock In \emph{Machine learning proceedings 1994}, pp.\  181--189.
  Elsevier, 1994.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Icml}, volume~99, pp.\  278--287, 1999.

\bibitem[Osband et~al.(2019)Osband, Van~Roy, Russo, Wen,
  et~al.]{osband2019deep}
Osband, I., Van~Roy, B., Russo, D.~J., Wen, Z., et~al.
\newblock Deep exploration via randomized value functions.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (124):\penalty0 1--62, 2019.

\bibitem[Rakhlin \& Sridharan(2015)Rakhlin and Sridharan]{rakhlin2015online}
Rakhlin, A. and Sridharan, K.
\newblock Online nonparametric regression with general loss functions.
\newblock \emph{arXiv preprint arXiv:1501.06598}, 2015.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Simchowitz \& Foster(2020)Simchowitz and Foster]{simchowitz2020naive}
Simchowitz, M. and Foster, D.
\newblock Naive exploration is optimal for online lqr.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8937--8948. PMLR, 2020.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and
  Jamieson]{simchowitz2019non}
Simchowitz, M. and Jamieson, K.~G.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 1153--1162, 2019.

\bibitem[Sun et~al.(2018)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2018model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock \emph{arXiv preprint arXiv:1811.08540}, 2018.

\bibitem[Uehara \& Sun(2021)Uehara and Sun]{uehara2021pessimistic}
Uehara, M. and Sun, W.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \emph{arXiv preprint arXiv:2107.06226}, 2021.

\bibitem[Wagenmaker et~al.(2021)Wagenmaker, Simchowitz, and
  Jamieson]{wagenmaker2021beyond}
Wagenmaker, A., Simchowitz, M., and Jamieson, K.
\newblock Beyond no regret: Instance-dependent pac reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2108.02717}, 2021.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020reinforcement}
Wang, R., Salakhutdinov, R.~R., and Yang, L.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded {E}luder dimension.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zhang(2021)]{zhang2021feel}
Zhang, T.
\newblock Feel-good thompson sampling for contextual bandits and reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2110.00871}, 2021.

\end{thebibliography}
