\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bellemare et~al.(2016)Bellemare, Ostrovski, Guez, Thomas, and
  Munos]{bellemare2016increasing}
Marc~G Bellemare, Georg Ostrovski, Arthur Guez, Philip~S Thomas, and R{\'e}mi
  Munos.
\newblock Increasing the action gap: New operators for reinforcement learning.
\newblock In \emph{Thirtieth AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Bohmer et~al.(2019{\natexlab{a}})Bohmer, Kurin, and
  Whiteson]{bhmer2019deep}
Wendelin Bohmer, Vitaly Kurin, and Shimon Whiteson.
\newblock Deep coordination graphs.
\newblock \emph{arXiv preprint arXiv:1910.00091}, 2019{\natexlab{a}}.

\bibitem[Bohmer et~al.(2019{\natexlab{b}})Bohmer, Rashid, and
  Whiteson]{bohmer2019exploration}
Wendelin Bohmer, Tabish Rashid, and Shimon Whiteson.
\newblock Exploration with unreliable intrinsic reward in multi-agent
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1906.02138}, 2019{\natexlab{b}}.

\bibitem[Claus and Boutilier(1998)]{claus1998dynamics}
Caroline Claus and Craig Boutilier.
\newblock The dynamics of reinforcement learning in cooperative multiagent
  systems.
\newblock \emph{AAAI/IAAI}, 1998\penalty0 (746-752):\penalty0 2, 1998.

\bibitem[Du et~al.(2019)Du, Han, Fang, Liu, Dai, and Tao]{du_liir:_2019}
Yali Du, Lei Han, Meng Fang, Ji~Liu, Tianhong Dai, and Dacheng Tao.
\newblock {LIIR}: Learning individual intrinsic reward in multi-agent
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pages
  4405--4416. 2019.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 503--556, 2005.

\bibitem[Foerster et~al.(2016)Foerster, Assael, De~Freitas, and
  Whiteson]{foerster2016learning}
Jakob Foerster, Ioannis~Alexandros Assael, Nando De~Freitas, and Shimon
  Whiteson.
\newblock Learning to communicate with deep multi-agent reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  2137--2145, 2016.

\bibitem[Foerster et~al.(2017)Foerster, Nardelli, Farquhar, Afouras, Torr,
  Kohli, and Whiteson]{foerster2017stabilising}
Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras,
  Philip~HS Torr, Pushmeet Kohli, and Shimon Whiteson.
\newblock Stabilising experience replay for deep multi-agent reinforcement
  learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1146--1155. JMLR. org, 2017.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and
  Whiteson]{foerster_counterfactual_2017}
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and
  Shimon Whiteson.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Proceedings of the Thirty-Second AAAI Conference on
  Artificial Intelligence}, 2018.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{ha2017hyper}
David Ha, Andrew Dai, and Quoc~V. Le.
\newblock {HyperNetworks}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1856--1865, 2018.

\bibitem[Iqbal and Sha(2019)]{iqbal2019actor}
Shariq Iqbal and Fei Sha.
\newblock Actor-attention-critic for multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2961--2970, 2019.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR}}, 2017.

\bibitem[Kraemer and Banerjee(2016)]{kraemer2016multi}
Landon Kraemer and Bikramjit Banerjee.
\newblock Multi-agent reinforcement learning as a rehearsal for decentralized
  planning.
\newblock \emph{Neurocomputing}, 190:\penalty0 82--94, 2016.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Ryan Lowe, Yi~Wu, Aviv Tamar, Jean Harb, OpenAI~Pieter Abbeel, and Igor
  Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6379--6390, 2017.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and
  Whiteson]{mahajan2019maven}
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson.
\newblock Maven: Multi-agent variational exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7611--7622, 2019.

\bibitem[Matignon et~al.(2007)Matignon, Laurent, and
  Le~Fort-Piat]{matignon2007hysteretic}
La{\"e}titia Matignon, Guillaume~J Laurent, and Nadine Le~Fort-Piat.
\newblock Hysteretic q-learning: an algorithm for decentralized reinforcement
  learning in cooperative multi-agent teams.
\newblock In \emph{2007 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 64--69. IEEE, 2007.

\bibitem[Melo(2001)]{melo2001convergence}
Francisco~S Melo.
\newblock Convergence of q-learning: A simple proof.
\newblock \emph{Institute Of Systems and Robotics, Tech. Rep}, pages 1--4,
  2001.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Oliehoek et~al.(2008)Oliehoek, Spaan, and
  Vlassis]{oliehoek2008optimal}
Frans~A Oliehoek, Matthijs~TJ Spaan, and Nikos Vlassis.
\newblock Optimal and approximate q-value functions for decentralized pomdps.
\newblock \emph{Journal of Artificial Intelligence Research}, 32:\penalty0
  289--353, 2008.

\bibitem[Oliehoek et~al.(2016)Oliehoek, Amato, et~al.]{oliehoek2016concise}
Frans~A Oliehoek, Christopher Amato, et~al.
\newblock \emph{A concise introduction to decentralized POMDPs}, volume~1.
\newblock Springer, 2016.

\bibitem[OroojlooyJadid and Hajinezhad(2019)]{oroojlooyjadid2019review}
Afshin OroojlooyJadid and Davood Hajinezhad.
\newblock A review of cooperative multi-agent deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1908.03963}, 2019.

\bibitem[Panait et~al.(2006)Panait, Luke, and Wiegand]{panait2006biasing}
Liviu Panait, Sean Luke, and R~Paul Wiegand.
\newblock Biasing coevolutionary search for optimal multiagent behaviors.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 10\penalty0
  (6):\penalty0 629--645, 2006.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, Witt, Farquhar, Foerster, and
  Whiteson]{rashid2018qmix}
Tabish Rashid, Mikayel Samvelyan, Christian~Schroeder Witt, Gregory Farquhar,
  Jakob Foerster, and Shimon Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4292--4301, 2018.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, de~Witt, Farquhar, Nardelli,
  Rudner, Hung, Torr, Foerster, and Whiteson]{samvelyan19smac}
Mikayel Samvelyan, Tabish Rashid, Christian~Schroeder de~Witt, Gregory
  Farquhar, Nantas Nardelli, Tim G.~J. Rudner, Chia-Man Hung, Philiph H.~S.
  Torr, Jakob Foerster, and Shimon Whiteson.
\newblock {The} {StarCraft} {Multi}-{Agent} {Challenge}.
\newblock \emph{CoRR}, abs/1902.04043, 2019.

\bibitem[Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi]{son2019qtran}
Kyunghwan Son, Daewoo Kim, Wan~Ju Kang, David~Earl Hostallero, and Yung Yi.
\newblock Qtran: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5887--5896, 2019.

\bibitem[Son et~al.(2020)Son, Ahn, Reyes, Shin, and Yi]{son2020qopt}
Kyunghwan Son, Sungsoo Ahn, Roben~Delos Reyes, Jinwoo Shin, and Yung Yi.
\newblock Qopt: Optimistic value function decentralization for cooperative
  multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.12010v1}, June 2020.

\bibitem[Sunehag et~al.(2018)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, and et~al.]{sunehag2018value}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z. Leibo, Karl
  Tuyls, and et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In \emph{Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, AAMAS ’18, page 2085–2087,
  Richland, SC, 2018. International Foundation for Autonomous Agents and
  Multiagent Systems.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tampuu et~al.(2017)Tampuu, Matiisen, Kodelja, Kuzovkin, Korjus, Aru,
  Aru, and Vicente]{tampuu_multiagent_2015}
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus,
  Juhan Aru, Jaan Aru, and Raul Vicente.
\newblock Multiagent cooperation and competition with deep reinforcement
  learning.
\newblock \emph{PloS one}, 2017.

\bibitem[Tan(1993)]{tan_multi-agent_1993}
Ming Tan.
\newblock Multi-agent reinforcement learning: {Independent} vs. cooperative
  agents.
\newblock In \emph{Proceedings of the Tenth International Conference on Machine
  Learning}, pages 330--337, 1993.

\bibitem[Wang et~al.(2020)Wang, Ren, Liu, Yu, and Zhang]{wang2020qplex}
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang.
\newblock Qplex: Duplex dueling multi-agent q-learning.
\newblock \emph{arXiv preprint arXiv:2008.01062}, 2020.

\bibitem[Watkins and Dayan(1992)]{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Wei et~al.(2018)Wei, Wicke, Freelan, and Luke]{wei2018multiagent}
Ermo Wei, Drew Wicke, David Freelan, and Sean Luke.
\newblock Multiagent soft q-learning.
\newblock In \emph{2018 AAAI Spring Symposium Series}, 2018.

\bibitem[Yang et~al.(2020{\natexlab{a}})Yang, Hao, Chen, Tang, Chen, Hu, Fan,
  and Wei]{yang2020q}
Yaodong Yang, Jianye Hao, Guangyong Chen, Hongyao Tang, Yingfeng Chen, Yujing
  Hu, Changjie Fan, and Zhongyu Wei.
\newblock Q-value path decomposition for deep multiagent reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{a}}.

\bibitem[Yang et~al.(2020{\natexlab{b}})Yang, Hao, Liao, Shao, Chen, Liu, and
  Tang]{yang2020qatten}
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and
  Hongyao Tang.
\newblock Qatten: A general framework for cooperative multiagent reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2002.03939}, 2020{\natexlab{b}}.

\bibitem[Yao et~al.(2019)Yao, Wen, Wang, and Tan]{yao_smixlambda:_2019}
Xinghu Yao, Chao Wen, Yuhui Wang, and Xiaoyang Tan.
\newblock {SMIX}(\${\textbackslash}lambda\$): Enhancing centralized value
  functions for cooperative multi-agent reinforcement learning.
\newblock 2019.

\end{thebibliography}
