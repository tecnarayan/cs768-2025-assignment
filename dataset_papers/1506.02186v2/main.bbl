\begin{thebibliography}{10}

\bibitem{agarwal}
A.~Agarwal and L.~Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2015.

\bibitem{bach2012optimization}
F.~Bach, R.~Jenatton, J.~Mairal, and G.~Obozinski.
\newblock Optimization with sparsity-inducing penalties.
\newblock {\em Foundations and Trends in Machine Learning}, 4(1):1--106, 2012.

\bibitem{Bauschke:2011}
H.~H. Bauschke and P.~L. Combettes.
\newblock {\em Convex Analysis and Monotone Operator Theory in Hilbert Spaces}.
\newblock Springer, 2011.

\bibitem{fista}
A.~Beck and M.~Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM Journal on Imaging Sciences}, 2(1):183--202, 2009.

\bibitem{bertsekas:2015}
D.~P. Bertsekas.
\newblock {\em Convex Optimization Algorithms}.
\newblock Athena Scientific, 2015.

\bibitem{saga}
A.~J. Defazio, F.~Bach, and S.~Lacoste{-}Julien.
\newblock {SAGA:} {A} fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Adv. Neural Information Processing Systems (NIPS)}, 2014.

\bibitem{finito}
A.~J. Defazio, T.~S. Caetano, and J.~Domke.
\newblock Finito: A faster, permutable incremental gradient method for big data
  problems.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2014.

\bibitem{frostig}
R.~Frostig, R.~Ge, S.~M. Kakade, and A.~Sidford.
\newblock Un-regularizing: approximate proximal point algorithms for empirical
  risk minimization.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2015.

\bibitem{guler:1992}
O.~G\"uler.
\newblock New proximal point algorithms for convex minimization.
\newblock {\em SIAM Journal on Optimization}, 2(4):649--664, 1992.

\bibitem{he2012accelerated}
B.~He and X.~Yuan.
\newblock An accelerated inexact proximal point algorithm for convex
  minimization.
\newblock {\em Journal of Optimization Theory and Applications},
  154(2):536--548, 2012.

\bibitem{hiriart1996convex}
J.-B. Hiriart-Urruty and C.~Lemar{\'e}chal.
\newblock {\em Convex Analysis and Minimization Algorithms I}.
\newblock Springer, 1996.

\bibitem{juditsky_nemirovsky_2012}
A.~Juditsky and A.~Nemirovski.
\newblock First order methods for nonsmooth convex large-scale optimization.
\newblock {\em Optimization for Machine Learning, MIT Press}, 2012.

\bibitem{conjugategradient}
G.~Lan.
\newblock An optimal randomized incremental gradient method.
\newblock {\em arXiv:1507.02000}, 2015.

\bibitem{miso}
J.~Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock {\em SIAM Journal on Optimization}, 25(2):829--855, 2015.

\bibitem{nemirovski}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em {SIAM} Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem{nesterov1983}
Y.~Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate {$O$(1/$k^2$)}.
\newblock {\em Soviet Mathematics Doklady}, 27(2):372--376, 1983.

\bibitem{nesterov}
Y.~Nesterov.
\newblock {\em Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer, 2004.

\bibitem{nesterov2012}
Y.~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{nesterov2013gradient}
Y.~Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock {\em Mathematical Programming}, 140(1):125--161, 2013.

\bibitem{Parikh13}
N.~Parikh and S.P. Boyd.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends in Optimization}, 1(3):123--231, 2014.

\bibitem{richtarik2014}
P.~Richt{\'a}rik and M.~Tak{\'a}{\v{c}}.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock {\em Mathematical Programming}, 144(1-2):1--38, 2014.

\bibitem{salzo2012inexact}
S.~Salzo and S.~Villa.
\newblock Inexact and accelerated proximal point algorithms.
\newblock {\em Journal of Convex Analysis}, 19(4):1167--1192, 2012.

\bibitem{proxinexact}
M.~Schmidt, N.~Le Roux, and F.~Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock In {\em Adv. Neural Information Processing Systems (NIPS)}, 2011.

\bibitem{sag}
M.~Schmidt, N.~Le Roux, and F.~Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em arXiv:1309.2388}, 2013.

\bibitem{sdca}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Proximal stochastic dual coordinate ascent.
\newblock {\em arXiv:1211.2717}, 2012.

\bibitem{accsdca}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock {\em Mathematical Programming}, 2015.

\bibitem{proxsvrg}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em {SIAM} Journal on Optimization}, 24(4):2057--2075, 2014.

\bibitem{zhangxiao}
Y.~Zhang and L.~Xiao.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2015.

\end{thebibliography}
