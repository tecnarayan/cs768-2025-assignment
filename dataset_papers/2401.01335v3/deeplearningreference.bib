Saunshi, Nikunj, et al. "A theoretical analysis of contrastive unsupervised representation learning." International Conference on Machine Learning. PMLR, 2019.
@article{liang2023factorized,
  title={Factorized Contrastive Learning: Going Beyond Multi-view Redundancy},
  author={Liang, Paul Pu and Deng, Zihao and Ma, Martin and Zou, James and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2306.05268},
  year={2023}
}

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020},
  year={2024}
}
@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  journal={arXiv preprint arXiv:2309.00267},
  year={2023}
}

@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}


@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{vecci1998learning,
  title={Learning and approximation capabilities of adaptive spline activation function neural networks},
  author={Vecci, Lorenzo and Piazza, Francesco and Uncini, Aurelio},
  journal={Neural Networks},
  volume={11},
  number={2},
  pages={259--270},
  year={1998},
  publisher={Elsevier}
}

@article{tresp2001mixtures,
  title={Mixtures of Gaussian processes},
  author={Tresp, Volker},
  journal={Advances in neural information processing systems},
  pages={654--660},
  year={2001},
  publisher={Citeseer}
}

@article{jordan1997hidden,
  title={Hidden Markov decision trees},
  author={Jordan, Michael I and Ghahramani, Zoubin and Saul, Lawrence K},
  journal={Advances in neural information processing systems},
  pages={501--507},
  year={1997},
  publisher={Citeseer}
}


@article{collobert2002parallel,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  journal={Neural computation},
  volume={14},
  number={5},
  pages={1105--1114},
  year={2002},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{allen2020feature,
  title={Feature purification: How adversarial training performs robust deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2005.10190},
  year={2020}
}

@article{allen2020towards,
  title={Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}


@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@inproceedings{xu2018global,
  title={Global convergence of langevin dynamics based algorithms for nonconvex optimization},
  author={Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3122--3133},
  year={2018}
}




@article{otto2000generalization,
  title={Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality},
  author={Otto, Felix and Villani, C{\'e}dric},
  journal={Journal of Functional Analysis},
  volume={173},
  number={2},
  pages={361--400},
  year={2000},
  publisher={Elsevier}
}

@article{fang2019over,
  title={Over Parameterized Two-level Neural Networks Can Learn Near Optimal Feature Representations},
  author={Fang, Cong and Dong, Hanze and Zhang, Tong},
  journal={arXiv preprint arXiv:1910.11508},
  year={2019}
}




@Article{zou2019gradient,
author="Zou, Difan
and Cao, Yuan
and Zhou, Dongruo
and Gu, Quanquan",
title="Gradient descent optimizes over-parameterized deep {ReLU} networks",
journal="Machine Learning",
year="2019",
month="Oct",
day="23"
}


@article{e2019comparative,
  title={A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1904.04326},
  year={2019}
}

@inproceedings{cao2019generalization,
  title={Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={the Thirty-Fourth AAAI Conference on Artificial Intelligence},
  year={2020}
}


@inproceedings{brutzkus2017globally,
  title={Globally optimal gradient descent for a convnet with gaussian inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={605--614},
  year={2017},
  organization={JMLR. org}
}
@inproceedings{oymak2018overparameterized,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle={International Conference on Machine Learning},
  pages={4951--4960},
  year={2019}
}
@article{zhang2019training,
  title={Training Over-parameterized Deep {ResNet} Is almost as Easy as Training a Two-layer Network},
  author={Zhang, Huishuai and Yu, Da and Chen, Wei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1903.07120},
  year={2019}
}
@article{wu2019global,
  title={Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network},
  author={Wu, Xiaoxia and Du, Simon S and Ward, Rachel},
  journal={arXiv preprint arXiv:1902.07111},
  year={2019}
}


@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{polyak1963gradient,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}






@article{slepian1962one,
  title={The one-sided barrier problem for Gaussian noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}


@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}



######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}





#####################Optimization Landscape


######################Generalization






@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}

@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1609.01037},
  year={2016}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}
@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}
@article{goel2016reliably,
  title={Reliably learning the {ReLU} in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}

@inproceedings{freeman2016topology,
  title={Topology and Geometry of Half-Rectified Network Optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}



@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@article{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:1711.00501},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@article{fu2018local,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}


@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}


@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}




@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}




@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}





@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################


@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@inproceedings{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6240--6249},
  year={2017}
}

@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}


@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################




@inproceedings{safran2017spurious,
  title={Spurious Local Minima are Common in Two-Layer ReLU Neural Networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4430--4438},
  year={2018}
}




@article{vaswani2018fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  journal={arXiv preprint arXiv:1810.07288},
  year={2018}
}

@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}




@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}





@article{bartlett2002rademacher,
  title={Rademacher and {Gaussian} complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}



@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}



@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations},
  year={2020}
}





@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  year={2019}
}




@inproceedings{allen2019can,
  title={What Can {ResNet} Learn Efficiently, Going Beyond Kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}



@article{nitanda2019refined,
  title={Refined Generalization Analysis of Gradient Descent for Over-parameterized Two-layer Neural Networks with Smooth Activations on Classification Problems},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}


@article{woodworth2019kernel,
  title={Kernel and Deep Regimes in Overparametrized Models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1906.05827},
  year={2019}
}






@article{venturi2018neural,
  title={Neural networks with finite intrinsic dimension have no spurious valleys},
  author={Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  volume={15},
  year={2018}
}






@inproceedings{ji2019implicit,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1772--1798},
  year={2019}
}





@article{bai2019beyond,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Bai, Yu and Lee, Jason D},
  journal={arXiv preprint arXiv:1910.01619},
  year={2019}
}




@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}


@article{belkin2019two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={arXiv preprint arXiv:1903.07571},
  year={2019}
}



@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}


@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}


















@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}


@article{soudry2017implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}




@article{slepian1962one,
  title={The one-sided barrier problem for {Gaussian} noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}




#####################Optimization Landscape


######################Generalization







@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}
@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}

@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1609.01037},
  year={2016}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}
@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}
@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}
@inproceedings{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={586--594},
  year={2016}
}
@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}


@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@inproceedings{nguyen2017loss,
  title={The Loss Surface of Deep and Wide Neural Networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={International Conference on Machine Learning},
  pages={2603--2612},
  year={2017}
}
@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}


@article{soudry2016no,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}
@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@article{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:1711.00501},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@article{fu2018local,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}


@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}





@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}



@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}



@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################


@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}


@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}


@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################
@inproceedings{yarotsky2018optimal,
  title={Optimal approximation of continuous functions by very deep ReLU networks},
  author={Yarotsky, Dmitry},
  booktitle={Conference On Learning Theory},
  pages={639--649},
  year={2018}
}




@article{nacson2018convergence,
  title={Convergence of Gradient Descent on Separable Data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Srebro, Nathan and Soudry, Daniel},
  journal={arXiv preprint arXiv:1803.01905},
  year={2018}
}

@article{nag2018,
  title={Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience. },
  author={Vaishnavh Nagarajan and J. Zico Kolter},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{minshuo2018,
  title={On Generalization Bounds for a Family of Recurrent Neural Networks. },
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{neyshabur2018towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}


@article{li2018tighter,
  title={On Tighter Generalization Bound for Deep Neural Networks: {CNNs}, {ResNets}, and Beyond},
  author={Li, Xingguo and Lu, Junwei and Wang, Zhaoran and Haupt, Jarvis and Zhao, Tuo},
  journal={arXiv preprint arXiv:1806.05159},
  year={2018}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@article{mou2017generalization,
  title={Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal={arXiv preprint arXiv:1707.05947},
  year={2017}
}



@article{chen2018stability,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{zhou2018generalization,
  title={Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization},
  author={Zhou, Yi and Liang, Yingbin and Zhang, Huishuai},
  journal={arXiv preprint arXiv:1802.06903},
  year={2018}
}



@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}




@article{haeffele2015global,
  title={Global optimality in tensor factorization, deep learning, and beyond},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  journal={arXiv preprint arXiv:1506.07540},
  year={2015}
}





@article{soudry2017exponentially,
  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},
  author={Soudry, Daniel and Hoffer, Elad},
  journal={arXiv preprint arXiv:1702.05777},
  year={2017}
}





@inproceedings{lin2018resnet,
  title={{ResNet} with one-neuron hidden layers is a Universal Approximator},
  author={Lin, Hongzhou and Jegelka, Stefanie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6172--6181},
  year={2018}
}





@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}



@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: {Gaussian} process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}



@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}


@article{song2018mean,
  title={A mean field view of the landscape of two-layers neural networks},
  author={Song, Mei and Montanari, A and Nguyen, P},
  journal={PNAS},
  volume={115},
  pages={E7665--E7671},
  year={2018}
}





@inproceedings{hardt2015train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016}
}



@inproceedings{gao2019learning,
  title={Learning One-hidden-layer Neural Networks under General Input Distributions},
  author={Gao, Weihao and Makkuva, Ashok and Oh, Sewoong and Viswanath, Pramod},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1950--1959},
  year={2019}
}


















@string{JASA = {Journal of the American Statistical Association}}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society, Series B}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{ICML  = {International Conference on Machine Learning}}
@string{TIT = {IEEE Transactions on Information Theory}}


@inproceedings{harvey2017nearly,
  title={Nearly-tight VC-dimension bounds for piecewise linear neural networks},
  author={Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  booktitle={Conference on Learning Theory},
  pages={1064--1068},
  year={2017}
}

@inproceedings{bartlett1999almost,
  title={Almost linear VC dimension bounds for piecewise polynomial networks},
  author={Bartlett, Peter L and Maiorov, Vitaly and Meir, Ron},
  booktitle={Advances in Neural Information Processing Systems},
  pages={190--196},
  year={1999}
}

@inproceedings{du2017convolutional,
	title={When is a Convolutional Filter Easy To Learn?},
	author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
	booktitle={International Conference on Learning Representations},
	year={2018}
}




@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}


@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@inproceedings{ren2015faster,
  title={Faster {R-CNN}: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={91--99},
  year={2015}
}
@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{baum1990polynomial,
  title={A polynomial time algorithm that learns two hidden unit nets},
  author={Baum, Eric B},
  journal={Neural Computation},
  volume={2},
  number={4},
  pages={510--522},
  year={1990},
  publisher={MIT Press}
}
@inproceedings{zhang2016convexified,
  title={Convexified Convolutional Neural Networks},
  author={Zhang, Yuchen and Liang, Percy and Wainwright, Martin J},
  booktitle={International Conference on Machine Learning},
  pages={4044--4053},
  year={2017}
}


@article{nguyen2017bloss,
  title={The loss surface and expressivity of deep convolutional neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  journal={arXiv preprint arXiv:1710.10928},
  year={2017}
}




@article{tian2016symmetry,
  title={Symmetry-breaking convergence analysis of certain two-layered neural networks with ReLU nonlinearity},
  author={Tian, Yuandong},
  year={2016}
}


@incollection{klivans2009baum,
  title={Baum’s algorithm learns intersections of halfspaces with respect to log-concave distributions},
  author={Klivans, Adam R and Long, Philip M and Tang, Alex K},
  booktitle={Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  pages={588--600},
  year={2009},
  publisher={Springer}
}


@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}


@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}

@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}

@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}

@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}
@inproceedings{yun2017global,
  title={Global optimality conditions for deep neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}


@article{soltanolkotabi2017theoretical,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={2},
  pages={742--769},
  year={2018},
  publisher={IEEE}
}


@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}

@inproceedings{hardt2016identity,
  title={Identity matters in deep learning},
  author={Hardt, Moritz and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}


@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@inproceedings{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}
\nshortmid




@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}



@inproceedings{yi2015regularized,
  title={Regularized em algorithms: A unified framework and statistical guarantees},
  author={Yi, Xinyang and Caramanis, Constantine},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1567--1575},
  year={2015}
}




@misc{hoeffding1940masstabinvariante,
  title={Masstabinvariante Korrelationtheorie, Schriften des Mathematis chen Instituts und des Instituts f{\"u}r Angewandte Mathematik der Universit{\"a}t Berlin 5, 181\# 233.(Translated in Fisher, NI and PK Sen (1994). The Collected Works of Wassily Hoeffding, New York},
  author={Hoeffding, W},
  year={1940},
  publisher={Springer-Verlag}
}


@article{cuadras2002covariance,
  title={On the covariance between functions},
  author={Cuadras, Carles M},
  journal={Journal of Multivariate Analysis},
  volume={81},
  number={1},
  pages={19--27},
  year={2002},
  publisher={Elsevier}
}


@incollection{sen1994impact,
  title={The impact of Wassily Hoeffding’s research on nonparametrics},
  author={Sen, Pranab K},
  booktitle={The Collected Works of Wassily Hoeffding},
  pages={29--55},
  year={1994},
  publisher={Springer}
}



@article{gordon1985some,
  title={Some inequalities for {Gaussian} processes and applications},
  author={Gordon, Yehoram},
  journal={Israel Journal of Mathematics},
  volume={50},
  number={4},
  pages={265--289},
  year={1985},
  publisher={Springer}
}


@inproceedings{goel2018learning,
  title={Learning One Convolutional Layer with Overlapping Patches},
  author={Goel, Surbhi and Klivans, Adam and Meka, Raghu},
  booktitle={International Conference on Machine Learning},
  pages={1778--1786},
  year={2018}
}


@article{du2018improved,
  title={Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps},
  author={Du, Simon S and Goel, Surbhi},
  journal={arXiv preprint arXiv:1805.07798},
  year={2018}
}

@inproceedings{du2018many,
  title={How Many Samples are Needed to Learn a Convolutional Neural Network?},
  author={Du, Simon S and Wang, Yining and Zhai, Xiyu and Balakrishnan, Sivaraman and Salakhutdinov, Ruslan and Singh, Aarti},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}



@book{talagrand2014upper,
  title={Upper and lower bounds for stochastic processes: modern methods and classical problems},
  author={Talagrand, Michel},
  volume={60},
  year={2014},
  publisher={Springer Science \& Business Media}
}





@inproceedings{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}
@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}
@inproceedings{brutzkus2017sgd,
  title={SGD learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  booktitle = {International Conference on Learning Representations},
  year={2018}
}

@inproceedings{nacson2019stochastic,
  title={Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate},
  author={Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3051--3059},
  year={2019}
}


@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}



@article{slepian1962one,
  title={The one-sided barrier problem for {Gaussian} noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{arora2018convergence,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  booktitle={International Conference on Learning Representations},
  year={2019}
}




#####################Optimization Landscape


######################Generalization



@inproceedings{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={597--607},
  year={2017}
}



@inproceedings{zhong2017recovery,
  title={Recovery Guarantees for One-hidden-layer Neural Networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  booktitle={International Conference on Machine Learning},
  pages={4140--4149},
  year={2017}
}

@article{tian2017analytical,
  title={An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}


@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  booktitle={Advances in neural information processing systems},
  pages={6231--6239},
  year={2017}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}
@inproceedings{telgarsky2016benefits,
  title={benefits of depth in neural networks},
  author={Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1517--1539},
  year={2016}
}
@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1135--1163},
  year={2018},
  publisher={JMLR. org}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}

@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}

@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{safran2016quality,
  title={On the quality of the initial basin in overspecified neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={774--782},
  year={2016}
}


@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}
@inproceedings{xie2017diverse,
  title={Diverse Neural Network Learns True Target Functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle={Artificial Intelligence and Statistics},
  pages={1216--1224},
  year={2017}
}
@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}
@inproceedings{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  booktitle={The Annals of Statistics},
  year={2018}
}
@inproceedings{du2017gradient,
  title={Gradient Descent Learns One-hidden-layer {CNN}: Don’t be Afraid of Spurious Local Minima},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong and Singh, Aarti and Poczos, Barnabas},
  booktitle={International Conference on Machine Learning},
  pages={1338--1347},
  year={2018}
}
@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}

@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@inproceedings{fu2018local,
  title={Local Geometry of Cross Entropy Loss in Learning One-Hidden-Layer Neural Networks},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)},
  pages={1972--1976},
  year={2019},
  organization={IEEE}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}




@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}



@inproceedings{liang2016deep,
  title={Why deep neural networks for function approximation?},
  author={Liang, Shiyu and Srikant, R},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}


@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}

@inproceedings{du2018power,
  title={On the Power of Over-parametrization in Neural Networks with Quadratic Activation},
  author={Du, Simon S and Lee, Jason D},
  booktitle={International Conference on Machine Learning},
  pages={1328--1337},
  year={2018}
}


@inproceedings{allen2018convergence,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019}
}


@inproceedings{du2018gradientdeep,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################
@inproceedings{neyshabur2017pac,
  title={A pac-bayesian approach to spectrally-normalized margin bounds for neural networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  booktitle={International Conference on Learning Representation},
  year={2018}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015}
}


@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@inproceedings{golowich2017size,
  title={Size-Independent Sample Complexity of Neural Networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018}
}

@inproceedings{arora2018stronger,
  title={Stronger Generalization Bounds for Deep Nets via a Compression Approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018}
}

@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################


@article{hanin2017universal,
  title={Universal function approximation by deep neural nets with bounded width and relu activations},
  author={Hanin, Boris},
  journal={arXiv preprint arXiv:1708.02691},
  year={2017}
}

@article{hanin2017approximating,
  title={Approximating Continuous Functions by {ReLU} Nets of Minimal Width},
  author={Hanin, Boris and Sellke, Mark},
  journal={arXiv preprint arXiv:1710.11278},
  year={2017}
}


@inproceedings{allen2018rnn,
  title={On the Convergence Rate of Training Recurrent Neural Networks},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{zhang2018learning,
  title={Learning One-hidden-layer {ReLU} Networks via Gradient Descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1524--1534},
  year={2019}
}

@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}


@inproceedings{allen2018learning,
  title={Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{nacson2018convergence,
  title={Convergence of Gradient Descent on Separable Data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019}
}



@article{nag2018,
  title={Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience. },
  author={Vaishnavh Nagarajan and J. Zico Kolter},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{minshuo2018,
  title={On Generalization Bounds for a Family of Recurrent Neural Networks. },
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}



@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@article{mou2017generalization,
  title={Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal={arXiv preprint arXiv:1707.05947},
  year={2017}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  booktitle={Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48},
  pages={1225--1234},
  year={2016},
  organization={JMLR. org}
}

@article{chen2018stability,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{zhou2018generalization,
  title={Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization},
  author={Zhou, Yi and Liang, Yingbin and Zhang, Huishuai},
  journal={arXiv preprint arXiv:1802.06903},
  year={2018}
}

@inproceedings{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Advances in neural information processing systems},
  pages={3036--3046},
  year={2018}
}


@inproceedings{arora2018optimization,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018}
}


@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}



@inproceedings{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2017}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}


@inproceedings{gunasekar2018characterizing,
  title={Characterizing Implicit Bias in Terms of Optimization Geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1827--1836},
  year={2018}
}


@inproceedings{liang2018just,
  title={Just Interpolate: Kernel" Ridgeless" Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  booktitle={The Annals of Statistics},
  year={2019}
}



@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018}
}

@inproceedings{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6151--6159},
  year={2017}
}



@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@inproceedings{soltanolkotabi2017learning,
  title={Learning {ReLUs} via gradient descent},
  author={Soltanolkotabi, Mahdi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2007--2017},
  year={2017}
}


@inproceedings{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2017}
}



@article{wei2018regularization,
  title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{soudry2017exponentially,
  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},
  author={Soudry, Daniel and Hoffer, Elad},
  journal={arXiv preprint arXiv:1702.05777},
  year={2017}
}

@article{zhou2017critical,
  title={Critical Points of Neural Networks: Analytical Forms and Landscape Properties},
  author={Zhou, Yi and Liang, Yingbin},
  journal={arXiv preprint arXiv:1710.11205},
  year={2017}
}

@inproceedings{yun2019small,
  title={Small nonlinearities in activation functions create bad local minima in neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representation},
  year={2019}
}

@article{sirignano2019mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  year={2019},
  publisher={Elsevier}
}



@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{rotskoff2018neural,
  title={Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@inproceedings{chizat2018note,
  title={On Lazy Training in Differentiable Programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}






@inproceedings{rahimi2009weighted,
  title={Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1313--1320},
  year={2009}
}

@inproceedings{mendelson2014learning,
  title={Learning without concentration},
  author={Mendelson, Shahar},
  booktitle={Conference on Learning Theory},
  pages={25--39},
  year={2014}
}





@inproceedings{maurer2016vector,
  title={A vector-contraction inequality for rademacher complexities},
  author={Maurer, Andreas},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={3--17},
  year={2016},
  organization={Springer}
}



@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}




@inproceedings{yehudai2019power,
  title={On the Power and Limitations of Random Features for Understanding Neural Networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@article{e2019comparative,
  title={A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1904.04326},
  year={2019}
}



@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}


@article{oymak2019towards,
  title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1902.04674},
  year={2019}
}

@inproceedings{neyshabur2018role,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@article{long2019size,
  title={Size-free generalization bounds for convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}

@inproceedings{jain2019making,
  title={Making the Last Iterate of SGD Information Theoretically Optimal},
  author={Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  year={2019}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@inproceedings{zou2019improved,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{cao2019generalizationsgd,
  title={Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@article{fang2019convexformulation,
  title={Convex Formulation of Overparameterized Deep Neural Networks},
  author={Fang, Cong and Gu, Yihong and Zhang, Weizhong and Zhang, Tong},
  journal={arXiv preprint arXiv:1911.07626},
  year={2019}
}


@inproceedings{nguyen2019connected,
  title={On Connected Sublevel Sets in Deep Learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019}
}



@inproceedings{cao2019tight,
  title={Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10611--10621},
  year={2019}
}




@inproceedings{frei2019algorithm,
  title={Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14769--14779},
  year={2019}
}


@inproceedings{nguyen2019connected,
  title={On Connected Sublevel Sets in Deep Learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019}
}


@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}



@inproceedings{frei2019algorithm,
  title={Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14769--14779},
  year={2019}
}



@article{sirignano2019mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  year={2019},
  publisher={Elsevier}
}





@inproceedings{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@inproceedings{basri2019convergence,
  title={The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies},
  author={Basri, Ronen and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@inproceedings{nakkiran2019sgd,
  title={SGD on Neural Networks Learns Functions of Increasing Complexity},
  author={Nakkiran, Preetum and Kaplun, Gal and Kalimeris, Dimitris and Yang, Tristan and Edelman, Benjamin L and Zhang, Fred and Barak, Boaz},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{rahaman2019spectral,
  title={On the Spectral Bias of Neural Networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019}
}



@inproceedings{vempala2018gradient,
  title={Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds},
  author={Vempala, Santosh and Wilmes, John},
  booktitle={Conference on Learning Theory},
  year={2019}
}



@inproceedings{su2019learning,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Prospective},
  author={Su, Lili and Yang, Pengkun},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@Article{zhang2019resnet,
author="Zhang, Huishuai
and Yu, Da
and Yi, Mingyang
and Chen, Wei
and Liu, Tie-Yan",
title="Convergence Theory of Learning Over-parameterized ResNet:
A Full Characterization",
journal="arXiv preprint arXiv:1903.07120",
year="2019",
month="Jul",
day="15"
}





@article{cao2019towards,
  title={Towards Understanding the Spectral Bias of Deep Learning},
  author={Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.01198},
  year={2019}
}



@article{chen2019much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1911.12360},
  year={2019}
}



@article{allen2020backward,
  title={Backward Feature Correction: How Deep Learning Performs Deep Learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}



@article{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}


@article{nacson2019lexicographic,
  title={Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason D and Srebro, Nathan and Soudry, Daniel},
  journal={arXiv preprint arXiv:1905.07325},
  year={2019}
}



@article{meir2003generalization,
  title={Generalization error bounds for Bayesian mixture algorithms},
  author={Meir, Ron and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={4},
  number={Oct},
  pages={839--860},
  year={2003}
}


@article{donsker1983asymptotic,
  title={Asymptotic evaluation of certain Markov process expectations for large time. IV},
  author={Donsker, Monroe D and Varadhan, SR Srinivasa},
  journal={Communications on Pure and Applied Mathematics},
  volume={36},
  number={2},
  pages={183--212},
  year={1983},
  publisher={Wiley Online Library}
}





@inproceedings{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11669--11680},
  year={2019}
}




@book{bakry2013analysis,
  title={Analysis and geometry of Markov diffusion operators},
  author={Bakry, Dominique and Gentil, Ivan and Ledoux, Michel},
  volume={348},
  year={2013},
  publisher={Springer Science \& Business Media}
}




@article{tzen2020mean,
  title={A mean-field theory of lazy training in two-layer neural nets: entropic regularization and controlled McKean-Vlasov dynamics},
  author={Tzen, Belinda and Raginsky, Maxim},
  journal={arXiv preprint arXiv:2002.01987},
  year={2020}
}



@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}



@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}





@article{wang2020benign,
  title={Benign Overfitting in Binary Classification of Gaussian Mixtures},
  author={Wang, Ke and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2011.09148},
  year={2020}
}


@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}



@article{muthukumar2020classification,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  journal={arXiv preprint arXiv:2005.08054},
  year={2020}
}



@article{chatterji2020finite,
  title={Finite-sample analysis of interpolating linear classifiers in the overparameterized regime},
  author={Chatterji, Niladri S and Long, Philip M},
  journal={arXiv preprint arXiv:2004.12019},
  year={2020}
}




@article{muthukumar2020harmless,
  title={Harmless interpolation of noisy data in regression},
  author={Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={67--83},
  year={2020},
  publisher={IEEE}
}


@article{tsigler2020benign,
  title={Benign overfitting in ridge regression},
  author={Tsigler, Alexander and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2009.14286},
  year={2020}
}




@article{wu2020optimal,
  title={On the Optimal Weighted $\ell_2$ Regularization in Overparameterized Linear Regression},
  author={Wu, Denny and Xu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@inproceedings{liao2020random,
  title={A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent},
  author={Liao, Zhenyu and Couillet, Romain and Mahoney, Michael},
  booktitle={34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
  year={2020}
}


@article{montanari2020interpolation,
  title={The interpolation phase transition in neural networks: Memorization and generalization under lazy training},
  author={Montanari, Andrea and Zhong, Yiqiao},
  journal={arXiv preprint arXiv:2007.12826},
  year={2020}
}


@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}



@article{hsu2020proliferation,
  title={On the proliferation of support vectors in high dimensions},
  author={Hsu, Daniel and Muthukumar, Vidya and Xu, Ji},
  journal={arXiv preprint arXiv:2009.10670},
  year={2020}
}



@article{donoho2008higher,
  title={Higher criticism thresholding: Optimal feature selection when useful features are rare and weak},
  author={Donoho, David and Jin, Jiashun},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={39},
  pages={14790--14795},
  year={2008},
  publisher={National Acad Sciences}
}



@article{jin2009impossibility,
  title={Impossibility of successful classification when useful features are rare and weak},
  author={Jin, Jiashun},
  journal={Proceedings of the National Academy of Sciences},
  volume={106},
  number={22},
  pages={8859--8864},
  year={2009},
  publisher={National Acad Sciences}
}



@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}





@article{cote2012chernoff,
  title={A Chernoff-type lower bound for the Gaussian Q-function},
  author={C{\^o}t{\'e}, Fran{\c{c}}ois D and Psaromiligkos, Ioannis N and Gross, Warren J},
  journal={arXiv preprint arXiv:1202.6483},
  year={2012}
}




@article{jelassi2021adam,
  title={Adam is no better than normalized SGD: Dissecting how adaptivity improves GAN performance},
  author={Jelassi, Samy and Mensch, Arthur and Gidel, Gauthier and Li, Yuanzhi},
  year={2021}
}





@inproceedings{arora2019theoretical,
  title={Theoretical analysis of auto rate-tuning by batch normalization},
  author={Arora, Sanjeev and Lyu, Kaifeng and Li, Zhiyuan},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}


@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{liang2006end,
  title={An end-to-end discriminative approach to machine translation},
  author={Liang, Percy and Bouchard-C{\^o}t{\'e}, Alexandre and Klein, Dan and Taskar, Ben},
  booktitle={Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  pages={761--768},
  year={2006}
}

@article{quattoni2004conditional,
  title={Conditional random fields for object recognition},
  author={Quattoni, Ariadna and Collins, Michael and Darrell, Trevor},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}

@inproceedings{wang2009max,
  title={Max-margin hidden conditional random fields for human action recognition},
  author={Wang, Yang and Mori, Greg},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={872--879},
  year={2009},
  organization={IEEE}
}



@inproceedings{anandkumar2012method,
  title={A method of moments for mixture models and hidden Markov models},
  author={Anandkumar, Animashree and Hsu, Daniel and Kakade, Sham M},
  booktitle={Conference on Learning Theory},
  pages={33--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{hsu2012identifiability,
  title={Identifiability and unmixing of latent parse trees},
  author={Hsu, Daniel J and Kakade, Sham M and Liang, Percy S},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@inproceedings{chaganty2013spectral,
  title={Spectral experts for estimating mixtures of linear regressions},
  author={Chaganty, Arun Tejasvi and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={1040--1048},
  year={2013},
  organization={PMLR}
}

@article{anandkumar2014tensor,
  title={Tensor decompositions for learning latent variable models},
  author={Anandkumar, Animashree and Ge, Rong and Hsu, Daniel and Kakade, Sham M and Telgarsky, Matus},
  journal={Journal of machine learning research},
  volume={15},
  pages={2773--2832},
  year={2014},
  publisher={Journal of Machine Learning Research}
}

@article{de1989mixtures,
  title={Mixtures of linear regressions},
  author={De Veaux, Richard D},
  journal={Computational Statistics \& Data Analysis},
  volume={8},
  number={3},
  pages={227--245},
  year={1989},
  publisher={Elsevier}
}

@article{faria2010fitting,
  title={Fitting mixtures of linear regressions},
  author={Faria, Susana and Soromenho, Gilda},
  journal={Journal of Statistical Computation and Simulation},
  volume={80},
  number={2},
  pages={201--225},
  year={2010},
  publisher={Taylor \& Francis}
}

@article{khalili2007variable,
  title={Variable selection in finite mixture of regression models},
  author={Khalili, Abbas and Chen, Jiahua},
  journal={Journal of the american Statistical association},
  volume={102},
  number={479},
  pages={1025--1038},
  year={2007},
  publisher={Taylor \& Francis}
}

@inproceedings{yi2014alternating,
  title={Alternating minimization for mixed linear regression},
  author={Yi, Xinyang and Caramanis, Constantine and Sanghavi, Sujay},
  booktitle={International Conference on Machine Learning},
  pages={613--621},
  year={2014},
  organization={PMLR}
}

@article{balakrishnan2017statistical,
  title={Statistical guarantees for the EM algorithm: From population to sample-based analysis},
  author={Balakrishnan, Sivaraman and Wainwright, Martin J and Yu, Bin},
  journal={The Annals of Statistics},
  volume={45},
  number={1},
  pages={77--120},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}





@inproceedings{joulin2016learning,
  title={Learning visual features from large weakly supervised data},
  author={Joulin, Armand and Maaten, Laurens van der and Jabri, Allan and Vasilache, Nicolas},
  booktitle={European Conference on Computer Vision},
  pages={67--84},
  year={2016},
  organization={Springer}
}


@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}


@article{novak2011metric,
  title={Metric index: An efficient and scalable solution for precise and approximate similarity search},
  author={Novak, David and Batko, Michal and Zezula, Pavel},
  journal={Information Systems},
  volume={36},
  number={4},
  pages={721--733},
  year={2011},
  publisher={Elsevier}
}



@article{jegou2010product,
  title={Product quantization for nearest neighbor search},
  author={Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2010},
  publisher={IEEE}
}

@inproceedings{wang2021understanding,
  title={Understanding the behaviour of contrastive loss},
  author={Wang, Feng and Liu, Huaping},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2495--2504},
  year={2021}
}
@inproceedings{wang2020understanding,
  title={Understanding contrastive representation learning through alignment and uniformity on the hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  booktitle={International Conference on Machine Learning},
  pages={9929--9939},
  year={2020},
  organization={PMLR}
}




@inproceedings{sivic2003video,
  title={Video Google: A text retrieval approach to object matching in videos},
  author={Sivic, Josef and Zisserman, Andrew},
  booktitle={Computer Vision, IEEE International Conference on},
  volume={3},
  pages={1470--1470},
  year={2003},
  organization={IEEE Computer Society}
}



@article{aghajanyan2022cm3,
  title={Cm3: A causal masked multimodal model of the internet},
  author={Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and others},
  journal={arXiv preprint arXiv:2201.07520},
  year={2022}
}



@article{luo2022clip4clip,
  title={CLIP4Clip: An empirical study of CLIP for end to end video clip retrieval and captioning},
  author={Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
  journal={Neurocomputing},
  volume={508},
  pages={293--304},
  year={2022},
  publisher={Elsevier}
}




@inproceedings{kraska2018case,
  title={The case for learned index structures},
  author={Kraska, Tim and Beutel, Alex and Chi, Ed H and Dean, Jeffrey and Polyzotis, Neoklis},
  booktitle={Proceedings of the 2018 international conference on management of data},
  pages={489--504},
  year={2018}
}




@inproceedings{marcus2018deep,
  title={Deep reinforcement learning for join order enumeration},
  author={Marcus, Ryan and Papaemmanouil, Olga},
  booktitle={Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management},
  pages={1--4},
  year={2018}
}



@article{marcus2020bao,
  title={Bao: Learning to steer query optimizers},
  author={Marcus, Ryan and Negi, Parimarjan and Mao, Hongzi and Tatbul, Nesime and Alizadeh, Mohammad and Kraska, Tim},
  journal={arXiv preprint arXiv:2004.03814},
  year={2020}
}



@article{cilibrasi2007google,
  title={The google similarity distance},
  author={Cilibrasi, Rudi L and Vitanyi, Paul MB},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={19},
  number={3},
  pages={370--383},
  year={2007},
  publisher={IEEE}
}

@book{greenwald2013oracle,
  title={Oracle essentials: Oracle database 12c},
  author={Greenwald, Rick and Stackowiak, Robert and Stern, Jonathan},
  year={2013},
  publisher={" O'Reilly Media, Inc."}
}


@book{murty2008programming,
  title={Programming amazon web services: S3, EC2, SQS, FPS, and SimpleDB},
  author={Murty, James},
  year={2008},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{elhemali2022amazon,
  title={Amazon $\{$DynamoDB$\}$: A Scalable, Predictably Performant, and Fully Managed $\{$NoSQL$\}$ Database Service},
  author={Elhemali, Mostafa and Gallagher, Niall and Tang, Bin and Gordon, Nick and Huang, Hao and Chen, Haibo and Idziorek, Joseph and Wang, Mengtian and Krog, Richard and Zhu, Zongpeng and others},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={1037--1048},
  year={2022}
}



@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013}
}

@article{vecci1998learning,
  title={Learning and approximation capabilities of adaptive spline activation function neural networks},
  author={Vecci, Lorenzo and Piazza, Francesco and Uncini, Aurelio},
  journal={Neural Networks},
  volume={11},
  number={2},
  pages={259--270},
  year={1998},
  publisher={Elsevier}
}

@article{tresp2001mixtures,
  title={Mixtures of Gaussian processes},
  author={Tresp, Volker},
  journal={Advances in neural information processing systems},
  pages={654--660},
  year={2001},
  publisher={Citeseer}
}

@article{jordan1997hidden,
  title={Hidden Markov decision trees},
  author={Jordan, Michael I and Ghahramani, Zoubin and Saul, Lawrence K},
  journal={Advances in neural information processing systems},
  pages={501--507},
  year={1997},
  publisher={Citeseer}
}


@article{collobert2002parallel,
  title={A parallel mixture of SVMs for very large scale problems},
  author={Collobert, Ronan and Bengio, Samy and Bengio, Yoshua},
  journal={Neural computation},
  volume={14},
  number={5},
  pages={1105--1114},
  year={2002},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{allen2020feature,
  title={Feature purification: How adversarial training performs robust deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2005.10190},
  year={2020}
}

@article{allen2020towards,
  title={Towards understanding ensemble, knowledge distillation and self-distillation in deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2012.09816},
  year={2020}
}

@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}


@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}


@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@inproceedings{xu2018global,
  title={Global convergence of langevin dynamics based algorithms for nonconvex optimization},
  author={Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3122--3133},
  year={2018}
}




@article{otto2000generalization,
  title={Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality},
  author={Otto, Felix and Villani, C{\'e}dric},
  journal={Journal of Functional Analysis},
  volume={173},
  number={2},
  pages={361--400},
  year={2000},
  publisher={Elsevier}
}

@article{fang2019over,
  title={Over Parameterized Two-level Neural Networks Can Learn Near Optimal Feature Representations},
  author={Fang, Cong and Dong, Hanze and Zhang, Tong},
  journal={arXiv preprint arXiv:1910.11508},
  year={2019}
}




@Article{zou2019gradient,
author="Zou, Difan
and Cao, Yuan
and Zhou, Dongruo
and Gu, Quanquan",
title="Gradient descent optimizes over-parameterized deep {ReLU} networks",
journal="Machine Learning",
year="2019",
month="Oct",
day="23"
}


@article{e2019comparative,
  title={A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1904.04326},
  year={2019}
}

@inproceedings{cao2019generalization,
  title={Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={the Thirty-Fourth AAAI Conference on Artificial Intelligence},
  year={2020}
}


@inproceedings{brutzkus2017globally,
  title={Globally optimal gradient descent for a convnet with gaussian inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={605--614},
  year={2017},
  organization={JMLR. org}
}
@inproceedings{oymak2018overparameterized,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle={International Conference on Machine Learning},
  pages={4951--4960},
  year={2019}
}
@article{zhang2019training,
  title={Training Over-parameterized Deep {ResNet} Is almost as Easy as Training a Two-layer Network},
  author={Zhang, Huishuai and Yu, Da and Chen, Wei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1903.07120},
  year={2019}
}
@article{wu2019global,
  title={Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network},
  author={Wu, Xiaoxia and Du, Simon S and Ward, Rachel},
  journal={arXiv preprint arXiv:1902.07111},
  year={2019}
}


@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{polyak1963gradient,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}


@article{zhang2002covering,
  title={Covering number bounds of certain regularized linear function classes},
  author={Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={2},
  number={Mar},
  pages={527--550},
  year={2002}
}


@article{slepian1962one,
  title={The one-sided barrier problem for Gaussian noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}





#####################Optimization Landscape


######################Generalization






@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}

@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1609.01037},
  year={2016}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}
@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}
@article{goel2016reliably,
  title={Reliably learning the {ReLU} in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}

@inproceedings{freeman2016topology,
  title={Topology and Geometry of Half-Rectified Network Optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}



@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@article{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:1711.00501},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@article{fu2018local,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}


@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}


@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}




@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}




@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}





@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################


@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@inproceedings{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6240--6249},
  year={2017}
}

@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}


@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################




@inproceedings{safran2017spurious,
  title={Spurious Local Minima are Common in Two-Layer ReLU Neural Networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4430--4438},
  year={2018}
}




@article{vaswani2018fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  journal={arXiv preprint arXiv:1810.07288},
  year={2018}
}

@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}




@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}







@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}



@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations},
  year={2020}
}





@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  year={2019}
}




@inproceedings{allen2019can,
  title={What Can {ResNet} Learn Efficiently, Going Beyond Kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}



@article{nitanda2019refined,
  title={Refined Generalization Analysis of Gradient Descent for Over-parameterized Two-layer Neural Networks with Smooth Activations on Classification Problems},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}


@article{woodworth2019kernel,
  title={Kernel and Deep Regimes in Overparametrized Models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1906.05827},
  year={2019}
}






@article{venturi2018neural,
  title={Neural networks with finite intrinsic dimension have no spurious valleys},
  author={Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  volume={15},
  year={2018}
}






@inproceedings{ji2019implicit,
  title={The implicit bias of gradient descent on nonseparable data},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1772--1798},
  year={2019}
}





@article{bai2019beyond,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Bai, Yu and Lee, Jason D},
  journal={arXiv preprint arXiv:1910.01619},
  year={2019}
}




@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}


@article{belkin2019two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={arXiv preprint arXiv:1903.07571},
  year={2019}
}



@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}


@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}


















@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}


@article{soudry2017implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}




@article{slepian1962one,
  title={The one-sided barrier problem for {Gaussian} noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}




#####################Optimization Landscape


######################Generalization







@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}
@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}

@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={arXiv preprint arXiv:1609.01037},
  year={2016}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}
@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}
@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}
@inproceedings{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={586--594},
  year={2016}
}
@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}


@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@inproceedings{nguyen2017loss,
  title={The Loss Surface of Deep and Wide Neural Networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={International Conference on Machine Learning},
  pages={2603--2612},
  year={2017}
}
@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}


@article{soudry2016no,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}
@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@article{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:1711.00501},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@article{fu2018local,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}


@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}





@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}



@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}



@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################


@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}


@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}


@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################
@inproceedings{yarotsky2018optimal,
  title={Optimal approximation of continuous functions by very deep ReLU networks},
  author={Yarotsky, Dmitry},
  booktitle={Conference On Learning Theory},
  pages={639--649},
  year={2018}
}




@article{nacson2018convergence,
  title={Convergence of Gradient Descent on Separable Data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Srebro, Nathan and Soudry, Daniel},
  journal={arXiv preprint arXiv:1803.01905},
  year={2018}
}

@article{nag2018,
  title={Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience. },
  author={Vaishnavh Nagarajan and J. Zico Kolter},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{minshuo2018,
  title={On Generalization Bounds for a Family of Recurrent Neural Networks. },
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{neyshabur2018towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}


@article{li2018tighter,
  title={On Tighter Generalization Bound for Deep Neural Networks: {CNNs}, {ResNets}, and Beyond},
  author={Li, Xingguo and Lu, Junwei and Wang, Zhaoran and Haupt, Jarvis and Zhao, Tuo},
  journal={arXiv preprint arXiv:1806.05159},
  year={2018}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@article{mou2017generalization,
  title={Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal={arXiv preprint arXiv:1707.05947},
  year={2017}
}



@article{chen2018stability,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{zhou2018generalization,
  title={Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization},
  author={Zhou, Yi and Liang, Yingbin and Zhang, Huishuai},
  journal={arXiv preprint arXiv:1802.06903},
  year={2018}
}



@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}




@article{haeffele2015global,
  title={Global optimality in tensor factorization, deep learning, and beyond},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  journal={arXiv preprint arXiv:1506.07540},
  year={2015}
}





@article{soudry2017exponentially,
  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},
  author={Soudry, Daniel and Hoffer, Elad},
  journal={arXiv preprint arXiv:1702.05777},
  year={2017}
}





@inproceedings{lin2018resnet,
  title={{ResNet} with one-neuron hidden layers is a Universal Approximator},
  author={Lin, Hongzhou and Jegelka, Stefanie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6172--6181},
  year={2018}
}





@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}



@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: {Gaussian} process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}



@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}


@article{song2018mean,
  title={A mean field view of the landscape of two-layers neural networks},
  author={Song, Mei and Montanari, A and Nguyen, P},
  journal={PNAS},
  volume={115},
  pages={E7665--E7671},
  year={2018}
}





@inproceedings{hardt2015train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016}
}



@inproceedings{gao2019learning,
  title={Learning One-hidden-layer Neural Networks under General Input Distributions},
  author={Gao, Weihao and Makkuva, Ashok and Oh, Sewoong and Viswanath, Pramod},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1950--1959},
  year={2019}
}


















@string{JASA = {Journal of the American Statistical Association}}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society, Series B}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{ICML  = {International Conference on Machine Learning}}
@string{TIT = {IEEE Transactions on Information Theory}}


@inproceedings{harvey2017nearly,
  title={Nearly-tight VC-dimension bounds for piecewise linear neural networks},
  author={Harvey, Nick and Liaw, Christopher and Mehrabian, Abbas},
  booktitle={Conference on Learning Theory},
  pages={1064--1068},
  year={2017}
}

@inproceedings{bartlett1999almost,
  title={Almost linear VC dimension bounds for piecewise polynomial networks},
  author={Bartlett, Peter L and Maiorov, Vitaly and Meir, Ron},
  booktitle={Advances in Neural Information Processing Systems},
  pages={190--196},
  year={1999}
}

@inproceedings{du2017convolutional,
	title={When is a Convolutional Filter Easy To Learn?},
	author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
	booktitle={International Conference on Learning Representations},
	year={2018}
}




@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}


@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@inproceedings{ren2015faster,
  title={Faster {R-CNN}: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={91--99},
  year={2015}
}
@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{baum1990polynomial,
  title={A polynomial time algorithm that learns two hidden unit nets},
  author={Baum, Eric B},
  journal={Neural Computation},
  volume={2},
  number={4},
  pages={510--522},
  year={1990},
  publisher={MIT Press}
}
@inproceedings{zhang2016convexified,
  title={Convexified Convolutional Neural Networks},
  author={Zhang, Yuchen and Liang, Percy and Wainwright, Martin J},
  booktitle={International Conference on Machine Learning},
  pages={4044--4053},
  year={2017}
}


@article{nguyen2017bloss,
  title={The loss surface and expressivity of deep convolutional neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  journal={arXiv preprint arXiv:1710.10928},
  year={2017}
}




@article{tian2016symmetry,
  title={Symmetry-breaking convergence analysis of certain two-layered neural networks with ReLU nonlinearity},
  author={Tian, Yuandong},
  year={2016}
}


@incollection{klivans2009baum,
  title={Baum’s algorithm learns intersections of halfspaces with respect to log-concave distributions},
  author={Klivans, Adam R and Long, Philip M and Tang, Alex K},
  booktitle={Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  pages={588--600},
  year={2009},
  publisher={Springer}
}


@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}


@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}

@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}

@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}

@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}
@inproceedings{yun2017global,
  title={Global optimality conditions for deep neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}


@article{soltanolkotabi2017theoretical,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={2},
  pages={742--769},
  year={2018},
  publisher={IEEE}
}


@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}

@inproceedings{hardt2016identity,
  title={Identity matters in deep learning},
  author={Hardt, Moritz and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}

@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}


@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}
@inproceedings{ge2017learning,
  title={Learning One-hidden-layer Neural Networks with Landscape Design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}
\nshortmid




@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}



@inproceedings{yi2015regularized,
  title={Regularized em algorithms: A unified framework and statistical guarantees},
  author={Yi, Xinyang and Caramanis, Constantine},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1567--1575},
  year={2015}
}




@misc{hoeffding1940masstabinvariante,
  title={Masstabinvariante Korrelationtheorie, Schriften des Mathematis chen Instituts und des Instituts f{\"u}r Angewandte Mathematik der Universit{\"a}t Berlin 5, 181\# 233.(Translated in Fisher, NI and PK Sen (1994). The Collected Works of Wassily Hoeffding, New York},
  author={Hoeffding, W},
  year={1940},
  publisher={Springer-Verlag}
}


@article{cuadras2002covariance,
  title={On the covariance between functions},
  author={Cuadras, Carles M},
  journal={Journal of Multivariate Analysis},
  volume={81},
  number={1},
  pages={19--27},
  year={2002},
  publisher={Elsevier}
}


@incollection{sen1994impact,
  title={The impact of Wassily Hoeffding’s research on nonparametrics},
  author={Sen, Pranab K},
  booktitle={The Collected Works of Wassily Hoeffding},
  pages={29--55},
  year={1994},
  publisher={Springer}
}



@article{gordon1985some,
  title={Some inequalities for {Gaussian} processes and applications},
  author={Gordon, Yehoram},
  journal={Israel Journal of Mathematics},
  volume={50},
  number={4},
  pages={265--289},
  year={1985},
  publisher={Springer}
}


@inproceedings{goel2018learning,
  title={Learning One Convolutional Layer with Overlapping Patches},
  author={Goel, Surbhi and Klivans, Adam and Meka, Raghu},
  booktitle={International Conference on Machine Learning},
  pages={1778--1786},
  year={2018}
}


@article{du2018improved,
  title={Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps},
  author={Du, Simon S and Goel, Surbhi},
  journal={arXiv preprint arXiv:1805.07798},
  year={2018}
}

@inproceedings{du2018many,
  title={How Many Samples are Needed to Learn a Convolutional Neural Network?},
  author={Du, Simon S and Wang, Yining and Zhai, Xiyu and Balakrishnan, Sivaraman and Salakhutdinov, Ruslan and Singh, Aarti},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}



@book{talagrand2014upper,
  title={Upper and lower bounds for stochastic processes: modern methods and classical problems},
  author={Talagrand, Michel},
  volume={60},
  year={2014},
  publisher={Springer Science \& Business Media}
}





@inproceedings{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}
@inproceedings{cotter2011better,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  booktitle={Advances in neural information processing systems},
  pages={1647--1655},
  year={2011}
}
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}
@inproceedings{brutzkus2017sgd,
  title={SGD learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  booktitle = {International Conference on Learning Representations},
  year={2018}
}

@inproceedings{nacson2019stochastic,
  title={Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate},
  author={Nacson, Mor Shpigel and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3051--3059},
  year={2019}
}


@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}



@article{slepian1962one,
  title={The one-sided barrier problem for {Gaussian} noise},
  author={Slepian, David},
  journal={Bell Labs Technical Journal},
  volume={41},
  number={2},
  pages={463--501},
  year={1962},
  publisher={Wiley Online Library}
}




######################## Teacher Network



######################### Deep Linear Network
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{arora2018convergence,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  booktitle={International Conference on Learning Representations},
  year={2019}
}




#####################Optimization Landscape


######################Generalization



@inproceedings{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={597--607},
  year={2017}
}



@inproceedings{zhong2017recovery,
  title={Recovery Guarantees for One-hidden-layer Neural Networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  booktitle={International Conference on Machine Learning},
  pages={4140--4149},
  year={2017}
}

@article{tian2017analytical,
  title={An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}


@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong and others},
  journal={Electronic Communications in Probability},
  volume={17},
  year={2012},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}


@article{dally2015high,
  title={High-performance hardware for machine learning},
  author={Dally, William},
  journal={NIPS Tutorial},
  year={2015}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@inproceedings{devlin2014fast,
  title={Fast and Robust Neural Network Joint Models for Statistical Machine Translation.},
  author={Devlin, Jacob and Zbib, Rabih and Huang, Zhongqiang and Lamar, Thomas and Schwartz, Richard M and Makhoul, John},
  booktitle={ACL (1)},
  pages={1370--1380},
  year={2014}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}

@inproceedings{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  booktitle={Advances in neural information processing systems},
  pages={6231--6239},
  year={2017}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}
@inproceedings{cohen2016convolutional,
  title={Convolutional rectifier networks as generalized tensor decompositions},
  author={Cohen, Nadav and Shashua, Amnon},
  booktitle={International Conference on Machine Learning},
  pages={955--963},
  year={2016}
}
@inproceedings{telgarsky2016benefits,
  title={benefits of depth in neural networks},
  author={Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1517--1539},
  year={2016}
}
@article{raghu2016expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1606.05336},
  year={2016}
}
@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithreyi and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3360--3368},
  year={2016}
}
@inproceedings{montufar2014number,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{vsima2006training,
  title={Training a single sigmoidal neuron is hard},
  author={{\v{S}}{\'\i}ma, Ji{\v{r}}{\'\i}},
  journal={Training},
  volume={14},
  number={11},
  year={2006},
  publisher={MIT Press}
}
@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}
@article{shamir2016distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1135--1163},
  year={2018},
  publisher={JMLR. org}
}
@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={International Conference on Machine Learning},
  pages={3067--3075},
  year={2017}
}
@article{shalev2017weight,
  title={Weight Sharing is Crucial to Succesful Optimization},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1706.00687},
  year={2017}
}

@article{goel2016reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  journal={arXiv preprint arXiv:1611.10258},
  year={2016}
}
@article{zhang2015learning,
  title={Learning halfspaces and neural networks with random initialization},
  author={Zhang, Yuchen and Lee, Jason D and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1511.07948},
  year={2015}
}
@article{sedghi2014provable,
  title={Provable methods for training neural networks with sparse connectivity},
  author={Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1412.2693},
  year={2014}
}
@article{janzamin2015beating,
  title={Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods},
  author={Janzamin, Majid and Sedghi, Hanie and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1506.08473},
  year={2015}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on Learning Theory},
  pages={797--842},
  year={2015}
}
@inproceedings{jin2016provable,
  title={Provable efficient online matrix completion via non-convex stochastic gradient descent},
  author={Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4520--4528},
  year={2016}
}
@article{jin2017escape,
  title={How to Escape Saddle Points Efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={arXiv preprint arXiv:1703.00887},
  year={2017}
}

@article{lu2017depth,
  title={Depth Creates No Bad Local Minima},
  author={Lu, Haihao and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:1702.08580},
  year={2017}
}

@article{taghvaei2017regularization,
  title={How regularization affects the critical points in linear networks},
  author={Taghvaei, Amirhossein and Kim, Jin W and Mehta, Prashant G},
  journal={arXiv preprint arXiv:1709.09625},
  year={2017}
}

@article{feizi2017porcupine,
  title={Porcupine Neural Networks:(Almost) All Local Optima are Global},
  author={Feizi, Soheil and Javadi, Hamid and Zhang, Jesse and Tse, David},
  journal={arXiv preprint arXiv:1710.02196},
  year={2017}
}



@article{carmon2017convex,
  title={" Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  journal={arXiv preprint arXiv:1705.02766},
  year={2017}
}


@inproceedings{safran2016quality,
  title={On the quality of the initial basin in overspecified neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={774--782},
  year={2016}
}


@article{arora2016provable,
  title={Provable learning of Noisy-or Networks},
  author={Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Risteski, Andrej},
  journal={arXiv preprint arXiv:1612.08795},
  year={2016}
}
@inproceedings{xie2017diverse,
  title={Diverse Neural Network Learns True Target Functions},
  author={Xie, Bo and Liang, Yingyu and Song, Le},
  booktitle={Artificial Intelligence and Statistics},
  pages={1216--1224},
  year={2017}
}
@inproceedings{zhang2017learnability,
  title={On the Learnability of Fully-Connected Neural Networks},
  author={Zhang, Yuchen and Lee, Jason and Wainwright, Martin and Jordan, Michael},
  booktitle={Artificial Intelligence and Statistics},
  pages={83--91},
  year={2017}
}
@inproceedings{jain2015fast,
  title={Fast exact matrix completion with finite samples},
  author={Jain, Prateek and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1007--1034},
  year={2015}
}
@inproceedings{pan2016expressiveness,
  title={Expressiveness of rectifier networks},
  author={Pan, Xingyuan and Srikumar, Vivek},
  booktitle={International Conference on Machine Learning},
  pages={2427--2435},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}
@inproceedings{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  booktitle={The Annals of Statistics},
  year={2018}
}
@inproceedings{du2017gradient,
  title={Gradient Descent Learns One-hidden-layer {CNN}: Don’t be Afraid of Spurious Local Minima},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong and Singh, Aarti and Poczos, Barnabas},
  booktitle={International Conference on Machine Learning},
  pages={1338--1347},
  year={2018}
}
@inproceedings{auer1996exponentially,
  title={Exponentially many local minima for single neurons},
  author={Auer, Peter and Herbster, Mark and Warmuth, Manfred K},
  booktitle={Advances in neural information processing systems},
  pages={316--322},
  year={1996}
}

@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009},
  organization={Citeseer}
}

@inproceedings{fu2018local,
  title={Local Geometry of Cross Entropy Loss in Learning One-Hidden-Layer Neural Networks},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)},
  pages={1972--1976},
  year={2019},
  organization={IEEE}
}

@inproceedings{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={927--935},
  year={2011}
}

@inproceedings{jain2013low,
  title={Low-rank matrix completion using alternating minimization},
  author={Jain, Prateek and Netrapalli, Praneeth and Sanghavi, Sujay},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={665--674},
  year={2013},
  organization={ACM}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@inproceedings{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={855--863},
  year={2014}
}




@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}



@inproceedings{liang2016deep,
  title={Why deep neural networks for function approximation?},
  author={Liang, Shiyu and Srikant, R},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{yarotsky2017error,
  title={Error bounds for approximations with deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}


@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{blum1989training,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

#########gaussian##############





@article{zhong2017learning,
  title={Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels},
  author={Zhong, Kai and Song, Zhao and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1711.03440},
  year={2017}
}


#############independent assumption 


@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}

@inproceedings{du2018power,
  title={On the Power of Over-parametrization in Neural Networks with Quadratic Activation},
  author={Du, Simon S and Lee, Jason D},
  booktitle={International Conference on Machine Learning},
  pages={1328--1337},
  year={2018}
}


@inproceedings{allen2018convergence,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019}
}


@inproceedings{du2018gradientdeep,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}


@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations},
  author={Bartlett, Peter and Helmbold, Dave and Long, Phil},
  booktitle={International Conference on Machine Learning},
  pages={520--529},
  year={2018}
}

@inproceedings{langford2002not,
  title={(Not) bounding the true error},
  author={Langford, John and Caruana, Rich},
  booktitle={Advances in Neural Information Processing Systems},
  pages={809--816},
  year={2002}
}


###############generalization#################
@inproceedings{neyshabur2017pac,
  title={A pac-bayesian approach to spectrally-normalized margin bounds for neural networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  booktitle={International Conference on Learning Representation},
  year={2018}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={1376--1401},
  year={2015}
}


@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@inproceedings{golowich2017size,
  title={Size-Independent Sample Complexity of Neural Networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018}
}

@inproceedings{arora2018stronger,
  title={Stronger Generalization Bounds for Deep Nets via a Compression Approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018}
}

@article{arora2016understanding,
  title={Understanding deep neural networks with rectified linear units},
  author={Arora, Raman and Basu, Amitabh and Mianjy, Poorya and Mukherjee, Anirbit},
  journal={arXiv preprint arXiv:1611.01491},
  year={2016}
}


###########more citation#################


@article{hanin2017universal,
  title={Universal function approximation by deep neural nets with bounded width and relu activations},
  author={Hanin, Boris},
  journal={arXiv preprint arXiv:1708.02691},
  year={2017}
}

@article{hanin2017approximating,
  title={Approximating Continuous Functions by {ReLU} Nets of Minimal Width},
  author={Hanin, Boris and Sellke, Mark},
  journal={arXiv preprint arXiv:1710.11278},
  year={2017}
}


@inproceedings{allen2018rnn,
  title={On the Convergence Rate of Training Recurrent Neural Networks},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{zhang2018learning,
  title={Learning One-hidden-layer {ReLU} Networks via Gradient Descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1524--1534},
  year={2019}
}

@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}


@inproceedings{allen2018learning,
  title={Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{nacson2018convergence,
  title={Convergence of Gradient Descent on Separable Data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019}
}



@article{nag2018,
  title={Deterministic PAC-Bayesian generalization bounds for deep networks via generalizing noise-resilience. },
  author={Vaishnavh Nagarajan and J. Zico Kolter},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}

@article{minshuo2018,
  title={On Generalization Bounds for a Family of Recurrent Neural Networks. },
  author={Chen, Minshuo and Li, Xingguo and Zhao, Tuo},
  journal={Integration of Deep Learning Theories, NeurIPS 2018},
  year={2018}
}



@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={2},
  number={Mar},
  pages={499--526},
  year={2002}
}

@article{mou2017generalization,
  title={Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal={arXiv preprint arXiv:1707.05947},
  year={2017}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  booktitle={Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48},
  pages={1225--1234},
  year={2016},
  organization={JMLR. org}
}

@article{chen2018stability,
  title={Stability and Convergence Trade-off of Iterative Optimization Algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{zhou2018generalization,
  title={Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization},
  author={Zhou, Yi and Liang, Yingbin and Zhang, Huishuai},
  journal={arXiv preprint arXiv:1802.06903},
  year={2018}
}

@inproceedings{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Advances in neural information processing systems},
  pages={3036--3046},
  year={2018}
}


@inproceedings{arora2018optimization,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018}
}


@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}



@inproceedings{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2017}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9461--9471},
  year={2018}
}


@inproceedings{gunasekar2018characterizing,
  title={Characterizing Implicit Bias in Terms of Optimization Geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1827--1836},
  year={2018}
}


@inproceedings{liang2018just,
  title={Just Interpolate: Kernel" Ridgeless" Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  booktitle={The Annals of Statistics},
  year={2019}
}



@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018}
}

@inproceedings{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6151--6159},
  year={2017}
}



@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@inproceedings{soltanolkotabi2017learning,
  title={Learning {ReLUs} via gradient descent},
  author={Soltanolkotabi, Mahdi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2007--2017},
  year={2017}
}


@inproceedings{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2017}
}



@article{wei2018regularization,
  title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{soudry2017exponentially,
  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},
  author={Soudry, Daniel and Hoffer, Elad},
  journal={arXiv preprint arXiv:1702.05777},
  year={2017}
}

@article{zhou2017critical,
  title={Critical Points of Neural Networks: Analytical Forms and Landscape Properties},
  author={Zhou, Yi and Liang, Yingbin},
  journal={arXiv preprint arXiv:1710.11205},
  year={2017}
}

@inproceedings{yun2019small,
  title={Small nonlinearities in activation functions create bad local minima in neural networks},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representation},
  year={2019}
}

@article{sirignano2019mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  year={2019},
  publisher={Elsevier}
}

@article{rotskoff2018neural,
  title={Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@inproceedings{chizat2018note,
  title={On Lazy Training in Differentiable Programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}






@inproceedings{rahimi2009weighted,
  title={Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1313--1320},
  year={2009}
}

@inproceedings{mendelson2014learning,
  title={Learning without concentration},
  author={Mendelson, Shahar},
  booktitle={Conference on Learning Theory},
  pages={25--39},
  year={2014}
}





@inproceedings{maurer2016vector,
  title={A vector-contraction inequality for rademacher complexities},
  author={Maurer, Andreas},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={3--17},
  year={2016},
  organization={Springer}
}



@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}




@inproceedings{yehudai2019power,
  title={On the Power and Limitations of Random Features for Understanding Neural Networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@article{e2019comparative,
  title={A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1904.04326},
  year={2019}
}



@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}


@article{oymak2019towards,
  title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1902.04674},
  year={2019}
}

@inproceedings{neyshabur2018role,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@article{long2019size,
  title={Size-free generalization bounds for convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}

@inproceedings{jain2019making,
  title={Making the Last Iterate of SGD Information Theoretically Optimal},
  author={Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  year={2019}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Taipei, Taiwan}
}

@inproceedings{zou2019improved,
  title={An Improved Analysis of Training Over-parameterized Deep Neural Networks},
  author={Zou, Difan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{cao2019generalizationsgd,
  title={Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@article{fang2019convexformulation,
  title={Convex Formulation of Overparameterized Deep Neural Networks},
  author={Fang, Cong and Gu, Yihong and Zhang, Weizhong and Zhang, Tong},
  journal={arXiv preprint arXiv:1911.07626},
  year={2019}
}


@inproceedings{nguyen2019connected,
  title={On Connected Sublevel Sets in Deep Learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019}
}



@inproceedings{cao2019tight,
  title={Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10611--10621},
  year={2019}
}




@inproceedings{frei2019algorithm,
  title={Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14769--14779},
  year={2019}
}


@inproceedings{nguyen2019connected,
  title={On Connected Sublevel Sets in Deep Learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019}
}


@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}



@inproceedings{frei2019algorithm,
  title={Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14769--14779},
  year={2019}
}



@article{sirignano2019mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  year={2019},
  publisher={Elsevier}
}





@inproceedings{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@inproceedings{basri2019convergence,
  title={The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies},
  author={Basri, Ronen and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}




@inproceedings{nakkiran2019sgd,
  title={SGD on Neural Networks Learns Functions of Increasing Complexity},
  author={Nakkiran, Preetum and Kaplun, Gal and Kalimeris, Dimitris and Yang, Tristan and Edelman, Benjamin L and Zhang, Fred and Barak, Boaz},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}


@inproceedings{rahaman2019spectral,
  title={On the Spectral Bias of Neural Networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019}
}



@inproceedings{vempala2018gradient,
  title={Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds},
  author={Vempala, Santosh and Wilmes, John},
  booktitle={Conference on Learning Theory},
  year={2019}
}



@inproceedings{su2019learning,
  title={On Learning Over-parameterized Neural Networks: A Functional Approximation Prospective},
  author={Su, Lili and Yang, Pengkun},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@Article{zhang2019resnet,
author="Zhang, Huishuai
and Yu, Da
and Yi, Mingyang
and Chen, Wei
and Liu, Tie-Yan",
title="Convergence Theory of Learning Over-parameterized ResNet:
A Full Characterization",
journal="arXiv preprint arXiv:1903.07120",
year="2019",
month="Jul",
day="15"
}





@article{cao2019towards,
  title={Towards Understanding the Spectral Bias of Deep Learning},
  author={Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.01198},
  year={2019}
}



@article{chen2019much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1911.12360},
  year={2019}
}



@article{allen2020backward,
  title={Backward Feature Correction: How Deep Learning Performs Deep Learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}



@article{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}


@article{nacson2019lexicographic,
  title={Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models},
  author={Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason D and Srebro, Nathan and Soudry, Daniel},
  journal={arXiv preprint arXiv:1905.07325},
  year={2019}
}



@article{meir2003generalization,
  title={Generalization error bounds for Bayesian mixture algorithms},
  author={Meir, Ron and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={4},
  number={Oct},
  pages={839--860},
  year={2003}
}


@article{donsker1983asymptotic,
  title={Asymptotic evaluation of certain Markov process expectations for large time. IV},
  author={Donsker, Monroe D and Varadhan, SR Srinivasa},
  journal={Communications on Pure and Applied Mathematics},
  volume={36},
  number={2},
  pages={183--212},
  year={1983},
  publisher={Wiley Online Library}
}





@inproceedings{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11669--11680},
  year={2019}
}




@book{bakry2013analysis,
  title={Analysis and geometry of Markov diffusion operators},
  author={Bakry, Dominique and Gentil, Ivan and Ledoux, Michel},
  volume={348},
  year={2013},
  publisher={Springer Science \& Business Media}
}




@article{tzen2020mean,
  title={A mean-field theory of lazy training in two-layer neural nets: entropic regularization and controlled McKean-Vlasov dynamics},
  author={Tzen, Belinda and Raginsky, Maxim},
  journal={arXiv preprint arXiv:2002.01987},
  year={2020}
}



@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}



@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}





@article{wang2020benign,
  title={Benign Overfitting in Binary Classification of Gaussian Mixtures},
  author={Wang, Ke and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2011.09148},
  year={2020}
}


@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}



@article{muthukumar2020classification,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  journal={arXiv preprint arXiv:2005.08054},
  year={2020}
}



@article{chatterji2020finite,
  title={Finite-sample analysis of interpolating linear classifiers in the overparameterized regime},
  author={Chatterji, Niladri S and Long, Philip M},
  journal={arXiv preprint arXiv:2004.12019},
  year={2020}
}




@article{muthukumar2020harmless,
  title={Harmless interpolation of noisy data in regression},
  author={Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={67--83},
  year={2020},
  publisher={IEEE}
}


@article{tsigler2020benign,
  title={Benign overfitting in ridge regression},
  author={Tsigler, Alexander and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2009.14286},
  year={2020}
}




@article{wu2020optimal,
  title={On the Optimal Weighted $\ell_2$ Regularization in Overparameterized Linear Regression},
  author={Wu, Denny and Xu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


@inproceedings{liao2020random,
  title={A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent},
  author={Liao, Zhenyu and Couillet, Romain and Mahoney, Michael},
  booktitle={34th Conference on Neural Information Processing Systems (NeurIPS 2020)},
  year={2020}
}


@article{montanari2020interpolation,
  title={The interpolation phase transition in neural networks: Memorization and generalization under lazy training},
  author={Montanari, Andrea and Zhong, Yiqiao},
  journal={arXiv preprint arXiv:2007.12826},
  year={2020}
}


@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}



@article{hsu2020proliferation,
  title={On the proliferation of support vectors in high dimensions},
  author={Hsu, Daniel and Muthukumar, Vidya and Xu, Ji},
  journal={arXiv preprint arXiv:2009.10670},
  year={2020}
}



@article{donoho2008higher,
  title={Higher criticism thresholding: Optimal feature selection when useful features are rare and weak},
  author={Donoho, David and Jin, Jiashun},
  journal={Proceedings of the National Academy of Sciences},
  volume={105},
  number={39},
  pages={14790--14795},
  year={2008},
  publisher={National Acad Sciences}
}



@article{jin2009impossibility,
  title={Impossibility of successful classification when useful features are rare and weak},
  author={Jin, Jiashun},
  journal={Proceedings of the National Academy of Sciences},
  volume={106},
  number={22},
  pages={8859--8864},
  year={2009},
  publisher={National Acad Sciences}
}



@inproceedings{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle={International Conference on Machine Learning},
  pages={541--549},
  year={2018},
  organization={PMLR}
}





@article{cote2012chernoff,
  title={A Chernoff-type lower bound for the Gaussian Q-function},
  author={C{\^o}t{\'e}, Fran{\c{c}}ois D and Psaromiligkos, Ioannis N and Gross, Warren J},
  journal={arXiv preprint arXiv:1202.6483},
  year={2012}
}




@article{jelassi2021adam,
  title={Adam is no better than normalized SGD: Dissecting how adaptivity improves GAN performance},
  author={Jelassi, Samy and Mensch, Arthur and Gidel, Gauthier and Li, Yuanzhi},
  year={2021}
}





@inproceedings{arora2019theoretical,
  title={Theoretical analysis of auto rate-tuning by batch normalization},
  author={Arora, Sanjeev and Lyu, Kaifeng and Li, Zhiyuan},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}


@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{liang2006end,
  title={An end-to-end discriminative approach to machine translation},
  author={Liang, Percy and Bouchard-C{\^o}t{\'e}, Alexandre and Klein, Dan and Taskar, Ben},
  booktitle={Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  pages={761--768},
  year={2006}
}

@article{quattoni2004conditional,
  title={Conditional random fields for object recognition},
  author={Quattoni, Ariadna and Collins, Michael and Darrell, Trevor},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}

@inproceedings{wang2009max,
  title={Max-margin hidden conditional random fields for human action recognition},
  author={Wang, Yang and Mori, Greg},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={872--879},
  year={2009},
  organization={IEEE}
}



@inproceedings{anandkumar2012method,
  title={A method of moments for mixture models and hidden Markov models},
  author={Anandkumar, Animashree and Hsu, Daniel and Kakade, Sham M},
  booktitle={Conference on Learning Theory},
  pages={33--1},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{hsu2012identifiability,
  title={Identifiability and unmixing of latent parse trees},
  author={Hsu, Daniel J and Kakade, Sham M and Liang, Percy S},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

@inproceedings{chaganty2013spectral,
  title={Spectral experts for estimating mixtures of linear regressions},
  author={Chaganty, Arun Tejasvi and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={1040--1048},
  year={2013},
  organization={PMLR}
}

@article{anandkumar2014tensor,
  title={Tensor decompositions for learning latent variable models},
  author={Anandkumar, Animashree and Ge, Rong and Hsu, Daniel and Kakade, Sham M and Telgarsky, Matus},
  journal={Journal of machine learning research},
  volume={15},
  pages={2773--2832},
  year={2014},
  publisher={Journal of Machine Learning Research}
}

@article{de1989mixtures,
  title={Mixtures of linear regressions},
  author={De Veaux, Richard D},
  journal={Computational Statistics \& Data Analysis},
  volume={8},
  number={3},
  pages={227--245},
  year={1989},
  publisher={Elsevier}
}

@article{faria2010fitting,
  title={Fitting mixtures of linear regressions},
  author={Faria, Susana and Soromenho, Gilda},
  journal={Journal of Statistical Computation and Simulation},
  volume={80},
  number={2},
  pages={201--225},
  year={2010},
  publisher={Taylor \& Francis}
}

@article{khalili2007variable,
  title={Variable selection in finite mixture of regression models},
  author={Khalili, Abbas and Chen, Jiahua},
  journal={Journal of the american Statistical association},
  volume={102},
  number={479},
  pages={1025--1038},
  year={2007},
  publisher={Taylor \& Francis}
}

@inproceedings{yi2014alternating,
  title={Alternating minimization for mixed linear regression},
  author={Yi, Xinyang and Caramanis, Constantine and Sanghavi, Sujay},
  booktitle={International Conference on Machine Learning},
  pages={613--621},
  year={2014},
  organization={PMLR}
}

@article{balakrishnan2017statistical,
  title={Statistical guarantees for the EM algorithm: From population to sample-based analysis},
  author={Balakrishnan, Sivaraman and Wainwright, Martin J and Yu, Bin},
  journal={The Annals of Statistics},
  volume={45},
  number={1},
  pages={77--120},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4510--4520},
  year={2018}
}

@article{wang2015high,
  title={High dimensional em algorithm: Statistical optimization and asymptotic normality},
  author={Wang, Zhaoran and Gu, Quanquan and Ning, Yang and Liu, Han},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{cao2022benign,
  title={Benign Overfitting in Two-layer Convolutional Neural Networks},
  author={Cao, Yuan and Chen, Zixiang and Belkin, Mikhail and Gu, Quanquan},
  journal={arXiv preprint arXiv:2202.06526},
  year={2022}
}

@inproceedings{li2020learning,
  title={Learning over-parametrized two-layer neural networks beyond ntk},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Conference on learning theory},
  pages={2613--2682},
  year={2020},
  organization={PMLR}
}

@article{zou2021understanding,
  title={Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization},
  author={Zou, Difan and Cao, Yuan and Li, Yuanzhi and Gu, Quanquan},
  journal={arXiv preprint arXiv:2108.11371},
  year={2021}
}



@inproceedings{wen2021toward,
  title={Toward understanding the feature learning process of self-supervised contrastive learning},
  author={Wen, Zixin and Li, Yuanzhi},
  booktitle={International Conference on Machine Learning},
  pages={11112--11122},
  year={2021},
  organization={PMLR}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@article{du2021glam,
  title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  journal={arXiv preprint arXiv:2112.06905},
  year={2021}
}

@article{chen2022towards,
  title={Towards Understanding Mixture of Experts in Deep Learning},
  author={Chen, Zixiang and Deng, Yihe and Wu, Yue and Gu, Quanquan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2208.02813},
  year={2022}
}

@article{dua2021tricks,
  title={Tricks for Training Sparse Translation Models},
  author={Dua, Dheeru and Bhosale, Shruti and Goswami, Vedanuj and Cross, James and Lewis, Mike and Fan, Angela},
  journal={arXiv preprint arXiv:2110.08246},
  year={2021}
}



@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@inproceedings{lewis2021base,
  title={Base layers: Simplifying training of large, sparse models},
  author={Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle={International Conference on Machine Learning},
  pages={6265--6274},
  year={2021},
  organization={PMLR}
}

@inproceedings{saunshi2019theoretical,
  title={A theoretical analysis of contrastive unsupervised representation learning},
  author={Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
  booktitle={International Conference on Machine Learning},
  pages={5628--5637},
  year={2019},
  organization={PMLR}
}

@article{lee2021predicting,
  title={Predicting what you already know helps: Provable self-supervised learning},
  author={Lee, Jason D and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={309--323},
  year={2021}
}

@article{fukumizu2009kernel,
  title={Kernel dimension reduction in regression},
  author={Fukumizu, Kenji and Bach, Francis R and Jordan, Michael I},
  year={2009}
}

@article{fukumizu2004dimensionality,
  title={Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces},
  author={Fukumizu, Kenji and Bach, Francis R and Jordan, Michael I},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Jan},
  pages={73--99},
  year={2004}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{pham2021combined,
  title={Combined scaling for zero-shot transfer learning},
  author={Pham, Hieu and Dai, Zihang and Ghiasi, Golnaz and Kawaguchi, Kenji and Liu, Hanxiao and Yu, Adams Wei and Yu, Jiahui and Chen, Yi-Ting and Luong, Minh-Thang and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2111.10050},
  year={2021}
}

@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2556--2565},
  year={2018}
}

@article{chen2015microsoft,
  title={Microsoft coco captions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint arXiv:1504.00325},
  year={2015}
}

@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "38--45"
}

@article{goel2022cyclip,
  title={Cyclip: Cyclic contrastive language-image pretraining},
  author={Goel, Shashank and Bansal, Hritik and Bhatia, Sumit and Rossi, Ryan and Vinay, Vishwa and Grover, Aditya},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={6704--6719},
  year={2022}
}

@inproceedings{li2022supervision,
      title={Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image  Pre-training Paradigm},
      author={Yangguang Li and Feng Liang and Lichen Zhao and Yufeng Cui and Wanli Ouyang and Jing Shao and Fengwei Yu and Junjie Yan},
      booktitle={International Conference on Learning Representations},
      year={2022},
      url={https://openreview.net/forum?id=zq1iJkNk3uN}
}

@inproceedings{zhang2022contrastive,
  title={Contrastive learning of medical visual representations from paired images and text},
  author={Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D and Langlotz, Curtis P},
  booktitle={Machine Learning for Healthcare Conference},
  pages={2--25},
  year={2022},
  organization={PMLR}
}

@inproceedings{desai2021virtex,
  title={Virtex: Learning visual representations from textual annotations},
  author={Desai, Karan and Johnson, Justin},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11162--11173},
  year={2021}
}

@inproceedings{gomez2017self,
  title={Self-supervised learning of visual features through embedding images into text topic spaces},
  author={Gomez, Lluis and Patel, Yash and Rusinol, Mar{\c{c}}al and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the ieee conference on computer vision and pattern recognition},
  pages={4230--4239},
  year={2017}
}

@article{thomee2016yfcc100m,
  title={YFCC100M: The new data in multimedia research},
  author={Thomee, Bart and Shamma, David A and Friedland, Gerald and Elizalde, Benjamin and Ni, Karl and Poland, Douglas and Borth, Damian and Li, Li-Jia},
  journal={Communications of the ACM},
  volume={59},
  number={2},
  pages={64--73},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{sariyildiz2020learning,
  title={Learning visual representations with caption annotations},
  author={Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part VIII 16},
  pages={153--170},
  year={2020},
  organization={Springer}
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@article{gao2022pyramidclip,
  title={Pyramidclip: Hierarchical feature alignment for vision-language model pretraining},
  author={Gao, Yuting and Liu, Jinfeng and Xu, Zihan and Zhang, Jun and Li, Ke and Ji, Rongrong and Shen, Chunhua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35959--35970},
  year={2022}
}

@article{saito2022prefix,
  title={Prefix conditioning unifies language and label supervision},
  author={Saito, Kuniaki and Sohn, Kihyuk and Zhang, Xiang and Li, Chun-Liang and Lee, Chen-Yu and Saenko, Kate and Pfister, Tomas},
  journal={arXiv preprint arXiv:2206.01125},
  year={2022}
}

@article{zadeh2020foundations,
  title={Foundations of multimodal co-learning},
  author={Zadeh, Amir and Liang, Paul Pu and Morency, Louis-Philippe},
  journal={Information Fusion},
  volume={64},
  pages={188--193},
  year={2020},
  publisher={Elsevier}
}

@article{huang2021makes,
  title={What makes multi-modal learning better than single (provably)},
  author={Huang, Yu and Du, Chenzhuang and Xue, Zihui and Chen, Xuanyao and Zhao, Hang and Huang, Longbo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10944--10956},
  year={2021}
}

@inproceedings{nakada2023understanding,
  title={Understanding multimodal contrastive learning and incorporating unpaired data},
  author={Nakada, Ryumei and Gulluk, Halil Ibrahim and Deng, Zhun and Ji, Wenlong and Zou, James and Zhang, Linjun},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4348--4380},
  year={2023},
  organization={PMLR}
}

@inproceedings{yao2022filip,
title={{FILIP}: Fine-grained Interactive Language-Image Pre-Training},
author={Lewei Yao and Runhui Huang and Lu Hou and Guansong Lu and Minzhe Niu and Hang Xu and Xiaodan Liang and Zhenguo Li and Xin Jiang and Chunjing Xu},
booktitle={International Conference on Learning Representations},
year={2022},
}


@inproceedings{mu2022slip,
  title={Slip: Self-supervision meets language-image pre-training},
  author={Mu, Norman and Kirillov, Alexander and Wagner, David and Xie, Saining},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI},
  pages={529--544},
  year={2022},
  organization={Springer}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@inproceedings{zhai2022lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18123--18133},
  year={2022}
}

@article{tsai2020demystifying,
  title={Demystifying self-supervised learning: An information-theoretical framework},
  author={Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:2006.05576},
  year={2020}
}

@article{mitrovic2020representation,
  title={Representation learning via invariant causal mechanisms},
  author={Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  journal={arXiv preprint arXiv:2010.07922},
  year={2020}
}

@article{tian2020understanding,
  title={Understanding self-supervised learning with dual deep networks},
  author={Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  journal={arXiv preprint arXiv:2010.00578},
  year={2020}
}

@article{tosh2021contrastive2,
  title={Contrastive estimation reveals topic posterior information to linear models},
  author={Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={281},
  pages={1--31},
  year={2021}
}
@article{saunshi2022understanding,
  title={Understanding contrastive learning requires incorporating inductive biases},
  author={Saunshi, Nikunj and Ash, Jordan and Goel, Surbhi and Misra, Dipendra and Zhang, Cyril and Arora, Sanjeev and Kakade, Sham and Krishnamurthy, Akshay},
  journal={arXiv preprint arXiv:2202.14037},
  year={2022}
}
@article{haochen2021provable,
  title={Provable guarantees for self-supervised deep learning with spectral contrastive loss},
  author={HaoChen, Jeff Z and Wei, Colin and Gaidon, Adrien and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{tosh2021contrastive,
  title={Contrastive estimation reveals topic posterior information to linear models},
  author={Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={281},
  pages={1--31},
  year={2021}
}

@article{lee2020predicting,
  title={Predicting what you already know helps: Provable self-supervised learning},
  author={Lee, Jason D and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
  journal={arXiv preprint arXiv:2008.01064},
  year={2020}
}

@article{wei2020theoretical,
  title={Theoretical analysis of self-training with deep networks on unlabeled data},
  author={Wei, Colin and Shen, Kendrick and Chen, Yining and Ma, Tengyu},
  journal={arXiv preprint arXiv:2010.03622},
  year={2020}
}

@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={689--696},
  year={2011}
}

@inproceedings{chen2021exploring,
  title={Exploring simple siamese representation learning},
  author={Chen, Xinlei and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={15750--15758},
  year={2021}
}

@software{Shariatnia_Simple_CLIP_2021,
author = {Shariatnia, M. Moein},
doi = {10.5281/zenodo.6845731},
month = {4},
title = {{Simple CLIP}},
version = {1.0.0},
year = {2021}
}

@inproceedings{recht2019imagenet,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International conference on machine learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}

@inproceedings{coates2011analysis,
  title={An analysis of single-layer networks in unsupervised feature learning},
  author={Coates, Adam and Ng, Andrew and Lee, Honglak},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={215--223},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{bossard2014food,
  title={Food-101--mining discriminative components with random forests},
  author={Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13},
  pages={446--461},
  year={2014},
  organization={Springer}
}

@inproceedings{cimpoi2014describing,
  title={Describing textures in the wild},
  author={Cimpoi, Mircea and Maji, Subhransu and Kokkinos, Iasonas and Mohamed, Sammy and Vedaldi, Andrea},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3606--3613},
  year={2014}
}

@inproceedings{nilsback2008automated,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing},
  pages={722--729},
  year={2008},
  organization={IEEE}
}

@inproceedings{parkhi2012cats,
  title={Cats and dogs},
  author={Parkhi, Omkar M and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, CV},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3498--3505},
  year={2012},
  organization={IEEE}
}

@inproceedings{lei2015predicting,
  title={Predicting deep zero-shot convolutional neural networks using textual descriptions},
  author={Lei Ba, Jimmy and Swersky, Kevin and Fidler, Sanja and others},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4247--4255},
  year={2015}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{mao2017least,
  title={Least squares generative adversarial networks},
  author={Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond YK and Wang, Zhen and Paul Smolley, Stephen},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2794--2802},
  year={2017}
}

@article{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mroueh2017fisher,
  title={Fisher gan},
  author={Mroueh, Youssef and Sercu, Tom},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{wu2022tinyvit,
  title={Tinyvit: Fast pretraining distillation for small vision transformers},
  author={Wu, Kan and Zhang, Jinnian and Peng, Houwen and Liu, Mengchen and Xiao, Bin and Fu, Jianlong and Yuan, Lu},
  booktitle={European Conference on Computer Vision},
  pages={68--85},
  year={2022},
  organization={Springer}
}

@article{jolicoeur2018relativistic,
  title={The relativistic discriminator: a key element missing from standard GAN},
  author={Jolicoeur-Martineau, Alexia},
  journal={arXiv preprint arXiv:1807.00734},
  year={2018}
}

@article{luo2023wizardcoder,
  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{yang2023decoding,
  title={Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data},
  author={Yang, Yu and Singh, Aaditya K and Elhoushi, Mostafa and Mahmoud, Anas and Tirumala, Kushal and Gloeckle, Fabian and Rozi{\`e}re, Baptiste and Wu, Carole-Jean and Morcos, Ari S and Ardalani, Newsha},
  journal={arXiv preprint arXiv:2312.02418},
  year={2023}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{yuan2023scaling,
  title={Scaling relationship on learning mathematical reasoning with large language models},
  author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Tan, Chuanqi and Zhou, Chang},
  journal={arXiv preprint arXiv:2308.01825},
  year={2023}
}

@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}

@article{singh2023beyond,
  title={Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models},
  author={Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and Anand, Ankesh and Patil, Piyush and Liu, Peter J and Harrison, James and Lee, Jaehoon and Xu, Kelvin and Parisi, Aaron and others},
  journal={arXiv preprint arXiv:2312.06585},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@article{josifoski2023exploiting,
  title={Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction},
  author={Josifoski, Martin and Sakota, Marija and Peyrard, Maxime and West, Robert},
  journal={arXiv preprint arXiv:2303.04132},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}

@article{deng2023rephrase,
  title={Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves},
  author={Deng, Yihe and Zhang, Weitong and Chen, Zixiang and Gu, Quanquan},
  journal={arXiv preprint arXiv:2311.04205},
  year={2023}
}

@article{prasad2023rephrase,
  title={Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models},
  author={Prasad, Archiki and Stengel-Eskin, Elias and Bansal, Mohit},
  journal={arXiv preprint arXiv:2310.05861},
  year={2023}
}

@misc{li2023textbooks,
      title={Textbooks Are All You Need II: phi-1.5 technical report}, 
      author={Yuanzhi Li and Sébastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},
      year={2023},
      eprint={2309.05463},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2023tinygsm,
  title={TinyGSM: achieving> 80\% on GSM8k with small language models},
  author={Liu, Bingbin and Bubeck, Sebastien and Eldan, Ronen and Kulkarni, Janardhan and Li, Yuanzhi and Nguyen, Anh and Ward, Rachel and Zhang, Yi},
  journal={arXiv preprint arXiv:2312.09241},
  year={2023}
}

@article{burnsweak,
  title={WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  year={2023}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009}
}

@article{soviany2022curriculum,
  title={Curriculum learning: A survey},
  author={Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  journal={International Journal of Computer Vision},
  volume={130},
  number={6},
  pages={1526--1565},
  year={2022},
  publisher={Springer}
}

@inproceedings{valentin2009babysteps,
  title = {Baby Steps: How “Less is More” in Unsupervised Dependency Parsing},
  author = {Valentin I. Spitkovsky and Hiyan Alshawi and Daniel Jurafsky},
  year = {2009},
  booktitle = {NIPS 2009 Workshop on Grammar Induction, Representation of Language and Language Learning}}

@article{kumar2010self,
  title={Self-paced learning for latent variable models},
  author={Kumar, M and Packer, Benjamin and Koller, Daphne},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@inproceedings{lee2011learning,
  title={Learning the easy things first: Self-paced visual category discovery},
  author={Lee, Yong Jae and Grauman, Kristen},
  booktitle={CVPR 2011},
  pages={1721--1728},
  year={2011},
  organization={IEEE}
}

@inproceedings{zhang2015self,
  title={A self-paced multiple-instance learning framework for co-saliency detection},
  author={Zhang, Dingwen and Meng, Deyu and Li, Chao and Jiang, Lu and Zhao, Qian and Han, Junwei},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={594--602},
  year={2015}
}

@article{cirik2016visualizing,
  title={Visualizing and understanding curriculum learning for long short-term memory networks},
  author={Cirik, Volkan and Hovy, Eduard and Morency, Louis-Philippe},
  journal={arXiv preprint arXiv:1611.06204},
  year={2016}
}

@article{zhang2018empirical,
  title={An empirical exploration of curriculum learning for neural machine translation},
  author={Zhang, Xuan and Kumar, Gaurav and Khayrallah, Huda and Murray, Kenton and Gwinnup, Jeremy and Martindale, Marianna J and McNamee, Paul and Duh, Kevin and Carpuat, Marine},
  journal={arXiv preprint arXiv:1811.00739},
  year={2018}
}

@inproceedings{liu2018curriculum,
  title={Curriculum Learning for Natural Answer Generation.},
  author={Liu, Cao and He, Shizhu and Liu, Kang and Zhao, Jun and others},
  booktitle={IJCAI},
  pages={4223--4229},
  year={2018}
}

@inproceedings{liu2021competence,
  title={Competence-based Multimodal Curriculum Learning for Medical Report Generation},
  author={Liu, Fenglin and Ge, Shen and Wu, Xian},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={3001--3012},
  year={2021}
}

@article{wu2022scaling,
  title={Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization},
  author={Wu, Junru and Liang, Yi and Akbari, Hassan and Wang, Zhangyang and Yu, Cong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36161--36173},
  year={2022}
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  note={}
}

@inproceedings{yu2021fine,
  title={Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach},
  author={Yu, Yue and Zuo, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1063--1077},
  year={2021}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@article{zoph2020rethinking,
  title={Rethinking pre-training and self-training},
  author={Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin Dogus and Le, Quoc},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={3833--3845},
  year={2020}
}

@article{joshi2020spanbert,
  title={Spanbert: Improving pre-training by representing and predicting spans},
  author={Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S and Zettlemoyer, Luke and Levy, Omer},
  journal={Transactions of the association for computational linguistics},
  volume={8},
  pages={64--77},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{gulcehre2023reinforced,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998},
  year={2023}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@misc{alignment_handbook2023,
  author = {Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Alexander M. Rush and Thomas Wolf},
  title = {The Alignment Handbook},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@Misc{accelerate,
  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  year =         {2022}
}

@article{samuel1959some,
  title={Some studies in machine learning using the game of checkers},
  author={Samuel, Arthur L},
  journal={IBM Journal of research and development},
  volume={3},
  number={3},
  pages={210--229},
  year={1959},
  publisher={IBM}
}

@article{anthony2017thinking,
  title={Thinking fast and slow with deep learning and tree search},
  author={Anthony, Thomas and Tian, Zheng and Barber, David},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{bansal2018emergent,
  title={Emergent Complexity via Multi-Agent Competition},
  author={Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{laurent2011world,
  title={The world of independent learners is not Markovian},
  author={Laurent, Guillaume J and Matignon, La{\"e}titia and Fort-Piat, Le and others},
  journal={International Journal of Knowledge-based and Intelligent Engineering Systems},
  volume={15},
  number={1},
  pages={55--64},
  year={2011},
  publisher={IOS Press}
}

@article{hernandez2018multiagent,
  title={Is multiagent deep reinforcement learning the answer or the question? A brief survey},
  author={Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
  journal={learning},
  volume={21},
  pages={22},
  year={2018}
}

@article{lanctot2017unified,
  title={A unified game-theoretic approach to multiagent reinforcement learning},
  author={Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and P{\'e}rolat, Julien and Silver, David and Graepel, Thore},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{muller2019generalized,
  title={A generalized training approach for multiagent learning},
  author={Muller, Paul and Omidshafiei, Shayegan and Rowland, Mark and Tuyls, Karl and Perolat, Julien and Liu, Siqi and Hennes, Daniel and Marris, Luke and Lanctot, Marc and Hughes, Edward and others},
  journal={arXiv preprint arXiv:1909.12823},
  year={2019}
}

@inproceedings{arulkumaran2019alphastar,
  title={Alphastar: An evolutionary computation perspective},
  author={Arulkumaran, Kai and Cully, Antoine and Togelius, Julian},
  booktitle={Proceedings of the genetic and evolutionary computation conference companion},
  pages={314--315},
  year={2019}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836}
}

@misc{alphastarblog,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojtek and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  year={2019}
}

@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}

@inproceedings{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}


@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{xie2020self,
  title={Self-training with noisy student improves imagenet classification},
  author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10687--10698},
  year={2020}
}
