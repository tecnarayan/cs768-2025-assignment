\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2010)Agarwal, Dekel, and Xiao]{Agarwal-2010-Optimal}
A.~Agarwal, O.~Dekel, and L.~Xiao.
\newblock Optimal algorithms for online convex optimization with multi-point
  bandit feedback.
\newblock In \emph{COLT}, pages 28--40. PMLR, 2010.

\bibitem[Agarwal et~al.(2013)Agarwal, Foster, Hsu, Kakade, and
  Rakhlin]{Agarwal-2013-Stochastic}
A.~Agarwal, D.~P. Foster, D.~Hsu, S.~M. Kakade, and A.~Rakhlin.
\newblock Stochastic convex optimization with bandit feedback.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (1):\penalty0
  213--240, 2013.

\bibitem[Arjevani et~al.(2020)Arjevani, Carmon, Duchi, Foster, Sekhari, and
  Sridharan]{Arjevani-2020-Second}
Y.~Arjevani, Y.~Carmon, J.~C. Duchi, D.~J. Foster, A.~Sekhari, and
  K.~Sridharan.
\newblock Second-order information in non-convex stochastic optimization: Power
  and limitations.
\newblock In \emph{COLT}, pages 242--299. PMLR, 2020.

\bibitem[Arjevani et~al.(2022)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{Arjevani-2022-Lower}
Y.~Arjevani, Y.~Carmon, J.~C. Duchi, D.~J. Foster, N.~Srebro, and B.~Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{Mathematical Programming}, pages 1--50, 2022.

\bibitem[Attouch et~al.(2013)Attouch, Bolte, and
  Svaiter]{Attouch-2013-Convergence}
H.~Attouch, J.~Bolte, and B.~F. Svaiter.
\newblock Convergence of descent methods for semi-algebraic and tame problems:
  proximal algorithms, forward-backward splitting, and regularized
  {G}auss-{S}eidel methods.
\newblock \emph{Mathematical Programming}, 137\penalty0 (1):\penalty0 91--129,
  2013.

\bibitem[Bandeira et~al.(2016)Bandeira, Boumal, and
  Voroninski]{Bandeira-2016-Low}
A.~S. Bandeira, N.~Boumal, and V.~Voroninski.
\newblock On the low-rank approach for semidefinite programs arising in
  synchronization and community detection.
\newblock In \emph{COLT}, pages 361--382. PMLR, 2016.

\bibitem[Beck and Hallak(2020)]{Beck-2020-Convergence}
A.~Beck and N.~Hallak.
\newblock On the convergence to stationary points of deterministic and
  randomized feasible descent directions methods.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0
  56--79, 2020.

\bibitem[Bena{\"\i}m et~al.(2005)Bena{\"\i}m, Hofbauer, and
  Sorin]{Benaim-2005-Stochastic}
M.~Bena{\"\i}m, J.~Hofbauer, and S.~Sorin.
\newblock Stochastic approximations and differential inclusions.
\newblock \emph{SIAM Journal on Control and Optimization}, 44\penalty0
  (1):\penalty0 328--348, 2005.

\bibitem[Bertsekas(1973)]{Bertsekas-1973-Stochastic}
D.~P. Bertsekas.
\newblock Stochastic optimization problems with nondifferentiable cost
  functionals.
\newblock \emph{Journal of Optimization Theory and Applications}, 12\penalty0
  (2):\penalty0 218--231, 1973.

\bibitem[Bhojanapalli et~al.(2016)Bhojanapalli, Neyshabur, and
  Srebro]{Bhojanapalli-2016-Global}
S.~Bhojanapalli, B.~Neyshabur, and N.~Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In \emph{NeurIPS}, pages 3880--3888, 2016.

\bibitem[Bian et~al.(2015)Bian, Chen, and Ye]{Bian-2015-Complexity}
W.~Bian, X.~Chen, and Y.~Ye.
\newblock Complexity analysis of interior point algorithms for non-{L}ipschitz
  and nonconvex minimization.
\newblock \emph{Mathematical Programming}, 149\penalty0 (1):\penalty0 301--327,
  2015.

\bibitem[Bolte and Pauwels(2021)]{Bolte-2021-Conservative}
J.~Bolte and E.~Pauwels.
\newblock Conservative set valued fields, automatic differentiation, stochastic
  gradient methods and deep learning.
\newblock \emph{Mathematical Programming}, 188\penalty0 (1):\penalty0 19--51,
  2021.

\bibitem[Bolte et~al.(2007)Bolte, Daniilidis, Lewis, and
  Shiota]{Bolte-2007-Clarke}
J.~Bolte, A.~Daniilidis, A.~Lewis, and M.~Shiota.
\newblock Clarke subgradients of stratifiable functions.
\newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (2):\penalty0
  556--572, 2007.

\bibitem[Bolte et~al.(2018)Bolte, Sabach, Teboulle, and
  Vaisbourd]{Bolte-2018-First}
J.~Bolte, S.~Sabach, M.~Teboulle, and Y.~Vaisbourd.
\newblock First order methods beyond convexity and {L}ipschitz gradient
  continuity with applications to quadratic inverse problems.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (3):\penalty0
  2131--2151, 2018.

\bibitem[Boumal et~al.(2016)Boumal, Voroninski, and
  Bandeira]{Boumal-2016-Nonconvex}
N.~Boumal, V.~Voroninski, and A.~S. Bandeira.
\newblock The non-convex {B}urer-{M}onteiro approach works on smooth
  semidefinite programs.
\newblock In \emph{NeurIPS}, pages 2765--2773, 2016.

\bibitem[Burke et~al.(2002{\natexlab{a}})Burke, Lewis, and
  Overton]{Burke-2002-Approximating}
J.~V. Burke, A.~S. Lewis, and M.~L. Overton.
\newblock Approximating subdifferentials by random sampling of gradients.
\newblock \emph{Mathematics of Operations Research}, 27\penalty0 (3):\penalty0
  567--584, 2002{\natexlab{a}}.

\bibitem[Burke et~al.(2002{\natexlab{b}})Burke, Lewis, and
  Overton]{Burke-2002-Two}
J.~V. Burke, A.~S. Lewis, and M.~L. Overton.
\newblock Two numerical methods for optimizing matrix stability.
\newblock \emph{Linear Algebra and its Applications}, 351:\penalty0 117--145,
  2002{\natexlab{b}}.

\bibitem[Burke et~al.(2005)Burke, Lewis, and Overton]{Burke-2005-Robust}
J.~V. Burke, A.~S. Lewis, and M.~L. Overton.
\newblock A robust gradient sampling algorithm for nonsmooth, nonconvex
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (3):\penalty0
  751--779, 2005.

\bibitem[Burke et~al.(2020)Burke, Curtis, Lewis, Overton, and
  Sim{\~o}es]{Burke-2020-Gradient}
J.~V. Burke, F.~E. Curtis, A.~S. Lewis, M.~L. Overton, and L.~E.~A. Sim{\~o}es.
\newblock Gradient sampling methods for nonsmooth optimization.
\newblock \emph{Numerical Nonsmooth Optimization: State of the Art Algorithms},
  pages 201--225, 2020.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and
  Sidford]{Carmon-2017-Convex}
Y.~Carmon, J.~C. Duchi, O.~Hinder, and A.~Sidford.
\newblock ``convex until proven guilty": Dimension-free acceleration of
  gradient descent on non-convex functions.
\newblock In \emph{ICML}, pages 654--663. PMLR, 2017.

\bibitem[Carmon et~al.(2018)Carmon, Duchi, Hinder, and
  Sidford]{Carmon-2018-Accelerated}
Y.~Carmon, J.~C. Duchi, O.~Hinder, and A.~Sidford.
\newblock Accelerated methods for nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1751--1772, 2018.

\bibitem[Carmon et~al.(2020)Carmon, Duchi, Hinder, and
  Sidford]{Carmon-2020-Lower}
Y.~Carmon, J.~C. Duchi, O.~Hinder, and A.~Sidford.
\newblock Lower bounds for finding stationary points {I}.
\newblock \emph{Mathematical Programming}, 184\penalty0 (1):\penalty0 71--120,
  2020.

\bibitem[Carmon et~al.(2021)Carmon, Duchi, Hinder, and
  Sidford]{Carmon-2021-Lower}
Y.~Carmon, J.~C. Duchi, O.~Hinder, and A.~Sidford.
\newblock Lower bounds for finding stationary points {II}: First-order methods.
\newblock \emph{Mathematical Programming}, 185\penalty0 (1):\penalty0 315--355,
  2021.

\bibitem[Chen et~al.(2019)Chen, Liu, Xu, Li, Lin, Hong, and Cox]{Chen-2019-Zo}
X.~Chen, S.~Liu, K.~Xu, X.~Li, X.~Lin, M.~Hong, and D.~Cox.
\newblock {ZO}-{A}da{MM}: zeroth-order adaptive momentum method for black-box
  optimization.
\newblock In \emph{NeurIPS}, pages 7204--7215, 2019.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{Choromanska-2015-Loss}
A.~Choromanska, M.~Henaff, M.~Mathieu, G.~B. Arous, and Y.~LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{AISTATS}, pages 192--204. PMLR, 2015.

\bibitem[Clarke(1990)]{Clarke-1990-Optimization}
F.~H. Clarke.
\newblock \emph{Optimization and Nonsmooth Analysis}.
\newblock SIAM, 1990.

\bibitem[Coste(2000)]{Coste-2000-Introduction}
M.~Coste.
\newblock \emph{An Introduction to {o}-Minimal Geometry}.
\newblock Istituti Editoriali E Poligrafici Internazionali Pisa, 2000.

\bibitem[Daniilidis and Drusvyatskiy(2020)]{Daniilidis-2020-Pathological}
A.~Daniilidis and D.~Drusvyatskiy.
\newblock Pathological subgradient dynamics.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (2):\penalty0
  1327--1338, 2020.

\bibitem[Davis and Drusvyatskiy(2019)]{Davis-2019-Stochastic}
D.~Davis and D.~Drusvyatskiy.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  207--239, 2019.

\bibitem[Davis et~al.(2020)Davis, Drusvyatskiy, Kakade, and
  Lee]{Davis-2020-Stochastic}
D.~Davis, D.~Drusvyatskiy, S.~Kakade, and J.~D. Lee.
\newblock Stochastic subgradient method converges on tame functions.
\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0
  (1):\penalty0 119--154, 2020.

\bibitem[Davis et~al.(2022)Davis, Drusvyatskiy, Lee, Padmanabhan, and
  Ye]{Davis-2022-Gradient}
D.~Davis, D.~Drusvyatskiy, Y.~T. Lee, S.~Padmanabhan, and G.~Ye.
\newblock A gradient sampling method with complexity guarantees for {L}ipschitz
  functions in high and low dimensions.
\newblock In \emph{NeurIPS}, page To appear, 2022.

\bibitem[Delfour(2019)]{Delfour-2019-Introduction}
M.~C. Delfour.
\newblock \emph{Introduction to Optimization and Hadamard Semidifferential
  Calculus}.
\newblock SIAM, 2019.

\bibitem[Drusvyatskiy and Paquette(2019)]{Drusvyatskiy-2019-Efficiency}
D.~Drusvyatskiy and C.~Paquette.
\newblock Efficiency of minimizing compositions of convex functions and smooth
  maps.
\newblock \emph{Mathematical Programming}, 178\penalty0 (1):\penalty0 503--558,
  2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{Duchi-2011-Adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (7):\penalty0 2121--2159, 2011.

\bibitem[Duchi and Ruan(2018)]{Duchi-2018-Stochastic}
J.~C. Duchi and F.~Ruan.
\newblock Stochastic methods for composite and weakly convex optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (4):\penalty0
  3229--3259, 2018.

\bibitem[Duchi et~al.(2012)Duchi, Bartlett, and
  Wainwright]{Duchi-2012-Randomized}
J.~C. Duchi, P.~L. Bartlett, and M.~J. Wainwright.
\newblock Randomized smoothing for stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  674--701, 2012.

\bibitem[Duchi et~al.(2015)Duchi, Jordan, Wainwright, and
  Wibisono]{Duchi-2015-Optimal}
J.~C. Duchi, M.~I. Jordan, M.~J. Wainwright, and A.~Wibisono.
\newblock Optimal rates for zero-order convex optimization: The power of two
  function evaluations.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (5):\penalty0 2788--2806, 2015.

\bibitem[Duffie(2010)]{Duffie-2010-Dynamic}
D.~Duffie.
\newblock \emph{Dynamic Asset Pricing Theory}.
\newblock Princeton University Press, 2010.

\bibitem[Flaxman et~al.(2005)Flaxman, Kalai, and McMahan]{Flaxman-2005-Online}
A.~D. Flaxman, A.~T. Kalai, and H.~B. McMahan.
\newblock Online convex optimization in the bandit setting: Gradient descent
  without a gradient.
\newblock In \emph{SODA}, pages 385--394, 2005.

\bibitem[Fuduli et~al.(2004)Fuduli, Gaudioso, and
  Giallombardo]{Fuduli-2004-Minimizing}
A.~Fuduli, M.~Gaudioso, and G.~Giallombardo.
\newblock Minimizing nonconvex nonsmooth functions via cutting planes and
  proximity control.
\newblock \emph{SIAM Journal on Optimization}, 14\penalty0 (3):\penalty0
  743--756, 2004.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{Ge-2015-Escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle points—online stochastic gradient for tensor
  decomposition.
\newblock In \emph{COLT}, pages 797--842. PMLR, 2015.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{Ge-2016-Matrix}
R.~Ge, J.~D. Lee, and T.~Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{NeurIPS}, pages 2981--2989, 2016.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{Ge-2017-Spurious}
R.~Ge, C.~Jin, and Y.~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock In \emph{ICML}, pages 1233--1242. PMLR, 2017.

\bibitem[Ghadimi and Lan(2013)]{Ghadimi-2013-Stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghadimi and Lan(2016)]{Ghadimi-2016-Accelerated}
S.~Ghadimi and G.~Lan.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1):\penalty0 59--99,
  2016.

\bibitem[Goldstein(1977)]{Goldstein-1977-Optimization}
A.~Goldstein.
\newblock Optimization of {L}ipschitz continuous functions.
\newblock \emph{Mathematical Programming}, 13\penalty0 (1):\penalty0 14--22,
  1977.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He-2016-Deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, pages 770--778, 2016.

\bibitem[Hong et~al.(2015)Hong, Nelson, and Xu]{Hong-2015-Discrete}
L.~J. Hong, B.~L. Nelson, and J.~Xu.
\newblock Discrete optimization via simulation.
\newblock In \emph{Handbook of Simulation Optimization}, pages 9--44. Springer,
  2015.

\bibitem[Huang et~al.(2022)Huang, Gao, Pei, and Huang]{Huang-2022-Accelerated}
F.~Huang, S.~Gao, J.~Pei, and H.~Huang.
\newblock Accelerated zeroth-order and first-order momentum methods from mini
  to minimax optimization.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (36):\penalty0 1--70, 2022.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{Huang-2017-Densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, pages 4700--4708, 2017.

\bibitem[Jain and Kar(2017)]{Jain-2017-Non}
P.~Jain and P.~Kar.
\newblock Non-convex optimization for machine learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  10\penalty0 (3-4):\penalty0 142--363, 2017.

\bibitem[Ji et~al.(2019)Ji, Wang, Zhou, and Liang]{Ji-2019-Improved}
K.~Ji, Z.~Wang, Y.~Zhou, and Y.~Liang.
\newblock Improved zeroth-order variance reduced algorithms and analysis for
  nonconvex optimization.
\newblock In \emph{ICML}, pages 3100--3109. PMLR, 2019.

\bibitem[Jin et~al.(2021)Jin, Netrapalli, Ge, Kakade, and
  Jordan]{Jin-2021-Nonconvex}
C.~Jin, P.~Netrapalli, R.~Ge, S.~M. Kakade, and M.~I. Jordan.
\newblock On nonconvex optimization for machine learning: Gradients,
  stochasticity, and saddle points.
\newblock \emph{Journal of the ACM (JACM)}, 68\penalty0 (2):\penalty0 1--29,
  2021.

\bibitem[Juditsky and Nemirovski(2008)]{Juditsky-2008-Large}
A.~Juditsky and A.~S. Nemirovski.
\newblock Large deviations of vector-valued martingales in 2-smooth normed
  spaces.
\newblock \emph{ArXiv Preprint: 0809.0813}, 2008.

\bibitem[Kingma and Ba(2015)]{Kingma-2014-Adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.
\newblock URL \url{https://openreview.net/forum?id=8gmWwjFyLj}.

\bibitem[Kiwiel(1996)]{Kiwiel-1996-Restricted}
K.~C. Kiwiel.
\newblock Restricted step and {L}evenberg-{M}arquardt techniques in proximal
  bundle methods for nonconvex nondifferentiable optimization.
\newblock \emph{SIAM Journal on Optimization}, 6\penalty0 (1):\penalty0
  227--249, 1996.

\bibitem[Kiwiel(2007)]{Kiwiel-2007-Convergence}
K.~C. Kiwiel.
\newblock Convergence of the gradient sampling algorithm for nonsmooth
  nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (2):\penalty0
  379--388, 2007.

\bibitem[Kornowski and Shamir(2021)]{Kornowski-2021-Oracle}
G.~Kornowski and O.~Shamir.
\newblock Oracle complexity in nonsmooth nonconvex optimization.
\newblock In \emph{NeurIPS}, pages 324--334, 2021.

\bibitem[Krizhevsky and Hinton(2009)]{Krizhevsky-2009-Learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{Lecun-1998-Gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lian et~al.(2016)Lian, Zhang, Hsieh, Huang, and
  Liu]{Lian-2016-Comprehensive}
X.~Lian, H.~Zhang, C-J. Hsieh, Y.~Huang, and J.~Liu.
\newblock A comprehensive linear speedup analysis for asynchronous stochastic
  parallel optimization from zeroth-order to first-order.
\newblock In \emph{NeurIPS}, pages 3062--3070, 2016.

\bibitem[Liu et~al.(2018)Liu, Kailkhura, Chen, Ting, Chang, and
  Amini]{Liu-2018-Zeroth}
S.~Liu, B.~Kailkhura, P-Y. Chen, P.~Ting, S.~Chang, and L.~Amini.
\newblock Zeroth-order stochastic variance reduction for nonconvex
  optimization.
\newblock In \emph{NeurIPS}, pages 3731--3741, 2018.

\bibitem[Loh and Wainwright(2015)]{Loh-2015-Regularized}
P-L. Loh and M.~J. Wainwright.
\newblock Regularized {M}-estimators with nonconvexity: Statistical and
  algorithmic theory for local optima.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 559--616,
  2015.

\bibitem[Ma et~al.(2020)Ma, Wang, Chi, and Chen]{Ma-2020-Implicit}
C.~Ma, K.~Wang, Y.~Chi, and Y.~Chen.
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval, matrix completion, and blind
  deconvolution.
\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0
  (3):\penalty0 451--632, 2020.

\bibitem[Majewski et~al.(2018)Majewski, Miasojedow, and
  Moulines]{Majewski-2018-Analysis}
S.~Majewski, B.~Miasojedow, and E.~Moulines.
\newblock Analysis of nonsmooth stochastic approximation: the differential
  inclusion approach.
\newblock \emph{ArXiv Preprint: 1805.01916}, 2018.

\bibitem[Mei et~al.(2017)Mei, Misiakiewicz, Montanari, and
  Oliveira]{Mei-2017-Solving}
S.~Mei, T.~Misiakiewicz, A.~Montanari, and R.~I. Oliveira.
\newblock Solving {SDP}s for synchronization and {M}ax{C}ut problems via the
  {G}rothendieck inequality.
\newblock In \emph{COLT}, pages 1476--1515. PMLR, 2017.

\bibitem[Murty and Kabadi(1987)]{Murty-1987-Some}
K.~G. Murty and S.~N. Kabadi.
\newblock Some {NP}-complete problems in quadratic and nonlinear programming.
\newblock \emph{Mathematical Programming}, 39\penalty0 (2):\penalty0 117--129,
  1987.

\bibitem[Nelson(2010)]{Nelson-2010-Optimization}
B.~L. Nelson.
\newblock Optimization via simulation over discrete decision variables.
\newblock In \emph{Risk and Optimization in an Uncertain World}, pages
  193--207. INFORMS, 2010.

\bibitem[Nemirovsky and Yudin(1983)]{Nemirovsky-1983-Problem}
A.~S. Nemirovsky and D.~B. Yudin.
\newblock \emph{Problem Complexity and Method Efficiency in Optimization}.
\newblock J. Wiley, 1983.

\bibitem[Nesterov(2013)]{Nesterov-2013-Gradient}
Y.~Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013.

\bibitem[Nesterov(2018)]{Nesterov-2018-Lectures}
Y.~Nesterov.
\newblock \emph{Lectures on Convex Optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Nesterov and Spokoiny(2017)]{Nesterov-2017-Random}
Y.~Nesterov and V.~Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17\penalty0
  (2):\penalty0 527--566, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, and Antiga]{Paszke-2019-Pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, and L.~Antiga.
\newblock Pytorch: an imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, pages 8026--8037, 2019.

\bibitem[Rockafellar and Wets(2009)]{Rockafellar-2009-Variational}
R.~T. Rockafellar and R.~J-B. Wets.
\newblock \emph{Variational Analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Shamir(2017)]{Shamir-2017-Optimal}
O.~Shamir.
\newblock An optimal algorithm for bandit and zero-order convex optimization
  with two-point feedback.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 1703--1713, 2017.

\bibitem[Shapiro(1990)]{Shapiro-1990-Concepts}
A.~Shapiro.
\newblock On concepts of directional differentiability.
\newblock \emph{Journal of Optimization Theory and Applications}, 66\penalty0
  (3):\penalty0 477--487, 1990.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{Srivastava-2014-Dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Stadtler(2008)]{Stadtler-2008-Supply}
H.~Stadtler.
\newblock Supply chain management — an overview.
\newblock \emph{Supply Chain Management and Advanced Planning}, pages 9--36,
  2008.

\bibitem[Sun et~al.(2018)Sun, Qu, and Wright]{Sun-2018-Geometric}
J.~Sun, Q.~Qu, and J.~Wright.
\newblock A geometric analysis of phase retrieval.
\newblock \emph{Foundations of Computational Mathematics}, 18\penalty0
  (5):\penalty0 1131--1198, 2018.

\bibitem[Tian et~al.(2022)Tian, Zhou, and So]{Tian-2022-Finite}
L.~Tian, K.~Zhou, and A.~M-C. So.
\newblock On the finite-time complexity and practical computation of
  approximate stationarity concepts of {L}ipschitz functions.
\newblock In \emph{ICML}, pages 21360--21379. PMLR, 2022.

\bibitem[Wainwright(2019)]{Wainwright-2019-High}
M.~J. Wainwright.
\newblock \emph{High-Dimensional Statistics: A Non-asymptotic Viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2017)Wang, Giannakis, and Eldar]{Wang-2017-Solving}
G.~Wang, G.~B. Giannakis, and Y.~C. Eldar.
\newblock Solving systems of random quadratic equations via truncated amplitude
  flow.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (2):\penalty0 773--794, 2017.

\bibitem[Wang et~al.(2018)Wang, Du, Balakrishnan, and
  Singh]{Wang-2018-Stochastic}
Y.~Wang, S.~Du, S.~Balakrishnan, and A.~Singh.
\newblock Stochastic zeroth-order optimization in high dimensions.
\newblock In \emph{AISTATS}, pages 1356--1365. PMLR, 2018.

\bibitem[Yousefian et~al.(2012)Yousefian, Nedi{\'c}, and
  Shanbhag]{Yousefian-2012-Stochastic}
F.~Yousefian, A.~Nedi{\'c}, and U.~V. Shanbhag.
\newblock On stochastic gradient and subgradient methods with adaptive
  steplength sequences.
\newblock \emph{Automatica}, 48\penalty0 (1):\penalty0 56--67, 2012.

\bibitem[Zhang et~al.(2020)Zhang, Lin, Jegelka, Sra, and
  Jadbabaie]{Zhang-2020-Complexity}
J.~Zhang, H.~Lin, S.~Jegelka, S.~Sra, and A.~Jadbabaie.
\newblock Complexity of finding stationary points of nonconvex nonsmooth
  functions.
\newblock In \emph{ICML}, pages 11173--11182. PMLR, 2020.

\end{thebibliography}
