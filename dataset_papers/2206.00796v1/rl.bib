%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Andrea Zanette at 2022-05-07 16:52:05 -0700 


%% Saved with string encoding Unicode (UTF-8) 


@string{aaai = {AAAI Conference on Artificial Intelligence (AAAI)}}

@string{aamas = {International Conference on Autonomous Agents and Multiagent Systems (AAMAS)}}

@string{acc = {American Control Conference (ACC)}}

@string{aiaa_info = {AIAA Infotech@Aerospace Conference}}

@string{aiaa_jacic = {Journal of Aerospace Computing, Information, and Communication}}

@string{allerton = {Allerton Conference on Communication, Control, and Compution}}

@string{atio = {AIAA Aviation Technology, Integration, and Operations Conference (ATIO)}}

@string{cacm = {Communications of the ACM}}

@string{cdc = {IEEE Conference on Decision and Control (CDC)}}

@string{colt = {Conference on Learning Theory (COLT)}}

@string{cvpr = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)}}

@string{dasc = {Digital Avionics Systems Conference (DASC)}}

@string{ecml = {European Conference on Machine Learning (ECML)}}

@string{gnc = {AIAA Guidance, Navigation, and Control Conference (GNC)}}

@string{icaart = {International Conference on Agents and Artificial Intelligence (ICAART)}}

@string{icaps = {International Conference on Automated Planning and Scheduling (ICAPS)}}

@string{icassp = {International Conference on Acoustics, Speech, and Signal Processing (ICASSP)}}

@string{iclr = {International Conference on Learning Representations}}

@string{icml = {International Conference on Machine Learning (ICML)}}

@string{icmla = {International Conference on Machine Learning and Applications (ICMLA)}}

@string{icra = {IEEE International Conference on Robotics and Automation (ICRA)}}

@string{icslp = {International Conference on Spoken Language Processing (ICSLP)}}

@string{ieee_csm = {IEEE Control Systems Magazine}}

@string{ieee_j_ac = {IEEE Transactions on Automatic Control}}

@string{ieeeaero = {IEEE Aerospace Conference}}

@string{ieeeciaig = {IEEE Transactions on Computational Intelligence and AI in Games}}

@string{ieeecst = {IEEE Transactions on Control Systems Technology}}

@string{ieeetac = {IEEE Transactions on Automatic Control}}

@string{ieeetsp = {IEEE Transactions on Signal Processing}}

@string{ijcai = {International Joint Conference on Artificial Intelligence (IJCAI)}}

@string{interspeech = {Annual Conference of the International Speech Communication Association (INTERSPEECH)}}

@string{iros = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}}

@string{itsc = {IEEE International Conference on Intelligent Transportation Systems (ITSC)}}

@string{iv = {IEEE Intelligent Vehicles Symposium (IV)}}

@string{jair = {Journal of Artificial Intelligence Research}}

@string{jgcd = {AIAA Journal of Guidance, Control, and Dynamics}}

@string{jmlr = {Journal of Machine Learning Research}}

@string{jota = {Journal of Optimization Theory and Applications}}

@string{lion = {Learning and Intelligent Optimization (LION)}}

@string{mit = {Massachusetts Institute of Technology}}

@string{mitaa = {Massachusetts Institute of Technology, Department of Aeronautics and Astronautics}}

@string{mitee = {Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science}}

@string{mitme = {Massachusetts Institute of Technology, Department of Mechanical Engineering}}

@string{mor = {Mathematics of Operations Research}}

@string{nips = {Advances in Neural Information Processing Systems (NIPS)}}

@string{or = {Operations Research}}

@string{rss = {Robotics: Science and Systems}}

@string{sigcomm = {ACM Special Interest Group on Data Communication (SIGCOMM)}}

@string{suaa = {Stanford University, Department of Aeronautics and Astronautics}}

@string{suee = {Stanford University, Department of Electrical Engineering}}

@string{sume = {Stanford University, Department of Mechanical Engineering}}

@string{tac = {IEEE Transactions on Automatic Control}}

@string{tacas = {International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS)}}

@string{taes = {IEEE Transactions on Aerospace and Electronic Systems}}

@string{uai = {Conference on Uncertainty in Artificial Intelligence (UAI)}}


@article{xu2021constraints,
	author = {Xu, Haoran and Zhan, Xianyuan and Zhu, Xiangyu},
	date-added = {2022-05-07 15:28:50 -0700},
	date-modified = {2022-05-07 15:28:50 -0700},
	journal = {arXiv preprint arXiv:2107.09003},
	title = {Constraints penalized q-learning for safe offline reinforcement learning},
	year = {2021}}

@article{santos2021understanding,
	author = {Santos, Pedro P and Melo, Francisco S and Sardinha, Alberto and Carvalho, Diogo S},
	date-added = {2022-05-07 15:27:11 -0700},
	date-modified = {2022-05-07 15:27:11 -0700},
	journal = {arXiv preprint arXiv:2111.11758},
	title = {Understanding the Impact of Data Distribution on Q-learning with Function Approximation},
	year = {2021}}

@article{shi2022pessimistic,
	author = {Shi, Laixi and Li, Gen and Wei, Yuting and Chen, Yuxin and Chi, Yuejie},
	date-added = {2022-05-07 15:25:58 -0700},
	date-modified = {2022-05-07 15:25:58 -0700},
	journal = {arXiv preprint arXiv:2202.13890},
	title = {Pessimistic Q-learning for offline reinforcement learning: Towards optimal sample complexity},
	year = {2022}}

@article{yan2022efficacy,
	author = {Yan, Yuling and Li, Gen and Chen, Yuxin and Fan, Jianqing},
	date-added = {2022-05-07 15:24:59 -0700},
	date-modified = {2022-05-07 15:24:59 -0700},
	journal = {arXiv preprint arXiv:2203.07368},
	title = {The efficacy of pessimism in asynchronous Q-learning},
	year = {2022}}

@article{li2022note,
	author = {Li, Ziniu and Xu, Tian and Yu, Yang},
	date-added = {2022-05-07 15:21:21 -0700},
	date-modified = {2022-05-07 15:21:21 -0700},
	journal = {arXiv preprint arXiv:2203.11489},
	title = {A Note on Target Q-learning For Solving Finite MDPs with A Generative Oracle},
	year = {2022}}

@article{liu2022provably,
	author = {Liu, Shuang and Su, Hao},
	date-added = {2022-05-07 15:18:04 -0700},
	date-modified = {2022-05-07 15:18:04 -0700},
	journal = {arXiv preprint arXiv:2204.10349},
	title = {Provably Efficient Kernelized Q-Learning},
	year = {2022}}

@misc{wagenmaker2021firstorder,
	archiveprefix = {arXiv},
	author = {Andrew Wagenmaker and Yifang Chen and Max Simchowitz and Simon S. Du and Kevin Jamieson},
	date-added = {2022-01-27 16:23:16 -0800},
	date-modified = {2022-01-27 16:23:16 -0800},
	eprint = {2112.03432},
	primaryclass = {cs.LG},
	title = {First-Order Regret in Reinforcement Learning with Linear Function Approximation: A Robust Estimation Approach},
	year = {2021}}

@misc{foster2021offline,
	archiveprefix = {arXiv},
	author = {Dylan J. Foster and Akshay Krishnamurthy and David Simchi-Levi and Yunzong Xu},
	date-added = {2022-01-26 20:50:11 -0800},
	date-modified = {2022-01-26 20:50:11 -0800},
	eprint = {2111.10919},
	primaryclass = {cs.LG},
	title = {Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation},
	year = {2021}}

@article{EveMan03,
	author = {E. Even-{D}ar and Y. Mansour},
	journal = jmlr,
	pages = {1--25},
	title = {Learning rates for {$Q$}-learning},
	volume = 5,
	year = 2003}

@article{JaaJorSin94,
	author = {T. Jaakkola and M. I. Jordan and S. P. Singh},
	journal = {Neural Computation},
	month = {November},
	number = 6,
	title = {On the Convergence of Stochastic Iterative Dynamic Programming Algorithms},
	volume = 6,
	year = 1994}

@misc{kong2021online,
	archiveprefix = {arXiv},
	author = {Dingwen Kong and Ruslan Salakhutdinov and Ruosong Wang and Lin F. Yang},
	date-added = {2021-12-06 17:47:06 -0800},
	date-modified = {2021-12-06 17:47:06 -0800},
	eprint = {2106.07203},
	primaryclass = {cs.LG},
	title = {Online Sub-Sampling for Reinforcement Learning with General Function Approximation},
	year = {2021}}

@article{zanette2021provable,
	author = {Zanette, Andrea and Wainwright, Martin J and Brunskill, Emma},
	date-added = {2021-11-02 15:01:22 -0700},
	date-modified = {2021-11-02 15:01:22 -0700},
	journal = {arXiv preprint arXiv:2108.08812},
	title = {Provable benefits of actor-critic methods for offline reinforcement learning},
	year = {2021}}

@article{xie2021bellman,
	author = {Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
	date-added = {2021-11-02 14:56:57 -0700},
	date-modified = {2021-11-02 14:56:57 -0700},
	journal = {arXiv preprint arXiv:2106.06926},
	title = {Bellman-consistent Pessimism for Offline Reinforcement Learning},
	year = {2021}}

@inproceedings{modi2020sample,
	author = {Modi, Aditya and Jiang, Nan and Tewari, Ambuj and Singh, Satinder},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2021-10-31 15:31:22 -0700},
	date-modified = {2021-10-31 15:31:22 -0700},
	organization = {PMLR},
	pages = {2010--2020},
	title = {Sample complexity of reinforcement learning using linearly combined model ensembles},
	year = {2020}}

@inproceedings{yang2021q,
	author = {Yang, Kunhe and Yang, Lin and Du, Simon},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2021-10-31 15:25:39 -0700},
	date-modified = {2021-10-31 15:25:39 -0700},
	organization = {PMLR},
	pages = {1576--1584},
	title = {Q-learning with logarithmic regret},
	year = {2021}}

@article{wagenmaker2021beyond,
	author = {Wagenmaker, Andrew and Simchowitz, Max and Jamieson, Kevin},
	date-added = {2021-10-31 15:23:30 -0700},
	date-modified = {2021-10-31 15:23:30 -0700},
	journal = {arXiv preprint arXiv:2108.02717},
	title = {Beyond No Regret: Instance-Dependent PAC Reinforcement Learning},
	year = {2021}}

@article{xu2021fine,
	author = {Xu, Haike and Ma, Tengyu and Du, Simon S},
	date-added = {2021-10-31 15:19:32 -0700},
	date-modified = {2021-10-31 15:19:32 -0700},
	journal = {arXiv preprint arXiv:2102.04692},
	title = {Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap},
	year = {2021}}

@inproceedings{he2021logarithmic,
	author = {He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-10-31 15:16:08 -0700},
	date-modified = {2021-10-31 15:16:08 -0700},
	organization = {PMLR},
	pages = {4171--4180},
	title = {Logarithmic regret for reinforcement learning with linear function approximation},
	year = {2021}}

@inproceedings{al2021adaptive,
	author = {Al Marjani, Aymen and Proutiere, Alexandre},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-10-31 15:13:31 -0700},
	date-modified = {2021-10-31 15:13:31 -0700},
	organization = {PMLR},
	pages = {7459--7468},
	title = {Adaptive sampling for best policy identification in markov decision processes},
	year = {2021}}

@article{tirinzoni2021fully,
	author = {Tirinzoni, Andrea and Pirotta, Matteo and Lazaric, Alessandro},
	date-added = {2021-10-31 15:12:35 -0700},
	date-modified = {2021-10-31 15:12:35 -0700},
	journal = {arXiv preprint arXiv:2106.13013},
	title = {A Fully Problem-Dependent Regret Lower Bound for Finite-Horizon MDPs},
	year = {2021}}

@article{yin2021towards,
	author = {Yin, Ming and Wang, Yu-Xiang},
	date-added = {2021-10-31 15:09:25 -0700},
	date-modified = {2021-10-31 15:09:25 -0700},
	journal = {arXiv preprint arXiv:2110.08695},
	title = {Towards Instance-Optimal Offline Reinforcement Learning with Pessimism},
	year = {2021}}

@article{agarwal2021online,
	author = {Agarwal, Naman and Chaudhuri, Syomantak and Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
	date-added = {2021-10-31 14:24:02 -0700},
	date-modified = {2021-10-31 14:24:02 -0700},
	journal = {arXiv preprint arXiv:2110.08440},
	title = {Online Target Q-learning with Reverse Experience Replay: Efficiently finding the Optimal Policy for Linear MDPs},
	year = {2021}}

@article{mnih2015human,
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	date-added = {2021-10-31 12:05:41 -0700},
	date-modified = {2021-10-31 12:05:41 -0700},
	journal = {nature},
	number = {7540},
	pages = {529--533},
	publisher = {Nature Publishing Group},
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	year = {2015}}

@article{carvalho2020new,
	author = {Carvalho, Diogo and Melo, Francisco S and Santos, Pedro},
	date-added = {2021-10-31 12:00:16 -0700},
	date-modified = {2021-10-31 12:00:16 -0700},
	journal = {Advances in Neural Information Processing Systems},
	pages = {19412--19421},
	title = {A new convergent variant of Q-learning with linear function approximation},
	volume = {33},
	year = {2020}}

@article{cai2019neural,
	author = {Cai, Qi and Yang, Zhuoran and Lee, Jason and Wang, Zhaoran},
	date-added = {2021-10-31 11:47:28 -0700},
	date-modified = {2021-10-31 11:47:28 -0700},
	title = {Neural temporal-difference learning converges to global optima},
	year = {2019}}

@inproceedings{riedmiller2005neural,
	author = {Riedmiller, Martin},
	booktitle = {European conference on machine learning},
	date-added = {2021-10-31 11:34:26 -0700},
	date-modified = {2021-10-31 11:34:26 -0700},
	organization = {Springer},
	pages = {317--328},
	title = {Neural fitted Q iteration--first experiences with a data efficient neural reinforcement learning method},
	year = {2005}}

@inproceedings{lakshminarayanan2018linear,
	author = {Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2021-10-31 11:14:15 -0700},
	date-modified = {2021-10-31 11:14:15 -0700},
	organization = {PMLR},
	pages = {1347--1355},
	title = {Linear stochastic approximation: How far does constant step-size and iterate averaging go?},
	year = {2018}}

@inproceedings{bhandari2018finite,
	author = {Bhandari, Jalaj and Russo, Daniel and Singal, Raghav},
	booktitle = {Conference on learning theory},
	date-added = {2021-10-31 11:06:37 -0700},
	date-modified = {2021-10-31 11:06:37 -0700},
	organization = {PMLR},
	pages = {1691--1692},
	title = {A finite time analysis of temporal difference learning with linear function approximation},
	year = {2018}}

@article{sutton1988learning,
	author = {Sutton, Richard S},
	date-added = {2021-10-31 10:57:45 -0700},
	date-modified = {2021-10-31 10:57:45 -0700},
	journal = {Machine learning},
	number = {1},
	pages = {9--44},
	publisher = {Springer},
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	year = {1988}}

@article{liu2020finite,
	author = {Liu, Bo and Liu, Ji and Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Petrik, Marek},
	date-added = {2021-10-31 10:54:38 -0700},
	date-modified = {2021-10-31 10:54:38 -0700},
	journal = {arXiv preprint arXiv:2006.14364},
	title = {Finite-sample analysis of proximal gradient td algorithms},
	year = {2020}}

@inproceedings{mehta2009q,
	author = {Mehta, Prashant and Meyn, Sean},
	booktitle = {Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference},
	date-added = {2021-10-31 10:52:46 -0700},
	date-modified = {2021-10-31 10:52:46 -0700},
	organization = {IEEE},
	pages = {3598--3605},
	title = {Q-learning and Pontryagin's minimum principle},
	year = {2009}}

@inproceedings{perkins2002existence,
	author = {Perkins, Theodore J and Pendrith, Mark D},
	booktitle = {ICML},
	date-added = {2021-10-31 10:47:20 -0700},
	date-modified = {2021-10-31 10:47:20 -0700},
	pages = {490--497},
	title = {On the existence of fixed points for Q-learning and Sarsa in partially observable domains},
	year = {2002}}

@inproceedings{xu2020finite,
	author = {Xu, Pan and Gu, Quanquan},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-10-31 10:39:04 -0700},
	date-modified = {2021-10-31 10:39:04 -0700},
	organization = {PMLR},
	pages = {10555--10565},
	title = {A finite-time analysis of Q-learning with neural network function approximation},
	year = {2020}}

@inproceedings{du2019rich,
	author = {Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-10-28 15:50:23 -0700},
	date-modified = {2021-10-28 15:50:32 -0700},
	organization = {PMLR},
	pages = {1665--1674},
	title = {Provably efficient RL with rich observations via latent state decoding},
	year = {2019}}

@article{weisz2021tensorplan,
	author = {Weisz, Gell{\'e}rt and Szepesv{\'a}ri, Csaba and Gy{\"o}rgy, Andr{\'a}s},
	date-added = {2021-10-28 15:34:29 -0700},
	date-modified = {2021-10-28 15:34:29 -0700},
	journal = {arXiv preprint arXiv:2110.02195},
	title = {TensorPlan and the Few Actions Lower Bound for Planning in MDPs under Linear Realizability of Optimal Value Functions},
	year = {2021}}

@article{wang2021exponential,
	author = {Wang, Yuanhao and Wang, Ruosong and Kakade, Sham M},
	date-added = {2021-10-28 15:33:36 -0700},
	date-modified = {2021-10-28 15:33:36 -0700},
	journal = {arXiv preprint arXiv:2103.12690},
	title = {An exponential lower bound for linearly-realizable MDPs with constant suboptimality gap},
	year = {2021}}

@article{mnih2013playing,
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	date-added = {2021-10-26 10:34:21 -0700},
	date-modified = {2021-10-26 10:34:21 -0700},
	journal = {arXiv preprint arXiv:1312.5602},
	title = {Playing atari with deep reinforcement learning},
	year = {2013}}

@article{even2003learning,
	author = {Even-Dar, Eyal and Mansour, Yishay and Bartlett, Peter},
	date-added = {2021-10-26 10:24:52 -0700},
	date-modified = {2021-10-26 10:24:52 -0700},
	journal = {Journal of machine learning Research},
	number = {1},
	title = {Learning Rates for Q-learning.},
	volume = {5},
	year = {2003}}

@article{kearns1999finite,
	author = {Kearns, Michael and Singh, Satinder},
	date-added = {2021-10-26 10:21:39 -0700},
	date-modified = {2021-10-26 10:21:39 -0700},
	journal = {Advances in neural information processing systems},
	pages = {996--1002},
	publisher = {MIT; 1998},
	title = {Finite-sample convergence rates for Q-learning and indirect algorithms},
	year = {1999}}

@article{szepesvari1998asymptotic,
	author = {Szepesv{\'a}ri, Csaba and others},
	date-added = {2021-10-26 10:19:12 -0700},
	date-modified = {2021-10-26 10:19:12 -0700},
	journal = {Advances in neural information processing systems},
	pages = {1064--1070},
	publisher = {Citeseer},
	title = {The asymptotic convergence-rate of Q-learning},
	year = {1998}}

@article{jaakkola1994convergence,
	author = {Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},
	date-added = {2021-10-26 10:17:04 -0700},
	date-modified = {2021-10-26 10:17:04 -0700},
	journal = {Neural computation},
	number = {6},
	pages = {1185--1201},
	publisher = {MIT Press},
	title = {On the convergence of stochastic iterative dynamic programming algorithms},
	volume = {6},
	year = {1994}}

@article{tsitsiklis1994asynchronous,
	author = {Tsitsiklis, John N},
	date-added = {2021-10-26 10:14:13 -0700},
	date-modified = {2021-10-26 10:14:13 -0700},
	journal = {Machine learning},
	number = {3},
	pages = {185--202},
	publisher = {Springer},
	title = {Asynchronous stochastic approximation and Q-learning},
	volume = {16},
	year = {1994}}

@article{wainwright2019stochastic,
	author = {Wainwright, Martin J},
	date-added = {2021-10-25 16:09:06 -0700},
	date-modified = {2021-10-25 16:09:06 -0700},
	journal = {arXiv preprint arXiv:1905.06265},
	title = {Stochastic approximation with cone-contractive operators: Sharp $\ell_{\infty}$-bounds for $Q$-learning},
	year = {2019}}

@article{zhang2020almost,
	author = {Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
	date-added = {2021-10-25 15:51:11 -0700},
	date-modified = {2021-10-25 15:51:11 -0700},
	journal = {Advances in Neural Information Processing Systems},
	title = {Almost Optimal Model-Free Reinforcement Learningvia Reference-Advantage Decomposition},
	volume = {33},
	year = {2020}}

@article{li2021q,
	author = {Li, Gen and Cai, Changxiao and Chen, Yuxin and Gu, Yuantao and Wei, Yuting and Chi, Yuejie},
	date-added = {2021-10-25 15:46:09 -0700},
	date-modified = {2021-10-25 15:46:09 -0700},
	journal = {arXiv preprint arXiv:2102.06548},
	title = {Is Q-learning minimax optimal? a tight sample complexity analysis},
	year = {2021}}

@article{watkins1992q,
	author = {Watkins, Christopher JCH and Dayan, Peter},
	date-added = {2021-10-25 15:42:06 -0700},
	date-modified = {2021-10-25 15:42:06 -0700},
	journal = {Machine learning},
	number = {3-4},
	pages = {279--292},
	publisher = {Springer},
	title = {Q-learning},
	volume = {8},
	year = {1992}}

@book{mohri2018foundations,
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	date-added = {2021-10-25 15:39:16 -0700},
	date-modified = {2021-10-25 15:39:16 -0700},
	publisher = {MIT press},
	title = {Foundations of machine learning},
	year = {2018}}

@article{watkins1989learning,
	author = {Watkins, Christopher {John Cornish Hellaby}},
	date-added = {2021-10-25 15:29:18 -0700},
	date-modified = {2021-10-25 15:29:18 -0700},
	publisher = {King's College, Cambridge United Kingdom},
	title = {Learning from delayed rewards},
	year = {1989}}

@article{lin1992self,
	author = {Lin, Long-Ji},
	date-added = {2021-10-17 17:09:16 -0700},
	date-modified = {2021-10-17 17:09:16 -0700},
	journal = {Machine learning},
	number = {3-4},
	pages = {293--321},
	publisher = {Springer},
	title = {Self-improving reactive agents based on reinforcement learning, planning and teaching},
	volume = {8},
	year = {1992}}

@article{wainwright2019variance,
	author = {Wainwright, Martin J},
	date-added = {2021-10-16 15:29:23 -0700},
	date-modified = {2021-10-25 16:11:29 -0700},
	journal = {arXiv preprint arXiv:1906.04697},
	title = {Variance-reduced Q-learning is minimax optimal},
	year = {2019}}

@article{li2020root,
	author = {Li, Chris Junchi and Mou, Wenlong and Wainwright, Martin J and Jordan, Michael I},
	date-added = {2021-10-16 15:24:45 -0700},
	date-modified = {2021-10-16 15:24:45 -0700},
	journal = {arXiv preprint arXiv:2008.12690},
	title = {Root-sgd: Sharp nonasymptotics and asymptotic efficiency in a single algorithm},
	year = {2020}}

@inproceedings{frostig2015competing,
	author = {Frostig, Roy and Ge, Rong and Kakade, Sham M and Sidford, Aaron},
	booktitle = {Conference on learning theory},
	date-added = {2021-10-16 15:23:51 -0700},
	date-modified = {2021-10-16 15:23:51 -0700},
	organization = {PMLR},
	pages = {728--763},
	title = {Competing with the empirical risk minimizer in a single pass},
	year = {2015}}

@article{srinivas2009gaussian,
	author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
	date-added = {2021-10-14 17:18:33 -0700},
	date-modified = {2021-10-14 17:18:33 -0700},
	journal = {arXiv preprint arXiv:0912.3995},
	title = {Gaussian process optimization in the bandit setting: No regret and experimental design},
	year = {2009}}

@inproceedings{beygelzimer2011contextual,
	author = {Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert},
	booktitle = {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	date-added = {2021-09-30 10:11:52 -0700},
	date-modified = {2021-09-30 10:11:52 -0700},
	organization = {JMLR Workshop and Conference Proceedings},
	pages = {19--26},
	title = {Contextual bandit algorithms with supervised learning guarantees},
	year = {2011}}

@article{tropp2015,
	author = {Joel A. Tropp},
	date-added = {2021-09-10 16:50:51 -0700},
	date-modified = {2021-09-10 16:51:04 -0700},
	doi = {10.1561/2200000048},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	number = {1-2},
	pages = {1-230},
	title = {An Introduction to Matrix Concentration Inequalities},
	url = {http://dx.doi.org/10.1561/2200000048},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1561/2200000048}}

@incollection{john2014extremum,
	author = {John, Fritz},
	booktitle = {Traces and emergence of nonlinear programming},
	date-added = {2021-06-17 11:39:24 -0700},
	date-modified = {2021-06-17 11:39:24 -0700},
	pages = {197--215},
	publisher = {Springer},
	title = {Extremum problems with inequalities as subsidiary conditions},
	year = {2014}}

@article{hao2021bootstrapping,
	author = {Hao, Botao and Ji, Xiang and Duan, Yaqi and Lu, Hao and Szepesv{\'a}ri, Csaba and Wang, Mengdi},
	date-added = {2021-05-26 11:58:45 -0700},
	date-modified = {2021-05-26 11:58:45 -0700},
	journal = {arXiv preprint arXiv:2102.03607},
	title = {Bootstrapping Statistical Inference for Off-Policy Evaluation},
	year = {2021}}

@article{duan2021risk,
	author = {Duan, Yaqi and Jin, Chi and Li, Zhiyuan},
	date-added = {2021-05-26 11:55:30 -0700},
	date-modified = {2021-05-26 11:55:30 -0700},
	journal = {arXiv preprint arXiv:2103.13883},
	title = {Risk Bounds and Rademacher Complexity in Batch Reinforcement Learning},
	year = {2021}}

@inproceedings{sutton1999policy,
	author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay and others},
	booktitle = {NIPs},
	date-added = {2021-05-25 14:29:48 -0700},
	date-modified = {2021-05-25 14:29:48 -0700},
	organization = {Citeseer},
	pages = {1057--1063},
	title = {Policy gradient methods for reinforcement learning with function approximation.},
	volume = {99},
	year = {1999}}

@inproceedings{voloshin2021minimax,
	author = {Voloshin, Cameron and Jiang, Nan and Yue, Yisong},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2021-05-25 14:19:27 -0700},
	date-modified = {2021-05-25 14:19:27 -0700},
	organization = {PMLR},
	pages = {1612--1620},
	title = {Minimax Model Learning},
	year = {2021}}

@article{uehara2021finite,
	author = {Uehara, Masatoshi and Imaizumi, Masaaki and Jiang, Nan and Kallus, Nathan and Sun, Wen and Xie, Tengyang},
	date-added = {2021-05-25 14:12:12 -0700},
	date-modified = {2021-05-25 14:12:12 -0700},
	journal = {arXiv preprint arXiv:2102.02981},
	title = {Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and first-order efficiency},
	year = {2021}}

@article{mannor2012lightning,
	author = {Mannor, Shie and Mebel, Ofir and Xu, Huan},
	date-added = {2021-05-25 13:55:34 -0700},
	date-modified = {2021-05-25 13:55:34 -0700},
	journal = {arXiv preprint arXiv:1206.4643},
	title = {Lightning does not strike twice: Robust MDPs with coupled uncertainty},
	year = {2012}}

@article{nair2020accelerating,
	author = {Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
	date-added = {2021-05-25 11:49:30 -0700},
	date-modified = {2021-05-25 11:49:30 -0700},
	journal = {arXiv preprint arXiv:2006.09359},
	title = {Accelerating online reinforcement learning with offline datasets},
	year = {2020}}

@article{siegel2020keep,
	author = {Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Heess, Nicolas and Riedmiller, Martin},
	date-added = {2021-05-25 11:47:36 -0700},
	date-modified = {2021-05-25 11:47:36 -0700},
	journal = {arXiv preprint arXiv:2002.08396},
	title = {Keep doing what worked: Behavioral modelling priors for offline reinforcement learning},
	year = {2020}}

@article{wang2020critic,
	author = {Wang, Ziyu and Novikov, Alexander and {\.Z}o{\l}na, Konrad and Springenberg, Jost Tobias and Reed, Scott and Shahriari, Bobak and Siegel, Noah and Merel, Josh and Gulcehre, Caglar and Heess, Nicolas and others},
	date-added = {2021-05-25 11:45:36 -0700},
	date-modified = {2021-05-25 11:45:36 -0700},
	journal = {arXiv preprint arXiv:2006.15134},
	title = {Critic regularized regression},
	year = {2020}}

@inproceedings{agarwal2020optimistic,
	author = {Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-05-25 11:18:53 -0700},
	date-modified = {2021-05-25 11:18:53 -0700},
	organization = {PMLR},
	pages = {104--114},
	title = {An optimistic perspective on offline reinforcement learning},
	year = {2020}}

@article{kumar2020conservative,
	author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
	date-added = {2021-05-25 11:09:37 -0700},
	date-modified = {2021-05-25 11:09:37 -0700},
	journal = {arXiv preprint arXiv:2006.04779},
	title = {Conservative q-learning for offline reinforcement learning},
	year = {2020}}

@article{wu2021uncertainty,
	author = {Wu, Yue and Zhai, Shuangfei and Srivastava, Nitish and Susskind, Joshua and Zhang, Jian and Salakhutdinov, Ruslan and Goh, Hanlin},
	date-added = {2021-05-25 10:58:59 -0700},
	date-modified = {2021-05-25 10:58:59 -0700},
	journal = {arXiv preprint arXiv:2105.08140},
	title = {Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning},
	year = {2021}}

@article{wu2019behavior,
	author = {Wu, Yifan and Tucker, George and Nachum, Ofir},
	date-added = {2021-05-25 10:53:30 -0700},
	date-modified = {2021-05-25 10:53:30 -0700},
	journal = {arXiv preprint arXiv:1911.11361},
	title = {Behavior regularized offline reinforcement learning},
	year = {2019}}

@article{jaques2019way,
	author = {Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
	date-added = {2021-05-25 10:18:01 -0700},
	date-modified = {2021-05-25 10:18:01 -0700},
	journal = {arXiv preprint arXiv:1907.00456},
	title = {Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
	year = {2019}}

@inproceedings{laroche2019safe,
	author = {Laroche, Romain and Trichelair, Paul and Des Combes, Remi Tachet},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-05-25 10:16:03 -0700},
	date-modified = {2021-05-25 10:16:03 -0700},
	organization = {PMLR},
	pages = {3652--3661},
	title = {Safe policy improvement with baseline bootstrapping},
	year = {2019}}

@article{nedic2003least,
	author = {Nedi{\'c}, A and Bertsekas, Dimitri P},
	date-added = {2021-05-16 10:43:02 -0700},
	date-modified = {2021-05-16 10:43:02 -0700},
	journal = {Discrete Event Dynamic Systems},
	number = {1},
	pages = {79--110},
	publisher = {Springer},
	title = {Least squares policy evaluation algorithms with linear function approximation},
	volume = {13},
	year = {2003}}

@article{weisz2021query,
	author = {Weisz, Gellert and Amortila, Philip and Janzer, Barnab{\'a}s and Abbasi-Yadkori, Yasin and Jiang, Nan and Szepesv{\'a}ri, Csaba},
	date-added = {2021-05-15 16:56:26 -0700},
	date-modified = {2021-05-15 16:56:26 -0700},
	journal = {arXiv preprint arXiv:2102.02049},
	title = {On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function},
	year = {2021}}

@article{nachum2019dualdice,
	author = {Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
	date-added = {2021-05-15 13:15:41 -0700},
	date-modified = {2021-05-15 13:15:41 -0700},
	journal = {arXiv preprint arXiv:1906.04733},
	title = {Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections},
	year = {2019}}

@article{nachum2020reinforcement,
	author = {Nachum, Ofir and Dai, Bo},
	date-added = {2021-05-15 13:14:53 -0700},
	date-modified = {2021-05-15 13:14:53 -0700},
	journal = {arXiv preprint arXiv:2001.01866},
	title = {Reinforcement learning via fenchel-rockafellar duality},
	year = {2020}}

@article{tang2019doubly,
	author = {Tang, Ziyang and Feng, Yihao and Li, Lihong and Zhou, Dengyong and Liu, Qiang},
	date-added = {2021-05-15 13:14:21 -0700},
	date-modified = {2021-05-15 13:14:21 -0700},
	journal = {arXiv preprint arXiv:1910.07186},
	title = {Doubly robust bias reduction in infinite horizon off-policy estimation},
	year = {2019}}

@article{kallus2019efficiently,
	author = {Kallus, Nathan and Uehara, Masatoshi},
	date-added = {2021-05-15 13:13:40 -0700},
	date-modified = {2021-05-15 13:13:40 -0700},
	journal = {arXiv preprint arXiv:1909.05850},
	title = {Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning},
	year = {2019}}

@article{jiang2020minimax,
	author = {Jiang, Nan and Huang, Jiawei},
	date-added = {2021-05-15 13:13:05 -0700},
	date-modified = {2021-05-15 13:13:05 -0700},
	journal = {arXiv preprint arXiv:2002.02081},
	title = {Minimax value interval for off-policy evaluation and policy optimization},
	year = {2020}}

@inproceedings{uehara2020minimax,
	author = {Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-05-15 13:12:37 -0700},
	date-modified = {2021-05-15 13:12:37 -0700},
	organization = {PMLR},
	pages = {9659--9668},
	title = {Minimax weight and q-function learning for off-policy evaluation},
	year = {2020}}

@inproceedings{yin2020asymptotically,
	author = {Yin, Ming and Wang, Yu-Xiang},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	date-added = {2021-05-15 13:11:50 -0700},
	date-modified = {2021-05-15 13:11:50 -0700},
	organization = {PMLR},
	pages = {3948--3958},
	title = {Asymptotically efficient off-policy evaluation for tabular reinforcement learning},
	year = {2020}}

@article{nachum2019algaedice,
	author = {Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
	date-added = {2021-05-15 13:09:57 -0700},
	date-modified = {2021-05-15 13:09:57 -0700},
	journal = {arXiv preprint arXiv:1912.02074},
	title = {Algaedice: Policy gradient from arbitrary experience},
	year = {2019}}

@article{yang2020off,
	author = {Yang, Mengjiao and Nachum, Ofir and Dai, Bo and Li, Lihong and Schuurmans, Dale},
	date-added = {2021-05-15 13:09:29 -0700},
	date-modified = {2021-05-15 13:09:29 -0700},
	journal = {arXiv preprint arXiv:2007.03438},
	title = {Off-policy evaluation via the regularized lagrangian},
	year = {2020}}

@inproceedings{farajtabar2018more,
	author = {Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-05-15 13:08:04 -0700},
	date-modified = {2021-05-15 13:08:04 -0700},
	organization = {PMLR},
	pages = {1447--1456},
	title = {More robust doubly robust off-policy evaluation},
	year = {2018}}

@article{zhang2020gendice,
	author = {Zhang, Ruiyi and Dai, Bo and Li, Lihong and Schuurmans, Dale},
	date-added = {2021-05-15 13:07:17 -0700},
	date-modified = {2021-05-15 13:07:17 -0700},
	journal = {arXiv preprint arXiv:2002.09072},
	title = {Gendice: Generalized offline estimation of stationary values},
	year = {2020}}

@article{liu2019neural,
	author = {Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
	date-added = {2021-05-15 13:04:08 -0700},
	date-modified = {2021-05-15 13:04:08 -0700},
	journal = {arXiv preprint arXiv:1906.10306},
	title = {Neural proximal/trust region policy optimization attains globally optimal policy},
	year = {2019}}

@article{wang2019neural,
	author = {Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
	date-added = {2021-05-15 13:03:42 -0700},
	date-modified = {2021-05-15 13:03:42 -0700},
	journal = {arXiv preprint arXiv:1909.01150},
	title = {Neural policy gradient methods: Global optimality and rates of convergence},
	year = {2019}}

@article{fu2020single,
	author = {Fu, Zuyue and Yang, Zhuoran and Wang, Zhaoran},
	date-added = {2021-05-15 13:03:21 -0700},
	date-modified = {2021-05-15 13:03:21 -0700},
	journal = {arXiv preprint arXiv:2008.00483},
	title = {Single-timescale actor-critic provably finds globally optimal policy},
	year = {2020}}

@inproceedings{fan2020theoretical,
	author = {Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
	booktitle = {Learning for Dynamics and Control},
	date-added = {2021-05-15 13:02:51 -0700},
	date-modified = {2021-05-15 13:02:51 -0700},
	organization = {PMLR},
	pages = {486--489},
	title = {A theoretical analysis of deep Q-learning},
	year = {2020}}

@article{liao2020batch,
	author = {Liao, Peng and Qi, Zhengling and Murphy, Susan},
	date-added = {2021-05-15 13:02:26 -0700},
	date-modified = {2021-05-15 13:02:26 -0700},
	journal = {arXiv preprint arXiv:2007.11771},
	title = {Batch Policy Learning in Average Reward Markov Decision Processes},
	year = {2020}}

@article{zhang2020variational,
	author = {Zhang, Junyu and Koppel, Alec and Bedi, Amrit Singh and Szepesvari, Csaba and Wang, Mengdi},
	date-added = {2021-05-15 13:01:47 -0700},
	date-modified = {2021-05-15 13:01:47 -0700},
	journal = {arXiv preprint arXiv:2007.02151},
	title = {Variational policy gradient method for reinforcement learning with general utilities},
	year = {2020}}

@article{scherrer2015approximate,
	author = {Scherrer, Bruno and Ghavamzadeh, Mohammad and Gabillon, Victor and Lesner, Boris and Geist, Matthieu},
	date-added = {2021-05-15 12:34:39 -0700},
	date-modified = {2021-05-15 12:34:39 -0700},
	journal = {J. Mach. Learn. Res.},
	pages = {1629--1676},
	title = {Approximate modified policy iteration and its application to the game of Tetris.},
	volume = {16},
	year = {2015}}

@article{farahmand2016regularized,
	author = {Farahmand, Amir-massoud and Ghavamzadeh, Mohammad and Szepesv{\'a}ri, Csaba and Mannor, Shie},
	date-added = {2021-05-15 12:28:26 -0700},
	date-modified = {2021-05-15 12:28:26 -0700},
	journal = {The Journal of Machine Learning Research},
	number = {1},
	pages = {4809--4874},
	publisher = {JMLR. org},
	title = {Regularized policy iteration with nonparametric function spaces},
	volume = {17},
	year = {2016}}

@article{antos2007fitted,
	author = {Antos, Andr{\'a}s and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
	date-added = {2021-05-15 11:54:46 -0700},
	date-modified = {2021-05-15 11:54:46 -0700},
	title = {Fitted Q-iteration in continuous action-space MDPs},
	year = {2007}}

@inproceedings{shani2020adaptive,
	author = {Shani, Lior and Efroni, Yonathan and Mannor, Shie},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	date-added = {2021-05-14 16:24:34 -0700},
	date-modified = {2021-05-14 16:24:34 -0700},
	number = {04},
	pages = {5668--5675},
	title = {Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps},
	volume = {34},
	year = {2020}}

@inproceedings{geist2019theory,
	author = {Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
	booktitle = {International Conference on Machine Learning},
	date-added = {2021-05-14 16:22:00 -0700},
	date-modified = {2021-05-14 16:22:00 -0700},
	organization = {PMLR},
	pages = {2160--2169},
	title = {A theory of regularized markov decision processes},
	year = {2019}}

@article{raskutti2015information,
	author = {Raskutti, Garvesh and Mukherjee, Sayan},
	date-added = {2021-05-14 16:13:13 -0700},
	date-modified = {2021-05-14 16:13:13 -0700},
	journal = {IEEE Transactions on Information Theory},
	number = {3},
	pages = {1451--1457},
	publisher = {IEEE},
	title = {The information geometry of mirror descent},
	volume = {61},
	year = {2015}}

@article{bhandari2020note,
	author = {Bhandari, Jalaj and Russo, Daniel},
	date-added = {2021-05-14 16:02:40 -0700},
	date-modified = {2021-05-14 16:02:40 -0700},
	journal = {arXiv preprint arXiv:2007.11120},
	title = {A note on the linear convergence of policy gradient methods},
	year = {2020}}

@article{lan2021policy,
	author = {Lan, Guanghui},
	date-added = {2021-05-14 16:01:45 -0700},
	date-modified = {2021-05-14 16:01:45 -0700},
	journal = {arXiv preprint arXiv:2102.00135},
	title = {Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes},
	year = {2021}}

@article{cen2020fast,
	author = {Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
	date-added = {2021-05-14 16:00:00 -0700},
	date-modified = {2021-05-14 16:00:00 -0700},
	journal = {arXiv preprint arXiv:2007.06558},
	title = {Fast global convergence of natural policy gradient methods with entropy regularization},
	year = {2020}}

@article{khodadadian2021linear,
	author = {Khodadadian, Sajad and Jhunjhunwala, Prakirt Raj and Varma, Sushil Mahavir and Maguluri, Siva Theja},
	date-added = {2021-05-14 15:57:35 -0700},
	date-modified = {2021-05-14 15:57:35 -0700},
	journal = {arXiv preprint arXiv:2105.01424},
	title = {On the Linear convergence of Natural Policy Gradient Algorithm},
	year = {2021}}

@article{bubeck2014convex,
	author = {Bubeck, S{\'e}bastien},
	date-added = {2021-05-14 15:34:21 -0700},
	date-modified = {2021-05-14 15:34:21 -0700},
	journal = {arXiv preprint arXiv:1405.4980},
	title = {Convex optimization: Algorithms and complexity},
	year = {2014}}

@article{agarwal2020flambe,
	author = {Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
	date-added = {2021-05-14 14:52:27 -0700},
	date-modified = {2021-05-14 14:52:27 -0700},
	journal = {arXiv preprint arXiv:2006.10814},
	title = {Flambe: Structural complexity and representation learning of low rank mdps},
	year = {2020}}

@article{modi2021model,
	author = {Modi, Aditya and Chen, Jinglin and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh},
	date-added = {2021-05-14 14:50:07 -0700},
	date-modified = {2021-05-14 14:50:07 -0700},
	journal = {arXiv preprint arXiv:2102.07035},
	title = {Model-free Representation Learning and Exploration in Low-rank MDPs},
	year = {2021}}

@article{rashidinejad2021bridging,
	author = {Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
	date-added = {2021-05-07 18:16:46 -0700},
	date-modified = {2021-05-07 18:16:46 -0700},
	journal = {arXiv preprint arXiv:2103.12021},
	title = {Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism},
	year = {2021}}

@article{du2021bilinear,
	author = {Du, Simon S and Kakade, Sham M and Lee, Jason D and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
	date-added = {2021-05-07 17:41:25 -0700},
	date-modified = {2021-05-07 17:41:25 -0700},
	journal = {arXiv preprint arXiv:2103.10897},
	title = {Bilinear classes: A structural framework for provable generalization in rl},
	year = {2021}}

@article{jin2021bellman,
	author = {Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
	date-added = {2021-05-07 17:39:29 -0700},
	date-modified = {2021-05-07 17:39:29 -0700},
	journal = {arXiv preprint arXiv:2102.00815},
	title = {Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms},
	year = {2021}}

@article{salkuyeh2018explicit,
	author = {Salkuyeh, Davod Khojasteh and Beik, Fatemeh Panjeh Ali},
	date-added = {2021-05-07 10:22:01 -0700},
	date-modified = {2021-05-07 10:22:01 -0700},
	journal = {International Journal of Applied and Computational Mathematics},
	number = {3},
	pages = {1--8},
	publisher = {Springer},
	title = {An explicit formula for the inverse of arrowhead and doubly arrow matrices},
	volume = {4},
	year = {2018}}

@book{Tsybakov09,
	address = {New York},
	author = {A. B. Tsybakov},
	date-added = {2021-05-06 12:05:50 -0700},
	date-modified = {2021-05-06 12:05:50 -0700},
	publisher = {Springer},
	title = {Introduction to Nonparametric Estimation},
	year = 2009}

@article{feng2021provably,
	author = {Feng, Fei and Yin, Wotao and Agarwal, Alekh and Yang, Lin F},
	date-added = {2021-05-05 17:49:36 -0700},
	date-modified = {2021-05-05 17:49:36 -0700},
	journal = {arXiv preprint arXiv:2103.11559},
	title = {Provably Correct Optimization and Exploration with Non-linear Policies},
	year = {2021}}

@article{zanette2021cautiously,
	author = {Zanette, Andrea and Cheng, Ching-An and Agarwal, Alekh},
	date-added = {2021-05-05 17:48:06 -0700},
	date-modified = {2021-05-05 17:48:06 -0700},
	journal = {arXiv preprint arXiv:2103.12923},
	title = {Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation},
	year = {2021}}

@article{levine2020offline,
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	date-added = {2021-05-05 17:45:09 -0700},
	date-modified = {2021-05-05 17:45:09 -0700},
	journal = {arXiv preprint arXiv:2005.01643},
	title = {Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
	year = {2020}}

@article{buckman2020importance,
	author = {Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
	date-added = {2021-05-05 16:54:33 -0700},
	date-modified = {2021-05-05 16:54:33 -0700},
	journal = {arXiv preprint arXiv:2009.06799},
	title = {The importance of pessimism in fixed-dataset policy optimization},
	year = {2020}}

@article{kumar2019stabilizing,
	author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
	date-added = {2021-05-05 16:53:41 -0700},
	date-modified = {2021-05-05 16:53:41 -0700},
	journal = {arXiv preprint arXiv:1906.00949},
	title = {Stabilizing off-policy q-learning via bootstrapping error reduction},
	year = {2019}}

@article{kidambi2020morel,
	author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
	date-added = {2021-05-05 16:53:19 -0700},
	date-modified = {2021-05-05 16:53:19 -0700},
	journal = {arXiv preprint arXiv:2005.05951},
	title = {Morel: Model-based offline reinforcement learning},
	year = {2020}}

@article{yu2020mopo,
	author = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
	date-added = {2021-05-05 16:52:53 -0700},
	date-modified = {2021-05-05 16:52:53 -0700},
	journal = {arXiv preprint arXiv:2005.13239},
	title = {Mopo: Model-based offline policy optimization},
	year = {2020}}

@article{jin2020pessimism,
	author = {Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
	date-added = {2021-05-03 10:46:21 -0700},
	date-modified = {2021-05-03 10:46:21 -0700},
	journal = {arXiv preprint arXiv:2012.15085},
	title = {Is Pessimism Provably Efficient for Offline RL?},
	year = {2020}}

@article{zanette2020exponential,
	author = {Zanette, Andrea},
	date-added = {2021-05-03 10:30:22 -0700},
	date-modified = {2021-05-03 10:30:22 -0700},
	journal = {arXiv preprint arXiv:2012.08005},
	title = {Exponential Lower Bounds for Batch Reinforcement Learning: Batch RL can be Exponentially Harder than Online RL},
	year = {2020}}

@article{kakade2001natural,
	author = {Kakade, Sham M},
	date-added = {2021-05-03 09:45:16 -0700},
	date-modified = {2021-05-03 09:45:16 -0700},
	journal = {Advances in neural information processing systems},
	title = {A natural policy gradient},
	volume = {14},
	year = {2001}}

@misc{amortila2020variant,
	archiveprefix = {arXiv},
	author = {Philip Amortila and Nan Jiang and Tengyang Xie},
	date-added = {2020-11-20 17:23:36 -0800},
	date-modified = {2020-11-20 17:23:36 -0800},
	eprint = {2011.01075},
	primaryclass = {cs.LG},
	title = {A Variant of the Wang-Foster-Kakade Lower Bound for the Discounted Setting},
	year = {2020}}

@inproceedings{xie2020Q,
	abstract = {We prove performance guarantees of two algorithms for approximating Q* in batch reinforcement learning. Compared to classical iterative methods such as Fitted Q-Iteration---whose performance loss incurs quadratic dependence on horizon---these methods estimate (some forms of) the Bellman error and enjoy linear-in-horizon error propagation, a property established for the first time for algorithms that rely solely on batch data and output stationary policies. One of the algorithms uses a novel and explicit importance-weighting correction to overcome the infamous "double sampling" difficulty in Bellman error estimation, and does not use any squared losses. Our analyses reveal its distinct characteristics and potential advantages compared to classical algorithms. },
	address = {Virtual},
	author = {Xie, Tengyang and Jiang, Nan},
	date-added = {2020-11-20 14:58:36 -0800},
	date-modified = {2020-11-20 14:58:49 -0800},
	editor = {Jonas Peters and David Sontag},
	month = {03--06 Aug},
	pages = {550--559},
	pdf = {http://proceedings.mlr.press/v124/xie20a/xie20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Q* Approximation Schemes for Batch Reinforcement Learning: A Theoretical Comparison},
	url = {http://proceedings.mlr.press/v124/xie20a.html},
	volume = {124},
	year = {2020},
	bdsk-url-1 = {http://proceedings.mlr.press/v124/xie20a.html}}

@inproceedings{xie2019towards,
	author = {Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-11-20 14:49:01 -0800},
	date-modified = {2020-11-20 14:49:01 -0800},
	pages = {9668--9678},
	title = {Towards optimal off-policy evaluation for reinforcement learning with marginalized importance sampling},
	year = {2019}}

@inproceedings{liu2018breaking,
	author = {Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-11-20 14:45:08 -0800},
	date-modified = {2020-11-20 14:45:08 -0800},
	pages = {5356--5366},
	title = {Breaking the curse of horizon: Infinite-horizon off-policy estimation},
	year = {2018}}

@article{li2015toward,
	author = {Li, Lihong and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
	date-added = {2020-11-20 14:40:59 -0800},
	date-modified = {2020-11-20 14:40:59 -0800},
	title = {Toward minimax off-policy value estimation},
	year = {2015}}

@inproceedings{thomas2016data,
	author = {Thomas, Philip and Brunskill, Emma},
	booktitle = {International Conference on Machine Learning},
	date-added = {2020-11-20 14:17:13 -0800},
	date-modified = {2020-11-20 14:17:13 -0800},
	pages = {2139--2148},
	title = {Data-efficient off-policy policy evaluation for reinforcement learning},
	year = {2016}}

@article{precup2000eligibility,
	author = {Precup, Doina},
	date-added = {2020-11-20 14:13:28 -0800},
	date-modified = {2020-11-20 14:13:28 -0800},
	journal = {Computer Science Department Faculty Publication Series},
	pages = {80},
	title = {Eligibility traces for off-policy policy evaluation},
	year = {2000}}

@inproceedings{jiang2016doubly,
	author = {Jiang, Nan and Li, Lihong},
	booktitle = {International Conference on Machine Learning},
	date-added = {2020-11-20 14:07:54 -0800},
	date-modified = {2020-11-20 14:07:54 -0800},
	organization = {PMLR},
	pages = {652--661},
	title = {Doubly robust off-policy value evaluation for reinforcement learning},
	year = {2016}}

@article{liu2020provably,
	author = {Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
	date-added = {2020-11-15 18:29:15 -0800},
	date-modified = {2020-11-15 18:29:15 -0800},
	journal = {arXiv preprint arXiv:2007.08202},
	title = {Provably good batch reinforcement learning without great exploration},
	year = {2020}}

@misc{hao2020sparse,
	archiveprefix = {arXiv},
	author = {Botao Hao and Yaqi Duan and Tor Lattimore and Csaba Szepesv{\'a}ri and Mengdi Wang},
	date-added = {2020-11-11 10:35:18 -0800},
	date-modified = {2020-11-11 10:35:18 -0800},
	eprint = {2011.04019},
	primaryclass = {cs.LG},
	title = {Sparse Feature Selection Makes Batch Reinforcement Learning More Sample Efficient},
	year = {2020}}

@article{wagenmaker2020experimental,
	author = {Wagenmaker, Andrew and Katz-Samuels, Julian and Jamieson, Kevin},
	date-added = {2020-11-11 08:57:31 -0800},
	date-modified = {2020-11-11 08:57:31 -0800},
	journal = {arXiv preprint arXiv:2011.00576},
	title = {Experimental Design for Regret Minimization in Linear Bandits},
	year = {2020}}

@article{xie2020batch,
	author = {Xie, Tengyang and Jiang, Nan},
	date-added = {2020-11-03 13:22:08 -0800},
	date-modified = {2020-11-03 13:22:08 -0800},
	journal = {arXiv preprint arXiv:2008.04990},
	title = {Batch Value-function Approximation with Only Realizability},
	year = {2020}}

@article{bradtke1996linear,
	author = {Bradtke, Steven J and Barto, Andrew G},
	date-added = {2020-11-03 12:56:44 -0800},
	date-modified = {2020-11-03 12:56:44 -0800},
	journal = {Machine learning},
	number = {1-3},
	pages = {33--57},
	publisher = {Springer},
	title = {Linear least-squares algorithms for temporal difference learning},
	volume = {22},
	year = {1996}}

@article{cui2020plug,
	author = {Cui, Qiwen and Yang, Lin F},
	date-added = {2020-11-03 12:48:16 -0800},
	date-modified = {2020-11-03 12:48:16 -0800},
	journal = {arXiv preprint arXiv:2010.05673},
	title = {Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?},
	year = {2020}}

@article{duan2020minimax,
	author = {Duan, Yaqi and Wang, Mengdi},
	date-added = {2020-11-03 12:19:13 -0800},
	date-modified = {2020-11-03 12:19:13 -0800},
	journal = {arXiv preprint arXiv:2002.09516},
	title = {Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation},
	year = {2020}}

@inproceedings{agarwal2020optimality,
	author = {Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
	booktitle = {Conference on Learning Theory},
	date-added = {2020-11-03 12:11:38 -0800},
	date-modified = {2020-11-03 12:11:38 -0800},
	pages = {64--66},
	title = {Optimality and approximation with policy gradient methods in Markov decision processes},
	year = {2020}}

@article{marjani2020best,
	author = {Marjani, Aymen Al and Proutiere, Alexandre},
	date-added = {2020-11-03 10:04:35 -0800},
	date-modified = {2020-11-03 10:04:35 -0800},
	journal = {arXiv preprint arXiv:2009.13405},
	title = {Best Policy Identification in discounted MDPs: Problem-specific Sample Complexity},
	year = {2020}}

@book{pukelsheim2006optimal,
	author = {Pukelsheim, Friedrich},
	date-added = {2020-11-02 08:10:37 -0800},
	date-modified = {2020-11-02 08:10:37 -0800},
	publisher = {SIAM},
	title = {Optimal design of experiments},
	year = {2006}}

@article{yin2020near,
	author = {Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
	date-added = {2020-11-01 12:28:59 -0800},
	date-modified = {2020-11-01 12:28:59 -0800},
	journal = {arXiv preprint arXiv:2007.03760},
	title = {Near Optimal Provable Uniform Convergence in Off-Policy Evaluation for Reinforcement Learning},
	year = {2020}}

@article{wang2020statistical,
	author = {Wang, Ruosong and Foster, Dean P and Kakade, Sham M},
	date-added = {2020-11-01 08:52:20 -0800},
	date-modified = {2020-11-01 08:52:20 -0800},
	journal = {arXiv preprint arXiv:2010.11895},
	title = {What are the Statistical Limits of Offline RL with Linear Function Approximation?},
	year = {2020}}

@article{weisz2020exponential,
	author = {Weisz, Gellert and Amortila, Philip and Szepesv{\'a}ri, Csaba},
	date-added = {2020-11-01 08:51:36 -0800},
	date-modified = {2020-11-01 08:51:36 -0800},
	journal = {arXiv preprint arXiv:2010.01374},
	title = {Exponential Lower Bounds for Planning in MDPs With Linearly-Realizable Optimal Action-Value Functions},
	year = {2020}}

@inproceedings{agarwal2020model,
	author = {Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
	booktitle = {Conference on Learning Theory},
	date-added = {2020-10-31 11:11:17 -0700},
	date-modified = {2020-10-31 11:11:17 -0700},
	pages = {67--83},
	title = {Model-based reinforcement learning with a generative model is minimax optimal},
	year = {2020}}

@conference{zanette2020provably,
	author = {Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel J and Brunskill, Emma},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-10-12 17:47:05 -0700},
	date-modified = {2020-10-12 17:47:37 -0700},
	title = {Provably Efficient Reward-Agnostic Navigation with Linear Value Iteration},
	year = {2020}}

@article{ayoub2020model,
	author = {Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin F},
	date-added = {2020-10-12 17:41:28 -0700},
	date-modified = {2020-10-12 17:41:28 -0700},
	journal = {arXiv preprint arXiv:2006.01107},
	title = {Model-Based Reinforcement Learning with Value-Targeted Regression},
	year = {2020}}

@article{cai2019provably,
	author = {Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
	date-added = {2020-10-12 17:39:52 -0700},
	date-modified = {2020-10-12 17:39:52 -0700},
	journal = {arXiv preprint arXiv:1912.05830},
	title = {Provably efficient exploration in policy optimization},
	year = {2019}}

@article{zhou2020provably,
	author = {Zhou, Dongruo and He, Jiafan and Gu, Quanquan},
	date-added = {2020-10-12 17:37:21 -0700},
	date-modified = {2020-10-12 17:37:21 -0700},
	journal = {arXiv preprint arXiv:2006.13165},
	title = {Provably Efficient Reinforcement Learning for Discounted MDPs with Feature Mapping},
	year = {2020}}

@inproceedings{munos2003error,
	author = {Munos, R{\'e}mi},
	booktitle = {ICML},
	date-added = {2020-10-12 17:30:10 -0700},
	date-modified = {2020-10-12 17:30:10 -0700},
	pages = {560--567},
	title = {Error bounds for approximate policy iteration},
	volume = {3},
	year = {2003}}

@article{zhang2020reinforcement,
	author = {Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
	date-added = {2020-10-12 17:11:41 -0700},
	date-modified = {2020-10-12 17:11:41 -0700},
	journal = {arXiv preprint arXiv:2009.13503},
	title = {Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon},
	year = {2020}}

@article{li2020breaking,
	author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
	date-added = {2020-10-12 17:07:51 -0700},
	date-modified = {2020-10-12 17:07:51 -0700},
	journal = {arXiv preprint arXiv:2005.12900},
	title = {Breaking the sample size barrier in model-based reinforcement learning with a generative model},
	year = {2020}}

@article{li2011concise,
	author = {Li, Shengqiao},
	date-added = {2020-10-08 17:16:41 -0700},
	date-modified = {2020-10-08 17:16:41 -0700},
	journal = {Asian Journal of Mathematics and Statistics},
	number = {1},
	pages = {66--70},
	title = {Concise formulas for the area and volume of a hyperspherical cap},
	volume = {4},
	year = {2011}}

@article{agarwal2020pc,
	author = {Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
	date-added = {2020-09-10 12:39:05 -0700},
	date-modified = {2020-09-10 12:39:05 -0700},
	journal = {arXiv preprint arXiv:2007.08459},
	title = {Pc-pg: Policy cover directed exploration for provable policy gradient learning},
	year = {2020}}

@inproceedings{mehta2017fast,
	author = {Mehta, Nishant},
	booktitle = {Artificial Intelligence and Statistics},
	date-added = {2020-09-10 10:44:05 -0700},
	date-modified = {2020-09-10 10:44:05 -0700},
	organization = {PMLR},
	pages = {1085--1093},
	title = {Fast rates with high probability in exp-concave statistical learning},
	year = {2017}}

@book{shalev2014understanding,
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	date-added = {2020-07-31 14:48:04 -0700},
	date-modified = {2020-07-31 14:48:04 -0700},
	publisher = {Cambridge university press},
	title = {Understanding machine learning: From theory to algorithms},
	year = {2014}}

@misc{gao2017properties,
	archiveprefix = {arXiv},
	author = {Bolin Gao and Lacra Pavel},
	date-added = {2020-06-30 20:18:17 -0700},
	date-modified = {2020-06-30 20:18:17 -0700},
	eprint = {1704.00805},
	primaryclass = {math.OC},
	title = {On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning},
	year = {2017}}

@article{auer2002using,
	author = {Auer, Peter},
	date-added = {2020-06-28 16:11:04 -0700},
	date-modified = {2020-06-28 16:11:04 -0700},
	journal = {Journal of Machine Learning Research},
	number = {Nov},
	pages = {397--422},
	title = {Using confidence bounds for exploitation-exploration trade-offs},
	volume = {3},
	year = {2002}}

@inproceedings{krishnamurthy2018semiparametric,
	author = {Krishnamurthy, Akshay and Wu, Steven and Syrgkanis, Vasilis},
	booktitle = {35th International Conference on Machine Learning, ICML 2018},
	date-added = {2020-06-28 16:10:05 -0700},
	date-modified = {2020-06-28 16:10:05 -0700},
	organization = {International Machine Learning Society (IMLS)},
	pages = {4330--4349},
	title = {Semiparametric contextual bandits},
	year = {2018}}

@article{gopalan2016low,
	author = {Gopalan, Aditya and Maillard, Odalric-Ambrym and Zaki, Mohammadi},
	date-added = {2020-06-28 16:07:49 -0700},
	date-modified = {2020-06-28 16:07:49 -0700},
	journal = {arXiv preprint arXiv:1609.01508},
	title = {Low-rank bandits with latent mixtures},
	year = {2016}}

@inproceedings{ghosh2017misspecified,
	author = {Ghosh, Avishek and Chowdhury, Sayak Ray and Gopalan, Aditya},
	booktitle = {Thirty-First AAAI Conference on Artificial Intelligence},
	date-added = {2020-06-28 16:07:09 -0700},
	date-modified = {2020-06-28 16:07:09 -0700},
	title = {Misspecified linear bandits},
	year = {2017}}

@article{tossou2019near,
	author = {Tossou, Aristide and Basu, Debabrota and Dimitrakakis, Christos},
	date-added = {2020-06-28 16:01:25 -0700},
	date-modified = {2020-06-28 16:01:25 -0700},
	journal = {arXiv preprint arXiv:1905.12425},
	title = {Near-optimal optimistic reinforcement learning using empirical bernstein inequalities},
	year = {2019}}

@inproceedings{zhang2019regret,
	author = {Zhang, Zihan and Ji, Xiangyang},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-06-28 16:00:21 -0700},
	date-modified = {2020-06-28 16:00:21 -0700},
	pages = {2827--2836},
	title = {Regret minimization for reinforcement learning by evaluating the optimal bias function},
	year = {2019}}

@conference{misra2019kinematic,
	author = {Misra, Dipendra and Henaff, Mikael and Krishnamurthy, Akshay and Langford, John},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2020-06-01 09:57:38 -0700},
	date-modified = {2020-06-04 13:14:40 -0700},
	journal = {arXiv preprint arXiv:1911.05815},
	title = {Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning},
	year = {2020}}

@inproceedings{du2019provablyefficient,
	abstract = {We study the exploration problem in episodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumptions, we demonstrate how to estimate a mapping from the observations to latent states inductively through a sequence of regression and clustering steps---where previously decoded latent states provide labels for later regression problems---and use it to construct good exploration policies. We provide finite-sample guarantees on the quality of the learned state decoding function and exploration policies, and complement our theory with an empirical evaluation on a class of hard exploration problems. Our method exponentially improves over $Q$-learning with na{\"\i}ve exploration, even when $Q$-learning has cheating access to latent states.},
	address = {Long Beach, California, USA},
	author = {Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	date-added = {2020-06-01 09:43:00 -0700},
	date-modified = {2020-06-01 09:43:34 -0700},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	month = {09--15 Jun},
	pages = {1665--1674},
	pdf = {http://proceedings.mlr.press/v97/du19b/du19b.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Provably efficient {RL} with Rich Observations via Latent State Decoding},
	url = {http://proceedings.mlr.press/v97/du19b.html},
	volume = {97},
	year = {2019},
	bdsk-url-1 = {http://proceedings.mlr.press/v97/du19b.html}}

@article{kiefer1960equivalence,
	author = {Kiefer, Jack and Wolfowitz, Jacob},
	date-added = {2020-05-31 23:04:19 -0700},
	date-modified = {2020-05-31 23:04:19 -0700},
	journal = {Canadian Journal of Mathematics},
	pages = {363--366},
	publisher = {Cambridge University Press},
	title = {The equivalence of two extremum problems},
	volume = {12},
	year = {1960}}

@inproceedings{gopalan2015thompson,
	author = {Gopalan, Aditya and Mannor, Shie},
	booktitle = {Conference on Learning Theory},
	date-added = {2020-05-31 15:57:25 -0700},
	date-modified = {2020-05-31 15:57:25 -0700},
	pages = {861--898},
	title = {Thompson sampling for learning parameterized markov decision processes},
	year = {2015}}

@inproceedings{ouyang2017learning,
	author = {Ouyang, Yi and Gagrani, Mukul and Nayyar, Ashutosh and Jain, Rahul},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-05-31 15:56:26 -0700},
	date-modified = {2020-05-31 15:56:26 -0700},
	pages = {1333--1342},
	title = {Learning unknown markov decision processes: A thompson sampling approach},
	year = {2017}}

@book{lattimore2020bandit,
	author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
	date-added = {2020-05-30 17:23:51 -0700},
	date-modified = {2020-05-30 17:24:55 -0700},
	publisher = {Cambridge University Press},
	title = {Bandit Algorithms},
	year = {2020}}

@misc{du2020agnostic,
	archiveprefix = {arXiv},
	author = {Simon S. Du and Jason D. Lee and Gaurav Mahajan and Ruosong Wang},
	date-added = {2020-05-30 16:28:25 -0700},
	date-modified = {2020-05-30 16:28:25 -0700},
	eprint = {2002.07125},
	primaryclass = {cs.LG},
	title = {Agnostic Q-learning with Function Approximation in Deterministic Systems: Tight Bounds on Approximation Error and Sample Complexity},
	year = {2020}}

@inproceedings{du2019provably,
	author = {Du, Simon S and Luo, Yuping and Wang, Ruosong and Zhang, Hanrui},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2020-05-30 09:07:41 -0700},
	date-modified = {2020-05-30 09:07:41 -0700},
	pages = {8058--8068},
	title = {Provably efficient Q-learning with function approximation via distribution shift error checking oracle},
	year = {2019}}

@misc{wang2020provably,
	archiveprefix = {arXiv},
	author = {Ruosong Wang and Ruslan Salakhutdinov and Lin F. Yang},
	date-added = {2020-05-26 14:46:46 -0700},
	date-modified = {2020-05-26 14:46:46 -0700},
	eprint = {2005.10804},
	primaryclass = {cs.LG},
	title = {Provably Efficient Reinforcement Learning with General Value Function Approximation},
	year = {2020}}

@article{wang2019optimism,
	author = {Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
	date-added = {2020-05-26 14:40:37 -0700},
	date-modified = {2020-05-26 14:40:37 -0700},
	journal = {arXiv preprint arXiv:1912.04136},
	title = {Optimism in Reinforcement Learning with Generalized Linear Function Approximation},
	year = {2019}}

@book{kochenderfer2019algorithms,
	author = {Kochenderfer, Mykel J and Wheeler, Tim A},
	date-added = {2020-05-25 13:32:03 -0700},
	date-modified = {2020-05-25 13:32:03 -0700},
	publisher = {Mit Press},
	title = {Algorithms for optimization},
	year = {2019}}

@misc{hazan2018provably,
	archiveprefix = {arXiv},
	author = {Elad Hazan and Sham M. Kakade and Karan Singh and Abby Van Soest},
	date-added = {2020-05-13 18:07:48 -0700},
	date-modified = {2020-05-13 18:07:48 -0700},
	eprint = {1812.02690},
	primaryclass = {cs.LG},
	title = {Provably Efficient Maximum Entropy Exploration},
	year = {2018}}

@conference{jin2020rewardfree,
	archiveprefix = {arXiv},
	author = {Chi Jin and Akshay Krishnamurthy and Max Simchowitz and Tiancheng Yu},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2020-05-13 17:46:27 -0700},
	date-modified = {2020-06-02 11:17:56 -0700},
	eprint = {2002.02794},
	primaryclass = {cs.LG},
	title = {Reward-Free Exploration for Reinforcement Learning},
	year = {2020}}

@conference{zanette2020learning,
	archiveprefix = {arXiv},
	author = {Andrea Zanette and Alessandro Lazaric and Mykel Kochenderfer and Emma Brunskill},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2020-04-01 15:05:33 -0700},
	date-modified = {2020-06-02 10:10:47 -0700},
	eprint = {2003.00153},
	primaryclass = {cs.LG},
	title = {Learning Near Optimal Policies with Low Inherent Bellman Error},
	year = {2020}}

@article{shreve1978alternative,
	author = {Shreve, Steven E and Bertsekas, Dimitri P},
	date-added = {2019-12-18 18:13:22 +0100},
	date-modified = {2019-12-18 18:13:22 +0100},
	journal = {SIAM Journal on control and optimization},
	number = {6},
	pages = {953--978},
	publisher = {SIAM},
	title = {Alternative theoretical frameworks for finite horizon discrete-time stochastic optimal control},
	volume = {16},
	year = {1978}}

@book{puterman1994markov,
	address = {New York, NY, USA},
	author = {Puterman, Martin L.},
	date-added = {2019-12-18 17:58:02 +0100},
	date-modified = {2019-12-18 17:58:02 +0100},
	isbn = {0471619779},
	publisher = {John Wiley \& Sons, Inc.},
	title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
	year = {1994}}

@inproceedings{dann2019policy,
	author = {Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
	booktitle = {International Conference on Machine Learning},
	date-added = {2019-12-18 17:46:26 +0100},
	date-modified = {2019-12-18 17:46:26 +0100},
	pages = {1507--1516},
	title = {Policy Certificates: Towards Accountable Reinforcement Learning},
	year = {2019}}

@article{de2000existence,
	author = {De Farias, Daniela Pucci and Van Roy, Benjamin},
	date-added = {2019-11-28 16:12:26 -0800},
	date-modified = {2019-11-28 16:12:26 -0800},
	journal = {Journal of Optimization theory and Applications},
	number = {3},
	pages = {589--608},
	publisher = {Springer},
	title = {On the existence of fixed points for approximate value iteration and temporal-difference learning},
	volume = {105},
	year = {2000}}

@inproceedings{chen2019information,
	author = {Chen, Jinglin and Jiang, Nan},
	booktitle = {International Conference on Machine Learning},
	date-added = {2019-11-27 14:37:35 -0800},
	date-modified = {2019-11-27 14:37:35 -0800},
	pages = {1042--1051},
	title = {Information-Theoretic Considerations in Batch Reinforcement Learning},
	year = {2019}}

@conference{lattimore2020learning,
	author = {Lattimore, Tor and Szepesvari, Csaba},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2019-11-26 16:15:34 -0800},
	date-modified = {2020-06-02 17:56:45 -0700},
	journal = {arXiv preprint arXiv:1911.07676},
	title = {Learning with Good Feature Representations in Bandits and in RL with a Generative Model},
	year = {2020}}

@article{van2019comments,
	author = {Van Roy, Benjamin and Dong, Shi},
	date-added = {2019-11-26 16:14:57 -0800},
	date-modified = {2019-11-26 16:14:57 -0800},
	journal = {arXiv preprint arXiv:1911.07910},
	title = {Comments on the Du-Kakade-Wang-Yang Lower Bounds},
	year = {2019}}

@article{abbasi2019exploration,
	author = {Abbasi-Yadkori, Yasin and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gellert},
	date-added = {2019-11-26 11:11:37 -0800},
	date-modified = {2019-11-26 11:11:37 -0800},
	journal = {arXiv preprint arXiv:1908.10479},
	title = {Exploration-Enhanced POLITEX},
	year = {2019}}

@inproceedings{abbasi2019politex,
	author = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesv{\'a}ri, Csaba and Weisz, Gell{\'e}rt},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	date-added = {2019-11-26 11:11:16 -0800},
	date-modified = {2019-11-26 11:11:16 -0800},
	pages = {3692--3702},
	title = {POLITEX: Regret bounds for policy iteration using expert prediction},
	volume = {97},
	year = {2019}}

@article{lazaric2012finite,
	author = {Lazaric, Alessandro and Ghavamzadeh, Mohammad and Munos, R{\'e}mi},
	date-added = {2019-11-26 10:56:14 -0800},
	date-modified = {2019-11-26 10:56:14 -0800},
	journal = {Journal of Machine Learning Research},
	number = {Oct},
	pages = {3041--3074},
	title = {Finite-sample analysis of least-squares policy iteration},
	volume = {13},
	year = {2012}}

@article{lagoudakis2003least,
	author = {Lagoudakis, Michail G and Parr, Ronald},
	date-added = {2019-11-26 10:05:33 -0800},
	date-modified = {2019-11-26 10:05:33 -0800},
	journal = {Journal of machine learning research},
	number = {Dec},
	pages = {1107--1149},
	title = {Least-squares policy iteration},
	volume = {4},
	year = {2003}}

@conference{zanette2020frequentist,
	author = {Zanette, Andrea and Brandfonbrener, David and Pirotta, Matteo and Lazaric, Alessandro},
	booktitle = {AISTATS},
	date-added = {2019-11-24 18:27:58 -0800},
	date-modified = {2020-05-31 15:11:41 -0700},
	journal = {arXiv preprint arXiv:1911.00567},
	title = {Frequentist Regret Bounds for Randomized Least-Squares Value Iteration},
	year = {2020}}

@conference{russo2019worst,
	author = {Russo, Daniel},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-11-24 18:27:38 -0800},
	date-modified = {2019-12-13 19:06:56 -0800},
	title = {Worst-Case Regret Bounds for Exploration via Randomized Value Functions},
	year = {2019}}

@article{abeille2017linear,
	author = {Abeille, Marc and Lazaric, Alessandro and others},
	date-added = {2019-11-24 18:26:01 -0800},
	date-modified = {2019-11-24 18:26:01 -0800},
	journal = {Electronic Journal of Statistics},
	number = {2},
	pages = {5165--5197},
	publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
	title = {Linear Thompson sampling revisited},
	volume = {11},
	year = {2017}}

@article{qian2018exploration,
	author = {Qian, Jian and Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro},
	date-added = {2019-11-24 17:51:40 -0800},
	date-modified = {2019-11-24 17:51:40 -0800},
	journal = {arXiv preprint arXiv:1812.04363},
	title = {Exploration Bonus for Regret Minimization in Undiscounted Discrete and Continuous Markov Decision Processes},
	year = {2018}}

@conference{efroni2019tight,
	author = {Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, Mohammad and Mannor, Shie},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-11-24 17:43:44 -0800},
	date-modified = {2019-12-13 19:05:38 -0800},
	title = {Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies},
	year = {2019}}

@inproceedings{jin2018q,
	author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-11-24 17:36:05 -0800},
	date-modified = {2019-11-24 17:36:05 -0800},
	pages = {4863--4873},
	title = {Is q-learning provably efficient?},
	year = {2018}}

@article{du2019good,
	author = {Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
	date-added = {2019-11-24 14:25:46 -0800},
	date-modified = {2019-11-24 14:25:46 -0800},
	journal = {arXiv preprint arXiv:1910.03016},
	title = {Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
	year = {2019}}

@conference{yang2020reinforcement,
	author = {Yang, Lin F and Wang, Mengdi},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2019-11-20 10:24:07 -0800},
	date-modified = {2020-06-02 10:37:18 -0700},
	journal = {arXiv preprint arXiv:1905.10389},
	title = {Reinforcement Leaning in Feature Space: Matrix Bandit, Kernels, and Regret Bound},
	year = {2020}}

@conference{jin2020provably,
	author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
	booktitle = {Conference on Learning Theory},
	date-added = {2019-11-20 10:23:44 -0800},
	date-modified = {2020-06-28 16:02:21 -0700},
	journal = {arXiv preprint arXiv:1907.05388},
	title = {Provably efficient reinforcement learning with linear function approximation},
	year = {2020}}

@article{vershynin2010introduction,
	author = {Vershynin, Roman},
	date-added = {2019-10-31 14:17:17 -0700},
	date-modified = {2019-10-31 14:17:17 -0700},
	journal = {arXiv preprint arXiv:1011.3027},
	title = {Introduction to the non-asymptotic analysis of random matrices},
	year = {2010}}

@book{wainwright2019high,
	author = {Wainwright, Martin J},
	date-added = {2019-09-17 14:27:14 -0700},
	date-modified = {2019-09-17 14:27:14 -0700},
	publisher = {Cambridge University Press},
	title = {High-dimensional statistics: A non-asymptotic viewpoint},
	volume = {48},
	year = {2019}}

@conference{zanette19limiting,
	author = {Andrea Zanette and Alessandro Lazaric and Mykel {J. Kochenderfer} and Emma Brunskill},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-07-19 19:32:28 +0200},
	date-modified = {2019-12-13 19:08:58 -0800},
	journal = {under Review},
	title = {Limiting Extrapolation in Linear Approximate Value Iteration},
	year = {2019}}

@inproceedings{zanette2019tighter,
	abstract = {Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory that provide strong problem-dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the barrier to using RL algorithms in practice. As a step towards this we derive an algorithm and analysis for finite horizon discrete MDPs with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL environment has special features but without apriori knowledge of the environment from the algorithm. As a result of our analysis, we also help address an open learning theory question \cite{jiang2018open} about episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound function of the number of episodes with no dependence on the horizon.},
	author = {Zanette, Andrea and Brunskill, Emma},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-added = {2019-07-19 19:13:37 +0200},
	date-modified = {2019-07-19 19:15:28 +0200},
	pdf = {http://proceedings.mlr.press/v97/zanette19a/zanette19a.pdf},
	title = {Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds},
	url = {http://proceedings.mlr.press/v97/zanette19a.html},
	year = {2019},
	bdsk-url-1 = {http://proceedings.mlr.press/v97/zanette19a.html}}

@article{zanette2017enriching,
	author = {Zanette, A and Ferronato, M and Janna, C},
	date-added = {2019-07-19 19:10:04 +0200},
	date-modified = {2019-07-19 19:10:04 +0200},
	journal = {International Journal for Numerical Methods in Engineering},
	number = {7},
	pages = {675--700},
	publisher = {Wiley Online Library},
	title = {Enriching the finite element method with meshfree particles in structural mechanics},
	volume = {110},
	year = {2017}}

@article{zanette2015enriching,
	author = {Zanette, Andrea and Ferronato, Massimiliano and Janna, Carlo},
	date-added = {2019-07-19 19:09:51 +0200},
	date-modified = {2019-07-19 19:09:51 +0200},
	journal = {PAMM},
	number = {1},
	pages = {691--692},
	publisher = {Wiley Online Library},
	title = {Enriching the Finite Element Method with meshfree techniques in structural mechanics},
	volume = {15},
	year = {2015}}

@conference{zanette2019b,
	author = {Andrea Zanette and Emma Brunskill and Mykel {J. Kochenderfer}},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-07-18 11:33:54 +0200},
	date-modified = {2019-12-13 19:08:41 -0800},
	journal = {under Review},
	title = {Almost Horizon-Free Structure-Aware Best Policy Identification with a Generative Model},
	year = {2019}}

@article{simchowitz2019non,
	author = {Simchowitz, Max and Jamieson, Kevin},
	date-added = {2019-05-25 11:47:16 -0700},
	date-modified = {2019-05-25 11:47:16 -0700},
	journal = {arXiv preprint arXiv:1905.03814},
	title = {Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs},
	year = {2019}}

@phdthesis{kakade2003sample,
	author = {Kakade, Sham Machandranath and others},
	date-added = {2019-05-25 11:19:23 -0700},
	date-modified = {2019-05-25 11:19:23 -0700},
	school = {University of London London, England},
	title = {On the sample complexity of reinforcement learning},
	year = {2003}}

@incollection{NIPS2011_4485,
	author = {Farahmand, Amir-massoud},
	booktitle = {Advances in Neural Information Processing Systems 24},
	date-added = {2019-05-24 13:13:14 -0700},
	date-modified = {2019-05-24 13:13:14 -0700},
	editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
	pages = {172--180},
	publisher = {Curran Associates, Inc.},
	title = {Action-Gap Phenomenon in Reinforcement Learning},
	url = {http://papers.nips.cc/paper/4485-action-gap-phenomenon-in-reinforcement-learning.pdf},
	year = {2011},
	bdsk-url-1 = {http://papers.nips.cc/paper/4485-action-gap-phenomenon-in-reinforcement-learning.pdf}}

@article{burnetas1997optimal,
	author = {Burnetas, Apostolos N and Katehakis, Michael N},
	date-added = {2019-05-21 18:49:31 -0700},
	date-modified = {2019-05-21 18:49:31 -0700},
	journal = {Mathematics of Operations Research},
	number = {1},
	pages = {222--255},
	publisher = {INFORMS},
	title = {Optimal adaptive policies for Markov decision processes},
	volume = {22},
	year = {1997}}

@inproceedings{tewari2008optimistic,
	author = {Tewari, Ambuj and Bartlett, Peter L},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-05-21 18:49:09 -0700},
	date-modified = {2019-05-21 18:49:09 -0700},
	pages = {1505--1512},
	title = {Optimistic linear programming gives logarithmic regret for irreducible MDPs},
	year = {2008}}

@inproceedings{ok2018exploration,
	author = {Ok, Jungseul and Proutiere, Alexandre and Tranos, Damianos},
	booktitle = {Advances in Neural Information Processing Systems},
	date-added = {2019-05-21 18:48:30 -0700},
	date-modified = {2019-05-21 18:48:30 -0700},
	pages = {8874--8882},
	title = {Exploration in Structured Reinforcement Learning},
	year = {2018}}

@inproceedings{wang2007dual,
	author = {Wang, Tao and Bowling, Michael and Schuurmans, Dale},
	booktitle = {2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
	date-added = {2019-05-01 21:07:14 -0700},
	date-modified = {2019-05-01 21:07:14 -0700},
	organization = {IEEE},
	pages = {44--51},
	title = {Dual representations for dynamic programming and reinforcement learning},
	year = {2007}}

@inproceedings{munos1999barycentric,
	author = {Munos, Remi and Moore, Andrew W},
	booktitle = nips,
	title = {Barycentric interpolators for continuous space and time reinforcement learning},
	year = {1999}}

@article{ormoneit2002kernel,
	author = {Ormoneit, Dirk and Sen, {\'S}aunak},
	journal = {Machine Learning},
	number = {2-3},
	pages = {161--178},
	title = {Kernel-based reinforcement learning},
	volume = {49},
	year = {2002}}

@inproceedings{melo2008analysis,
	author = {Melo, Francisco S and Meyn, Sean P and Ribeiro, M Isabel},
	booktitle = icml,
	title = {An analysis of reinforcement learning with function approximation},
	year = {2008}}

@article{baxter_2001,
	author = {Baxter, J. and Bartlett, P. L.},
	doi = {10.1613/jair.806},
	journal = jair,
	pages = {319--350},
	title = {Infinite-Horizon Policy-Gradient Estimation},
	volume = {15},
	year = {2001},
	bdsk-url-1 = {http://dx.doi.org/10.1613/jair.806}}

@article{peters2008natural,
	author = {Peters, Jan and Schaal, Stefan},
	date-added = {2019-03-07 10:21:46 -0800},
	date-modified = {2019-03-07 10:21:46 -0800},
	journal = {Neurocomputing},
	number = {7-9},
	pages = {1180--1190},
	title = {Natural actor-critic},
	volume = {71},
	year = {2008}}

@inproceedings{azar2011dynamic,
	author = {Azar, Mohammad Gheshlaghi and Kappen, Bert and others},
	booktitle = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
	date-added = {2019-03-06 21:14:19 -0800},
	date-modified = {2019-03-06 21:14:19 -0800},
	title = {Dynamic policy programming with function approximation},
	year = {2011}}

@incollection{bartlett2003introduction,
	author = {Bartlett, Peter L},
	booktitle = {Advanced Lectures on Machine Learning},
	date-added = {2019-03-06 20:06:15 -0800},
	date-modified = {2019-03-06 20:06:15 -0800},
	pages = {184--202},
	publisher = {Springer},
	title = {An introduction to reinforcement learning theory: Value function methods},
	year = {2003}}

@inproceedings{kolter2011fixed,
	author = {Kolter, J Zico},
	booktitle = nips,
	date-added = {2019-03-06 17:47:27 -0800},
	date-modified = {2019-03-06 17:47:27 -0800},
	pages = {2169--2177},
	title = {The fixed points of off-policy TD},
	year = {2011}}

@article{liu2018proximal,
	author = {Liu, Bo and Gemp, Ian and Ghavamzadeh, Mohammad and Liu, Ji and Mahadevan, Sridhar and Petrik, Marek},
	date-added = {2019-03-06 17:45:18 -0800},
	date-modified = {2019-03-06 17:45:18 -0800},
	journal = jair,
	pages = {461--494},
	title = {Proximal gradient temporal difference learning: Stable reinforcement learning with polynomial sample complexity},
	volume = {63},
	year = {2018}}

@inproceedings{agarwal2014taming,
	author = {Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
	booktitle = {International Conference on Machine Learning},
	pages = {1638--1646},
	title = {Taming the monster: A fast and simple algorithm for contextual bandits},
	year = {2014}}

@inproceedings{sutton2009fast,
	author = {Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
	booktitle = icml,
	date-added = {2019-03-06 17:44:45 -0800},
	date-modified = {2019-03-06 17:44:45 -0800},
	pages = {993--1000},
	title = {Fast gradient-descent methods for temporal-difference learning with linear function approximation},
	year = {2009}}

@article{Ghiassian2018OnlineOP,
	author = {Sina Ghiassian and Andrew Patterson and Martha White and Richard S. Sutton and Adam White},
	date-added = {2019-03-06 16:48:34 -0800},
	date-modified = {2019-03-06 16:48:34 -0800},
	journal = {CoRR},
	title = {Online Off-policy Prediction},
	volume = {abs/1811.02597},
	year = {2018}}

@incollection{gordon1995stable,
	author = {Gordon, Geoffrey J},
	booktitle = icml,
	date-added = {2019-03-06 12:06:45 -0800},
	date-modified = {2019-03-06 12:06:45 -0800},
	pages = {261--268},
	title = {Stable function approximation in dynamic programming},
	year = {1995}}

@inproceedings{gordon1996stable,
	author = {Gordon, Geoffrey J},
	booktitle = nips,
	date-added = {2019-03-06 12:06:45 -0800},
	date-modified = {2019-03-06 12:06:45 -0800},
	title = {Stable fitted reinforcement learning},
	year = {1996}}

@inproceedings{farahmand2010error,
	author = {Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
	booktitle = nips,
	date-added = {2019-03-06 11:59:52 -0800},
	date-modified = {2019-03-06 11:59:52 -0800},
	title = {Error propagation for approximate policy and value iteration},
	year = {2010}}

@article{munos2008finite,
	author = {Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
	date-added = {2019-03-06 11:20:54 -0800},
	date-modified = {2019-03-06 11:20:54 -0800},
	journal = jmlr,
	number = {May},
	pages = {815--857},
	title = {Finite-time bounds for fitted value iteration},
	volume = {9},
	year = {2008}}

@inproceedings{munos2005error,
	author = {Munos, R{\'e}mi},
	booktitle = aaai,
	date-added = {2019-03-06 09:47:41 -0800},
	date-modified = {2019-03-06 09:47:41 -0800},
	title = {Error bounds for approximate value iteration},
	year = {2005}}

@book{friedman2001elements,
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	date-added = {2019-03-06 09:44:43 -0800},
	date-modified = {2019-03-06 09:44:43 -0800},
	number = {10},
	publisher = {Springer},
	title = {The Elements of Statistical Learning},
	year = {2001}}

@book{powell2007approximate,
	author = {Powell, Warren B},
	date-added = {2019-03-06 09:28:01 -0800},
	date-modified = {2019-03-06 09:28:01 -0800},
	publisher = {Wiley},
	title = {Approximate Dynamic Programming: Solving the curses of dimensionality},
	year = {2007}}

@book{bertsekas1996neuro,
	author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
	date-added = {2019-03-06 09:25:32 -0800},
	date-modified = {2019-03-06 09:25:32 -0800},
	publisher = {Athena Scientific},
	title = {Neuro-dynamic programming},
	year = {1996}}

@book{trefethen1997numerical,
	author = {Trefethen, Lloyd N and Bau III, David},
	date-added = {2019-03-04 20:35:07 -0800},
	date-modified = {2019-03-04 20:35:07 -0800},
	publisher = {SIAM},
	title = {Numerical Linear Algebra},
	year = {1997}}

@book{golub2012matrix,
	author = {Golub, Gene H and Van Loan, Charles F},
	date-added = {2019-03-04 20:34:36 -0800},
	date-modified = {2019-03-04 20:34:36 -0800},
	publisher = {JHU Press},
	title = {Matrix Computations},
	year = {2012}}

@inproceedings{soare2014best,
	author = {Soare, Marta and Lazaric, Alessandro and Munos, R{\'e}mi},
	booktitle = nips,
	date-added = {2019-03-04 19:55:28 -0800},
	date-modified = {2019-03-04 19:55:28 -0800},
	pages = {828--836},
	title = {Best-arm identification in linear bandits},
	year = {2014}}

@inproceedings{tsitsiklis1997analysis,
	author = {Tsitsiklis, John N and Van Roy, Benjamin},
	booktitle = nips,
	title = {Analysis of temporal-diffference learning with function approximation},
	year = {1997}}

@inproceedings{kumaraswamy2018context,
	author = {Kumaraswamy, Raksha and Schlegel, Matthew and White, Adam and White, Martha},
	booktitle = nips,
	title = {Context-dependent upper-confidence bounds for directed exploration},
	year = {2018}}

@incollection{baird1995residual,
	author = {Baird, Leemon},
	booktitle = icml,
	title = {Residual algorithms: Reinforcement learning with function approximation},
	year = {1995}}

@article{tsitsiklis1996feature,
	author = {Tsitsiklis, John N and Van Roy, Benjamin},
	date-added = {2019-01-14 07:43:45 -0800},
	date-modified = {2019-01-14 07:43:45 -0800},
	journal = {Machine Learning},
	number = {1-3},
	pages = {59--94},
	title = {Feature-based methods for large scale dynamic programming},
	volume = {22},
	year = {1996}}

@inproceedings{boyan1995generalization,
	author = {Boyan, Justin A and Moore, Andrew W},
	booktitle = nips,
	title = {Generalization in reinforcement learning: Safely approximating the value function},
	year = {1995}}

@inproceedings{city5203,
	abstract = {This paper gives specific divergence examples of value-iteration for several major Reinforcement Learning and Adaptive Dynamic Programming algorithms, when using a function approximator for the value function. These divergence examples differ from previous divergence examples in the literature, in that they are applicable for a greedy policy, i.e. in a ?value iteration? scenario. Perhaps surprisingly, with a greedy policy, it is also possible to get divergence for the algorithms TD(1) and Sarsa(1). In addition to these divergences, we also achieve divergence for the Adaptive Dynamic Programming algorithms HDP, DHP and GDHP.},
	author = {M. Fairbank and E. Alonso},
	booktitle = {International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2019-01-14 07:31:12 -0800},
	date-modified = {2019-01-14 07:31:12 -0800},
	doi = {10.1109/IJCNN.2012.6252792},
	keywords = {Adaptive Dynamic Programming, Reinforcement Learning, Greedy Policy, Value Iteration, Divergence},
	note = {{\copyright} 2012 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.},
	publisher = {IEEE Press},
	title = {The divergence of reinforcement learning algorithms with value-iteration and function approximation},
	url = {http://openaccess.city.ac.uk/5203/},
	year = {2012},
	bdsk-url-1 = {http://openaccess.city.ac.uk/5203/},
	bdsk-url-2 = {https://doi.org/10.1109/IJCNN.2012.6252792}}

@inproceedings{osband2014near,
	author = {Osband, Ian and Van Roy, Benjamin},
	booktitle = nips,
	title = {Near-optimal reinforcement learning in factored MDPs},
	year = {2014}}

@inproceedings{bellemare2016unifying,
	author = {Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
	booktitle = nips,
	title = {Unifying count-based exploration and intrinsic motivation},
	year = {2016}}

@article{silver2017mastering,
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
	journal = {Nature},
	number = {7676},
	pages = {354},
	title = {Mastering the game of {G}o without human knowledge},
	volume = {550},
	year = {2017}}

@article{silver2016mastering,
	author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
	date-added = {2019-01-14 05:31:27 -0800},
	date-modified = {2019-01-14 05:31:27 -0800},
	journal = {Nature},
	number = {7587},
	pages = {484},
	title = {Mastering the game of Go with deep neural networks and tree search},
	volume = {529},
	year = {2016}}

@inproceedings{osband2016deep,
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	booktitle = nips,
	title = {Deep exploration via bootstrapped {DQN}},
	year = {2016}}

@inproceedings{osband2016generalization,
	author = {Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
	booktitle = icml,
	title = {Generalization and Exploration via Randomized Value Functions},
	year = {2016}}

@article{hossain2013constructing,
	author = {Hossain, M Zahid and Amin, M Ashraful},
	date-added = {2019-01-11 14:28:30 +0100},
	date-modified = {2019-01-11 14:28:30 +0100},
	journal = {American Journal of Computational Mathematics},
	number = {1},
	pages = {11},
	title = {On constructing approximate convex hull},
	volume = {3},
	year = {2013}}

@misc{sartipizadeh2016computing,
	archiveprefix = {arXiv},
	author = {Hossein Sartipizadeh and Tyrone L. Vincent},
	date-added = {2019-01-11 14:26:05 +0100},
	date-modified = {2019-01-11 14:26:05 +0100},
	eprint = {1603.04422},
	primaryclass = {cs.CG},
	title = {Computing the Approximate Convex Hull in High Dimensions},
	year = {2016}}

@misc{blum2017approximate,
	archiveprefix = {arXiv},
	author = {Avrim Blum and Vladimir Braverman and Ananya Kumar and Harry Lang and Lin F. Yang},
	date-added = {2019-01-11 14:24:06 +0100},
	date-modified = {2019-01-11 14:24:06 +0100},
	eprint = {1712.04564},
	primaryclass = {cs.CG},
	title = {Approximate Convex Hull of Data Streams},
	year = {2017}}

@misc{graham2017approximate,
	archiveprefix = {arXiv},
	author = {Robert Graham and Adam M. Oberman},
	date-added = {2019-01-11 14:11:53 +0100},
	date-modified = {2019-01-11 14:11:53 +0100},
	eprint = {1703.01350},
	primaryclass = {cs.CG},
	title = {Approximate Convex Hulls: sketching the convex hull using curvature},
	year = {2017}}

@book{boyd2004convex,
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	date-added = {2019-01-11 11:55:54 +0100},
	date-modified = {2019-01-11 11:55:54 +0100},
	publisher = {Cambridge University Press},
	title = {Convex Optimization},
	year = {2004}}

@inproceedings{auer2007logarithmic,
	author = {Auer, Peter and Ortner, Ronald},
	booktitle = nips,
	title = {Logarithmic online regret bounds for undiscounted reinforcement learning},
	year = {2007}}

@inproceedings{krishnamurthy2016pac,
	author = {Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
	booktitle = nips,
	date-added = {2019-01-09 17:51:10 +0100},
	date-modified = {2019-01-09 17:51:10 +0100},
	pages = {1840--1848},
	title = {PAC reinforcement learning with rich observations},
	year = {2016}}

@inproceedings{kocsis2006bandit,
	author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
	booktitle = ecml,
	date-added = {2019-01-09 17:49:40 +0100},
	date-modified = {2019-01-09 17:49:40 +0100},
	pages = {282--293},
	title = {Bandit based monte-carlo planning},
	year = {2006}}

@article{kearns2002sparse,
	author = {Kearns, Michael and Mansour, Yishay and Ng, Andrew Y},
	date-added = {2019-01-09 17:44:41 +0100},
	date-modified = {2019-01-09 17:44:41 +0100},
	journal = {Machine Learning},
	number = {2-3},
	pages = {193--208},
	publisher = {Springer},
	title = {A sparse sampling algorithm for near-optimal planning in large Markov decision processes},
	volume = {49},
	year = {2002}}

@article{dimakopoulou2018scalable,
	author = {Dimakopoulou, Maria and Osband, Ian and Van Roy, Benjamin},
	date-added = {2019-01-09 17:36:39 +0100},
	date-modified = {2019-01-09 17:36:39 +0100},
	journal = {arXiv preprint arXiv:1805.08948},
	title = {Scalable Coordinated Exploration in Concurrent Reinforcement Learning},
	year = {2018}}

@inproceedings{pazis2016efficient,
	author = {Pazis, Jason and Parr, Ronald},
	booktitle = aaai,
	date-added = {2019-01-09 17:33:22 +0100},
	date-modified = {2019-01-09 17:33:22 +0100},
	pages = {1977--1985},
	title = {Efficient PAC-Optimal Exploration in Concurrent, Continuous State MDPs with Delayed Updates.},
	year = {2016}}

@inproceedings{jong2007model,
	author = {Jong, Nicholas K and Stone, Peter},
	booktitle = {International Symposium on Abstraction, Reformulation, and Approximation},
	date-added = {2019-01-09 17:31:10 +0100},
	date-modified = {2019-01-09 17:31:10 +0100},
	organization = {Springer},
	pages = {258--272},
	title = {Model-based exploration in continuous state spaces},
	year = {2007}}

@inproceedings{kakade2003exploration,
	author = {Sham M. Kakade and Michael Kearns and John Langford},
	booktitle = icml,
	date-added = {2019-01-09 17:17:33 +0100},
	date-modified = {2019-01-09 17:17:45 +0100},
	title = {Exploration in Metric State Spaces},
	year = {2003}}

@inproceedings{pazis2013pac,
	acmid = {2891568},
	author = {Pazis, Jason and Parr, Ronald},
	booktitle = aaai,
	date-added = {2019-01-09 16:58:37 +0100},
	date-modified = {2019-01-09 16:58:52 +0100},
	location = {Bellevue, Washington},
	numpages = {8},
	pages = {774--781},
	title = {PAC Optimal Exploration in Continuous Space Markov Decision Processes},
	url = {http://dl.acm.org/citation.cfm?id=2891460.2891568},
	year = {2013},
	bdsk-url-1 = {http://dl.acm.org/citation.cfm?id=2891460.2891568}}

@inproceedings{chen2018scalable,
	author = {Yichen Chen and Lihong Li and Mengdi Wang},
	booktitle = icml,
	date-added = {2019-01-09 15:58:56 +0100},
	date-modified = {2019-01-09 16:37:28 +0100},
	title = {Scalable Bilinear pi-Learning Using State and Action Features},
	year = {2018}}

@article{sun2018model,
	author = {Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
	date-added = {2019-01-09 15:32:53 +0100},
	date-modified = {2019-01-09 15:32:53 +0100},
	journal = {arXiv preprint arXiv:1811.08540},
	title = {Model-Based Reinforcement Learning in Contextual Decision Processes},
	year = {2018}}

@inproceedings{dann2018oracle,
	author = {Dann, Christoph and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
	booktitle = nips,
	date-added = {2019-01-09 14:19:50 +0100},
	date-modified = {2019-01-09 14:19:50 +0100},
	pages = {1429--1439},
	title = {On Oracle-Efficient PAC RL with Rich Observations},
	year = {2018}}

@inproceedings{jiang17contextual,
	abstract = {This paper studies systematic exploration for reinforcement learning (RL) with rich observations and function approximation. We introduce contextual decision processes (CDPs), that unify most prior RL settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exploration to learn near-optimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samples that is polynomial in all relevant parameters but independent of the number of unique contexts. Our approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for RL with function approximation.},
	address = {International Convention Centre, Sydney, Australia},
	author = {Nan Jiang and Akshay Krishnamurthy and Alekh Agarwal and John Langford and Robert E. Schapire},
	booktitle = icml,
	date-added = {2019-01-09 14:17:31 +0100},
	date-modified = {2019-01-09 14:17:44 +0100},
	editor = {Doina Precup and Yee Whye Teh},
	month = {06--11 Aug},
	pages = {1704--1713},
	pdf = {http://proceedings.mlr.press/v70/jiang17c/jiang17c.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Contextual Decision Processes with low {B}ellman rank are {PAC}-Learnable},
	url = {http://proceedings.mlr.press/v70/jiang17c.html},
	volume = {70},
	year = {2017},
	bdsk-url-1 = {http://proceedings.mlr.press/v70/jiang17c.html}}

@inproceedings{lattimore2012pac,
	author = {Lattimore, Tor and Hutter, Marcus},
	booktitle = {International Conference on Algorithmic Learning Theory},
	date-added = {2019-01-06 16:54:27 +0100},
	date-modified = {2019-01-06 16:54:27 +0100},
	organization = {Springer},
	pages = {320--334},
	title = {{PAC} bounds for discounted {MDP}s},
	year = {2012}}

@inproceedings{mansour1999complexity,
	author = {Mansour, Yishay and Singh, Satinder},
	booktitle = uai,
	date-added = {2019-01-06 16:53:50 +0100},
	date-modified = {2019-01-06 16:53:50 +0100},
	organization = {Morgan Kaufmann},
	pages = {401--408},
	title = {On the complexity of policy iteration},
	year = {1999}}

@inproceedings{radlinski2008learning,
	author = {Radlinski, Filip and Kleinberg, Robert and Joachims, Thorsten},
	booktitle = icml,
	date-added = {2019-01-05 18:20:33 +0100},
	date-modified = {2019-01-05 18:20:33 +0100},
	organization = {ACM},
	pages = {784--791},
	title = {Learning diverse rankings with multi-armed bandits},
	year = {2008}}

@inproceedings{pandey2007bandits,
	author = {Pandey, Sandeep and Agarwal, Deepak and Chakrabarti, Deepayan and Josifovski, Vanja},
	booktitle = {SIAM International Conference on Data Mining},
	date-added = {2019-01-05 18:20:02 +0100},
	date-modified = {2019-01-05 18:20:02 +0100},
	organization = {SIAM},
	pages = {216--227},
	title = {Bandits for taxonomies: A model-based approach},
	year = {2007}}

@inproceedings{agarwal2009explore,
	author = {Agarwal, Deepak and Chen, Bee-Chung and Elango, Pradheep},
	booktitle = {IEEE International Conference on Data Mining},
	title = {Explore/exploit schemes for web content optimization},
	year = {2009}}

@inproceedings{chakrabarti2009mortal,
	author = {Chakrabarti, Deepayan and Kumar, Ravi and Radlinski, Filip and Upfal, Eli},
	booktitle = nips,
	date-added = {2019-01-05 18:18:53 +0100},
	date-modified = {2019-01-05 18:18:53 +0100},
	pages = {273--280},
	title = {Mortal multi-armed bandits},
	year = {2009}}

@article{kearns2002near,
	author = {Kearns, Michael and Singh, Satinder},
	date-added = {2019-01-05 12:10:56 +0100},
	date-modified = {2019-01-05 12:10:56 +0100},
	journal = {Machine Learning},
	number = {2-3},
	pages = {209--232},
	publisher = {Springer},
	title = {Near-optimal reinforcement learning in polynomial time},
	volume = {49},
	year = {2002}}

@article{mannor2004sample,
	author = {Mannor, Shie and Tsitsiklis, John N},
	date-added = {2019-01-04 18:31:47 +0100},
	date-modified = {2019-01-04 18:31:47 +0100},
	journal = jmlr,
	number = {Jun},
	pages = {623--648},
	title = {The sample complexity of exploration in the multi-armed bandit problem},
	volume = {5},
	year = {2004}}

@book{sutton2018reinforcement,
	author = {Sutton, Richard S and Barto, Andrew G},
	date-added = {2019-01-04 13:31:11 +0100},
	date-modified = {2019-01-04 13:31:11 +0100},
	publisher = {MIT Press},
	title = {Reinforcement learning: An introduction},
	year = {2018}}

@article{liu2018simple,
	author = {Liu, Yao and Brunskill, Emma},
	date-added = {2018-12-28 20:30:04 +0100},
	date-modified = {2018-12-28 20:30:04 +0100},
	journal = {arXiv preprint arXiv:1805.09045},
	title = {When Simple Exploration is Sample Efficient: Identifying Sufficient Conditions for Random Exploration to Yield PAC RL Algorithms},
	year = {2018}}

@article{chen2017nearly,
	author = {Chen, Lijie and Li, Jian and Qiao, Mingda},
	date-added = {2018-12-28 19:51:10 +0100},
	date-modified = {2018-12-28 19:51:10 +0100},
	journal = {arXiv preprint arXiv:1702.03605},
	title = {Nearly instance optimal sample complexity bounds for top-k arm selection},
	year = {2017}}

@inproceedings{karnin2013almost,
	author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
	booktitle = icml,
	date-added = {2018-12-28 16:15:07 +0100},
	date-modified = {2018-12-28 16:15:07 +0100},
	title = {Almost optimal exploration in multi-armed bandits},
	year = {2013}}

@article{kaufmann2016complexity,
	author = {Kaufmann, Emilie and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
	date-added = {2018-12-28 16:07:58 +0100},
	date-modified = {2018-12-28 16:07:58 +0100},
	journal = jmlr,
	number = {1},
	pages = {1--42},
	publisher = {JMLR. org},
	title = {On the complexity of best-arm identification in multi-armed bandit models},
	volume = {17},
	year = {2016}}

@inproceedings{jamieson2014lil,
	author = {Jamieson, Kevin and Malloy, Matthew and Nowak, Robert and Bubeck, S{\'e}bastien},
	booktitle = colt,
	date-added = {2018-12-28 15:58:12 +0100},
	date-modified = {2018-12-28 15:58:12 +0100},
	pages = {423--439},
	title = {lil'ucb: An optimal exploration algorithm for multi-armed bandits},
	year = {2014}}

@inproceedings{jiang2016structural,
	author = {Jiang, Nan and Singh, Satinder P and Tewari, Ambuj},
	booktitle = ijcai,
	date-added = {2018-12-24 17:30:55 +0100},
	date-modified = {2018-12-24 17:30:55 +0100},
	pages = {1640--1647},
	title = {On Structural Properties of MDPs that Bound Loss Due to Shallow Planning.},
	year = {2016}}

@article{ortner2018regret,
	author = {Ortner, Ronald},
	date-added = {2018-12-24 11:36:33 +0100},
	date-modified = {2018-12-24 11:36:33 +0100},
	journal = {arXiv preprint arXiv:1808.01813},
	title = {Regret Bounds for Reinforcement Learning via Markov Chain Concentration},
	year = {2018}}

@article{strehl2009reinforcement,
	author = {Strehl, Alexander L and Li, Lihong and Littman, Michael L},
	date-added = {2018-12-24 11:22:17 +0100},
	date-modified = {2018-12-24 11:22:17 +0100},
	journal = jmlr,
	number = {Nov},
	pages = {2413--2444},
	title = {Reinforcement learning in finite MDPs: PAC analysis},
	volume = {10},
	year = {2009}}

@article{lattimore2014near,
	author = {Lattimore, Tor and Hutter, Marcus},
	date-added = {2018-12-24 11:21:07 +0100},
	date-modified = {2018-12-24 11:21:07 +0100},
	journal = {Theoretical Computer Science},
	pages = {125--143},
	publisher = {Elsevier},
	title = {Near-optimal {PAC} bounds for discounted {MDP}s},
	volume = {558},
	year = {2014}}

@inproceedings{carpentier2016tight,
	author = {Carpentier, Alexandra and Locatelli, Andrea},
	booktitle = colt,
	date-added = {2018-12-24 11:06:45 +0100},
	date-modified = {2018-12-24 11:06:45 +0100},
	pages = {590--604},
	title = {Tight (lower) bounds for the fixed budget best arm identification bandit problem},
	year = {2016}}

@inproceedings{bubeck2009pure,
	author = {Bubeck, S{\'e}bastien and Munos, R{\'e}mi and Stoltz, Gilles},
	booktitle = {International Conference on Algorithmic Learning Theory},
	date-added = {2018-12-24 10:43:25 +0100},
	date-modified = {2018-12-24 10:43:25 +0100},
	title = {Pure exploration in multi-armed bandits problems},
	year = {2009}}

@inproceedings{mnih2008empirical,
	author = {Mnih, Volodymyr and Szepesv{\'a}ri, Csaba and Audibert, Jean-Yves},
	booktitle = icml,
	date-added = {2018-12-24 10:36:50 +0100},
	date-modified = {2018-12-24 10:36:50 +0100},
	organization = {ACM},
	pages = {672--679},
	title = {Empirical bernstein stopping},
	year = {2008}}

@inproceedings{maron1994hoeffding,
	author = {Maron, Oded and Moore, Andrew W},
	booktitle = nips,
	date-added = {2018-12-24 10:33:57 +0100},
	date-modified = {2018-12-24 10:33:57 +0100},
	pages = {59--66},
	title = {Hoeffding races: Accelerating model selection search for classification and function approximation},
	year = {1994}}

@inproceedings{gabillon2012best,
	author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro},
	booktitle = nips,
	date-added = {2018-12-24 10:24:58 +0100},
	date-modified = {2018-12-24 10:24:58 +0100},
	pages = {3212--3220},
	title = {Best arm identification: A unified approach to fixed budget and fixed confidence},
	year = {2012}}

@book{nocedal2006numerical,
	author = {Nocedal, Jorge and Wright, Stephen},
	publisher = {Springer},
	title = {Numerical Optimization},
	year = {2006}}

@inproceedings{gabillon2011multi,
	author = {Gabillon, Victor and Ghavamzadeh, Mohammad and Lazaric, Alessandro and Bubeck, S{\'e}bastien},
	booktitle = nips,
	date-added = {2018-12-24 10:18:40 +0100},
	date-modified = {2018-12-24 10:18:40 +0100},
	pages = {2222--2230},
	title = {Multi-bandit best arm identification},
	year = {2011}}

@inproceedings{brunskill2010policies,
	author = {Brunskill, Emma},
	booktitle = icaps,
	date-added = {2018-12-14 05:52:28 +0100},
	date-modified = {2018-12-14 05:52:28 +0100},
	pages = {218--221},
	title = {When Policies Can Be Trusted: Analyzing a Criteria to Identify Optimal Policies in MDPs with Unknown Model Parameters.},
	year = {2010}}

@conference{yang2019sample,
	author = {Yang, Lin F and Wang, Mengdi},
	booktitle = {International Conference on Machine Learning (ICML)},
	date-modified = {2019-12-13 19:07:53 -0800},
	journal = {arXiv preprint arXiv:1902.04779},
	title = {Sample-Optimal Parametric Q-Learning with Linear Transition Models},
	year = {2019}}

@inproceedings{jiang2018open,
	author = {Jiang, Nan and Agarwal, Alekh},
	booktitle = colt,
	date-added = {2018-12-10 13:48:34 -0800},
	date-modified = {2018-12-10 13:48:34 -0800},
	pages = {3395--3398},
	title = {Open Problem: The Dependence of Sample Complexity Lower Bounds on Planning Horizon},
	year = {2018}}

@article{white1993,
	author = {White, Douglas J},
	date-added = {2018-07-27 19:25:17 +0200},
	date-modified = {2018-07-27 19:25:24 +0200},
	journal = {Journal of the Operational Research Society},
	number = {11},
	pages = {1073--1096},
	publisher = {Taylor \& Francis},
	title = {A survey of applications of Markov decision processes},
	volume = {44},
	year = {1993}}

@inproceedings{petrusel2013,
	author = {Petrusel, Razvan},
	booktitle = {International Conference on Business Information Systems},
	date-added = {2018-07-27 19:20:02 +0200},
	date-modified = {2018-07-27 19:20:08 +0200},
	organization = {Springer},
	pages = {125--137},
	title = {Using {M}arkov decision process for recommendations based on aggregated decision data models},
	year = {2013}}

@article{espinosa2010,
	author = {Espinosa, Enrique D and Frausto, Juan and Rivera, Ernesto J},
	date-added = {2018-07-27 19:17:58 +0200},
	date-modified = {2018-07-27 19:18:05 +0200},
	journal = {Service Science},
	number = {4},
	pages = {245--269},
	title = {Markov decision processes for optimizing human workflows},
	volume = {2},
	year = {2010}}

@inproceedings{codemo2013,
	author = {Codemo, Claudio G and Erseghe, Tomaso and Zanella, Andrea},
	booktitle = {IEEE International Conference on Communications (ICC)},
	date-added = {2018-07-27 17:11:25 +0000},
	date-modified = {2018-07-27 17:11:34 +0000},
	title = {Energy storage optimization strategies for smart grids},
	year = {2013}}

@inproceedings{li2008,
	author = {Li, Yingzi and Niu, Jincang and Luan, Ru and Yue, Yuntao},
	booktitle = {IEEE International Conference on Electrical Machines and Systems},
	date-added = {2018-07-27 17:01:37 +0000},
	date-modified = {2018-07-27 17:01:45 +0000},
	title = {Research of multi-power structure optimization for grid-connected photovoltaic system based on Markov decision-making model},
	year = {2008}}

@inproceedings{kim2014,
	author = {Kim, Junhyuk and Kong, Peng-Yong and Song, Nah-Oak and Rhee, June-Koo Kevin and Al-Araji, Saleh},
	booktitle = {IEEE Wireless Communications and Networking Conference (WCNC)},
	date-added = {2018-07-27 16:20:44 +0000},
	date-modified = {2018-07-27 16:20:49 +0000},
	title = {{MDP} based dynamic base station management for power conservation in self-organizing networks},
	year = {2014}}

@inproceedings{Zanette18b,
	author = {Andrea Zanette and Junzi Zhang and Mykel {J. Kochenderfer}},
	booktitle = ecml,
	date-added = {2018-07-26 14:31:43 +0000},
	date-modified = {2018-10-03 09:56:27 -0700},
	title = {Robust Super-Level Set Estimation using Gaussian Processes},
	year = {2018}}

@inproceedings{Sidford18,
	author = {Aaron Sidford and Mengdi Wang and Xian Wu and Lin F. Yang and Yinyu Ye},
	booktitle = nips,
	title = {Near-Optimal Time and Sample Complexities for for Solving Discounted Markov Decision Process with a Generative Model},
	year = {2018}}

@inproceedings{Abbasi11,
	author = {Yasin Abbasi-Yadkori and David Pal and Csaba Szepesvari},
	booktitle = nips,
	date-added = {2018-06-07 20:43:50 +0000},
	date-modified = {2018-06-07 20:44:28 +0000},
	title = {Improved Algorithms for Linear Stochastic Bandits},
	year = {2011}}

@inproceedings{Bubeck2012,
	abstract = {We present a new bandit algorithm, SAO (Stochastic and Adversarial Optimal) whose regret is (essentially) optimal both for adversarial rewards and for stochastic rewards. Specifically, SAO combines the \emphO(\emphn) worst-case regret of Exp3 (Auer et al., 2002b) and the (poly)logarithmic regret of UCB1 (Auer et al., 2002a) for stochastic rewards. Adversarial rewards and stochastic rewards are the two main settings in the literature on multi-armed bandits (MAB). Prior work on MAB treats them separately, and does not attempt to jointly optimize for both. This result falls into the general agenda to design algorithms that combine the optimal worst-case performance with improved guarantees for ``nice'' problem instances.},
	author = {S{\'e}bastien Bubeck and Aleksandrs Slivkins},
	booktitle = colt,
	date-added = {2018-06-06 21:48:05 +0000},
	date-modified = {2018-06-06 22:01:52 +0000},
	title = {The Best of Both Worlds: Stochastic and Adversarial Bandits},
	year = {2012},
	bdsk-url-1 = {http://proceedings.mlr.press/v23/bubeck12b.html}}

@inproceedings{Dann17,
	author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
	booktitle = nips,
	date-added = {2018-06-04 20:32:01 +0000},
	date-modified = {2018-06-04 21:23:52 +0000},
	title = {Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning},
	year = {2017},
	bdsk-url-1 = {http://papers.nips.cc/paper/7154-unifying-pac-and-regret-uniform-pac-bounds-for-episodic-reinforcement-learning.pdf}}

@article{antos2008learning,
	author = {Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
	date-added = {2018-05-27 06:02:22 +0000},
	date-modified = {2018-05-27 06:02:22 +0000},
	journal = {Machine Learning},
	number = {1},
	pages = {89--129},
	publisher = {Springer},
	title = {Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path},
	volume = {71},
	year = {2008}}

@inproceedings{maei2010toward,
	author = {Maei, Hamid Reza and Szepesv{\'a}ri, Csaba and Bhatnagar, Shalabh and Sutton, Richard S},
	booktitle = icml,
	date-added = {2018-05-27 05:51:46 +0000},
	date-modified = {2018-05-27 05:51:46 +0000},
	pages = {719--726},
	title = {Toward off-policy learning control with function approximation.},
	year = {2010}}

@article{burnetas1997,
	author = {Burnetas, Apostolos N and Katehakis, Michael N},
	date-added = {2018-05-27 05:36:33 +0000},
	date-modified = {2018-05-27 05:36:40 +0000},
	journal = {Mathematics of Operations Research},
	number = {1},
	pages = {222--255},
	publisher = {INFORMS},
	title = {Optimal adaptive policies for Markov decision processes},
	volume = {22},
	year = {1997}}

@inproceedings{Agrawal2017,
	author = {Agrawal, Shipra and Jia, Randy},
	booktitle = nips,
	date-added = {2018-05-27 05:31:36 +0000},
	date-modified = {2018-05-27 05:32:17 +0000},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {1184--1194},
	publisher = {Curran Associates, Inc.},
	title = {Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
	url = {http://papers.nips.cc/paper/6718-optimistic-posterior-sampling-for-reinforcement-learning-worst-case-regret-bounds.pdf},
	year = {2017},
	bdsk-url-1 = {http://papers.nips.cc/paper/6718-optimistic-posterior-sampling-for-reinforcement-learning-worst-case-regret-bounds.pdf}}

@inproceedings{WR13,
	author = {Zheng Wen and Benjamin {Van Roy}},
	booktitle = nips,
	date-added = {2018-05-17 18:04:11 +0000},
	date-modified = {2018-05-17 18:07:05 +0000},
	title = {Efficient Exploration and Value Function Generalization in Deterministic Systems},
	year = {2013}}

@article{BLLLN09,
	author = {Emma Brunskill and {Bethany R.} Leffler and Lihong Li and {Michael L.} Littman and Nicholas Roy},
	date-added = {2018-05-17 17:58:35 +0000},
	date-modified = {2018-05-17 18:00:55 +0000},
	journal = jmlr,
	title = {Provably Efficient Learning with Typed Parametric Models},
	year = {2009}}

@inproceedings{BLLLR08,
	author = {Emma Brunskill and Bethany Leffler and Lihong Li and {Michael L.} Littman and Nicholas Roy},
	booktitle = uai,
	date-added = {2018-05-17 17:55:34 +0000},
	date-modified = {2018-05-17 17:57:40 +0000},
	title = {CORL: A Continuous-state Offset-dynamics Reinforcement Learner},
	year = {2008}}

@inproceedings{OVR13,
	author = {Ian Osband and Benjamin {Van Roy} and Daniel Russo},
	booktitle = nips,
	date-added = {2018-05-17 05:08:17 +0000},
	date-modified = {2018-05-17 05:09:10 +0000},
	title = {(More) Efficient Reinforcement Learning via Posterior Sampling},
	year = {2013}}

@inproceedings{ABM10,
	author = {{Jean-Yves} Audibert and Sebastien Bubeck and Remi Munos},
	booktitle = colt,
	date-added = {2018-05-16 22:14:50 +0000},
	date-modified = {2018-05-17 05:35:38 +0000},
	title = {Best Arm Identification in Multi-Armed Bandits},
	year = {2010}}

@inproceedings{Zanette18a,
	author = {Andrea Zanette and Emma Brunskill},
	booktitle = icml,
	date-added = {2018-05-16 19:10:09 +0000},
	date-modified = {2018-10-03 09:56:22 -0700},
	title = {Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs},
	year = {2018}}

@inproceedings{Azar12,
	author = {{Mohammad Gheshlaghi} Azar and Remi Munos and Hilbert J. Kappen},
	booktitle = icml,
	date-added = {2018-05-16 19:06:13 +0000},
	date-modified = {2018-06-14 04:05:16 +0000},
	title = {On the Sample Complexity of Reinforcement Learning with a Generative Model},
	year = {2012}}

@article{Azar2013_journal,
	abstract = {We consider the problems of learning the optimal action-value function and the optimal policy in discounted-reward Markov decision processes (MDPs). We prove new PAC bounds on the sample-complexity of two well-known model-based reinforcement learning (RL) algorithms in the presence of a generative model of the MDP: value iteration and policy iteration. The first result indicates that for an MDP with N state-action pairs and the discount factor $\gamma$[0,1) only O(Nlog(N/$\delta$)/((1$\gamma$)3$\epsilon$2)) state-transition samples are required to find an $\epsilon$-optimal estimation of the action-value function with the probability (w.p.) 1$\delta$. Further, we prove that, for small values of $\epsilon$, an order of O(Nlog(N/$\delta$)/((1$\gamma$)3$\epsilon$2)) samples is required to find an $\epsilon$-optimal policy w.p. 1$\delta$. We also prove a matching lower bound of $\Theta$(Nlog(N/$\delta$)/((1$\gamma$)3$\epsilon$2)) on the sample complexity of estimating the optimal action-value function with $\epsilon$ accuracy. To the best of our knowledge, this is the first minimax result on the sample complexity of RL: the upper bounds match the lower bound in terms of N, $\epsilon$, $\delta$ and 1/(1$\gamma$) up to a constant factor. Also, both our lower bound and upper bound improve on the state-of-the-art in terms of their dependence on 1/(1$\gamma$).},
	author = {Gheshlaghi Azar, Mohammad and Munos, R{\'e}mi and Kappen, Hilbert J.},
	day = {01},
	doi = {10.1007/s10994-013-5368-1},
	journal = {Machine Learning},
	month = {06},
	number = {3},
	pages = {325--349},
	title = {Minimax PAC bounds on the sample complexity of reinforcement learning with a generative model},
	url = {https://doi.org/10.1007/s10994-013-5368-1},
	volume = {91},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s10994-013-5368-1}}

@inproceedings{OV17,
	author = {Ian Osband and Benjamin Van Roy},
	booktitle = icml,
	date-added = {2018-05-13 04:00:45 +0000},
	date-modified = {2018-05-13 04:02:20 +0000},
	title = {Why is Posterior Sampling Better than Optimism for Reinforcement Learning?},
	year = {2017}}

@inproceedings{OV14,
	author = {Ian Osband and Benjamin Van Roy},
	booktitle = nips,
	date-added = {2018-05-11 01:09:01 +0000},
	date-modified = {2018-05-11 01:09:37 +0000},
	title = {Model-based Reinforcement Learning and the Eluder Dimension},
	year = {2014}}

@article{Audibert09,
	author = {Jean Yves Audibert and Remi Munos and Csaba Szepesvari},
	date-added = {2018-05-10 17:24:04 +0000},
	date-modified = {2018-10-02 12:13:21 -0700},
	journal = {Theoretical Computer Science},
	title = {Exploration-exploitation trade-off using variance estimates in multi-armed bandits},
	year = {2009}}

@article{KWY18,
	author = {Sham Kakade and Mengdi Wang and Lin F. Yang},
	date-added = {2018-04-24 22:55:55 +0000},
	date-modified = {2018-05-17 06:49:41 +0000},
	journal = {Arxiv},
	title = {Variance Reduction Methods for Sublinear Reinforcement Learning},
	year = {2018}}

@inproceedings{TM18,
	author = {{Mohammad Sadegh} Talebi and {Odalric-Ambrym} Maillard},
	booktitle = {ALT},
	date-added = {2018-04-24 22:48:09 +0000},
	date-modified = {2018-04-24 22:59:06 +0000},
	title = {Variance-Aware Regret Bounds for Undiscounted Reinforcement Learning in MDPs},
	year = {2018}}

@unpublished{Fruit18,
	author = {Ronan Fruit and Matteo Pirotta and Alessandro Lazaric and Ronald Ortner},
	date-added = {2018-04-24 22:28:31 +0000},
	date-modified = {2018-10-02 16:50:25 -0700},
	note = {https://arxiv.org/abs/1802.04020},
	title = {Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning},
	year = {2018}}

@book{bernstein,
	author = {S. N. Bernstein},
	date-added = {2018-04-03 20:08:22 +0000},
	date-modified = {2018-04-03 20:10:42 +0000},
	title = {Theory of Probability},
	year = {1927}}

@inproceedings{MP09,
	author = {Andreas Maurer and Massimiliano Pontil},
	booktitle = colt,
	date-added = {2018-02-21 23:13:25 +0000},
	date-modified = {2018-05-06 20:29:07 +0000},
	title = {Empirical Bernstein Bounds and Sample Variance Penalization},
	year = {2009}}

@techreport{Weissman03,
	author = {Tsachy Weissman and Erik Ordentlich and Gadiel Seroussi and Sergio Verdu and Marcelo J. Weinberger},
	date-added = {2018-02-21 23:05:29 +0000},
	date-modified = {2018-02-21 23:09:17 +0000},
	institution = {Hewlett-Packard Labs},
	title = {Inequalities for the l1 deviation of the empirical distribution},
	year = {2003}}

@article{EMM06,
	author = {Eyal {Even-Dar} and Shie Mannor and Yishay Mansour},
	date-modified = {2018-05-19 22:53:44 +0000},
	journal = jmlr,
	title = {Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems},
	year = {2006}}

@inproceedings{OV16,
	author = {Ian Osband and Benjamin {Van Roy}},
	booktitle = {Arxiv},
	date-added = {2018-01-28 23:13:58 +0000},
	date-modified = {2018-05-17 06:49:57 +0000},
	note = {https://arxiv.org/pdf/1608.02732.pdf},
	title = {On Lower Bounds for Regret in Reinforcement Learning},
	url = {https://arxiv.org/pdf/1608.02732.pdf},
	year = {2016},
	bdsk-url-1 = {https://arxiv.org/pdf/1608.02732.pdf}}

@inproceedings{modelselection,
	author = {Odalric-Ambrym Maillard and Ryabko, Daniil and R\'{e}mi Munos},
	booktitle = nips,
	date-added = {2017-12-07 00:50:38 +0000},
	date-modified = {2017-12-07 00:54:04 +0000},
	title = {Selecting the State-Representation in Reinforcement Learning},
	year = {2011}}

@inproceedings{BayesianClustering,
	author = {Travis Mandel and Yun-En Liu and Emma Brunskill and and Zoran Popovic},
	booktitle = ijcai,
	title = {Efficient Bayesian clustering for reinforcement learning},
	year = {2016}}

@inproceedings{InfinitePOMDPs,
	author = {Finale Doshi-Velez},
	booktitle = nips,
	title = {The Infinite Partially Observable Markov Decision Process},
	year = {2009}}

@inproceedings{Apprenticeship,
	author = {Pieter Abbeel and Andrew Y. Ng},
	booktitle = icml,
	title = {Apprenticeship learning via inverse reinforcement learning},
	year = {2004}}

@article{Hoeffding,
	author = {Wassily Hoeffding},
	date-added = {2017-11-30 00:02:47 +0000},
	date-modified = {2017-11-30 00:04:38 +0000},
	journal = {Journal of the American Statistical Association},
	title = {Probability inequalities for sums of bounded random variables},
	year = {1963}}

@book{stochasticorders,
	author = {Moshe Shaked and J. George Shanthikumar},
	date-added = {2017-11-21 04:40:22 +0000},
	date-modified = {2017-11-21 04:41:48 +0000},
	publisher = {Springer},
	title = {Stochastic Orders},
	year = {2007}}

@book{CL06,
	author = {Nicol\`{o} Cesa-Bianchi and G\'{a}bor Lugosi},
	date-added = {2017-11-18 21:50:17 +0000},
	date-modified = {2018-05-09 16:46:36 +0000},
	publisher = {Cambridge University Press},
	title = {Prediction, Learning, and Games},
	year = {2006}}

@article{POMDPsComplexity,
	author = {Christopher Lusena and Judy Goldsmith and Martin Mundhenk},
	journal = jair,
	title = {Nonapproximability results for partially observable Markov decision processes},
	year = {2001}}

@inproceedings{AG12,
	author = {Shipra Agrawal and Navin Goyal},
	booktitle = colt,
	date-modified = {2018-05-19 22:54:22 +0000},
	title = {Analysis of Thompson Sampling for the Multi-armed Bandit Problem},
	year = {2012}}

@article{Auer02,
	author = {Peter Auer and Nicolo Cesa Bianchi and Paul Fischer},
	date-added = {2017-11-12 01:59:40 +0000},
	date-modified = {2018-10-02 12:39:45 -0700},
	journal = {Machine Learning},
	title = {Finite-time Analysis of the Multiarmed Bandit Problem},
	year = {2002}}

@inproceedings{Maillard14,
	author = {Odalric-Ambrym Maillard and Timothy A. Mann and Shie Mannor},
	booktitle = nips,
	date-added = {2017-11-12 01:05:17 +0000},
	date-modified = {2018-10-02 11:55:56 -0700},
	title = {``How hard is my {MDP}?'' The distribution-norm to the rescue},
	year = {2014}}

@article{BC12,
	author = {S{\'e}bastien Bubeck and Nicol{\`o} Cesa-Bianchi},
	date-added = {2017-11-04 17:34:17 +0000},
	date-modified = {2018-05-10 17:19:45 +0000},
	journal = {Foundations and Trends in Machine Learning},
	title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
	year = {2012}}

@inproceedings{Bartlett09,
	author = {Peter L. Bartlett and Ambuj Tewari},
	booktitle = uai,
	date-added = {2017-11-04 17:00:20 +0000},
	date-modified = {2018-10-02 16:46:07 -0700},
	title = {REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs},
	year = {2009}}

@inproceedings{Dann15,
	author = {Christoph Dann and Emma Brunskill},
	booktitle = nips,
	date-added = {2017-11-04 16:58:01 +0000},
	date-modified = {2018-06-06 21:35:15 +0000},
	title = {Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning},
	year = {2015}}

@inproceedings{Azar17,
	author = {Mohammad Gheshlaghi Azar and Ian Osband and Remi Munos},
	booktitle = icml,
	date-added = {2017-11-04 16:53:32 +0000},
	date-modified = {2018-06-06 21:35:49 +0000},
	title = {Minimax Regret Bounds for Reinforcement Learning},
	year = {2017}}

@article{Jaksch10,
	author = {Thomas Jaksch and Ronald Ortner and Peter Auer},
	date-modified = {2018-10-02 16:23:32 -0700},
	journal = jmlr,
	title = {Near-optimal Regret Bounds for Reinforcement Learning},
	year = {2010}}
