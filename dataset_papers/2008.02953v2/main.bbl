\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Jiang et~al.(2019)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang2019fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock \emph{arXiv preprint arXiv:1912.02178}, 2019.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Liang et~al.(2017)Liang, Poggio, Rakhlin, and Stokes]{liang2017fisher}
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes.
\newblock Fisher-rao metric, geometry, and complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.01530}, 2017.

\bibitem[Nagarajan and Kolter(2019)]{DBLP:journals/corr/abs-1901-01672}
Vaishnavh Nagarajan and J.~Zico Kolter.
\newblock Generalization in deep networks: The role of distance from
  initialization.
\newblock \emph{CoRR}, abs/1901.01672, 2019.
\newblock URL \url{http://arxiv.org/abs/1901.01672}.

\bibitem[Wu et~al.(2018)Wu, Ren, Liao, and Grosse]{wu2018understanding}
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse.
\newblock Understanding short-horizon bias in stochastic meta-optimization.
\newblock \emph{arXiv preprint arXiv:1803.02021}, 2018.

\bibitem[Huber(1992)]{huber1992robust}
Peter~J Huber.
\newblock Robust estimation of a location parameter.
\newblock In \emph{Breakthroughs in statistics}, pages 492--518. Springer,
  1992.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Xu et~al.(2019)Xu, Ton, Kim, Kosiorek, and Teh]{xu2019metafun}
Jin Xu, Jean-Francois Ton, Hyunjik Kim, Adam~R Kosiorek, and Yee~Whye Teh.
\newblock Metafun: Meta-learning with iterative functional updates.
\newblock \emph{arXiv preprint arXiv:1912.02738}, 2019.

\bibitem[Garnelo et~al.(2018)Garnelo, Schwarz, Rosenbaum, Viola, Rezende,
  Eslami, and Teh]{garnelo2018neural}
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo~J Rezende,
  SM~Eslami, and Yee~Whye Teh.
\newblock Neural processes.
\newblock \emph{arXiv preprint arXiv:1807.01622}, 2018.

\bibitem[Vapnik(1999)]{vapnik1999overview}
Vladimir~N Vapnik.
\newblock An overview of statistical learning theory.
\newblock \emph{IEEE transactions on neural networks}, 10\penalty0
  (5):\penalty0 988--999, 1999.

\bibitem[McAllester(1999)]{mcallester1999pac}
David~A McAllester.
\newblock Pac-bayesian model averaging.
\newblock In \emph{COLT}, volume~99, pages 164--170. Citeseer, 1999.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018non}
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan~P Adams, and Peter Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock \emph{arXiv preprint arXiv:1804.05862}, 2018.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1376--1401, 2015.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6240--6249, 2017.

\bibitem[Jiang et~al.(2018)Jiang, Krishnan, Mobahi, and
  Bengio]{jiang2018predicting}
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio.
\newblock Predicting the generalization gap in deep networks with margin
  distributions.
\newblock \emph{arXiv preprint arXiv:1810.00113}, 2018.

\bibitem[Yak et~al.(2019)Yak, Gonzalvo, and Mazzawi]{yak2019towards}
Scott Yak, Javier Gonzalvo, and Hanna Mazzawi.
\newblock Towards task and architecture-independent generalization gap
  predictors.
\newblock \emph{arXiv preprint arXiv:1906.01550}, 2019.

\bibitem[Unterthiner et~al.(2020)Unterthiner, Keysers, Gelly, Bousquet, and
  Tolstikhin]{unterthiner2020predicting}
Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya
  Tolstikhin.
\newblock Predicting neural network accuracy from weights.
\newblock \emph{arXiv preprint arXiv:2002.11448}, 2020.

\bibitem[Lee and Choi(2018)]{Lee2018}
Yoonho Lee and Seungjin Choi.
\newblock Gradient-based meta-learning with learned layerwise metric and
  subspace.
\newblock 2018.

\bibitem[Thrun and Pratt(1998)]{thrun1998learning}
Sebastian Thrun and Lorien Pratt.
\newblock Learning to learn: Introduction and overview.
\newblock In \emph{Learning to learn}, pages 3--17. Springer, 1998.

\bibitem[Schmidhuber et~al.(1996)Schmidhuber, Zhao, and
  Wiering]{schmidhuber1996simple}
Juergen Schmidhuber, Jieyu Zhao, and MA~Wiering.
\newblock Simple principles of metalearning.
\newblock \emph{Technical report IDSIA}, 69:\penalty0 1--23, 1996.

\bibitem[Ravi and Larochelle(2016)]{ravi2016optimization}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock 2016.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4077--4087, 2017.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock \emph{arXiv preprint arXiv:1703.03400}, 2017.

\bibitem[Kim et~al.(2018)Kim, Yoon, Dia, Kim, Bengio, and Ahn]{kim2018bayesian}
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin
  Ahn.
\newblock Bayesian model-agnostic meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.03836}, 2018.

\bibitem[Balaji et~al.(2018)Balaji, Sankaranarayanan, and
  Chellappa]{balaji2018metareg}
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.
\newblock Metareg: Towards domain generalization using meta-regularization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  998--1008, 2018.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6389--6399, 2018.

\bibitem[LeCun(1998)]{lecun1998mnist}
Yann LeCun.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Clanuwat et~al.(2018)Clanuwat, Bober-Irizar, Kitamoto, Lamb, Yamamoto,
  and Ha]{clanuwat2018deep}
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
  Yamamoto, and David Ha.
\newblock Deep learning for classical japanese literature, 2018.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Dvoretzky et~al.(1956)Dvoretzky, Kiefer, and
  Wolfowitz]{dvoretzky1956asymptotic}
Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz.
\newblock Asymptotic minimax character of the sample distribution function and
  of the classical multinomial estimator.
\newblock \emph{The Annals of Mathematical Statistics}, pages 642--669, 1956.

\bibitem[Massart(1990)]{massart1990tight}
Pascal Massart.
\newblock The tight constant in the dvoretzky-kiefer-wolfowitz inequality.
\newblock \emph{The annals of Probability}, pages 1269--1283, 1990.

\bibitem[Kosorok(2007)]{kosorok2007introduction}
Michael~R Kosorok.
\newblock \emph{Introduction to empirical processes and semiparametric
  inference}.
\newblock Springer Science \& Business Media, 2007.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\end{thebibliography}
