\begin{thebibliography}{10}

\bibitem{aji2017sparse}
A.~F. Aji and K.~Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{alistarh2017qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em NeurIPS}, pages 1709--1720, 2017.

\bibitem{alistarh2018convergence}
D.~Alistarh, T.~Hoefler, M.~Johansson, N.~Konstantinov, S.~Khirirat, and
  C.~Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In {\em NeurIPS}, pages 5977--5987, 2018.

\bibitem{bekkerman2011scaling}
R.~Bekkerman, M.~Bilenko, and J.~Langford.
\newblock {\em Scaling up machine learning: Parallel and distributed
  approaches}.
\newblock Cambridge University Press, 2011.

\bibitem{chang2011libsvm}
C.-C. Chang and C.-J. Lin.
\newblock Libsvm: a library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):27, 2011.

\bibitem{de2017understanding}
C.~De~Sa, M.~Feldman, C.~R{\'e}, and K.~Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In {\em ACM SIGARCH Computer Architecture News}, volume~45, pages
  561--574. ACM, 2017.

\bibitem{de2018high}
C.~De~Sa, M.~Leszczynski, J.~Zhang, A.~Marzoev, C.~R. Aberger, K.~Olukotun, and
  C.~R{\'e}.
\newblock High-accuracy low-precision training.
\newblock {\em arXiv preprint:1803.03383}, 2018.

\bibitem{dean2012large}
J.~Dean, G.~Corrado, R.~Monga, K.~Chen, M.~Devin, M.~Mao, A.~Senior, P.~Tucker,
  K.~Yang, Q.~V. Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In {\em NeurIPS}, pages 1223--1231, 2012.

\bibitem{huo2016asynchronous}
Z.~Huo and H.~Huang.
\newblock Asynchronous stochastic gradient descent with variance reduction for
  non-convex optimization.
\newblock {\em arXiv preprint arXiv:1604.03584}, 2016.

\bibitem{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em NeurIPS}, pages 315--323, 2013.

\bibitem{Lan2012An}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1-2):365--397, 2012.

\bibitem{lian2015asynchronous}
X.~Lian, Y.~Huang, Y.~Li, and J.~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In {\em NeurIPS}, pages 2737--2745, 2015.

\bibitem{nedic2009distributed}
A.~Nedic and A.~Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48--61, 2009.

\bibitem{nesterov1983method}
Y.~Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In {\em Soviet Mathematics Doklady}, volume~27, pages 372--376, 1983.

\bibitem{nesterov2003introductory}
Y.~Nesterov.
\newblock {\em Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer, 2003.

\bibitem{recht2011hogwild}
B.~Recht, C.~Re, S.~Wright, and F.~Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em NeurIPS}, pages 693--701, 2011.

\bibitem{reddi2015variance}
S.~J. Reddi, A.~Hefny, S.~Sra, B.~Poczos, and A.~J. Smola.
\newblock On variance reduction in stochastic gradient descent and its
  asynchronous variants.
\newblock In {\em NeurIPS}, pages 2647--2655, 2015.

\bibitem{Reddi2016Fast}
S.~J. Reddi, S.~Sra, B.~Poczos, and A.~Smola.
\newblock Fast stochastic methods for nonsmooth nonconvex optimization.
\newblock In {\em NeurIPS}, 2016.

\bibitem{seide20141}
F.~Seide, H.~Fu, J.~Droppo, G.~Li, and D.~Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem{shang2017fast}
F.~Shang, Y.~Liu, J.~Cheng, and J.~Zhuo.
\newblock Fast stochastic variance reduced gradient method with momentum
  acceleration for machine learning.
\newblock {\em arXiv preprint arXiv:1703.07948}, 2017.

\bibitem{stich2018sparsified}
S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi.
\newblock Sparsified sgd with memory.
\newblock In {\em NeurIPS}, pages 4452--4463, 2018.

\bibitem{wang2018atomo}
H.~Wang, S.~Sievert, Z.~Charles, D.~Papailiopoulos, and S.~Wright.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In {\em NeurIPS}, 2018.

\bibitem{wang2017memory}
J.~Wang, W.~Wang, and N.~Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch-prox.
\newblock {\em Conference on Learning Theory (COLT)}, 2017.

\bibitem{wangni2017gradient}
J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In {\em NeurIPS}, 2018.

\bibitem{wen2017terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em NeurIPS}, pages 1509--1519, 2017.

\bibitem{wu2018error}
J.~Wu, W.~Huang, J.~Huang, and T.~Zhang.
\newblock Error compensated quantized sgd and its applications to large-scale
  distributed optimization.
\newblock In {\em ICML}, 2018.

\bibitem{yu2019AC}
Y.~Yu, J.~Wu, and J.~Huang.
\newblock Exploring fast and communication-efficient algorithms in large-scale
  distributed networks.
\newblock In {\em AISTATS}, 2019.

\bibitem{zhang2017zipml}
H.~Zhang, J.~Li, K.~Kara, D.~Alistarh, J.~Liu, and C.~Zhang.
\newblock Zipml: Training linear models with end-to-end low precision, and a
  little bit of deep learning.
\newblock In {\em ICML}, pages 4035--4043, 2017.

\end{thebibliography}
