\begin{thebibliography}{10}

\bibitem{ahmad2016action}
Z.~F. Ahmad, R.~C. Holte, and M.~Bowling.
\newblock Action selection for hammer shots in curling.
\newblock In {\em IJCAI}, pages 561--567, 2016.

\bibitem{auer2002using}
P.~Auer.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):397--422, 2002.

\bibitem{chaslot2007progressive}
G.~Chaslot, M.~Winands, and B.~Bouzy.
\newblock Progressive strategies for {M}onte-{C}arlo tree search.
\newblock 2007.

\bibitem{couetoux2011continuous}
A.~Cou{\"e}toux, J.-B. Hoock, N.~Sokolovska, O.~Teytaud, and N.~Bonnard.
\newblock Continuous upper confidence trees.
\newblock In {\em International Conference on Learning and Intelligent
  Optimization}, pages 433--445. Springer, 2011.

\bibitem{coulom2007computing}
R.~Coulom.
\newblock Computing “{ELO} ratings” of move patterns in the game of go.
\newblock {\em Icga Journal}, 30(4):198--208, 2007.

\bibitem{Glorot10understandingthe}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em In Proceedings of the International Conference on Artificial
  Intelligence and Statistics. Society for Artificial Intelligence and
  Statistics}, 2010.

\bibitem{adam14}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em International Conference on Learning Representations}, 12 2014.

\bibitem{kocsis2006bandit}
L.~Kocsis and C.~Szepesv{\'a}ri.
\newblock Bandit based monte-carlo planning.
\newblock In {\em European conference on machine learning}, pages 282--293.
  Springer, 2006.

\bibitem{lee2018deep}
K.~Lee, S.-A. Kim, J.~Choi, and S.-W. Lee.
\newblock Deep reinforcement learning in continuous action spaces: a case study
  in the game of simulated curling.
\newblock In {\em International Conference on Machine Learning}, pages
  2943--2952, 2018.

\bibitem{nadaraya1964estimating}
E.~A. Nadaraya.
\newblock On estimating regression.
\newblock {\em Theory of Probability \& Its Applications}, 9(1):141--142, 1964.

\bibitem{ontanon2017combinatorial}
S.~Onta{\~n}{\'o}n.
\newblock Combinatorial multi-armed bandits for real-time strategy games.
\newblock {\em Journal of Artificial Intelligence Research}, 58:665--702, 2017.

\bibitem{rosin2011multi}
C.~D. Rosin.
\newblock Multi-armed bandits with episode context.
\newblock {\em Annals of Mathematics and Artificial Intelligence},
  61(3):203--230, 2011.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem{silver2017mastering2}
D.~Silver, T.~Hubert, J.~Schrittwieser, I.~Antonoglou, M.~Lai, A.~Guez,
  M.~Lanctot, L.~Sifre, D.~Kumaran, T.~Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock {\em arXiv preprint arXiv:1712.01815}, 2017.

\bibitem{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354, 2017.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock 2018.

\bibitem{watson1964smooth}
G.~S. Watson.
\newblock Smooth regression analysis.
\newblock {\em Sankhy{\=a}: The Indian Journal of Statistics, Series A}, pages
  359--372, 1964.

\bibitem{yee2016monte}
T.~Yee, V.~Lis{\`y}, and M.~H. Bowling.
\newblock Monte-{C}arlo tree search in continuous action spaces with execution
  uncertainty.
\newblock In {\em IJCAI}, pages 690--697, 2016.

\end{thebibliography}
