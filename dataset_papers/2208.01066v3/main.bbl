\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balcan(2020)]{balcan2020data}
Maria-Florina Balcan.
\newblock Data-driven algorithm design.
\newblock In Tim Roughgarden, editor, \emph{Beyond Worst Case Analysis of
  Algorithms}. Cambridge University Press, 2020.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2019.

\bibitem[Bello et~al.(2016)Bello, Pham, Le, Norouzi, and
  Bengio]{bello2016neural}
Irwan Bello, Hieu Pham, Quoc~V Le, Mohammad Norouzi, and Samy Bengio.
\newblock Neural combinatorial optimization with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.09940}, 2016.

\bibitem[Bengio et~al.(1995)Bengio, Bengio, Cloutier, and
  Gecsei]{bengio1995optimization}
Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei.
\newblock On the optimization of a synaptic learning rule.
\newblock In \emph{Preprints Conf. Optimality in Artificial and Biological
  Neural Networks}, 1995.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston]{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  41--48, 2009.

\bibitem[Bhattamishra et~al.(2020{\natexlab{a}})Bhattamishra, Ahuja, and
  Goyal]{bhattamishra2020ability}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the ability and limitations of transformers to recognize formal
  languages.
\newblock \emph{arXiv preprint arXiv:2009.11264}, 2020{\natexlab{a}}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{b}})Bhattamishra, Patel, and
  Goyal]{bhattamishra2020computational}
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
\newblock On the computational power of transformers and its implications in
  sequence modeling.
\newblock \emph{arXiv preprint arXiv:2006.09286}, 2020{\natexlab{b}}.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, et~al.]{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock \emph{arXiv preprint arXiv:2204.06745}, 2022.

\bibitem[Blanc et~al.(2021)Blanc, Lange, Qiao, and Tan]{blanc2021decision}
Guy Blanc, Jane Lange, Mingda Qiao, and Li-Yang Tan.
\newblock Decision tree heuristics can fail, even in the smoothed setting.
\newblock \emph{arXiv preprint arXiv:2107.00819}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Brutzkus et~al.(2020)Brutzkus, Daniely, and Malach]{brutzkus2020id3}
Alon Brutzkus, Amit Daniely, and Eran Malach.
\newblock Id3 learns juntas for smoothed product distributions.
\newblock In \emph{Conference on Learning Theory}, pages 902--915. PMLR, 2020.

\bibitem[Chan et~al.(2022)Chan, Santoro, Lampinen, Wang, Singh, Richemond,
  McClelland, and Hill]{chan2022data}
Stephanie~CY Chan, Adam Santoro, Andrew~K Lampinen, Jane~X Wang, Aaditya Singh,
  Pierre~H Richemond, Jay McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent few-shot learning in
  transformers.
\newblock \emph{arXiv preprint arXiv:2205.05055}, 2022.

\bibitem[Chen and Guestrin(2016)]{chen2016xgboost}
Tianqi Chen and Carlos Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{conference on knowledge discovery and data mining (KDD)},
  2016.

\bibitem[Chen et~al.(2021)Chen, Zhong, Zha, Karypis, and He]{chen2021meta}
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He~He.
\newblock Meta-learning via language model in-context tuning.
\newblock \emph{arXiv preprint arXiv:2110.07814}, 2021.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Dinh et~al.(2022)Dinh, Zeng, Zhang, Lin, Rajput, Gira, Sohn,
  Papailiopoulos, and Lee]{dinh2022lift}
Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Shashank Rajput, Michael Gira,
  Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee.
\newblock Lift: Language-interfaced fine-tuning for non-language machine
  learning tasks.
\newblock \emph{arXiv preprint arXiv:2206.06565}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and
  Zhang]{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds,
  Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Elman(1993)]{elman1993learning}
Jeffrey~L Elman.
\newblock Learning and development in neural networks: The importance of
  starting small.
\newblock \emph{Cognition}, 1993.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning (ICML)}, 2017.

\bibitem[Friedman(2001)]{friedman2001greedy}
Jerome~H Friedman.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock \emph{Annals of statistics}, 2001.

\bibitem[Garnelo et~al.(2018{\natexlab{a}})Garnelo, Rosenbaum, Maddison,
  Ramalho, Saxton, Shanahan, Teh, Rezende, and Eslami]{garnelo2018conditional}
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David
  Saxton, Murray Shanahan, Yee~Whye Teh, Danilo Rezende, and SM~Ali Eslami.
\newblock Conditional neural processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2018{\natexlab{a}}.

\bibitem[Garnelo et~al.(2018{\natexlab{b}})Garnelo, Schwarz, Rosenbaum, Viola,
  Rezende, Eslami, and Teh]{garnelo2018neural}
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo~J Rezende,
  SM~Eslami, and Yee~Whye Teh.
\newblock Neural processes.
\newblock \emph{arXiv preprint arXiv:1807.01622}, 2018{\natexlab{b}}.

\bibitem[Hahn(2020)]{hahn2020theoretical}
Michael Hahn.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  2020.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter2001learning}
Sepp Hochreiter, A~Steven Younger, and Peter~R Conwell.
\newblock Learning to learn using gradient descent.
\newblock In \emph{International conference on artificial neural networks
  (ICANN)}, 2001.

\bibitem[Horvitz et~al.(2001)Horvitz, Ruan, Gomes, Kautz, Selman, and
  Chickering]{horvitz2001bayesian}
Eric Horvitz, Yongshao Ruan, Carla Gomes, Henry Kautz, Bart Selman, and Max
  Chickering.
\newblock A bayesian approach to tackling hard computational problems
  (preliminary report).
\newblock \emph{Electronic Notes in Discrete Mathematics}, 2001.

\bibitem[Hospedales et~al.(2020)Hospedales, Antoniou, Micaelli, and
  Storkey]{hospedales2020meta}
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey.
\newblock Meta-learning in neural networks: A survey.
\newblock \emph{arXiv preprint arXiv:2004.05439}, 2020.

\bibitem[Khalil et~al.(2017)Khalil, Dai, Zhang, Dilkina, and
  Song]{khalil2017learning}
Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le~Song.
\newblock Learning combinatorial optimization algorithms over graphs.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Kim et~al.(2019)Kim, Mnih, Schwarz, Garnelo, Eslami, Rosenbaum,
  Vinyals, and Teh]{kim2019attentive}
Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan
  Rosenbaum, Oriol Vinyals, and Yee~Whye Teh.
\newblock Attentive neural processes.
\newblock \emph{arXiv preprint arXiv:1901.05761}, 2019.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kirsch and Schmidhuber(2021)]{kirsch2021meta}
Louis Kirsch and J{\"u}rgen Schmidhuber.
\newblock Meta learning backpropagation and improving it.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Lampinen et~al.(2022)Lampinen, Dasgupta, Chan, Matthewson, Tessler,
  Creswell, McClelland, Wang, and Hill]{lampinen2022can}
Andrew~K Lampinen, Ishita Dasgupta, Stephanie~CY Chan, Kory Matthewson,
  Michael~Henry Tessler, Antonia Creswell, James~L McClelland, Jane~X Wang, and
  Felix Hill.
\newblock Can language models learn from explanations in context?
\newblock \emph{arXiv preprint arXiv:2204.02329}, 2022.

\bibitem[Li and Malik(2016)]{li2016learning}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock \emph{arXiv preprint arXiv:1606.01885}, 2016.

\bibitem[Lieber et~al.(2021)Lieber, Sharir, Lenz, and
  Shoham]{lieber2021jurassic}
Opher Lieber, Or~Sharir, Barak Lenz, and Yoav Shoham.
\newblock Jurassic-1: Technical details and evaluation.
\newblock \emph{White Paper. AI21 Labs}, 2021.

\bibitem[Liu et~al.(2021)Liu, Shen, Zhang, Dolan, Carin, and
  Chen]{liu2021makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu
  Chen.
\newblock What makes good in-context examples for gpt-$3 $?
\newblock \emph{arXiv preprint arXiv:2101.06804}, 2021.

\bibitem[Lu et~al.(2021{\natexlab{a}})Lu, Grover, Abbeel, and
  Mordatch]{lu2021pretrained}
Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.
\newblock Pretrained transformers as universal computation engines.
\newblock \emph{arXiv preprint arXiv:2103.05247}, 2021{\natexlab{a}}.

\bibitem[Lu et~al.(2021{\natexlab{b}})Lu, Bartolo, Moore, Riedel, and
  Stenetorp]{lu2021fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock \emph{arXiv preprint arXiv:2104.08786}, 2021{\natexlab{b}}.

\bibitem[Min et~al.(2021{\natexlab{a}})Min, Lewis, Hajishirzi, and
  Zettlemoyer]{min2021noisy}
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Noisy channel language model prompting for few-shot text
  classification.
\newblock \emph{arXiv preprint arXiv:2108.04106}, 2021{\natexlab{a}}.

\bibitem[Min et~al.(2021{\natexlab{b}})Min, Lewis, Zettlemoyer, and
  Hajishirzi]{min2021metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock Metaicl: Learning to learn in context.
\newblock \emph{arXiv preprint arXiv:2110.15943}, 2021{\natexlab{b}}.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and
  Zettlemoyer]{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock \emph{arXiv preprint arXiv:2202.12837}, 2022.

\bibitem[Mishra et~al.(2018)Mishra, Rohaninejad, Chen, and
  Abbeel]{mishra2018simple}
Nikhil Mishra, Mostafa Rohaninejad, Xi~Chen, and Pieter Abbeel.
\newblock A simple neural attentive meta-learner.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Mishra et~al.(2021)Mishra, Khashabi, Baral, Choi, and
  Hajishirzi]{mishra2021reframing}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh
  Hajishirzi.
\newblock Reframing instructional prompts to gptk's language.
\newblock \emph{arXiv preprint arXiv:2109.07830}, 2021.

\bibitem[M{\"u}ller et~al.(2021)M{\"u}ller, Hollmann, Arango, Grabocka, and
  Hutter]{muller2021transformers}
Samuel M{\"u}ller, Noah Hollmann, Sebastian~Pineda Arango, Josif Grabocka, and
  Frank Hutter.
\newblock Transformers can do bayesian inference.
\newblock \emph{arXiv preprint arXiv:2112.10510}, 2021.

\bibitem[Naik and Mammone(1992)]{naik1992meta}
Devang~K Naik and Richard~J Mammone.
\newblock Meta-neural networks that learn by learning.
\newblock In \emph{International Joint Conference on Neural Networks (IJCNN)},
  1992.

\bibitem[Nakkiran(2019)]{nakkiran2019more}
Preetum Nakkiran.
\newblock More data can hurt for linear regression: Sample-wise double descent.
\newblock \emph{arXiv preprint arXiv:1912.07242}, 2019.

\bibitem[Nguyen and Grover(2022)]{nguyen2022transformer}
Tung Nguyen and Aditya Grover.
\newblock Transformer neural processes: Uncertainty-aware meta learning via
  sequence modeling.
\newblock \emph{arXiv preprint arXiv:2207.04179}, 2022.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez,
  Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{parmar2018image}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 2011.

\bibitem[P{\'e}rez et~al.(2019)P{\'e}rez, Marinkovi{\'c}, and
  Barcel{\'o}]{perez2019turing}
Jorge P{\'e}rez, Javier Marinkovi{\'c}, and Pablo Barcel{\'o}.
\newblock On the turing completeness of modern neural network architectures.
\newblock \emph{arXiv preprint arXiv:1901.03429}, 2019.

\bibitem[Pesut(2022)]{pesut2022who}
Lovre Pesut.
\newblock Who models the models that model models? an exploration of gpt-3's
  in-context model fitting ability, 2022.
\newblock URL
  \url{https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{OpenAI blog}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 2019.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Ravi and Larochelle(2017)]{ravi2016optimization}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock \emph{International Conference for Learning Representations (ICLR)},
  2017.

\bibitem[Razeghi et~al.(2022)Razeghi, Logan~IV, Gardner, and
  Singh]{razeghi2022impact}
Yasaman Razeghi, Robert~L Logan~IV, Matt Gardner, and Sameer Singh.
\newblock Impact of pretraining term frequencies on few-shot reasoning.
\newblock \emph{arXiv preprint arXiv:2202.07206}, 2022.

\bibitem[Rong(2021)]{rong_2021}
Frieda Rong.
\newblock Extrapolating to unnatural language processing with gpt-3's
  in-context learning: The good, the bad, and the mysterious), 2021.
\newblock URL \url{http://ai.stanford.edu/blog/in-context-learning/}.

\bibitem[Rubin et~al.(2021)Rubin, Herzig, and Berant]{rubin2021learning}
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
\newblock Learning to retrieve prompts for in-context learning.
\newblock \emph{arXiv preprint arXiv:2112.08633}, 2021.

\bibitem[Sanger(1994)]{sanger1994neural}
Terence~D Sanger.
\newblock Neural network learning control of robot manipulators using gradually
  increasing task difficulty.
\newblock \emph{IEEE transactions on Robotics and Automation}, 1994.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro2016meta}
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy
  Lillicrap.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International conference on machine learning (ICML)}, 2016.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
J{\"u}rgen Schmidhuber.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Borgnia, Gupta, Huang,
  Vishkin, Goldblum, and Goldstein]{schwarzschild2021can}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah
  Goldblum, and Tom Goldstein.
\newblock Can you learn an algorithm? generalizing from easy to hard problems
  with recurrent networks.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Selsam et~al.(2018)Selsam, Lamm, Benedikt, Liang, de~Moura, Dill,
  et~al.]{selsam2018learning}
Daniel Selsam, Matthew Lamm, B~Benedikt, Percy Liang, Leonardo de~Moura,
  David~L Dill, et~al.
\newblock Learning a sat solver from single-bit supervision.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Snell et~al.(2021)Snell, Zhong, Klein, and
  Steinhardt]{snell2021approximating}
Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt.
\newblock Approximating how single head attention learns.
\newblock \emph{arXiv preprint arXiv:2103.07601}, 2021.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Thrun and Pratt(2012)]{thrun2012learning}
Sebastian Thrun and Lorien Pratt.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 1996.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Vinyals et~al.(2015)Vinyals, Fortunato, and
  Jaitly]{vinyals2015pointer}
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
\newblock Pointer networks.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Neural Information Processing Systems (NeurIPS)}, 2015.

\bibitem[Weiss et~al.(2021)Weiss, Goldberg, and Yahav]{weiss2021thinking}
Gail Weiss, Yoav Goldberg, and Eran Yahav.
\newblock Thinking like transformers.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}. Association for
  Computational Linguistics (ACL), 2020.

\bibitem[Wu et~al.(2020)Wu, Dyer, and Neyshabur]{wu2020curricula}
Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock When do curricula work?
\newblock \emph{arXiv preprint arXiv:2012.03107}, 2020.

\bibitem[Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma]{xie2022an}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Xu et~al.(2008)Xu, Hutter, Hoos, and Leyton-Brown]{xu2008satzilla}
Lin Xu, Frank Hutter, Holger~H Hoos, and Kevin Leyton-Brown.
\newblock Satzilla: portfolio-based algorithm selection for sat.
\newblock \emph{Journal of artificial intelligence research}, 2008.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock \emph{arXiv preprint arXiv:2105.11115}, 2021.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J Reddi, and
  Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Backurs, Bubeck, Eldan, Gunasekar, and
  Wagner]{zhang2022unveiling}
Yi~Zhang, Arturs Backurs, S{\'e}bastien Bubeck, Ronen Eldan, Suriya Gunasekar,
  and Tal Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task.
\newblock \emph{arXiv preprint arXiv:2206.04301}, 2022.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh]{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\end{thebibliography}
