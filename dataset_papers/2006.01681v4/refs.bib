
@article{trask_neural_2018,
	title = {Neural {Arithmetic} {Logic} {Units}},
	url = {http://arxiv.org/abs/1808.00508},
	abstract = {Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.},
	language = {en},
	urldate = {2019-09-17},
	journal = {arXiv:1808.00508 [cs]},
	author = {Trask, Andrew and Hill, Felix and Reed, Scott and Rae, Jack and Dyer, Chris and Blunsom, Phil},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.00508},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Trask et al. - 2018 - Neural Arithmetic Logic Units.pdf:/home/niklas/Zotero/storage/AN8CXZYW/Trask et al. - 2018 - Neural Arithmetic Logic Units.pdf:application/pdf}
}

@article{champion_data-driven_2019,
	title = {Data-driven discovery of coordinates and governing equations},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1906995116},
	doi = {10.1073/pnas.1906995116},
	abstract = {The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam’s razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.},
	language = {en},
	number = {45},
	urldate = {2020-06-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Champion, Kathleen and Lusch, Bethany and Kutz, J. Nathan and Brunton, Steven L.},
	month = nov,
	year = {2019},
	pages = {22445--22451},
	file = {Champion et al. - 2019 - Data-driven discovery of coordinates and governing.pdf:/home/niklas/Zotero/storage/DIZMESCD/Champion et al. - 2019 - Data-driven discovery of coordinates and governing.pdf:application/pdf}
}

@article{rudin_stop_2019,
	title = {Stop {Explaining} {Black} {Box} {Machine} {Learning} {Models} for {High} {Stakes} {Decisions} and {Use} {Interpretable} {Models} {Instead}},
	url = {http://arxiv.org/abs/1811.10154},
	abstract = {Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the ﬁrst place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward – it is to design models that are inherently interpretable. This manuscript clariﬁes the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identiﬁes challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.},
	language = {en},
	urldate = {2020-06-02},
	journal = {arXiv:1811.10154 [cs, stat]},
	author = {Rudin, Cynthia},
	month = sep,
	year = {2019},
	note = {arXiv: 1811.10154},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Rudin - 2019 - Stop Explaining Black Box Machine Learning Models .pdf:/home/niklas/Zotero/storage/SHWKEIMF/Rudin - 2019 - Stop Explaining Black Box Machine Learning Models .pdf:application/pdf}
}

@article{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2020-06-01},
	journal = {arXiv:1806.07366 [cs, stat]},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv: 1806.07366},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/niklas/Zotero/storage/MU2ZZ7P3/1806.html:text/html;arXiv Fulltext PDF:/home/niklas/Zotero/storage/82FK8WRQ/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf}
}

@article{vargas-de-leon_volterra-type_2015,
	title = {Volterra-type {Lyapunov} functions for fractional-order epidemic systems},
	volume = {24},
	issn = {10075704},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1007570414005747},
	doi = {10.1016/j.cnsns.2014.12.013},
	abstract = {In this paper we prove an elementary lemma which estimates fractional derivatives of Volterra-type Lyapunov functions in the sense Caputo when a 2 ð0; 1Þ. Moreover, by using this result, we study the uniform asymptotic stability of some Caputo-type epidemic systems with a pair of fractional-order differential equations. These epidemic systems are the Susceptible–Infected–Susceptible (SIS), Susceptible–Infected–Recovered (SIR) and Susceptible–Infected–Recovered–Susceptible (SIRS) models and Ross–Macdonald model for vector-borne diseases. We show that the unique endemic equilibrium is uniformly asymptotically stable if the basic reproductive number is greater than one. We illustrate our theoretical results with numerical simulations using the Adams–Bashforth–Moulton scheme implemented in the fde12 Matlab function.},
	language = {en},
	number = {1-3},
	urldate = {2020-06-01},
	journal = {Communications in Nonlinear Science and Numerical Simulation},
	author = {Vargas-De-León, Cruz},
	month = jul,
	year = {2015},
	pages = {75--85},
	file = {Vargas-De-León - 2015 - Volterra-type Lyapunov functions for fractional-or.pdf:/home/niklas/Zotero/storage/C5XG7BRV/Vargas-De-León - 2015 - Volterra-type Lyapunov functions for fractional-or.pdf:application/pdf}
}

@article{saxton_analysing_2019,
	title = {Analysing {Mathematical} {Reasoning} {Abilities} of {Neural} {Models}},
	url = {http://arxiv.org/abs/1904.01557},
	abstract = {Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.},
	urldate = {2020-05-30},
	journal = {arXiv:1904.01557 [cs, stat]},
	author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.01557},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/niklas/Zotero/storage/F5KGG7PE/1904.html:text/html;arXiv Fulltext PDF:/home/niklas/Zotero/storage/2DNZYSBT/Saxton et al. - 2019 - Analysing Mathematical Reasoning Abilities of Neur.pdf:application/pdf}
}

@article{madsen_measuring_2019,
    author={Andreas Madsen and Alexander Rosenberg Johansen},
    title={Measuring Arithmetic Extrapolation Performance},
    booktitle={Science meets Engineering of Deep Learning at 33rd Conference on Neural Information Processing Systems (NeurIPS 2019)},
    address={Vancouver, Canada},
    journal={CoRR},
    volume={abs/1910.01888},
    month={October},
    year={2019},
    url={http://arxiv.org/abs/1910.01888},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    arxivId = {2001.05016},
    eprint={1910.01888}
}

@article{lample_deep_2019,
	title = {Deep {Learning} for {Symbolic} {Mathematics}},
	url = {http://arxiv.org/abs/1912.01412},
	abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
	language = {en},
	urldate = {2020-05-30},
	journal = {arXiv:1912.01412 [cs]},
	author = {Lample, Guillaume and Charton, François},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.01412},
	keywords = {Computer Science - Machine Learning, Computer Science - Symbolic Computation},
	file = {Lample and Charton - 2019 - Deep Learning for Symbolic Mathematics.pdf:/home/niklas/Zotero/storage/2NKVUP5U/Lample and Charton - 2019 - Deep Learning for Symbolic Mathematics.pdf:application/pdf}
}

@article{kurach_neural_2016,
	title = {Neural {Random}-{Access} {Machines}},
	url = {http://arxiv.org/abs/1511.06392},
	abstract = {In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation.},
	language = {en},
	urldate = {2020-05-30},
	journal = {arXiv:1511.06392 [cs]},
	author = {Kurach, Karol and Andrychowicz, Marcin and Sutskever, Ilya},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.06392},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {Kurach et al. - 2016 - Neural Random-Access Machines.pdf:/home/niklas/Zotero/storage/YYM79C9X/Kurach et al. - 2016 - Neural Random-Access Machines.pdf:application/pdf}
}

@article{kalchbrenner_grid_2016,
	title = {Grid {Long} {Short}-{Term} {Memory}},
	url = {http://arxiv.org/abs/1507.01526},
	abstract = {This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a uniﬁed way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to signiﬁcantly outperform the standard LSTM. We then give results for two empirical tasks. We ﬁnd that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to deﬁne a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.},
	language = {en},
	urldate = {2020-05-30},
	journal = {arXiv:1507.01526 [cs]},
	author = {Kalchbrenner, Nal and Danihelka, Ivo and Graves, Alex},
	month = jan,
	year = {2016},
	note = {arXiv: 1507.01526},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Kalchbrenner et al. - 2016 - Grid Long Short-Term Memory.pdf:/home/niklas/Zotero/storage/EFR7IDSY/Kalchbrenner et al. - 2016 - Grid Long Short-Term Memory.pdf:application/pdf}
}

@article{graves_neural_2014,
	title = {Neural {Turing} {Machines}},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efﬁciently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	language = {en},
	urldate = {2020-05-30},
	journal = {arXiv:1410.5401 [cs]},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	month = dec,
	year = {2014},
	note = {arXiv: 1410.5401},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Graves et al. - 2014 - Neural Turing Machines.pdf:/home/niklas/Zotero/storage/P2K6NLYE/Graves et al. - 2014 - Neural Turing Machines.pdf:application/pdf}
}

@article{chen_neural_2018,
	title = {Neural {Arithmetic} {Expression} {Calculator}},
	url = {http://arxiv.org/abs/1809.08590},
	abstract = {This paper presents a pure neural solver for arithmetic expression calculation (AEC) problem. Previous work utilizes the powerful capabilities of deep neural networks and attempts to build an end-to-end model to solve this problem. However, most of these methods can only deal with the additive operations. It is still a challenging problem to solve the complex expression calculation problem, which includes the adding, subtracting, multiplying, dividing and bracketing operations. In this work, we regard the arithmetic expression calculation as a hierarchical reinforcement learning problem. An arithmetic operation is decomposed into a series of sub-tasks, and each sub-task is dealt with by a skill module. The skill module could be a basic module performing elementary operations, or interactive module performing complex operations by invoking other skill models. With curriculum learning, our model can deal with a complex arithmetic expression calculation with the deep hierarchical structure of skill models. Experiments show that our model signiﬁcantly outperforms the previous models for arithmetic expression calculation.},
	language = {en},
	urldate = {2020-05-30},
	journal = {arXiv:1809.08590 [cs]},
	author = {Chen, Kaiyu and Dong, Yihan and Qiu, Xipeng and Chen, Zitian},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.08590},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Chen et al. - 2018 - Neural Arithmetic Expression Calculator.pdf:/home/niklas/Zotero/storage/UFZ7XRW2/Chen et al. - 2018 - Neural Arithmetic Expression Calculator.pdf:application/pdf}
}

@article{innes_fashionable_2018,
	title = {Fashionable {Modelling} with {Flux}},
	url = {http://arxiv.org/abs/1811.01457},
	abstract = {Machine learning as a discipline has seen an incredible surge of interest in recent years due in large part to a perfect storm of new theory, superior tooling, renewed interest in its capabilities. We present in this paper a framework named Flux that shows how further refinement of the core ideas of machine learning, built upon the foundation of the Julia programming language, can yield an environment that is simple, easily modifiable, and performant. We detail the fundamental principles of Flux as a framework for differentiable programming, give examples of models that are implemented within Flux to display many of the language and framework-level features that contribute to its ease of use and high productivity, display internal compiler techniques used to enable the acceleration and performance that lies at the heart of Flux, and finally give an overview of the larger ecosystem that Flux fits inside of.},
	urldate = {2020-05-29},
	journal = {arXiv:1811.01457 [cs]},
	author = {Innes, Michael and Saba, Elliot and Fischer, Keno and Gandhi, Dhairya and Rudilosso, Marco Concetto and Joy, Neethu Mariya and Karmali, Tejan and Pal, Avik and Shah, Viral},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.01457},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/home/niklas/Zotero/storage/QYPLMBST/1811.html:text/html;arXiv Fulltext PDF:/home/niklas/Zotero/storage/6PWCM2AF/Innes et al. - 2018 - Fashionable Modelling with Flux.pdf:application/pdf}
}

@article{schlor_inalu_2020,
	title = {{iNALU}: {Improved} {Neural} {Arithmetic} {Logic} {Unit}},
	shorttitle = {{iNALU}},
	url = {http://arxiv.org/abs/2003.07629},
	abstract = {Neural networks have to capture mathematical relationships in order to learn various tasks. They approximate these relations implicitly and therefore often do not generalize well. The recently proposed Neural Arithmetic Logic Unit (NALU) is a novel neural architecture which is able to explicitly represent the mathematical relationships by the units of the network to learn operations such as summation, subtraction or multiplication. Although NALUs have been shown to perform well on various downstream tasks, an in-depth analysis reveals practical shortcomings by design, such as the inability to multiply or divide negative input values or training stability issues for deeper networks. We address these issues and propose an improved model architecture. We evaluate our model empirically in various settings from learning basic arithmetic operations to more complex functions. Our experiments indicate that our model solves stability issues and outperforms the original NALU model in means of arithmetic precision and convergence.},
	language = {en},
	urldate = {2020-05-27},
	journal = {arXiv:2003.07629 [cs]},
	author = {Schlör, Daniel and Ring, Markus and Hotho, Andreas},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.07629},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Schlör et al. - 2020 - iNALU Improved Neural Arithmetic Logic Unit.pdf:/home/niklas/Zotero/storage/E699RFKI/Schlör et al. - 2020 - iNALU Improved Neural Arithmetic Logic Unit.pdf:application/pdf}
}

@techreport{taghvaei_fractional_2020,
	type = {preprint},
	title = {Fractional {SIR} {Epidemiological} {Models}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2020.04.28.20083865},
	abstract = {The purpose of this work is to make a case for epidemiological models with fractional exponent in the contribution of sub-populations to the transmission rate. More speciﬁcally, we question the standard assumption in the literature on epidemiological models, where the transmission rate dictating propagation of infections is taken to be proportional to the product between the infected and susceptible sub-populations; a model that relies on strong mixing between the two groups and widespread contact between members of the groups. We content, that contact between infected and susceptible individuals, especially during the early phases of an epidemic, takes place over a (possibly diﬀused) boundary between the respective subpopulations. As a result, the rate of transmission depends on the product of fractional powers instead. The intuition relies on the fact that infection grows in geographically concentrated cells, in contrast to the standard product model that relies on complete mixing of the susceptible to infected sub-populations. We validate the hypothesis of fractional exponents i) by numerical simulation for disease propagation in graphs imposing a local structure to allowed disease transmissions and ii) by ﬁtting the model to a COVID-19 data set provided by John Hopkins University (JHUCSSE) for the period Jan-31-20 to Mar-24-20, for the countries of Italy, Germany, Iran, and France.},
	language = {en},
	urldate = {2020-05-26},
	institution = {Epidemiology},
	author = {Taghvaei, Amirhossein and Georgiou, Tryphon T. and Norton, Larry and Tannenbaum, Allen R},
	month = apr,
	year = {2020},
	doi = {10.1101/2020.04.28.20083865},
	file = {Taghvaei et al. - 2020 - Fractional SIR Epidemiological Models.pdf:/home/niklas/Zotero/storage/TZTKJIVN/Taghvaei et al. - 2020 - Fractional SIR Epidemiological Models.pdf:application/pdf}
}

@article{rackauckas_universal_2020,
	title = {Universal {Differential} {Equations} for {Scientific} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2001.04385},
	abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring "big data". In this work we develop a new methodology, universal differential equations (UDEs), which augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating climate simulations by 15,000x, can be handled by training UDEs.},
	urldate = {2020-05-17},
	journal = {arXiv:2001.04385 [cs, math, q-bio, stat]},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.04385},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Dynamical Systems, Quantitative Biology - Quantitative Methods},
	file = {arXiv.org Snapshot:/home/niklas/Zotero/storage/IMSJALB2/2001.html:text/html;arXiv Fulltext PDF:/home/niklas/Zotero/storage/SVESTRMC/Rackauckas et al. - 2020 - Universal Differential Equations for Scientific Ma.pdf:application/pdf}
}

@article{suzgun_evaluating_2018,
	title = {On {Evaluating} the {Generalization} of {LSTM} {Models} in {Formal} {Languages}},
	language = {en},
	author = {Suzgun, Mirac and Belinkov, Yonatan and Shieber, Stuart M},
	year = {2018},
	pages = {10},
	file = {Suzgun et al. - On Evaluating the Generalization of LSTM Models in.pdf:/home/niklas/Zotero/storage/9UELWSSJ/Suzgun et al. - On Evaluating the Generalization of LSTM Models in.pdf:application/pdf}
}

@article{lake_generalization_2018,
	title = {Generalization without systematicity: {On} the compositional skills of sequence-to-sequence recurrent networks},
	shorttitle = {Generalization without systematicity},
	url = {http://arxiv.org/abs/1711.00350},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.},
	urldate = {2020-05-16},
	journal = {arXiv:1711.00350 [cs]},
	author = {Lake, Brenden M. and Baroni, Marco},
	month = jun,
	year = {2018},
	note = {arXiv: 1711.00350},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/home/niklas/Zotero/storage/9PL5IRGH/1711.html:text/html;arXiv Fulltext PDF:/home/niklas/Zotero/storage/USMWTJ7R/Lake and Baroni - 2018 - Generalization without systematicity On the compo.pdf:application/pdf}
}

@article{gallistel_finding_2018,
	title = {Finding numbers in the brain},
	volume = {373},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2017.0119},
	doi = {10.1098/rstb.2017.0119},
	language = {en},
	number = {1740},
	urldate = {2020-05-16},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Gallistel, C. R.},
	month = feb,
	year = {2018},
	pages = {20170119},
	file = {Gallistel - 2018 - Finding numbers in the brain.pdf:/home/niklas/Zotero/storage/6M4BXWVF/Gallistel - 2018 - Finding numbers in the brain.pdf:application/pdf}
}

@book{dehaene_number_2011,
	title = {The {Number} {Sense}: {How} the {Mind} {Creates} {Mathematics}, {Revised} and {Updated} {Edition}},
	isbn = {978-0-19-991039-7},
	shorttitle = {The {Number} {Sense}},
	abstract = {Our understanding of how the human brain performs mathematical calculations is far from complete, but in recent years there have been many exciting breakthroughs by scientists all over the world. Now, in The Number Sense, Stanislas Dehaene offers a fascinating look at this recent research, in an enlightening exploration of the mathematical mind. Dehaene begins with the eye-opening discovery that animals--including rats, pigeons, raccoons, and chimpanzees--can perform simple mathematical calculations, and that human infants also have a rudimentary number sense. Dehaene suggests that this rudimentary number sense is as basic to the way the brain understands the world as our perception of color or of objects in space, and, like these other abilities, our number sense is wired into the brain. These are but a few of the wealth of fascinating observations contained here. We also discover, for example, that because Chinese names for numbers are so short, Chinese people can remember up to nine or ten digits at a time--English-speaking people can only remember seven. The book also explores the unique abilities of idiot savants and mathematical geniuses, and we meet people whose minute brain lesions render their mathematical ability useless. This new and completely updated edition includes all of the most recent scientific data on how numbers are encoded by single neurons, and which brain areas activate when we perform calculations. Perhaps most important, The Number Sense reaches many provocative conclusions that will intrigue anyone interested in learning, mathematics, or the mind. "A delight." --Ian Stewart, New Scientist "Read The Number Sense for its rich insights into matters as varying as the cuneiform depiction of numbers, why Jean Piaget's theory of stages in infant learning is wrong, and to discover the brain regions involved in the number sense." --The New York Times Book Review "Dehaene weaves the latest technical research into a remarkably lucid and engrossing investigation. Even readers normally indifferent to mathematics will find themselves marveling at the wonder of minds making numbers." --Booklist},
	language = {en},
	publisher = {Oxford University Press},
	author = {Dehaene, Stanislas},
	month = apr,
	year = {2011},
	note = {Google-Books-ID: 1p6XWYuwpjUC},
	keywords = {Mathematics / General, Psychology / Cognitive Psychology \& Cognition, Science / Life Sciences / Neuroscience}
}

@article{faber_neural_2020,
	title = {Neural {Status} {Registers}},
	url = {http://arxiv.org/abs/2004.07085},
	abstract = {Neural networks excel at approximating functions and ﬁnding patterns in complex and challenging domains. Yet, they fail to learn simple but precise computation. Recent work addressed the ability to add, subtract, and multiply numbers but is lacking a component to drive control ﬂow. True computer intelligence should also be able to decide when to perform what operation. In this paper, we introduce the Neural Status Register (NSR), inspired by physical Status Registers. At the heart of the NSR are arithmetic comparisons between inputs. With theoretically principled changes to physical Status Registers, the NSR allows end-toend diﬀerentiation and learns such comparisons reliably. But the NSR also extrapolates: it generalizes to unseen data distributions. For example, the NSR trains on single digits and correctly predicts numbers that are up to 14 orders of magnitude larger. This suggests that the NSR captures the true underlying arithmetic. In follow-up experiments, we use the NSR to control the computation of a downstream arithmetic unit to learn piecewise functions. We can also learn more challenging tasks through redundancy. Finally, we use the NSR to learn an upstream convolutional neural network to compare images of MNIST digits to decide which image contains the larger digit.},
	language = {en},
	urldate = {2020-04-23},
	journal = {arXiv:2004.07085 [cs, stat]},
	author = {Faber, Lukas and Wattenhofer, Roger},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.07085},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Faber and Wattenhofer - 2020 - Neural Status Registers.pdf:/home/niklas/Zotero/storage/HAU3KHJF/Faber and Wattenhofer - 2020 - Neural Status Registers.pdf:application/pdf}
}

@article{kaiser_neural_2016,
	title = {Neural {GPUs} {Learn} {Algorithms}},
	url = {http://arxiv.org/abs/1511.08228},
	abstract = {Learning an algorithm from examples is a fundamental problem that has been widely studied. It has been addressed using neural networks too, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded.},
	language = {en},
	urldate = {2020-04-23},
	journal = {arXiv:1511.08228 [cs]},
	author = {Kaiser, \L{}ukasz and Sutskever, Ilya},
	month = mar,
	year = {2016},
	note = {arXiv: 1511.08228},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {Kaiser and Sutskever - 2016 - Neural GPUs Learn Algorithms.pdf:/home/niklas/Zotero/storage/Q8KJHJMR/Kaiser and Sutskever - 2016 - Neural GPUs Learn Algorithms.pdf:application/pdf}
}

@article{rackauckas_differentialequationsjl_2017,
	title = {{DifferentialEquations}.jl – {A} {Performant} and {Feature}-{Rich} {Ecosystem} for {Solving} {Differential} {Equations} in {Julia}},
	volume = {5},
	issn = {2049-9647},
	url = {http://openresearchsoftware.metajnl.com/articles/10.5334/jors.151/},
	doi = {10.5334/jors.151},
	language = {en},
	urldate = {2020-04-07},
	journal = {Journal of Open Research Software},
	author = {Rackauckas, Christopher and Nie, Qing},
	month = may,
	year = {2017},
	pages = {15},
	file = {Rackauckas and Nie - 2017 - DifferentialEquations.jl – A Performant and Featur.pdf:/home/niklas/Zotero/storage/YGABHTNL/Rackauckas and Nie - 2017 - DifferentialEquations.jl – A Performant and Featur.pdf:application/pdf}
}

@article{madsen_neural_2020,
    author = {Andreas Madsen and Alexander Rosenberg Johansen},
    title = {{Neural Arithmetic Units}},
    booktitle = {8th International Conference on Learning Representations, ICLR 2020},
    volume = {abs/2001.05016},
    year = {2020},
    url = {http://arxiv.org/abs/2001.05016},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    arxivId = {2001.05016},
    eprint={2001.05016}
}

@article{kermack_contribution_1927,
	title = {A contribution to the mathematical theory of epidemics},
	volume = {115},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1927.0118},
	doi = {10.1098/rspa.1927.0118},
	abstract = {(1) One of the most striking features in the study of epidemics is the difficulty of finding a causal factor which appears to be adequate to account for the magnitude of the frequent epidemics of disease which visit almost every population. It was with a view to obtaining more insight regarding the effects of the various factors which govern the spread of contagious epidemics that the present investigation was undertaken. Reference may here be made to the work of Ross and Hudson (1915-17) in which the same problem is attacked. The problem is here carried to a further stage, and it is considered from a point of view which is in one sense more general. The problem may be summarised as follows: One (or more) infected person is introduced into a community of individuals, more or less susceptible to the disease in question. The disease spreads from the affected to the unaffected by contact infection. Each infected person runs through the course of his sickness, and finally is removed from the number of those who are sick, by recovery or by death. The chances of recovery or death vary from day to day during the course of his illness. The chances that the affected may convey infection to the unaffected are likewise dependent upon the stage of the sickness. As the epidemic spreads, the number of unaffected members of the community becomes reduced. Since the course of an epidemic is short compared with the life of an individual, the population may be considered as remaining constant, except in as far as it is modified by deaths due to the epidemic disease itself. In the course of time the epidemic may come to an end. One of the most important probems in epidemiology is to ascertain whether this termination occurs only when no susceptible individuals are left, or whether the interplay of the various factors of infectivity, recovery and mortality, may result in termination, whilst many susceptible individuals are still present in the unaffected population. It is difficult to treat this problem in its most general aspect. In the present communication discussion will be limited to the case in which all members of the community are initially equally susceptible to the disease, and it will be further assumed that complete immunity is conferred by a single infection.},
	number = {772},
	urldate = {2020-09-30},
	journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
	author = {Kermack, William Ogilvy and McKendrick, A. G. and Walker, Gilbert Thomas},
	month = aug,
	year = {1927},
	note = {Publisher: Royal Society},
	pages = {700--721},
	file = {Full Text PDF:/home/niklas/Zotero/storage/XJPYNVCZ/Kermack et al. - 1927 - A contribution to the mathematical theory of epide.pdf:application/pdf;Snapshot:/home/niklas/Zotero/storage/QZCGP3D2/rspa.1927.html:text/html}
}

@article{lipton_mythos_2017,
	title = {The {Mythos} of {Model} {Interpretability}},
	url = {http://arxiv.org/abs/1606.03490},
	abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
	urldate = {2020-10-06},
	journal = {arXiv:1606.03490 [cs, stat]},
	author = {Lipton, Zachary C.},
	month = mar,
	year = {2017},
	note = {arXiv: 1606.03490},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/niklas/Zotero/storage/Q3SPWBW2/Lipton - 2017 - The Mythos of Model Interpretability.pdf:application/pdf;arXiv.org Snapshot:/home/niklas/Zotero/storage/H633IZAD/1606.html:text/html}
}
