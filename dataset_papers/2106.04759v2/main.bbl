\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\EM}{\em}
\providecommand{\RNtxt}{\relax}
\RNtxt{}

\bibitem[Alistarh et~al.(2017)D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka,
  M.~Vojnovic]{alistarh2017qsgd}
{\EM Alistarh Dan, Grubic Demjan, Li~Jerry, Tomioka Ryota, Vojnovic Milan}.
\newblock QSGD: Communication-efficient SGD via gradient quantization and
  encoding \allowbreak\newblock// Advances in Neural Information Processing
  Systems. 2017.  1709--1720.

\bibitem[Allen-Zhu et~al.(2018)Z.~Allen-Zhu, Y.~Li,
  Z.~Song]{allen2018convergence}
{\EM Allen-Zhu Zeyuan, Li~Yuanzhi, Song Zhao}.
\newblock On the convergence rate of training recurrent neural networks
  \allowbreak\newblock// arXiv preprint arXiv:1810.12065. 2018.

\bibitem[Arjevani, Shamir(2015)Y.~Arjevani,
  O.~Shamir]{arjevani2015communication}
{\EM Arjevani Yossi, Shamir Ohad}.
\newblock Communication complexity of distributed convex learning and
  optimization \allowbreak\newblock// Advances in neural information processing
  systems. 2015.  1756--1764.

\bibitem[Beck(2014)A.~Beck]{beck2014introduction}
{\EM Beck Amir}.
\newblock Introduction to nonlinear optimization: Theory, algorithms, and
  applications with MATLAB. 2014.

\bibitem[Bijral et~al.(2016)A.~S. Bijral, A.~D. Sarwate,
  N.~Srebro]{bijral2016data}
{\EM Bijral Avleen~S, Sarwate Anand~D, Srebro Nathan}.
\newblock On data dependence in distributed stochastic optimization
  \allowbreak\newblock// arXiv preprint arXiv:1603.04379. 2016.

\bibitem[Chang, Lin(2011)C.-C. Chang, C.-J. Lin]{CC01a}
{\EM Chang Chih-Chung, Lin Chih-Jen}.
\newblock {LIBSVM}: A library for support vector machines
  \allowbreak\newblock// ACM Transactions on Intelligent Systems and
  Technology. 2011. 2. 27:1--27:27.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Charles, Papailiopoulos(2018)Z.~Charles,
  D.~Papailiopoulos]{charles2018stability}
{\EM Charles Zachary, Papailiopoulos Dimitris}.
\newblock Stability and generalization of learning algorithms that converge to
  global optima \allowbreak\newblock// International Conference on Machine
  Learning. 2018.  745--754.

\bibitem[Chen et~al.(2012)X.~Chen, A.~Eversole, G.~Li, D.~Yu,
  F.~Seide]{chen2012pipelined}
{\EM Chen Xie, Eversole Adam, Li~Gang, Yu~Dong, Seide Frank}.
\newblock Pipelined back-propagation for context-dependent deep neural networks
  \allowbreak\newblock// Thirteenth Annual Conference of the International
  Speech Communication Association. 2012.

\bibitem[Dekel et~al.(2012)O.~Dekel, R.~Gilad-Bachrach, O.~Shamir,
  L.~Xiao]{dekel2012optimal}
{\EM Dekel Ofer, Gilad-Bachrach Ran, Shamir Ohad, Xiao Lin}.
\newblock Optimal distributed online prediction using mini-batches
  \allowbreak\newblock// Journal of Machine Learning Research. 2012. 13, Jan.
  165--202.

\bibitem[Dieuleveut, Patel(2019)A.~Dieuleveut, K.~K.
  Patel]{dieuleveut2019communication}
{\EM Dieuleveut Aymeric, Patel Kumar~Kshitij}.
\newblock Communication trade-offs for Local-SGD with large step size
  \allowbreak\newblock// Advances in Neural Information Processing Systems.
  2019.  13579--13590.

\bibitem[Godichon-Baggioni, Saadane(2020)A.~Godichon-Baggioni,
  S.~Saadane]{godichon2020rates}
{\EM Godichon-Baggioni Antoine, Saadane Sofiane}.
\newblock On the rates of convergence of parallelized averaged stochastic
  gradient algorithms \allowbreak\newblock// Statistics. 2020. 54, 3. 618--635.

\bibitem[Grubic et~al.(2018)D.~Grubic, L.~K. Tam, D.~Alistarh,
  C.~Zhang]{grubic2018synchronous}
{\EM Grubic Demjan, Tam Leo~K, Alistarh Dan, Zhang Ce}.
\newblock Synchronous multi-gpu deep learning with low-precision communication:
  An experimental study \allowbreak\newblock// Proceedings of the 21st
  International Conference on Extending Database Technology. 2018.  145--156.

\bibitem[Haddadpour et~al.(2019)F.~Haddadpour, M.~M. Kamani, M.~Mahdavi,
  V.~Cadambe]{haddadpour2019local}
{\EM Haddadpour Farzin, Kamani Mohammad~Mahdi, Mahdavi Mehrdad, Cadambe
  Viveck}.
\newblock Local SGD with periodic averaging: Tighter analysis and adaptive
  synchronization \allowbreak\newblock// Advances in Neural Information
  Processing Systems. 2019.  11080--11092.

\bibitem[Hendrikx et~al.(2019)H.~Hendrikx, F.~Bach,
  L.~Massouli{\'e}]{hendrikx2019accelerated}
{\EM Hendrikx Hadrien, Bach Francis, Massouli{\'e} Laurent}.
\newblock An accelerated decentralized stochastic proximal algorithm for finite
  sums \allowbreak\newblock// Advances in Neural Information Processing
  Systems. 2019.  952--962.

\bibitem[Kairouz et~al.(2019)P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet,
  M.~Bennis, A.~N. Bhagoji, K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings,
  et~al.]{kairouz2019advances}
{\EM Kairouz Peter, McMahan H~Brendan, Avent Brendan, Bellet Aur{\'e}lien,
  Bennis Mehdi, Bhagoji Arjun~Nitin, Bonawitz Keith, Charles Zachary, Cormode
  Graham, Cummings Rachel, others }.
\newblock Advances and open problems in federated learning
  \allowbreak\newblock// arXiv preprint arXiv:1912.04977. 2019.

\bibitem[Karimi et~al.(2016)H.~Karimi, J.~Nutini, M.~Schmidt]{karimi2016linear}
{\EM Karimi Hamed, Nutini Julie, Schmidt Mark}.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition \allowbreak\newblock// Joint European
  Conference on Machine Learning and Knowledge Discovery in Databases. 2016.
  795--811.

\bibitem[Khaled et~al.(2020)A.~Khaled, K.~Mishchenko,
  P.~Richt{\'a}rik]{khaled2019tighter}
{\EM Khaled A, Mishchenko K, Richt{\'a}rik P}.
\newblock Tighter theory for local SGD on identical and heterogeneous data
  \allowbreak\newblock// The 23rd International Conference on Artificial
  Intelligence and Statistics (AISTATS 2020). 2020.

\bibitem[Koloskova et~al.(2020)A.~Koloskova, N.~Loizou, S.~Boreiri, M.~Jaggi,
  S.~U. Stich]{koloskova2020unified}
{\EM Koloskova Anastasia, Loizou Nicolas, Boreiri Sadra, Jaggi Martin, Stich
  Sebastian~U}.
\newblock A Unified Theory of Decentralized SGD with Changing Topology and
  Local Updates \allowbreak\newblock// arXiv preprint arXiv:2003.10422. 2020.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)J.~Kone{\v{c}}n{\`y}, H.~B. McMahan,
  F.~X. Yu, P.~Richt{\'a}rik, A.~T. Suresh, D.~Bacon]{konevcny2016federated}
{\EM Kone{\v{c}}n{\`y} Jakub, McMahan H~Brendan, Yu~Felix~X, Richt{\'a}rik
  Peter, Suresh Ananda~Theertha, Bacon Dave}.
\newblock Federated learning: Strategies for improving communication efficiency
  \allowbreak\newblock// arXiv preprint arXiv:1610.05492. 2016.

\bibitem[Lin et~al.(2018{\natexlab{a}})T.~Lin, S.~U. Stich, K.~K. Patel,
  M.~Jaggi]{lin2018don}
{\EM Lin Tao, Stich Sebastian~U, Patel Kumar~Kshitij, Jaggi Martin}.
\newblock Don't Use Large Mini-Batches, Use Local SGD \allowbreak\newblock//
  arXiv preprint arXiv:1808.07217. 2018{\natexlab{a}}.

\bibitem[Lin et~al.(2018{\natexlab{b}})Y.~Lin, S.~Han, H.~Mao, Y.~Wang,
  B.~Dally]{lin2017deep}
{\EM Lin Yujun, Han Song, Mao Huizi, Wang Yu, Dally Bill}.
\newblock Deep Gradient Compression: Reducing the Communication Bandwidth for
  Distributed Training \allowbreak\newblock// International Conference on
  Learning Representations. 2018{\natexlab{b}}.

\bibitem[Madden et~al.(2020)L.~Madden, E.~Dall'Anese,
  S.~Becker]{madden2020high}
{\EM Madden Liam, Dall'Anese Emiliano, Becker Stephen}.
\newblock High probability convergence and uniform stability bounds for
  nonconvex stochastic gradient descent \allowbreak\newblock// arXiv preprint
  arXiv:2006.05610. 2020.

\bibitem[Mangasarian(1995)L.~Mangasarian]{mangasarian1995parallel}
{\EM Mangasarian LO}.
\newblock Parallel gradient distribution in unconstrained optimization
  \allowbreak\newblock// SIAM Journal on Control and Optimization. 1995. 33, 6.
  1916--1925.

\bibitem[McDonald et~al.(2010)R.~McDonald, K.~Hall,
  G.~Mann]{mcdonald2010distributed}
{\EM McDonald Ryan, Hall Keith, Mann Gideon}.
\newblock Distributed training strategies for the structured perceptron
  \allowbreak\newblock// Human language technologies: The 2010 annual
  conference of the North American chapter of the association for computational
  linguistics. 2010.  456--464.

\bibitem[McMahan et~al.(2017)B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, B.~A.
  y~Arcas]{mcmahan2016communication}
{\EM McMahan Brendan, Moore Eider, Ramage Daniel, Hampson Seth, Arcas
  Blaise~Aguera y}.
\newblock Communication-Efficient Learning of Deep Networks from Decentralized
  Data \allowbreak\newblock// Artificial Intelligence and Statistics. 2017.
  1273--1282.

\bibitem[Mcdonald et~al.(2009)R.~Mcdonald, M.~Mohri, N.~Silberman, D.~Walker,
  G.~S. Mann]{mcdonald2009efficient}
{\EM Mcdonald Ryan, Mohri Mehryar, Silberman Nathan, Walker Dan, Mann
  Gideon~S}.
\newblock Efficient large-scale distributed training of conditional maximum
  entropy models \allowbreak\newblock// Advances in neural information
  processing systems. 2009.  1231--1239.

\bibitem[Rosenblatt, Nadler(2016)J.~D. Rosenblatt,
  B.~Nadler]{rosenblatt2016optimality}
{\EM Rosenblatt Jonathan~D, Nadler Boaz}.
\newblock On the optimality of averaging in distributed statistical learning
  \allowbreak\newblock// Information and Inference: A Journal of the IMA. 2016.
  5, 4. 379--404.

\bibitem[Schmidt, Roux(2013)M.~Schmidt, N.~L. Roux]{schmidt2013fast}
{\EM Schmidt Mark, Roux Nicolas~Le}.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition \allowbreak\newblock// arXiv preprint arXiv:1308.6370. 2013.

\bibitem[Sergeev, Del~Balso(2018)A.~Sergeev, M.~Del~Balso]{sergeev2018horovod}
{\EM Sergeev Alexander, Del~Balso Mike}.
\newblock Horovod: fast and easy distributed deep learning in TensorFlow
  \allowbreak\newblock// arXiv preprint arXiv:1802.05799. 2018.

\bibitem[Spiridonoff et~al.(2020)A.~Spiridonoff, A.~Olshevsky, I.~C.
  Paschalidis]{olshevsky2018robust}
{\EM Spiridonoff Artin, Olshevsky Alex, Paschalidis Ioannis~Ch}.
\newblock Robust asynchronous stochastic gradient-push: asymptotically optimal
  and network-independent performance for strongly convex functions
  \allowbreak\newblock// Journal of Machine Learning Research. 2020.

\bibitem[Stich(2019)S.~U. Stich]{stich2018local}
{\EM Stich Sebastian~U.}
\newblock Local {SGD} Converges Fast and Communicates Little
  \allowbreak\newblock// International Conference on Learning Representations.
  2019.

\bibitem[Stich et~al.(2018)S.~U. Stich, J.-B. Cordonnier,
  M.~Jaggi]{stich2018sparsified}
{\EM Stich Sebastian~U, Cordonnier Jean-Baptiste, Jaggi Martin}.
\newblock Sparsified SGD with memory \allowbreak\newblock// Advances in Neural
  Information Processing Systems. 2018.  4447--4458.

\bibitem[Stich, Karimireddy(2019)S.~U. Stich, S.~P.
  Karimireddy]{stich2019error}
{\EM Stich Sebastian~U, Karimireddy Sai~Praneeth}.
\newblock The error-feedback framework: Better rates for SGD with delayed
  gradients and compressed communication \allowbreak\newblock// arXiv preprint
  arXiv:1909.05350. 2019.

\bibitem[Tang et~al.(2020)Z.~Tang, S.~Shi, X.~Chu, W.~Wang,
  B.~Li]{tang2020communication}
{\EM Tang Zhenheng, Shi Shaohuai, Chu Xiaowen, Wang Wei, Li~Bo}.
\newblock Communication-Efficient Distributed Deep Learning: A Comprehensive
  Survey \allowbreak\newblock// arXiv preprint arXiv:2003.06307. 2020.

\bibitem[Wang, Joshi(2018{\natexlab{a}})J.~Wang, G.~Joshi]{wang2018adaptive}
{\EM Wang Jianyu, Joshi Gauri}.
\newblock Adaptive communication strategies to achieve the best error-runtime
  trade-off in local-update SGD \allowbreak\newblock// Systems for ML.
  2018{\natexlab{a}}.

\bibitem[Wang, Joshi(2018{\natexlab{b}})J.~Wang, G.~Joshi]{wang2018cooperative}
{\EM Wang Jianyu, Joshi Gauri}.
\newblock Cooperative SGD: A unified framework for the design and analysis of
  communication-efficient SGD algorithms \allowbreak\newblock// arXiv preprint
  arXiv:1808.07576. 2018{\natexlab{b}}.

\bibitem[Wang et~al.(2019)J.~Wang, A.~K. Sahu, Z.~Yang, G.~Joshi,
  S.~Kar]{wang2019matcha}
{\EM Wang Jianyu, Sahu Anit~Kumar, Yang Zhouyi, Joshi Gauri, Kar Soummya}.
\newblock MATCHA: Speeding Up Decentralized SGD via Matching Decomposition
  Sampling \allowbreak\newblock// arXiv preprint arXiv:1905.09435. 2019.

\bibitem[Wangni et~al.(2018)J.~Wangni, J.~Wang, J.~Liu,
  T.~Zhang]{wangni2018gradient}
{\EM Wangni Jianqiao, Wang Jialei, Liu Ji, Zhang Tong}.
\newblock Gradient sparsification for communication-efficient distributed
  optimization \allowbreak\newblock// Advances in Neural Information Processing
  Systems. 2018.  1299--1309.

\bibitem[Woodworth et~al.(2020)B.~Woodworth, K.~K. Patel, S.~U. Stich, Z.~Dai,
  B.~Bullins, H.~B. McMahan, O.~Shamir, N.~Srebro]{woodworth2020local}
{\EM Woodworth Blake, Patel Kumar~Kshitij, Stich Sebastian~U, Dai Zhen, Bullins
  Brian, McMahan H~Brendan, Shamir Ohad, Srebro Nathan}.
\newblock Is Local SGD Better than Minibatch SGD? \allowbreak\newblock// arXiv
  preprint arXiv:2002.07839. 2020.

\bibitem[Yadan et~al.(2013)O.~Yadan, K.~Adams, Y.~Taigman,
  M.~Ranzato]{yadan2013multi}
{\EM Yadan Omry, Adams Keith, Taigman Yaniv, Ranzato Marc'Aurelio}.
\newblock Multi-gpu training of convnets \allowbreak\newblock// arXiv preprint
  arXiv:1312.5853. 2013.

\bibitem[Yu et~al.(2019)H.~Yu, S.~Yang, S.~Zhu]{yu2019parallel}
{\EM Yu~Hao, Yang Sen, Zhu Shenghuo}.
\newblock Parallel restarted SGD with faster convergence and less
  communication: Demystifying why model averaging works for deep learning
  \allowbreak\newblock// Proceedings of the AAAI Conference on Artificial
  Intelligence.  33. 2019.  5693--5700.

\bibitem[Yuan, Ma(2020)H.~Yuan, T.~Ma]{FedAC}
{\EM Yuan Honglin, Ma~Tengyu}.
\newblock Federated Accelerated Stochastic Gradient Descent
  \allowbreak\newblock// Advances in Neural Information Processing Systems.
  33. 2020.  5332--5344.

\bibitem[Zhang et~al.(2016)J.~Zhang, C.~De~Sa, I.~Mitliagkas,
  C.~R{\'e}]{zhang2016parallel}
{\EM Zhang Jian, De~Sa Christopher, Mitliagkas Ioannis, R{\'e}~Christopher}.
\newblock Parallel SGD: When does averaging help? \allowbreak\newblock// arXiv
  preprint arXiv:1606.07365. 2016.

\bibitem[Zhang et~al.(2013{\natexlab{a}})S.~Zhang, C.~Zhang, Z.~You, R.~Zheng,
  B.~Xu]{zhang2013asynchronous}
{\EM Zhang Shanshan, Zhang Ce, You Zhao, Zheng Rong, Xu~Bo}.
\newblock Asynchronous stochastic gradient descent for DNN training
  \allowbreak\newblock// 2013 IEEE International Conference on Acoustics,
  Speech and Signal Processing. 2013{\natexlab{a}}.  6660--6663.

\bibitem[Zhang et~al.(2015)S.~Zhang, A.~E. Choromanska,
  Y.~LeCun]{zhang2015deep}
{\EM Zhang Sixin, Choromanska Anna~E, LeCun Yann}.
\newblock Deep learning with elastic averaging SGD \allowbreak\newblock//
  Advances in neural information processing systems. 2015.  685--693.

\bibitem[Zhang et~al.(2013{\natexlab{b}})Y.~Zhang, J.~Duchi, M.~I. Jordan,
  M.~J. Wainwright]{zhang2013information}
{\EM Zhang Yuchen, Duchi John, Jordan Michael~I, Wainwright Martin~J}.
\newblock Information-theoretic lower bounds for distributed statistical
  estimation with communication constraints \allowbreak\newblock// Advances in
  Neural Information Processing Systems. 2013{\natexlab{b}}.  2328--2336.

\bibitem[Zhang et~al.(2013{\natexlab{c}})Y.~Zhang, J.~C. Duchi, M.~J.
  Wainwright]{zhang2013communication}
{\EM Zhang Yuchen, Duchi John~C, Wainwright Martin~J}.
\newblock Communication-efficient algorithms for statistical optimization
  \allowbreak\newblock// Journal of Machine Learning Research.
  2013{\natexlab{c}}. 14, 1. 3321--3363.

\bibitem[Zhou, Cong(2018)F.~Zhou, G.~Cong]{zhou2017convergence}
{\EM Zhou Fan, Cong Guojing}.
\newblock On the convergence properties of a K-step averaging stochastic
  gradient descent algorithm for nonconvex optimization \allowbreak\newblock//
  Proceedings of the 27th International Joint Conference on Artificial
  Intelligence. 2018.  3219--3227.

\bibitem[Zinkevich et~al.(2010)M.~Zinkevich, M.~Weimer, L.~Li, A.~J.
  Smola]{zinkevich2010parallelized}
{\EM Zinkevich Martin, Weimer Markus, Li~Lihong, Smola Alex~J}.
\newblock Parallelized stochastic gradient descent \allowbreak\newblock//
  Advances in neural information processing systems. 2010.  2595--2603.

\end{thebibliography}
