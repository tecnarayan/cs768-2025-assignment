\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C., and
  Weisz, G.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3692--3702. PMLR, 2019.

\bibitem[Agarwal et~al.(2020)Agarwal, Henaff, Kakade, and Sun]{agarwal2020pc}
Agarwal, A., Henaff, M., Kakade, S., and Sun, W.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 13399--13412, 2020.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.~E.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM journal on computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, and
  Kakade]{bubeck2012towards}
Bubeck, S., Cesa-Bianchi, N., and Kakade, S.~M.
\newblock Towards minimax policies for online linear optimization with bandit
  feedback.
\newblock In \emph{Conference on Learning Theory}, pp.\  41--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1283--1294. PMLR, 2020.

\bibitem[Dani et~al.(2008)Dani, Kakade, and Hayes]{dani2008price}
Dani, V., Kakade, S.~M., and Hayes, T.
\newblock The price of bandit information for online optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 20, 2008.

\bibitem[Foster et~al.(2016)Foster, Li, Lykouris, Sridharan, and
  Tardos]{foster2016learning}
Foster, D.~J., Li, Z., Lykouris, T., Sridharan, K., and Tardos, E.
\newblock Learning in games: Robustness of fast convergence.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[He et~al.(2022)He, Zhou, and Gu]{he2022near}
He, J., Zhou, D., and Gu, Q.
\newblock Near-optimal policy optimization algorithms for learning adversarial
  linear mixture mdps.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4259--4280. PMLR, 2022.

\bibitem[Ito(2021)]{ito2021parameter}
Ito, S.
\newblock Parameter-free multi-armed bandit algorithms with hybrid
  data-dependent regret bounds.
\newblock In \emph{Conference on Learning Theory}, pp.\  2552--2583. PMLR,
  2021.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Jin, Luo, Sra, and
  Yu]{jin2020learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4860--4869. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020{\natexlab{b}}.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kong et~al.(2023)Kong, Zhang, Wang, and Li]{kong2023improved}
Kong, F., Zhang, X., Wang, B., and Li, S.
\newblock Improved regret bounds for linear adversarial mdps via linear
  optimization.
\newblock \emph{arXiv preprint arXiv:2302.06834}, 2023.

\bibitem[Lancewicki et~al.(2023)Lancewicki, Rosenberg, and
  Sotnikov]{lancewicki2023delay}
Lancewicki, T., Rosenberg, A., and Sotnikov, D.
\newblock Delay-adapted policy optimization and improved regret for adversarial
  mdp with delayed bandit feedback.
\newblock \emph{arXiv preprint arXiv:2305.07911}, 2023.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and
  Szepesv{\'a}ri]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Luo et~al.(2021{\natexlab{a}})Luo, Wei, and Lee]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock \emph{arXiv preprint arXiv:2107.08346}, 2021{\natexlab{a}}.

\bibitem[Luo et~al.(2021{\natexlab{b}})Luo, Wei, and
  Lee]{luo2021policy_nipsver}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22931--22942, 2021{\natexlab{b}}.

\bibitem[Meng \& Zheng(2010)Meng and Zheng]{meng2010optimal}
Meng, L. and Zheng, B.
\newblock The optimal perturbation bounds of the moore--penrose inverse under
  the frobenius norm.
\newblock \emph{Linear algebra and its applications}, 432\penalty0
  (4):\penalty0 956--963, 2010.

\bibitem[Neu \& Olkhovskaya(2020)Neu and Olkhovskaya]{neu2020efficient}
Neu, G. and Olkhovskaya, J.
\newblock Efficient and robust algorithms for adversarial linear contextual
  bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  3049--3068. PMLR,
  2020.

\bibitem[Neu \& Olkhovskaya(2021)Neu and Olkhovskaya]{neu2021online}
Neu, G. and Olkhovskaya, J.
\newblock Online learning in mdps with linear function approximation and bandit
  feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10407--10417, 2021.

\bibitem[Putta \& Agrawal(2022)Putta and Agrawal]{putta2022scale}
Putta, S.~R. and Agrawal, S.
\newblock Scale-free adversarial multi armed bandits.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  910--930. PMLR, 2022.

\bibitem[Rosenberg \& Mansour(2019)Rosenberg and Mansour]{rosenberg2019online}
Rosenberg, A. and Mansour, Y.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5478--5486. PMLR, 2019.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8604--8613. PMLR, 2020.

\bibitem[Sherman et~al.(2023)Sherman, Koren, and Mansour]{sherman2023improved}
Sherman, U., Koren, T., and Mansour, Y.
\newblock Improved regret for efficient online reinforcement learning with
  linear function approximation.
\newblock \emph{arXiv preprint arXiv:2301.13087}, 2023.

\bibitem[Tropp(2012)]{tropp2012user}
Tropp, J.~A.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of computational mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Wagenmaker \& Jamieson(2022)Wagenmaker and
  Jamieson]{wagenmaker2022instance}
Wagenmaker, A. and Jamieson, K.~G.
\newblock Instance-dependent near-optimal policy identification in linear mdps
  via online experiment design.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 5968--5981, 2022.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Salakhutdinov]{wang2020reward}
Wang, R., Du, S.~S., Yang, L., and Salakhutdinov, R.~R.
\newblock On reward-free reinforcement learning with linear function
  approximation.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17816--17826, 2020.

\bibitem[Wei \& Luo(2018)Wei and Luo]{wei2018more}
Wei, C.-Y. and Luo, H.
\newblock More adaptive algorithms for adversarial bandits.
\newblock In \emph{Conference On Learning Theory}, pp.\  1263--1291. PMLR,
  2018.

\bibitem[Wei et~al.(2021)Wei, Jahromi, Luo, and Jain]{wei2021learning}
Wei, C.-Y., Jahromi, M.~J., Luo, H., and Jain, R.
\newblock Learning infinite-horizon average-reward mdps with linear function
  approximation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3007--3015. PMLR, 2021.

\bibitem[Yang \& Wang(2020)Yang and Wang]{yang2020reinforcement}
Yang, L. and Wang, M.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10746--10756. PMLR, 2020.

\bibitem[Zanette et~al.(2021)Zanette, Cheng, and
  Agarwal]{zanette2021cautiously}
Zanette, A., Cheng, C.-A., and Agarwal, A.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  4473--4525. PMLR,
  2021.

\bibitem[Zheng et~al.(2019)Zheng, Luo, Diakonikolas, and
  Wang]{zheng2019equipping}
Zheng, K., Luo, H., Diakonikolas, I., and Wang, L.
\newblock Equipping experts/bandits with long-term memory.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhou et~al.(2021)Zhou, He, and Gu]{zhou2021provably}
Zhou, D., He, J., and Gu, Q.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12793--12802. PMLR, 2021.

\bibitem[Zimin \& Neu(2013)Zimin and Neu]{zimin2013online}
Zimin, A. and Neu, G.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Zimmert \& Lattimore(2022)Zimmert and Lattimore]{zimmert2022return}
Zimmert, J. and Lattimore, T.
\newblock Return of the bias: Almost minimax optimal high probability bounds
  for adversarial linear bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  3285--3312. PMLR,
  2022.

\end{thebibliography}
