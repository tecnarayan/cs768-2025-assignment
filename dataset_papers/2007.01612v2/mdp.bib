@article{Ora19,
  title={A modern introduction to online learning},
  author={Orabona, Francesco},
  journal={arXiv preprint arXiv:1912.13213},
  year={2019}
}


@article{PLBN21,
  title={Near Optimal Policy Optimization via {REPS}},
  author={Pacchiano, Aldo and Lee, Jonathan and Bartlett, Peter and Nachum, Ofir},
  journal={arXiv preprint arXiv:2103.09756},
  year={2021}
}

@inproceedings{PMA10,
  author    = {Jan Peters and
               Katharina M{\"u}lling and
               Yasemin Altun},
  title     = {Relative Entropy Policy Search},
  pages     = {1607--1612},
  crossref  = {AAAI2010}}


@article{Pen94,
	title = {Sub-hessians, super-hessians and conjugation},
	journal = {Nonlinear Analysis: Theory, Methods and Applications},
	volume = {23},
	number = {6},
	pages = {689-702},
	year = {1994},
	issn = {0362-546X},
	doi = {https://doi.org/10.1016/0362-546X(94)90212-7},
	url = {https://www.sciencedirect.com/science/article/pii/0362546X94902127},
	author = {Jean-Paul Penot},
	keywords = {Conjugacy, convex function, ellipsoidal subdifferential derivatives, generalized derivatives, hessian, Legendre-Fenchel transform, polarity, sub-hessian, super-hessian}
}


@inproceedings{MB04,
	Author = {McMahan, H. Brendan and Blum, Avrim},
	Pages = {109--123},
	Title = {Online Geometric Optimization in the Bandit Setting Against an Adaptive Adversary},
	Crossref = {COLT2004}}


@inproceedings{AK04,
	Author = {Awerbuch, Baruch and Kleinberg, Robert D.},
	Pages = {45--53},
	Title = {Adaptive routing with end-to-end feedback: distributed learning and geometric approaches},
	Crossref = {stoc2004}}

@inproceedings{qreps,
  title={Logistic {Q}-Learning},
  author={Bas-Serrano, Joan and Curi, Sebastian and Krause, Andreas and Neu, Gergely},
  booktitle={AI \& Statistics},
  pages = {3610-3618},
  year={2021}
}

@article{CESABIANCHI20121404,
	title = "Combinatorial bandits",
	journal = "Journal of Computer and System Sciences",
	volume = "78",
	number = "5",
	pages = "1404 - 1422",
	year = "2012",
	note = "JCSS Special Issue: Cloud Computing 2011",
	issn = "0022-0000",
	doi = "https://doi.org/10.1016/j.jcss.2012.01.001",
	url = "http://www.sciencedirect.com/science/article/pii/S0022000012000219",
	author = "Nicolo\` Cesa-Bianchi and Gabor Lugosi",
	keywords = "Online prediction, Adversarial bandit problems, Online linear optimization"
}

@misc{lykouris2020corruption,
	title={Corruption robust exploration in episodic reinforcement learning}, 
	author={Thodoris Lykouris and Max Simchowitz and Aleksandrs Slivkins and Wen Sun},
	year={2020},
	eprint={1911.08689},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{wang2020rewardfree,
	title={On Reward-Free Reinforcement Learning with Linear Function Approximation}, 
	author={Ruosong Wang and Simon S. Du and Lin F. Yang and Ruslan Salakhutdinov},
	year={2020},
	eprint={2006.11274},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}




@InProceedings{wei2020learning,
  title = 	 { Learning Infinite-horizon Average-reward {MDPs} with Linear Function Approximation },
  author =       {Wei, Chen-Yu and Jafarnia Jahromi, Mehdi and Luo, Haipeng and Jain, Rahul},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3007--3015},
  year = 	 {2021}
}



@inproceedings{agarwal2020flambe,
	title={{FLAMBE}: Structural Complexity and Representation Learning of Low Rank {MDP}s},
	author={Alekh Agarwal and Sham Kakade and Akshay Krishnamurthy and Wen Sun},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	year={2020}
}

@inproceedings{Kakade2003OnTS,
  title={On the sample complexity of reinforcement learning.},
  author={Sham M. Kakade},
  year={2003}
}



@inproceedings{NPB20,
  title={A unifying view of optimism in episodic reinforcement learning},
  author={Neu, Gergely and Pike-Burke, Ciara},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{KaLa02,
	Author = {Kakade, Sham and Langford, John},
	booktitle = {International Conference on Machine Learning},
	year = {2002},
	Pages = {267--274},
	Title = {Approximately Optimal Approximate Reinforcement Learning}}

@inproceedings{JJLSY20,
  title={Learning Adversarial {MDPs} with Bandit Feedback and Unknown Transition},
  author={Jin, Chi and Jin, Tiancheng and Luo, Haipeng and Sra, Suvrit and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  year={2020}
}


@incollection{NIPS2007_3371,
	title = {The Price of Bandit Information for Online Optimization},
	author = {Varsha Dani and Kakade, Sham M and Thomas P. Hayes},
	booktitle = {Advances in Neural Information Processing Systems 20},
	pages = {345--352},
	year = {2008}
}

@article{10.1287/moor.1090.0396,
	author = {Even-Dar, Eyal and Kakade, Sham. M. and Mansour, Yishay},
	title = {Online {Markov} Decision Processes},
	year = {2009},
	publisher = {INFORMS},
	volume = {34},
	number = {3},
	journal = {Math. Oper. Res.},
	pages = {726–736}
}

@article{DBLP,
	author    = {Sergey Levine and
	Chelsea Finn and
	Trevor Darrell and
	Pieter Abbeel},
	title     = {End-to-End Training of Deep Visuomotor Policies},
	journal   = {CoRR},
	volume    = {abs/1504.00702},
	year      = {2015},
	url       = {http://arxiv.org/abs/1504.00702},
	archivePrefix = {arXiv},
	eprint    = {1504.00702},
	timestamp = {Mon, 13 Aug 2018 16:47:04 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/LevineFDA15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Jin2019LearningAM,
	title={Learning Adversarial MDPs with Bandit Feedback and Unknown Transition},
	author={Tiancheng Jin and Haipeng Luo},
	journal={ArXiv},
	year={2019},
	volume={abs/1912.01192}
}

@inproceedings{10.1145/3005745.3005750,
	author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
	title = {Resource Management with Deep Reinforcement Learning},
	year = {2016},
	isbn = {9781450346610},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3005745.3005750},
	doi = {10.1145/3005745.3005750},
	booktitle = {Proceedings of the 15th ACM Workshop on Hot Topics in Networks},
	pages = {50–56},
	numpages = {7},
	location = {Atlanta, GA, USA},
	series = {HotNets ’16}
}

@inproceedings{12986728,
	author = {Azar, Mohammad Gheshlaghi and Munos, Remi and Ghavamzadeh, Mohammad and Kappen, Hilbert J.},
	title = {Speedy Q-Learning},
	year = {2011},
	isbn = {9781618395993},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
	pages = {2411–2419},
	numpages = {9},
	location = {Granada, Spain},
	series = {NIPS’11}
}




@incollection{NIPS2010_4048,
	title = {Online Markov Decision Processes under Bandit Feedback},
	author = {Neu, Gergely and Andras Antos and Gy\"{o}rgy, Andr\'{a}s and {\text Cs}aba Szepesv\'{a}ri},
	booktitle = {Advances in Neural Information Processing Systems 23},
	editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
	pages = {1804--1812},
	year = {2010},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4048-online-markov-decision-processes-under-bandit-feedback.pdf}
}


@article{robotics,
	author = {Kober, Jens and Bagnell, J. and Peters, Jan},
	year = {2013},
	month = {09},
	pages = {1238-1274},
	title = {Reinforcement Learning in Robotics: A Survey},
	volume = {32},
	journal = {The International Journal of Robotics Research},
	doi = {10.1177/0278364913495721}
}


@book{SB18,
	Author = {Sutton, R.S. and Barto, A.G.},
	Publisher = {online draft},
	Title = {Reinforcement Learning: An Introduction (second edition)},
	Year = {2018}
}


@book{Sze10,
	Author = {{Sz}epesv\'{a}ri, {\relax Cs}.},
	Booktitle = {Algorithms for Reinforcement Learning},
	Publisher = {Morgan \& Claypool Publishers},
	Series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	Title = {Algorithms for Reinforcement Learning},
	Year = {2010}
}


@ARTICLE{2019arXiv191205830C,
	author = {{Cai}, Qi and {Yang}, Zhuoran and {Jin}, Chi and {Wang}, Zhaoran},
	title = {Provably Efficient Exploration in Policy Optimization},
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	year = 2019,
	month = dec,
	eid = {arXiv:1912.05830},
	pages = {arXiv:1912.05830},
	archivePrefix = {arXiv},
	eprint = {1912.05830},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191205830C},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@inproceedings{2020NO,
	author = {{Neu}, Gergely and {Olkhovskaya}, Julia},
	title = {Efficient and Robust Algorithms for Adversarial Linear Contextual Bandits},
        year = {2020},
        booktitle = {Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)},
        pages = {3049-3068}
}


@inproceedings{2019arXiv190705388J,
	author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and
	Jordan, Michael I.},
	title = {Provably Efficient Reinforcement Learning with Linear Function Approximation},
        year = {2020},
        booktitle = {Proceedings of the 33rd Annual Conference on Learning Theory (COLT 2020)},
	pages = {2137--2143}
}



@inproceedings{yao2012approximate,
	title={Approximate policy iteration with linear action models},
	author={Yao, Hengshuai and Szepesv{\'a}ri, {\text Cs}aba},
	booktitle={Twenty-Sixth AAAI Conference on Artificial Intelligence},
	year={2012}
}

@inproceedings{factlin,
	author = {Yao, Hengshuai and Szepesv\'ari, {\text Cs}aba and Pires, Bernardo and Zhang, Xinhua},
	year = {2014},
	month = {10},
	pages = {},
	title = {Pseudo-{MDPs} and Factored Linear Action Models},
	doi = {10.1109/ADPRL.2014.7010633}
}

@inproceedings{PS16,
  title={Policy error bounds for model-based reinforcement learning with factored linear models},
  author={Pires, Bernardo {\'A}vila and Szepesv{\'a}ri, {\text Cs}aba},
  booktitle={Conference on Learning Theory},
  pages={121--151},
  year={2016}
}

@incollection{NIPS2018_7765,
	title = {Near-Optimal Time and Sample Complexities for Solving Markov Decision Processes with a Generative Model},
	author = {Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin and Ye, Yinyu},
	booktitle = {Advances in Neural Information Processing Systems 31},
	pages = {5186--5196},
	year = {2018}
}

@ARTICLE{2010arXiv1004.2027G,
	author = {{Azar}, Mohammad and {Gomez}, Vicenc and {Kappen}, Hilbert J.},
	title = "{Dynamic Policy Programming}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Machine Learning},
	year = 2010,
	month = apr,
	eid = {arXiv:1004.2027},
	pages = {arXiv:1004.2027},
	archivePrefix = {arXiv},
	eprint = {1004.2027},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2010arXiv1004.2027G},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@inproceedings{azar2012sample,
  title={On the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={1707--1714},
  year={2012}
}


@article{bradtke1996linear,
	title={Linear least-squares algorithms for temporal difference learning},
	author={Bradtke, Steven J and Barto, Andrew G},
	journal={Machine learning},
	volume={22},
	number={1-3},
	pages={33--57},
	year={1996},
	publisher={Springer}
}

@inproceedings{melo2007q,
	title={Q-learning with linear function approximation},
	author={Melo, Francisco S and Ribeiro, M Isabel},
	booktitle={International Conference on Computational Learning Theory},
	pages={308--322},
	year={2007},
	organization={Springer}
}

@book{Puterman,
	author = {Puterman, Martin L.},
	title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
	year = {1994},
	isbn = {0471619779},
	publisher = {John Wiley and Sons, Inc.},
	address = {USA},
	edition = {1st}
}
@inproceedings{NGS13,
	author = {Neu, Gergely and Gy\"{o}rgy, Andr\'as and Szepesv\'ari, {\text Cs}aba and Antos, András},
	year = {2013},
	month = {01},
	pages = {1804-1812},
	title = {Online {M}arkov Decision Processes Under Bandit Feedback},
	volume = {59},
	journal = {Automatic Control, IEEE Transactions on},
	doi = {10.1109/TAC.2013.2292137}
}

@InProceedings{pmlr-v22-neu12,
	title = 	 {The adversarial stochastic shortest path problem with unknown transition probabilities},
	author = 	 {Gergely Neu and Andr\'as Gy\"orgy and {\text Cs}aba Szepesv\'ari},
	booktitle = 	 {Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {805--813},
	year = 	 {2012}
}

@inproceedings{NGS10,
	author = {Neu, Gergely and Gy\"{o}rgy, Andr\'as and Szepesv\'ari, {\text Cs}aba},
	year = {2010},
	pages = {231-243},
	title = {The Online Loop-free Stochastic Shortest-Path Problem},
	booktitle = {Proceedings of the 23rd Annual Conference on Learning Theory (COLT 2010)}
}

@incollection{NIPS20134974,
	title = {Online learning in episodic Markovian decision processes by relative entropy policy search},
	author = {Zimin, Alexander and Neu, Gergely},
	booktitle = {Advances in Neural Information Processing Systems 26},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
	pages = {1583--1591},
	year = {2013},
	publisher = {Curran Associates, Inc.}
}

@inproceedings{DGySz14,
  title={Online learning in {Markov} decision processes with changing cost sequences},
  author={Dick, Travis and Gy\"orgy, Andras and Szepesv\'ari, {\relax{Cs}}aba},
  booktitle={International Conference on Machine Learning},
  pages={512--520},
  year={2014}
}

@inproceedings{ABBLSW19,
  title={{POLITEX}: Regret bounds for policy iteration using expert prediction},
  author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesv\'ari, {\text Cs}aba and Weisz, Gell{\'e}rt},
  booktitle={International Conference on Machine Learning},
  pages={3692--3702},
  year={2019}
}

@article{ALSW19,
  title={Exploration-Enhanced {POLITEX}},
  author={Abbasi-Yadkori, Yasin and Lazic, Nevena and Szepesv\'ari, {\text Cs}aba and Weisz, Gell\'ert},
  journal={arXiv preprint arXiv:1908.10479},
  year={2019}
}



@InProceedings{pmlrv97rosenberg19a,
	title = 	 {Online Convex Optimization in Adversarial {M}arkov Decision Processes},
	author = 	 {Rosenberg, Aviv and Mansour, Yishay},
	booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
	pages = 	 {5478--5486},
	year = 	 {2019}
}



@inproceedings{YW19,
  title={Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound},
  author={Yang, Lin F and Wang, Mengdi},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  year={2020}
}


@article{AuerCFS02,
  author = {Auer, Peter and Cesa-Bianchi, Nicolò and Freund, Yoav and Schapire, Robert E.},
  journal = {SIAM J. Comput.},
  keywords = {dblp},
  number = 1,
  pages = {48-77},
  timestamp = {2018-11-15T15:37:41.000+0100},
  title = {The Nonstochastic Multiarmed Bandit Problem.},
  volume = 32,
  year = 2002
}

@inproceedings{AL99,
  title={Associative Reinforcement Learning using Linear Probabilistic Concepts},
  author={Abe, Naoki and Long, Philip M},
  booktitle={Proceedings of the Sixteenth International Conference on Machine Learning},
  pages={3--11},
  year={1999}
}

@misc{syrgkanis2016efficient,
	title={Efficient Algorithms for Adversarial Contextual Learning},
	author={Vasilis Syrgkanis and Akshay Krishnamurthy and Robert E. Schapire},
	year={2016},
	eprint={1602.02454},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@incollection{TewariM17,
	author = {Tewari, Ambuj and Murphy, Susan A.},
	booktitle = {Mobile Health - Sensors, Analytic Methods, and Applications},
	crossref = {books/sp/RMK2017},
	pages = {495-517},
	title = {From Ads to Interventions: Contextual Bandits in Mobile Health.},
	year = 2017
}



@inproceedings{bartlett09regal,
	Author = {Bartlett, Peter L. and Tewari, Ambuj},
	Pdf = {research/bartlett09regal.pdf},
	Title = {{REGAL}: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating {MDP}s},
	Crossref = {UAI2009}}

@inproceedings{tewari08optimistic,
	Author = {Tewari, Ambuj and Bartlett, Peter L.},
	Pages = {1505--1512},
	Pdf = {research/tewari08optimistic.pdf},
	Title = {Optimistic Linear Programming gives Logarithmic Regret for Irreducible {MDPs}},
	booktitle = {Advances in Neural Information Processing Systems 20}
	}



@inproceedings{auer06logarithmic,
	author = {Auer, Peter and Ortner, Ronald},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
	pages = {49--56},
	publisher = {MIT Press},
	title = {Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning},
	volume = {19},
	year = {2007}
}


@article{BK97,
	Author = {Burnetas, A. N. and Katehakis, M. N.},
	Date-Added = {2008-10-14 22:30:41 +0200},
	Date-Modified = {2009-10-04 18:03:13 -0600},
	Journal = {Mathematics of Operations Research},
	Number = {1},
	Pages = {222--255},
	Title = {Optimal Adaptive Policies for {M}arkov {D}ecision {P}rocesses},
	Volume = {22},
	Year = {1997}}



@book{Cao07,
	Address = {New York},
	Author = {Cao, X. R.},
	Booktitle = {Stochastic Learning and Optimization: A Sensitivity-Based Approach},
	Date-Added = {2009-09-13 22:33:02 -0600},
	Date-Modified = {2009-09-13 22:33:52 -0600},
	Publisher = {Springer},
	Title = {Stochastic Learning and Optimization: A Sensitivity-Based Approach},
	Year = {2007}}

	
	
@article{jaksch10ucrl,
	Year = {2010},
	Address = {Cambridge, MA, USA},
	Author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	Issn = {1532-4435},
	Journal = {Journal of Machine Learning Research},
	Month = {August},
	Pages = {1563--1600},
	Publisher = {MIT Press},
	Title = {Near-optimal Regret Bounds for Reinforcement Learning},
	Volume = {99}
}


@inproceedings{auer1995gambling,
  title={Gambling in a rigged casino: The adversarial multi-armed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Freund, Yoav and Schapire, Robert E},
  booktitle={Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on},
  pages={322--331},
  year={1995},
  organization={IEEE}
}



%@article{auerlinear, author = {Auer, Peter}, title = {Using Confidence Bounds for Exploitation-Exploration Trade-Offs}, year = {2003}, issue_date = {March 2003}, publisher = {JMLR.org}, volume = {3}, number = {null}, issn = {1532-4435}, journal = {J. Mach. Learn. Res.}, month = mar, pages = {397–422}, numpages = {26}, keywords = {online Learning, exploitation-exploration, bandit problem, reinforcement learning, linear value function} }

@InProceedings{Lihong1,
	title = 	 {Contextual Bandits with Linear Payoff Functions},
	author = 	 {Wei Chu and Lihong Li and Lev Reyzin and Robert Schapire},
	booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {208--214},
	year = 	 {2011}
}



@inproceedings{kanade09sleeping,
  author = {Kanade, Varun and McMahan, H. Brendan and Bryan, Brent},
  crossref = {AISTAT2009},
  pages = {272-279},
  title = {Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards.}
}


@inproceedings{McMaBlu04,
	Author = {McMahan, H. Brendan and Blum, Avrim},
	Pages = {109--123},
	Title = {Online Geometric Optimization in the Bandit Setting Against an Adaptive Adversary},
	Crossref = {COLT2004}}

@inproceedings{AweKlein04,
	Author = {Awerbuch, Baruch and Kleinberg, Robert D.},
	Pages = {45--53},
	Title = {Adaptive routing with end-to-end feedback: distributed learning and geometric approaches},
	Crossref = {stoc2004}}

@inproceedings{DHK08,
  title={{The price of bandit information for online optimization}},
  author={Dani, V. and Hayes, T. and Kakade, S.},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  volume={20},
  pages={345--352},
  year={2008},
}


@article{RM51,
  author = {Robbins, H. and Monro, S.},
  journal = {Annals of Mathematical Statistics},
  pages = {400--407},
  title = {A stochastic approximation method},
  volume = 22,
  year = 1951,
}



@inproceedings{BM13,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate {$O(1/n)$}},
  author={Bach, Francis and Moulines, Eric},
  crossref = {NIPS26},
  pages={773--781},
  year={2013}
}


@InProceedings{NR18,
  title = 	 {Iterate Averaging as Regularization for Stochastic Gradient Descent},
  author = 	 {Neu, Gergely and Rosasco, Lorenzo},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {3222--3242},
  year = 	 {2018},
  editor = 	 {Bubeck, S\'ebastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher = 	 {PMLR}
}


@ARTICLE{2012arXiv1202.3079B,
	author = {{Bubeck}, S{\'e}bastien and {Cesa-Bianchi}, Nicol{\`o} and
	{Kakade}, Sham M.},
	title = "{Towards minimax policies for online linear optimization with bandit feedback}",
	journal = {arXiv e-prints},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	year = 2012,
	month = feb,
	eid = {arXiv:1202.3079},
	pages = {arXiv:1202.3079},
	archivePrefix = {arXiv},
	eprint = {1202.3079},
	primaryClass = {cs.LG},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2012arXiv1202.3079B},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@article{GyW96,
  title={On the averaged stochastic approximation for linear regression},
  author={Gy{\"o}rfi, L{\'a}szl{\'o} and Walk, Harro},
  journal={SIAM Journal on Control and Optimization},
  volume={34},
  number={1},
  pages={31--61},
  year={1996},
  publisher={SIAM}
}



@article{audibert13regret,
  author    = {Audibert, J.-Y. and
               Bubeck, S. and
               Lugosi, G.},
  title     = {Regret in Online Combinatorial Optimization},
  journal   = {Mathematics of Operations Research},
  volume    = {39},
  pages     = {31-45},
  year      = {2014}
}


@article{BEL17,
  title={Kernel-based methods for bandit convex optimization},
  author={Bubeck, S{\'e}bastien and Lee, Yin Tat and Eldan, Ronen},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={72--85},
  year={2017}
}


@inproceedings{NV14,
  author = {Gergely Neu and Michal Valko},
  title  = {Online combinatorial optimization with stochastic decision sets and adversarial losses},
  booktitle = {NIPS-27},
  pages  = {2780-2788},
  year   = {2014}
}

@inproceedings{FKL19,
  title={Model selection for contextual bandits},
  author={Foster, Dylan J and Krishnamurthy, Akshay and Luo, Haipeng},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14714--14725},
  year={2019}
}


@article{RT10,
 author = {P. Rusmevichientong and J. Tsitsiklis},
 title = {Linearly Parameterized Bandits},
 journal = {Mathematics of Operations Research},
 volume = {35},
 year = {2010},
 pages = {395--411},
 }

@incollection{NIPS2011_4417,
	title = {Improved Algorithms for Linear Stochastic Bandits},
	author = {Yasin Abbasi-{Y}adkori and P\'{a}l, D\'{a}vid and {\relax Cs}aba Szepesv\'{a}ri},
	booktitle = {Advances in Neural Information Processing Systems 24},
	pages = {2312--2320},
	year = {2011},
}


%@article{NCB, author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Fischer, Paul}, title = {Finite-Time Analysis of the Multiarmed Bandit Problem}, year = {2002}, issue_date = {May 2002}, publisher = {Kluwer Academic Publishers}, address = {USA}, volume = {47}, number = {2–3}, issn = {0885-6125}, url = {https://doi.org/10.1023/A:1013689704352}, doi = {10.1023/A:1013689704352}, journal = {Mach. Learn.}, month = may, pages = {235–256}, numpages = {22}, keywords = {finite horizon regret, bandit problems, adaptive allocation rules} }

%@article{auerucb, author = {Auer, Peter}, title = {Using Confidence Bounds for Exploitation-Exploration Trade-Offs}, year = {2003}, issue_date = {March 2003}, publisher = {JMLR.org}, volume = {3}, number = {null}, issn = {1532-4435}, journal = {J. Mach. Learn. Res.}, month = mar, pages = {397–422}, numpages = {26}, keywords = {reinforcement learning, bandit problem, linear value function, online Learning, exploitation-exploration} }

@inproceedings{Lihong,
	author = {Li, Lihong and Chu, Wei and Langford, John and Wang, Xuanhui},
	title = {Unbiased Offline Evaluation of Contextual-Bandit-Based News Article Recommendation Algorithms},
	year = {2011},
	isbn = {9781450304931},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1935826.1935878},
	doi = {10.1145/1935826.1935878},
	booktitle = {Proceedings of the Fourth ACM International Conference on Web Search and Data Mining},
	pages = {297–306},
	numpages = {10},
	keywords = {offline evaluation, recommendation, contextual bandit, multi-armed bandit, benchmark dataset},
	location = {Hong Kong, China},
	series = {WSDM ’11}
}


@inproceedings{LCLS10,
  title={A contextual-bandit approach to personalized news article recommendation},
  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={661--670},
  year={2010},
  organization={ACM}
}


@inproceedings{filippi10genlin,
  author    = {Sarah Filippi and
               Olivier Capp{\'e} and
               Aur{\'e}lien Garivier and
               {\text{Cs}}aba Szepesv{\'a}ri},
  title     = {Parametric Bandits: The Generalized Linear Case},
  booktitle = {NIPS},
  year      = {2010},
  pages     = {586-594}
}

@article{SKKS09,
  title={Gaussian process optimization in the bandit setting: No regret and experimental design},
  author={Srinivas, Niranjan and Krause, Andreas and Kakade, Sham M and Seeger, Matthias},
  journal={arXiv preprint arXiv:0912.3995},
  year={2009}
}

@inproceedings{CCLVR19,
  title={Gaussian Process Optimization with Adaptive Sketching: Scalable and No Regret},
  author={Calandriello, Daniele and Carratino, Luigi and Lazaric, Alessandro and Valko, Michal and Rosasco, Lorenzo},
  booktitle={Conference on Learning Theory},
  pages={533--557},
  year={2019}
}


@incollection{DLB17,
title = {Unifying {PAC} and Regret: Uniform {PAC} Bounds for Episodic Reinforcement Learning},
author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
booktitle = {Advances in Neural Information Processing Systems 30},
pages = {5713--5723},
year = {2017}
}

@inproceedings{DB15,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2818--2826},
  year={2015}
}

@inproceedings{JAZBJ18,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4863--4873},
  year={2018}
}

@inproceedings{AOM17,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning},
  pages={263--272},
  year={2017}
}

@inproceedings{FPL18,
  title={Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning},
  author={Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Ortner, Ronald},
  booktitle={International Conference on Machine Learning},
  pages={1573--1581},
  year={2018}
}

@inproceedings{QFPL19,
  title={Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs},
  author={Qian, Jian and Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4891--4900},
  year={2019}
}



@Article{Aue02,
  author =	{P. Auer},
  title =	{Using Confidence Bounds for Exploitation-Exploration
		 Trade-offs},
  journal =	{Journal of Machine Learning Research},
  volume =	{3},
  year = 	{2002},
  pages =	{397--422},
}


@inproceedings{LSz17,
  title={The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits},
  author={Lattimore, Tor and Szepesv\'ari, {\relax Cs}aba},
  booktitle={Artificial Intelligence and Statistics},
  pages={728--737},
  year={2017}
}

@article{LSz19book,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, {\relax Cs}aba},
  year = {2019},
  journal={book draft}
}


@book{bubeck12survey,
  author = {Bubeck, S\'ebastien and Cesa-Bianchi, Nicol\`o},
  title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
  publisher = {Now Publishers Inc},
  year = 2012
}

@article{DKWY19,
  title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}

@article{VRD19,
  title={Comments on the {Du-Kakade-Wang-Yang} Lower Bounds},
  author={Van Roy, Benjamin and Dong, Shi},
  journal={arXiv preprint arXiv:1911.07910},
  year={2019}
}



@InProceedings{LSz19, title = {Learning with Good Feature Representations in Bandits and in {RL} with a Generative Model}, author = {Lattimore, Tor and Szepesvari, Csaba and Weisz, Gellert}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {5662--5670}, year = {2020}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/lattimore20a/lattimore20a.pdf}, abstract = {The construction in the recent paper by Du et al. [2019] implies that searching for a near-optimal action in a bandit sometimes requires examining essentially all the actions, even if the learner is given linear features in R^d that approximate the rewards with a small uniform error. We use the Kiefer-Wolfowitz theorem to prove a positive result that by checking only a few actions, a learner can always find an action that is suboptimal with an error of at most O($\epsilon$$\sqrt{}$d) where $\epsilon$ is the approximation error of the features. Thus, features are useful when the approximation error is small relative to the dimensionality of the features. The idea is applied to stochastic bandits and reinforcement learning with a generative model where the learner has access to d-dimensional linear features that approximate the action-value functions for all policies to an accuracy of $\epsilon$. For linear bandits, we prove a bound on the regret of order d$\sqrt{}$(n log(k)) + $\epsilon$n$\sqrt{}$d log(n) with k the number of actions and n the horizon. For RL we show that approximate policy iteration can learn a policy that is optimal up to an additive error of order $\epsilon$$\sqrt{}$d/(1 − $\gamma$)^2 and using about d/($\epsilon$^2(1 − $\gamma$)^4) samples from the generative model. These bounds are independent of the finer details of the features. We also investigate how the structure of the feature set impacts the tradeoff between sample complexity and estimation error.} }

@inproceedings{AG13b,
  title={Thompson sampling for contextual bandits with linear payoffs},
  author={Agrawal, Shipra and Goyal, Navin},
  booktitle={International Conference on Machine Learning},
  pages={127--135},
  year={2013}
}

@article{auer2002finite,
  AUTHOR         = {P. Auer and N. Cesa-Bianchi and P. Fischer},
  TITLE          = {Finite-time Analysis of the Multiarmed Bandit Problem},
  YEAR           = {2002},
  JOURNAL        = {Machine Learning Journal},
  VOLUME         = {47},
  NUMBER         = {2-3},
  PAGES          = {235--256}
}


@inproceedings{NeuBartok13,
  author =       {Neu, Gergely and Bart\'ok, G\'abor},
  title =        {An Efficient Algorithm for Learning with Semi-Bandit Feedback},
  booktitle =    {Proceedings of the 24th International Conference on Algorithmic Learning Theory (ALT 2013)},
  year  = {2013},
  pages =        {234-248}}


@article{NB16,
 author = {Gergely Neu and G\'abor Bart\'ok},
 title  = {Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits},
 journal = {Journal of Machine Learning Research},
 volume = {17},
 issue = {154},
 pages = {1-21},
 year = {2016}
}

@inproceedings{FK18,
  title={Contextual bandits with surrogate losses: Margin bounds and efficient algorithms},
  author={Foster, Dylan J and Krishnamurthy, Akshay},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2621--2632},
  year={2018}
}

@inproceedings{pmlrv48rakhlin16,
  title={{BISTRO}: An Efficient Relaxation-Based Method for Contextual Bandits},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  booktitle={International Conference on Machine Learning},
  pages={1977--1985},
  year={2016}
}

@inproceedings{SKS16,
  title={Efficient algorithms for adversarial contextual learning},
  author={Syrgkanis, Vasilis and Krishnamurthy, Akshay and Schapire, Robert},
  booktitle={International Conference on Machine Learning},
  pages={2159--2168},
  year={2016}
}


@article{LW94,
 author = {Littlestone, N. and Warmuth, M.},
 title = {The weighted majority algorithm},
 journal = {Information and Computation},
 volume = {108},
 year = {1994},
 pages = {212--261},
} 

@inproceedings{BDPSS09,
	Author = {Ben-David, S. and P{\'a}l, D. and Shalev-Shwartz, S.},
	Bibsource = {DBLP, http://dblp.uni-trier.de},
	Crossref = {COLT2009},
	Title = {Agnostic Online Learning}}

@inproceedings{SLKS16,
  title={Improved regret bounds for oracle-based adversarial contextual bandits},
  author={Syrgkanis, Vasilis and Luo, Haipeng and Krishnamurthy, Akshay and Schapire, Robert E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3135--3143},
  year={2016}
}



@inproceedings{KSST08,
  title={Efficient bandit algorithms for online multiclass prediction},
  author={Kakade, Sham M and Shalev-Shwartz, Shai and Tewari, Ambuj},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={440--447},
  year={2008},
  organization={ACM}
}

@inproceedings{BOZ17,
  title={Efficient Online Bandit Multiclass Learning with $\widetilde{O}(\sqrt{T})$ Regret},
  author={Beygelzimer, Alina and Orabona, Francesco and Zhang, Chicheng},
  booktitle={International Conference on Machine Learning},
  pages={488--497},
  year={2017}
}


@incollection{KAD16,
title = {Contextual semibandits via supervised learning oracles},
author = {Krishnamurthy, Akshay and , Alekh and Dud\'ik, Miro},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {2388--2396},
year = {2016},
publisher = {Curran Associates, Inc.}
}

@article{implicit,
	author = {Neu, Gergely},
	year = {2015},
	month = {06},
	pages = {},
	title = {Explore no more: Improved high-probability regret bounds for non-stochastic bandits}
}


@Book{BoLuMa13,
  author = 	 {S. Boucheron and G. Lugosi and P. Massart},
  title = 	 {Concentration inequalities:A Nonasymptotic Theory of Independence},
  publisher = 	 {Oxford University Press},
  year = 	 {2013}
}


@book{CBLu06:book,
	Address = {New York, NY, USA},
	Author = {Cesa-Bianchi, N. and Lugosi, G.},
	Date-Added = {2008-10-14 22:30:41 +0200},
	Date-Modified = {2008-10-14 22:30:43 +0200},
	Publisher = {Cambridge University Press},
	Title = {Prediction, Learning, and Games},
	Year = {2006}}


@inproceedings{Neu15b,
  author = {Neu, Gergely},
  title  = {Explore no more: Improved high-probability regret bounds for non-stochastic bandits},
  pages = {3150--3158},
  crossref = {NIPS28} 
}


@inproceedings{AHKLLS14,
 author = {Alekh Agarwal and Daniel Hsu and Satyen Kale and John Langford and Lihong Li and Robert Schapire},
 title = {Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits},
 pages = {1638–1646},
 crossref = {ICML2014}
}

@inproceedings{KSST08,
  title={Efficient bandit algorithms for online multiclass prediction},
  author={Kakade, Sham M and Shalev-Shwartz, Shai and Tewari, Ambuj},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={440--447},
  year={2008},
  organization={ACM}
}


@inproceedings{BLLRS11,
  author    = {Alina Beygelzimer and John Langford and Lihong Li and Lev Reyzin and Robert E. Schapire},
  title     = {Contextual Bandit Algorithms with Supervised Learning Guarantees},
  crossref  = {AISTAT2011},
  pages     = {19-26},
}



@inproceedings{Neu15, 
 author = {Neu, Gergely},
 crossref = {COLT2015},
 title = {First-order regret bounds for combinatorial semi-bandits},
 pages = {1360-1375},
 year = {2015}
}



@InProceedings{AKLLS17,
  title = 	 {Open Problem: First-Order Regret Bounds for Contextual Bandits},
  author = 	 {Alekh Agarwal and Akshay Krishnamurthy and John Langford and Haipeng Luo and Schapire Robert E.},
  booktitle = 	 {Proceedings of the 30th Conference on Learning Theory},
  pages = 	 {4--7},
  year = 	 {2017}
}

@InProceedings{AZBL18,
  title = 	 {Make the Minority Great Again: First-Order Regret Bound for Contextual Bandits},
  author = 	 {Allen-Zhu, Zeyuan and Bubeck, Sebastien and Li, Yuanzhi},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {186--194},
  year = 	 {2018}
}

@inproceedings{FK18,
  title={Contextual bandits with surrogate losses: Margin bounds and efficient algorithms},
  author={Foster, Dylan J and Krishnamurthy, Akshay},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2621--2632},
  year={2018}
}

@inproceedings{DHKKLRZ11,
 author = {Dud\'ik, Miroslav and Hsu, Daniel and Kale, Satyen and Karampatziakis, Nikos and Langford, John and Reyzin, Lev and Zhang, Tong},
 title = {Efficient Optimal Learning for Contextual Bandits},
 booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
 series = {UAI'11},
 year = {2011},
 pages = {169--178},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States},
} 


@inproceedings{RVC19,
  title={Weighted Linear Bandits for Non-Stationary Environments},
  author={Russac, Yoan and Vernade, Claire and Capp{\'e}, Olivier},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12017--12026},
  year={2019}
}

@inproceedings{CSZ19,
  title={Learning to Optimize under Non-Stationarity},
  author={Cheung, Wang Chi and Simchi-Levi, David and Zhu, Ruihao},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1079--1087},
  year={2019}
}

@misc{kim2019nearoptimal,
	title={Near-optimal Oracle-efficient Algorithms for Stationary and Non-Stationary Stochastic Linear Bandits},
	author={Baekjin Kim and Ambuj Tewari},
	year={2019},
	eprint={1912.05695},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}
