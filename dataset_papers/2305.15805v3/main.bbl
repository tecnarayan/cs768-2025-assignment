\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahl et~al.(1983)Bahl, Jelinek, and Mercer]{bahl1983maximum}
Lalit~R Bahl, Frederick Jelinek, and Robert~L Mercer.
\newblock A maximum likelihood approach to continuous speech recognition.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, \penalty0 (2):\penalty0 179--190, 1983.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Gao, Choi, et~al.]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem[Bolya et~al.(2022)Bolya, Fu, Dai, Zhang, Feichtenhofer, and
  Hoffman]{bolya2022token}
Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph
  Feichtenhofer, and Judy Hoffman.
\newblock Token merging: Your vit but faster.
\newblock \emph{arXiv preprint arXiv:2210.09461}, 2022.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020{\natexlab{a}})Choromanski, Likhosherstov,
  Dohan, Song, Gane, Sarlos, Hawkins, Davis, Belanger, Colwell, and
  Weller]{choromanski2020masked}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy
  Colwell, and Adrian Weller.
\newblock Masked language modeling for proteins via linearly scalable
  long-context transformers, 2020{\natexlab{a}}.

\bibitem[Choromanski et~al.(2020{\natexlab{b}})Choromanski, Likhosherstov,
  Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser,
  et~al.]{choromanski2020rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020{\natexlab{b}}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Dai et~al.(2020)Dai, Lai, Yang, and Le]{dai2020funnel}
Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le.
\newblock Funnel-transformer: Filtering out sequential redundancy for efficient
  language processing.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 4271--4282, 2020.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16344--16359, 2022.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{dettmers2022llm}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Frantar and Alistarh(2023{\natexlab{a}})]{frantar2023massive}
Elias Frantar and Dan Alistarh.
\newblock Massive language models can be accurately pruned in one-shot.
\newblock \emph{arXiv preprint arXiv:2301.00774}, 2023{\natexlab{a}}.

\bibitem[Frantar and Alistarh(2023{\natexlab{b}})]{frantar2023sparsegpt}
Elias Frantar and Dan Alistarh.
\newblock Sparsegpt: Massive language models can be accurately pruned in
  one-shot, 2023{\natexlab{b}}.

\bibitem[Frantar et~al.(2022)Frantar, Ashkboos, Hoefler, and
  Alistarh]{frantar2022gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Frantar et~al.(2023)Frantar, Singh, and Alistarh]{frantar2023optimal}
Elias Frantar, Sidak~Pal Singh, and Dan Alistarh.
\newblock Optimal brain compression: A framework for accurate post-training
  quantization and pruning, 2023.

\bibitem[Hao et~al.(2021)Hao, Dong, Wei, and Xu]{hao2021self}
Yaru Hao, Li~Dong, Furu Wei, and Ke~Xu.
\newblock Self-attention attribution: Interpreting information interactions
  inside transformer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 12963--12971, 2021.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{Hassibi1993OptimalBS}
Babak Hassibi, David~G. Stork, and Gregory~J. Wolff.
\newblock Optimal brain surgeon and general network pruning.
\newblock \emph{IEEE International Conference on Neural Networks}, pages
  293--299 vol.1, 1993.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Ivanov et~al.(2021)Ivanov, Dryden, Ben-Nun, Li, and
  Hoefler]{ivanov2021data}
Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler.
\newblock Data movement is all you need: A case study on optimizing
  transformers.
\newblock \emph{Proceedings of Machine Learning and Systems}, 3:\penalty0
  711--732, 2021.

\bibitem[Jaegle et~al.(2021)Jaegle, Gimeno, Brock, Zisserman, Vinyals, and
  Carreira]{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and
  Joao Carreira.
\newblock Perceiver: General perception with iterative attention, 2021.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem[Kim et~al.(2022)Kim, Shen, Thorsley, Gholami, Kwon, Hassoun, and
  Keutzer]{kim2022learned}
Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph
  Hassoun, and Kurt Keutzer.
\newblock Learned token pruning for transformers.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 784--794, 2022.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[K{\"o}pf et~al.(2023)K{\"o}pf, Kilcher, von R{\"u}tte, Anagnostidis,
  Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi, et~al.]{kopf2023openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis,
  Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver
  Stanley, Rich{\'a}rd Nagyfi, et~al.
\newblock Openassistant conversations--democratizing large language model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.07327}, 2023.

\bibitem[Kwon et~al.(2022)Kwon, Kim, Mahoney, Hassoun, Keutzer, and
  Gholami]{kwon2022fast}
Woosuk Kwon, Sehoon Kim, Michael~W. Mahoney, Joseph Hassoun, Kurt Keutzer, and
  Amir Gholami.
\newblock A fast post-training pruning framework for transformers, 2022.

\bibitem[Lee et~al.(2023)Lee, Kang, Lee, and Hwang]{leesparse}
Heejun Lee, Minki Kang, Youngwan Lee, and Sung~Ju Hwang.
\newblock Sparse token transformer with attention back tracking.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Lee et~al.(2019)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2019set}
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam~R. Kosiorek, Seungjin Choi, and
  Yee~Whye Teh.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks, 2019.

\bibitem[Lin et~al.(2022)Lin, Wang, Liu, and Qiu]{lin2022survey}
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.
\newblock A survey of transformers.
\newblock \emph{AI Open}, 2022.

\bibitem[Martins et~al.(2020)Martins, Farinhas, Treviso, Niculae, Aguiar, and
  Figueiredo]{martins2020sparse}
Andr{\'e} Martins, Ant{\'o}nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro
  Aguiar, and Mario Figueiredo.
\newblock Sparse and continuous attention mechanisms.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20989--21001, 2020.

\bibitem[Noci et~al.(2022)Noci, Anagnostidis, Biggio, Orvieto, Singh, and
  Lucchi]{noci2022signal}
Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak~Pal
  Singh, and Aurelien Lucchi.
\newblock Signal propagation in transformers: Theoretical perspectives and the
  role of rank collapse.
\newblock \emph{arXiv preprint arXiv:2206.03126}, 2022.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1904.01038}, 2019.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{peng2021random}
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah~A. Smith, and
  Lingpeng Kong.
\newblock Random feature attention, 2021.

\bibitem[Peters et~al.(2019)Peters, Niculae, and Martins]{peters2019sparse}
Ben Peters, Vlad Niculae, and Andr{\'e}~FT Martins.
\newblock Sparse sequence-to-sequence models.
\newblock \emph{arXiv preprint arXiv:1905.05702}, 2019.

\bibitem[Pope et~al.(2022)Pope, Douglas, Chowdhery, Devlin, Bradbury, Levskaya,
  Heek, Xiao, Agrawal, and Dean]{pope2022efficiently}
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
  Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean.
\newblock Efficiently scaling transformer inference.
\newblock \emph{arXiv preprint arXiv:2211.05102}, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Ramsauer et~al.(2020)Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich,
  Adler, Gruber, Holzleitner, Pavlovi{\'c}, Sandve,
  et~al.]{ramsauer2020hopfield}
Hubert Ramsauer, Bernhard Sch{\"a}fl, Johannes Lehner, Philipp Seidl, Michael
  Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovi{\'c},
  Geir~Kjetil Sandve, et~al.
\newblock Hopfield networks is all you need.
\newblock \emph{arXiv preprint arXiv:2008.02217}, 2020.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64\penalty0 (9):\penalty0 99--106,
  2021.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers, 2021.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem[Shi et~al.(2021)Shi, Gao, Ren, Xu, Liang, Li, and
  Kwok]{shi2021sparsebert}
Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James
  Tin-Yau Kwok.
\newblock Sparsebert: Rethinking the importance analysis in self-attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  9547--9557. PMLR, 2021.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock \emph{arXiv preprint arXiv:1906.02243}, 2019.

\bibitem[Sun et~al.(2021)Sun, Krishna, Mattarella-Micke, and
  Iyyer]{sun2021long}
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer.
\newblock Do long-range language models actually use long-range context?
\newblock \emph{arXiv preprint arXiv:2109.09115}, 2021.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler]{tay2020efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.(2020).
\newblock \emph{arXiv preprint cs.LG/2009.06732}, 2020.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Tsallis(1988)]{tsallis1988possible}
Constantino Tsallis.
\newblock Possible generalization of boltzmann-gibbs statistics.
\newblock \emph{Journal of statistical physics}, 52:\penalty0 479--487, 1988.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vaswani et~al.(2018)Vaswani, Bengio, Brevdo, Chollet, Gomez, Gouws,
  Jones, Kaiser, Kalchbrenner, Parmar, et~al.]{vaswani2018tensor2tensor}
Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan~N Gomez,
  Stephan Gouws, Llion Jones, {\L}ukasz Kaiser, Nal Kalchbrenner, Niki Parmar,
  et~al.
\newblock Tensor2tensor for neural machine translation.
\newblock \emph{arXiv preprint arXiv:1803.07416}, 2018.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity, 2020.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 conference on empirical methods in
  natural language processing: system demonstrations}, pages 38--45, 2020.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and
  Han]{xiao2022smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for
  large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2022.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages
  10524--10533. PMLR, 2020.

\bibitem[Yao et~al.(2022)Yao, Yazdani~Aminabadi, Zhang, Wu, Li, and
  He]{yao2022zeroquant}
Zhewei Yao, Reza Yazdani~Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
  Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for
  large-scale transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27168--27183, 2022.

\bibitem[Yun et~al.(2020)Yun, Chang, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2020n}
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank
  Reddi, and Sanjiv Kumar.
\newblock O (n) connections are expressive enough: Universal approximability of
  sparse transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 13783--13794, 2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris
  Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang,
  et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17283--17297, 2020.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[Zhu and Soricut(2021)]{zhu2021h}
Zhenhai Zhu and Radu Soricut.
\newblock H-transformer-1d: Fast one-dimensional hierarchical attention for
  sequences.
\newblock \emph{arXiv preprint arXiv:2107.11906}, 2021.

\end{thebibliography}
