\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Wang et~al.(2023)Wang, Zhang, Xie, Yao, Tian, Wang, Xi, Cheng, Liu, Zheng, et~al.]{wang2023easyedit}
Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, et~al.
\newblock Easyedit: An easy-to-use knowledge editing framework for large language models.
\newblock \emph{arXiv preprint arXiv:2308.07269}, 2023.

\bibitem[Yao et~al.(2023)Yao, Wang, Tian, Cheng, Li, Deng, Chen, and Zhang]{yao2023editing}
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.
\newblock Editing large language models: Problems, methods, and opportunities.
\newblock \emph{arXiv preprint arXiv:2305.13172}, 2023.

\bibitem[Wang et~al.(2024)Wang, Zhu, Liu, Zheng, Chen, and Li]{10.1145/3698590}
Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li.
\newblock Knowledge editing for large language models: A survey.
\newblock \emph{ACM Comput. Surv.}, October 2024.
\newblock ISSN 0360-0300.
\newblock \doi{10.1145/3698590}.
\newblock URL \url{https://doi.org/10.1145/3698590}.

\bibitem[Zhang et~al.(2024)Zhang, Yao, Tian, Wang, Deng, Wang, Xi, Mao, Zhang, Ni, et~al.]{zhang2024comprehensive}
Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et~al.
\newblock A comprehensive study of knowledge editing for large language models.
\newblock \emph{arXiv preprint arXiv:2401.01286}, 2024.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2022.

\bibitem[Mitchell et~al.(2022{\natexlab{a}})Mitchell, Lin, Bosselut, Finn, and Manning]{mitchell2022fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D Manning.
\newblock Fast model editing at scale.
\newblock In \emph{International Conference on Learning Representations}, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/pdf?id=0DcZxeWfOPt}.

\bibitem[Mitchell et~al.(2022{\natexlab{b}})Mitchell, Lin, Bosselut, Finn, and Manning]{mitchell2022memory}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D. Manning.
\newblock Memory-based model editing at scale.
\newblock In \emph{International Conference on Machine Learning}, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/pdf/2206.06520.pdf}.

\bibitem[Zheng et~al.(2023)Zheng, Li, Dong, Fan, Wu, Xu, and Chang]{zheng-etal-2023-edit}
Ce~Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang.
\newblock Can we edit factual knowledge by in-context learning?
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4862--4876, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.296}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.296}.

\bibitem[De~Cao et~al.(2021)De~Cao, Aziz, and Titov]{de-cao-etal-2021-editing}
Nicola De~Cao, Wilker Aziz, and Ivan Titov.
\newblock Editing factual knowledge in language models.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 6491--6506, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.522}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.522}.

\bibitem[Tan et~al.(2023)Tan, Zhang, and Fu]{tan2023massive}
Chenmien Tan, Ge~Zhang, and Jie Fu.
\newblock Massive editing for large language models via meta learning.
\newblock \emph{arXiv preprint arXiv:2311.04661}, 2023.

\bibitem[Hartvigsen et~al.(2023)Hartvigsen, Sankaranarayanan, Palangi, Kim, and Ghassemi]{hartvigsen2023aging}
Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi.
\newblock Aging with grace: Lifelong model editing with discrete key-value adaptors.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Huang et~al.(2023{\natexlab{a}})Huang, Shen, Zhang, Zhou, Rong, and Xiong]{huang2023transformer}
Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.
\newblock Transformer-patcher: One mistake worth one neuron.
\newblock \emph{arXiv preprint arXiv:2301.09785}, 2023{\natexlab{a}}.

\bibitem[Dong et~al.(2022)Dong, Dai, Song, Xu, Sui, and Li]{dong2022calibrating}
Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li.
\newblock Calibrating factual knowledge in pretrained language models.
\newblock \emph{arXiv preprint arXiv:2210.03329}, 2022.

\bibitem[Dai et~al.(2021)Dai, Dong, Hao, Sui, Chang, and Wei]{dai2021knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock \emph{arXiv preprint arXiv:2104.08696}, 2021.

\bibitem[Cheng et~al.(2023)Cheng, Tian, Liu, Chen, Wang, Chen, and Zhang]{cheng-etal-2023-edit}
Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi~Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang.
\newblock Can we edit multimodal large language models?
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 13877--13888, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.856}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.856}.

\bibitem[Rombach et~al.(2021)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2021highresolution}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models, 2021.

\bibitem[Liu et~al.(2019)Liu, Li, Garcia-Duran, Niepert, Onoro-Rubio, and Rosenblum]{liu2019mmkg}
Ye~Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David~S Rosenblum.
\newblock Mmkg: multi-modal knowledge graphs.
\newblock In \emph{European Semantic Web Conference}, pages 459--474. Springer, 2019.

\bibitem[Levy et~al.(2017)Levy, Seo, Choi, and Zettlemoyer]{levy-etal-2017-zero}
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.
\newblock Zero-shot relation extraction via reading comprehension.
\newblock In Roger Levy and Lucia Specia, editors, \emph{Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)}, pages 333--342, Vancouver, Canada, August 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/K17-1034}.
\newblock URL \url{https://aclanthology.org/K17-1034}.

\bibitem[Hewlett et~al.(2016)Hewlett, Lacoste, Jones, Polosukhin, Fandrianto, Han, Kelcey, and Berthelot]{hewlett-etal-2016-wikireading}
Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, and David Berthelot.
\newblock {W}iki{R}eading: A novel large-scale language understanding task over {W}ikipedia.
\newblock In Katrin Erk and Noah~A. Smith, editors, \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1535--1545, Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1145}.
\newblock URL \url{https://aclanthology.org/P16-1145}.

\bibitem[Zhong et~al.(2023)Zhong, Wu, Manning, Potts, and Chen]{zhong-etal-2023-mquake}
Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen.
\newblock {MQ}u{AKE}: Assessing knowledge editing in language models via multi-hop questions.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 15686--15702, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.971}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.971}.

\bibitem[Cohen et~al.(2023)Cohen, Biran, Yoran, Globerson, and Geva]{cohen2023evaluating}
Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva.
\newblock Evaluating the ripple effects of knowledge editing in language models.
\newblock \emph{arXiv preprint arXiv:2307.12976}, 2023.

\bibitem[Ye et~al.(2023{\natexlab{a}})Ye, Xu, Xu, Ye, Yan, Zhou, Wang, Hu, Shi, Shi, Jiang, Li, Xu, Chen, Tian, Qian, Zhang, and Huang]{ye2023mplugowl}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi~Qian, Ji~Zhang, and Fei Huang.
\newblock mplug-owl: Modularization empowers large language models with multimodality, 2023{\natexlab{a}}.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Liu et~al.(2023)Liu, Li, Wu, and Lee]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2304.08485}, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.08485}.

\bibitem[Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{Qwen-VL}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.
\newblock \emph{arXiv preprint arXiv:2308.12966}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey, Chang, Dai, Uszkoreit, Le, and Petrov]{kwiatkowski-etal-2019-natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: A benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 452--466, 2019.
\newblock \doi{10.1162/tacl_a_00276}.
\newblock URL \url{https://aclanthology.org/Q19-1026}.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {BLIP-2:} bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{ICML}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}, 2023.

\bibitem[Ye et~al.(2023{\natexlab{b}})Ye, Xu, Ye, Yan, Hu, Liu, Qian, Zhang, Huang, and Zhou]{ye2023mplugowl2}
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi~Qian, Ji~Zhang, Fei Huang, and Jingren Zhou.
\newblock mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration, 2023{\natexlab{b}}.

\bibitem[Huang et~al.(2023{\natexlab{b}})Huang, Shen, Zhang, Zhou, Rong, and Xiong]{huang2023transformerpatcher}
Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.
\newblock Transformer-patcher: One mistake worth one neuron, 2023{\natexlab{b}}.

\bibitem[Han et~al.(2023)Han, Li, Tan, Yuanlong, Chai, and Pan]{han-etal-2023-improving}
Xiaoqi Han, Ru~Li, Hongye Tan, Wang Yuanlong, Qinghua Chai, and Jeff Pan.
\newblock Improving sequential model editing with fact retrieval.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 11209--11224, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-emnlp.749}.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.749}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\end{thebibliography}
