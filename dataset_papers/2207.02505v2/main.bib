@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICML  =	{ICML})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@inproceedings{vaswani2017attention,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention is All you Need},
  booktitle = NIPS,
  year      = {2017},
}

@inproceedings{dosovitskiy2021animage,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  booktitle = ICLR,
  year      = {2021},
}

@inproceedings{kipf2017semi,
  author    = {Thomas N. Kipf and
               Max Welling},
  title     = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = ICLR,
  year      = {2017},
}

@inproceedings{gilmer2017neural,
  author    = {Justin Gilmer and
               Samuel S. Schoenholz and
               Patrick F. Riley and
               Oriol Vinyals and
               George E. Dahl},
  title     = {Neural Message Passing for Quantum Chemistry},
  year      = {2017},
  booktitle = ICML,
}


@article{velikovic2022message,
  author    = {Petar Velickovic},
  title     = {Message passing all the way up},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{velikovic2018graph,
  author    = {Petar Velickovic and
               Guillem Cucurull and
               Arantxa Casanova and
               Adriana Romero and
               Pietro Li{\`{o}} and
               Yoshua Bengio},
  title     = {Graph Attention Networks},
  booktitle = ICLR,
  year      = {2018},
}

@inproceedings{hamilton2017inductive,
  author    = {William L. Hamilton and
               Zhitao Ying and
               Jure Leskovec},
  title     = {Inductive Representation Learning on Large Graphs},
  booktitle = NIPS,
  year      = {2017},
}

@article{li2020deepergcn,
  author    = {Guohao Li and
               Chenxin Xiong and
               Ali K. Thabet and
               Bernard Ghanem},
  title     = {DeeperGCN: All You Need to Train Deeper GCNs},
  journal   = {arXiv},
  year      = {2020},
}

@inproceedings{li2021training,
  author    = {Guohao Li and
               Matthias M{\"{u}}ller and
               Bernard Ghanem and
               Vladlen Koltun},
  title     = {Training Graph Neural Networks with 1000 Layers},
  booktitle = ICML,
  year      = {2021},
}

@inproceedings{xu2019how,
  author    = {Keyulu Xu and
               Weihua Hu and
               Jure Leskovec and
               Stefanie Jegelka},
  title     = {How Powerful are Graph Neural Networks?},
  booktitle = ICLR,
  year      = {2019},
}

@article{min2022transformer,
  author    = {Erxue Min and
               Runfa Chen and
               Yatao Bian and
               Tingyang Xu and
               Kangfei Zhao and
               Wenbing Huang and
               Peilin Zhao and
               Junzhou Huang and
               Sophia Ananiadou and
               Yu Rong},
  title     = {Transformer for Graphs: An Overview from Architecture Perspective},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{shiv2019novel,
  author    = {Vighnesh Leonardo Shiv and
               Chris Quirk},
  title     = {Novel positional encodings to enable tree-based transformers},
  booktitle = NIPS,
  year      = {2019},
}

@inproceedings{wang2019self,
  author    = {Xing Wang and
               Zhaopeng Tu and
               Longyue Wang and
               Shuming Shi},
  title     = {Self-Attention with Structural Position Representations},
  booktitle = {EMNLP-IJCNLP},
  year      = {2019},
}

@inproceedings{nguyen2022universal,
  author    = {Dai Quoc Nguyen and
               Tu Dinh Nguyen and
               Dinh Phung},
  title     = {Universal Graph Transformer Self-Attention Networks},
  booktitle = {WWW},
  year      = {2022},
}

@article{dwivedi2020a,
  author    = {Vijay Prakash Dwivedi and
               Xavier Bresson},
  title     = {A Generalization of Transformer Networks to Graphs},
  journal   = {arXiv},
  year      = {2020},
}

@inproceedings{rong2020self,
  author    = {Yu Rong and
               Yatao Bian and
               Tingyang Xu and
               Weiyang Xie and
               Ying Wei and
               Wenbing Huang and
               Junzhou Huang},
  title     = {Self-Supervised Graph Transformer on Large-Scale Molecular Data},
  booktitle = NIPS,
  year      = {2020},
}

@inproceedings{lin2021mesh,
  author    = {Kevin Lin and
               Lijuan Wang and
               Zicheng Liu},
  title     = {Mesh Graphormer},
  booktitle = ICCV,
  year      = {2021},
}

@inproceedings{ying2021do,
  author    = {Chengxuan Ying and
               Tianle Cai and
               Shengjie Luo and
               Shuxin Zheng and
               Guolin Ke and
               Di He and
               Yanming Shen and
               Tie{-}Yan Liu},
  title     = {Do Transformers Really Perform Bad for Graph Representation?},
  booktitle   = NIPS,
  year      = {2021},
}

@article{hussain2021edge,
  author    = {Md. Shamim Hussain and
               Mohammed J. Zaki and
               Dharmashankar Subramanian},
  title     = {Edge-augmented Graph Transformers: Global Self-attention is Enough
               for Graphs},
  journal   = {arXiv},
  year      = {2021},
}

@article{kreuzer2021rethinking,
  author    = {Devin Kreuzer and
               Dominique Beaini and
               William L. Hamilton and
               Vincent L{\'{e}}tourneau and
               Prudencio Tossou},
  title     = {Rethinking Graph Transformers with Spectral Attention},
  journal   = NIPS,
  year      = {2021},
}

@article{lim2022sign,
  author    = {Derek Lim and
               Joshua Robinson and
               Lingxiao Zhao and
               Tess E. Smidt and
               Suvrit Sra and
               Haggai Maron and
               Stefanie Jegelka},
  title     = {Sign and Basis Invariant Networks for Spectral Graph Representation
               Learning},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{li2018deeper,
  author    = {Qimai Li and
               Zhichao Han and
               Xiao{-}Ming Wu},
  title     = {Deeper Insights Into Graph Convolutional Networks for Semi-Supervised
               Learning},
  booktitle = AAAI,
  year      = {2018},
}

@article{cai2020a,
  author    = {Chen Cai and
               Yusu Wang},
  title     = {A Note on Over-Smoothing for Graph Neural Networks},
  journal   = {arXiv},
  year      = {2020},
}

@inproceedings{oono2020graph,
  author    = {Kenta Oono and
               Taiji Suzuki},
  title     = {Graph Neural Networks Exponentially Lose Expressive Power for Node
               Classification},
  booktitle = ICLR,
  year      = {2020},
}

@article{park2022grpe,
  author    = {Wonpyo Park and
               Woonggi Chang and
               Donggeon Lee and
               Juntae Kim and
               Seung-won Hwang},
  title     = {GRPE: Relative Positional Encoding for Graph Transformer},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{kitaev2020reformer,
  author    = {Nikita Kitaev and
               Lukasz Kaiser and
               Anselm Levskaya},
  title     = {Reformer: The Efficient Transformer},
  booktitle = ICLR,
  year      = {2020},
}

@inproceedings{choromanski2021rethinking,
  author    = {Krzysztof Marcin Choromanski and
               Valerii Likhosherstov and
               David Dohan and
               Xingyou Song and
               Andreea Gane and
               Tam{\'{a}}s Sarl{\'{o}}s and
               Peter Hawkins and
               Jared Quincy Davis and
               Afroz Mohiuddin and
               Lukasz Kaiser and
               David Benjamin Belanger and
               Lucy J. Colwell and
               Adrian Weller},
  title     = {Rethinking Attention with Performers},
  booktitle = ICLR,
  year      = {2021},
}

@article{wang2020linformer,
  author    = {Sinong Wang and
               Belinda Z. Li and
               Madian Khabsa and
               Han Fang and
               Hao Ma},
  title     = {Linformer: Self-Attention with Linear Complexity},
  journal   = {arXiv},
  year      = {2020},
}

@inproceedings{katharopoulos2020transformers,
  author    = {Angelos Katharopoulos and
               Apoorv Vyas and
               Nikolaos Pappas and
               Fran{\c{c}}ois Fleuret},
  title     = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
               Attention},
  booktitle = ICML,
  year      = {2020},
}

@inproceedings{peng2021random,
  author    = {Hao Peng and
               Nikolaos Pappas and
               Dani Yogatama and
               Roy Schwartz and
               Noah A. Smith and
               Lingpeng Kong},
  title     = {Random Feature Attention},
  booktitle = ICLR,
  year      = {2021},
}

@inproceedings{xiong2021nystromformer,
  author    = {Yunyang Xiong and
               Zhanpeng Zeng and
               Rudrasis Chakraborty and
               Mingxing Tan and
               Glenn Fung and
               Yin Li and
               Vikas Singh},
  title     = {Nystr{\"{o}}mformer: {A} Nystr{\"{o}}m-based Algorithm for
               Approximating Self-Attention},
  booktitle = AAAI,
  year      = {2021},
}

@article{ma2021luna,
  author    = {Xuezhe Ma and
               Xiang Kong and
               Sinong Wang and
               Chunting Zhou and
               Jonathan May and
               Hao Ma and
               Luke Zettlemoyer},
  title     = {Luna: Linear Unified Nested Attention},
  journal   = NIPS,
  year      = {2021},
}

@inproceedings{maron2019invariant,
  author    = {Haggai Maron and
               Heli Ben{-}Hamu and
               Nadav Shamir and
               Yaron Lipman},
  title     = {Invariant and Equivariant Graph Networks},
  booktitle = ICLR,
  year      = {2019},
}

@inproceedings{maron2019provably,
  author    = {Haggai Maron and
               Heli Ben{-}Hamu and
               Hadar Serviansky and
               Yaron Lipman},
  title     = {Provably Powerful Graph Networks},
  booktitle = NIPS,
  year      = {2019},
}

@article{dwivedi2020benchmarking,
  author    = {Vijay Prakash Dwivedi and
               Chaitanya K. Joshi and
               Thomas Laurent and
               Yoshua Bengio and
               Xavier Bresson},
  title     = {Benchmarking Graph Neural Networks},
  journal   = {arXiv},
  year      = {2020},
}

@article{hu2021ogb,
  author    = {Weihua Hu and
               Matthias Fey and
               Hongyu Ren and
               Maho Nakata and
               Yuxiao Dong and
               Jure Leskovec},
  title     = {{OGB-LSC:} {A} Large-Scale Challenge for Machine Learning on Graphs},
  journal   = {arXiv},
  year      = {2021},
}

@inproceedings{ravanbakhsh2017equivariance,
  author    = {Siamak Ravanbakhsh and
               Jeff G. Schneider and
               Barnab{\'{a}}s P{\'{o}}czos},
  title     = {Equivariance Through Parameter-Sharing},
  booktitle = ICML,
  year      = {2017},
}

@inproceedings{yeh2022equivariance,
  author    = {Raymond A. Yeh and
               Yuan{-}Ting Hu and
               Mark Hasegawa{-}Johnson and
               Alexander G. Schwing},
  title     = {Equivariance Discovery by Learned Parameter-Sharing},
  booktitle = {AISTATS},
  year      = {2022},
}

@article{likhosherstov2021on,
  author    = {Valerii Likhosherstov and
               Krzysztof Choromanski and
               Adrian Weller},
  title     = {On the Expressive Power of Self-Attention Matrices},
  journal   = {arXiv},
  year      = {2021},
}

@inproceedings{kim2021transformers,
  author    = {Jinwoo Kim and
               Saeyoon Oh and
               Seunghoon Hong},
  title     = {Transformers Generalize DeepSets and Can be Extended to Graphs and Hypergraphs},
  booktitle = NIPS,
  year      = {2021},
}

@article{steiner2021how,
  author    = {Andreas Steiner and
               Alexander Kolesnikov and
               Xiaohua Zhai and
               Ross Wightman and
               Jakob Uszkoreit and
               Lucas Beyer},
  title     = {How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  journal   = {arXiv},
  year      = {2021},
}

@inproceedings{tompson2015efficient,
  author    = {Jonathan Tompson and
               Ross Goroshin and
               Arjun Jain and
               Yann LeCun and
               Christoph Bregler},
  title     = {Efficient object localization using Convolutional Networks},
  booktitle = CVPR,
  year      = {2015},
}

@inproceedings{huang2016deep,
  author    = {Gao Huang and
               Yu Sun and
               Zhuang Liu and
               Daniel Sedra and
               Kilian Q. Weinberger},
  title     = {Deep Networks with Stochastic Depth},
  booktitle = ECCV,
  year      = {2016},
}

@article{zhao2022graph,
  author    = {Tong Zhao and
               Gang Liu and
               Stephan G{\"{u}}nnemann and
               Meng Jiang},
  title     = {Graph Data Augmentation for Graph Machine Learning: {A} Survey},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{kong2022robust,
  author    = {Kezhi Kong and
               Guohao Li and
               Mucong Ding and
               Zuxuan Wu and
               Chen Zhu and
               Bernard Ghanem and
               Gavin Taylor and
               Tom Goldstein},
  title     = {Robust Optimization as Data Augmentation for Large-scale Graphs},
  booktitle = CVPR,
  year      = {2022},
}

@article{zhu2021an,
  author    = {Yanqiao Zhu and
               Yichen Xu and
               Qiang Liu and
               Shu Wu},
  title     = {An Empirical Study of Graph Contrastive Learning},
  journal   = {arXiv},
  year      = {2021},
}

@inproceedings{sun2021mocl,
  author    = {Mengying Sun and
               Jing Xing and
               Huijun Wang and
               Bin Chen and
               Jiayu Zhou},
  title     = {MoCL: Contrastive Learning on Molecular Graphs with Multi-level Domain
               Knowledge},
  booktitle = {KDD},
  year      = {2021},
}

@inproceedings{hutchinson2021lietransformer,
  author    = {Michael Hutchinson and
               Charline Le Lan and
               Sheheryar Zaidi and
               Emilien Dupont and
               Yee Whye Teh and
               Hyunjik Kim},
  title     = {LieTransformer: Equivariant Self-Attention for Lie Groups},
  booktitle = ICML,
  year      = {2021},
}

@inproceedings{raghu2021do,
  author    = {Maithra Raghu and
               Thomas Unterthiner and
               Simon Kornblith and
               Chiyuan Zhang and
               Alexey Dosovitskiy},
  title     = {Do Vision Transformers See Like Convolutional Neural Networks?},
  booktitle = NIPS,
  year      = {2021},
}

@inproceedings{zaheer2017deep,
  author    = {Manzil Zaheer and
               Satwik Kottur and
               Siamak Ravanbakhsh and
               Barnab{\'{a}}s P{\'{o}}czos and
               Ruslan Salakhutdinov and
               Alexander J. Smola},
  title     = {Deep Sets},
  booktitle = NIPS,
  year      = {2017},
}

@inproceedings{devlin2019bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {NAACL-HLT},
  year      = {2019},
}

@inproceedings{brown2020language,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  booktitle = NIPS,
  year      = {2020},
}

@article{zopf20221_wl,
  author    = {Markus Zopf},
  title     = {1-WL Expressiveness Is (Almost) All You Need},
  journal   = {arXiv},
  year      = {2022},
}

@article{tay2020efficient,
  author    = {Yi Tay and
               Mostafa Dehghani and
               Dara Bahri and
               Donald Metzler},
  title     = {Efficient Transformers: {A} Survey},
  journal   = {arXiv},
  year      = {2020},
}

@article{nakata2020pubchemqc,
  author    = {Nakata, Maho and
               Shimazaki, Tomomi and
               Hashimoto, Masatomo and
               Maeda, Toshiyuki},
  title     = {PubChemQC PM6: Data Sets of 221 Million Molecules with Optimized Molecular Geometries and Electronic Properties},
  journal   = {Journal of Chemical Information and Modeling},
  year      = {2020},
}

@inproceedings{xiong2020on,
  author    = {Ruibin Xiong and
               Yunchang Yang and
               Di He and
               Kai Zheng and
               Shuxin Zheng and
               Chen Xing and
               Huishuai Zhang and
               Yanyan Lan and
               Liwei Wang and
               Tie{-}Yan Liu},
  title     = {On Layer Normalization in the Transformer Architecture},
  booktitle = ICML,
  year      = {2020},
}

@article{hendrycks2018gaussian,
  author    = {Dan Hendrycks and
               Kevin Gimpel},
  title     = {Gaussian Error Linear Units (GELUs)},
  journal   = {arXiv},
  year      = {2018},
}

@inproceedings{lee2019set,
  author    = {Juho Lee and
               Yoonho Lee and
               Jungtaek Kim and
               Adam R. Kosiorek and
               Seungjin Choi and
               Yee Whye Teh},
  title     = {Set Transformer: {A} Framework for Attention-based Permutation-Invariant Neural Networks},
  booktitle = ICML,
  year      = {2019},
}

@inproceedings{yun2020are,
  author    = {Chulhee Yun and
               Srinadh Bhojanapalli and
               Ankit Singh Rawat and
               Sashank J. Reddi and
               Sanjiv Kumar},
  title     = {Are Transformers universal approximators of sequence-to-sequence functions?},
  booktitle = ICLR,
  year      = {2020},
}

@article{ba2016layer,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer Normalization},
  journal   = {arXiv},
  year      = {2016},
}

@inproceedings{chen2020can,
  author    = {Zhengdao Chen and
               Lei Chen and
               Soledad Villar and
               Joan Bruna},
  title     = {Can Graph Neural Networks Count Substructures?},
  booktitle = NIPS,
  year      = {2020},
}

@inproceedings{maron2019on,
  author    = {Haggai Maron and
               Ethan Fetaya and
               Nimrod Segol and
               Yaron Lipman},
  title     = {On the Universality of Invariant Networks},
  booktitle = ICML,
  year      = {2019},
}

@inproceedings{chen2021decision,
  author    = {Lili Chen and
               Kevin Lu and
               Aravind Rajeswaran and
               Kimin Lee and
               Aditya Grover and
               Michael Laskin and
               Pieter Abbeel and
               Aravind Srinivas and
               Igor Mordatch},
  title     = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
  booktitle = NIPS,
  year      = {2021},
}

@inproceedings{andrew2021perceiver,
  author    = {Andrew Jaegle and
               Felix Gimeno and
               Andy Brock and
               Oriol Vinyals and
               Andrew Zisserman and
               Jo{\~{a}}o Carreira},
  title     = {Perceiver: General Perception with Iterative Attention},
  booktitle = ICML,
  year      = {2021},
}

@article{andrew2021perceiverio,
  author    = {Andrew Jaegle and
               Sebastian Borgeaud and
               Jean{-}Baptiste Alayrac and
               Carl Doersch and
               Catalin Ionescu and
               David Ding and
               Skanda Koppula and
               Daniel Zoran and
               Andrew Brock and
               Evan Shelhamer and
               Olivier J. H{\'{e}}naff and
               Matthew M. Botvinick and
               Andrew Zisserman and
               Oriol Vinyals and
               Jo{\~{a}}o Carreira},
  title     = {Perceiver {IO:} {A} General Architecture for Structured Inputs {\&}
               Outputs},
  journal   = {arXiv},
  year      = {2021},
}

@article{reed2022ageneralist,
  author    = {Scott Reed and
               Konrad {\.{Z}}o{\/{l}}na and
               Emilio Parisotto and
               Sergio G{\'{„Öê}}mez Colmenarejo and
               Alexander Novikov and
               Gabriel Barth-Maron and
               Mai Gim{\'{e}}nez and
               Yury Sulsky and
               Jackie Kay and
               Jost Tobias Springenberg and
               Tom Eccles and
               Jake Bruce and
               Ali Razavi and
               Ashley Edwards and
               Nicolas Heess and
               Yutian Chen and
               Raia Hadsell and
               Oriol Vinyals and
               Mahyar Bordbar and
               Nando de Freitas},
  title     = {A Generalist Agent},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{choromanski2017the,
  author    = {Krzysztof Marcin Choromanski and
               Mark Rowland and
               Adrian Weller},
  title     = {The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings},
  booktitle = NIPS,
  year      = {2017},
}

@inproceedings{yu2016orthogonal,
  author    = {Felix X. Yu and
               Ananda Theertha Suresh and
               Krzysztof Marcin Choromanski and
               Daniel N. Holtmann{-}Rice and
               Sanjiv Kumar},
  title     = {Orthogonal Random Features},
  booktitle = NIPS,
  year      = {2016},
}

@article{chan2018spectral,
  author    = {T.{-}H. Hubert Chan and
               Anand Louis and
               Zhihao Gavin Tang and
               Chenzi Zhang},
  title     = {Spectral Properties of Hypergraph Laplacian and Approximation Algorithms},
  journal   = {J. {ACM}},
  year      = {2018},
}

@inproceedings{cohen2016group,
  author    = {Taco Cohen and
               Max Welling},
  title     = {Group Equivariant Convolutional Networks},
  booktitle = ICML,
  year      = {2016},
}

@inproceedings{cohen2017steerable,
  author    = {Taco S. Cohen and
               Max Welling},
  title     = {Steerable CNNs},
  booktitle = ICLR,
  year      = {2017},
}

@inproceedings{weiler20183d,
  author    = {Maurice Weiler and
               Mario Geiger and
               Max Welling and
               Wouter Boomsma and
               Taco Cohen},
  title     = {3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric
               Data},
  booktitle = NIPS,
  year      = {2018},
}

@article{thomas2018tensor,
  author    = {Nathaniel Thomas and
               Tess E. Smidt and
               Steven Kearnes and
               Lusann Yang and
               Li Li and
               Kai Kohlhoff and
               Patrick Riley},
  title     = {Tensor Field Networks: Rotation- and Translation-Equivariant Neural
               Networks for 3D Point Clouds},
  journal   = {arXiv},
  year      = {2018},
}

@article{lecun1989backpropagation,
  author    = {Yann LeCun and
               Bernhard E. Boser and
               John S. Denker and
               Donnie Henderson and
               Richard E. Howard and
               Wayne E. Hubbard and
               Lawrence D. Jackel},
  title     = {Backpropagation Applied to Handwritten Zip Code Recognition},
  journal   = {Neural Comput.},
  year      = {1989},
}

@inproceedings{krizhevsky2012imagenet,
  author    = {Alex Krizhevsky and
               Ilya Sutskever and
               Geoffrey E. Hinton},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = NIPS,
  year      = {2012},
}

@inproceedings{maron2020onlearning,
  author    = {Haggai Maron and
               Or Litany and
               Gal Chechik and
               Ethan Fetaya},
  title     = {On Learning Sets of Symmetric Elements},
  booktitle = ICML,
  year      = {2020},
}

@inproceedings{pan2022permutation,
  author    = {Horace Pan and
               Risi Kondor},
  title     = {Permutation Equivariant Layers for Higher Order Interactions},
  booktitle = {AISTATS},
  year      = {2022},
}

@article{albooyeh2019incidence,
  author    = {Marjan Albooyeh and
               Daniele Bertolini and
               Siamak Ravanbakhsh},
  title     = {Incidence Networks for Geometric Deep Learning},
  journal   = {arXiv},
  year      = {2019},
}

@article{bronstein2017geometric,
  author    = {Michael M. Bronstein and
               Joan Bruna and
               Yann LeCun and
               Arthur Szlam and
               Pierre Vandergheynst},
  title     = {Geometric Deep Learning: Going beyond Euclidean data},
  journal   = {{IEEE} Signal Process. Mag.},
  year      = {2017},
}

@inproceedings{serviansky2020set,
  author    = {Hadar Serviansky and
               Nimrod Segol and
               Jonathan Shlomi and
               Kyle Cranmer and
               Eilam Gross and
               Haggai Maron and
               Yaron Lipman},
  title     = {Set2Graph: Learning Graphs From Sets},
  booktitle = NIPS,
  year      = {2020},
}

@inproceedings{xu2021vitae,
  author    = {Yufei Xu and
               Qiming Zhang and
               Jing Zhang and
               Dacheng Tao},
  title     = {ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive
               Bias},
  booktitle = NIPS,
  year      = {2021},
}

@article{wang2022deepnet,
  author    = {Hongyu Wang and
               Shuming Ma and
               Li Dong and
               Shaohan Huang and
               Dongdong Zhang and
               Furu Wei},
  title     = {DeepNet: Scaling Transformers to 1, 000 Layers},
  journal   = {arXiv},
  year      = {2022},
}

@article{choromanski2021from,
  author    = {Krzysztof Choromanski and
               Han Lin and
               Haoxian Chen and
               Tianyi Zhang and
               Arijit Sehanobish and
               Valerii Likhosherstov and
               Jack Parker-Holder and
               Tamas Sarlos and
               Adrian Weller and
               Thomas Weingarten},
  title     = {From Block-Toeplitz Matrices to Differential Equations on Graphs: Towards a General Theory for Scalable Masked Transformers},
  journal   = {arXiv},
  year      = {2021},
}

@article{shi2022benchmarking,
  author    = {Yu Shi and
               Shuxin Zheng and
               Guolin Ke and
               Yifei Shen and
               Jiacheng You and
               Jiyan He and
               Shengjie Luo and
               Chang Liu and
               Di He and
               Tie{-}Yan Liu},
  title     = {Benchmarking Graphormer on Large-Scale Molecular Modeling Datasets},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{dai2021coatnet,
  author    = {Zihang Dai and
               Hanxiao Liu and
               Quoc V. Le and
               Mingxing Tan},
  title     = {CoAtNet: Marrying Convolution and Attention for All Data Sizes},
  booktitle = NIPS,
  year      = {2021},
}

@inproceedings{yuan2021incorporating,
  author    = {Kun Yuan and
               Shaopeng Guo and
               Ziwei Liu and
               Aojun Zhou and
               Fengwei Yu and
               Wei Wu},
  title     = {Incorporating Convolution Designs into Visual Transformers},
  booktitle = ICCV,
  year      = {2021},
}

@inproceedings{wies2021which,
  author    = {Noam Wies and
               Yoav Levine and
               Daniel Jannai and
               Amnon Shashua},
  title     = {Which transformer architecture fits my data? {A} vocabulary bottleneck
               in self-attention},
  booktitle = ICML,
  year      = {2021},
}

@article{he2021masked,
  author    = {Kaiming He and
               Xinlei Chen and
               Saining Xie and
               Yanghao Li and
               Piotr Doll{\'{a}}r and
               Ross B. Girshick},
  title     = {Masked Autoencoders Are Scalable Vision Learners},
  journal   = {arXiv},
  year      = {2021},
}

@article{hanin2017approximating,
  author    = {Boris Hanin and
               Mark Sellke},
  title     = {Approximating Continuous Functions by ReLU Nets of Minimal Width},
  journal   = {arXiv},
  year      = {2017},
}

@article{hornik1989multilayer,
  author    = {Kurt Hornik and
               Maxwell B. Stinchcombe and
               Halbert White},
  title     = {Multilayer feedforward networks are universal approximators},
  journal   = {Neural Networks},
  year      = {1989},
}

@article{barabasi1999emergence,
  author    = {Albert-Laszlo Barabasi and
               Reka Albert},
  title     = {Emergence of Scaling in Random Networks},
  journal   = {arXiv},
  year      = {1999},
}

@inproceedings{loshchilov2019decoupled,
  author    = {Ilya Loshchilov and
               Frank Hutter},
  title     = {Decoupled Weight Decay Regularization},
  booktitle = ICLR,
  year      = {2019},
}

@inproceedings{brody2022how,
  author    = {Shaked Brody and
               Uri Alon and
               Eran Yahav},
  title     = {How Attentive are Graph Attention Networks?},
  booktitle = ICLR,
  year      = {2022},
}

@article{wang2022approximately,
  author    = {Rui Wang and
               Robin Walters and
               Rose Yu},
  title     = {Approximately Equivariant Networks for Imperfectly Symmetric Dynamics},
  journal   = {arXiv},
  year      = {2022},
}

@inproceedings{yun2020o(n),
  author    = {Chulhee Yun and
               Yin{-}Wen Chang and
               Srinadh Bhojanapalli and
               Ankit Singh Rawat and
               Sashank J. Reddi and
               Sanjiv Kumar},
  title     = {O(n) Connections are Expressive Enough: Universal Approximability
               of Sparse Transformers},
  booktitle = NIPS,
  year      = {2020},
}

@inproceedings{zaheer2020big,
  author    = {Manzil Zaheer and
               Guru Guruganesh and
               Kumar Avinava Dubey and
               Joshua Ainslie and
               Chris Alberti and
               Santiago Onta{\~{n}}{\'{o}}n and
               Philip Pham and
               Anirudh Ravula and
               Qifan Wang and
               Li Yang and
               Amr Ahmed},
  title     = {Big Bird: Transformers for Longer Sequences},
  booktitle = NIPS,
  year      = {2020},
}

@inproceedings{bhattamishra2020on,
  author    = {Satwik Bhattamishra and
               Arkil Patel and
               Navin Goyal},
  title     = {On the Computational Power of Transformers and Its Implications in
               Sequence Modeling},
  booktitle = {CoNLL},
  year      = {2020},
}

@inproceedings{bhojanapalli2020low,
  author    = {Srinadh Bhojanapalli and
               Chulhee Yun and
               Ankit Singh Rawat and
               Sashank J. Reddi and
               Sanjiv Kumar},
  title     = {Low-Rank Bottleneck in Multi-head Attention Models},
  booktitle = ICML,
  year      = {2020},
}

@inproceedings{cordonnier2020on,
  author    = {Jean{-}Baptiste Cordonnier and
               Andreas Loukas and
               Martin Jaggi},
  title     = {On the Relationship between Self-Attention and Convolutional Layers},
  booktitle = ICLR,
  year      = {2020},
}

@article{andreoli2019convolution,
  author    = {Andreoli, Jean-Marc},
  title     = {Convolution, Attention and Structure Embedding},
  journal   = {arXiv},
  year      = {2019},
}

@article{schur2018pitfalls,
  author    = {Oleksandr Shchur and
               Maximilian Mumme and
               Aleksandar Bojchevski and
               Stephan G{\"{u}}nnemann},
  title     = {Pitfalls of Graph Neural Network Evaluation},
  journal   = {arXiv},
  year      = {2018},
}

@article{rozemberczki2021multi,
  author    = {Benedek Rozemberczki and
               Carl Allen and
               Rik Sarkar},
  title     = {Multi-Scale attributed node embedding},
  journal   = {J. Complex Networks},
  year      = {2021},
}

@article{gorban2016approximation,
  author    = {Alexander N. Gorban and
               Ivan Yu. Tyukin and
               Danil V. Prokhorov and
               Konstantin I. Sofeikov},
  title     = {Approximation with random bases: Pro et Contra},
  journal   = {Inf. Sci.},
  year      = {2016},
}

@inproceedings{shen2021efficient,
  author    = {Zhuoran Shen and
               Mingyuan Zhang and
               Haiyu Zhao and
               Shuai Yi and
               Hongsheng Li},
  title     = {Efficient Attention: Attention with Linear Complexities},
  booktitle = {WACV},
  year      = {2021},
}

@inproceedings{keriven2019universal,
  author    = {Nicolas Keriven and
               Gabriel Peyr{\'{e}}},
  title     = {Universal Invariant and Equivariant Graph Neural Networks},
  booktitle = NIPS,
  year      = {2019},
}

@misc{zico2019champs,
  title = {First Place Solution of 2019 CHAMPS Predicting Molecular Properties Challenge},
  author    = {Zico Kolter and
               Shaojie Bai and
               Devin Wilmott and
               Mordechai Kornbluth and
               Jonathan Mailoa},
  howpublished = {\url{https://www.kaggle.com/c/champs-scalar-coupling/discussion/106575}},
  year      = {2019},
}

@inproceedings{kim2021how,
  author    = {Dongkwan Kim and
               Alice Oh},
  title     = {How to Find Your Friendly Neighborhood: Graph Attention Design with
               Self-Supervision},
  booktitle = {ICLR},
}

@inproceedings{kingma2015adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {ICLR},
  year      = {2015},
}
