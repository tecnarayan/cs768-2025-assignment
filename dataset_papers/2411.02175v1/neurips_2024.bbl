\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abati et~al.(2020)Abati, Tomczak, Blankevoort, Calderara, Cucchiara, and Bejnordi]{conditional_gated}
Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, and Babak~Ehteshami Bejnordi.
\newblock Conditional channel gated networks for task-aware continual learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3931--3940, 2020.

\bibitem[Ahrens et~al.(2024)Ahrens, Lehmann, Lee, and Wermter]{read_between_layers}
Kyra Ahrens, Hans~Hergen Lehmann, Jae~Hee Lee, and Stefan Wermter.
\newblock Read between the layers: Leveraging intra-layer representations for rehearsal-free continual learning with pre-trained models.
\newblock \emph{Transactions on Machine Learning Research}, 2024.

\bibitem[Arani et~al.(2021)Arani, Sarfraz, and Zonooz]{arani2021learning}
Elahe Arani, Fahad Sarfraz, and Bahram Zonooz.
\newblock Learning fast, learning slow: A general continual learning method based on complementary learning system.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and Calderara]{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara.
\newblock Dark experience for general continual learning: a strong, simple baseline.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 15920--15930, 2020.

\bibitem[Castro et~al.(2018)Castro, Mar{\'\i}n-Jim{\'e}nez, Guil, Schmid, and Alahari]{EEIL}
Francisco~M Castro, Manuel~J Mar{\'\i}n-Jim{\'e}nez, Nicol{\'a}s Guil, Cordelia Schmid, and Karteek Alahari.
\newblock End-to-end incremental learning.
\newblock In \emph{European Conference on Computer Vision}, pages 233--248, 2018.

\bibitem[Chen et~al.(2022)Chen, Ge, Tong, Wang, Song, Wang, and Luo]{adaptformer}
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.
\newblock Adaptformer: Adapting vision transformers for scalable visual recognition.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16664--16678, 2022.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{ViT}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gao et~al.(2023)Gao, Zhao, Sun, Xi, Zhang, Ghanem, and Zhang]{gao2023unified}
Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang.
\newblock A unified continual learning framework with general parameter-efficient tuning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 11483--11493, 2023.

\bibitem[Han et~al.(2024{\natexlab{a}})Han, Lin, Sun, Gao, Yan, Ding, Gao, and Xia]{anchor-robust-ft}
Jinwei Han, Zhiwen Lin, Zhongyisun Sun, Yingguo Gao, Ke~Yan, Shouhong Ding, Yuan Gao, and Gui-Song Xia.
\newblock Anchor-based robust finetuning of vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 26919--26928, 2024{\natexlab{a}}.

\bibitem[Han et~al.(2024{\natexlab{b}})Han, Gao, Liu, Zhang, and Zhang]{pet_survey}
Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai~Qian Zhang.
\newblock Parameter-efficient fine-tuning for large models: A comprehensive survey, 2024{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo, Desai, Zhu, Parajuli, Guo, Song, Steinhardt, and Gilmer]{inr}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 8340--8349, October 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Zhao, Basart, Steinhardt, and Song]{ina}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 15262--15271, 2021{\natexlab{b}}.

\bibitem[Hoerl and Kennard(1970)]{Ridge_regressio}
Arthur~E Hoerl and Robert~W Kennard.
\newblock Ridge regression: Biased estimation for nonorthogonal problems.
\newblock \emph{Technometrics}, 12\penalty0 (1):\penalty0 55--67, 1970.

\bibitem[Hou et~al.(2019)Hou, Pan, Loy, Wang, and Lin]{LUCIR}
Saihui Hou, Xinyu Pan, Chen~Change Loy, Zilei Wang, and Dahua Lin.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 831--839, 2019.

\bibitem[Janson et~al.(2022)Janson, Zhang, Aljundi, and Elhoseiny]{simple_baseline}
Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny.
\newblock A simple baseline that questions the use of pretrained-models in continual learning.
\newblock In \emph{NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications}, 2022.

\bibitem[Jia et~al.(2022)Jia, Tang, Chen, Cardie, Belongie, Hariharan, and Lim]{VPT}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock In \emph{European Conference on Computer Vision}, 2022.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, et~al.]{EWC}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0 (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kumaran et~al.(2016)Kumaran, Hassabis, and McClelland]{cls_theory}
Dharshan Kumaran, Demis Hassabis, and James~L McClelland.
\newblock What learning systems do intelligent agents need? complementary learning systems theory updated.
\newblock \emph{Trends in cognitive sciences}, 20\penalty0 (7):\penalty0 512--534, 2016.

\bibitem[Li and Hoiem(2017)]{LwF}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock \emph{IEEE Transactions on Pattern analysis and machine intelligence}, 40\penalty0 (12):\penalty0 2935--2947, 2017.

\bibitem[Lian et~al.(2022)Lian, Zhou, Feng, and Wang]{ssf}
Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
\newblock Scaling \& shifting your features: A new baseline for efficient model tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 109--123, 2022.

\bibitem[Liu et~al.(2021)Liu, Schiele, and Sun]{adaptiveAggregation}
Yaoyao Liu, Bernt Schiele, and Qianru Sun.
\newblock Adaptive aggregation networks for class-incremental learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2544--2553, 2021.

\bibitem[McDonnell et~al.(2023)McDonnell, Gong, Parvaneh, Abbasnejad, and van~den Hengel]{mcdonnell2024ranpac}
Mark~D McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton van~den Hengel.
\newblock Ranpac: Random projections and pre-trained models for continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Mensink et~al.(2013)Mensink, Verbeek, Perronnin, and Csurka]{distance-based}
Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka.
\newblock Distance-based image classification: Generalizing to new classes at near-zero cost.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 35\penalty0 (11):\penalty0 2624--2637, 2013.

\bibitem[Ostapenko et~al.(2019)Ostapenko, Puscas, Klein, Jahnichen, and Nabi]{ostapenko2019learning}
Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi.
\newblock Learning to remember: A synaptic plasticity driven framework for continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 11321--11329, 2019.

\bibitem[Panos et~al.(2023)Panos, Kobe, Reino, Aljundi, and Turner]{panos2023first}
Aristeidis Panos, Yuriko Kobe, Daniel~Olmeda Reino, Rahaf Aljundi, and Richard~E Turner.
\newblock First session adaptation: A strong replay-free baseline for class-incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 18820--18830, 2023.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and Wang]{domainnet}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 1406--1415, 2019.

\bibitem[Qi et~al.(2018)Qi, Brown, and Lowe]{Imprinting}
Hang Qi, Matthew Brown, and David~G Lowe.
\newblock Low-shot learning with imprinted weights.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5822--5830, 2018.

\bibitem[Ramasesh et~al.(2021)Ramasesh, Lewkowycz, and Dyer]{ramasesh2021effect}
Vinay~Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer.
\newblock Effect of scale on catastrophic forgetting in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Razdaibiedina et~al.(2022)Razdaibiedina, Mao, Hou, Khabsa, Lewis, and Almahairi]{razdaibiedina2022progressive}
Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and Amjad Almahairi.
\newblock Progressive prompts: Continual learning for language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and Lampert]{icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2001--2010, 2017.

\bibitem[Shi et~al.(2022)Shi, Zhou, Liang, Jiang, Feng, Torr, Bai, and Tan]{cwd}
Yujun Shi, Kuangqi Zhou, Jian Liang, Zihang Jiang, Jiashi Feng, Philip~HS Torr, Song Bai, and Vincent~YF Tan.
\newblock Mimicking the oracle: An initial phase decorrelation approach for class incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 16722--16731, 2022.

\bibitem[Smith et~al.(2023)Smith, Karlinsky, Gutta, Cascante-Bonilla, Kim, Arbelle, Panda, Feris, and Kira]{smith2023coda}
James~Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira.
\newblock Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11909--11919, 2023.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{ProtoNet}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Tan et~al.(2024)Tan, Zhou, Xiang, Wang, Wu, and Li]{ssiat}
Yuwen Tan, Qinhao Zhou, Xiang Xiang, Ke~Wang, Yuchuan Wu, and Yongbin Li.
\newblock Semantically-shifted incremental adapter-tuning is a continual vitransformer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024.

\bibitem[Van~der Maaten and Hinton(2008)]{t-SNE}
Laurens Van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock \emph{Journal of machine learning research}, 9\penalty0 (11), 2008.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{CUB}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock \emph{Technical report}, 2011.

\bibitem[Wang et~al.(2023)Wang, Xie, Zhang, Huang, Su, and Zhu]{HiDe-Prompt}
Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, and Jun Zhu.
\newblock Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Huang, and Hong]{S-prompt}
Yabin Wang, Zhiwu Huang, and Xiaopeng Hong.
\newblock S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5682--5695, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Zhang, Ebrahimi, Sun, Zhang, Lee, Ren, Su, Perot, Dy, et~al.]{wang2022dualprompt}
Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et~al.
\newblock Dualprompt: Complementary prompting for rehearsal-free continual learning.
\newblock In \emph{European Conference on Computer Vision}, pages 631--648. Springer, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Zhang, Lee, Zhang, Sun, Ren, Su, Perot, Dy, and Pfister]{l2p}
Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister.
\newblock Learning to prompt for continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 139--149, 2022{\natexlab{c}}.

\bibitem[Xu and Zhu(2018)]{xu2018reinforced}
Ju~Xu and Zhanxing Zhu.
\newblock Reinforced continual learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Yoon et~al.(2018)Yoon, Yang, Lee, and Hwang]{yoon2018lifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock In \emph{6th International Conference on Learning Representations, ICLR 2018}. International Conference on Learning Representations, ICLR, 2018.

\bibitem[Yoon et~al.(2020)Yoon, Kim, Seo, and Moon]{XtarNet}
Sung~Whan Yoon, Do-Yeon Kim, Jun Seo, and Jaekyun Moon.
\newblock Xtarnet: Learning to extract task-adaptive representation for incremental few-shot learning.
\newblock In \emph{International conference on machine learning}, 2020.

\bibitem[Yu et~al.(2020)Yu, Twardowski, Liu, Herranz, Wang, Cheng, Jui, and Weijer]{SDC}
Lu~Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van~de Weijer.
\newblock Semantic drift compensation for class-incremental learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6982--6991, 2020.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{synaptic}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In \emph{International conference on machine learning}, pages 3987--3995, 2017.

\bibitem[Zhai et~al.(2019)Zhai, Puigcerver, Kolesnikov, Ruyssen, Riquelme, Lucic, Djolonga, Pinto, Neumann, Dosovitskiy, et~al.]{vtab}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre~Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et~al.
\newblock A large-scale study of representation learning with the visual task adaptation benchmark.
\newblock \emph{arXiv preprint arXiv:1910.04867}, 2019.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Kang, Chen, and Wei]{slca}
Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei.
\newblock Slca: Slow learner with classifier alignment for continual learning on a pre-trained model.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 19148--19158, October 2023.

\bibitem[Zhang et~al.(2022)Zhang, Yin, Shao, and Liu]{ob}
Yuanhan Zhang, Zhenfei Yin, Jing Shao, and Ziwei Liu.
\newblock Benchmarking omni-vision representation through the lens of visual realms.
\newblock In \emph{European Conference on Computer Vision}, pages 594--611. Springer, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Lu, Xu, Cheng, Guo, Niu, and Fang]{CABD}
Linglan Zhao, Jing Lu, Yunlu Xu, Zhanzhan Cheng, Dashan Guo, Yi~Niu, and Xiangzhong Fang.
\newblock Few-shot class-incremental learning via class-aware bilateral distillation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 11838--11847, 2023.

\bibitem[Zhou et~al.(2024{\natexlab{a}})Zhou, Cai, Ye, Zhan, and Liu]{adam}
Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu.
\newblock Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need.
\newblock \emph{International Journal of Computer Vision}, pages 1--21, 2024{\natexlab{a}}.

\bibitem[Zhou et~al.(2024{\natexlab{b}})Zhou, Sun, Ning, Ye, and Zhan]{zhou2024continual}
Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, and De-Chuan Zhan.
\newblock Continual learning with pre-trained models: A survey.
\newblock In \emph{Proceedings of the 33rd International Joint Conference on Artificial Intelligence (IJCAI)}, pages 8363--8371, 2024{\natexlab{b}}.

\bibitem[Zhou et~al.(2024{\natexlab{c}})Zhou, Sun, Ye, and Zhan]{ease}
Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan Zhan.
\newblock Expandable subspace ensemble for pre-trained model-based class-incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024{\natexlab{c}}.

\bibitem[Zhu et~al.(2024)Zhu, Sun, Li, Shen, Yan, Ding, Kuang, and Wu]{model_Tailor}
Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke~Yan, Shouhong Ding, Kun Kuang, and Chao Wu.
\newblock Model tailor: Mitigating catastrophic forgetting in multi-modal large language models.
\newblock In \emph{International conference on machine learning}, 2024.

\end{thebibliography}
