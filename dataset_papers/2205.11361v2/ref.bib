@article{oymak2021provable,
  title={Provable Super-Convergence With a Large Cyclical Learning Rate},
  author={Oymak, Samet},
  journal={IEEE Signal Processing Letters},
  volume={28},
  pages={1645--1649},
  year={2021},
  publisher={IEEE}
}


@article{schilling1998feller,
  title={Feller processes generated by pseudo-differential operators: On the {H}ausdorff dimension of their sample paths},
  author={Schilling, Ren{\'e} L},
  journal={Journal of Theoretical Probability},
  volume={11},
  number={2},
  pages={303--330},
  year={1998},
  publisher={Springer}
}

@inproceedings{smith2019super,
  title={Super-convergence: Very fast training of neural networks using large learning rates},
  author={Smith, Leslie N and Topin, Nicholay},
  booktitle={Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications},
  volume={11006},
  pages={1100612},
  year={2019},
  organization={International Society for Optics and Photonics}
}

@article{golatkar2019time,
  title={Time matters in regularizing deep networks: Weight decay and data augmentation affect early learning dynamics, matter little near convergence},
  author={Golatkar, Aditya and Achille, Alessandro and Soatto, Stefano},
  journal={arXiv preprint arXiv:1905.13277},
  year={2019}
}

@article{yedida2021lipschitzlr,
  title={LipschitzLR: Using theoretically computed adaptive learning rates for fast convergence},
  author={Yedida, Rahul and Saha, Snehanshu and Prashanth, Tejas},
  journal={Applied Intelligence},
  volume={51},
  number={3},
  pages={1460--1478},
  year={2021},
  publisher={Springer}
}

@article{vaswani2019painless,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={3732--3745},
  year={2019}
}

@article{blier2018learning,
  title={Learning with random learning rates},
  author={Blier, L{\'e}onard and Wolinski, Pierre and Ollivier, Yann},
  journal={arXiv preprint arXiv:1810.01322},
  year={2018}
}

@inproceedings{wu2019demystifying,
  title={Demystifying learning rate policies for high accuracy training of deep neural networks},
  author={Wu, Yanzhao and Liu, Ling and Bae, Juhyun and Chow, Ka-Ho and Iyengar, Arun and Pu, Calton and Wei, Wenqi and Yu, Lei and Zhang, Qi},
  booktitle={2019 IEEE International Conference on Big Data (Big Data)},
  pages={1971--1980},
  year={2019},
  organization={IEEE}
}

@book{applebaum2009levy,
  title={L{\'e}vy Processes and Stochastic Calculus},
  author={Applebaum, David},
  year={2009},
  publisher={Cambridge University Press}
}

@article{smith2017don,
  title={Don't decay the learning rate, increase the batch size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@article{agarwal2021acceleration,
  title={Acceleration via Fractal Learning Rate Schedules},
  author={Agarwal, Naman and Goel, Surbhi and Zhang, Cyril},
  journal={arXiv preprint arXiv:2103.01338},
  year={2021}
}

@article{smith2018disciplined,
  title={A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay},
  author={Smith, Leslie N},
  journal={arXiv preprint arXiv:1803.09820},
  year={2018}
}

@inproceedings{hanin2021data,
  title={How Data Augmentation affects Optimization for Linear Regression},
  author={Hanin, Boris and Sun, Yi},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{camuto2021fractal,
  title={Fractal Structure and Generalization Properties of Stochastic Optimization Algorithms},
  author={Camuto, Alexander and Deligiannidis, George and Erdogdu, Murat A and G{\"u}rb{\"u}zbalaban, Mert and {\c{S}}im{\c{s}}ekli, Umut and Zhu, Lingjiong},
  journal={arXiv preprint arXiv:2106.04881},
  year={2021}
}

@article{csimcsekli2020hausdorff,
  title={Hausdorff dimension, heavy tails, and generalization in neural networks},
  author={{\c{S}}im{\c{s}}ekli, Umut and Sener, Ozan and Deligiannidis, George and Erdogdu, Murat A},
  journal={arXiv preprint arXiv:2006.09313},
  year={2020}
}

@article{li2021validity,
  title={On the Validity of Modeling {SGD} with Stochastic Differential Equations ({SDE}s)},
  author={Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2102.12470},
  year={2021}
}

@article{hodgkinson2021generalization,
  title={Generalization Properties of Stochastic Optimizers via Trajectory Analysis},
  author={Hodgkinson, Liam and {\c{S}}im{\c{s}}ekli, Umut and Khanna, Rajiv and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2108.00781},
  year={2021}
}

@inproceedings{hodgkinson2021multiplicative,
  title={Multiplicative noise and heavy tails in stochastic optimization},
  author={Hodgkinson, Liam and Mahoney, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4262--4274},
  year={2021},
  organization={PMLR}
}

@article{nguyen2019first,
  title={First exit time analysis of stochastic gradient descent under heavy-tailed gradient noise},
  author={Nguyen, Thanh Huy and {\c{S}}im{\c{s}}ekli, Umut and G{\"u}rb{\"u}zbalaban, Mert and Richard, Ga{\"e}l},
  journal={arXiv preprint arXiv:1906.09069},
  year={2019}
}

@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  pages={5827--5837},
  year={2019},
  organization={PMLR}
}

@article{camuto2021asymmetric,
  title={Asymmetric Heavy Tails and Implicit Bias in {G}aussian Noise Injections},
  author={Camuto, Alexander and Wang, Xiaoyu and Zhu, Lingjiong and Holmes, Chris and G{\"u}rb{\"u}zbalaban, Mert and {\c{S}}im{\c{s}}ekli, Umut},
  journal={arXiv preprint arXiv:2102.07006},
  year={2021}
}

@article{gaspard1988sporadicity,
  title={Sporadicity: between periodic and chaotic dynamical behaviors},
  author={Gaspard, Pierre and Wang, X-J},
  journal={Proceedings of the National Academy of Sciences},
  volume={85},
  number={13},
  pages={4591--4595},
  year={1988},
  publisher={National Acad Sciences}
}

@article{saxena2021training,
  title={Training With Data Dependent Dynamic Learning Rates},
  author={Saxena, Shreyas and Vyas, Nidhi and DeCoste, Dennis},
  journal={arXiv preprint arXiv:2105.13464},
  year={2021}
}

@INPROCEEDINGS{Islam2011,
  author={Islam, Mobarakol and Rihab Rana, Md. and Ahmed, Sultan Uddin and Enamul Kabir, A. N. M. and Shahjahan, Md.},
  booktitle={2011 International Conference on Emerging Trends in Electrical and Computer Technology}, 
  title={Training neural network with chaotic learning rate}, 
  year={2011},
  volume={},
  number={},
  pages={781-785},
  doi={10.1109/ICETECT.2011.5760224}}
  
@article{chevyrev2020superdiffusive,
  title={Superdiffusive limits for deterministic fast--slow dynamical systems},
  author={Chevyrev, Ilya and Friz, Peter K and Korepanov, Alexey and Melbourne, Ian},
  journal={Probability Theory and Related Fields},
  volume={178},
  number={3},
  pages={735--770},
  year={2020},
  publisher={Springer}
}

@article{gottwald2021simulation,
  title={Simulation of Non-{L}ipschitz Stochastic Differential Equations Driven by $\alpha$-Stable Noise: A Method Based on Deterministic Homogenization},
  author={Gottwald, Georg A and Melbourne, Ian},
  journal={Multiscale Modeling \& Simulation},
  volume={19},
  number={2},
  pages={665--687},
  year={2021},
  publisher={SIAM}
}

@inproceedings{chevyrev2016multiscale,
  title={Multiscale systems, homogenization, and rough paths},
  author={Chevyrev, Ilya and Friz, Peter K and Korepanov, Alexey and Melbourne, Ian and Zhang, Huilin},
  booktitle={International Conference in Honor of the 75th Birthday of SRS Varadhan},
  pages={17--48},
  year={2016},
  organization={Springer}
}

@article{geiping2021stochastic,
  title={Stochastic training is not necessary for generalization},
  author={Geiping, Jonas and Goldblum, Micah and Pope, Phillip E and Moeller, Michael and Goldstein, Tom},
  journal={arXiv preprint arXiv:2109.14119},
  year={2021}
}

@inproceedings{simsekli2020fractional,
  title={Fractional underdamped {L}angevin dynamics: Retargeting {SGD} with momentum under heavy-tailed gradient noise},
  author={Simsekli, Umut and Zhu, Lingjiong and Teh, Yee Whye and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  pages={8970--8980},
  year={2020},
  organization={PMLR}
}

@article{kong2020stochasticity,
  title={Stochasticity of deterministic gradient descent: Large learning rate for multiscale objective function},
  author={Kong, Lingkai and Tao, Molei},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2625--2638},
  year={2020}
}

@article{chechkin2014marcus,
  title={Marcus versus {S}tratonovich for systems with jump noise},
  author={Chechkin, Alexei and Pavlyukevich, Ilya},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={47},
  number={34},
  pages={342001},
  year={2014},
  publisher={IOP Publishing}
}

@article{chevyrev2019canonical,
  title={Canonical RDEs and general semimartingales as rough paths},
  author={Chevyrev, Ilya and Friz, Peter K},
  journal={The Annals of Probability},
  volume={47},
  number={1},
  pages={420--463},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  booktitle={ICLR},
  year={2021}
}

@article{elkabetz2021continuous,
  title={Continuous vs. discrete optimization of deep neural networks},
  author={Elkabetz, Omer and Cohen, Nadav},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{orvieto2022anticorrelated,
      title={Anticorrelated Noise Injection for Improved Generalization}, 
      author={Antonio Orvieto and Hans Kersting and Frank Proske and Francis Bach and Aurelien Lucchi},
      year={2022},
      eprint={2202.02831},
      archivePrefix={arXiv}
}

@article{gottwald2013homogenization,
  title={Homogenization for deterministic maps and multiplicative noise},
  author={Gottwald, Georg A and Melbourne, Ian},
  journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={469},
  number={2156},
  pages={20130201},
  year={2013},
  publisher={The Royal Society Publishing}
}

@article{gouezel2004central,
  title={Central limit theorem and stable laws for intermittent maps},
  author={Gou{\"e}zel, S{\'e}bastien},
  journal={Probability Theory and Related Fields},
  volume={128},
  number={1},
  pages={82--122},
  year={2004},
  publisher={Springer}
}

@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in SGD},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  pages={3964--3975},
  year={2021},
  organization={PMLR}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }


@article{goldberger2000physiobank,
	title={Physio{B}ank, {P}hysio{T}oolkit, and {P}hysio{N}et: components of a new research resource for complex physiologic signals},
	author={Goldberger, Ary L and Amaral, Luis AN and Glass, Leon and Hausdorff, Jeffrey M and Ivanov, Plamen Ch and Mark, Roger G and Mietus, Joseph E and Moody, George B and Peng, Chung-Kang and Stanley, H Eugene},
	journal={Circulation},
	volume={101},
	number={23},
	pages={e215--e220},
	year={2000},
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@article{krizhevsky2009learning,
	title={Learning multiple layers of features from tiny images},
	author={Krizhevsky, Alex},
	year={2009},
}

@INPROCEEDINGS{Mah12,
  author =       {M.~W.~Mahoney},
  title =        {Approximate Computation and Implicit Regularization for Very Large-scale Data Analysis},
  booktitle =    {Proceedings of the 31st ACM Symposium on Principles of Database Systems},
  year =         {2012},
  pages =        {143--154},
}

@article{lim2021noisy,
  title={Noisy recurrent neural networks},
  author={Lim, Soon Hoe and Erichson, N Benjamin and Hodgkinson, Liam and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@article{young1936,
author = {L. C. Young},
title = {{An inequality of the Hölder type, connected with Stieltjes integration}},
volume = {67},
journal = {Acta Mathematica},
number = {none},
publisher = {Institut Mittag-Leffler},
pages = {251 -- 282},
year = {1936},
doi = {10.1007/BF02401743},
URL = {https://doi.org/10.1007/BF02401743}
}

@book{friz_victoir_2010, 
place={Cambridge}, 
series={Cambridge Studies in Advanced Mathematics}, title={Multidimensional Stochastic Processes as Rough Paths: Theory and Applications},
DOI={10.1017/CBO9780511845079},
publisher={Cambridge University Press},
author={Friz, Peter K. and Victoir, Nicolas B.},
year={2010},
collection={Cambridge Studies in Advanced Mathematics}
}

@book{gnedenko_kolmogorov,
  title     = "Limit Distributions for Sums of Independent Random Variables",
  author    = "Boris Vladimirovich Gnedenko and Andrey Nikolaevich Kolmogorov",
  year      = "1954",
  publisher = "Cambridge, Addison-Wesley",
}

@book{samoradnitsky2017stable,
  title={Stable Non-Gaussian Random Processes: Stochastic Models with Infinite Variance},
  author={Samoradnitsky, G.},
  year={2017},
  publisher={CRC Press}
}

@article{thaler1980estimates,
  title={Estimates of the invariant densities of endomorphisms with indifferent fixed points},
  author={Thaler, Maximilian},
  journal={Israel Journal of Mathematics},
  volume={37},
  number={4},
  pages={303--314},
  year={1980},
  publisher={Springer}
}



@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{cauchy1847methode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin and others},
  journal={Comp. Rend. Sci. Paris},
  volume={25},
  number={1847},
  pages={536--538},
  year={1847}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}

@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as {S}{G}{D}},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}

@book{pavliotis2008multiscale,
  title={Multiscale Methods: Averaging and Homogenization},
  author={Pavliotis, Grigoris and Stuart, Andrew},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@article{marcus1981modeling,
  title={Modeling and approximation of stochastic differential equations driven by semimartingales},
  author={Marcus, Steven I},
  journal={Stochastics: An International Journal of Probability and Stochastic Processes},
  volume={4},
  number={3},
  pages={223--245},
  year={1981},
  publisher={Taylor \& Francis}
}

@article{neelakantan2015adding,
  title={Adding gradient noise improves learning for very deep networks},
  author={Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
  journal={arXiv preprint arXiv:1511.06807},
  year={2015}
}