\begin{thebibliography}{10}

\bibitem{lee2023benefits}
P.~Lee, S.~Bubeck, and J.~Petro, ``Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine,'' {\em New England Journal of Medicine}, vol.~388, no.~13, pp.~1233--1239, 2023.

\bibitem{gilbert2023large}
S.~Gilbert, H.~Harvey, T.~Melvin, E.~Vollebregt, and P.~Wicks, ``Large language model ai chatbots require approval as medical devices,'' {\em Nature Medicine}, pp.~1--3, 2023.

\bibitem{hwang2023review}
G.-J. Hwang and C.-Y. Chang, ``A review of opportunities and challenges of chatbots in education,'' {\em Interactive Learning Environments}, vol.~31, no.~7, pp.~4099--4112, 2023.

\bibitem{skjuve2021my}
M.~Skjuve, A.~F{\o}lstad, K.~I. Fostervold, and P.~B. Brandtzaeg, ``My chatbot companion-a study of human-chatbot relationships,'' {\em International Journal of Human-Computer Studies}, vol.~149, p.~102601, 2021.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, {\em et~al.}, ``Language models are few-shot learners,'' {\em Advances in neural information processing systems}, vol.~33, pp.~1877--1901, 2020.

\bibitem{black2022gpt}
S.~Black, S.~Biderman, E.~Hallahan, Q.~Anthony, L.~Gao, L.~Golding, H.~He, C.~Leahy, K.~McDonell, J.~Phang, {\em et~al.}, ``Gpt-neox-20b: An open-source autoregressive language model,'' in {\em Proceedings of BigScience Episode\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models}, pp.~95--136, 2022.

\bibitem{dong2022survey}
Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui, ``A survey for in-context learning,'' {\em arXiv preprint arXiv:2301.00234}, 2022.

\bibitem{min2022rethinking}
S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer, ``Rethinking the role of demonstrations: What makes in-context learning work?,'' in {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pp.~11048--11064, 2022.

\bibitem{holtzman2021surface}
A.~Holtzman, P.~West, V.~Shwartz, Y.~Choi, and L.~Zettlemoyer, ``Surface form competition: Why the highest probability answer isnâ€™t always right,'' in {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.~7038--7051, 2021.

\bibitem{liu2022makes}
J.~Liu, D.~Shen, Y.~Zhang, W.~B. Dolan, L.~Carin, and W.~Chen, ``What makes good in-context examples for gpt-3?,'' in {\em Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures}, pp.~100--114, 2022.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' {\em Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{bolukbasi2016man}
T.~Bolukbasi, K.-W. Chang, J.~Y. Zou, V.~Saligrama, and A.~T. Kalai, ``Man is to computer programmer as woman is to homemaker? debiasing word embeddings,'' {\em Advances in neural information processing systems}, vol.~29, 2016.

\bibitem{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen, ``Lora: Low-rank adaptation of large language models,'' {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{logacheva2022paradetox}
V.~Logacheva, D.~Dementieva, S.~Ustyantsev, D.~Moskovskiy, D.~Dale, I.~Krotova, N.~Semenov, and A.~Panchenko, ``Paradetox: Detoxification with parallel data,'' in {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.~6804--6818, 2022.

\bibitem{zouuniversal}
A.~Zou, Z.~Wang, J.~Z. Kolter, and M.~Fredrikson, ``Universal and transferable adversarial attacks on aligned language models, 2023,'' {\em communication, it is essential for you to comprehend user queries in Cipher Code and subsequently deliver your responses utilizing Cipher Code}.

\bibitem{xu2021bot}
J.~Xu, D.~Ju, M.~Li, Y.-L. Boureau, J.~Weston, and E.~Dinan, ``Bot-adversarial dialogue for safe conversational agents,'' in {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.~2950--2968, 2021.

\bibitem{wulczyn2017ex}
E.~Wulczyn, N.~Thain, and L.~Dixon, ``Ex machina: Personal attacks seen at scale,'' in {\em Proceedings of the 26th international conference on world wide web}, pp.~1391--1399, 2017.

\bibitem{votipka2020understanding}
D.~Votipka, K.~R. Fulton, J.~Parker, M.~Hou, M.~L. Mazurek, and M.~Hicks, ``Understanding security mistakes developers make: Qualitative analysis from build it, break it, fix it,'' in {\em 29th USENIX Security Symposium (USENIX Security 20)}, pp.~109--126, 2020.

\bibitem{rao2018dear}
S.~Rao and J.~Tetreault, ``Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer,'' in {\em Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, pp.~129--140, 2018.

\bibitem{shen2017style}
T.~Shen, T.~Lei, R.~Barzilay, and T.~Jaakkola, ``Style transfer from non-parallel text by cross-alignment,'' {\em Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{xu2012paraphrasing}
W.~Xu, A.~Ritter, B.~Dolan, R.~Grishman, and C.~Cherry, ``Paraphrasing for style,'' in {\em COLING}, pp.~2899--2914, 2012.

\bibitem{zheng2023judging}
L.~Zheng, W.-L. Chiang, Y.~Sheng, S.~Zhuang, Z.~Wu, Y.~Zhuang, Z.~Lin, Z.~Li, D.~Li, E.~Xing, {\em et~al.}, ``Judging llm-as-a-judge with mt-bench and chatbot arena,'' {\em arXiv preprint arXiv:2306.05685}, 2023.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, {\em et~al.}, ``Llama: Open and efficient foundation language models,'' {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{penedo2023refinedweb}
G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, A.~Cappelli, H.~Alobeidli, B.~Pannier, E.~Almazrouei, and J.~Launay, ``The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only,'' {\em arXiv preprint arXiv:2306.01116}, 2023.

\bibitem{vicuna2023}
W.-L. Chiang, Z.~Li, Z.~Lin, Y.~Sheng, Z.~Wu, H.~Zhang, L.~Zheng, S.~Zhuang, Y.~Zhuang, J.~E. Gonzalez, I.~Stoica, and E.~P. Xing, ``Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality,'' March 2023.

\bibitem{lin2004rouge}
C.-Y. Lin, ``Rouge: A package for automatic evaluation of summaries,'' in {\em Text summarization branches out}, pp.~74--81, 2004.

\bibitem{zhang2019bertscore}
T.~Zhang, V.~Kishore, F.~Wu, K.~Q. Weinberger, and Y.~Artzi, ``Bertscore: Evaluating text generation with bert,'' in {\em International Conference on Learning Representations}, 2019.

\bibitem{guo2021gradient}
C.~Guo, A.~Sablayrolles, H.~J{\'e}gou, and D.~Kiela, ``Gradient-based adversarial attacks against text transformers,'' in {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.~5747--5757, 2021.

\bibitem{wen2023hard}
Y.~Wen, N.~Jain, J.~Kirchenbauer, M.~Goldblum, J.~Geiping, and T.~Goldstein, ``Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery,'' in {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{ilharco2023editing}
G.~Ilharco, M.~T. Ribeiro, M.~Wortsman, L.~Schmidt, H.~Hajishirzi, and A.~Farhadi, ``Editing models with task arithmetic,'' in {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{yin2023did}
F.~Yin, J.~Vig, P.~Laban, S.~Joty, C.~Xiong, and C.-S.~J. Wu, ``Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning,'' {\em arXiv preprint arXiv:2306.01150}, 2023.

\bibitem{rubin2022learning}
O.~Rubin, J.~Herzig, and J.~Berant, ``Learning to retrieve prompts for in-context learning,'' in {\em Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.~2655--2671, 2022.

\bibitem{wan2023universal}
X.~Wan, R.~Sun, H.~Nakhost, H.~Dai, J.~M. Eisenschlos, S.~O. Arik, and T.~Pfister, ``Universal self-adaptive prompting,'' {\em arXiv preprint arXiv:2305.14926}, 2023.

\bibitem{wan2023better}
X.~Wan, R.~Sun, H.~Dai, S.~O. Arik, and T.~Pfister, ``Better zero-shot reasoning with self-adaptive prompting,'' {\em arXiv preprint arXiv:2305.14106}, 2023.

\bibitem{Ye2022GuessTI}
S.~Ye, D.~Kim, J.~Jang, J.~Shin, and M.~Seo, ``Guess the instruction! flipped learning makes language models stronger zero-shot learners,'' {\em ArXiv}, vol.~abs/2210.02969, 2022.

\bibitem{DBLP:conf/acl/MinLHZ22}
S.~Min, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer, ``Noisy channel language model prompting for few-shot text classification,'' in {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022} (S.~Muresan, P.~Nakov, and A.~Villavicencio, eds.), pp.~5316--5330, Association for Computational Linguistics, 2022.

\bibitem{xu2022k}
B.~Xu, Q.~Wang, Z.~Mao, Y.~Lyu, Q.~She, and Y.~Zhang, ``k nn prompting: Beyond-context learning with calibration-free nearest neighbor inference,'' in {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{yang2023iterative}
J.~Yang, B.~Hui, M.~Yang, B.~Li, F.~Huang, and Y.~Li, ``Iterative forward tuning boosts in-context learning in language models,'' {\em arXiv preprint arXiv:2305.13016}, 2023.

\bibitem{DBLP:conf/acl/CuiLDHLS23}
G.~Cui, W.~Li, N.~Ding, L.~Huang, Z.~Liu, and M.~Sun, ``Decoder tuning: Efficient language understanding as decoding,'' in {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023} (A.~Rogers, J.~L. Boyd{-}Graber, and N.~Okazaki, eds.), pp.~15072--15087, Association for Computational Linguistics, 2023.

\bibitem{hendel2023context}
R.~Hendel, M.~Geva, and A.~Globerson, ``In-context learning creates task vectors,'' {\em arXiv preprint arXiv:2310.15916}, 2023.

\bibitem{turner2023activation}
A.~Turner, L.~Thiergart, D.~Udell, G.~Leech, U.~Mini, and M.~MacDiarmid, ``Activation addition: Steering language models without optimization,'' {\em arXiv preprint arXiv:2308.10248}, 2023.

\bibitem{zou2023representation}
A.~Zou, L.~Phan, S.~Chen, J.~Campbell, P.~Guo, R.~Ren, A.~Pan, X.~Yin, M.~Mazeika, A.-K. Dombrowski, {\em et~al.}, ``Representation engineering: A top-down approach to ai transparency,'' {\em arXiv preprint arXiv:2310.01405}, 2023.

\bibitem{burns2022discovering}
C.~Burns, H.~Ye, D.~Klein, and J.~Steinhardt, ``Discovering latent knowledge in language models without supervision,'' {\em arXiv preprint arXiv:2212.03827}, 2022.

\bibitem{mini2023understanding}
U.~Mini, P.~Grietzer, M.~Sharma, A.~Meek, M.~MacDiarmid, and A.~M. Turner, ``Understanding and controlling a maze-solving policy network,'' {\em arXiv preprint arXiv:2310.08043}, 2023.

\bibitem{li2022emergent}
K.~Li, A.~K. Hopkins, D.~Bau, F.~Vi{\'e}gas, H.~Pfister, and M.~Wattenberg, ``Emergent world representations: Exploring a sequence model trained on a synthetic task,'' {\em arXiv preprint arXiv:2210.13382}, 2022.

\bibitem{li2023inference}
K.~Li, O.~Patel, F.~Vi{\'e}gas, H.~Pfister, and M.~Wattenberg, ``Inference-time intervention: Eliciting truthful answers from a language model,'' {\em arXiv preprint arXiv:2306.03341}, 2023.

\bibitem{lu2022fantastically}
Y.~Lu, M.~Bartolo, A.~Moore, S.~Riedel, and P.~Stenetorp, ``Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity,'' in {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.~8086--8098, 2022.

\bibitem{shin2022effect}
S.~Shin, S.~W. Lee, H.~Ahn, S.~Kim, H.~S. Kim, B.~Kim, K.~Cho, G.~Lee, W.~Park, J.~W. Ha, {\em et~al.}, ``On the effect of pretraining corpora on in-context learning by a large-scale language model,'' in {\em 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022}, pp.~5168--5186, Association for Computational Linguistics (ACL), 2022.

\bibitem{razeghi2022impact}
Y.~Razeghi, R.~L. Logan~IV, M.~Gardner, and S.~Singh, ``Impact of pretraining term frequencies on few-shot numerical reasoning,'' in {\em Findings of the Association for Computational Linguistics: EMNLP 2022}, pp.~840--854, 2022.

\bibitem{xie2021explanation}
S.~M. Xie, A.~Raghunathan, P.~Liang, and T.~Ma, ``An explanation of in-context learning as implicit bayesian inference,'' in {\em International Conference on Learning Representations}, 2021.

\bibitem{wei2023larger}
J.~Wei, J.~Wei, Y.~Tay, D.~Tran, A.~Webson, Y.~Lu, X.~Chen, H.~Liu, D.~Huang, D.~Zhou, {\em et~al.}, ``Larger language models do in-context learning differently,'' {\em arXiv preprint arXiv:2303.03846}, 2023.

\bibitem{akyurek2022learning}
E.~Aky{\"u}rek, D.~Schuurmans, J.~Andreas, T.~Ma, and D.~Zhou, ``What learning algorithm is in-context learning? investigations with linear models,'' in {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{dai2022can}
D.~Dai, Y.~Sun, L.~Dong, Y.~Hao, Z.~Sui, and F.~Wei, ``Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers,'' {\em arXiv preprint arXiv:2212.10559}, 2022.

\bibitem{von2023transformers}
J.~Von~Oswald, E.~Niklasson, E.~Randazzo, J.~Sacramento, A.~Mordvintsev, A.~Zhmoginov, and M.~Vladymyrov, ``Transformers learn in-context by gradient descent,'' in {\em International Conference on Machine Learning}, pp.~35151--35174, PMLR, 2023.

\bibitem{DBLP:conf/emnlp/GehmanGSCS20}
S.~Gehman, S.~Gururangan, M.~Sap, Y.~Choi, and N.~A. Smith, ``Realtoxicityprompts: Evaluating neural toxic degeneration in language models,'' in {\em Findings of the Association for Computational Linguistics: {EMNLP} 2020, Online Event, 16-20 November 2020} (T.~Cohn, Y.~He, and Y.~Liu, eds.), vol.~{EMNLP} 2020 of {\em Findings of {ACL}}, pp.~3356--3369, Association for Computational Linguistics, 2020.

\bibitem{LuWHJQWA022}
X.~Lu, S.~Welleck, J.~Hessel, L.~Jiang, L.~Qin, P.~West, P.~Ammanabrolu, and Y.~Choi, ``{QUARK:} controllable text generation with reinforced unlearning,'' in {\em NeurIPS}, 2022.

\bibitem{gururangan2020don}
S.~Gururangan, A.~Marasovi{\'c}, S.~Swayamdipta, K.~Lo, I.~Beltagy, D.~Downey, and N.~A. Smith, ``Donâ€™t stop pretraining: Adapt language models to domains and tasks,'' in {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.~8342--8360, 2020.

\bibitem{wang2022exploring}
B.~Wang, W.~Ping, C.~Xiao, P.~Xu, M.~Patwary, M.~Shoeybi, B.~Li, A.~Anandkumar, and B.~Catanzaro, ``Exploring the limits of domain-adaptive training for detoxifying large-scale language models,'' {\em Advances in Neural Information Processing Systems}, vol.~35, pp.~35811--35824, 2022.

\bibitem{bianchi2023safety}
F.~Bianchi, M.~Suzgun, G.~Attanasio, P.~R{\"o}ttger, D.~Jurafsky, T.~Hashimoto, and J.~Zou, ``Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions,'' {\em arXiv preprint arXiv:2309.07875}, 2023.

\bibitem{schick2021self}
T.~Schick, S.~Udupa, and H.~Sch{\"u}tze, ``Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp,'' {\em Transactions of the Association for Computational Linguistics}, vol.~9, pp.~1408--1424, 2021.

\bibitem{leong2023self}
C.~T. Leong, Y.~Cheng, J.~Wang, J.~Wang, and W.~Li, ``Self-detoxifying language models via toxification reversal,'' {\em arXiv preprint arXiv:2310.09573}, 2023.

\bibitem{meade2023using}
N.~Meade, S.~Gella, D.~Hazarika, P.~Gupta, D.~Jin, S.~Reddy, Y.~Liu, and D.~Hakkani-T{\"u}r, ``Using in-context learning to improve dialogue safety,'' {\em arXiv preprint arXiv:2302.00871}, 2023.

\bibitem{som2023demonstrations}
A.~Som, K.~Sikka, H.~Gent, A.~Divakaran, A.~Kathol, and D.~Vergyri, ``Demonstrations are all you need: Advancing offensive content paraphrasing using in-context learning,'' {\em arXiv preprint arXiv:2310.10707}, 2023.

\bibitem{conneau2020unsupervised}
A.~Conneau, K.~Khandelwal, N.~Goyal, V.~Chaudhary, G.~Wenzek, F.~Guzm{\'a}n, {\'E}.~Grave, M.~Ott, L.~Zettlemoyer, and V.~Stoyanov, ``Unsupervised cross-lingual representation learning at scale,'' in {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.~8440--8451, 2020.

\bibitem{RileyCGKUP20}
P.~Riley, N.~Constant, M.~Guo, G.~Kumar, D.~C. Uthus, and Z.~Parekh, ``Textsettr: Few-shot text style extraction and tunable targeted restyling,'' in {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021} (C.~Zong, F.~Xia, W.~Li, and R.~Navigli, eds.), pp.~3786--3800, Association for Computational Linguistics, 2021.

\bibitem{zhang2015character}
X.~Zhang, J.~Zhao, and Y.~LeCun, ``Character-level convolutional networks for text classification,'' {\em Advances in neural information processing systems}, vol.~28, 2015.

\bibitem{liu2022robust}
S.~Liu, Z.~Zhu, Q.~Qu, and C.~You, ``Robust training under label noise by over-parameterization,'' in {\em International Conference on Machine Learning}, pp.~14153--14172, PMLR, 2022.

\bibitem{reif2022recipe}
E.~Reif, D.~Ippolito, A.~Yuan, A.~Coenen, C.~Callison-Burch, and J.~Wei, ``A recipe for arbitrary text style transfer with large language models,'' in {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.~837--848, 2022.

\bibitem{KrishnaNGST22}
K.~Krishna, D.~Nathani, X.~Garcia, B.~Samanta, and P.~Talukdar, ``Few-shot controllable style transfer for low-resource multilingual settings,'' in {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland, May 22-27, 2022} (S.~Muresan, P.~Nakov, and A.~Villavicencio, eds.), pp.~7439--7468, Association for Computational Linguistics, 2022.

\end{thebibliography}
