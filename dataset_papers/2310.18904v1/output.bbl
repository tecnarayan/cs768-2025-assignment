\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ash et~al.(2021)Ash, Goel, Krishnamurthy, and
  Misra]{ash2021investigating}
Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra.
\newblock Investigating the role of negatives in contrastive representation
  learning.
\newblock \emph{arXiv preprint arXiv:2106.09943}, 2021.

\bibitem[Bao et~al.(2021)Bao, Nagano, and Nozawa]{bao2021sharp}
Han Bao, Yoshihiro Nagano, and Kento Nozawa.
\newblock Sharp learning bounds for contrastive unsupervised representation
  learning.
\newblock \emph{arXiv preprint arXiv:2110.02501}, 2021.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{swav}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020.

\bibitem[Chen and He(2021)]{simsiam}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Chen et~al.(2021)Chen, Xie, and He]{mocov3}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock \emph{arXiv preprint arXiv:2104.02057}, 2021.

\bibitem[Cui et~al.(2023)Cui, Huang, Wang, and Wang]{cui2023rethinking}
Jingyi Cui, Weiran Huang, Yifei Wang, and Yisen Wang.
\newblock Rethinking weak supervision in helping contrastive learning.
\newblock In \emph{ICML}, 2023.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Ding et~al.(2006)Ding, Li, Peng, and Park]{ding2006orthogonal}
Chris Ding, Tao Li, Wei Peng, and Haesun Park.
\newblock Orthogonal nonnegative matrix t-factorizations for clustering.
\newblock In \emph{SIGKDD}, 2006.

\bibitem[Dittadi et~al.(2020)Dittadi, Tr{\"a}uble, Locatello, W{\"u}thrich,
  Agrawal, Winther, Bauer, and Sch{\"o}lkopf]{dittadi2020transfer}
Andrea Dittadi, Frederik Tr{\"a}uble, Francesco Locatello, Manuel W{\"u}thrich,
  Vaibhav Agrawal, Ole Winther, Stefan Bauer, and Bernhard Sch{\"o}lkopf.
\newblock On the transfer of disentangled representations in realistic
  settings.
\newblock \emph{arXiv preprint arXiv:2010.14407}, 2020.

\bibitem[Dwibedi et~al.(2021)Dwibedi, Aytar, Tompson, Sermanet, and
  Zisserman]{nnclr}
Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew
  Zisserman.
\newblock With a little help from my friends: Nearest-neighbor contrastive
  learning of visual representations.
\newblock In \emph{ICCV}, 2021.

\bibitem[Eckart and Young(1936)]{eckart1936approximation}
Carl Eckart and Gale Young.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.

\bibitem[Geirhos et~al.(2018)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{geirhos2018imagenet}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A
  Wichmann, and Wieland Brendel.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape
  bias improves accuracy and robustness.
\newblock \emph{arXiv preprint arXiv:1811.12231}, 2018.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{rotation}
Spyros Gidaris, Praveer Singh, and Nikos Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In \emph{ICLR}, 2018.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{BYOL}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, C.~Tallec, Pierre~H.
  Richemond, Elena Buchatskaya, C.~Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, B.~Piot, K.~Kavukcuoglu,
  R{\'e}mi Munos, and Michal Valko.
\newblock Bootstrap your own latent: a new approach to self-supervised
  learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Guo et~al.(2023)Guo, Wang, Du, and Wang]{guo2023contranorm}
Xiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang.
\newblock Contranorm: A contrastive learning perspective on oversmoothing and
  beyond.
\newblock In \emph{ICLR}, 2023.

\bibitem[HaoChen and Ma(2022)]{haochen2022theoretical}
Jeff~Z HaoChen and Tengyu Ma.
\newblock A theoretical study of inductive biases in contrastive learning.
\newblock \emph{arXiv preprint arXiv:2211.14699}, 2022.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen}
Jeff~Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{mae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock \emph{arXiv preprint arXiv:2111.06377}, 2021.

\bibitem[Hyvarinen and Morioka(2016)]{hyvarinen2016unsupervised}
Aapo Hyvarinen and Hiroshi Morioka.
\newblock Unsupervised feature extraction by time-contrastive learning and
  nonlinear ica.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Jing et~al.(2022)Jing, Vincent, LeCun, and
  Tian]{jing2021understanding}
Li~Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Johansson et~al.(2016)Johansson, Shalit, and
  Sontag]{johansson2016learning}
Fredrik Johansson, Uri Shalit, and David Sontag.
\newblock Learning representations for counterfactual inference.
\newblock In \emph{ICML}, 2016.

\bibitem[Kalantidis et~al.(2020)Kalantidis, Sariyildiz, Pion, Weinzaepfel, and
  Larlus]{kalantidis2020hard}
Yannis Kalantidis, Mert~Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and
  Diane Larlus.
\newblock Hard negative mixing for contrastive learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Locatello et~al.(2019)Locatello, Abbati, Rainforth, Bauer,
  Sch{\"o}lkopf, and Bachem]{locatello2019fairness}
Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard
  Sch{\"o}lkopf, and Olivier Bachem.
\newblock On the fairness of disentangled representations.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{InfoNCE}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Roeder et~al.(2021)Roeder, Metz, and Kingma]{roeder2021linear}
Geoffrey Roeder, Luke Metz, and Durk Kingma.
\newblock On linear identifiability of learned representations.
\newblock In \emph{ICML}, 2021.

\bibitem[Saunshi et~al.(2019)Saunshi, Plevrakis, Arora, Khodak, and
  Khandeparkar]{arora}
Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and
  Hrishikesh Khandeparkar.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In \emph{ICML}, 2019.

\bibitem[Saunshi et~al.(2022)Saunshi, Ash, Goel, Misra, Zhang, Arora, Kakade,
  and Krishnamurthy]{saunshi2022understanding}
Nikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev
  Arora, Sham Kakade, and Akshay Krishnamurthy.
\newblock Understanding contrastive learning requires incorporating inductive
  biases.
\newblock In \emph{ICML}, 2022.

\bibitem[Wang and Isola(2020)]{wang2020understanding}
Tongzhou Wang and Phillip Isola.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{ICML}, 2020.

\bibitem[Wang et~al.(2021)Wang, Geng, Jiang, Li, Wang, Yang, and
  Lin]{wang2021residual}
Yifei Wang, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang,
  and Zhouchen Lin.
\newblock Residual relaxation for multi-view representation learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Wang et~al.(2022)Wang, Zhang, Wang, Yang, and Lin]{wang2022chaos}
Yifei Wang, Qi~Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin.
\newblock Chaos is a ladder: A new theoretical understanding of contrastive
  learning via augmentation overlap.
\newblock In \emph{ICLR}, 2022.

\bibitem[Wang et~al.(2023)Wang, Zhang, Du, Yang, Lin, and
  Wang]{wang2023message}
Yifei Wang, Qi~Zhang, Tianqi Du, Jiansheng Yang, Zhouchen Lin, and Yisen Wang.
\newblock A message passing perspective on learning dynamics of contrastive
  learning.
\newblock In \emph{ICLR}, 2023.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{barlowtwins}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Wang, and Wang]{zhang2022mask}
Qi~Zhang, Yifei Wang, and Yisen Wang.
\newblock How mask matters: Towards theoretical understandings of masked
  autoencoders.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Wang, and Wang]{zhang2023on}
Qi~Zhang, Yifei Wang, and Yisen Wang.
\newblock On the generalization of multi-modal contrastive learning.
\newblock In \emph{ICML}, 2023.

\bibitem[Zhuo et~al.(2023)Zhuo, Wang, Ma, and Wang]{zhuo2023towards}
Zhijian Zhuo, Yifei Wang, Jinwen Ma, and Yisen Wang.
\newblock Towards a unified theoretical understanding of non-contrastive
  learning via rank differential mechanism.
\newblock In \emph{ICLR}, 2023.

\end{thebibliography}
