\begin{thebibliography}{10}

\bibitem{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock {Understanding deep learning requires rethinking generalization}.
\newblock In {\em ICLR}, 2017.

\bibitem{arpit2017closer}
D.~Arpit, S.~Jastrzebski, M.S. Kanwal, T.~Maharaj, A.~Fischer, A.~Courville,
  and Y.~Bengio.
\newblock {A Closer Look at Memorization in Deep Networks}.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, 2017.

\bibitem{geman1992neural}
S.~Geman, E.~Bienenstock, and R.~Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock {\em Neural computation}, 4(1):1--58, 1992.

\bibitem{Neyshabur2015}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock {In search of the real inductive bias: On the role of implicit
  regularization in deep learning}.
\newblock In {\em ICLR}, 2015.

\bibitem{spigler2018jamming}
S~Spigler, M~Geiger, S~d’Ascoli, L~Sagun, G~Biroli, and M~Wyart.
\newblock A jamming transition from under-to over-parametrization affects
  generalization in deep learning.
\newblock {\em Journal of Physics A: Mathematical and Theoretical},
  52(47):474001, 2019.

\bibitem{nakkiran2020deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{gunasekar2018characterizing}
S.~Gunasekar, J.~Lee, D.~Soudry, and N.~Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In {\em International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018.

\bibitem{gunasekar2018implicit}
S.~Gunasekar, J.~D. Lee, D.~Soudry, and N.~Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem{Soudry2018}
D.~Soudry, E.~Hoffer, and N.~Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{ji2019implicit}
Z.~Ji and M.~Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In {\em Conference on Learning Theory}, pages 1772--1798. PMLR, 2019.

\bibitem{arora2019implicit}
S.~Arora, N.~Cohen, W.~Hu, and Y.~Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{chizat2020implicit}
L.~Chizat and F.~Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In {\em Conference on Learning Theory}, pages 1305--1338. PMLR, 2020.

\bibitem{advani2020highdimensional}
M.S. Advani, A.M. Saxe, and H.~Sompolinsky.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em Neural Networks}, 132:428 -- 446, 2020.

\bibitem{neal2018modern}
B.~Neal, S.~Mittal, A.~Baratin, V.~Tantia, M.~Scicluna, S.~Lacoste-Julien, and
  I.~Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock {\em arXiv preprint arXiv:1810.08591}, 2018.

\bibitem{mei2019generalization}
S.~Mei and A.~Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock {\em Communications on Pure and Applied Mathematics}, 2019.

\bibitem{belkin2019reconciling}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem{hastie2022surprises}
T.~Hastie, A.~Montanari, S.~Rosset, and R.J. Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em The Annals of Statistics}, 50(2):949--986, 2022.

\bibitem{dascoli2020double}
S.~d'Ascoli, M.~Refinetti, G.~Biroli, and F.~Krzakala.
\newblock Double trouble in double descent : Bias and variance(s) in the lazy
  regime.
\newblock In {\em ICML}, 2020.

\bibitem{adlam2020understanding}
B.~Adlam and J.~Pennington.
\newblock Understanding double descent requires a fine-grained bias-variance
  decomposition.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 11022--11032. Curran Associates, Inc., 2020.

\bibitem{lin2021causes}
L.~Lin and E.~Dobriban.
\newblock What causes the test error? going beyond bias-variance via anova.
\newblock {\em J. Mach. Learn. Res.}, 22:155--1, 2021.

\bibitem{geiger2020scaling}
M.~Geiger, A.~Jacot, S.~Spigler, F.~Gabriel, L.~Sagun, S.~d’Ascoli,
  G.~Biroli, C.~Hongler, and M.~Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(2):023401, 2020.

\bibitem{bartlett2020benign}
P.L. Bartlett, P.M. Long, G.~Lugosi, and A.~Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30063--30070, 2020.

\bibitem{liu2020bad}
S.~Liu, D.~Papailiopoulos, and D.~Achlioptas.
\newblock Bad global minima exist and sgd can reach them.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 8543--8552. Curran Associates, Inc., 2020.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf, 2009.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em CVPR09}, 2009.

\bibitem{lecun1998}
Y.~LeCun and C.~Cortes.
\newblock {The MNIST database of handwritten digits}, 1998.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hastie01statisticallearning}
T.~Hastie, R.~Tibshirani, and J.~Friedman.
\newblock {\em The Elements of Statistical Learning}.
\newblock Springer Series in Statistics. Springer New York Inc., New York, NY,
  USA, 2001.

\bibitem{bengio2013representation}
Y.~Bengio, A.~Courville, and P.~Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  35(8):1798--1828, 2013.

\bibitem{ansuini2019intrinsic}
A.~Ansuini, A.~Laio, J.~H. Macke, and D.~Zoccolan.
\newblock Intrinsic dimension of data representations in deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6109--6119, 2019.

\bibitem{hestness2017deep}
J.~Hestness, S.~Narang, N.~Ardalani, G.~Diamos, H.~Jun, H.~Kianinejad,
  M.~Patwary, M.~Ali, Y.~Yang, and Y.~Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv:1712.00409}, 2017.

\bibitem{rosenfeld2020constructive}
J.S. Rosenfeld, A.~Rosenfeld, Y.~Belinkov, and N.~Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.B. Brown, B.~Chess, R.~Child, S.~Gray,
  A.~Radford, J.~Wu, and D.~Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{henighan2020scaling}
T.~Henighan, J.~Kaplan, M.~Katz, M.~Chen, C.~Hesse, J.~Jackson, H.~Jun, T.B.
  Brown, P.~Dhariwal, S.~Gray, et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock {\em arXiv preprint arXiv:2010.14701}, 2020.

\bibitem{bahri2021explaining}
Y.~Bahri, E.~Dyer, J.~Kaplan, J.~Lee, and U.~Sharma.
\newblock Explaining neural scaling laws.
\newblock {\em arXiv preprint arXiv:2102.06701}, 2021.

\bibitem{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  8571--8580, 2018.

\bibitem{du2018gradient}
S.~Du, J.~Lee, Y.~Tian, A.~Singh, and B.~Poczos.
\newblock Gradient descent learns one-hidden-layer {CNN}: Don’t be afraid of
  spurious local minima.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pages 1339--1348, 2018.

\bibitem{chizat2019lazy}
L.~Chizat, E.~Oyallon, and F.~Bach.
\newblock On lazy training in differentiable programming.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2937--2947, 2019.

\bibitem{geiger2020disentangling}
M.~Geiger, S.~Spigler, A.~Jacot, and M.~Wyart.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(11):113301, 2020.

\bibitem{Mei2018}
S.~Mei, A.~Montanari, and P.~Nguyen.
\newblock {A mean field view of the landscape of two-layer neural networks}.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671, 2018.

\bibitem{rotskoff2018interacting}
G.M. Rotskoff and E.~Vanden-Eijnden.
\newblock {Parameters as interacting particles: long time convergence and
  asymptotic error scaling of neural networks}.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  7146--7155, 2018.

\bibitem{Chizat2018}
L.~Chizat and F.~Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  3040--3050, 2018.

\bibitem{Sirignano2018}
J.~Sirignano and K.~Spiliopoulos.
\newblock {Mean field analysis of neural networks: A central limit theorem}.
\newblock {\em Stochastic Processes and their Applications}, 2019.

\bibitem{goldt2019dynamics}
S.~Goldt, M.S. Advani, A.M. Saxe, F.~Krzakala, and L.~Zdeborov{\'a}.
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock In {\em Advances in Neural Information Processing Systems 32}, 2019.

\bibitem{refinetti2021classifying}
M.~Refinetti, S.~Goldt, F.~Krzakala, and L.~Zdeborova.
\newblock Classifying high-dimensional gaussian mixtures: Where kernel methods
  fail and neural networks succeed.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 8936--8947. PMLR, 18--24 Jul 2021.

\bibitem{kuditipudi2019explaining}
R.~Kuditipudi, X.~Wang, H.~Lee, Y.~Zhang, Z.~Li, W.~Hu, R.~Ge, and S.~Arora.
\newblock Explaining landscape connectivity of low-cost solutions for
  multilayer nets.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{shevchenko2020landscape}
A.~Shevchenko and M.~Mondelli.
\newblock Landscape connectivity and dropout stability of sgd solutions for
  over-parameterized neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  8773--8784. PMLR, 2020.

\bibitem{nguyen2021connectivity}
Q.~Nguyen, P.~Brechet, and M.~Mondelli.
\newblock On connectivity of solutions in deep learning: The role of
  over-parameterization and feature quality.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34. Curran Associates, Inc., 2021.

\bibitem{hinton2012improving}
G.~E. Hinton, N.~Srivastava, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em arXiv:1207.0580}, 2012.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{micikevicius2018mixed}
P.~Micikevicius, S.~Narang, J.~Alben, G.~Diamos, E.~Elsen, D.~Garcia,
  B.~Ginsburg, M.~Houston, O.~Kuchaiev, G.~Venkatesh, and H.~Wu.
\newblock Mixed precision training.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{bello2021revisiting}
I.~Bello, W.~Fedus, X.~Du, E.~D. Cubuk, A.~Srinivas, T.-Y. Lin, J.~Shlens, and
  B.~Zoph.
\newblock Revisiting resnets: Improved training and scaling strategies.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 22614--22627. Curran Associates, Inc., 2021.

\bibitem{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock {\em arXiv preprint arXiv:1706.02677}, 2017.

\bibitem{yang2020rethinking}
Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi~Ma.
\newblock Rethinking bias-variance trade-off for generalization of neural
  networks.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 10767--10777. PMLR, 13--18
  Jul 2020.

\end{thebibliography}
