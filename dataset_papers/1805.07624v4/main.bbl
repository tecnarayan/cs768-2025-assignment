\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andersen et~al.(1969)Andersen, Gross, Lomo, and Sveen]{sveen}
Andersen, P., Gross, G.~N., Lomo, T., and Sveen, O.
\newblock {{P}articipation of inhibitory and excitatory interneurones in the
  control of hippocampal cortical output}.
\newblock \emph{UCLA Forum Med Sci}, 1969.

\bibitem[Ba \& Caruana(2014)Ba and Caruana]{dodeep}
Ba, J. and Caruana, R.
\newblock Do deep nets really need to be deep?
\newblock In \emph{Proc. NIPS}. 2014.

\bibitem[Chatzis(2018)]{Chatzis18}
Chatzis, S.
\newblock Indian buffet process deep generative models for semi-supervised
  classification.
\newblock In \emph{IEEE ICASSP}, 2018.

\bibitem[Dai et~al.(2018)Dai, Zhu, Guo, and Wipf]{dai18d}
Dai, B., Zhu, C., Guo, B., and Wipf, D.
\newblock Compressing neural networks using the variational information
  bottleneck.
\newblock In \emph{Proc. ICML}, 2018.

\bibitem[Douglas \& Martin(2004)Douglas and Martin]{Douglas}
Douglas, R.~J. and Martin, K.~A.
\newblock {{N}euronal circuits of the neocortex}.
\newblock \emph{Annu. Rev. Neurosci.}, 27, 2004.

\bibitem[Eccles et~al.(1967)Eccles, Szentagothai, and Ito]{eccles}
Eccles, J.~C., Szentagothai, J., and Ito, M.
\newblock \emph{The cerebellum as a neuronal machine}.
\newblock Springer-Verlag, 1967.

\bibitem[Gal \& Ghahramani(2015)Gal and Ghahramani]{Gal2015DropoutB}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock \emph{arXiv:1506.02142}, 2015.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Warde-Farley, Mirza, Courville, and
  Bengio]{maxout}
Goodfellow, I.~J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y.
\newblock Maxout networks.
\newblock In \emph{Proc. ICML}, 2013.

\bibitem[Graves(2011)]{Graves2011}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Proc. NIPS}, 2011.

\bibitem[Griffiths \& Ghahramani(2005)Griffiths and Ghahramani]{Griffiths05}
Griffiths, T.~L. and Ghahramani, Z.
\newblock Infinite latent feature models and the indian buffet process.
\newblock In \emph{Proc. NIPS}, 2005.

\bibitem[Grossberg(1988)]{grossberg}
Grossberg, S.
\newblock The art of adaptive pattern recognition by a self-organizing neural
  network.
\newblock \emph{Computer}, 1988.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{Distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{NIPS Deep Learning and Representation Learning Workshop},
  2015.

\bibitem[Ishwaran \& James(2001)Ishwaran and James]{Ishwaran2001}
Ishwaran, H. and James, L.~F.
\newblock Gibbs sampling methods for stick-breaking priors.
\newblock \emph{Journal of the American Statistical Association}, 2001.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{Jang2017}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization using gumbel-softmax.
\newblock In \emph{Proc. ICLR}, 2017.

\bibitem[Kandel et~al.(1991)Kandel, Schwartz, and
  Jessell]{principles_neural_science}
Kandel, E.~R., Schwartz, J.~H., and Jessell, T.~M. (eds.).
\newblock \emph{Principles of Neural Science}.
\newblock Third edition, 1991.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{Krizhevsky}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Kumaraswamy(1980)]{Kumaraswamy1980}
Kumaraswamy, P.
\newblock A generalized probability density function for double-bounded random
  processes.
\newblock \emph{Journal of Hydrology}, \penalty0 (1), 1980.

\bibitem[Lansner(2009)]{lasner}
Lansner, A.
\newblock Associative memory models: from the cell-assembly theory to
  biophysically detailed cortex simulations.
\newblock \emph{Trends in Neurosciences}, 32\penalty0 (3), 2009.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 2015.

\bibitem[Lee \& Seung(1999)Lee and Seung]{lee99}
Lee, D.~D. and Seung, H.~S.
\newblock Learning the parts of objects by nonnegative matrix factorization.
\newblock \emph{Nature}, 401, 1999.

\bibitem[Louizos et~al.(2017)Louizos, Ullrich, and Welling]{Welling}
Louizos, C., Ullrich, K., and Welling, M.
\newblock Bayesian compression for deep learning.
\newblock In \emph{Proc. NIPS}, 2017.

\bibitem[Maass(1999)]{Maass:1999}
Maass, W.
\newblock Neural computation with winner-take-all as the only nonlinear
  operation.
\newblock In \emph{Proc. NIPS}, 1999.

\bibitem[Maass(2000)]{Maass:2000}
Maass, W.
\newblock {{O}n the computational power of winner-take-all}.
\newblock \emph{Neural Comput}, Nov 2000.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{Maddison2017}
Maddison, C.~J., Mnih, A., and Teh, Y.~W.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock In \emph{Proc. ICLR}, 2017.

\bibitem[McCloskey \& Cohen(1989)McCloskey and Cohen]{mccloskey}
McCloskey, M. and Cohen, N.~J.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock Psychology of Learning and Motivation. 1989.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and Vetrov]{Molchanov}
Molchanov, D., Ashukha, A., and Vetrov, D.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proc. ICML}, 2017.

\bibitem[Nalisnick \& Smyth(2016)Nalisnick and Smyth]{nali}
Nalisnick, E. and Smyth, P.
\newblock Stick-breaking variational autoencoders.
\newblock In \emph{Proc. ICLR}, 2016.

\bibitem[Neklyudov et~al.(2017)Neklyudov, Molchanov, Ashukha, and
  Vetrov]{Neklyudov}
Neklyudov, K., Molchanov, D., Ashukha, A., and Vetrov, D.~P.
\newblock Structured bayesian pruning via log-normal multiplicative noise.
\newblock In \emph{Proc. NIPS}. 2017.

\bibitem[Olshausen \& Field(1996)Olshausen and Field]{ols96}
Olshausen, B.~A. and Field, D.~J.
\newblock {{E}mergence of simple-cell receptive field properties by learning a
  sparse code for natural images}.
\newblock \emph{Nature}, 1996.

\bibitem[Srivastava et~al.(2013)Srivastava, Masci, Kazerounian, Gomez, and
  Schmidhuber]{Compete}
Srivastava, R.~K., Masci, J., Kazerounian, S., Gomez, F., and Schmidhuber, J.
\newblock Compete to compute.
\newblock In \emph{Proc. NIPS}. Curran Associates, Inc., 2013.

\bibitem[Stefanis(1969)]{Stefanis}
Stefanis, C.
\newblock {{I}nterneuronal mechanisms in the cortex}.
\newblock \emph{UCLA Forum Med Sci}, 1969.

\bibitem[Teh et~al.(2007)Teh, {G\"or\"ur}, and Ghahramani]{TehGorGha2007}
Teh, Y.~W., {G\"or\"ur}, D., and Ghahramani, Z.
\newblock Stick-breaking construction for the {I}ndian buffet process.
\newblock In \emph{Proc. AISTATS}, 2007.

\bibitem[Theodoridis(2015)]{theo}
Theodoridis, S.
\newblock \emph{Machine Learning: A Bayesian and Optimization Perspective}.
\newblock Academic Press, 2015.

\bibitem[Zuras et~al.(2008)]{ieee754}
Zuras, D. et~al.
\newblock {IEEE Standard for Floating-Point Arithmetic}.
\newblock Technical report, IEEE, August 2008.

\end{thebibliography}
