\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al{-}Shedivat et~al.(2018)Al{-}Shedivat, Bansal, Burda, Sutskever,
  Mordatch, and Abbeel]{alshedivat2018mpg}
Al{-}Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mordatch, I., and
  Abbeel, P.
\newblock Continuous adaptation via meta-learning in nonstationary and
  competitive environments.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net, 2018.
\newblock URL \url{https://openreview.net/forum?id=Sk2u1g-0-}.

\bibitem[Axelrod \& Hamilton(1981)Axelrod and Hamilton]{Axelrod84}
Axelrod, R. and Hamilton, W.~D.
\newblock The evolution of cooperation.
\newblock \emph{Science}, 211\penalty0 (4489):\penalty0 1390--1396, 1981.

\bibitem[Barhate(2021)]{pytorch_minimal_ppo}
Barhate, N.
\newblock Minimal pytorch implementation of proximal policy optimization.
\newblock \url{https://github.com/nikhilbarhate99/PPO-PyTorch}, 2021.

\bibitem[Camerer et~al.(2003)Camerer, Ho, and
  Chong]{camerer2003cognitivehierarchies}
Camerer, C., Ho, T., and Chong, J.-K.
\newblock A cognitive hierarchy theory of one-shot games and experimental
  analysis.
\newblock \emph{SSRN Electronic Journal}, 09 2003.
\newblock \doi{10.2139/ssrn.411061}.

\bibitem[Chakraborty \& Stone(2014)Chakraborty and
  Stone]{chakraborty2014multiagent}
Chakraborty, D. and Stone, P.
\newblock Multiagent learning in the presence of memory-bounded agents.
\newblock \emph{Autonomous agents and multi-agent systems}, 28\penalty0
  (2):\penalty0 182--213, 2014.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017maml}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of \emph{Proceedings of Machine Learning Research}, pp.\
   1126--1135, 2017.

\bibitem[Foerster et~al.(2018{\natexlab{a}})Foerster, Chen, Al-Shedivat,
  Whiteson, Abbeel, and Mordatch]{foerster_learning_2018}
Foerster, J., Chen, R.~Y., Al-Shedivat, M., Whiteson, S., Abbeel, P., and
  Mordatch, I.
\newblock Learning with opponent-learning awareness.
\newblock In \emph{Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, pp.\  122--130,
  2018{\natexlab{a}}.

\bibitem[Foerster et~al.(2018{\natexlab{b}})Foerster, Farquhar, Al{-}Shedivat,
  Rockt{\"{a}}schel, Xing, and Whiteson]{foerster2018dice}
Foerster, J.~N., Farquhar, G., Al{-}Shedivat, M., Rockt{\"{a}}schel, T., Xing,
  E.~P., and Whiteson, S.
\newblock Dice: The infinitely differentiable monte carlo estimator.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1524--1533. {PMLR},
  2018{\natexlab{b}}.
\newblock URL \url{http://proceedings.mlr.press/v80/foerster18a.html}.

\bibitem[Harper et~al.(2017)Harper, Knight, Jones, Koutsovoulos, Glynatsi, and
  Campbell]{Harper_2017}
Harper, M., Knight, V., Jones, M., Koutsovoulos, G., Glynatsi, N.~E., and
  Campbell, O.
\newblock Reinforcement learning produces dominant strategies for the iterated
  prisoner’s dilemma.
\newblock \emph{PLOS ONE}, 12\penalty0 (12):\penalty0 e0188046, 2017.

\bibitem[Harsanyi et~al.(1988)Harsanyi, Selten, et~al.]{harsanyi1988general}
Harsanyi, J.~C., Selten, R., et~al.
\newblock A general theory of equilibrium selection in games.
\newblock \emph{MIT Press Books}, 1, 1988.

\bibitem[Kim et~al.(2021)Kim, Liu, Riemer, Sun, Abdulhai, Habibi, Lopez{-}Cot,
  Tesauro, and How]{kim2021meta-mapg}
Kim, D., Liu, M., Riemer, M., Sun, C., Abdulhai, M., Habibi, G., Lopez{-}Cot,
  S., Tesauro, G., and How, J.~P.
\newblock A policy gradient algorithm for learning to learn in multiagent
  reinforcement learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pp.\  5541--5550, 2021.

\bibitem[Kuhn(1953)]{kuhn1953_posg}
Kuhn, H.~W.
\newblock \emph{Extensive games and the problem of information}.
\newblock Princeton University Press, Princeton, NJ, 1953.

\bibitem[Lerer \& Peysakhovich(2017)Lerer and
  Peysakhovich]{lerer_2017_coingame}
Lerer, A. and Peysakhovich, A.
\newblock Maintaining cooperation in complex social dilemmas using deep
  reinforcement learning.
\newblock \emph{CoRR}, abs/1707.01068, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.01068}.

\bibitem[Letcher et~al.(2019{\natexlab{a}})Letcher, Balduzzi, Racani{\`{e}}re,
  Martens, Foerster, Tuyls, and Graepel]{letcher_differentiable_2019}
Letcher, A., Balduzzi, D., Racani{\`{e}}re, S., Martens, J., Foerster, J.~N.,
  Tuyls, K., and Graepel, T.
\newblock Differentiable game mechanics.
\newblock \emph{J. Mach. Learn. Res.}, 20:\penalty0 84:1--84:40,
  2019{\natexlab{a}}.

\bibitem[Letcher et~al.(2019{\natexlab{b}})Letcher, Foerster, Balduzzi,
  Rockt{\"{a}}schel, and Whiteson]{letcher_stable_2019}
Letcher, A., Foerster, J.~N., Balduzzi, D., Rockt{\"{a}}schel, T., and
  Whiteson, S.
\newblock Stable opponent shaping in differentiable games.
\newblock In \emph{7th International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Mealing \& Shapiro(2017)Mealing and Shapiro]{mealing2017opponent}
Mealing, R. and Shapiro, J.~L.
\newblock Opponent modeling by expectation-maximization and sequence prediction
  in simplified poker.
\newblock \emph{{IEEE} Trans. Comput. Intell. {AI} Games}, 9\penalty0
  (1):\penalty0 11--24, 2017.

\bibitem[Oliehoek \& Amato(2016)Oliehoek and Amato]{oliehoek_concise_2016}
Oliehoek, F.~A. and Amato, C.
\newblock \emph{A {Concise} {Introduction} to {Decentralized} {POMDPs}}.
\newblock {SpringerBriefs} in {Intelligent} {Systems}. Springer International
  Publishing, 2016.
\newblock ISBN 978-3-319-28927-4.
\newblock \doi{10.1007/978-3-319-28929-8}.
\newblock URL \url{https://www.springer.com/gp/book/9783319289274}.

\bibitem[Press \& Dyson(2012)Press and Dyson]{press_iterated_2012}
Press, W.~H. and Dyson, F.~J.
\newblock Iterated {Prisoner}’s {Dilemma} contains strategies that dominate
  any evolutionary opponent.
\newblock \emph{Proceedings of the National Academy of Sciences}, 109\penalty0
  (26):\penalty0 10409--10413, 2012.
\newblock ISSN 0027-8424.

\bibitem[Raileanu et~al.(2018)Raileanu, Denton, Szlam, and
  Fergus]{raileanu2018modelingothers}
Raileanu, R., Denton, E., Szlam, A., and Fergus, R.
\newblock Modeling others using oneself in multi-agent reinforcement learning.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4254--4263. {PMLR},
  2018.
\newblock URL \url{http://proceedings.mlr.press/v80/raileanu18a.html}.

\bibitem[Rapoport \& Chammah(1966)Rapoport and Chammah]{rapoport1966chicken}
Rapoport, A. and Chammah, A.~M.
\newblock The game of chicken.
\newblock \emph{American Behavioral Scientist}, 10\penalty0 (3):\penalty0
  10--28, 1966.
\newblock \doi{10.1177/000276426601000303}.
\newblock URL \url{https://doi.org/10.1177/000276426601000303}.

\bibitem[Rapoport et~al.(1965)Rapoport, Chammah, and
  Orwant]{rapoport1965prisoner}
Rapoport, A., Chammah, A.~M., and Orwant, C.~J.
\newblock \emph{Prisoner's dilemma: A study in conflict and cooperation},
  volume 165.
\newblock University of Michigan press, 1965.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock arXiv preprint arXiv:1707.06347, 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{silver2017go}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T.~P.,
  Hui, F., Sifre, L., van~den Driessche, G., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nat.}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Such et~al.(2017)Such, Madhavan, Conti, Lehman, Stanley, and
  Clune]{such2017deepneuroevolution}
Such, F.~P., Madhavan, V., Conti, E., Lehman, J., Stanley, K.~O., and Clune, J.
\newblock Deep neuroevolution: Genetic algorithms are a competitive alternative
  for training deep neural networks for reinforcement learning.
\newblock arXiv preprint arXiv:1712.06567, 2017.

\bibitem[Synnaeve \& Bessi{\`{e}}re(2011)Synnaeve and
  Bessi{\`{e}}re]{synnaeve2011bayesian}
Synnaeve, G. and Bessi{\`{e}}re, P.
\newblock A bayesian model for opening prediction in {RTS} games with
  application to starcraft.
\newblock In \emph{{IEEE} Conference on Computational Intelligence and Games},
  pp.\  281--288, 2011.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang,
  Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard,
  Budden, Sulsky, Molloy, Paine, G{\"{u}}l{\c{c}}ehre, Wang, Pfaff, Wu, Ring,
  Yogatama, W{\"{u}}nsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu,
  Hassabis, Apps, and Silver]{vinyals2019alphastar}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D.,
  Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J.~P.,
  Jaderberg, M., Vezhnevets, A.~S., Leblond, R., Pohlen, T., Dalibard, V.,
  Budden, D., Sulsky, Y., Molloy, J., Paine, T.~L., G{\"{u}}l{\c{c}}ehre,
  {\c{C}}., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., W{\"{u}}nsch,
  D., McKinney, K., Smith, O., Schaul, T., Lillicrap, T.~P., Kavukcuoglu, K.,
  Hassabis, D., Apps, C., and Silver, D.
\newblock Grandmaster level in starcraft {II} using multi-agent reinforcement
  learning.
\newblock \emph{Nat.}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Weber \& Mateas(2009)Weber and Mateas]{weber2009datamining}
Weber, B.~G. and Mateas, M.
\newblock A data mining approach to strategy prediction.
\newblock In \emph{Proceedings of the 2009 {IEEE} Symposium on Computational
  Intelligence and Games}, pp.\  140--147, 2009.

\bibitem[Wen et~al.(2019)Wen, Yang, Luo, Wang, and Pan]{wen2019probabilistic}
Wen, Y., Yang, Y., Luo, R., Wang, J., and Pan, W.
\newblock Probabilistic recursive reasoning for multi-agent reinforcement
  learning.
\newblock In \emph{7th International Conference on Learning Representations},
  2019.

\bibitem[Willi et~al.(2022)Willi, Treutlein, Letcher, and
  Foerster]{willi2022cola}
Willi, T., Treutlein, J., Letcher, A., and Foerster, J.
\newblock {COLA:} consistent learning with opponent-learning awareness.
\newblock 2022.

\bibitem[Wu et~al.(2021)Wu, Li, Zhao, Xu, Zhang, Fu, An, and Xing]{wu2021l2e}
Wu, Z., Li, K., Zhao, E., Xu, H., Zhang, M., Fu, H., An, B., and Xing, J.
\newblock L2e: Learning to exploit your opponent, 2021.

\bibitem[Xie et~al.(2020)Xie, Losey, Tolsma, Finn, and
  Sadigh]{xie2020learninglatent}
Xie, A., Losey, D.~P., Tolsma, R., Finn, C., and Sadigh, D.
\newblock Learning latent representations to influence multi-agent interaction.
\newblock In \emph{4th Conference on Robot Learning}, volume 155 of
  \emph{Proceedings of Machine Learning Research}, pp.\  575--588, 2020.

\bibitem[Xiong et~al.(2021)Xiong, Zintgraf, Beck, Vuorio, and
  Whiteson]{xiong_2021_meta_consistency}
Xiong, Z., Zintgraf, L.~M., Beck, J., Vuorio, R., and Whiteson, S.
\newblock On the practical consistency of meta-reinforcement learning
  algorithms.
\newblock \emph{CoRR}, abs/2112.00478, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.00478}.

\bibitem[Yu et~al.(2021)Yu, Jiang, Jiang, and Lu]{yu2021mbom}
Yu, X., Jiang, J., Jiang, H., and Lu, Z.
\newblock Model-based opponent modeling.
\newblock arXiv preprint arXiv:2108.01843, 2021.

\bibitem[Zhmoginov et~al.(2022)Zhmoginov, Sandler, and
  Vladymyrov]{zhmoginov2022hypertransformer}
Zhmoginov, A., Sandler, M., and Vladymyrov, M.
\newblock Hypertransformer: Model generation for supervised and semi-supervised
  few-shot learning.
\newblock arXiv preprint arXiv:2201.04182, 2022.

\end{thebibliography}
