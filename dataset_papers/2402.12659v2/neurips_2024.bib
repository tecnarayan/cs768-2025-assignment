@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}
@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}
@article{xie2023wall,
  title={The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges},
  author={Xie, Qianqian and Han, Weiguang and Lai, Yanzhao and Peng, Min and Huang, Jimin},
  journal={arXiv preprint arXiv:2304.05351},
  year={2023}
}
@article{lopez2023can,
  title={Can chatgpt forecast stock price movements? return predictability and large language models},
  author={Lopez-Lira, Alejandro and Tang, Yuehua},
  journal={arXiv preprint arXiv:2304.07619},
  year={2023}
}
@article{xie2023pixiu,
  title={PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance},
  author={Xie, Qianqian and Han, Weiguang and Zhang, Xiao and Lai, Yanzhao and Peng, Min and Lopez-Lira, Alejandro and Huang, Jimin},
  journal={arXiv preprint arXiv:2306.05443},
  year={2023}
}
@article{li2023chatgpt,
  title={Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks},
  author={Li, Xianzhi and Zhu, Xiaodan and Ma, Zhiqiang and Liu, Xiaomo and Shah, Sameena},
  journal={arXiv preprint arXiv:2305.05862},
  year={2023}
}
@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@inproceedings{shah2022flue,
  title={When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain},
  author={Shah, Raj and Chawla, Kunal and Eidnani, Dheeraj and Shah, Agam and Du, Wendi and Chava, Sudheer and Raman, Natraj and Smiley, Charese and Chen, Jiaao and Yang, Diyi},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={2322--2335},
  year={2022}
}
@article{lu2023bbt,
  title={BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark},
  author={Lu, Dakuan and Liang, Jiaqing and Xu, Yipei and He, Qianyu and Geng, Yipeng and Han, Mengkun and Xin, Yingsi and Wu, Hengkui and Xiao, Yanghua},
  journal={arXiv preprint arXiv:2302.09432},
  year={2023}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@misc{huang2023memory,
      title={Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents}, 
      author={Ziheng Huang and Sebastian Gutierrez and Hemanth Kamana and Stephen MacNeil},
      year={2023},
      eprint={2308.01542},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{liffiton2023codehelp,
      title={CodeHelp: Using Large Language Models with Guardrails for Scalable Support in Programming Classes}, 
      author={Mark Liffiton and Brad Sheese and Jaromir Savelka and Paul Denny},
      year={2023},
      eprint={2308.06921},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@misc{park2022social,
      title={Social Simulacra: Creating Populated Prototypes for Social Computing Systems}, 
      author={Joon Sung Park and Lindsay Popowski and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
      year={2022},
      eprint={2208.04024},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}
@misc{sumers2023cognitive,
      title={Cognitive Architectures for Language Agents}, 
      author={Theodore R. Sumers and Shunyu Yao and Karthik Narasimhan and Thomas L. Griffiths},
      year={2023},
      eprint={2309.02427},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@misc{wang2023survey,
      title={A Survey on Large Language Model based Autonomous Agents}, 
      author={Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen},
      year={2023},
      eprint={2308.11432},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}
@misc{wang2023survey,
      title={A Survey on Large Language Model based Autonomous Agents}, 
      author={Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen},
      year={2023},
      eprint={2308.11432},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{Wang2023RecAgentAN,
  title={RecAgent: A Novel Simulation Paradigm for Recommender Systems},
  author={Lei Wang and Jingsen Zhang and Xu Chen and Yankai Lin and Ruihua Song and Wayne Xin Zhao and Ji-rong Wen},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.02552},
  url={https://api.semanticscholar.org/CorpusID:263888378}
}
@misc{wu2023bloomberggpt,
      title={BloombergGPT: A Large Language Model for Finance}, 
      author={Shijie Wu and Ozan Irsoy and Steven Lu and Vadim Dabravolski and Mark Dredze and Sebastian Gehrmann and Prabhanjan Kambadur and David Rosenberg and Gideon Mann},
      year={2023},
      eprint={2303.17564},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{wang2023fingpt,
      title={FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets}, 
      author={Neng Wang and Hongyang Yang and Christina Dan Wang},
      year={2023},
      eprint={2310.04793},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{Sorensen_2022,
   title={An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels},
   url={http://dx.doi.org/10.18653/v1/2022.acl-long.60},
   DOI={10.18653/v1/2022.acl-long.60},
   booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
   publisher={Association for Computational Linguistics},
   author={Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher and Shaw, Alexander and Rogers, Kyle and Delorey, Alexia and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
   year={2022} }
@misc{yu2023finmem,
      title={FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and Character Design}, 
      author={Yangyang Yu and Haohang Li and Zhi Chen and Yuechen Jiang and Yang Li and Denghui Zhang and Rong Liu and Jordan W. Suchow and Khaldoun Khashanah},
      year={2023},
      eprint={2311.13743},
      archivePrefix={arXiv},
      primaryClass={q-fin.CP}
}
@article{schneider2012cattell,
  title={The Cattell-Horn-Carroll model of intelligence.},
  author={Schneider, W Joel and McGrew, Kevin S},
  year={2012},
  publisher={The Guilford Press}
}
@misc{mcgrew2009chc,
  title={CHC theory and the human cognitive abilities project: Standing on the shoulders of the giants of psychometric intelligence research},
  author={McGrew, Kevin S},
  journal={Intelligence},
  volume={37},
  number={1},
  pages={1--10},
  year={2009},
  publisher={Elsevier}
}
@misc{malo2013good,
      title={Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts}, 
      author={Pekka Malo and Ankur Sinha and Pyry Takala and Pekka Korhonen and Jyrki Wallenius},
      year={2013},
      eprint={1307.5336},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{zhang2023instruction,
      title={Instruction Tuning for Large Language Models: A Survey}, 
      author={Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
      year={2023},
      eprint={2308.10792},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yang2023investlm,
      title={InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning}, 
      author={Yi Yang and Yixuan Tang and Kar Yan Tam},
      year={2023},
      eprint={2309.13064},
      archivePrefix={arXiv},
      primaryClass={q-fin.GN}
}

@misc{zhang2023xuanyuan,
      title={XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters}, 
      author={Xuanyu Zhang and Qing Yang and Dongliang Xu},
      year={2023},
      eprint={2305.12002},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{hongyuan-etal-2023-finbart,
    title = "{F}in{BART}: A Pre-trained Seq2seq Language Model for {C}hinese Financial Tasks",
    author = "Hongyuan, Dong  and
      Wanxiang, Che  and
      Xiaoyu, He  and
      Guidong, Zheng  and
      Junjie, Wen",
    editor = "Sun, Maosong  and
      Qin, Bing  and
      Qiu, Xipeng  and
      Jiang, Jing  and
      Han, Xianpei",
    booktitle = "Proceedings of the 22nd Chinese National Conference on Computational Linguistics",
    month = aug,
    year = "2023",
    address = "Harbin, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2023.ccl-1.77",
    pages = "906--917",
    abstract = "{``}Pretrained language models are making a more profound impact on our lives than ever before. They exhibit promising performance on a variety of general domain Natural Language Process-ing (NLP) tasks. However, few work focuses on Chinese financial NLP tasks, which comprisea significant portion of social communication. To this end, we propose FinBART, a pretrainedseq2seq language model for Chinese financial communication tasks. Experiments show thatFinBART outperforms baseline models on a series of downstream tasks including text classifica-tion, sequence labeling and text generation. We further pretrain the model on customer servicecorpora, and results show that our model outperforms baseline models and achieves promisingperformance on various real world customer service text mining tasks.{''}",
    language = "English",
}
@misc{zhang2023cgce,
      title={CGCE: A Chinese Generative Chat Evaluation Benchmark for General and Financial Domains}, 
      author={Xuanyu Zhang and Bingbing Li and Qing Yang},
      year={2023},
      eprint={2305.14471},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lei2023cfbenchmark,
      title={CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model}, 
      author={Yang Lei and Jiangtong Li and Ming Jiang and Junjie Hu and Dawei Cheng and Zhijun Ding and Changjun Jiang},
      year={2023},
      eprint={2311.05812},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023large,
      title={Large Language Models in Finance: A Survey}, 
      author={Yinheng Li and Shaofei Wang and Han Ding and Hang Chen},
      year={2023},
      eprint={2311.10723},
      archivePrefix={arXiv},
      primaryClass={q-fin.GN}
}	
	
@misc{araci2019finbert,
      title={FinBERT: Financial Sentiment Analysis with Pre-trained Language Models}, 
      author={Dogu Araci},
      year={2019},
      eprint={1908.10063},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yang2020finbert,
      title={FinBERT: A Pretrained Language Model for Financial Communications}, 
      author={Yi Yang and Mark Christopher Siy UY and Allen Huang},
      year={2020},
      eprint={2006.08097},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{ijcai2020p622,
  title     = {FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining},
  author    = {Liu, Zhuang and Huang, Degen and Huang, Kaiyu and Li, Zhuang and Zhao, Jun},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {4513--4519},
  year      = {2020},
  month     = {7},
  note      = {Special Track on AI in FinTech},
  doi       = {10.24963/ijcai.2020/622},
  url       = {https://doi.org/10.24963/ijcai.2020/622},
}
@article{li05210v,
  author={Liu, W. and Ye, T. and Jägermeyr, J. and Müller, C. and Chen, S. and Liu, X. and Shi, P.},
  title={Future climate change significantly alters interannual wheat yield variability over half of harvested areas},
  year={2021},
  journal={Environ. Res. Lett.},
  volume={16},
  number={9},
  pages={094045},
  doi={10.1088/1748-9326/ac1fbb},
}

@misc{clark2020electra,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lee2024survey,
      title={A Survey of Large Language Models in Finance (FinLLMs)}, 
      author={Jean Lee and Nicholas Stevens and Soyeon Caren Han and Minseok Song},
      year={2024},
      eprint={2402.02315},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{maia2018www,
author = {Maia, Macedo and Handschuh, Siegfried and Freitas, Andre and Davis, Brian and McDermott, Ross and Zarrouk, Manel and Balahur, Alexandra},
year = {2018},
month = {04},
pages = {1941-1942},
title = {WWW'18 Open Challenge: Financial Opinion Mining and Question Answering},
isbn = {9781450356404},
journal = {WWW '18: Companion Proceedings of the The Web Conference 2018},
doi = {10.1145/3184558.3192301}
}
@inproceedings{cortis-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News",
    author = "Cortis, Keith  and
      Freitas, Andr{\'e}  and
      Daudert, Tobias  and
      Huerlimann, Manuela  and
      Zarrouk, Manel  and
      Handschuh, Siegfried  and
      Davis, Brian",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S17-2089",
    doi = "10.18653/v1/S17-2089",
    pages = "519--535",
    abstract = "This paper discusses the {``}Fine-Grained Sentiment Analysis on Financial Microblogs and News{''} task as part of SemEval-2017, specifically under the {``}Detecting sentiment, humour, and truth{''} theme. This task contains two tracks, where the first one concerns Microblog messages and the second one covers News Statements and Headlines. The main goal behind both tracks was to predict the sentiment score for each of the mentioned companies/stocks. The sentiment scores for each text instance adopted floating point values in the range of -1 (very negative/bearish) to 1 (very positive/bullish), with 0 designating neutral sentiment. This task attracted a total of 32 participants, with 25 participating in Track 1 and 29 in Track 2.",
}
@misc{sinha2020impact,
      title={Impact of News on the Commodity Market: Dataset and Results}, 
      author={Ankur Sinha and Tanmay Khandait},
      year={2020},
      eprint={2009.04202},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{salinas-alvarado-etal-2015-domain,
    title = "Domain Adaption of Named Entity Recognition to Support Credit Risk Assessment",
    author = "Salinas Alvarado, Julio Cesar  and
      Verspoor, Karin  and
      Baldwin, Timothy",
    editor = "Hachey, Ben  and
      Webster, Kellie",
    booktitle = "Proceedings of the Australasian Language Technology Association Workshop 2015",
    month = dec,
    year = "2015",
    address = "Parramatta, Australia",
    url = "https://aclanthology.org/U15-1010",
    pages = "84--90",
}
@inproceedings{loukas-etal-2022-finer,
    title = "{F}i{NER}: Financial Numeric Entity Recognition for {XBRL} Tagging",
    author = "Loukas, Lefteris  and
      Fergadiotis, Manos  and
      Chalkidis, Ilias, et al.",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.303",
    doi = "10.18653/v1/2022.acl-long.303",
    pages = "4419--4431",
    abstract = "Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139, a dataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction datasets, FiNER-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms BERT{'}s performance, allowing word-level BILSTMs to perform better. To improve BERT{'}s performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes. We also experiment with FIN-BERT, an existing BERT model for the financial domain, and release our own BERT (SEC-BERT), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on XBRL tagging.",
}
@misc{sharma2023finred,
      title={FinRED: A Dataset for Relation Extraction in Financial Domain}, 
      author={Soumya Sharma and Tapas Nayak and Arusarka Bose and Ajay Kumar Meena and Koustuv Dasgupta and Niloy Ganguly and Pawan Goyal},
      year={2023},
      eprint={2306.03736},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{mariko-etal-2020-financial,
    title = "The Financial Document Causality Detection Shared Task ({F}in{C}ausal 2020)",
    author = "Mariko, Dominique  and
      Abi-Akl, Hanna  and
      Labidurie, Estelle  and
      Durfort, Stephane  and
      De Mazancourt, Hugues  and
      El-Haj, Mahmoud",
    editor = "El-Haj, Dr Mahmoud  and
      Athanasakou, Dr Vasiliki, et al.",
    booktitle = "Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "COLING",
    url = "https://aclanthology.org/2020.fnp-1.3",
    pages = "23--32",
    abstract = "We present the FinCausal 2020 Shared Task on Causality Detection in Financial Documents and the associated FinCausal dataset, and discuss the participating systems and results. Two sub-tasks are proposed: a binary classification task (Task 1) and a relation extraction task (Task 2). A total of 16 teams submitted runs across the two Tasks and 13 of them contributed with a system description paper. This workshop is associated to the Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation (FNP-FNS 2020), held at The 28th International Conference on Computational Linguistics (COLING{'}2020), Barcelona, Spain on September 12, 2020.",
}
@inproceedings{loukas-etal-2022-finer,
    title = "{F}i{NER}: Financial Numeric Entity Recognition for {XBRL} Tagging",
    author = "Loukas, Lefteris  and
      Fergadiotis, Manos  and
      Chalkidis, Ilias  and
      Spyropoulou, Eirini  and
      Malakasiotis, Prodromos  and
      Androutsopoulos, Ion  and
      Paliouras, Georgios",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.303",
    doi = "10.18653/v1/2022.acl-long.303",
    pages = "4419--4431",
    abstract = "Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139, a dataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction datasets, FiNER-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms BERT{'}s performance, allowing word-level BILSTMs to perform better. To improve BERT{'}s performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes. We also experiment with FIN-BERT, an existing BERT model for the financial domain, and release our own BERT (SEC-BERT), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on XBRL tagging.",
}
@misc{chen2022finqa,
      title={FinQA: A Dataset of Numerical Reasoning over Financial Data}, 
      author={Zhiyu Chen and Wenhu Chen and Charese Smiley and Sameena Shah, et al.},
      year={2022},
      eprint={2109.00122},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chen2022convfinqa,
      title={ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering}, 
      author={Zhiyu Chen and Shiyang Li and Charese Smiley and Zhiqiang Ma and Sameena Shah and William Yang Wang},
      year={2022},
      eprint={2210.03849},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{CIKM2020MAEC,
author = {Li, Jiazheng and Yang, Linyi and Smyth, Barry and Dong, Ruihai},
title = {MAEC: A Multimodal Aligned Earnings Conference Call Dataset for Financial Risk Prediction},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412879},
doi = {10.1145/3340531.3412879},
abstract = {In the area of natural language processing, various financial datasets have informed recent research and analysis including financial news, financial reports, social media, and audio data from earnings calls. We introduce a new, large-scale multi-modal, text-audio paired, earnings-call dataset named MAEC, based on S&amp;P 1500 companies. We describe the main features of MAEC, how it was collected and assembled, paying particular attention to the text-audio alignment process used. We present the approach used in this work as providing a suitable framework for processing similar forms of data in the future. The resulting dataset is more than six times larger than those currently available to the research community and we discuss its potential in terms of current and future research challenges and opportunities. All resources of this work are available at https://github.com/Earnings-Call-Dataset/},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3063–3070},
numpages = {8},
keywords = {multimodal aligned datasets, earnings conference calls, financial risk prediction},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}
@misc{gerz2021multilingual,
      title={Multilingual and Cross-Lingual Intent Detection from Spoken Data}, 
      author={Daniela Gerz and Pei-Hao Su and Razvan Kusztos and Avishek Mondal and Michał Lis and Eshan Singhal and Nikola Mrkšić and Tsung-Hsien Wen and Ivan Vulić},
      year={2021},
      eprint={2104.08524},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{jorgensen-etal-2023-multifin,
    title = "{M}ulti{F}in: A Dataset for Multilingual Financial {NLP}",
    author = "J{\o}rgensen, Rasmus  and
      Brandt, Oliver  and
      Hartmann, Mareike  and
      Dai, Xiang  and
      Igel, Christian  and
      Elliott, Desmond",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.66",
    doi = "10.18653/v1/2023.findings-eacl.66",
    pages = "894--909",
    abstract = "Financial information is generated and distributed across the world, resulting in a vast amount of domain-specific multilingual data. Multilingual models adapted to the financial domain would ease deployment when an organization needs to work with multiple languages on a regular basis. For the development and evaluation of such models, there is a need for multilingual financial language processing datasets. We describe MultiFin {--} a publicly available financial dataset consisting of real-world article headlines covering 15 languages across different writing systems and language families. The dataset consists of hierarchical label structure providing two classification tasks: multi-label and multi-class. We develop our annotation schema based on a real-world application and annotate our dataset using both {`}label by native-speaker{'} and {`}translate-then-label{'} approaches. The evaluation of several popular multilingual models, e.g., mBERT, XLM-R, and mT5, show that although decent accuracy can be achieved in high-resource languages, there is substantial room for improvement in low-resource languages.",
}
@inproceedings{CIKM2020MAEC,
author = {Li, Jiazheng and Yang, Linyi and Smyth, Barry and Dong, Ruihai},
title = {MAEC: A Multimodal Aligned Earnings Conference Call Dataset for Financial Risk Prediction},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412879},
doi = {10.1145/3340531.3412879},
abstract = {In the area of natural language processing, various financial datasets have informed recent research and analysis including financial news, financial reports, social media, and audio data from earnings calls. We introduce a new, large-scale multi-modal, text-audio paired, earnings-call dataset named MAEC, based on S&amp;P 1500 companies. We describe the main features of MAEC, how it was collected and assembled, paying particular attention to the text-audio alignment process used. We present the approach used in this work as providing a suitable framework for processing similar forms of data in the future. The resulting dataset is more than six times larger than those currently available to the research community and we discuss its potential in terms of current and future research challenges and opportunities. All resources of this work are available at https://github.com/Earnings-Call-Dataset/},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3063–3070},
numpages = {8},
keywords = {multimodal aligned datasets, earnings conference calls, financial risk prediction},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}
  @article{zheng2020global,
  title={Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context},
  author={Zheng, Xinyi and Burdick, Doug and Popa, Lucian and Zhong, Peter and Wang, Nancy Xin Ru},
  journal={Winter Conference for Applications in Computer Vision (WACV)},
  year={2021}
}

@misc{zhou2021trade,
      title={Trade the Event: Corporate Events Detection for News-Based Event-Driven Trading}, 
      author={Zhihan Zhou and Liqian Ma and Han Liu},
      year={2021},
      eprint={2105.12825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{lee2023stockemotions,
      title={StockEmotions: Discover Investor Emotions for Financial Sentiment Analysis and Multivariate Time Series}, 
      author={Jean Lee and Hoyoul Luis Youn and Josiah Poon and Soyeon Caren Han},
      year={2023},
      eprint={2301.09279},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2023cfgpt,
      title={CFGPT: Chinese Financial Assistant with Large Language Model}, 
      author={Jiangtong Li and Yuxuan Bian and Guoxuan Wang and Yang Lei and Dawei Cheng and Zhijun Ding and Changjun Jiang},
      year={2023},
      eprint={2309.10654},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{Mathur2022MONOPOLYFP,
  title={MONOPOLY: Financial Prediction from MONetary POLicY Conference Videos Using Multimodal Cues},
  author={Puneet Mathur and Atula Tejaswi Neerkaje and Malika Chhibber and Ramit Sawhney and Fuming Guo and Franck Dernoncourt and Sanghamitra Dutta and Dinesh Manocha},
  journal={Proceedings of the 30th ACM International Conference on Multimedia},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252782110}
}

%% baselines
@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  pages={13},
  year={2023}
}
@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu, et al.},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@article{baichuan2023baichuan2,
  title={Baichuan 2: Open Large-scale Language Models},
  author={Baichuan},
  journal={arXiv preprint arXiv:2309.10305},
  url={https://arxiv.org/abs/2309.10305},
  year={2023}
}
@misc{team2023internlm,
  title={Internlm: A multilingual language model with progressively enhanced capabilities},
  author={Team, InternLM},
  journal={2023-01-06)[2023-09-27]. https://github.com/InternLM/InternLM},
  year={2023}
}
@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}
@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}
@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{han2023select,
  title={Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning},
  author={Han, Weiguang and Zhang, Boyi and Xie, Qianqian and Peng, Min and Lai, Yanzhao and Huang, Jimin},
  journal={arXiv preprint arXiv:2301.10724},
  year={2023}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}
@inproceedings{sanh2022multitask,
  title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Le Scao, Teven and Raja, Arun and others},
  booktitle={ICLR 2022-Tenth International Conference on Learning Representations},
  year={2022}
}
@article{longpre2023flan,
  title={The flan collection: Designing data and methods for effective instruction tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@article{zhang2021mengzi,
  title={Mengzi: Towards lightweight yet ingenious pre-trained models for chinese},
  author={Zhang, Zhuosheng and Zhang, Hanqing and Chen, Keming and Guo, Yuhang and Hua, Jingyun and Wang, Yulong and Zhou, Ming},
  journal={arXiv preprint arXiv:2110.06696},
  year={2021}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}
@article{malo2014good,
  title={Good debt or bad debt: Detecting semantic orientations in economic texts},
  author={Malo, Pekka and Sinha, Ankur and Korhonen, Pekka and Wallenius, Jyrki and Takala, Pyry},
  journal={Journal of the Association for Information Science and Technology},
  volume={65},
  number={4},
  pages={782--796},
  year={2014},
  publisher={Wiley Online Library}
}

@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}
@inproceedings{sinha2021impact,
  title={Impact of news on the commodity market: Dataset and results},
  author={Sinha, Ankur and Khandait, Tanmay},
  booktitle={Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2},
  pages={589--601},
  year={2021},
  organization={Springer}
}
@inproceedings{alvarado2015domain,
  title={Domain adaption of named entity recognition to support credit risk assessment},
  author={Alvarado, Julio Cesar Salinas and Verspoor, Karin and Baldwin, Timothy},
  booktitle={Proceedings of the Australasian Language Technology Association Workshop 2015},
  pages={84--90},
  year={2015}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}
@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}
@inproceedings{chen2021finqa,
  title={FinQA: A Dataset of Numerical Reasoning over Financial Data},
  author={Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan R and others},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3697--3711},
  year={2021}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@inproceedings{gpt-neox-20b,
  title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
  url={https://arxiv.org/abs/2204.06745},
  year={2022}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={arXiv preprint arXiv:2206.14858},
  year={2022}
}
@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@inproceedings{soun2022accurate,
  title={Accurate Stock Movement Prediction with Self-supervised Learning from Sparse Noisy Tweets},
  author={Soun, Yejun and Yoo, Jaemin and Cho, Minyong and Jeon, Jihyeong and Kang, U},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)},
  pages={1691--1700},
  year={2022},
  organization={IEEE}
}
@inproceedings{xu2018stock,
  title={Stock movement prediction from tweets and historical prices},
  author={Xu, Yumo and Cohen, Shay B},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1970--1979},
  year={2018}
}
@inproceedings{wu2018hybrid,
  title={Hybrid deep sequential modeling for social text-driven stock prediction},
  author={Wu, Huizhe and Zhang, Wei and Shen, Weiwei and Wang, Jun},
  booktitle={Proceedings of the 27th ACM international conference on information and knowledge management},
  pages={1627--1630},
  year={2018}
}
@misc{BELLE,
  author = {Ji, Yunjie and Deng, Yong and Gong, Yan and Peng, Yiping and Niu, Qiang and Ma, Baochang and Li, Xiangang},
  title = {BELLE: Be Everyone's Large Language model Engine},
  year = {2023},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/LianjiaTech/BELLE}},
}
@article{belle2023exploring,
  title={Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases},
  author={Ji, Yunjie and Deng, Yong and Gong, Yan and Peng, Yiping and Niu, Qiang and Zhang, Lei and Ma, Baochang and Li, Xiangang},
  journal={arXiv preprint arXiv:2303.14742},
  year={2023}
}
@article{Han2023SelectAT,
  title={Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning},
  author={Weiguang Han and Boyi Zhang and Qianqian Xie and Min Peng and Yanzhao Lai and Jimin Huang},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.10724}
}

@article{feng2018enhancing,
  title={Enhancing stock movement prediction with adversarial training},
  author={Feng, Fuli and Chen, Huimin and He, Xiangnan and Ding, Ji and Sun, Maosong and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:1810.09936},
  year={2018}
}
@inproceedings{yoo2021accurate,
  title={Accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts},
  author={Yoo, Jaemin and Soun, Yejun and Park, Yong-chan and Kang, U},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={2037--2045},
  year={2021}
}

@inproceedings{cortis2017semeval,
  title={Semeval-2017 task 5: Fine-grained sentiment analysis on financial microblogs and news},
  author={Cortis, Keith and Freitas, Andr{\'e} and Daudert, Tobias and Huerlimann, Manuela and Zarrouk, Manel and Handschuh, Siegfried and Davis, Brian},
  booktitle={Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)},
  pages={519--535},
  year={2017}
}

@article{islam2023financebench,
  title={FinanceBench: A New Benchmark for Financial Question Answering},
  author={Islam, Pranab and Kannappan, Anand and Kiela, Douwe and Qian, Rebecca and Scherrer, Nino and Vidgen, Bertie},
  journal={arXiv preprint arXiv:2311.11944},
  year={2023}
}

@article{koncel2023bizbench,
  title={Bizbench: A quantitative reasoning benchmark for business and finance},
  author={Koncel-Kedziorski, Rik and Krumdick, Michael and Lai, Viet and Reddy, Varshini and Lovering, Charles and Tanner, Chris},
  journal={arXiv preprint arXiv:2311.06602},
  year={2023}
}

@inproceedings{dalal2023calm,
  title={CALM-Bench: A Multi-task Benchmark for Evaluating Causality-Aware Language Models},
  author={Dalal, Dhairya and Buitelaar, Paul and Arcan, Mihael},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2023},
  pages={296--311},
  year={2023}
}

@article{chen2023disc,
  title={Disc-finllm: A chinese financial large language model based on multiple experts fine-tuning},
  author={Chen, Wei and Wang, Qiushi and Long, Zefei and Zhang, Xianyin and Lu, Zhongtian and Li, Bingxuan and Wang, Siyuan and Xu, Jiarong and Bai, Xiang and Huang, Xuanjing and others},
  journal={arXiv preprint arXiv:2310.15205},
  year={2023}
}
@misc{team2023FinEva,
  title={Fin-Eva Version 1.0},
  author={Team, Fin-Eva},
  journal={https://github.com/alipay/financial_evaluation_dataset},
  year={2023}
}

@inproceedings{shah2023trillion,
    title = "Trillion Dollar Words: A New Financial Dataset, Task {\&} Market Analysis",
    author = "Shah, Agam  and
      Paturi, Suvan  and
      Chava, Sudheer",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.368",
    doi = "10.18653/v1/2023.acl-long.368",
    pages = "6664--6679",
    abstract = "Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.",
}
@inproceedings{sy2023fine,
  title={Fine-Grained Argument Understanding with BERT Ensemble Techniques: A Deep Dive into Financial Sentiment Analysis},
  author={Sy, Eugene and Peng, Tzu-Cheng and Huang, Shih-Hsuan and Lin, Heng-Yu and Chang, Yung-Chun},
  booktitle={Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023)},
  pages={242--249},
  year={2023}
}
@inproceedings{jorgensen2023multifin,
  title={MultiFin: A Dataset for Multilingual Financial NLP},
  author={J{\o}rgensen, Rasmus and Brandt, Oliver and Hartmann, Mareike and Dai, Xiang and Igel, Christian and Elliott, Desmond},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2023},
  pages={864--879},
  year={2023}
}
@article{yang2020generating,
  title={Generating plausible counterfactual explanations for deep transformers in financial text classification},
  author={Yang, Linyi and Kenny, Eoin M and Ng, Tin Lok James and Yang, Yi and Smyth, Barry and Dong, Ruihai},
  journal={arXiv preprint arXiv:2010.12512},
  year={2020}
}
@inproceedings{chen2023multi,
  title={Multi-Lingual ESG Issue Identification},
  author={Chen, Chung-Chi and Tseng, Yu-Min and Kang, Juyeon and Lhuissier, Ana{\"\i}s and Day, Min-Yuh and Tu, Teng-Tsai and Chen, Hsin-Hsi},
  booktitle={Proceedings of the Fifth Workshop on Financial Technology and Natural Language Processing and the Second Multimodal AI For Financial Forecasting},
  pages={111--115},
  year={2023}
}
@article{shah2023finer,
  title={Finer: Financial named entity recognition dataset and weak-supervision model},
  author={Shah, Agam and Vithani, Ruchit and Gullapalli, Abhinav and Chava, Sudheer},
  journal={arXiv preprint arXiv:2302.11157},
  year={2023}
}
@inproceedings{sharma2022finred,
  title={FinRED: A dataset for relation extraction in financial domain},
  author={Sharma, Soumya and Nayak, Tapas and Bose, Arusarka and Meena, Ajay Kumar and Dasgupta, Koustuv and Ganguly, Niloy and Goyal, Pawan},
  booktitle={Companion Proceedings of the Web Conference 2022},
  pages={595--597},
  year={2022}
}
@article{mariko2020financial,
  title={Financial document causality detection shared task (fincausal 2020)},
  author={Mariko, Dominique and Akl, Hanna Abi and Labidurie, Estelle and Durfort, Stephane and De Mazancourt, Hugues and El-Haj, Mahmoud},
  journal={arXiv preprint arXiv:2012.02505},
  year={2020}
}
@inproceedings{goutte2005probabilistic,
  title={A probabilistic interpretation of precision, recall and F-score, with implication for evaluation},
  author={Goutte, Cyril and Gaussier, Eric},
  booktitle={European conference on information retrieval},
  pages={345--359},
  year={2005},
  organization={Springer}
}
@inproceedings{derczynski2016complementarity,
    title = "Complementarity, {F}-score, and {NLP} Evaluation",
    author = "Derczynski, Leon",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1040",
    pages = "261--266",
    abstract = "This paper addresses the problem of quantifying the differences between entity extraction systems, where in general only a small proportion a document should be selected. Comparing overall accuracy is not very useful in these cases, as small differences in accuracy may correspond to huge differences in selections over the target minority class. Conventionally, one may use per-token complementarity to describe these differences, but it is not very useful when the set is heavily skewed. In such situations, which are common in information retrieval and entity recognition, metrics like precision and recall are typically used to describe performance. However, precision and recall fail to describe the differences between sets of objects selected by different decision strategies, instead just describing the proportional amount of correct and incorrect objects selected. This paper presents a method for measuring complementarity for precision, recall and F-score, quantifying the difference between entity extraction approaches.",
}
@article{zhu2021tat,
  title={TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance},
  author={Zhu, Fengbin and Lei, Wenqiang and Huang, Youcheng and Wang, Chao and Zhang, Shuo and Lv, Jiancheng and Feng, Fuli and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2105.07624},
  year={2021}
}
@inproceedings{sharma2023financial,
  title={Financial Numeric Extreme Labelling: A dataset and benchmarking},
  author={Sharma, Soumya and Khatuya, Subhendu and Hegde, Manjunath and Shaikh, Afreen and Dasgupta, Koustuv and Goyal, Pawan and Ganguly, Niloy},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={3550--3561},
  year={2023}
}
@article{lamm2018textual,
  title={Textual analogy parsing: What's shared and what's compared among analogous facts},
  author={Lamm, Matthew and Chaganty, Arun Tejasvi and Manning, Christopher D and Jurafsky, Dan and Liang, Percy},
  journal={arXiv preprint arXiv:1809.02700},
  year={2018}
}
@article{kim2023we,
  title={How are We Detecting Inconsistent Method Names? An Empirical Study from Code Review Perspective},
  author={Kim, Kisub and Zhou, Xin and Kim, Dongsun and Lawall, Julia and Liu, Kui and Bissyand{\'e}, Tegawend{\'e} F and Klein, Jacques and Lee, Jaekwon and Lo, David},
  journal={arXiv preprint arXiv:2308.12701},
  year={2023}
}
@article{mukherjee2022ectsum,
  title={Ectsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts},
  author={Mukherjee, Rajdeep and Bohra, Abhinav and Banerjee, Akash and Sharma, Soumya and Hegde, Manjunath and Shaikh, Afreen and Shrivastava, Shivani and Dasgupta, Koustuv and Ganguly, Niloy and Ghosh, Saptarshi and others},
  journal={arXiv preprint arXiv:2210.12467},
  year={2022}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}
@article{yuan2021bartscore,
  title={Bartscore: Evaluating generated text as text generation},
  author={Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27263--27277},
  year={2021}
}
@misc{misc_german_credit_data_144,
  author       = {Hofmann,Hans},
  title        = {{Statlog (German Credit Data)}},
  year         = {1994},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5NC77}
}
@misc{misc_australian_credit_approval_143,
  author       = {Quinlan,Ross},
  title        = {{Statlog (Australian Credit Approval)}},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C59012}
}
@article{feng2023empowering,
  title={Empowering many, biasing a few: Generalist credit scoring through large language models},
  author={Feng, Duanyu and Dai, Yongfu and Huang, Jimin and Zhang, Yifang and Xie, Qianqian and Han, Weiguang and Lopez-Lira, Alejandro and Wang, Hao},
  journal={arXiv preprint arXiv:2310.00566},
  year={2023}
}
@article{chicco2020advantages,
  title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
  author={Chicco, Davide and Jurman, Giuseppe},
  journal={BMC genomics},
  volume={21},
  number={1},
  pages={1--13},
  year={2020},
  publisher={BioMed Central}
}
@article{punt2017strategic,
  title={Strategic management decision-making in a complex world: quantifying, understanding, and using trade-offs},
  author={Punt, Andr{\'e} E},
  journal={ICES Journal of Marine Science},
  volume={74},
  number={2},
  pages={499--510},
  year={2017},
  publisher={Oxford University Press}
}
@article{ariel1987monthly,
  title={A monthly effect in stock returns},
  author={Ariel, Robert A},
  journal={Journal of financial economics},
  volume={18},
  number={1},
  pages={161--174},
  year={1987},
  publisher={Elsevier}
}
@article{sharpe1998sharpe,
  title={The sharpe ratio},
  author={Sharpe, William F},
  journal={Streetwise--the Best of the Journal of Portfolio Management},
  volume={3},
  pages={169--85},
  year={1998},
  publisher={Princeton University Press NJ}
}
@article{zhou2023forecasting,
  title={Forecasting the equity premium: Do deep neural network models work?},
  author={Zhou, Xianzheng and Zhou, Hui and Long, Huaigang},
  journal={Modern Finance},
  volume={1},
  number={1},
  pages={1--11},
  year={2023},
  publisher={Fundacja Naukowa Instytut Wsp{\'o}{\l}czesnych Finans{\'o}w}
}
@article{chapman2009tracking,
  title={Tracking the fin trade: genetic stock identification in western Atlantic scalloped hammerhead sharks Sphyrna lewini},
  author={Chapman, Demian D and Pinhal, Danillo and Shivji, Mahmood S},
  journal={Endangered Species Research},
  volume={9},
  number={3},
  pages={221--228},
  year={2009}
}
@article{magdon2004maximum,
  title={Maximum drawdown},
  author={Magdon-Ismail, Malik and Atiya, Amir F},
  journal={Risk Magazine},
  volume={17},
  number={10},
  pages={99--102},
  year={2004}
}
@misc{yang2023fingpt,
      title={FinGPT: Open-Source Financial Large Language Models}, 
      author={Hongyang Yang and Xiao-Yang Liu and Christina Dan Wang},
      year={2023},
      eprint={2306.06031},
      archivePrefix={arXiv},
      primaryClass={q-fin.ST}
}
@article{liu2023fingpt,
  title={Fingpt: Democratizing internet-scale data for financial large language models},
  author={Liu, Xiao-Yang and Wang, Guoxuan and Zha, Daochen},
  journal={arXiv preprint arXiv:2307.10485},
  year={2023}
}


@misc{liu2023dynamic,
      title={Dynamic Datasets and Market Environments for Financial Reinforcement Learning}, 
      author={Xiao-Yang Liu and Ziyi Xia and Hongyang Yang and Jiechao Gao and Daochen Zha and Ming Zhu and Christina Dan Wang and Zhaoran Wang and Jian Guo},
      year={2023},
      eprint={2304.13174},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{liu2022finrlmeta,
      title={FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning}, 
      author={Xiao-Yang Liu and Ziyi Xia and Jingyang Rui and Jiechao Gao and Hongyang Yang and Ming Zhu and Christina Dan Wang and Zhaoran Wang and Jian Guo},
      year={2022},
      eprint={2211.03107},
      archivePrefix={arXiv},
      primaryClass={q-fin.TR}
}
@misc{zhang2024dolares,
      title={D\'olares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English}, 
      author={Xiao Zhang and Ruoyu Xiang and Chenhan Yuan and Duanyu Feng and Weiguang Han and Alejandro Lopez-Lira and Xiao-Yang Liu and Sophia Ananiadou and Min Peng and Jimin Huang and Qianqian Xie},
      year={2024},
      eprint={2402.07405},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{dai2024laiw,
      title={LAiW: A Chinese Legal Large Language Models Benchmark}, 
      author={Yongfu Dai and Duanyu Feng and Jimin Huang and Haochen Jia and Qianqian Xie and Yifang Zhang and Weiguang Han and Wei Tian and Hao Wang},
      year={2024},
      eprint={2310.05620},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yuan2023future,
      title={Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models}, 
      author={Chenhan Yuan and Qianqian Xie and Jimin Huang and Sophia Ananiadou},
      year={2023},
      eprint={2310.01074},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{feng2024empowering,
      title={Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models}, 
      author={Duanyu Feng and Yongfu Dai and Jimin Huang and Yifang Zhang and Qianqian Xie and Weiguang Han and Zhengyu Chen and Alejandro Lopez-Lira and Hao Wang},
      year={2024},
      eprint={2310.00566},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{Han2023SelectAT,
  title={Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning},
  author={Weiguang Han and Boyi Zhang and Qianqian Xie and Min Peng and Yanzhao Lai and Jimin Huang},
  journal={Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256231505}
}
@misc{han2023mastering,
      title={Mastering Pair Trading with Risk-Aware Recurrent Reinforcement Learning}, 
      author={Weiguang Han and Jimin Huang and Qianqian Xie and Boyi Zhang and Yanzhao Lai and Min Peng},
      year={2023},
      eprint={2304.00364},
      archivePrefix={arXiv},
      primaryClass={q-fin.CP}
}
@article{cao2022ai,
  title={Ai in finance: challenges, techniques, and opportunities},
  author={Cao, Longbing},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={3},
  pages={1--38},
  year={2022},
  publisher={ACM New York, NY}
}
@article{zhao2024revolutionizing,
  title={Revolutionizing finance with llms: An overview of applications and insights},
  author={Zhao, Huaqin and Liu, Zhengliang and Wu, Zihao and Li, Yiwei and Yang, Tianze and Shu, Peng and Xu, Shaochen and Dai, Haixing and Zhao, Lin and Mai, Gengchen and others},
  journal={arXiv preprint arXiv:2401.11641},
  year={2024}
}
@book{costantino2008information,
  title={Information extraction in finance},
  author={Costantino, Marco and Coletti, Paolo},
  volume={8},
  year={2008},
  publisher={Wit Press}
}
@article{loughran2020textual,
  title={Textual analysis in finance},
  author={Loughran, Tim and McDonald, Bill},
  journal={Annual Review of Financial Economics},
  volume={12},
  pages={357--375},
  year={2020},
  publisher={Annual Reviews}
}
@inproceedings{la2020end,
  title={End-to-end training for financial report summarization},
  author={La Quatra, Moreno and Cagliero, Luca},
  booktitle={Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation},
  pages={118--123},
  year={2020}
}
@article{abu1996introduction,
  title={Introduction to financial forecasting},
  author={Abu-Mostafa, Yaser S and Atiya, Amir F},
  journal={Applied intelligence},
  volume={6},
  pages={205--213},
  year={1996},
  publisher={Springer}
}
@book{aziz2019machine,
  title={Machine learning and AI for risk management},
  author={Aziz, Saqib and Dowling, Michael},
  year={2019},
  publisher={Springer International Publishing}
}
@article{paiva2019decision,
  title={Decision-making for financial trading: A fusion approach of machine learning and portfolio selection},
  author={Paiva, Felipe Dias and Cardoso, Rodrigo Tom{\'a}s Nogueira and Hanaoka, Gustavo Peixoto and Duarte, Wendel Moreira},
  journal={Expert Systems with Applications},
  volume={115},
  pages={635--655},
  year={2019},
  publisher={Elsevier}
}
@misc{2308.09975,
Author = {Liwen Zhang and Weige Cai and Zhaowei Liu and Zhi Yang and Wei Dai and Yujie Liao and Qianru Qin and Yifei Li and Xingyu Liu and Zhiqiang Liu and Zhoufan Zhu and Anbo Wu and Xin Guo and Yun Chen},
Title = {FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models},
Year = {2023},
Eprint = {arXiv:2308.09975},
}

@article{touvron2023llama1,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

