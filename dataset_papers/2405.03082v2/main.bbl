\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abels et~al.(2019)Abels, Roijers, Lenaerts, Now{\'e}, and Steckelmacher]{abels2019dynamic}
Abels, A., Roijers, D., Lenaerts, T., Now{\'e}, A., and Steckelmacher, D.
\newblock Dynamic weights in multi-objective deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\  11--20. PMLR, 2019.

\bibitem[Alegre et~al.(2022)Alegre, Felten, Talbi, Danoy, Now{\'e}, Bazzan, and da~Silva]{Alegre+2022bnaic}
Alegre, L.~N., Felten, F., Talbi, E.-G., Danoy, G., Now{\'e}, A., Bazzan, A. L.~C., and da~Silva, B.~C.
\newblock {MO-Gym}: A library of multi-objective reinforcement learning environments.
\newblock In \emph{Proceedings of the 34th Benelux Conference on Artificial Intelligence BNAIC/Benelearn 2022}, 2022.

\bibitem[Barrett \& Narayanan(2008)Barrett and Narayanan]{barrett2008learning}
Barrett, L. and Narayanan, S.
\newblock Learning all optimal policies with multiple criteria.
\newblock In \emph{Proceedings of the 25th international conference on Machine learning}, pp.\  41--47, 2008.

\bibitem[Bhatia(2013)]{bhatia2013matrix}
Bhatia, R.
\newblock \emph{Matrix analysis}, volume 169.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Sutton, Ghavamzadeh, and Lee]{bhatnagar2009natural}
Bhatnagar, S., Sutton, R.~S., Ghavamzadeh, M., and Lee, M.
\newblock Natural actor--critic algorithms.
\newblock \emph{Automatica}, 45\penalty0 (11):\penalty0 2471--2482, 2009.

\bibitem[Cai et~al.(2023)Cai, Xue, Zhang, Xue, Liu, Zhan, Wang, Zuo, Xie, Zheng, et~al.]{CaiXueZha_23}
Cai, Q., Xue, Z., Zhang, C., Xue, W., Liu, S., Zhan, R., Wang, X., Zuo, T., Xie, W., Zheng, D., et~al.
\newblock Two-stage constrained actor-critic for short video recommendation.
\newblock In \emph{Proceedings of the ACM Web Conference 2023}, pp.\  865--875, 2023.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Du, Xia, and Wang]{CheDuXia_21}
Chen, X., Du, Y., Xia, L., and Wang, J.
\newblock Reinforcement recommendation with user multi-aspect preference.
\newblock In \emph{Proceedings of the Web Conference 2021}, pp.\  425--435, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Zhou, Chen, and Zou]{CheZhoChe_21}
Chen, Z., Zhou, Y., Chen, R., and Zou, S.
\newblock Sample and communication-efficient decentralized actor-critic algorithms with finite-time analysis.
\newblock \emph{arXiv preprint arXiv:2109.03699}, 2021{\natexlab{b}}.

\bibitem[Danilova et~al.(2022)Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, and Shibaev]{DanDvuGas_22}
Danilova, M., Dvurechensky, P., Gasnikov, A., Gorbunov, E., Guminov, S., Kamzolov, D., and Shibaev, I.
\newblock Recent theoretical advances in non-convex optimization.
\newblock In \emph{High-Dimensional Optimization and Probability: With a View Towards Data Science}, pp.\  79--163. Springer, 2022.

\bibitem[D{\'e}sid{\'e}ri(2012)]{Des_12}
D{\'e}sid{\'e}ri, J.-A.
\newblock Multiple-gradient descent algorithm (mgda) for multiobjective optimization.
\newblock \emph{Comptes Rendus Mathematique}, 350\penalty0 (5-6):\penalty0 313--318, 2012.

\bibitem[Doan et~al.(2019)Doan, Maguluri, and Romberg]{DoaMagRom_19}
Doan, T., Maguluri, S., and Romberg, J.
\newblock Finite-time analysis of distributed td (0) with linear function approximation on multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1626--1635. PMLR, 2019.

\bibitem[Doan et~al.(2018)Doan, Maguluri, and Romberg]{doan2018distributed}
Doan, T.~T., Maguluri, S.~T., and Romberg, J.
\newblock Distributed stochastic approximation for solving network optimization problems under random quantization.
\newblock \emph{arXiv preprint arXiv:1810.11568}, 2018.

\bibitem[Fernando et~al.(2022)Fernando, Shen, Liu, Chaudhury, Murugesan, and Chen]{FerSheLiu_22}
Fernando, H.~D., Shen, H., Liu, M., Chaudhury, S., Murugesan, K., and Chen, T.
\newblock Mitigating gradient bias in multi-objective learning: A provably convergent approach.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Fliege et~al.(2019)Fliege, Vaz, and Vicente]{FliVazVic_19}
Fliege, J., Vaz, A. I.~F., and Vicente, L.~N.
\newblock Complexity of gradient descent for multiobjective optimization.
\newblock \emph{Optimization Methods and Software}, 34\penalty0 (5):\penalty0 949--959, 2019.

\bibitem[G{\'a}bor et~al.(1998)G{\'a}bor, Kalm{\'a}r, and Szepesv{\'a}ri]{gabor1998multi}
G{\'a}bor, Z., Kalm{\'a}r, Z., and Szepesv{\'a}ri, C.
\newblock Multi-criteria reinforcement learning.
\newblock In \emph{ICML}, volume~98, pp.\  197--205, 1998.

\bibitem[Ge et~al.(2022)Ge, Zhao, Yu, Paul, Hu, Hsieh, and Zhang]{GeZhaYu_22}
Ge, Y., Zhao, X., Yu, L., Paul, S., Hu, D., Hsieh, C.-C., and Zhang, Y.
\newblock Toward pareto efficient fairness-utility trade-off in recommendation through reinforcement learning.
\newblock In \emph{Proceedings of the fifteenth ACM international conference on web search and data mining}, pp.\  316--324, 2022.

\bibitem[Grondman et~al.(2012)Grondman, Busoniu, Lopes, and Babuska]{grondman2012survey}
Grondman, I., Busoniu, L., Lopes, G.~A., and Babuska, R.
\newblock A survey of actor-critic reinforcement learning: Standard and natural policy gradients.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 42\penalty0 (6):\penalty0 1291--1307, 2012.

\bibitem[Guo et~al.(2021)Guo, Hu, and Zhang]{GuoHuZha_21}
Guo, X., Hu, A., and Zhang, J.
\newblock Theoretical guarantees of fictitious discount algorithms for episodic reinforcement learning and global convergence of policy gradient methods.
\newblock \emph{arXiv preprint arXiv:2109.06362}, 2021.

\bibitem[Hairi et~al.(2022)Hairi, Liu, and Lu]{HaiLiuLu_22}
Hairi, F., Liu, J., and Lu, S.
\newblock Finite-time convergence and sample complexity of multi-agent actor-critic reinforcement learning with average reward.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{KonTsi_99}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Kumar et~al.(2019)Kumar, Koppel, and Ribeiro]{kumar2019sample}
Kumar, H., Koppel, A., and Ribeiro, A.
\newblock On the sample complexity of actor-critic for reinforcement learning.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Lakshminarayanan \& Szepesvari(2018)Lakshminarayanan and Szepesvari]{LakSze_18}
Lakshminarayanan, C. and Szepesvari, C.
\newblock Linear stochastic approximation: How far does constant step-size and iterate averaging go?
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  1347--1355. PMLR, 2018.

\bibitem[Levin \& Peres(2017)Levin and Peres]{levin2017markov}
Levin, D.~A. and Peres, Y.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0 (1):\penalty0 1334--1373, 2016.

\bibitem[Liu \& Vicente(2021)Liu and Vicente]{liu2021stochastic}
Liu, S. and Vicente, L.~N.
\newblock The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning.
\newblock \emph{Annals of Operations Research}, pp.\  1--30, 2021.

\bibitem[Mao et~al.(2016)Mao, Alizadeh, Menache, and Kandula]{mao2016resource}
Mao, H., Alizadeh, M., Menache, I., and Kandula, S.
\newblock Resource management with deep reinforcement learning.
\newblock In \emph{the 15th ACM Workshop on Hot Topics in Networks}, pp.\  50--56, 2016.

\bibitem[Miettinen(1999)]{Mie_99}
Miettinen, K.
\newblock \emph{Nonlinear multiobjective optimization}, volume~12.
\newblock Springer Science \& Business Media, 1999.

\bibitem[Petersen et~al.(2019)Petersen, Yang, Grathwohl, Cockrell, Santiago, An, and Faissol]{petersen2019deep}
Petersen, B.~K., Yang, J., Grathwohl, W.~S., Cockrell, C., Santiago, C., An, G., and Faissol, D.~M.
\newblock Deep reinforcement learning and simulation as a path toward precision medicine.
\newblock \emph{Journal of Computational Biology}, 26\penalty0 (6):\penalty0 597--604, 2019.

\bibitem[Qiu et~al.(2021)Qiu, Yang, Ye, and Wang]{QiuYanYe_21}
Qiu, S., Yang, Z., Ye, J., and Wang, Z.
\newblock On finite-time convergence of actor-critic algorithm.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory}, 2\penalty0 (2):\penalty0 652--664, 2021.

\bibitem[Raghu et~al.(2017{\natexlab{a}})Raghu, Komorowski, Ahmed, Celi, Szolovits, and Ghassemi]{raghu2017deep}
Raghu, A., Komorowski, M., Ahmed, I., Celi, L., Szolovits, P., and Ghassemi, M.
\newblock Deep reinforcement learning for sepsis treatment.
\newblock \emph{arXiv preprint arXiv:1711.09602}, 2017{\natexlab{a}}.

\bibitem[Raghu et~al.(2017{\natexlab{b}})Raghu, Komorowski, Celi, Szolovits, and Ghassemi]{raghu2017continuous}
Raghu, A., Komorowski, M., Celi, L.~A., Szolovits, P., and Ghassemi, M.
\newblock Continuous state-space models for optimal sepsis treatment: a deep reinforcement learning approach.
\newblock In \emph{Machine Learning for Healthcare Conference}, pp.\  147--163. PMLR, 2017{\natexlab{b}}.

\bibitem[Roijers et~al.(2018)Roijers, Steckelmacher, and Now{\'e}]{roijers2018multi}
Roijers, D.~M., Steckelmacher, D., and Now{\'e}, A.
\newblock Multi-objective reinforcement learning for the expected utility of the return.
\newblock In \emph{Proceedings of the Adaptive and Learning Agents workshop at FAIM}, volume 2018, 2018.

\bibitem[Sener \& Koltun(2018)Sener and Koltun]{SenKol_18}
Sener, O. and Koltun, V.
\newblock Multi-task learning as multi-objective optimization.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Srikant \& Ying(2019)Srikant and Ying]{srikant2019finite}
Srikant, R. and Ying, L.
\newblock Finite-time error bounds for linear stochastic approximation andtd learning.
\newblock In \emph{Conference on Learning Theory}, pp.\  2803--2830. PMLR, 2019.

\bibitem[Stamenkovic et~al.(2022)Stamenkovic, Karatzoglou, Arapakis, Xin, and Katevas]{StaKarAra_22}
Stamenkovic, D., Karatzoglou, A., Arapakis, I., Xin, X., and Katevas, K.
\newblock Choosing the best of both worlds: Diverse and novel recommendations through multi-objective reinforcement learning.
\newblock In \emph{Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining}, pp.\  957--965, 2022.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{SutBar_18}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, Mansour, et~al.]{SutMcASin_99}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., Mansour, Y., et~al.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock In \emph{NIPs}, volume~99, pp.\  1057--1063. Citeseer, 1999.

\bibitem[Theocharous et~al.(2015)Theocharous, Thomas, and Ghavamzadeh]{theocharous2015personalized}
Theocharous, G., Thomas, P.~S., and Ghavamzadeh, M.
\newblock Personalized ad recommendation systems for life-time value optimization with guarantees.
\newblock In \emph{the 24th International Joint Conference on Artificial Intelligence}, 2015.

\bibitem[Tsitsiklis \& Van~Roy(1999)Tsitsiklis and Van~Roy]{TsiVan_99}
Tsitsiklis, J.~N. and Van~Roy, B.
\newblock Average cost temporal-difference learning.
\newblock \emph{Automatica}, 35\penalty0 (11):\penalty0 1799--1808, 1999.

\bibitem[Van~Moffaert \& Now{\'e}(2014)Van~Moffaert and Now{\'e}]{van2014multi}
Van~Moffaert, K. and Now{\'e}, A.
\newblock Multi-objective reinforcement learning using sets of pareto dominating policies.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0 (1):\penalty0 3483--3512, 2014.

\bibitem[Wei et~al.(2022)Wei, Liu, and Ying]{WeiLiuYin_22}
Wei, H., Liu, X., and Ying, L.
\newblock Triple-q: A model-free algorithm for constrained reinforcement learning with sublinear regret and zero constraint violation.
\newblock In Camps-Valls, G., Ruiz, F. J.~R., and Valera, I. (eds.), \emph{Proceedings of The 25th International Conference on Artificial Intelligence and Statistics}, volume 151 of \emph{Proceedings of Machine Learning Research}, pp.\  3274--3307. PMLR, 28--30 Mar 2022.
\newblock URL \url{https://proceedings.mlr.press/v151/wei22a.html}.

\bibitem[Wen et~al.(2023)Wen, Liu, Fedorov, Zhang, Yin, Chu, Hassani, Sun, Liu, Wang, et~al.]{wen2023rankitect}
Wen, W., Liu, K.-H., Fedorov, I., Zhang, X., Yin, H., Chu, W., Hassani, K., Sun, M., Liu, J., Wang, X., et~al.
\newblock Rankitect: Ranking architecture search battling world-class engineers at meta scale.
\newblock \emph{arXiv preprint arXiv:2311.08430}, 2023.

\bibitem[Xu et~al.(2020)Xu, Wang, and Liang]{XuWanLia_20}
Xu, T., Wang, Z., and Liang, Y.
\newblock Improving sample complexity bounds for (natural) actor-critic algorithms.
\newblock \emph{arXiv preprint arXiv:2004.12956}, 2020.

\bibitem[Yang et~al.(2024)Yang, Liu, Liu, Dong, and Momma]{YanLiuLiu_23}
Yang, H., Liu, Z., Liu, J., Dong, C., and Momma, M.
\newblock Federated multi-objective learning.
\newblock 2024.

\bibitem[Yang et~al.(2019)Yang, Sun, and Narasimhan]{yang2019generalized}
Yang, R., Sun, X., and Narasimhan, K.
\newblock A generalized algorithm for multi-objective reinforcement learning and policy adaptation.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zerbinati et~al.(2011)Zerbinati, Desideri, and Duvigneau]{zerbinati2011comparison}
Zerbinati, A., Desideri, J.-A., and Duvigneau, R.
\newblock \emph{Comparison between MGDA and PAES for multi-objective optimization}.
\newblock PhD thesis, INRIA, 2011.

\bibitem[Zhang et~al.(2018)Zhang, Yang, Liu, Zhang, and Basar]{ZhaYanLiu_18}
Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T.
\newblock Fully decentralized multi-agent reinforcement learning with networked agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5872--5881. PMLR, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Liu, Liu, Zhu, and Lu]{ZhaLiuLiu_21}
Zhang, X., Liu, Z., Liu, J., Zhu, Z., and Lu, S.
\newblock Taming communication and sample complexities in decentralized policy evaluation for cooperative multi-agent reinforcement learning.
\newblock In \emph{Advances Neural Information Processing Systems (NeurIPS)}, Virtual Event, December 2021.

\bibitem[Zhou et~al.(2022)Zhou, Zhang, Jiang, Zhong, Gu, and Zhu]{ZhoZhaJia_22}
Zhou, S., Zhang, W., Jiang, J., Zhong, W., Gu, J., and Zhu, W.
\newblock On the convergence of stochastic multi-objective gradient manipulation and beyond.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 38103--38115, 2022.

\bibitem[Zou et~al.(2019)Zou, Xia, Ding, Song, Liu, and Yin]{zou2019reinforcement}
Zou, L., Xia, L., Ding, Z., Song, J., Liu, W., and Yin, D.
\newblock Reinforcement learning to optimize long-term user engagement in recommender systems.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pp.\  2810--2818, 2019.

\end{thebibliography}
