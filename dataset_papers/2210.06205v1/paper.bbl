\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and
  Zaremba]{andrychowicz2017hindsight}
M.~Andrychowicz, F.~Wolski, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, O.~Pieter~Abbeel, and W.~Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Brier et~al.(1950)]{brier1950verification}
G.~W. Brier et~al.
\newblock Verification of forecasts expressed in terms of probability.
\newblock \emph{Monthly weather review}, 78\penalty0 (1):\penalty0 1--3, 1950.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Campbell and Beronov(2019)]{campbell2019sparsevi}
T.~Campbell and B.~Beronov.
\newblock Sparse variational inference: Bayesian coresets from scratch.
\newblock In \emph{Advances in Neural Information Processing Systems 32
  (NeurIPS 2019)}, 2019.

\bibitem[Campbell and Broderick(2018)]{trevor2018giga}
T.~Campbell and T.~Broderick.
\newblock Bayesian coreset construction via greedy iterative geodesic ascent.
\newblock In \emph{Proceedings of The 35th International Conference on Machine
  Learning (ICML 2018)}, 2018.

\bibitem[Campbell and Broderick(2019)]{trevor2019hilbert}
T.~Campbell and T.~Broderick.
\newblock Automated scalable bayesian inference via hilbert coresets.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 551--588, 2019.

\bibitem[Carreira-Perpinan and Hinton(2005)]{carreira2005contrastive}
M.~A. Carreira-Perpinan and G.~Hinton.
\newblock On contrastive divergence learning.
\newblock In \emph{International workshop on artificial intelligence and
  statistics}, pages 33--40. PMLR, 2005.

\bibitem[Cazenavette et~al.(2022)Cazenavette, Wang, Torralba, Efros, and
  Zhu]{cazenavette2022tm}
G.~Cazenavette, T.~Wang, A.~Torralba, A.~A. Efros, and J.-Y. Zhu.
\newblock Dataset distillation by matching training trajectories.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2022.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014sghmc}
T.~Chen, E.~B. Fox, and C.~Guestrin.
\newblock Stochastic gradient {Hamiltonian Monte Carlo}.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning (ICML 2014)}, 2014.

\bibitem[Duane et~al.(1987)Duane, Kennedy, Pendleton, and
  Roweth]{duane1987hybrid}
S.~Duane, A.~D. Kennedy, B.~J. Pendleton, and D.~Roweth.
\newblock Hybrid {Monte Carlo}.
\newblock \emph{Physics Letters B}, 195\penalty0 (2):\penalty0 216 -- 222,
  1987.

\bibitem[Dwork et~al.(2014)Dwork, Roth, et~al.]{dwork2014algorithmic}
C.~Dwork, A.~Roth, et~al.
\newblock The algorithmic foundations of differential privacy.
\newblock \emph{Found. Trends Theor. Comput. Sci.}, 9\penalty0 (3-4):\penalty0
  211--407, 2014.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019robustness}
D.~Hendrycks and T.~Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem[Hinton(2002)]{hinton2002cd}
G.~E. Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural Computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.
\newblock \doi{10.1162/089976602760128018}.

\bibitem[Huggins et~al.(2016)Huggins, Campbell, and
  Broderick]{Huggins2016coresets}
J.~Huggins, T.~Campbell, and T.~Broderick.
\newblock Coresets for bayesian logistic regression.
\newblock In \emph{Advances in Neural Information Processing Systems 29 (NIPS
  2016)}, 2016.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Le~Cam(2012)]{le2012asymptotic}
L.~Le~Cam.
\newblock \emph{Asymptotic methods in statistical decision theory}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Lin(1992)]{lin1992self}
L.-J. Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 293--321, 1992.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
S.~Mandt, M.~D. Hoffman, and D.~M. Blei.
\newblock Stochastic gradient descent as approximate bayesian inference.
\newblock \emph{arXiv preprint arXiv:1704.04289}, 2017.

\bibitem[Manousakas et~al.(2020)Manousakas, Xu, Mascolo, and
  Campbell]{manousakas2020bayesian}
D.~Manousakas, Z.~Xu, C.~Mascolo, and T.~Campbell.
\newblock Bayesian pseudocoresets.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS 2020)}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and Hauskrecht]{naeini2015ece}
M.~P. Naeini, G.~Cooper, and M.~Hauskrecht.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Neal(2010)]{neal2010mcmc}
R.~M. Neal.
\newblock {MCMC} using {Hamiltonian} dynamics.
\newblock \emph{Handbook of Markov Chain Monte Carlo}, 54:\penalty0 113--162,
  2010.

\bibitem[Nguyen et~al.(2020)Nguyen, Chen, and Lee]{nguyen2020kip}
T.~Nguyen, Z.~Chen, and J.~Lee.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Novak, Xiao, and
  Lee]{nguyen2020kipinfinite}
T.~Nguyen, R.~Novak, L.~Xiao, and J.~Lee.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS 2021)}, 2021.

\bibitem[Park and Kim(2022)]{park2022blurs}
N.~Park and S.~Kim.
\newblock Blurs behave like ensembles: Spatial smoothings to improve accuracy,
  uncertainty, and robustness.
\newblock In \emph{International Conference on Machine Learning}, pages
  17390--17419. PMLR, 2022.

\bibitem[Shao et~al.(2021)Shao, Shi, Yi, Chen, and Hsieh]{shao2021adversarial}
R.~Shao, Z.~Shi, J.~Yi, P.-Y. Chen, and C.-J. Hsieh.
\newblock On the adversarial robustness of vision transformers.
\newblock \emph{arXiv preprint arXiv:2103.15670}, 2021.

\bibitem[Van~der Vaart(1998)]{van1998asymptotic}
A.~Van~der Vaart.
\newblock Asymptotic statistics. cambridge.
\newblock \emph{UK: Cam}, 1998.

\bibitem[Wang et~al.(2018)Wang, Zhu, Torralba, and Efros]{wang2018dataset}
T.~Wang, J.-Y. Zhu, A.~Torralba, and A.~A. Efros.
\newblock Dataset distillation.
\newblock \emph{arXiv preprint arXiv:1811.10959}, 2018.

\bibitem[Welling(2009)]{welling2009herding}
M.~Welling.
\newblock Herding dynamical weights to learn.
\newblock In \emph{Proceedings of the 26st International Conference on Machine
  Learning (ICML 2009)}, 2009.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, Świątkowski, Tran, Mandt,
  Snoek, Salimans, Jenatton, and Nowozin]{Florian2019howposterior}
F.~Wenzel, K.~Roth, B.~S. Veeling, J.~Świątkowski, L.~Tran, S.~Mandt,
  J.~Snoek, T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock How good is the bayes posterior in deep neural networks really?
\newblock In \emph{Proceedings of The 37th International Conference on Machine
  Learning (ICML 2020)}, 2020.

\bibitem[Wolf(2011)]{Wolf2011kcenter}
G.~W. Wolf.
\newblock Facility location: concepts, models, algorithms and case studies.
\newblock 2011.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and Beyer]{Zhai2021vit}
X.~Zhai, A.~Kolesnikov, N.~Houlsby, and L.~Beyer.
\newblock Scaling vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.04560}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,
  X.~Li, X.~V. Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao and Bilen(2021{\natexlab{a}})]{zhao2020dcsiamese}
B.~Zhao and H.~Bilen.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In \emph{Proceedings of The 38th International Conference on Machine
  Learning (ICML 2021)}, 2021{\natexlab{a}}.

\bibitem[Zhao and Bilen(2021{\natexlab{b}})]{zhao2021DM}
B.~Zhao and H.~Bilen.
\newblock Dataset condensation with distribution matching.
\newblock \emph{arXiv preprint arXiv:2110.04181}, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{zhao2020dcgm}
B.~Zhao, K.~R. Mopuri, and H.~Bilen.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\end{thebibliography}
