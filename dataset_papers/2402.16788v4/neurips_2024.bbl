\begin{thebibliography}{109}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida, J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Adams et~al.(2018)Adams, Pennington, Johnson, Smith, Ovadia, Patton, and Saunderson]{adams2018estimating}
R.~P. Adams, J.~Pennington, M.~J. Johnson, J.~Smith, Y.~Ovadia, B.~Patton, and J.~Saunderson.
\newblock Estimating the spectral density of large implicit matrices.
\newblock \emph{arXiv preprint arXiv:1802.03451}, 2018.

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Song, Yun, Jadbabaie, and Sra]{ahn2023linear}
K.~Ahn, X.~Cheng, M.~Song, C.~Yun, A.~Jadbabaie, and S.~Sra.
\newblock Linear attention is (maybe) all you need (to understand transformer optimization).
\newblock \emph{arXiv preprint arXiv:2310.01082}, 2023.

\bibitem[Avron and Toledo(2011)]{avron2011randomized}
H.~Avron and S.~Toledo.
\newblock Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix.
\newblock \emph{Journal of the ACM (JACM)}, 58\penalty0 (2):\penalty0 1--34, 2011.

\bibitem[Bachlechner et~al.(2021)Bachlechner, Majumder, Mao, Cottrell, and McAuley]{bachlechner2021rezero}
T.~Bachlechner, B.~P. Majumder, H.~Mao, G.~Cottrell, and J.~McAuley.
\newblock Rezero is all you need: Fast convergence at large depth.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1352--1361. PMLR, 2021.

\bibitem[Bai and Golub(1996)]{bai1996bounds}
Z.~Bai and G.~H. Golub.
\newblock Bounds for the trace of the inverse and the determinant of symmetric positive definite matrices.
\newblock \emph{Annals of Numerical Mathematics}, 4:\penalty0 29--38, 1996.

\bibitem[Bai et~al.(1996)Bai, Fahey, and Golub]{bai1996some}
Z.~Bai, G.~Fahey, and G.~Golub.
\newblock Some large-scale matrix computation problems.
\newblock \emph{Journal of Computational and Applied Mathematics}, 74\penalty0 (1-2):\penalty0 71--89, 1996.

\bibitem[Beck and Tetruashvili(2013)]{beck2013convergence}
A.~Beck and L.~Tetruashvili.
\newblock On the convergence of block coordinate descent type methods.
\newblock \emph{SIAM journal on Optimization}, 23\penalty0 (4):\penalty0 2037--2060, 2013.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and Anandkumar]{bernstein2018signsgd}
J.~Bernstein, Y.-X. Wang, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pages 560--569. PMLR, 2018.

\bibitem[Bock and Wei{\ss}(2019)]{bock2019non}
S.~Bock and M.~Wei{\ss}.
\newblock Non-convergence and limit cycles in the adam optimizer.
\newblock In \emph{Artificial Neural Networks and Machine Learning--ICANN 2019: Deep Learning: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17--19, 2019, Proceedings, Part II 28}, pages 232--243. Springer, 2019.

\bibitem[Brezinski(1990)]{brezinski1990direct}
C.~Brezinski.
\newblock A direct proof of the christoffel-darboux identity and its equivalence to the recurrence relationship.
\newblock \emph{Journal of Computational and Applied Mathematics}, 32\penalty0 (1-2):\penalty0 17--25, 1990.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi, Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
P.~Chaudhari, A.~Choromanska, S.~Soatto, Y.~LeCun, C.~Baldassi, C.~Borgs, J.~Chayes, L.~Sagun, and R.~Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019\penalty0 (12):\penalty0 124018, 2019.

\bibitem[Chen et~al.(2022)Chen, Shen, Zou, and Liu]{chen2022towards}
C.~Chen, L.~Shen, F.~Zou, and W.~Liu.
\newblock Towards practical adam: Non-convexity, convergence theory, and mini-batch acceleration.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0 (1):\penalty0 10411--10457, 2022.

\bibitem[Chen et~al.(2021)Chen, Kunstner, and Schmidt]{chen2021heavy}
J.~Chen, F.~Kunstner, and M.~Schmidt.
\newblock Heavy-tailed noise does not explain the gap between sgd and adam on transformers.
\newblock In \emph{13th Annual Workshop on Optimization for Machine Learning}, 2021.

\bibitem[Chen et~al.(2018)Chen, Firat, Bapna, Johnson, Macherey, Foster, Jones, Parmar, Schuster, Chen, et~al.]{chen2018best}
M.~X. Chen, O.~Firat, A.~Bapna, M.~Johnson, W.~Macherey, G.~Foster, L.~Jones, N.~Parmar, M.~Schuster, Z.~Chen, et~al.
\newblock The best of both worlds: Combining recent advances in neural machine translation.
\newblock \emph{arXiv preprint arXiv:1804.09849}, 2018.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{chen2019convergence}
X.~Chen, S.~Liu, R.~Sun, and M.~Hong.
\newblock On the convergence of a class of adam-type algorithms for non-convex optimization.
\newblock In \emph{7th International Conference on Learning Representations, ICLR 2019}, 2019.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham, H.~W. Chung, C.~Sutton, S.~Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Collobert(2004)]{collobert2004large}
R.~Collobert.
\newblock Large scale machine learning.
\newblock Technical report, Universit{\'e} de Paris VI, 2004.

\bibitem[Crawshaw et~al.(2022)Crawshaw, Liu, Orabona, Zhang, and Zhuang]{crawshaw2022robustness}
M.~Crawshaw, M.~Liu, F.~Orabona, W.~Zhang, and Z.~Zhuang.
\newblock Robustness to unbounded smoothness of generalized signsgd.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 9955--9968, 2022.

\bibitem[Cullum and Willoughby(2002)]{cullum2002lanczos}
J.~K. Cullum and R.~A. Willoughby.
\newblock \emph{Lanczos algorithms for large symmetric eigenvalue computations: Vol. I: Theory}.
\newblock SIAM, 2002.

\bibitem[Da~Silva and Gazeau(2020)]{da2020general}
A.~B. Da~Silva and M.~Gazeau.
\newblock A general system of differential equations to model first-order adaptive algorithms.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5072--5113, 2020.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
T.~Dao, D.~Fu, S.~Ermon, A.~Rudra, and C.~R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16344--16359, 2022.

\bibitem[D{\'e}fossez et~al.(2022)D{\'e}fossez, Bottou, Bach, and Usunier]{defossez2022simple}
A.~D{\'e}fossez, L.~Bottou, F.~Bach, and N.~Usunier.
\newblock A simple convergence proof of adam and adagrad.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
M.~Dehghani, J.~Djolonga, B.~Mustafa, P.~Padlewski, J.~Heek, J.~Gilmer, A.~P. Steiner, M.~Caron, R.~Geirhos, I.~Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pages 7480--7512. PMLR, 2023.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{dong2021attention}
Y.~Dong, J.-B. Cordonnier, and A.~Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly exponentially with depth.
\newblock In \emph{International Conference on Machine Learning}, pages 2793--2803. PMLR, 2021.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Epperson(2013)]{epperson2013introduction}
J.~F. Epperson.
\newblock An introduction to numerical methods and analysis.
\newblock 2013.

\bibitem[Forsythe and Straus(1955)]{forsythe1955best}
G.~E. Forsythe and E.~G. Straus.
\newblock On best conditioned matrices.
\newblock \emph{Proceedings of the American Mathematical Society}, 6\penalty0 (3):\penalty0 340--345, 1955.

\bibitem[Gadat and Gavra(2022)]{gadat2022asymptotic}
S.~Gadat and I.~Gavra.
\newblock Asymptotic study of stochastic adaptive algorithms in non-convex landscape.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0 (1):\penalty0 10357--10410, 2022.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and Xiao]{ghorbani2019investigation}
B.~Ghorbani, S.~Krishnan, and Y.~Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue density.
\newblock In \emph{International Conference on Machine Learning}, pages 2232--2241. PMLR, 2019.

\bibitem[Goh(2017)]{goh2017why}
G.~Goh.
\newblock Why momentum really works.
\newblock \emph{Distill}, 2017.
\newblock \doi{10.23915/distill.00006}.
\newblock URL \url{http://distill.pub/2017/momentum}.

\bibitem[Golub and Meurant(2009)]{golub2009matrices}
G.~H. Golub and G.~Meurant.
\newblock \emph{Matrices, moments and quadrature with applications}, volume~30.
\newblock Princeton University Press, 2009.

\bibitem[Golub and Strako{\v{s}}(1994)]{golub1994estimates}
G.~H. Golub and Z.~Strako{\v{s}}.
\newblock Estimates in quadratic formulas.
\newblock \emph{Numerical Algorithms}, 8:\penalty0 241--268, 1994.

\bibitem[Golub and Welsch(1969)]{golub1969calculation}
G.~H. Golub and J.~H. Welsch.
\newblock Calculation of gauss quadrature rules.
\newblock \emph{Mathematics of computation}, 23\penalty0 (106):\penalty0 221--230, 1969.

\bibitem[Goujaud et~al.(2022)Goujaud, Scieur, Dieuleveut, Taylor, and Pedregosa]{goujaud2022super}
B.~Goujaud, D.~Scieur, A.~Dieuleveut, A.~B. Taylor, and F.~Pedregosa.
\newblock Super-acceleration with cyclical step-sizes.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3028--3065. PMLR, 2022.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
A.~Gu and T.~Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur2018gradient}
G.~Gur-Ari, D.~A. Roberts, and E.~Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Huang et~al.(2020)Huang, Perez, Ba, and Volkovs]{huang2020improving}
X.~S. Huang, F.~Perez, J.~Ba, and M.~Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{International Conference on Machine Learning}, pages 4475--4483. PMLR, 2020.

\bibitem[Hutchinson(1989)]{hutchinson1989stochastic}
M.~F. Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation}, 18\penalty0 (3):\penalty0 1059--1076, 1989.

\bibitem[Jiang et~al.(2023)Jiang, Malik, and Li]{jiang2023does}
K.~Jiang, D.~Malik, and Y.~Li.
\newblock How does adaptive optimization impact local neural network geometry?
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child, S.~Gray, A.~Radford, J.~Wu, and D.~Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kunstner et~al.(2023)Kunstner, Chen, Lavington, and Schmidt]{kunstner2023noise}
F.~Kunstner, J.~Chen, J.~W. Lavington, and M.~Schmidt.
\newblock Noise is not the main factor behind the gap between sgd and adam on transformers, but sign descent might be.
\newblock \emph{arXiv preprint arXiv:2304.13960}, 2023.

\bibitem[Lanczos(1950)]{lanczos1950iteration}
C.~Lanczos.
\newblock An iteration method for the solution of the eigenvalue problem of linear differential and integral operators.
\newblock 1950.

\bibitem[Lavezzi et~al.(2022)Lavezzi, Guye, and Ciarci{\`a}]{lavezzi2022nonlinear}
G.~Lavezzi, K.~Guye, and M.~Ciarci{\`a}.
\newblock Nonlinear programming solvers for unconstrained and constrained optimization problems: a benchmark analysis.
\newblock \emph{arXiv preprint arXiv:2204.05297}, 2022.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[LeCun et~al.(2002)LeCun, Bottou, Orr, and M{\"u}ller]{lecun2002efficient}
Y.~LeCun, L.~Bottou, G.~B. Orr, and K.-R. M{\"u}ller.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 9--50. Springer, 2002.

\bibitem[Li et~al.(2023)Li, Rakhlin, and Jadbabaie]{li2023convergence}
H.~Li, A.~Rakhlin, and A.~Jadbabaie.
\newblock Convergence of adam under relaxed assumptions.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Liao and Mahoney(2021)]{liao2021hessian}
Z.~Liao and M.~W. Mahoney.
\newblock Hessian eigenspectra of more realistic nonlinear models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 20104--20117, 2021.

\bibitem[Lin et~al.(2016)Lin, Saad, and Yang]{lin2016approximating}
L.~Lin, Y.~Saad, and C.~Yang.
\newblock Approximating spectral densities of large matrices.
\newblock \emph{SIAM review}, 58\penalty0 (1):\penalty0 34--65, 2016.

\bibitem[Liu et~al.(2023)Liu, Li, Hall, Liang, and Ma]{liu2023sophia}
H.~Liu, Z.~Li, D.~Hall, P.~Liang, and T.~Ma.
\newblock Sophia: A scalable stochastic second-order optimizer for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2305.14342}, 2023.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and Han]{liu2019variance}
L.~Liu, H.~Jiang, P.~He, W.~Chen, X.~Liu, J.~Gao, and J.~Han.
\newblock On the variance of the adaptive learning rate and beyond. arxiv 2019.
\newblock \emph{arXiv preprint arXiv:1908.03265}, 2019.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
L.~Liu, X.~Liu, J.~Gao, W.~Chen, and J.~Han.
\newblock Understanding the difficulty of training transformers.
\newblock \emph{arXiv preprint arXiv:2004.08249}, 2020.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Luo et~al.(2018)Luo, Xiong, Liu, and Sun]{luo2018adaptive}
L.~Luo, Y.~Xiong, Y.~Liu, and X.~Sun.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lv et~al.(2023)Lv, Yang, Liu, Gao, Guo, and Qiu]{lv2023full}
K.~Lv, Y.~Yang, T.~Liu, Q.~Gao, Q.~Guo, and X.~Qiu.
\newblock Full parameter fine-tuning for large language models with limited resources.
\newblock \emph{arXiv preprint arXiv:2306.09782}, 2023.

\bibitem[Malladi et~al.(2023)Malladi, Gao, Nichani, Damian, Lee, Chen, and Arora]{malladi2023fine}
S.~Malladi, T.~Gao, E.~Nichani, A.~Damian, J.~D. Lee, D.~Chen, and S.~Arora.
\newblock Fine-tuning language models with just forward passes.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 53038--53075, 2023.

\bibitem[Merrill et~al.(2020)Merrill, Ramanujan, Goldberg, Schwartz, and Smith]{merrill2020effects}
W.~Merrill, V.~Ramanujan, Y.~Goldberg, R.~Schwartz, and N.~Smith.
\newblock Effects of parameter norm growth during transformer training: Inductive bias from gradient descent.
\newblock \emph{arXiv preprint arXiv:2010.09697}, 2020.

\bibitem[Molybog et~al.(2023)Molybog, Albert, Chen, DeVito, Esiobu, Goyal, Koura, Narang, Poulton, Silva, et~al.]{molybog2023theory}
I.~Molybog, P.~Albert, M.~Chen, Z.~DeVito, D.~Esiobu, N.~Goyal, P.~S. Koura, S.~Narang, A.~Poulton, R.~Silva, et~al.
\newblock A theory on adam instability in large-scale machine learning.
\newblock \emph{arXiv preprint arXiv:2304.09871}, 2023.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course}, volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Nguyen and Salazar(2019)]{nguyen2019transformers}
T.~Q. Nguyen and J.~Salazar.
\newblock Transformers without tears: Improving the normalization of self-attention.
\newblock \emph{arXiv preprint arXiv:1910.05895}, 2019.

\bibitem[Noci et~al.(2022)Noci, Anagnostidis, Biggio, Orvieto, Singh, and Lucchi]{noci2022signal}
L.~Noci, S.~Anagnostidis, L.~Biggio, A.~Orvieto, S.~P. Singh, and A.~Lucchi.
\newblock Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27198--27211, 2022.

\bibitem[Pan and Li(2023)]{pan2023toward}
Y.~Pan and Y.~Li.
\newblock Toward understanding why adam converges faster than sgd for transformers.
\newblock \emph{arXiv preprint arXiv:2306.00204}, 2023.

\bibitem[Papyan(2018)]{papyan2018full}
V.~Papyan.
\newblock The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size.
\newblock \emph{arXiv preprint arXiv:1811.07062}, 2018.

\bibitem[Papyan(2019)]{papyan2019measurements}
V.~Papyan.
\newblock Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians.
\newblock \emph{arXiv preprint arXiv:1901.08244}, 2019.

\bibitem[Papyan(2020)]{papyan2020traces}
V.~Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 10197--10260, 2020.

\bibitem[Pearlmutter(1994)]{pearlmutter1994fast}
B.~A. Pearlmutter.
\newblock Fast exact multiplication by the hessian.
\newblock \emph{Neural computation}, 6\penalty0 (1):\penalty0 147--160, 1994.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2018convergence}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Saad(2011)]{saad2011numerical}
Y.~Saad.
\newblock \emph{Numerical methods for large eigenvalue problems: revised edition}.
\newblock SIAM, 2011.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016eigenvalues}
L.~Sagun, L.~Bottou, and Y.~LeCun.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock \emph{arXiv preprint arXiv:1611.07476}, 2016.

\bibitem[Sagun et~al.(2017)Sagun, Evci, Guney, Dauphin, and Bottou]{sagun2017empirical}
L.~Sagun, U.~Evci, V.~U. Guney, Y.~Dauphin, and L.~Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural networks.
\newblock \emph{arXiv preprint arXiv:1706.04454}, 2017.

\bibitem[Sankar et~al.(2021)Sankar, Khasbage, Vigneswaran, and Balasubramanian]{sankar2021deeper}
A.~R. Sankar, Y.~Khasbage, R.~Vigneswaran, and V.~N. Balasubramanian.
\newblock A deeper look at the hessian eigenspectrum of deep neural networks and its applications to regularization.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 9481--9488, 2021.

\bibitem[Shi et~al.(2020)Shi, Li, Hong, and Sun]{shi2020rmsprop}
N.~Shi, D.~Li, M.~Hong, and R.~Sun.
\newblock Rmsprop converges with proper hyper-parameter.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Sun(2019)]{sun2019optimization}
R.~Sun.
\newblock Optimization for deep learning: theory and algorithms.
\newblock \emph{arXiv preprint arXiv:1912.08957}, 2019.

\bibitem[Sun and Ye(2021)]{sun2021worst}
R.~Sun and Y.~Ye.
\newblock Worst-case complexity of cyclic coordinate descent: O (n\^{} 2) o (n 2) gap with randomized version.
\newblock \emph{Mathematical Programming}, 185:\penalty0 487--520, 2021.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai, Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
I.~O. Tolstikhin, N.~Houlsby, A.~Kolesnikov, L.~Beyer, X.~Zhai, T.~Unterthiner, J.~Yung, A.~Steiner, D.~Keysers, J.~Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 24261--24272, 2021.

\bibitem[Ubaru et~al.(2017)Ubaru, Chen, and Saad]{ubaru2017fast}
S.~Ubaru, J.~Chen, and Y.~Saad.
\newblock Fast estimation of tr(f(a)) via stochastic lanczos quadrature.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 38\penalty0 (4):\penalty0 1075--1099, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Zhang, Zhang, Meng, Ma, Liu, and Chen]{wang2022provable}
B.~Wang, Y.~Zhang, H.~Zhang, Q.~Meng, Z.-M. Ma, T.-Y. Liu, and W.~Chen.
\newblock Provable adaptivity in adam.
\newblock \emph{arXiv preprint arXiv:2208.09900}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Fu, Zhang, Zheng, and Chen]{wang2023closing}
B.~Wang, J.~Fu, H.~Zhang, N.~Zheng, and W.~Chen.
\newblock Closing the gap between the upper bound and lower bound of adam's iteration complexity.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhang, Ma, and Chen]{wang2023convergence}
B.~Wang, H.~Zhang, Z.~Ma, and W.~Chen.
\newblock Convergence of adagrad for non-convex objectives: Simple proofs and relaxed assumptions.
\newblock In \emph{The Thirty Sixth Annual Conference on Learning Theory}, pages 161--190. PMLR, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Ma, Dong, Huang, Zhang, and Wei]{wang2022deepnet}
H.~Wang, S.~Ma, L.~Dong, S.~Huang, D.~Zhang, and F.~Wei.
\newblock Deepnet: Scaling transformers to 1,000 layers.
\newblock \emph{arXiv preprint arXiv:2203.00555}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Li, Xiao, Zhu, Li, Wong, and Chao]{wang2019learning}
Q.~Wang, B.~Li, T.~Xiao, J.~Zhu, C.~Li, D.~F. Wong, and L.~S. Chao.
\newblock Learning deep transformer models for machine translation.
\newblock \emph{arXiv preprint arXiv:1906.01787}, 2019.

\bibitem[{Wikipedia}(2023)]{enwiki:1191539517}
{Wikipedia}.
\newblock Gaussian quadrature --- {Wikipedia}{,} the free encyclopedia, 2023.
\newblock URL \url{https://en.wikipedia.org/w/index.php?title=Gaussian_quadrature&oldid=1191539517}.
\newblock [Online; accessed 20-January-2024].

\bibitem[Wortsman et~al.(2023)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
M.~Wortsman, P.~J. Liu, L.~Xiao, K.~Everett, A.~Alemi, B.~Adlam, J.~D. Co-Reyes, I.~Gur, A.~Kumar, R.~Novak, et~al.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock \emph{arXiv preprint arXiv:2309.14322}, 2023.

\bibitem[Wu et~al.(2020)Wu, Zhu, Wu, Wang, and Ge]{wu2020dissecting}
Y.~Wu, X.~Zhu, C.~Wu, A.~Wang, and R.~Ge.
\newblock Dissecting hessian: Understanding common structure of hessian in neural networks.
\newblock \emph{arXiv preprint arXiv:2010.04261}, 2020.

\bibitem[Xiao et~al.(2021)Xiao, Singh, Mintun, Darrell, Doll{\'a}r, and Girshick]{xiao2021early}
T.~Xiao, M.~Singh, E.~Mintun, T.~Darrell, P.~Doll{\'a}r, and R.~Girshick.
\newblock Early convolutions help transformers see better.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 30392--30400, 2021.

\bibitem[Xie et~al.(2022)Xie, Wang, Zhang, Sato, and Sugiyama]{xie2022adaptive}
Z.~Xie, X.~Wang, H.~Zhang, I.~Sato, and M.~Sugiyama.
\newblock Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum.
\newblock In \emph{International conference on machine learning}, pages 24430--24459. PMLR, 2022.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{xiong2020layer}
R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan, L.~Wang, and T.~Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages 10524--10533. PMLR, 2020.

\bibitem[Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan, et~al.]{yang2023baichuan}
A.~Yang, B.~Xiao, B.~Wang, B.~Zhang, C.~Bian, C.~Yin, C.~Lv, D.~Pan, D.~Wang, D.~Yan, et~al.
\newblock Baichuan 2: Open large-scale language models.
\newblock \emph{arXiv preprint arXiv:2309.10305}, 2023.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
G.~Yang, E.~J. Hu, I.~Babuschkin, S.~Sidor, X.~Liu, D.~Farhi, N.~Ryder, J.~Pachocki, W.~Chen, and J.~Gao.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Yao et~al.(2018)Yao, Gholami, Lei, Keutzer, and Mahoney]{yao2018hessian}
Z.~Yao, A.~Gholami, Q.~Lei, K.~Keutzer, and M.~W. Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to adversaries.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Z.~Yao, A.~Gholami, K.~Keutzer, and M.~W. Mahoney.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE international conference on big data (Big data)}, pages 581--590. IEEE, 2020.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and Kumar]{zaheer2018adaptive}
M.~Zaheer, S.~Reddi, D.~Sachan, S.~Kale, and S.~Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia, et~al.]{zeng2022glm}
A.~Zeng, X.~Liu, Z.~Du, Z.~Wang, H.~Lai, M.~Ding, Z.~Yang, Y.~Xu, W.~Zheng, X.~Xia, et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock \emph{arXiv preprint arXiv:2210.02414}, 2022.

\bibitem[Zhai et~al.(2023)Zhai, Likhomanenko, Littwin, Busbridge, Ramapuram, Zhang, Gu, and Susskind]{zhai2023stabilizing}
S.~Zhai, T.~Likhomanenko, E.~Littwin, D.~Busbridge, J.~Ramapuram, Y.~Zhang, J.~Gu, and J.~M. Susskind.
\newblock Stabilizing transformer training by preventing attention entropy collapse.
\newblock In \emph{International Conference on Machine Learning}, pages 40770--40803. PMLR, 2023.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Titov, and Sennrich]{zhang2019improving}
B.~Zhang, I.~Titov, and R.~Sennrich.
\newblock Improving deep transformer with depth-scaled initialization and merged attention.
\newblock \emph{arXiv preprint arXiv:1908.11365}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue, and Grosse]{zhang2019algorithmic}
G.~Zhang, L.~Li, Z.~Nado, J.~Martens, S.~Sachdeva, G.~Dahl, C.~Shallue, and R.~B. Grosse.
\newblock Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model.
\newblock \emph{Advances in neural information processing systems}, 32, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2019{\natexlab{c}})Zhang, He, Sra, and Jadbabaie]{zhang2019gradient}
J.~Zhang, T.~He, S.~Sra, and A.~Jadbabaie.
\newblock Why gradient clipping accelerates training: A theoretical justification for adaptivity.
\newblock \emph{arXiv preprint arXiv:1905.11881}, 2019{\natexlab{c}}.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and Sra]{zhang2020adaptive}
J.~Zhang, S.~P. Karimireddy, A.~Veit, S.~Kim, S.~Reddi, S.~Kumar, and S.~Sra.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15383--15393, 2020.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Chen, Shi, Sun, and Luo]{zhang2022adam}
Y.~Zhang, C.~Chen, N.~Shi, R.~Sun, and Z.-Q. Luo.
\newblock Adam can converge without any modification on update rules.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 28386--28399, 2022{\natexlab{b}}.

\bibitem[Zhou et~al.(2018)Zhou, Chen, Cao, Tang, Yang, and Gu]{zhou2018convergence}
D.~Zhou, J.~Chen, Y.~Cao, Y.~Tang, Z.~Yang, and Q.~Gu.
\newblock On the convergence of adaptive gradient methods for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1808.05671}, 2018.

\bibitem[Zou et~al.(2019)Zou, Shen, Jie, Zhang, and Liu]{zou2019sufficient}
F.~Zou, L.~Shen, Z.~Jie, W.~Zhang, and W.~Liu.
\newblock A sufficient condition for convergences of adam and rmsprop.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition}, pages 11127--11135, 2019.

\end{thebibliography}
