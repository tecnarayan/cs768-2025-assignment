\begin{thebibliography}{10}

\bibitem{bill2023fine}
Desir{\'e}e Bill and Theodor Eriksson.
\newblock Fine-tuning a llm using reinforcement learning from human feedback for a therapy chatbot application, 2023.

\bibitem{chang2023privacy}
Yansong Chang, Kai Zhang, Junqing Gong, and Haifeng Qian.
\newblock Privacy-preserving federated learning via functional encryption, revisited.
\newblock {\em IEEE Transactions on Information Forensics and Security}, 18:1855--1869, 2023.

\bibitem{cho2023heterogeneous}
Yae~Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, Matt Barnes, and Gauri Joshi.
\newblock Heterogeneous lora for federated fine-tuning of on-device foundation models.
\newblock In {\em International Workshop on Federated Learning in the Age of Foundation Models in Conjunction with NeurIPS 2023}, 2023.

\bibitem{dong2023towards}
Xin~Luna Dong, Seungwhan Moon, Yifan~Ethan Xu, Kshitiz Malik, and Zhou Yu.
\newblock Towards next-generation intelligent assistants leveraging llm techniques.
\newblock In {\em Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages 5792--5793, 2023.

\bibitem{he2022sparseadapter}
Shwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao.
\newblock Sparseadapter: An easy approach for improving the parameter-efficiency of adapters.
\newblock {\em arXiv preprint arXiv:2210.04284}, 2022.

\bibitem{he2023mera}
Shwai He, Run-Ze Fan, Liang Ding, Li~Shen, Tianyi Zhou, and Dacheng Tao.
\newblock Mera: Merging pretrained adapters for few-shot learning.
\newblock {\em arXiv preprint arXiv:2308.15982}, 2023.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In {\em International Conference on Machine Learning}, pages 2790--2799. PMLR, 2019.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{kelly2023bing}
Dominique Kelly, Yimin Chen, Sarah~E Cornwell, Nicole~S Delellis, Alex Mayhew, Sodiq Onaolapo, and Victoria~L Rubin.
\newblock Bing chat: The future of search engines?
\newblock {\em Proceedings of the Association for Information Science and Technology}, 60(1):1007--1009, 2023.

\bibitem{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock {\em arXiv preprint arXiv:1907.02189}, 2019.

\bibitem{luo2023wizardmath}
Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.
\newblock Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.
\newblock {\em arXiv preprint arXiv:2308.09583}, 2023.

\bibitem{mao2024survey}
Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu~Mi, Zhonghao Hu, and Yunjun Gao.
\newblock A survey on lora of large language models.
\newblock {\em arXiv preprint arXiv:2407.11046}, 2024.

\bibitem{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock In {\em Artificial intelligence and statistics}, pages 1273--1282. PMLR, 2017.

\bibitem{pfeiffer2020adapterfusion}
Jonas Pfeiffer, Aishwarya Kamath, Andreas R{\"u}ckl{\'e}, Kyunghyun Cho, and Iryna Gurevych.
\newblock Adapterfusion: Non-destructive task composition for transfer learning.
\newblock {\em arXiv preprint arXiv:2005.00247}, 2020.

\bibitem{singhal2023large}
Karan Singhal, Shekoofeh Azizi, Tao Tu, S~Sara Mahdavi, Jason Wei, Hyung~Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et~al.
\newblock Large language models encode clinical knowledge.
\newblock {\em Nature}, 620(7972):172--180, 2023.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{thirunavukarasu2023large}
Arun~James Thirunavukarasu, Darren Shu~Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting~Fang Tan, and Daniel Shu~Wei Ting.
\newblock Large language models in medicine.
\newblock {\em Nature medicine}, 29(8):1930--1940, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{wang2023fedhyper}
Ziyao Wang, Jianyu Wang, and Ang Li.
\newblock Fedhyper: A universal and robust learning rate scheduler for federated learning with hypergradient descent.
\newblock {\em arXiv preprint arXiv:2310.03156}, 2023.

\bibitem{yu2021differentially}
Da~Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin~A Inan, Gautam Kamath, Janardhan Kulkarni, Yin~Tat Lee, Andre Manoel, Lukas Wutschitz, et~al.
\newblock Differentially private fine-tuning of language models.
\newblock {\em arXiv preprint arXiv:2110.06500}, 2021.

\bibitem{zaken2021bitfit}
Elad~Ben Zaken, Shauli Ravfogel, and Yoav Goldberg.
\newblock Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models.
\newblock {\em arXiv preprint arXiv:2106.10199}, 2021.

\bibitem{zhang2021survey}
Chen Zhang, Yu~Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao.
\newblock A survey on federated learning.
\newblock {\em Knowledge-Based Systems}, 216:106775, 2021.

\bibitem{zhang2023towards}
Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang, Guoyin Wang, and Yiran Chen.
\newblock Towards building the federated gpt: Federated instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.05644}, 2023.

\bibitem{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu.
\newblock Tinyllama: An open-source small language model, 2024.

\bibitem{zhang2023adaptive}
Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu~Cheng, Weizhu Chen, and Tuo Zhao.
\newblock Adaptive budget allocation for parameter-efficient fine-tuning.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{zhao2018federated}
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
\newblock Federated learning with non-iid data.
\newblock {\em arXiv preprint arXiv:1806.00582}, 2018.

\bibitem{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\end{thebibliography}
