\providecommand{\AC}{A.-C}\providecommand{\CA}{C.-A}\providecommand{\CH}{C.-H}\providecommand{\CN}{C.-N}\providecommand{\CC}{C.-C}\providecommand{\CJ}{C.-J}\providecommand{\HJ}{H.-J}\providecommand{\HY}{H.-Y}\providecommand{\JC}{J.-C}\providecommand{\JP}{J.-P}\providecommand{\JB}{J.-B}\providecommand{\JF}{J.-F}\providecommand{\JJ}{J.-J}\providecommand{\JM}{J.-M}\providecommand{\KW}{K.-W}\providecommand{\KR}{K.-R}\providecommand{\PL}{P.-L}\providecommand{\RE}{R.-E}\providecommand{\SJ}{S.-J}\providecommand{\XR}{X.-R}\providecommand{\WX}{W.-X}\providecommand{\PL}{P.-L}\providecommand{\YX}{Y.-X}
\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2019)Agrawal, Amos, Barratt, Boyd, Diamond, and
  Kolter]{Agrawal_Amos_Barratt_Boyd_Diamond_Kolter19}
A.~Agrawal, B.~Amos, S.~Barratt, S.~Boyd, S.~Diamond, and J.~Z. Kolter.
\newblock Differentiable convex optimization layers.
\newblock In \emph{Advances in neural information processing systems}, pages
  9558--9570, 2019.

\bibitem[Amos and Kolter(2017)]{Amos_Brandon17}
B.~Amos and J.~Z. Kolter.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In \emph{ICML}, volume~70, pages 136--145, 2017.

\bibitem[Baydin et~al.(2018)Baydin, Pearlmutter, Radul, and
  Siskind]{Baydin_Pearlmutter_Radul_Siskind18}
A.~G. Baydin, B.~A. Pearlmutter, A.~A. Radul, and J.~M. Siskind.
\newblock Automatic differentiation in machine learning: a survey.
\newblock \emph{J. Mach. Learn. Res.}, 18\penalty0 (153):\penalty0 1--43, 2018.

\bibitem[Bengio(2000)]{Bengio00}
Y.~Bengio.
\newblock Gradient-based optimization of hyperparameters.
\newblock \emph{Neural computation}, 12\penalty0 (8):\penalty0 1889--1900,
  2000.

\bibitem[Bergstra and Bengio(2012)]{Bergstra_Bengio12}
J.~Bergstra and Y.~Bengio.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{J. Mach. Learn. Res.}, 2012.

\bibitem[Bergstra et~al.(2013)Bergstra, Yamins, and Cox]{Bergstra13}
J.~Bergstra, D.~Yamins, and D.~D. Cox.
\newblock Hyperopt: A python library for optimizing the hyperparameters of
  machine learning algorithms.
\newblock In \emph{Proceedings of the 12th Python in science conference}, pages
  13--20, 2013.

\bibitem[Borgerding et~al.(2017)Borgerding, Schniter, and Rangan]{Borgerding17}
M.~Borgerding, P.~Schniter, and S.~Rangan.
\newblock Amp-inspired deep networks for sparse linear inverse problems.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (16):\penalty0 4293--4308, 2017.

\bibitem[Bouhlel et~al.(2019)Bouhlel, Hwang, Bartoli, Lafage, Morlier, and
  Martins]{Bouhlel_Hwang_Bartoli_Lafage_Morlier_Martins2019}
M.~A. Bouhlel, J.~T. Hwang, N.~Bartoli, R.~Lafage, J.~Morlier, and J.~R. R.~A.
  Martins.
\newblock A python surrogate modeling framework with derivatives.
\newblock \emph{Advances in Engineering Software}, page 102662, 2019.
\newblock ISSN 0965-9978.
\newblock \doi{https://doi.org/10.1016/j.advengsoft.2019.03.005}.

\bibitem[Bousquet et~al.(2017)Bousquet, Gelly, Kurach, Teytaud, and
  Vincent]{Bousquet_Gelly_Kurach_Teyaud_Vincent17}
O.~Bousquet, S.~Gelly, K.~Kurach, O.~Teytaud, and D.~Vincent.
\newblock Critical hyper-parameters: No random, no cry.
\newblock \emph{arXiv preprint arXiv:1706.03200}, 2017.

\bibitem[Breheny and Huang(2011)]{Breheny_Huang11}
P.~Breheny and J.~Huang.
\newblock Coordinate descent algorithms for nonconvex penalized regression,
  with applications to biological feature selection.
\newblock \emph{Ann. Appl. Stat.}, 5\penalty0 (1):\penalty0 232, 2011.

\bibitem[Brochu et~al.(2010)Brochu, Cora, and Freitas]{Brochu_Cora_deFreitas10}
E.~Brochu, V.~M. Cora, and N.~De Freitas.
\newblock A tutorial on {Bayesian} optimization of expensive cost functions,
  with application to active user modeling and hierarchical reinforcement
  learning.
\newblock 2010.

\bibitem[Chapelle et~al.(2002)Chapelle, Vapnik, Bousquet, and
  Mukherjee]{Chapelle_Vapnick_Bousquet_Mukherjee02}
O.~Chapelle, V.~Vapnik, O.~Bousquet, and S.~Mukherjee.
\newblock Choosing multiple parameters for support vector machines.
\newblock \emph{Machine learning}, 46\penalty0 (1-3):\penalty0 131--159, 2002.

\bibitem[Combettes and Wajs(2005)]{Combettes_Wajs05}
P.~L. Combettes and V.~R. Wajs.
\newblock Signal recovery by proximal forward-backward splitting.
\newblock \emph{Multiscale Modeling \& Simulation}, 4\penalty0 (4):\penalty0
  1168--1200, 2005.

\bibitem[Daubechies et~al.(2004)Daubechies, Defrise, and {De
  Mol}]{Daubechies_Defrise_DeMol04}
I.~Daubechies, M.~Defrise, and C.~{De Mol}.
\newblock An iterative thresholding algorithm for linear inverse problems with
  a sparsity constraint.
\newblock \emph{Comm. Pure Appl. Math.}, 57\penalty0 (11):\penalty0 1413--1457,
  2004.

\bibitem[Deledalle et~al.(2014)Deledalle, Vaiter, Fadili, and
  Peyr{\'e}]{Deledalle_Vaiter_Fadili_Peyre14}
{\CA}.~Deledalle, S.~Vaiter, J.~Fadili, and G.~Peyr{\'e}.
\newblock {Stein Unbiased GrAdient estimator of the Risk} ({SUGAR}) for
  multiple parameter selection.
\newblock \emph{SIAM J. Imaging Sci.}, 7\penalty0 (4):\penalty0 2448--2487,
  2014.

\bibitem[Domke(2012)]{Domke12}
J.~Domke.
\newblock Generic methods for optimization-based modeling.
\newblock In \emph{AISTATS}, volume~22, pages 318--326, 2012.

\bibitem[Dossal et~al.(2013)Dossal, Kachour, Fadili, Peyr{\'e}, and
  Chesneau]{Dossal_Kachour_Fadili_Peyre_Chesneau12}
C.~Dossal, M.~Kachour, M.J. Fadili, G.~Peyr{\'e}, and C.~Chesneau.
\newblock The degrees of freedom of the lasso for general design matrix.
\newblock \emph{Statistica Sinica}, 23\penalty0 (2):\penalty0 809--828, 2013.

\bibitem[Efron(1986)]{Efron86}
B.~Efron.
\newblock How biased is the apparent error rate of a prediction rule?
\newblock \emph{J. Amer. Statist. Assoc.}, 81\penalty0 (394):\penalty0
  461--470, 1986.

\bibitem[Evans and Gariepy(1992)]{evan1992measure}
L.~C. Evans and R.~F. Gariepy.
\newblock \emph{Measure theory and fine properties of functions}.
\newblock CRC Press, 1992.

\bibitem[Foo et~al.(2008)Foo, Do, and Ng]{Foo_Do_Ng08}
C.~S. Foo, C.~B. Do, and A.~Y. Ng.
\newblock Efficient multiple hyperparameter learning for log-linear models.
\newblock In \emph{Advances in neural information processing systems}, pages
  377--384, 2008.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{Franceschi_Donini_Frasconi_Pontil17}
L.~Franceschi, M.~Donini, P.~Frasconi, and M.~Pontil.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{ICML}, pages 1165--1173, 2017.

\bibitem[Frecon et~al.(2018)Frecon, Salzo, and Pontil]{Frecon_Salzo_Pontil2018}
J.~Frecon, S.~Salzo, and M.~Pontil.
\newblock Bilevel learning of the group lasso structure.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8301--8311, 2018.

\bibitem[Friedman et~al.(2010)Friedman, Hastie, and
  Tibshirani]{Friedman_Hastie_Tibshirani10}
J.~Friedman, T.~J. Hastie, and R.~Tibshirani.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock \emph{J. Stat. Softw.}, 33\penalty0 (1):\penalty0 1--22, 2010.

\bibitem[Gregor and LeCun(2010)]{Gregor_LeCun10}
K.~Gregor and Y.~LeCun.
\newblock Learning fast approximations of sparse coding.
\newblock In \emph{ICML}, pages 399--406, 2010.

\bibitem[Hale et~al.(2008)Hale, Yin, and Zhang]{Hale_Yin_Zhang08}
E.~Hale, W.~Yin, and Y.~Zhang.
\newblock Fixed-point continuation for $\ell_1$-minimization: Methodology and
  convergence.
\newblock \emph{SIAM J. Optim.}, 19\penalty0 (3):\penalty0 1107--1130, 2008.

\bibitem[Kunisch and Pock(2013)]{Kunisch_Pock13}
K.~Kunisch and T.~Pock.
\newblock A bilevel optimization approach for parameter learning in variational
  models.
\newblock \emph{SIAM J. Imaging Sci.}, 6\penalty0 (2):\penalty0 938--983, 2013.

\bibitem[Lam et~al.(2015)Lam, Pitrou, and Seibert]{Lam_Pitrou_Seibert15}
S.~K. Lam, A.~Pitrou, and S.~Seibert.
\newblock {Numba: A LLVM-based Python JIT Compiler}.
\newblock In \emph{Proceedings of the Second Workshop on the LLVM Compiler
  Infrastructure in HPC}, pages 1--6. ACM, 2015.

\bibitem[Larsen et~al.(1996)Larsen, Hansen, Svarer, and
  Ohlsson]{Larsen_Hansen_Svarer_Ohlsson96}
J.~Larsen, L.~K. Hansen, C.~Svarer, and M.~Ohlsson.
\newblock Design and regularization of neural networks: the optimal use of a
  validation set.
\newblock In \emph{Neural Networks for Signal Processing VI. Proceedings of the
  1996 IEEE Signal Processing Society Workshop}, 1996.

\bibitem[Larsen et~al.(2012)Larsen, Svarer, Andersen, and
  Hansen]{Larsen_Svarer_Andersen_Hansen12}
J.~Larsen, C.~Svarer, L.~N. Andersen, and L.~K. Hansen.
\newblock Adaptive regularization in neural network modeling.
\newblock In \emph{Neural Networks: Tricks of the Trade - Second Edition},
  pages 111--130. Springer, 2012.

\bibitem[Liu et~al.(2018)Liu, Chen, Wang, and Yin]{Liu18}
J.~Liu, X.~Chen, Z.~Wang, and W.~Yin.
\newblock Alista: Analytic weights are as good as learned weights in lista.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Liu et~al.(2011)Liu, Yang, et~al.]{Liu_Yang11}
W.~Liu, Y.~Yang, et~al.
\newblock Parametric or nonparametric? a parametricness index for model
  selection.
\newblock \emph{Ann. Statist.}, 39\penalty0 (4):\penalty0 2074--2102, 2011.

\bibitem[Lorraine et~al.(2019)Lorraine, Vicol, and
  Duvenaud]{Lorraine_Vicol_Duvenaud2019}
J.~Lorraine, P.~Vicol, and D.~Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock \emph{arXiv preprint arXiv:1911.02590}, 2019.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{MacLaurin_Duvenaud_Adams15}
D.~Maclaurin, D.~Duvenaud, and Ryan Adams.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{ICML}, volume~37, pages 2113--2122, 2015.

\bibitem[Mairal et~al.(2012)Mairal, Bach, and Ponce]{Mairal_Bach_Ponce12}
J.~Mairal, F.~Bach, and J.~Ponce.
\newblock Task-driven dictionary learning.
\newblock \emph{{IEEE} Trans. Pattern Anal. Mach. Intell.}, 34\penalty0
  (4):\penalty0 791--804, 2012.

\bibitem[Massias et~al.(2018)Massias, Gramfort, and
  Salmon]{Massias_Gramfort_Salmon18}
M.~Massias, A.~Gramfort, and J.~Salmon.
\newblock {Celer: a Fast Solver for the Lasso with Dual Extrapolation}.
\newblock In \emph{ICML}, volume~80, pages 3315--3324, 2018.

\bibitem[Massias et~al.(2019)Massias, Vaiter, Gramfort, and
  Salmon]{Massias_Vaiter_Gramfort_Salmon19}
M.~Massias, S.~Vaiter, A.~Gramfort, and J.~Salmon.
\newblock Dual extrapolation for sparse generalized linear models.
\newblock \emph{arXiv preprint arXiv:1907.05830}, 2019.

\bibitem[Niculae and Blondel(2017)]{Niculae_Blondel17}
V.~Niculae and M.~Blondel.
\newblock A regularized framework for sparse and structured neural attention.
\newblock In \emph{Advances in neural information processing systems}, pages
  3338--3348, 2017.

\bibitem[Nocedal and Wright(2006)]{Nocedal_Wright06}
J.~Nocedal and S.~J. Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer Series in Operations Research and Financial Engineering.
  Springer, New York, second edition, 2006.

\bibitem[Pedregosa(2016)]{Pedregosa16}
F.~Pedregosa.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{ICML}, 2016.

\bibitem[Ramani et~al.(2008)Ramani, Blu, and Unser]{Ramani_Blu_Unser08}
S.~Ramani, T.~Blu, and M.~Unser.
\newblock Monte-{C}arlo {SURE}: a black-box optimization of regularization
  parameters for general denoising algorithms.
\newblock \emph{{IEEE} Trans. Image Process.}, 17\penalty0 (9):\penalty0
  1540--1554, 2008.

\bibitem[Seeger(2008)]{Seeger08}
M.~W. Seeger.
\newblock Cross-validation optimization for large scale structured
  classification kernel methods.
\newblock \emph{J. Mach. Learn. Res.}, 9:\penalty0 1147--1178, 2008.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and
  Adams]{Snoek_Larochelle_Ryan12}
J.~Snoek, H.~Larochelle, and R.~P. Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in neural information processing systems}, 2012.

\bibitem[Soubies et~al.(2017)Soubies, Blanc-F{\'e}raud, and
  Aubert]{Soubies_Blanc-FeraudAubert16}
E.~Soubies, L.~Blanc-F{\'e}raud, and G.~Aubert.
\newblock A unified view of exact continuous penalties for {$\ell_2$-$\ell_0$}
  minimization.
\newblock \emph{SIAM J. Optim.}, 27\penalty0 (3):\penalty0 2034--2060, 2017.

\bibitem[Stein(1981)]{Stein81}
C.~M. Stein.
\newblock Estimation of the mean of a multivariate normal distribution.
\newblock \emph{Ann. Statist.}, 9\penalty0 (6):\penalty0 1135--1151, 1981.

\bibitem[Stone and Ramer(1965)]{Stone_Ramer65}
L.~R.~A. Stone and J.C. Ramer.
\newblock {Estimating WAIS IQ from Shipley Scale scores: Another
  cross-validation}.
\newblock \emph{Journal of clinical psychology}, 21\penalty0 (3):\penalty0
  297--297, 1965.

\bibitem[Sun et~al.(2019)Sun, Jeong, Nutini, and
  Schmidt]{Sun_Jeong_Nutini_Schmidt2019}
Y.~Sun, H.~Jeong, J.~Nutini, and M.~Schmidt.
\newblock Are we there yet? manifold identification of gradient-related
  proximal methods.
\newblock In \emph{AISTATS}, volume~89, pages 1110--1119, 2019.

\bibitem[Tibshirani(1996)]{Tibshirani96}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 58\penalty0
  (1):\penalty0 267--288, 1996.

\bibitem[Tibshirani(2013)]{Tibshirani13}
R.~J. Tibshirani.
\newblock The lasso problem and uniqueness.
\newblock \emph{Electron. J. Stat.}, 7:\penalty0 1456--1490, 2013.

\bibitem[Tibshirani and Taylor(2011)]{Tibshirani_Taylor11}
R.~J. Tibshirani and J.~Taylor.
\newblock The solution path of the generalized lasso.
\newblock \emph{Ann. Statist.}, 39\penalty0 (3):\penalty0 1335--1371, 2011.

\bibitem[Tseng and Yun(2009)]{Tseng_Yun09}
P.~Tseng and S.~Yun.
\newblock Block-coordinate gradient descent method for linearly constrained
  nonsmooth separable optimization.
\newblock \emph{J. Optim. Theory Appl.}, 140\penalty0 (3):\penalty0 513, 2009.

\bibitem[Vaiter et~al.(2013)Vaiter, Deledalle, Peyr{\'e}, Dossal, and
  Fadili]{Vaiter_Deledalle_Peyre_Dossal_Fadili13}
S.~Vaiter, \CA. Deledalle, G.~Peyr{\'e}, C.~Dossal, and J.~Fadili.
\newblock Local behavior of sparse analysis regularization: Applications to
  risk estimation.
\newblock \emph{Appl. Comput. Harmon. Anal.}, 35\penalty0 (3):\penalty0
  433--451, 2013.

\bibitem[Vaiter et~al.(2017)Vaiter, Deledalle, Peyré, Fadili, and
  Dossal]{Vaiter_Deledalle_Peyre_Fadili_Dossal17}
S.~Vaiter, C.-A. Deledalle, G.~Peyré, J.~M. Fadili, and C.~Dossal.
\newblock The degrees of freedom of partly smooth regularizers.
\newblock \emph{Ann. Inst. Stat. Math.}, 69\penalty0 (4):\penalty0 791--832,
  2017.

\bibitem[Wu et~al.(2019)Wu, Guo, Li, and Zhang]{Wu_Guo_Li_Zhang19}
K.~Wu, Y.~Guo, Z.~Li, and C.~Zhang.
\newblock Sparse coding with gated learned ista.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Xin et~al.(2016)Xin, Wang, Gao, Wipf, and
  Wang]{Xin_Wang_Gao_Wipf_Wang16}
B.~Xin, Y.~Wang, W.~Gao, D.~Wipf, and B.~Wang.
\newblock Maximal sparsity with deep networks?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4340--4348, 2016.

\bibitem[Yuan and Lin(2006)]{Yuan_Lin06}
M.~Yuan and Y.~Lin.
\newblock Model selection and estimation in regression with grouped variables.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 68\penalty0
  (1):\penalty0 49--67, 2006.

\bibitem[Zhang(2010)]{Zhang10}
{\CH}.~Zhang.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{Ann. Statist.}, 38\penalty0 (2):\penalty0 894--942, 2010.

\bibitem[Zou(2006)]{Zou06}
H.~Zou.
\newblock The adaptive lasso and its oracle properties.
\newblock \emph{J. Amer. Statist. Assoc.}, 101\penalty0 (476):\penalty0
  1418--1429, 2006.

\bibitem[Zou and Hastie(2005)]{Zou_Hastie05}
H.~Zou and T.~J. Hastie.
\newblock Regularization and variable selection via the elastic net.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 67\penalty0
  (2):\penalty0 301--320, 2005.

\bibitem[Zou et~al.(2007)Zou, Hastie, and Tibshirani]{Zou_Hastie_Tibshirani07}
H.~Zou, T.~J. Hastie, and R.~Tibshirani.
\newblock On the ``degrees of freedom'' of the lasso.
\newblock \emph{Ann. Statist.}, 35\penalty0 (5):\penalty0 2173--2192, 2007.

\end{thebibliography}
