\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aguilera et~al.(2014)Aguilera, Delgado, Dolz, and
  Aguero]{ref:aguilera2014quadratic}
Aguilera, R., Delgado, R., Dolz, D., and Aguero, J.
\newblock Quadratic {MPC} with $\ell_0$-input constraint.
\newblock \emph{IFAC Proceedings Volumes}, 19:\penalty0 10888--10893, 2014.

\bibitem[Atamturk \& Gomez(2020)Atamturk and Gomez]{ref:atamturk2020safe}
Atamturk, A. and Gomez, A.
\newblock Safe screening rules for $\ell_0$-regression from perspective
  relaxations.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  421--430, 2020.

\bibitem[Baye \& Parker(1984)Baye and Parker]{ref:baye1984combining}
Baye, M.~R. and Parker, D.~F.
\newblock Combining ridge and principal component regression:a money demand
  illustration.
\newblock \emph{Communications in Statistics - Theory and Methods}, 13\penalty0
  (2):\penalty0 197--205, 1984.

\bibitem[Beck \& Eldar(2012)Beck and Eldar]{ref:beck2012sparsity}
Beck, A. and Eldar, Y.
\newblock Sparsity constrained nonlinear optimization: Optimality conditions
  and algorithms.
\newblock \emph{SIAM Journal on Optimization}, 23:\penalty0 1480–1509, 2012.

\bibitem[Beck \& Hallak(2015)Beck and Hallak]{ref:beck2015on}
Beck, A. and Hallak, N.
\newblock On the minimization over sparse symmetric sets: Projections,
  optimality conditions, and algorithms.
\newblock \emph{Mathematics of Operations Research}, 41:\penalty0 196--223,
  2015.

\bibitem[Bertsimas \& Cory-Wright(2020)Bertsimas and
  Cory-Wright]{ref:bertsimas2020scalable}
Bertsimas, D. and Cory-Wright, R.
\newblock A scalable algorithm for sparse portfolio selection.
\newblock \emph{arXiv preprint arXiv:1811.00138}, 2020.

\bibitem[Bertsimas \& van Parys(2017)Bertsimas and van
  Parys]{ref:bertsimas2017sparse}
Bertsimas, D. and van Parys, B.
\newblock Sparse high-dimensional regression: Exact scalable algorithms and
  phase transitions.
\newblock \emph{The Annals of Statistics}, 48:\penalty0 300--323, 2017.

\bibitem[Bertsimas et~al.(2016)Bertsimas, King, and
  Mazumder]{ref:bertsimas2016best}
Bertsimas, D., King, A., and Mazumder, R.
\newblock Best subset selection via a modern optimization lens.
\newblock \emph{The Annals of Statistics}, 44\penalty0 (2):\penalty0 813--852,
  2016.

\bibitem[Bertsimas et~al.(2017)Bertsimas, Copenhaver, and
  Mazumder]{ref:bertsimas2017trimmed}
Bertsimas, D., Copenhaver, M.~S., and Mazumder, R.
\newblock The trimmed {L}asso: Sparsity and robustness.
\newblock \emph{arXiv preprint arXiv:1409.8033}, 2017.

\bibitem[Bertsimas et~al.(2020{\natexlab{a}})Bertsimas, Cory-Wright, and
  Pauphilet]{ref:bertsimas2020solving}
Bertsimas, D., Cory-Wright, R., and Pauphilet, J.
\newblock Solving large-scale sparse {PCA} to certifiable (near) optimality.
\newblock \emph{arXiv preprint arXiv:2005.05195}, 2020{\natexlab{a}}.

\bibitem[Bertsimas et~al.(2020{\natexlab{b}})Bertsimas, Cory-Wright, and
  Pauphilet]{ref:bertsimas2020unified}
Bertsimas, D., Cory-Wright, R., and Pauphilet, J.
\newblock A unified approach to mixed-integer optimization: Nonlinear
  formulations and scalable algorithms.
\newblock \emph{arXiv preprint arXiv:1907.02109}, 2020{\natexlab{b}}.

\bibitem[Boyd et~al.(2003)Boyd, Xiao, and Mutapcic]{ref:boyd2003subgradient}
Boyd, S., Xiao, L., and Mutapcic, A.
\newblock Subgradient methods notes, {S}tanford {U}niversity.
\newblock \url{web.stanford.edu/class/ee392o/subgrad_method.pdf}, 2003.

\bibitem[Dedieu et~al.(2020)Dedieu, Hazimeh, and
  Mazumder]{ref:dedieu2020learning}
Dedieu, A., Hazimeh, H., and Mazumder, R.
\newblock Learning sparse classifiers: Continuous and mixed integer
  optimization perspectives.
\newblock \emph{arXiv preprint arXiv:2001.06471}, 2020.

\bibitem[Dua \& Graff(2017)Dua and Graff]{ref:dua2017uci}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Ghaoui \& Lebret(1997)Ghaoui and Lebret]{ref:ghaoui1997robust}
Ghaoui, L.~E. and Lebret, H.
\newblock Robust solutions to least-squares problems with uncertain data.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 18\penalty0
  (4):\penalty0 1035–1064, 1997.

\bibitem[Gomez \& Prokopyev(2018)Gomez and Prokopyev]{ref:gomez2018mixed}
Gomez, A. and Prokopyev, O.
\newblock A mixed-integer fractional optimization approach to best subset
  selection.
\newblock \emph{Optimization-online}, 2018.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and
  Friedman]{ref:hastie2009elements}
Hastie, T., Tibshirani, R., and Friedman, J.
\newblock \emph{The Elements of Statistical Learning: Data Mining, Inference
  and Prediction}.
\newblock Springer, 2 edition, 2009.

\bibitem[Hastie et~al.(2017)Hastie, Tibshirani, and
  Tibshirani]{ref:hastie2017extended}
Hastie, T., Tibshirani, R., and Tibshirani, R.~J.
\newblock Extended comparisons of best subset selection, forward stepwise
  selection, and the lasso.
\newblock \emph{arXiv preprint arXiv:1707.08692}, 2017.

\bibitem[Hazimeh et~al.(2020)Hazimeh, Mazumder, and
  Saab]{ref:hazimeh2020sparse}
Hazimeh, H., Mazumder, R., and Saab, A.
\newblock Sparse regression at scale: Branch-and-bound rooted in first-order
  optimization.
\newblock \emph{arXiv preprint arXiv:2004.06152}, 2020.

\bibitem[Hoerl \& Kennard(1970)Hoerl and Kennard]{ref:hoerl1970ridge}
Hoerl, A.~E. and Kennard, R.~W.
\newblock Ridge regression: Biased estimation for nonorthogonal problems.
\newblock \emph{Technometrics}, 12:\penalty0 55--67, 1970.

\bibitem[Liu et~al.(2003)Liu, Kuang, Gong, and Hou]{ref:liu2003principal}
Liu, R., Kuang, J., Gong, Q., and Hou, X.
\newblock Principal component regression analysis with {SPSS}.
\newblock \emph{Computer Methods and Programs in Biomedicine}, 71\penalty0
  (2):\penalty0 141 -- 147, 2003.

\bibitem[L\"ofberg(2004)]{ref:lofberg2004yalmip}
L\"ofberg, J.
\newblock {YALMIP}: {A} toolbox for modeling and optimization in {MATLAB}.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  pp.\  284--289, 2004.

\bibitem[Mazumder et~al.(2020)Mazumder, Radchenko, and
  Dedieu]{ref:mazumder2020subset}
Mazumder, R., Radchenko, P., and Dedieu, A.
\newblock Subset selection with shrinkage: {S}parse linear modeling when the
  {SNR} is low.
\newblock \emph{arXiv preprint arXiv:1708.03288}, 2020.

\bibitem[Miller(2002)]{ref:miller2002subset}
Miller, A.
\newblock \emph{Subset Selection in Regression}.
\newblock CRC Press, 2002.

\bibitem[{MOSEK ApS}(2019)]{mosek}
{MOSEK ApS}.
\newblock \emph{The MOSEK optimization toolbox. Version 9.2.}, 2019.
\newblock URL \url{https://docs.mosek.com/9.2/cmdtools/index.html}.

\bibitem[Natarajan(1995)]{ref:natarajan1995sparse}
Natarajan, B.~K.
\newblock Sparse approximate solutions to linear systems.
\newblock \emph{SIAM Journal on Computing}, 24\penalty0 (2):\penalty0 227--234,
  1995.

\bibitem[Nesterov(2003)]{ref:nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer, 2003.

\bibitem[Næs \& Martens(1988)Næs and Martens]{ref:naes1988principal}
Næs, T. and Martens, H.
\newblock Principal component regression in {NIR} analysis: Viewpoints,
  background details and selection of components.
\newblock \emph{Journal of Chemometrics}, 2\penalty0 (2):\penalty0 155--167,
  1988.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
Ribeiro, M.~T., Singh, S., and Guestrin, C.
\newblock {W}hy should {I} trust you? {E}xplaining the predictions of any
  classifier.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  1135--1144, 2016.

\bibitem[Rockafellar \& Wets(2009)Rockafellar and
  Wets]{ref:rockafellar2009variational}
Rockafellar, R.~T. and Wets, R. J.-B.
\newblock \emph{Variational {A}nalysis}.
\newblock Springer, 2009.

\bibitem[Tibshirani(1996)]{ref:tibshirani1996regression}
Tibshirani, R.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society, Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[Udell \& Townsend(2019)Udell and Townsend]{udell2019big}
Udell, M. and Townsend, A.
\newblock Why are big data matrices approximately low rank?
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 1\penalty0
  (1):\penalty0 144--160, 2019.

\bibitem[Xie \& Deng(2020)Xie and Deng]{ref:xie2020scalable}
Xie, W. and Deng, X.
\newblock Scalable algorithms for the sparse ridge regression.
\newblock \emph{arXiv preprint arXiv:1806.03756}, 2020.

\bibitem[Yuan et~al.(2020)Yuan, Shen, and Zheng]{ref:ganzhao2020block}
Yuan, G., Shen, L., and Zheng, W.-S.
\newblock A block decomposition algorithm for sparse optimization.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  275–285, 2020.

\bibitem[Zou \& Hastie(2005)Zou and Hastie]{ref:zou2005regularization}
Zou, H. and Hastie, T.
\newblock Regularization and variable selection via the elastic net.
\newblock \emph{Journal of the Royal Statistical Society, Series B
  (Methodological)}, 67:\penalty0 301--320, 2005.

\end{thebibliography}
