\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Berg and Neuhaus(1991)]{berg1991multicanonical}
Bernd~A Berg and Thomas Neuhaus.
\newblock Multicanonical algorithms for first order phase transitions.
\newblock \emph{Physics Letters B}, 267\penalty0 (2):\penalty0 249--253, 1991.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Dalalyan(2017)]{dalalyan2017theoretical}
Arnak~S Dalalyan.
\newblock Theoretical guarantees for approximate sampling from smooth and log-concave densities.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 79\penalty0 (3):\penalty0 651--676, 2017.

\bibitem[Deng et~al.(2020{\natexlab{a}})Deng, Feng, Gao, Liang, and Lin]{deng2020non}
Wei Deng, Qi~Feng, Liyao Gao, Faming Liang, and Guang Lin.
\newblock Non-convex learning via replica exchange stochastic gradient mcmc.
\newblock In \emph{International Conference on Machine Learning}, pages 2474--2483. PMLR, 2020{\natexlab{a}}.

\bibitem[Deng et~al.(2020{\natexlab{b}})Deng, Lin, and Liang]{deng2020contour}
Wei Deng, Guang Lin, and Faming Liang.
\newblock A contour stochastic gradient langevin dynamics algorithm for simulations of multi-modal distributions.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 15725--15736, 2020{\natexlab{b}}.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Goshvadi et~al.(2023)Goshvadi, Sun, Liu, Nova, Zhang, Grathwohl, Schuurmans, and Dai]{goshvadi2023discs}
Katayoon Goshvadi, Haoran Sun, Xingchao Liu, Azade Nova, Ruqi Zhang, Will~Sussman Grathwohl, Dale Schuurmans, and Hanjun Dai.
\newblock Discs: A benchmark for discrete sampling.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.

\bibitem[Grathwohl et~al.(2021)Grathwohl, Swersky, Hashemi, Duvenaud, and Maddison]{grathwohl2021gwg}
Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, and Chris~J. Maddison.
\newblock Oops {I} took {A} gradient: Scalable sampling for discrete distributions.
\newblock \emph{CoRR}, abs/2102.04509, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.04509}.

\bibitem[Hinton(2002)]{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800, 2002.

\bibitem[Jones(2004)]{jones2004markov}
Galin~L Jones.
\newblock On the markov chain central limit theorem.
\newblock 2004.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Marinari and Parisi(1992)]{marinari1992simulated}
Enzo Marinari and Giorgio Parisi.
\newblock Simulated tempering: a new monte carlo scheme.
\newblock \emph{Europhysics letters}, 19\penalty0 (6):\penalty0 451, 1992.

\bibitem[Neal(2001)]{neal2001annealed}
Radford~M Neal.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and computing}, 11:\penalty0 125--139, 2001.

\bibitem[Rhodes and Gutmann(2022)]{rhodes2022enhanced}
Benjamin Rhodes and Michael Gutmann.
\newblock Enhanced gradient-based mcmc in discrete spaces, 2022.

\bibitem[Ruder(2016)]{ruder2016overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Sansone(2022)]{sansone2022lsb}
Emanuele Sansone.
\newblock Lsb: Local self-balancing mcmc in discrete spaces.
\newblock In \emph{International Conference on Machine Learning}, pages 19205--19220. PMLR, 2022.

\bibitem[Sun et~al.(2021)Sun, Dai, Xia, and Ramamurthy]{sun2021path}
Haoran Sun, Hanjun Dai, Wei Xia, and Arun Ramamurthy.
\newblock Path auxiliary proposal for mcmc in discrete space.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Sun et~al.(2022)Sun, Dai, and Schuurmans]{sun2022optimal}
Haoran Sun, Hanjun Dai, and Dale Schuurmans.
\newblock Optimal scaling for locally balanced proposals in discrete spaces, 2022.

\bibitem[Sun et~al.(2023{\natexlab{a}})Sun, Dai, Sutton, Schuurmans, and Dai]{sun2023anyscale}
Haoran Sun, Bo~Dai, Charles Sutton, Dale Schuurmans, and Hanjun Dai.
\newblock Any-scale balanced samplers for discrete space.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=lEkl0jdSb7B}.

\bibitem[Sun et~al.(2023{\natexlab{b}})Sun, Dai, Dai, Zhou, and Schuurmans]{sun2023discrete}
Haoran Sun, Hanjun Dai, Bo~Dai, Haomin Zhou, and Dale Schuurmans.
\newblock Discrete langevin samplers via wasserstein gradient flow.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 6290--6313. PMLR, 2023{\natexlab{b}}.

\bibitem[Swendsen and Wang(1986)]{swendsen1986replica}
Robert~H Swendsen and Jian-Sheng Wang.
\newblock Replica monte carlo simulation of spin-glasses.
\newblock \emph{Physical review letters}, 57\penalty0 (21):\penalty0 2607, 1986.

\bibitem[Swendsen and Wang(1987)]{swendsen1987nonuniversal}
Robert~H Swendsen and Jian-Sheng Wang.
\newblock Nonuniversal critical dynamics in monte carlo simulations.
\newblock \emph{Physical review letters}, 58\penalty0 (2):\penalty0 86, 1987.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tijmen Tieleman.
\newblock Training restricted boltzmann machines using approximations to the likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine learning}, pages 1064--1071, 2008.

\bibitem[Vats et~al.(2019)Vats, Flegal, and Jones]{vats2019multivariate}
Dootika Vats, James~M Flegal, and Galin~L Jones.
\newblock Multivariate output analysis for markov chain monte carlo.
\newblock \emph{Biometrika}, 106\penalty0 (2):\penalty0 321--337, 2019.

\bibitem[Wolff(1989)]{wolff1989collective}
Ulli Wolff.
\newblock Collective monte carlo updating for spin systems.
\newblock \emph{Physical Review Letters}, 62\penalty0 (4):\penalty0 361, 1989.

\bibitem[Xiang et~al.(2023)Xiang, Zhu, Lei, Xu, and Zhang]{xiang2023efficient}
Yue Xiang, Dongyao Zhu, Bowen Lei, Dongkuan Xu, and Ruqi Zhang.
\newblock Efficient informed proposals for discrete distributions via newtonâ€™s series approximation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 7288--7310. PMLR, 2023.

\bibitem[Zanella(2017)]{zanella2017informed}
Giacomo Zanella.
\newblock Informed proposals for local mcmc in discrete spaces, 2017.

\bibitem[Zhang et~al.(2020)Zhang, Li, Zhang, Chen, and Wilson]{Zhang2020Cyclical}
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew~Gordon Wilson.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkeS1RVtPS}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Liu, and Liu]{zhang2022langevin}
Ruqi Zhang, Xingchao Liu, and Qiang Liu.
\newblock A langevin-like sampler for discrete distributions.
\newblock In \emph{International Conference on Machine Learning}, pages 26375--26396. PMLR, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Liu, and Liu]{zhang2022langevinlike}
Ruqi Zhang, Xingchao Liu, and Qiang Liu.
\newblock A langevin-like sampler for discrete distributions, 2022{\natexlab{b}}.

\bibitem[Ziyin et~al.(2021)Ziyin, Li, Simon, and Ueda]{ziyin2021sgd}
Liu Ziyin, Botao Li, James~B Simon, and Masahito Ueda.
\newblock Sgd can converge to local maxima.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\end{thebibliography}
