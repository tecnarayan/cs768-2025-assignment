\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66. PMLR, 2020.

\bibitem[Bedi et~al.(2022)Bedi, Chakraborty, Parayil, Sadler, Tokekar, and
  Koppel]{bedi2022hidden}
Bedi, A.~S., Chakraborty, S., Parayil, A., Sadler, B.~M., Tokekar, P., and
  Koppel, A.
\newblock On the hidden biases of policy mirror ascent in continuous action
  spaces.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1716--1731. PMLR, 2022.

\bibitem[Bertsekas(2011)]{bertsekas2011approximate}
Bertsekas, D.~P.
\newblock Approximate policy iteration: A survey and some new methods.
\newblock \emph{Journal of Control Theory and Applications}, 9\penalty0
  (3):\penalty0 310--335, 2011.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{bhandari2018}
Bhandari, J., Russo, D., and Singal, R.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock \emph{CoRR}, abs/1806.02450, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.02450}.

\bibitem[Borkar \& Konda(1997)Borkar and Konda]{borkar1997actor}
Borkar, V.~S. and Konda, V.~R.
\newblock The actor-critic algorithm as multi-time-scale stochastic
  approximation.
\newblock \emph{Sadhana}, 22:\penalty0 525--543, 1997.

\bibitem[Borkar \& Meyn(2000)Borkar and Meyn]{borkar2000ode}
Borkar, V.~S. and Meyn, S.~P.
\newblock The ode method for convergence of stochastic approximation and
  reinforcement learning.
\newblock \emph{SIAM Journal on Control and Optimization}, 38\penalty0
  (2):\penalty0 447--469, 2000.

\bibitem[Chen \& Zhao(2022)Chen and Zhao]{chen2022finite}
Chen, X. and Zhao, L.
\newblock Finite-time analysis of single-timescale actor-critic, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.09921}.

\bibitem[Dorfman \& Levy(2022)Dorfman and Levy]{dorfman2022}
Dorfman, R. and Levy, K.~Y.
\newblock Adapting to mixing time in stochastic optimization with {M}arkovian
  data.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  5429--5446. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/dorfman22a.html}.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, Johansson, and Jordan]{duchi2012}
Duchi, J.~C., Agarwal, A., Johansson, M., and Jordan, M.~I.
\newblock Ergodic mirror descent.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1549--1578, 2012.
\newblock \doi{10.1137/110836043}.
\newblock URL \url{https://doi.org/10.1137/110836043}.

\bibitem[Gu et~al.(2016)Gu, Holly, Lillicrap, and Levine]{gu2016deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation.
\newblock \emph{arXiv preprint arXiv:1610.00633}, 1, 2016.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Heaton et~al.(2016)Heaton, Polson, and Witte]{heaton2016deep}
Heaton, J., Polson, N.~G., and Witte, J.
\newblock Deep portfolio theory.
\newblock \emph{arXiv preprint arXiv:1605.07230}, 2016.

\bibitem[Hinton et~al.(2006)Hinton, Osindero, and Teh]{hinton2006fast}
Hinton, G.~E., Osindero, S., and Teh, Y.-W.
\newblock A fast learning algorithm for deep belief nets.
\newblock \emph{Neural computation}, 18\penalty0 (7):\penalty0 1527--1554,
  2006.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{Konda2000}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock In Solla, S., Leen, T., and M\"{u}ller, K. (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~12. MIT Press, 1999.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf}.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2017imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Communications of the ACM}, 60\penalty0 (6):\penalty0 84--90,
  2017.

\bibitem[Kumar et~al.(2019)Kumar, Koppel, and Ribeiro]{kumar2019sample}
Kumar, H., Koppel, A., and Ribeiro, A.
\newblock On the sample complexity of actor-critic method for reinforcement
  learning with function approximation.
\newblock \emph{arXiv preprint arXiv:1910.08412}, 2019.

\bibitem[Leahy et~al.(2022)Leahy, Kerimkulov, Siska, and
  Szpruch]{leahy2022convergence}
Leahy, J.-M., Kerimkulov, B., Siska, D., and Szpruch, L.
\newblock Convergence of policy gradient for entropy regularized mdps with
  neural network approximation in the mean-field regime.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12222--12252. PMLR, 2022.

\bibitem[Levin \& Peres(2017)Levin and Peres]{levin2017markov}
Levin, D.~A. and Peres, Y.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Li(2019)]{li2019reinforcement}
Li, Y.
\newblock Reinforcement learning applications.
\newblock \emph{arXiv preprint arXiv:1908.06973}, 2019.

\bibitem[Liu et~al.(2021)Liu, Ho, Wang, Gao, Jin, and Zhang]{liu2021federated}
Liu, M., Ho, S., Wang, M., Gao, L., Jin, Y., and Zhang, H.
\newblock Federated learning meets natural language processing: A survey.
\newblock \emph{arXiv preprint arXiv:2107.12603}, 2021.

\bibitem[McAfee et~al.(2012)McAfee, Brynjolfsson, Davenport, Patil, and
  Barton]{mcafee2012big}
McAfee, A., Brynjolfsson, E., Davenport, T.~H., Patil, D., and Barton, D.
\newblock Big data: the management revolution.
\newblock \emph{Harvard business review}, 90\penalty0 (10):\penalty0 60--68,
  2012.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6820--6829. PMLR, 2020.

\bibitem[Melo et~al.(2008)Melo, Meyn, and Ribeiro]{melo2008analysis}
Melo, F.~S., Meyn, S.~P., and Ribeiro, M.~I.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  664--671, 2008.

\bibitem[Mitrophanov(2005)]{mitrophanov2005sensitivity}
Mitrophanov, A.~Y.
\newblock Sensitivity and convergence of uniformly ergodic markov chains.
\newblock \emph{Journal of Applied Probability}, 42\penalty0 (4):\penalty0
  1003--1014, 2005.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937. PMLR, 2016.

\bibitem[Nagaraj et~al.(2020)Nagaraj, Wu, Bresler, Jain, and
  Netrapalli]{bresler2020}
Nagaraj, D., Wu, X., Bresler, G., Jain, P., and Netrapalli, P.
\newblock Least squares regression with markovian data: Fundamental limits and
  algorithms.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/c22abfa379f38b5b0411bc11fa9bf92f-Abstract.html}.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and
  Restelli]{papini2018stochastic}
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., and Restelli, M.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{International conference on machine learning}, pp.\
  4026--4035. PMLR, 2018.

\bibitem[Pirotta et~al.(2013)Pirotta, Restelli, and
  Bascetta]{pirotta2013adaptive}
Pirotta, M., Restelli, M., and Bascetta, L.
\newblock Adaptive step-size for policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Pirotta et~al.(2015)Pirotta, Restelli, and
  Bascetta]{pirotta2015policy}
Pirotta, M., Restelli, M., and Bascetta, L.
\newblock Policy gradient in lipschitz markov decision processes.
\newblock \emph{Machine Learning}, 100:\penalty0 255--283, 2015.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Qiu et~al.(2021{\natexlab{a}})Qiu, Yang, Ye, and Wang]{qiu2021}
Qiu, S., Yang, Z., Ye, J., and Wang, Z.
\newblock On finite-time convergence of actor-critic algorithm.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  2\penalty0 (2):\penalty0 652--664, 2021{\natexlab{a}}.
\newblock \doi{10.1109/JSAIT.2021.3078754}.

\bibitem[Qiu et~al.(2021{\natexlab{b}})Qiu, Yang, Ye, and Wang]{qiu2021finite}
Qiu, S., Yang, Z., Ye, J., and Wang, Z.
\newblock On finite-time convergence of actor-critic algorithm.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  2\penalty0 (2):\penalty0 652--664, 2021{\natexlab{b}}.

\bibitem[Riemer et~al.(2021)Riemer, Raparthy, Cases, Subbaraj, Touzel, and
  Rish]{riemer2021continual}
Riemer, M., Raparthy, S.~C., Cases, I., Subbaraj, G., Touzel, M.~P., and Rish,
  I.
\newblock Continual learning in environments with polynomial mixing times.
\newblock \emph{arXiv preprint arXiv:2112.07066}, 2021.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sutton(1988)]{sutton1988}
Sutton, R.
\newblock Learning to predict by the method of temporal differences.
\newblock \emph{Machine Learning}, 3:\penalty0 9--44, 08 1988.
\newblock \doi{10.1007/BF00115009}.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Tadi{\'c}(2001)]{tadic2001convergence}
Tadi{\'c}, V.
\newblock On the convergence of temporal-difference learning with linear
  function approximation.
\newblock \emph{Machine learning}, 42:\penalty0 241--267, 2001.

\bibitem[Tsitsiklis \& Van~Roy(1997)Tsitsiklis and Van~Roy]{roy1997}
Tsitsiklis, J. and Van~Roy, B.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.
\newblock \doi{10.1109/9.580874}.

\bibitem[Wang et~al.(2019)Wang, Cai, Yang, and Wang]{wang2019neural}
Wang, L., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Wu et~al.(2020)Wu, Zhang, Xu, and Gu]{wu2020finite}
Wu, Y.~F., Zhang, W., Xu, P., and Gu, Q.
\newblock A finite-time analysis of two time-scale actor-critic methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17617--17628, 2020.

\bibitem[Xu et~al.(2020{\natexlab{a}})Xu, Gao, and Gu]{xu2020improved}
Xu, P., Gao, F., and Gu, Q.
\newblock An improved convergence analysis of stochastic variance-reduced
  policy gradient.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  541--551.
  PMLR, 2020{\natexlab{a}}.

\bibitem[Xu et~al.(2020{\natexlab{b}})Xu, Wang, and Liang]{xu2020improving}
Xu, T., Wang, Z., and Liang, Y.
\newblock Improving sample complexity bounds for (natural) actor-critic
  algorithms.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA,
  2020{\natexlab{b}}. Curran Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Zhang et~al.(2020)Zhang, Koppel, Zhu, and Basar]{zhang2020global}
Zhang, K., Koppel, A., Zhu, H., and Basar, T.
\newblock Global convergence of policy gradient methods to (almost) locally
  optimal policies.
\newblock \emph{SIAM Journal on Control and Optimization}, 58\penalty0
  (6):\penalty0 3586--3612, 2020.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Zou, S., Xu, T., and Liang, Y.
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
