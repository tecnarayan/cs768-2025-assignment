@STRING{CROWNCOM = {Proc. Cognitive Radio Oriented Wireless Netw. Commun.}}
@STRING{SSP = {Proc. Workshop Stat. Sig. Process.}}
@STRING{EUSIPCO = {Proc. European  Sig. Process. Conf.}}
@STRING{ASILOMAR = {Proc. Asilomar Conf. Sig., Syst., Comput.}}
@STRING{CAMSAP = {Proc. IEEE Int. Workshop Comput. Advances Multi-Sensor 
Adaptive Process.}}
@STRING{CIP = {Proc. Cognitive Inf. Process.}}
@STRING{dke = {Data \& Knowledge Engineering}}
@STRING{DYSPAN = {Proc. IEEE Int. Symp. New Frontiers Dynamic Spectrum Access 
Netw.}}
@STRING{ELL = {Electron. Lett.}}
@STRING{ETT = {Euro. Trans. Telecomms.}}
@STRING{GLOBECOM = {Proc. IEEE Global Commun. Conf.}}
@STRING{ICASSP = {Proc. IEEE Int. Conf. Acoust., Speech, Sig. Process.}}
@STRING{GLOBALSIP = {Global Conf. Sig. Inf. Process.}}
@STRING{IEEEAP = {IEEE Trans. Antennas Propag.}}
@STRING{IEEECM = {IEEE Commun. Mag.}}
@STRING{IEEEIP = {IEEE Trans. Image Process.}}
@STRING{IEEEJSAC = {IEEE J. Sel. Areas Commun.}}
@STRING{IEEEJSTSP = {IEEE J. Sel. Topics Sig. Process.}}
@STRING{IEEELCOM = {IEEE Commun. Lett.}}
@STRING{IEEEPAMI = {IEEE Trans. Pattern Anal. Mach. Intel.}}
@STRING{IEEESENSORS = {IEEE Sensors J.}}
@STRING{IEEESPL = {IEEE Sig. Process. Lett.}}
@STRING{IEEESPM = {IEEE Sig. Process. Mag.}}
@STRING{IEEECM = {IEEE Commun. Mag.}}
@STRING{IEEETASSP = {IEEE Trans. Acoust., Speech, Sig. Process.}}
@STRING{IEEETCOM = {IEEE Trans. Commun.}}
@STRING{IEEETIT = {IEEE Trans. Inf. Theory}}
@STRING{IEEETIP = {IEEE Trans. Image Process.}}
@STRING{IEEETSP = {IEEE Trans. Sig. Process.}}
@STRING{IEEETSPIN = {IEEE Trans. Sig. and Info. Process. over Net.}}
@STRING{IEEETSIPN = {IEEE Trans. Sig. Info. Process. Netw.}}
@STRING{IEEETVT = {IEEE Trans. Veh. Technol.}}
@STRING{IEEETWC = {IEEE Trans. Wireless Commun.}}
@STRING{IEEETMC = {IEEE Trans. Mobile Comput.}}
@STRING{IEEEWC = {IEEE Wireless Commun.}}
@STRING{IETC = {IET Commun.}}
@STRING{ITA = {Proc. Inf. Theory Appl. Workshop}}
@STRING{joop = {J. Object-Oriented Programming}}
@STRING{JSSC = {IEEE J. Solid-State Circuits}}
@STRING{lnai = {Lecture Notes Artificial Intelligence}}
@STRING{lncs = {Lecture Notes Comput. Sci.}}
@STRING{mibi = {Medizinische Informatik und Bioinformatik}}
@STRING{PROCIEEE = {Proc. IEEE}}
@STRING{tkde = {IEEE Trans. Knowledge and Data Engineering}}
@STRING{tods = {ACM Trans. Database Syst.}}
@STRING{tois = {ACM Trans. Inf. Syst.}}

@STRING{JMLR = {J. Mach. Learn. Res.} }
@STRING{ICML = {Proc. Int. Conf. Machine Learning} }
@STRING{AAAI = {Proc. AAAI Conf. Artif. Intel.} }
@STRING{AISTATS = {Proc. Int. Conf. Artif. Intel. and Stats.} }
@STRING{UAI = {Proc. Conf. Uncerntainty in Artif. Intel.} }
@STRING{ICLR = {Proc. Int. Conf. Learning Represention} }
@STRING{NIPS = {Proc. Adv.  Neural Info. Processing Systems}}
@STRING{FTML = {Foundations Trends Mach. Learn.}}
@STRING{COLT = {Proc. Annual Conf. Learning Theory}}
@STRING{CVPR = {Proc. Conf. Computer Vision and Pattern Recognition}}
@string{ACL = {Proc. Conf. Assoc. Comput. Linguist. Meet.}}
@string{EMNLP = {Proc. Conf. Empir. Methods Nat. Lang. Process.}}




@book{nesterov2004,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2004},
  publisher={Springer Science \& Business Media}
}


@inproceedings{allen2018,
  title={How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex {SGD}},
  author={Allen-Zhu, Zeyuan},
  booktitle=NIPS,
  pages={1165--1175},
  address = {Montreal, Canada},
  year={2018}
}

@inproceedings{allen2016,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle=ICML,
  pages={699--707},
  address = {New York City, NY},
  year={2016}
}

@article{bottou2018,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{robbins1951,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@book{gubner2006,
  title={Probability and random processes for electrical and computer engineers},
  author={Gubner, John A},
  year={2006},
  publisher={Cambridge University Press}
}

@article{barzilai1988,
  title={Two-point step size gradient methods},
  author={Barzilai, Jonathan and Borwein, Jonathan M},
  journal={IMA Journal of Numerical Analysis},
  volume={8},
  number={1},
  pages={141--148},
  year={1988},
  publisher={Oxford University Press}
}


@article{ghadimi2013,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
} 


%%%%%%%%%%%%%%%%%% Variance reduction %%%%%%%%%%%%%%%%%%%
@inproceedings{nguyen2017,
  title={{SARAH}: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={Proc. Intl. Conf. Machine Learning},
  address = {Sydney, Australia},
  year={2017}
}


@article{pham2019,
  title={Prox{SARAH}: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization},
  author={Pham, Nhan H and Nguyen, Lam M and Phan, Dzung T and Tran-Dinh, Quoc},
  journal={arXiv preprint arXiv:1902.05679},
  year={2019}
}

@article{li2019l2s,
  title={On the Convergence of {SARAH} and Beyond},
  author={Li, Bingcong and Ma, Meng, and Giannakis, Georgios B},
  journal={arXiv preprint arXiv:1906.02351},
  year={2019}
}


@article{xiao2014,
  title={A proximal stochastic gradient method with progressive variance reduction},
  author={Xiao, Lin and Zhang, Tong},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={2057--2075},
  year={2014},
  publisher={SIAM}
}


@inproceedings{defazio2014,
  title={{SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  address = {Montreal, Canada},
  pages={1646--1654},
  year={2014}
}


@article{shalev2013,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={Feb},
  pages={567--599},
  year={2013}
}



@inproceedings{nitanda2014,
  title={Stochastic proximal gradient descent with acceleration techniques},
  author={Nitanda, Atsushi},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  address = {Montreal, Canada},
  pages={1574--1582},
  year={2014}
}

@inproceedings{zhou2018mig,
  title={A Simple Stochastic Variance Reduced Algorithm with Fast Convergence Rates},
  author={Zhou, Kaiwen and Shang, Fanhua and Cheng, James},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={5975--5984},
  year={2018}
}


@inproceedings{agarwal2014,
  title={A lower bound for the optimization of finite sums},
  author={Agarwal, Alekh and Bottou, Leon},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={78--86},
  address = {Lille, France},
  year={2015}
}



@article{allen2014,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014}
}




@inproceedings{kulunchakov2019,
  title={Estimate Sequences for Variance-Reduced Stochastic Composite Optimization},
  author={Kulunchakov, Andrei and Mairal, Julien},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  year={2019}
}


@article{bubeck2015,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}


@article{mcmahan2010,
  title={Adaptive bound optimization for online convex optimization},
  author={McMahan, H Brendan and Streeter, Matthew},
  journal={arXiv preprint arXiv:1002.4908},
  year={2010}
}

@article{duchi2011,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}



@article{ramachandran2015,
  title={Overcoming communication delays in distributed frequency regulation},
  author={Ramachandran, Thiagarajan and Nazari, Masoud H and Grijalva, Santiago and Egerstedt, Magnus},
  journal=IEEE_J_PWRS,
  volume={31},
  number={4},
  pages={2965--2973},
  year={2015},
  publisher={IEEE}
}


@article{beck2009,
  title={A fast iterative shrinkage-thresholding algorithm for linear inverse problems},
  author={Beck, Amir and Teboulle, Marc},
  journal={SIAM journal on imaging sciences},
  volume={2},
  number={1},
  pages={183--202},
  year={2009},
  publisher={SIAM}
}

@article{nemirovsky1983,
  title={Problem complexity and method efficiency in optimization.},
  author={Nemirovsky, Arkadii Semenovich and Yudin, David Borisovich},
  year={1983}
}


@article{yang2016,
  title={Unified convergence analysis of stochastic momentum methods for convex and non-convex optimization},
  author={Yang, Tianbao and Lin, Qihang and Li, Zhe},
  journal={arXiv preprint arXiv:1604.03257},
  year={2016}
}



@article{o2015,
  title={Adaptive restart for accelerated gradient schemes},
  author={O{'}donoghue, Brendan and Candes, Emmanuel},
  journal={Foundations of computational mathematics},
  volume={15},
  number={3},
  pages={715--732},
  year={2015},
  publisher={Springer}
}

@inproceedings{su2014,
  title={A differential equation for modeling {N}esterov accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={2510--2518},
  year={2014}
}

@inproceedings{krichene2015,
  title={Accelerated mirror descent in continuous and discrete time},
  author={Krichene, Walid and Bayen, Alexandre and Bartlett, Peter L},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={2845--2853},
  year={2015}
}



@inproceedings{duchi2008,
  title={Efficient projections onto the l 1-ball for learning in high dimensions},
  author={Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Chandra, Tushar},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={272--279},
  year={2008},
  organization={ACM}
}


@inproceedings{lei2019fw,
  title={Primal-Dual Block Generalized {F}rank-{W}olfe},
  author={Lei, Qi and Zhuo, Jiacheng and Caramanis, Constantine and Dhillon, Inderjit S and Dimakis, Alexandros G},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={13866--13875},
  year={2019}
}


@inproceedings{defazio2019,
  title={On the Curved Geometry of Accelerated Optimization},
  author={Defazio, Aaron},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={1764--1773},
  year={2019}
}

@article{shi2019,
  title={Acceleration via Symplectic Discretization of High-Resolution Differential Equations},
  author={Shi, Bin and Du, Simon S and Su, Weijie J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1902.03694},
  year={2019}
}


@inproceedings{zhang2018direct,
  title={Direct Runge-Kutta discretization achieves acceleration},
  author={Zhang, Jingzhao and Mokhtari, Aryan and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={3900--3909},
  year={2018}
}


@inproceedings{allen2016cd,
  title={Even faster accelerated coordinate descent using non-uniform sampling},
  author={Allen-Zhu, Zeyuan and Qu, Zheng and Richt{\'a}rik, Peter and Yuan, Yang},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={1110--1119},
  year={2016}
}


@inproceedings{nesterov1983,
  title={A method of solving a convex programming problem with convergence rate $1/k^2$},
  author={Nesterov, Y},
  booktitle={Soviet Math. Dokl},
  year = {1983},
  volume={27}
}


@inproceedings{garber2015,
  title={Faster rates for the {F}rank-{W}olfe method over strongly-convex sets},
  author={Garber, Dan and Hazan, Elad},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  year={2015}
}




@article{tibshirani1996,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}


@inproceedings{candes2006,
  title={Compressive sampling},
  author={Cand{\`e}s, Emmanuel J and others},
  booktitle={Proceedings of the international congress of mathematicians},
  volume={3},
  pages={1433--1452},
  year={2006},
  organization={Madrid, Spain}
}


@inproceedings{lacoste2015,
  title={On the global linear convergence of {F}rank-{W}olfe optimization variants},
  author={Lacoste-Julien, Simon and Jaggi, Martin},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={496--504},
  year={2015}
}


@article{damla2008,
  title={Linear convergence of a modified {F}rank-{W}olfe algorithm for computing minimum-volume enclosing ellipsoids},
  author={Damla Ahipasaoglu, S and Sun, Peng and Todd, Michael J},
  journal={Optimisation Methods and Software},
  volume={23},
  number={1},
  pages={5--19},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{levitin1966,
  title={Constrained minimization methods},
  author={Levitin, Evgeny S and Polyak, Boris T},
  journal={USSR Computational mathematics and mathematical physics},
  volume={6},
  number={5},
  pages={1--50},
  year={1966},
  publisher={Elsevier}
}


@article{polyak1964,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{dunn1979,
  title={Rates of convergence for conditional gradient algorithms near singular and nonsingular extremals},
  author={Dunn, Joseph C},
  journal={SIAM Journal on Control and Optimization},
  volume={17},
  number={2},
  pages={187--211},
  year={1979},
  publisher={SIAM}
}


@article{nesterov2015,
  title={Universal gradient methods for convex optimization problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={152},
  number={1-2},
  pages={381--404},
  year={2015},
  publisher={Springer}
}



@inproceedings{allen2017fw,
  title={Linear convergence of a {F}rank-{W}olfe type algorithm over trace-norm balls},
  author={Allen-Zhu, Zeyuan and Hazan, Elad and Hu, Wei and Li, Yuanzhi},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={6191--6200},
  year={2017}
}


@inproceedings{lei2019pdfw,
  title={Primal-Dual Block Generalized {F}rank-{W}olfe},
  author={Lei, Qi and Zhuo, Jiacheng and Caramanis, Constantine and Dhillon, Inderjit S and Dimakis, Alexandros G},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={13866--13875},
  year={2019}
}

@article{lan2016,
  title={Conditional gradient sliding for convex optimization},
  author={Lan, Guanghui and Zhou, Yi},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={2},
  pages={1379--1409},
  year={2016},
  publisher={SIAM}
}

%%%%%%%%%%%%%%%%%%


@article{bell2007,
  title={Lessons from the {N}etflix prize challenge.},
  author={Bell, Robert M and Koren, Yehuda},
  journal={SiGKDD Explorations},
  volume={9},
  number={2},
  pages={75--79},
  year={2007},
  publisher={Citeseer}
}


@inproceedings{bennett2007,
  title={The {N}etflix prize},
  author={Bennett, James and Lanning, Stan and others},
  booktitle={Proc. KDD cup and workshop},
  volume={2007},
  pages={35},
  year={2007},
  organization={New York, NY, USA.}
}


@article{fazel2002,
  title={Matrix rank minimization with applications},
  author={Fazel, Maryam},
  year={2002},
  publisher={PhD thesis, Stanford University}
}


@article{harchaoui2015,
  title={Conditional gradient algorithms for norm-regularized smooth convex optimization},
  author={Harchaoui, Zaid and Juditsky, Anatoli and Nemirovski, Arkadi},
  journal={Mathematical Programming},
  volume={152},
  number={1-2},
  pages={75--112},
  year={2015},
  publisher={Springer}
}


@article{zou2005,
  title={Regularization and variable selection via the elastic net},
  author={Zou, Hui and Hastie, Trevor},
  journal={Journal of the royal statistical society: series B (statistical methodology)},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Wiley Online Library}
}

@inproceedings{liu2016efficient,
  title={Efficient k-Support-Norm Regularized Minimization via Fully Corrective {F}rank-{W}olfe Method.},
  author={Liu, Bo and Yuan, Xiao-Tong and Zhang, Shaoting and Liu, Qingshan and Metaxas, Dimitris N},
  booktitle={Proc. Intl. Joint Conf. on Artifical Intelligence},
  pages={1760--1766},
  year={2016}
}

@article{diakonikolas2017,
  title={Accelerated extra-gradient descent: A novel accelerated first-order method},
  author={Diakonikolas, Jelena and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1706.04680},
  year={2017}
}


@inproceedings{kavis2019,
  title={UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization},
  author={Kavis, Ali and Levy, Kfir Y and Bach, Francis and Cevher, Volkan},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={6257--6266},
  year={2019}
}

@article{harchaoui2015,
  title={Conditional gradient algorithms for norm-regularized smooth convex optimization},
  author={Harchaoui, Zaid and Juditsky, Anatoli and Nemirovski, Arkadi},
  journal={Mathematical Programming},
  volume={152},
  number={1-2},
  pages={75--112},
  year={2015},
  publisher={Springer}
}



@article{korpelevich1976,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, GM},
  journal={Matecon: translations of Russian and East European mathematical economics},
  volume={12},
  pages={747--756},
  year={1976}
}


@article{nemirovski2004,
  title={Prox-method with rate of convergence $O (1/t)$ for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
  author={Nemirovski, Arkadi},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={1},
  pages={229--251},
  year={2004},
  publisher={SIAM}
}


@inproceedings{garber2016linear,
  title={Linear-memory and decomposition-invariant linearly convergent conditional gradient algorithm for structured polytopes},
  author={Garber, Dan and Meshi, Ofer},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={1001--1009},
  year={2016}
}

@inproceedings{braun2017,
  title={Lazifying Conditional Gradient Algorithms},
  author={Braun, G{\'a}bor and Pokutta, Sebastian and Zink, Daniel},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={566--575},
  year={2017}
}




@article{pedregosa2018,
  title={Step-size adaptivity in projection-free optimization},
  author={Pedregosa, Fabian and Askari, Armin and Negiar, Geoffrey and Jaggi, Martin},
  journal={arXiv preprint arXiv:1806.05123},
  year={2018}
}


@article{lan2013complexity,
  title={The complexity of large-scale convex programming under a linear optimization oracle},
  author={Lan, Guanghui},
  journal={arXiv preprint arXiv:1309.5550},
  year={2013}
}



@inproceedings{luise2019sinkhorn,
  title={Sinkhorn Barycenters with Free Support via {F}rank-{W}olfe Algorithm},
  author={Luise, Giulia and Salzo, Saverio and Pontil, Massimiliano and Ciliberto, Carlo},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={9318--9329},
  year={2019}
}


@article{bach2020,
  title={On the Effectiveness of Richardson Extrapolation in Machine Learning},
  author={Bach, Francis},
  journal={arXiv preprint arXiv:2002.02835},
  year={2020}
}



@inproceedings{abernethy2018faster,
  title={Faster Rates for Convex-Concave Games},
  author={Abernethy, Jacob and Lai, Kevin A and Levy, Kfir Y and Wang, Jun-Kun},
  booktitle={Conference On Learning Theory},
  pages={1595--1625},
  year={2018}
}

@article{kerdreux2020,
  title={Projection-Free Optimization on Uniformly Convex Sets},
  author={Kerdreux, Thomas and dâ€™Aspremont, Alexandre and Pokutta, Sebastian},
  journal={arXiv preprint arXiv:2004.11053},
  year={2020}
}

@article{garber2020,
  title={Revisiting {F}rank-{W}olfe for Polytopes: Strict Complementary and Sparsity},
  author={Garber, Dan},
  journal={arXiv preprint arXiv:2006.00558},
  year={2020}
}

@inproceedings{li2019bb,
  title={Almost Tune-Free Variance Reduction},
  author={Li, Bingcong and Wang, Lingda and Giannakis, Georgios B},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  year={2020}
}

@inproceedings{ding2020spectral,
  title={Spectral {F}rank-{W}olfe Algorithm: Strict Complementarity and Linear Convergence},
 author={Ding, Lijun and Fei, Yingjie and Xu, Qiantong and Yang, Chengrun},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  year={2020}
}


@article{zhang2017randomized,
  title={Randomized block {F}rank--{W}olfe for convergent large-scale learning},
  author={Zhang, Liang and Wang, Gang and Romero, Daniel and Giannakis, Georgios B},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={24},
  pages={6448--6461},
  year={2017},
  publisher={IEEE}
}

@article{zhang2016scalable,
  title={Scalable electric vehicle charging protocols},
  author={Zhang, Liang and Kekatos, Vassilis and Giannakis, Georgios B},
  journal={IEEE Transactions on Power Systems},
  volume={32},
  number={2},
  pages={1451--1462},
  year={2016},
  publisher={IEEE}
}

@inproceedings{lacoste2015sequential,
  title={Sequential Kernel Herding: {F}rank-{W}olfe Optimization for Particle Filtering},
  author={Lacoste-Julien, Simon and Lindsten, Fredrik and Bach, Francis},
  booktitle={Proc. Intl. Conf. on Artificial Intelligence and Statistics},
  pages={544--552},
  year={2015}
}


@article{fukushima1984,
  title={A modified {F}rank-{W}olfe algorithm for solving the traffic assignment problem},
  author={Fukushima, Masao},
  journal={Transportation Research Part B: Methodological},
  volume={18},
  number={2},
  pages={169--177},
  year={1984},
  publisher={Elsevier}
}


@inproceedings{malitsky2019,
  title={Adaptive gradient descent without descent},
  author={Malitsky, Yura and Mishchenko, Konstantin},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  year={2020}
}


@inproceedings{li2020revisit,
  title={Revisit of estimate sequence for accelerated gradient methods},
  author={Li, Bingcong and Couti{\~n}o, Mario and Giannakis, Georgios B},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3602--3606},
  year={2020},
  organization={IEEE}
}


@inproceedings{abernethy2017,
  title={On {F}rank-{W}olfe and equilibrium computation},
  author={Abernethy, Jacob D and Wang, Jun-Kun},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={6584--6593},
  year={2017}
}

@article{nesterov2018,
  title={Complexity bounds for primal-dual methods minimizing the model of objective function},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={171},
  number={1-2},
  pages={311--330},
  year={2018},
  publisher={Springer}
}


@article{li2020,
  title={A Momentum-Guided {F}rank-{W}olfe Algorithm},
  author={Li, Bingcong and Coutino, Mario and Giannakis, Georgios B and Leus, Geert},
   journal={IEEE Trans. on Signal Processing},
   volume={69},
   pages={3597-3611},
   year={2021}
}


@inproceedings{li2020extra,
  title={Enhancing {F}rank {W}olfe with an Extra Subproblem},
  author={Li, Bingcong and Wang, Lingda and Giannakis, Georgios B and Zhao, Zhizhen},
  booktitle={Proc. of 35th AAAI Conf. on Artificial Intelligence},
  year={2021}
}

@inproceedings{zhang2020one,
  title={One sample stochastic {F}rank-{W}olfe},
  author={Zhang, Mingrui and Shen, Zebang and Mokhtari, Aryan and Hassani, Hamed and Karbasi, Amin},
  booktitle={Proc. Intl. Conf. on Artificial Intelligence and Statistics},
  pages={4012--4023},
  year={2020},
  organization={PMLR}
}

@inproceedings{allen2017linear,
  title={Linear convergence of a {F}rank-{W}olfe type algorithm over trace-norm balls},
  author={Allen-Zhu, Zeyuan and Hazan, Elad and Hu, Wei and Li, Yuanzhi},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={6191--6200},
  year={2017}
}


@inproceedings{schwing2014,
  title={Globally convergent parallel {MAP} {LP} relaxation solver using the {F}rank-{W}olfe algorithm},
  author={Schwing, Alexander and Hazan, Tamir and Pollefeys, Marc and Urtasun, Raquel},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  pages={487--495},
  year={2014}
}

@inproceedings{ye2020,
  title={Good Subnetworks Provably Exist: Pruning via Greedy Forward Selection},
  author={Ye, Mao and Gong, Chengyue and Nie, Lizhen and Zhou, Denny and Klivans, Adam and Liu, Qiang},
  booktitle={Proc. Intl. Conf. on Machine Learning},
  year={2020}
}

@article{zhang2017randomized,
  title={Randomized block Frank--Wolfe for convergent large-scale learning},
  author={Zhang, Liang and Wang, Gang and Romero, Daniel and Giannakis, Georgios B},
  journal={IEEE Trans. on Signal Processing},
  volume={65},
  number={24},
  pages={6448--6461},
  year={2017},
  publisher={IEEE}
}

@inproceedings{ghadimi2015,
  title={Global convergence of the heavy-ball method for convex optimization},
  author={Ghadimi, Euhanna and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  booktitle={Proc. of European control conference},
  pages={310--315},
  year={2015}
}

@article{diakonikolas2019approximate,
  title={The approximate duality gap technique: A unified theory of first-order methods},
  author={Diakonikolas, Jelena and Orecchia, Lorenzo},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={660--689},
  year={2019},
  publisher={SIAM}
}

@article{nguyen2021memory,
  title={Memory-Efficient Convex Optimization for Self-Dictionary Separable Nonnegative Matrix Factorization: A Frank-Wolfe Approach},
  author={Nguyen, Tri and Fu, Xiao and Wu, Ruiyuan},
  journal={arXiv preprint arXiv:2109.11135},
  year={2021}
}

@article{zafar2019fairness,
  title={Fairness constraints: A flexible approach for fair classification},
  author={Zafar, Muhammad Bilal and Valera, Isabel and Gomez-Rodriguez, Manuel and Gummadi, Krishna P},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={2737--2778},
  year={2019},
  publisher={JMLR. org}
}



%%%%%%%%%%%%%%%%SAM%%%

@inproceedings{du2022,
	author={Jiawei Du and Hanshu Yan and Jiashi Feng and Joey Tianyi Zhou and Liangli Zhen and Rick Siow Mong Goh and Vincent Y. F. Tan},
	title={Efficient Sharpness-aware Minimization for Improved Training of Neural Networks},
	booktitle=ICLR,
	year=2022,
}


@inproceedings{mi2022,
	author={Peng Mi and Li Shen and Tianhe Ren and Yiyi Zhou and Xiaoshuai Sun and Rongrong Ji and Dacheng Tao},
	title={Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation
  Approach},
  	booktitle=NIPS,
	year=2022,
        volume=35,
}

@inproceedings{zhuang2022,
	author={Juntang Zhuang and Boqing Gong and Liangzhe Yuan and Yin Cui and Hartwig Adam and Nicha Dvornek and Sekhar Tatikonda and James Duncan and Ting Liu},
	title={Surrogate Gap Minimization Improves Sharpness-Aware Training.},
	booktitle=ICLR,
	year=2022,
}



@inproceedings{reddi2016,
  title={Stochastic {F}rank-{W}olfe methods for nonconvex optimization},
  author={Reddi, Sashank J and Sra, Suvrit and P{\'o}czos, Barnab{\'a}s and Smola, Alex},
  booktitle={Allerton conference on communication, control, and computing},
  pages={1244--1251},
  year={2016},
  organization={IEEE}
}



@inproceedings{foret2021,
	author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
	title={Sharpness-Aware Minimization for Efficiently Improving Generalization},
	booktitle=ICLR,
	year=2021,
}


@article{mokhtari2018,
  title={Stochastic conditional gradient methods: From convex minimization to submodular maximization},
  author={Mokhtari, Aryan and Hassani, Hamed and Karbasi, Amin},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={4232--4280},
  year={2020},
  publisher={JMLR}
}


@inproceedings{li2021heavy,
  title={Heavy Ball Momentum for Conditional Gradient},
  author={Li, Bingcong and Sadeghi, Alireza and Giannakis, Georgios},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  year={2021}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle=NIPS,
  volume={30},
  year={2017}
}

@inproceedings{kingma2014,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle=ICLR,
  year={2014}
}

@inproceedings{loshchilov2017adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle=ICLR,
  year={2017}
}


@inproceedings{johnson2013,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Proc. Advances in Neural Info. Process. Syst.},
  pages={315--323},
  address = {Lake Tahoe, Nevada},
  year={2013}
}

@inproceedings{nguyen2017,
  title={{SARAH}: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle=ICML,
  address = {Sydney, Australia},
  year={2017}
}


@inproceedings{maksym2022,
	author={Maksym Andriushchenko and Nicolas Flammarion},
	pages={639-668},
	title={Towards Understanding Sharpness-Aware Minimization},
	booktitle=ICML,
	year=2022,
        organization={PMLR},
}


@article{zhang2016,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}



@inproceedings{kwon2021,
	author={Jungmin Kwon and Jeongseop Kim and Hyunseo Park and In Kwon Choi},
	pages={5905-5914},
	title={A{SAM}: Adaptive Sharpness-Aware Minimization For Scale-Invariant Learning Of Deep Neural Networks},
	booktitle=ICML,
	year=2021,
        organization={PMLR},
}


@article{cutout2017,
	author={Terrance Devries and Graham W. Taylor},
	title={Improved Regularization of Convolutional Neural Networks with Cutout.},
	volume={abs/1708.04552},
	year=2017,
}


@inproceedings{sam4vit,
	author={Xiangning Chen and Cho-Jui Hsieh and Boqing Gong},
	title={When Vision Transformers Outperform {ResNets} without Pre-training or Strong Data Augmentations},
	booktitle=ICLR,
	year=2022,
}


@inproceedings{kim2022,
	author={Minyoung Kim and Da Li and Shell Xu Hu and Timothy M. Hospedales},
	pages={11148-11161},
	title={Fisher {SAM}: Information Geometry and Sharpness Aware Minimisation.},
	booktitle=ICML,
	year=2022,
}

@inproceedings{wilson2017,
	author={Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Nathan Srebro and Recht, Benjamin and Srebro, Nati},
	pages={4148-4158},
	title={The Marginal Value of Adaptive Gradient Methods in Machine Learning.},
	booktitle=ICML,
	volume=30,
	year=2017,
}

@inproceedings{stanislaw2020,
	author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho and Krzysztof J. Geras},
	title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks.},
	booktitle=ICLR,
	year=2020,
}


@inproceedings{behrooz2019,
	author={Behrooz Ghorbani and Shankar Krishnan and Ying Xiao},
	pages={2232-2241},
	title={An Investigation into Neural Net Optimization via Hessian Eigenvalue Density.},
	booktitle=ICML,
	year=2019,
}


@inproceedings{gasam2022,
	author={Zhiyuan Zhang and Ruixuan Luo and Qi Su and Xu Sun},
	title={{GA-SAM}: Gradient-Strength based Adaptive Sharpness-Aware Minimization for Improved Generalization},
	booktitle={Proc. Conf. Empirical Methods in Natural Language Processing},
	year=2022,
}


@inproceedings{yang2022,
	author={Yang Zhao and Hao Zhang and Xiuyuan Hu},
	pages={26982-26992},
	title={Penalizing Gradient Norm for Efficiently Improving Generalization in Deep Learning},
	booktitle=ICML,
	year=2022,
}


@inproceedings{jiang2020,
  title={Fantastic Generalization Measures and Where to Find Them},
  author={Yiding Jiang and Behnam Neyshabur and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
  booktitle=ICLR,
  year={2020},
}


@inproceedings{imagenet2009,
  title={Image{N}et: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=CVPR,
  pages={248--255},
  year={2009}
}



@article{iwslt2014,
	author={M Cettolo and J Niehues and S Stker and L Bentivogli and M Federico},
	title={Report on the 11th IWSLT evaluation campaign, IWSLT 2014},
	year=2014,
}


@inproceedings{pratik2017,
	author={Pratik Chaudhari and Anna Choromanska and Stefano Soatto and Yann LeCun and Carlo Baldassi and Christian Borgs and Jennifer Chayes and Levent Sagun and Riccardo Zecchina},
	title={Entropy-{SGD}: {B}iasing gradient descent into wide valleys},
	booktitle=ICLR,
	year=2017,
}

@article{dropout2014,
	author={Nitish Srivastava and Geoffrey E. Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	pages={1929-1958},
	title={Dropout: a simple way to prevent neural networks from overfitting},
	journal=JMLR,
	volume=15,
	year=2014,
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv:1810.04805},
  year={2018}
}

@inproceedings{gpt3,
	author={Brown Tom and Mann Benjamin and Ryder Nick and Subbiah Melanie and Kaplan Jared and Dhariwal Prafulla and Neelakantan Arvind and Shyam Pranav and Sastry Girish and Askell Amanda and Agarwal Sandhini and Herbert-Voss Ariel and Krueger Gretchen and Henighan Tom and Child Rewon and Ramesh Aditya and Ziegler Daniel M. and Wu Jeffrey and Winter Clemens and Hesse Christopher and Chen Mark and Sigler Eric and Litwin Mateusz and Gray Scott and Chess Benjamin and Clark Jack and Berner Christopher and McCandlish Sam and Radford Alec and Sutskever Ilya and Amodei Dario},
	pages={1877-1901},
	title={Language Models are Few-Shot Learners},
	booktitle=NIPS,
	volume=33,
	year=2020,
}


@inproceedings{wen2023,
	author={Kaiyue Wen and Tengyu Ma AND Z	hiyuan Li},
	title={How Does Sharpness-Aware Minimization Minimizes Sharpness},
	booktitle=ICLR,
	year=2023,
}

@inproceedings{keskar2016,
	author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
	title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.},
	booktitle=ICLR,
	year=2016,
}


@article{Jastrzebski2018,
  title={Three factors influencing minima in {SGD}},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv:1711.04623},
  year={2017}
}


@inproceedings{dziugaite2017,
	author={Gintare Karolina Dziugaite and Daniel M. Roy},
	title={Computing Nonvacuous Generalization Bounds For Deep (Stochastic) Neural Networks With Many More Parameters Than Training Data},
	booktitle=UAI,
	year=2017,
}

@inproceedings{behnam2017,
	author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Nathan Srebro and Srebro, Nati},
	pages={5947-5956},
	title={Exploring Generalization in Deep Learning.},
	booktitle=NIPS,
	volume=30,
	year=2017,
}


@inproceedings{wang2022,
	author={Ziqiao Wang and Yongyi Mao},
	title={On the Generalization of Models Trained with {SGD}: Information-Theoretic Bounds and Implications},
	booktitle=ICLR,
	year=2022,
}

@inproceedings{zheng2021,
	author={Yaowei Zheng and Richong Zhang and Yongyi Mao},
	pages={8152-8161},
	title={Regularizing Neural Networks via Adversarial Model Perturbation},
	booktitle=CVPR,
	year=2021,
}


@inproceedings{wu2020,
	author={Dongxian Wu and Shu-Tao Xia and Yisen Wang},
	pages={2958-2969},
	title={Adversarial Weight Perturbation Helps Robust Generalization},
	volume=33,
	booktitle=NIPS,
	year=2020,
}

@inproceedings{liu2022,
	author={Yong Liu and Siqi Mai and Xiangning Chen and Cho-Jui Hsieh and Yang You},
	pages={12350-12360},
	title={Towards Efficient and Scalable Sharpness-Aware Minimization},
	booktitle=CVPR,
	year=2022,
}


@article{zhao222,
	author={Yang Zhao and Hao Zhang and Xiuyuan Hu},
	title={{SS-SAM}: Stochastic Scheduled Sharpness-Aware Minimization for Efficiently Training Deep Neural Networks},
	journal={arXiv:2203.09962},
	year=2022,
}


@inproceedings{du2022saf,
	author={Jiawei Du and Daquan Zhou and Jiashi Feng and Vincent Y. F. Tan and Joey Tianyi Zhou},
	title={Sharpness-Aware Training for Free},
	booktitle=NIPS,
	year=2022,
}


@book{mardia2000directional,
  title={Directional Statistics},
  author={Mardia, K. V.  and  Jupp, P. E.},
  publisher={Directional statistics},
  year={2000},
}


@inproceedings{izmailov2018,
	author={Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry P. Vetrov and Andrew Gordon Wilson},
	pages={876-885},
	title={Averaging Weights Leads To Wider Optima And Better Generalization},
	booktitle=UAI,
	year=2018,
}


@inproceedings{barrett2020implicit,
  title={Implicit gradient regularization},
  author={Barrett, David and Dherin, Benoit},
  booktitle=ICLR,
  year={2021}
}


@article{kim2023multi,
  title={Exploring the effect of multi-step ascent in sharpness-aware minimization},
  author={Kim, Hoki and Park, Jinseong and Choi, Yujin and Lee, Woojin and Lee, Jaewook},
  journal={arXiv:2302.10181},
  year={2023}
}



@inproceedings{wang2023,
  title={Sharpness-aware gradient matching for domain generalization},
  author={Wang, Pengfei and Zhang, Zhaoxiang and Lei, Zhen and Zhang, Lei},
  booktitle=CVPR,
  pages={3769--3778},
  year={2023}
}


@inproceedings{jiang2023,
  title={An Adaptive Policy to Employ Sharpness-Aware Minimization},
  author={Weisen Jiang and Hansi Yang and Yu Zhang and James Kwok},
  booktitle=ICLR,
  year={2023},
}


@article{bartlett2022dynamics,
  title={The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima},
  author={Bartlett, Peter and Long, Philip and Bousquet, Olivier},
  journal=JMLR,
  volume={24},
  number={316},
  pages={1--36},
  year={2023}
}


@article{gong2021ast,
  title={Ast: Audio spectrogram transformer},
  author={Gong, Yuan and Chung, Yu-An and Glass, James},
  journal={arXiv preprint arXiv:2104.01778},
  year={2021}
}


@inproceedings{li2023vasso,
  title={Enhancing Sharpness-Aware Optimization Through Variance Suppression},
  author={Li, Bingcong and Giannakis, Georgios B},
  booktitle=NIPS,
  volume=36,
  year={2023}
}

%%%%%%%%%%%

@inproceedings{wen2023more,
  title={Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  booktitle=NIPS,
  volume=36,
  year={2023}
}


@inproceedings{neyshabur2015,
  title={{Path-SGD}: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  booktitle=NIPS,
  volume={28},
  year={2015}
}


@inproceedings{dinh2017,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle=ICML,
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}


@inproceedings{gonon2023path,
  title={A path-norm toolkit for modern networks: consequences, promises and challenges},
  author={Antoine Gonon and Nicolas Brisebarre and Elisa Riccietti and R{\'e}mi Gribonval},
  booktitle=ICLR,
  year={2024},
}


@inproceedings{hu2021lora,
  title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
  author={Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle=ICLR,
  year={2022},
}


@inproceedings{malladi2023,
  title={Fine-Tuning Language Models with Just Forward Passes},
  author={Sadhika Malladi and Tianyu Gao and Eshaan Nichani and Alex Damian and Jason D. Lee and Danqi Chen and Sanjeev Arora},
  booktitle=NIPS,
  year={2023},
  volume=36,
}


@article{roberta2019,
  title={{RoBERTa}: A robustly optimized {BERT} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@article{opt2022,
  title={{OPT}: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle=ACL,
  year={2021},
  pages={4582--4597},
}


@inproceedings{houlsby2019,
  title={Parameter-efficient transfer learning for {NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle=ICML,
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}


@inproceedings{neyshabur2017pac,
  title={A {PAC}-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},
  author={Behnam Neyshabur and Srinadh Bhojanapalli and Nathan Srebro},
  booktitle=ICLR,
  year={2018},
}


@inproceedings{gardent2017webnlg,
  title={The {WebNLG} challenge: Generating text from {RDF} data},
  author={Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and Perez-Beltrachini, Laura},
  booktitle={Proc. Int. Conf. Nat. Lang. Gener.},
  pages={124--133},
  organization={ACL},
  year={2017}
}


@inproceedings{andriushchenko2023modern,
  title={A modern look at the relationship between sharpness and generalization},
  author={Andriushchenko, Maksym and Croce, Francesco and M{\"u}ller, Maximilian and Hein, Matthias and Flammarion, Nicolas},
  booktitle=ICML,
  year={2023}
}


@article{hayou2024lora+,
  title={LoRA+: Efficient Low Rank Adaptation of Large Models},
  author={Hayou, Soufiane and Ghosh, Nikhil and Yu, Bin},
  journal={arXiv preprint arXiv:2402.12354},
  year={2024}
}

@inproceedings{arora2018,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle=ICML,
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@inproceedings{ahn2024learning,
  title={Learning threshold neurons via edge of stability},
  author={Ahn, Kwangjun and Bubeck, S{\'e}bastien and Chewi, Sinho and Lee, Yin Tat and Suarez, Felipe and Zhang, Yi},
  booktitle=NIPS,
  volume={36},
  year={2023}
}


@inproceedings{chen2024why-sam-over-sgd,
  title={Why Does Sharpness-Aware Minimization Generalize Better Than {SGD}?},
  author={Chen, Zixiang and Zhang, Junkai and Kou, Yiwen and Chen, Xiangning and Hsieh, Cho-Jui and Gu, Quanquan},
  booktitle=NIPS,
  volume={36},
  year={2023}
}


@article{sidak2024,
  title={Hallmarks of Optimization Trajectories in Neural Networks and {LLMs}: The Lengths, Bends, and Dead Ends},
  author={Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Scholkopf},
  journal={arXiv preprint arXiv:2403.07379},
  year={2024}
}


@inproceedings{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  booktitle=NIPS,
  volume={32},
  year={2019}
}


@article{zhu2021global,
  title={The global optimization geometry of low-rank matrix optimization},
  author={Zhu, Zhihui and Li, Qiuwei and Tang, Gongguo and Wakin, Michael B},
  journal={IEEE Transactions on Information Theory},
  volume={67},
  number={2},
  pages={1308--1331},
  year={2021},
  publisher={IEEE}
}


@inproceedings{de2015global,
  title={Global convergence of stochastic gradient descent for some non-convex matrix problems},
  author={De Sa, Christopher and Re, Christopher and Olukotun, Kunle},
  booktitle=ICML,
  pages={2332--2341},
  year={2015},
  organization={PMLR}
}


@inproceedings{si2024practical,
  title={Practical sharpness-aware minimization cannot converge all the way to optima},
  author={Si, Dongkuk and Yun, Chulhee},
  booktitle=NIPS,
  volume={36},
  year={2023}
}


@inproceedings{dai2024crucial,
  title={The crucial role of normalization in sharpness-aware minimization},
  author={Dai, Yan and Ahn, Kwangjun and Sra, Suvrit},
  booktitle=NIPS,
  volume={36},
  year={2023}
}


@article{andriushchenko2024,
  title={Sharpness-aware minimization leads to low-rank features},
  author={Andriushchenko, Maksym and Bahri, Dara and Mobahi, Hossein and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@inproceedings{de2015,
  title={Global convergence of stochastic gradient descent for some non-convex matrix problems},
  author={De Sa, Christopher and Re, Christopher and Olukotun, Kunle},
  booktitle=ICML,
  pages={2332--2341},
  year={2015},
  organization={PMLR}
}


@inproceedings{cohen2020gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  booktitle=ICLR,
  year={2020}
}


@inproceedings{springer2023,
  title={Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning},
  author={Springer, Jacob Mitchell and Nagarajan, Vaishnavh and Raghunathan, Aditi},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}


@inproceedings{abbas2022sharp,
  title={{Sharp-MAML}: Sharpness-aware model-agnostic meta learning},
  author={Abbas, Momin and Xiao, Quan and Chen, Lisha and Chen, Pin-Yu and Chen, Tianyi},
  booktitle=ICML,
  pages={10--32},
  year={2022},
  organization={PMLR}
}


@inproceedings{bahri2021,
  title={Sharpness-Aware Minimization Improves Language Model Generalization},
  author={Dara Bahri and Hossein Mobahi and Yi Tay},
  year={2022},
  booktitle=ACL,
  pages={7360--7371}
}


@inproceedings{gidel2019implicit,
  title={Implicit regularization of discrete gradient dynamics in linear neural networks},
  author={Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
  booktitle=NIPS,
  volume={32},
  year={2019}
}

@inproceedings{tahmasebi2023scale,
  title={On Scale-Invariant Sharpness Measures},
  author={Tahmasebi, Behrooz and Soleymani, Ashkan and Jegelka, Stefanie and Jaillet, Patrick},
  booktitle={NeurIPS Workshop Math. Mod. Mach. Learn.},
  year={2023}
}


@article{sheen2024,
  title={Implicit Regularization of Gradient Flow on One-Layer Softmax Attention},
  author={Sheen, Heejune and Chen, Siyu and Wang, Tianhao and Zhou, Harrison H},
  journal={arXiv preprint arXiv:2403.08699},
  year={2024}
}


@misc{gradientaccumulation,
  title={Gradient accumulation},
  author={HuggingFace},
  url={https://huggingface.co/docs/accelerate/en/usage_guides/gradient_accumulation},
}


@inproceedings{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ziwei Ji and Matus Telgarsky},
  booktitle=ICLR,
  year={2019},
}


@inproceedings{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Kaifeng Lyu and Jian Li},
  booktitle=ICLR,
  year={2020},
}


@inproceedings{li2021what,
  title={What Happens after {SGD} Reaches Zero Loss? -- {A} Mathematical Framework},
  author={Zhiyuan Li and Tianhao Wang and Sanjeev Arora},
  booktitle=ICLR,
  year={2022},
}


@inproceedings{arora2022eos,
  title={Understanding gradient descent on the edge of stability in deep learning},
  author={Arora, Sanjeev and Li, Zhiyuan and Panigrahi, Abhishek},
  booktitle=ICML,
  pages={948--1024},
  year={2022},
  organization={PMLR}
}


@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle=ICML,
  pages={322--332},
  year={2019},
  organization={PMLR}
}


@inproceedings{arora2018convergence,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Sanjeev Arora and Nadav Cohen and Noah Golowich and Wei Hu},
  booktitle=ICLR,
  year={2019},
}


@inproceedings{shamir2019,
  title={Exponential convergence time of gradient descent for one-dimensional deep linear neural networks},
  author={Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={2691--2713},
  year={2019},
  organization={PMLR}
}


@inproceedings{du2018,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  booktitle=NIPS,
  volume={31},
  year={2018}
}


@inproceedings{ge2017no,
  title={No spurious local minima in nonconvex low rank problems: A unified geometric analysis},
  author={Ge, Rong and Jin, Chi and Zheng, Yi},
  booktitle=ICML,
  pages={1233--1242},
  year={2017},
  organization={PMLR}
}


@inproceedings{bartlett2018id,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
  author={Bartlett, Peter and Helmbold, Dave and Long, Philip},
  booktitle=ICML,
  pages={521--530},
  year={2018},
  organization={PMLR}
}


@InProceedings{sherborne2023,
  author    = {Sherborne, Tom and Saphra, Naomi and Dasigi, Pradeep and Peng, Hao},
  booktitle = ICLR,
  title     = {{TRAM}: Bridging Trust Regions and Sharpness Aware Minimization},
  year      = {2023},
}


@inproceedings{zhang2023domain,
  title={Domain-Inspired Sharpness Aware Minimization Under Domain Shifts},
  author={Zhang, Ruipeng and Fan, Ziqing and Yao, Jiangchao and Zhang, Ya and Wang, Yanfeng},
  booktitle=ICLR,
  year={2023}
}

@inproceedings{woodworth2020,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle=COLT,
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}


@inproceedings{loshchilov2017,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle=ICLR,
  year={2019},
}

@inproceedings{agarwala2023,
  title={{SAM} operates far from home: eigenvalue regularization as a dynamical phenomenon},
  author={Agarwala, Atish and Dauphin, Yann},
  booktitle=ICML,
  pages={152--168},
  year={2023},
  organization={PMLR}
}


@inproceedings{zhou2021sharpness,
  title={Sharpness-aware minimization with dynamic reweighting},
  author={Zhou, Wenxuan and Liu, Fangyu and Zhang, Huan and Chen, Muhao},
  booktitle=EMNLP,
  year={2022},
  pages={5686--5699},
}


@inproceedings{wang2019superglue,
  title={Super{GLUE}: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle=NIPS,
  volume={32},
  year={2019}
}


@inproceedings{wang2018glue,
  title={{GLUE}: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle=ICLR,
  year={2019}
}


@inproceedings{mnli,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  booktitle={Proc. Conf. North Am. Chapter Assoc. Comput. Linguist.},
  year={2018},
  pages={1112--1122},
}

@inproceedings{sst2,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle=EMNLP,
  pages={1631--1642},
  year={2013}
}


@inproceedings{mrpc,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Proc. Int. Workshop Paraphrasing},
  year={2005}
}


@article{cola,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Trans. Assoc. Comput. Linguist.},
  volume={7},
  pages={625--641},
  year={2019},
}


@inproceedings{qnli,
  title={Know what you don't know: Unanswerable questions for {SQuAD}},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle=ACL,
  year={2018},
  pages={784--789},
}



@inproceedings{stsb,
  title={{SemEval}-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, I{\~n}igo and Specia, Lucia},
  booktitle={Proc. Int. Workshop Semant. Eval.},
  year={2017},
  publisher={ACL},
  pages={1--14},
}


@article{cb,
  title={The {CommitmentBank}: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  journal={Proc. Sinn und Bedeutung},
  volume={23},
  number={2},
  pages={107--124},
  year={2019}
}


@article{record,
  title={{ReCoRD}: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}


@inproceedings{copa,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={AAAI Spring Symposium Series},
  year={2011}
}


@inproceedings{trec,
  title={Building a question answering test collection},
  author={Voorhees, Ellen M and Tice, Dawn M},
  booktitle={Proc. Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.},
  pages={200--207},
  year={2000}
}


@inproceedings{squad,
  title={{SQuAD}: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle=EMNLP,
  year={2016},
  pages={2383--2392},
}



@inproceedings{snli,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  booktitle=EMNLP,
  pages={632--642},
  year={2015}
}

@inproceedings{dettmers2024qlora,
  title={Q{LoRA}: Efficient finetuning of quantized {LLM}s},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle=NIPS,
  volume={36},
  year={2023}
}


@inproceedings{zhang2023adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle=ICLR,
  year={2023}
}

@inproceedings{kopiczko2023vera,
  title={Ve{RA}: Vector-based Random Matrix Adaptation},
  author={Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki M Asano},
  booktitle=ICLR,
  year={2024},
}

@article{xia2024chain,
  title={Chain of {LoRA}: Efficient fine-tuning of language models via residual learning},
  author={Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  journal={arXiv preprint arXiv:2401.04151},
  year={2024}
}


@inproceedings{chen2023longlora,
  title={{Long-LoRA}: Efficient Fine-tuning of Long-Context Large Language Models},
  author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
  booktitle=ICLR,
  year={2024},
}


@inproceedings{tu2016low,
  title={Low-rank solutions of linear matrix equations via procrustes flow},
  author={Tu, Stephen and Boczar, Ross and Simchowitz, Max and Soltanolkotabi, Mahdi and Recht, Ben},
  booktitle=ICML,
  pages={964--973},
  year={2016},
  organization={PMLR}
}

@inproceedings{tarmoun2021,
  title={Understanding the dynamics of gradient flow in overparameterized linear models},
  author={Tarmoun, Salma and Franca, Guilherme and Haeffele, Benjamin D and Vidal, Rene},
  booktitle={International Conference on Machine Learning},
  pages={10153--10161},
  year={2021},
  organization={PMLR}
}


%%%%%%%

@inproceedings{singh2024closed,
  title={Closed form of the Hessian spectrum for some Neural Networks},
  author={Singh, Sidak Pal and Hofmann, Thomas},
  booktitle={High-dimensional Learning Dynamics 2024: The Emergence of Structure and Reasoning},
  year={2024}
}

@article{tahmasebi2024universal,
  title={A Universal Class of Sharpness-Aware Minimization Algorithms},
  author={Tahmasebi, Behrooz and Soleymani, Ashkan and Bahri, Dara and Jegelka, Stefanie and Jaillet, Patrick},
  journal={arXiv preprint arXiv:2406.03682},
  year={2024}
}