\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbe et~al.(2023)Abbe, Adsera, and Misiakiewicz]{abbe2023sgd}
Emmanuel Abbe, Enric~Boix Adsera, and Theodor Misiakiewicz.
\newblock Sgd learning on neural networks: leap complexity and saddle-to-saddle
  dynamics.
\newblock In \emph{The Thirty Sixth Annual Conference on Learning Theory},
  pages 2552--2623. PMLR, 2023.

\bibitem[Anderson et~al.(2010)Anderson, Guionnet, and
  Zeitouni]{anderson2010introduction}
Greg~W Anderson, Alice Guionnet, and Ofer Zeitouni.
\newblock \emph{An introduction to random matrices}.
\newblock Cambridge university press, 2010.

\bibitem[Aubin et~al.(2019{\natexlab{a}})Aubin, Loureiro, Maillard, Krzakala,
  and Zdeborov{\'a}]{aubin2019spiked}
Benjamin Aubin, Bruno Loureiro, Antoine Maillard, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock The spiked matrix model with generative priors.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Aubin et~al.(2019{\natexlab{b}})Aubin, Maillard, Barbier, Krzakala,
  Macris, and Zdeborová]{aubin2019committee}
Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas
  Macris, and Lenka Zdeborová.
\newblock The committee machine: computational to statistical gaps in learning
  a two-layers neural network.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124023, jan 2019{\natexlab{b}}.

\bibitem[Aubin et~al.(2020)Aubin, Loureiro, Baker, Krzakala, and
  Zdeborov{\'a}]{aubin2020exact}
Benjamin Aubin, Bruno Loureiro, Antoine Baker, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock Exact asymptotics for phase retrieval and compressed sensing with
  random generative priors.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pages 55--73.
  PMLR, 2020.

\bibitem[Barbier et~al.(2016)Barbier, Dia, Macris, and
  Krzakala]{barbier2016mutual}
Jean Barbier, Mohamad Dia, Nicolas Macris, and Florent Krzakala.
\newblock The mutual information in random linear estimation.
\newblock In \emph{2016 54th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 625--632. IEEE, 2016.

\bibitem[Barbier et~al.(2019)Barbier, Krzakala, Macris, Miolane, and
  Zdeborov{\'a}]{barbier2019optimal}
Jean Barbier, Florent Krzakala, Nicolas Macris, L{\'e}o Miolane, and Lenka
  Zdeborov{\'a}.
\newblock Optimal errors and phase transitions in high-dimensional generalized
  linear models.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (12):\penalty0 5451--5460, 2019.

\bibitem[Ben~Arous et~al.(2019)Ben~Arous, Mei, Montanari, and
  Nica]{arous2019landscape}
G\'erard Ben~Arous, Song Mei, Andrea Montanari, and Mihai Nica.
\newblock The landscape of the spiked tensor model.
\newblock \emph{Communications on Pure and Applied Mathematics}, 72\penalty0
  (11):\penalty0 2282--2330, 2019.

\bibitem[Ben~Arous et~al.(2021)Ben~Arous, Gheissari, and
  Jagannath]{arous2021online}
G\'erard Ben~Arous, Reza Gheissari, and Aukosh Jagannath.
\newblock Online stochastic gradient descent on non-convex losses from
  high-dimensional inference.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (106):\penalty0 1--51, 2021.

\bibitem[Benaych-Georges and Nadakuditi(2011)]{benaych2011eigenvalues}
Florent Benaych-Georges and Raj~Rao Nadakuditi.
\newblock The eigenvalues and eigenvectors of finite, low rank perturbations of
  large random matrices.
\newblock \emph{Advances in Mathematics}, 227\penalty0 (1):\penalty0 494--521,
  2011.

\bibitem[Berthier et~al.(2020)Berthier, Montanari, and
  Nguyen]{berthier2020state}
Raphael Berthier, Andrea Montanari, and Phan-Minh Nguyen.
\newblock State evolution for approximate message passing with non-separable
  functions.
\newblock \emph{Information and Inference: A Journal of the IMA}, 9\penalty0
  (1):\penalty0 33--79, 2020.

\bibitem[Bietti et~al.(2023)Bietti, Bruna, and
  Pillaud-Vivien]{bietti2023learning}
Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien.
\newblock On learning gaussian multi-index models with gradient flow.
\newblock \emph{arXiv preprint arXiv:2310.19793}, 2023.

\bibitem[Bun et~al.(2016)Bun, Allez, Bouchaud, and Potters]{bun2016rotational}
Jo{\"e}l Bun, Romain Allez, Jean-Philippe Bouchaud, and Marc Potters.
\newblock Rotational invariant estimator for general noisy matrices.
\newblock \emph{IEEE Transactions on Information Theory}, 62\penalty0
  (12):\penalty0 7475--7490, 2016.

\bibitem[Cai et~al.(2022)Cai, Huang, Li, and Wang]{cai2022solving}
Jian-Feng Cai, Meng Huang, Dong Li, and Yang Wang.
\newblock Solving phase retrieval with random initial guess is nearly as good
  as by spectral initialization.
\newblock \emph{Applied and Computational Harmonic Analysis}, 58:\penalty0
  60--84, 2022.

\bibitem[Camilli and M{\'e}zard(2023)]{camilli2023matrix}
Francesco Camilli and Marc M{\'e}zard.
\newblock Matrix factorization with neural networks.
\newblock \emph{Physical Review E}, 107\penalty0 (6):\penalty0 064308, 2023.

\bibitem[Camilli and M{\'e}zard(2024)]{camilli2024decimation}
Francesco Camilli and Marc M{\'e}zard.
\newblock The decimation scheme for symmetric matrix factorization.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  57\penalty0 (8):\penalty0 085002, 2024.

\bibitem[Cand{\`e}s and Li(2014)]{candes2014solving}
Emmanuel~J Cand{\`e}s and Xiaodong Li.
\newblock Solving quadratic equations via phaselift when there are about as
  many equations as unknowns.
\newblock \emph{Foundations of Computational Mathematics}, 14:\penalty0
  1017--1026, 2014.

\bibitem[Candes et~al.(2013)Candes, Strohmer, and
  Voroninski]{candes2013phaselift}
Emmanuel~J Candes, Thomas Strohmer, and Vladislav Voroninski.
\newblock Phaselift: Exact and stable signal recovery from magnitude
  measurements via convex programming.
\newblock \emph{Communications on Pure and Applied Mathematics}, 66\penalty0
  (8):\penalty0 1241--1274, 2013.

\bibitem[Charbonneau et~al.(2023)Charbonneau, Marinari, Parisi,
  Ricci-tersenghi, Sicuro, Zamponi, and Mezard]{charbonneau2023spin}
Patrick Charbonneau, Enzo Marinari, Giorgio Parisi, Federico Ricci-tersenghi,
  Gabriele Sicuro, Francesco Zamponi, and Marc Mezard.
\newblock \emph{Spin Glass Theory and Far Beyond: Replica Symmetry Breaking
  after 40 Years}.
\newblock World Scientific, 2023.

\bibitem[Chen and Candes(2015)]{chen2015solving}
Yuxin Chen and Emmanuel Candes.
\newblock Solving random quadratic systems of equations is nearly as easy as
  solving linear systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Chen et~al.(2019)Chen, Chi, Fan, and Ma]{chen2019gradient}
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma.
\newblock Gradient descent with random initialization: Fast global convergence
  for nonconvex phase retrieval.
\newblock \emph{Mathematical Programming}, 176:\penalty0 5--37, 2019.

\bibitem[Collins-Woodfin et~al.(2023)Collins-Woodfin, Paquette, Paquette, and
  Seroussi]{collins2023hitting}
Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar
  Seroussi.
\newblock Hitting the high-dimensional notes: An ode for sgd learning dynamics
  on glms and multi-index models.
\newblock \emph{arXiv preprint arXiv:2308.08977}, 2023.

\bibitem[Cui et~al.(2023)Cui, Krzakala, and Zdeborova]{Cui_extensive_width}
Hugo Cui, Florent Krzakala, and Lenka Zdeborova.
\newblock {B}ayes-optimal learning of deep random networks of extensive-width.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}. PMLR, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/cui23b.html}.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Damian et~al.(2024)Damian, Pillaud-Vivien, Lee, and
  Bruna]{damian2024computational}
Alex Damian, Loucas Pillaud-Vivien, Jason~D Lee, and Joan Bruna.
\newblock The computational complexity of learning gaussian single-index
  models.
\newblock \emph{arXiv preprint arXiv:2403.05529}, 2024.

\bibitem[Dandi et~al.(2024)Dandi, Stephan, Krzakala, Loureiro, and
  Zdeborov{\'a}]{dandi2024universality}
Yatin Dandi, Ludovic Stephan, Florent Krzakala, Bruno Loureiro, and Lenka
  Zdeborov{\'a}.
\newblock Universality laws for gaussian mixtures in generalized linear models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Demanet and Hand(2014)]{demanet2014stable}
Laurent Demanet and Paul Hand.
\newblock Stable optimizationless recovery from phaseless linear measurements.
\newblock \emph{Journal of Fourier Analysis and Applications}, 20:\penalty0
  199--221, 2014.

\bibitem[Dong et~al.(2023)Dong, Valzania, Maillard, Pham, Gigan, and
  Unser]{dong2023phase}
Jonathan Dong, Lorenzo Valzania, Antoine Maillard, Thanh-an Pham, Sylvain
  Gigan, and Michael Unser.
\newblock Phase retrieval: From computational imaging to machine learning: A
  tutorial.
\newblock \emph{IEEE Signal Processing Magazine}, 40\penalty0 (1):\penalty0
  45--57, 2023.

\bibitem[Donoho(2000)]{donoho2000high}
David~L Donoho.
\newblock High-dimensional data analysis: The curses and blessings of
  dimensionality.
\newblock \emph{AMS math challenges lecture}, 1\penalty0 (2000):\penalty0 32,
  2000.

\bibitem[Donoho et~al.(2009)Donoho, Maleki, and Montanari]{donoho2009message}
David~L Donoho, Arian Maleki, and Andrea Montanari.
\newblock Message-passing algorithms for compressed sensing.
\newblock \emph{Proceedings of the National Academy of Sciences}, 106\penalty0
  (45):\penalty0 18914--18919, 2009.

\bibitem[Du and Lee(2018)]{du2018power}
Simon Du and Jason Lee.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock In \emph{International conference on machine learning}, pages
  1329--1338. PMLR, 2018.

\bibitem[Engel(2001)]{engel2001statistical}
Andreas Engel.
\newblock \emph{Statistical mechanics of learning}.
\newblock Cambridge University Press, 2001.

\bibitem[Gabri{\'e}(2020)]{gabrie2020mean}
Marylou Gabri{\'e}.
\newblock Mean-field inference methods for neural networks.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  53\penalty0 (22):\penalty0 223002, 2020.

\bibitem[Gamarnik et~al.(2019)Gamarnik, K{\i}z{\i}lda{\u{g}}, and
  Zadik]{gamarnik2019stationary}
David Gamarnik, Eren~C K{\i}z{\i}lda{\u{g}}, and Ilias Zadik.
\newblock Stationary points of shallow neural networks with quadratic
  activation function.
\newblock \emph{arXiv preprint arXiv:1912.01599}, 2019.

\bibitem[Gamarnik et~al.(2022)Gamarnik, Moore, and
  Zdeborov{\'a}]{gamarnik2022disordered}
David Gamarnik, Cristopher Moore, and Lenka Zdeborov{\'a}.
\newblock Disordered systems insights on computational hardness.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2022\penalty0 (11):\penalty0 114015, 2022.

\bibitem[Gardner and Derrida(1989)]{gardner1989three}
Elizabeth Gardner and Bernard Derrida.
\newblock Three unfinished works on the optimal storage capacity of networks.
\newblock \emph{Journal of Physics A: Mathematical and General}, 22\penalty0
  (12):\penalty0 1983, 1989.

\bibitem[Gerbelot and Berthier(2023)]{gerbelot2023graph}
C{\'e}dric Gerbelot and Rapha{\"e}l Berthier.
\newblock Graph-based approximate message passing iterations.
\newblock \emph{Information and Inference: A Journal of the IMA}, 12\penalty0
  (4):\penalty0 2562--2628, 2023.

\bibitem[Guionnet and Zeitouni(2002)]{guionnet2002large}
Alice Guionnet and Ofer Zeitouni.
\newblock Large deviations asymptotics for spherical integrals.
\newblock \emph{Journal of functional analysis}, 188\penalty0 (2):\penalty0
  461--515, 2002.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Guo et~al.(2005)Guo, Shamai, and Verd{\'u}]{guo2005mutual}
Dongning Guo, Shlomo Shamai, and Sergio Verd{\'u}.
\newblock Mutual information and minimum mean-square error in gaussian
  channels.
\newblock \emph{IEEE transactions on information theory}, 51\penalty0
  (4):\penalty0 1261--1282, 2005.

\bibitem[Gy{\"o}rgyi(1990)]{gyorgyi1990first}
G{\'e}za Gy{\"o}rgyi.
\newblock First-order transition to perfect generalization in a neural network
  with binary synapses.
\newblock \emph{Physical Review A}, 41\penalty0 (12):\penalty0 7097, 1990.

\bibitem[Hand et~al.(2018)Hand, Leong, and Voroninski]{hand2018phase}
Paul Hand, Oscar Leong, and Vlad Voroninski.
\newblock Phase retrieval under a generative prior.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Harish-Chandra(1957)]{harish1957differential}
Harish-Chandra.
\newblock Differential operators on a semisimple lie algebra.
\newblock \emph{American Journal of Mathematics}, pages 87--120, 1957.

\bibitem[Helmke and Moore(2012)]{helmke2012optimization}
Uwe Helmke and John~B Moore.
\newblock \emph{Optimization and dynamical systems}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Hu and Lu(2022)]{hu2022universality}
Hong Hu and Yue~M Lu.
\newblock Universality laws for high-dimensional learning with random features.
\newblock \emph{IEEE Transactions on Information Theory}, 69\penalty0
  (3):\penalty0 1932--1964, 2022.

\bibitem[Itzykson and Zuber(1980)]{itzykson1980planar}
Claude Itzykson and J-B Zuber.
\newblock The planar approximation. ii.
\newblock \emph{Journal of Mathematical Physics}, 21\penalty0 (3):\penalty0
  411--421, 1980.

\bibitem[Javanmard and Montanari(2013)]{javanmard2013state}
Adel Javanmard and Andrea Montanari.
\newblock State evolution for general approximate message passing algorithms,
  with applications to spatial coupling.
\newblock \emph{Information and Inference: A Journal of the IMA}, 2\penalty0
  (2):\penalty0 115--144, 2013.

\bibitem[Kunisky et~al.(2019)Kunisky, Wein, and Bandeira]{kunisky2019notes}
Dmitriy Kunisky, Alexander~S Wein, and Afonso~S Bandeira.
\newblock Notes on computational hardness of hypothesis testing: Predictions
  using the low-degree likelihood ratio.
\newblock In \emph{ISAAC Congress (International Society for Analysis, its
  Applications and Computation)}, pages 1--50. Springer, 2019.

\bibitem[Le~Cam(1960)]{cam1960locally}
Lucien Le~Cam.
\newblock Locally asymptotically normal families of distributions. certain
  approximations to families of distributions and their use in the theory of
  estimation and testing hypotheses.
\newblock \emph{Univ. California Publ. Statist.}, 3:\penalty0 37, 1960.

\bibitem[Lee and Schnelli(2016)]{lee2016tracy}
Ji~Oon Lee and Kevin Schnelli.
\newblock Tracy-widom distribution for the largest eigenvalue of real sample
  covariance matrices with general population.
\newblock \emph{The Annals of Applied Probability}, pages 3786--3839, 2016.

\bibitem[Lesieur et~al.(2017)Lesieur, Miolane, Lelarge, Krzakala, and
  Zdeborov{\'a}]{lesieur2017statistical}
Thibault Lesieur, L{\'e}o Miolane, Marc Lelarge, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock Statistical and computational phase transitions in spiked tensor
  estimation.
\newblock In \emph{2017 IEEE International Symposium on Information Theory
  (ISIT)}, pages 511--515. IEEE, 2017.

\bibitem[Li et~al.(2020)Li, Luo, and Lyu]{litowards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Luo et~al.(2019)Luo, Alghamdi, and Lu]{luo2019optimal}
Wangyu Luo, Wael Alghamdi, and Yue~M Lu.
\newblock Optimal spectral initialization for signal recovery with applications
  to phase retrieval.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (9):\penalty0 2347--2356, 2019.

\bibitem[Maillard and Bandeira(2023)]{maillard2023exact}
Antoine Maillard and Afonso~S Bandeira.
\newblock Exact threshold for approximate ellipsoid fitting of random points.
\newblock \emph{arXiv preprint arXiv:2310.05787}, 2023.

\bibitem[Maillard and Kunisky(2024)]{maillard2023fitting}
Antoine Maillard and Dmitriy Kunisky.
\newblock Fitting an ellipsoid to random points: predictions using the replica
  method.
\newblock \emph{IEEE Transactions on Information Theory}, 2024.

\bibitem[Maillard et~al.(2020)Maillard, Loureiro, Krzakala, and
  Zdeborov{\'a}]{maillard2020phase}
Antoine Maillard, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Phase retrieval in high dimensions: Statistical and computational
  phase transitions.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11071--11082, 2020.

\bibitem[Maillard et~al.(2022{\natexlab{a}})Maillard, Krzakala, Lu, and
  Zdeborov{\'a}]{maillard2022construction}
Antoine Maillard, Florent Krzakala, Yue~M Lu, and Lenka Zdeborov{\'a}.
\newblock Construction of optimal spectral methods in phase retrieval.
\newblock In \emph{Mathematical and Scientific Machine Learning}, pages
  693--720. PMLR, 2022{\natexlab{a}}.

\bibitem[Maillard et~al.(2022{\natexlab{b}})Maillard, Krzakala, M{\'e}zard, and
  Zdeborov{\'a}]{maillard2022perturbative}
Antoine Maillard, Florent Krzakala, Marc M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock Perturbative construction of mean-field equations in extensive-rank
  matrix factorization and denoising.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2022\penalty0 (8):\penalty0 083301, 2022{\natexlab{b}}.

\bibitem[Maillard et~al.(2023)Maillard, Bandeira, Belius, Dokmani{\'c}, and
  Nakajima]{maillard2023injectivity}
Antoine Maillard, Afonso~S Bandeira, David Belius, Ivan Dokmani{\'c}, and Shuta
  Nakajima.
\newblock Injectivity of relu networks: perspectives from statistical physics.
\newblock \emph{arXiv preprint arXiv:2302.14112}, 2023.

\bibitem[Maillard et~al.(2024)Maillard, Troiani, Martin, Krzakala, and
  Lenka]{github_repo}
Antoine Maillard, Emanuele Troiani, Simon Martin, Florent Krzakala, and
  Zdeborov\'a Lenka.
\newblock Numerical code used for experimental results.
\newblock \url{https://github.com/SPOC-group/ExtensiveWidthQuadraticSamples},
  2024.

\bibitem[Marchenko and Pastur(1967)]{marchenko1967distribution}
Vladimir~Alexandrovich Marchenko and Leonid~Andreevich Pastur.
\newblock Distribution of eigenvalues for some sets of random matrices.
\newblock \emph{Matematicheskii Sbornik}, 114\penalty0 (4):\penalty0 507--536,
  1967.

\bibitem[Martin et~al.(2024)Martin, Bach, and Biroli]{martin2024impact}
Simon Martin, Francis Bach, and Giulio Biroli.
\newblock On the impact of overparameterization on the training of a shallow
  neural network in high dimensions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3655--3663. PMLR, 2024.

\bibitem[Mezard and Montanari(2009)]{mezard2009information}
Marc Mezard and Andrea Montanari.
\newblock \emph{Information, physics, and computation}.
\newblock Oxford University Press, 2009.

\bibitem[M{\'e}zard et~al.(1987)M{\'e}zard, Parisi, and
  Virasoro]{mezard1987spin}
Marc M{\'e}zard, Giorgio Parisi, and Miguel~Angel Virasoro.
\newblock \emph{Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem[Mignacco et~al.(2021)Mignacco, Urbani, and
  Zdeborov{\'a}]{mignacco2021stochasticity}
Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborov{\'a}.
\newblock Stochasticity helps to navigate rough landscapes: comparing
  gradient-descent-based algorithms in the phase retrieval problem.
\newblock \emph{Machine Learning: Science and Technology}, 2\penalty0
  (3):\penalty0 035029, 2021.

\bibitem[Mondelli and Montanari(2019)]{mondelli2018fundamental}
Marco Mondelli and Andrea Montanari.
\newblock Fundamental limits of weak recovery with applications to phase
  retrieval.
\newblock \emph{Foundations of Computational Mathematics}, 19\penalty0
  (3):\penalty0 703--773, Jun 2019.

\bibitem[Montanari and Saeed(2022)]{montanari2022universality}
Andrea Montanari and Basil~N Saeed.
\newblock Universality of empirical risk minimization.
\newblock In \emph{Conference on Learning Theory}, pages 4310--4312. PMLR,
  2022.

\bibitem[Montanari and Sen(2024)]{montanari2024friendly}
Andrea Montanari and Subhabrata Sen.
\newblock A friendly tutorial on mean-field spin glass techniques for
  non-physicists.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  17\penalty0 (1):\penalty0 1--173, 2024.

\bibitem[Opper and Haussler(1991)]{Opper1991GeneralizationPO}
Opper and Haussler.
\newblock Generalization performance of bayes optimal classification algorithm
  for learning a perceptron.
\newblock \emph{Physical review letters}, 66 20:\penalty0 2677--2680, 1991.

\bibitem[Perry et~al.(2020)Perry, Wein, and Bandeira]{perry2020statistical}
Amelia Perry, Alexander~S Wein, and Afonso~S Bandeira.
\newblock Statistical limits of spiked tensor models.
\newblock In \emph{Annales de l’Institut Henri Poincar{\'e}-Probabilit{\'e}s
  et Statistiques}, volume~56, pages 230--264, 2020.

\bibitem[Pourkamali et~al.(2024)Pourkamali, Barbier, and
  Macris]{pourkamali2023matrix}
Farzad Pourkamali, Jean Barbier, and Nicolas Macris.
\newblock Matrix inference in growing rank regimes.
\newblock \emph{IEEE Transactions on Information Theory}, 2024.

\bibitem[Rangan(2011)]{rangan2011generalized}
Sundeep Rangan.
\newblock Generalized approximate message passing for estimation with random
  linear mixing.
\newblock In \emph{2011 IEEE International Symposium on Information Theory
  Proceedings}, pages 2168--2172. IEEE, 2011.

\bibitem[Ros et~al.(2019)Ros, Ben~Arous, Biroli, and Cammarota]{ros2019complex}
Valentina Ros, G\'erard Ben~Arous, Giulio Biroli, and Chiara Cammarota.
\newblock Complex energy landscapes in spiked-tensor and simple glassy models:
  Ruggedness, arrangements of local minima, and phase transitions.
\newblock \emph{Physical Review X}, 9\penalty0 (1):\penalty0 011003, 2019.

\bibitem[Sarao~Mannelli et~al.(2020{\natexlab{a}})Sarao~Mannelli, Biroli,
  Cammarota, Krzakala, Urbani, and Zdeborov{\'a}]{sarao2020complex}
Stefano Sarao~Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala,
  Pierfrancesco Urbani, and Lenka Zdeborov{\'a}.
\newblock Complex dynamics in simple neural networks: Understanding gradient
  flow in phase retrieval.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3265--3274, 2020{\natexlab{a}}.

\bibitem[Sarao~Mannelli et~al.(2020{\natexlab{b}})Sarao~Mannelli,
  Vanden-Eijnden, and Zdeborov{\'a}]{sarao2020optimization}
Stefano Sarao~Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborov{\'a}.
\newblock Optimization and generalization of shallow neural networks with
  quadratic activation functions.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 13445--13455, 2020{\natexlab{b}}.

\bibitem[Schwarze(1993)]{schwarze1993learning}
Henry Schwarze.
\newblock Learning a rule in a multilayer neural network.
\newblock \emph{Journal of Physics A: Mathematical and General}, 26\penalty0
  (21):\penalty0 5781, 1993.

\bibitem[Semerjian(2024)]{semerjian2024matrix}
Guilhem Semerjian.
\newblock Matrix denoising: Bayes-optimal estimators via low-degree
  polynomials.
\newblock \emph{arXiv preprint arXiv:2402.16719}, 2024.

\bibitem[Seung et~al.(1992)Seung, Sompolinsky, and
  Tishby]{seung1992statistical}
Hyunjune~Sebastian Seung, Haim Sompolinsky, and Naftali Tishby.
\newblock Statistical mechanics of learning from examples.
\newblock \emph{Physical review A}, 45\penalty0 (8):\penalty0 6056, 1992.

\bibitem[Silverstein and Choi(1995)]{silverstein1995analysis}
Jack~W Silverstein and Sang-Il Choi.
\newblock Analysis of the limiting spectral distribution of large dimensional
  random matrices.
\newblock \emph{Journal of Multivariate Analysis}, 54\penalty0 (2):\penalty0
  295--309, 1995.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (2):\penalty0 742--769, 2018.

\bibitem[Sompolinsky et~al.(1990)Sompolinsky, Tishby, and
  Seung]{sompolinsky1990learning}
Haim Sompolinsky, Naftali Tishby, and H~Sebastian Seung.
\newblock Learning from examples in large neural networks.
\newblock \emph{Physical Review Letters}, 65\penalty0 (13):\penalty0 1683,
  1990.

\bibitem[Song et~al.(2021)Song, Zadik, and Bruna]{song2021cryptographic}
Min~Jae Song, Ilias Zadik, and Joan Bruna.
\newblock On the cryptographic hardness of learning single periodic neurons.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 29602--29615, 2021.

\bibitem[Speicher(1993)]{speicher1993free}
Roland Speicher.
\newblock Free convolution and the random sum of matrices.
\newblock \emph{Publications of the Research Institute for Mathematical
  Sciences}, 29\penalty0 (5):\penalty0 731--744, 1993.

\bibitem[St{\"o}ger and Soltanolkotabi(2021)]{stoger2021small}
Dominik St{\"o}ger and Mahdi Soltanolkotabi.
\newblock Small random initialization is akin to spectral learning:
  Optimization and generalization guarantees for overparametrized low-rank
  matrix reconstruction.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 23831--23843, 2021.

\bibitem[Tulino and Verd{\'u}(2004)]{tulino2004random}
Antonia~M Tulino and Sergio Verd{\'u}.
\newblock Random matrix theory and wireless communications.
\newblock \emph{Foundations and Trends{\textregistered} in Communications and
  Information Theory}, 1\penalty0 (1):\penalty0 1--182, 2004.

\bibitem[Venturi et~al.(2019)Venturi, Bandeira, and Bruna]{venturi2019spurious}
Luca Venturi, Afonso~S Bandeira, and Joan Bruna.
\newblock Spurious valleys in one-hidden-layer neural network optimization
  landscapes.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (133):\penalty0 1--34, 2019.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Watkin et~al.(1993)Watkin, Rau, and Biehl]{watkin1993statistical}
Timothy~LH Watkin, Albrecht Rau, and Michael Biehl.
\newblock The statistical mechanics of learning a rule.
\newblock \emph{Reviews of Modern Physics}, 65\penalty0 (2):\penalty0 499,
  1993.

\bibitem[Wigner(1955)]{wigner1955characteristic}
Eugene~P Wigner.
\newblock Characteristic vectors of bordered matrices with infinite dimensions.
\newblock \emph{Annals of Mathematics}, pages 548--564, 1955.

\bibitem[Zdeborov{\'a} and Krzakala(2016)]{zdeborova2016statistical}
Lenka Zdeborov{\'a} and Florent Krzakala.
\newblock Statistical physics of inference: Thresholds and algorithms.
\newblock \emph{Advances in Physics}, 65\penalty0 (5):\penalty0 453--552, 2016.

\end{thebibliography}
