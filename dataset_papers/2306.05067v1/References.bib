% Datasets
% ADE20K ?
@inproceedings{zhou2017scene,
  title={Scene parsing through ade20k dataset},
  author={Zhou, Bolei and Zhao, Hang and Puig, Xavier and Fidler, Sanja and Barriuso, Adela and Torralba, Antonio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}
% FPN
@inproceedings{lin2017feature,
  title={Feature pyramid networks for object detection},
  author={Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2017}
}
% VTAB ? 
@article{zhai2019large,
  title={A large-scale study of representation learning with the visual task adaptation benchmark},
  author={Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and others},
  journal={arXiv preprint arXiv:1910.04867},
  year={2019}
}
% KITTI
@article{geiger2013vision,
  title={Vision meets robotics: The kitti dataset},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}
% FGVC
% CUB
@article{wah2011caltech,
  title={The caltech-ucsd birds-200-2011 dataset},
  author={Wah, Catherine and Branson, Steve and Welinder, Peter and Perona, Pietro and Belongie, Serge},
  year={2011},
  publisher={California Institute of Technology}
}
% NaBirds
@inproceedings{van2015building,
  title={Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection},
  author={Van Horn, Grant and Branson, Steve and Farrell, Ryan and Haber, Scott and Barry, Jessie and Ipeirotis, Panos and Perona, Pietro and Belongie, Serge},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2015}
}
% Dogs
@inproceedings{khosla2011novel,
  title={Novel dataset for fine-grained image categorization: Stanford dogs},
  author={Khosla, Aditya and Jayadevaprakash, Nityananda and Yao, Bangpeng and Li, Fei-Fei},
  booktitle={Proc. CVPR workshop on fine-grained visual categorization (FGVC)},
  volume={2},
  number={1},
  year={2011},
  organization={Citeseer}
}
% Cars
@inproceedings{gebru2017fine,
  title={Fine-grained car detection for visual census estimation},
  author={Gebru, Timnit and Krause, Jonathan and Wang, Yilun and Chen, Duyun and Deng, Jia and Fei-Fei, Li},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}
% Flowers
@inproceedings{nilsback2008automated,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing},
  year={2008},
  organization={IEEE}
}

%  ImageNet
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2009},
  organization={IEEE}
}

%  -------------------------------------------------------------------------------------------------------------------------------------

% Deep neural networks abstract information
@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  year={2015},
  publisher={Elsevier}
}
@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{kriegeskorte2015deep,
  title={Deep neural networks: a new framework for modelling biological vision and brain information processing},
  author={Kriegeskorte, Nikolaus},
  journal={biorxiv},
  year={2015},
  publisher={Cold Spring Harbor Laboratory}
}
%  -------------------------------------------------------------------------------------------------------------------------------------
% ViT
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
% DeiT
@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  year={2021},
  organization={PMLR}
}
% Swin Transformer
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}
% Do vision transformres see likt CCNs?
@article{raghu2021vision,
  title={Do vision transformers see like convolutional neural networks?},
  author={Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
% SETR
@inproceedings{zheng2021rethinking,
  title={Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  author={Zheng, Sixiao and Lu, Jiachen and Zhao, Hengshuang and Zhu, Xiatian and Luo, Zekun and Wang, Yabiao and Fu, Yanwei and Feng, Jianfeng and Xiang, Tao and Torr, Philip HS and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}
%  -------------------------------------------------------------------------------------------------------------------------------------
% SSL
% SimCLR
@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  year={2020},
  organization={PMLR}
}
% MoCo
@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2020}
}
%  MAE
@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
% SimMIM
@inproceedings{xie2022simmim,
  title={Simmim: A simple framework for masked image modeling},
  author={Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}
 % MoCo v3
 @inproceedings{chen2021empirical,
  title={An empirical study of training self-supervised vision transformers},
  author={Chen, Xinlei and Xie, Saining and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}
% DINO
@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}
% BEIT
@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}
% IBOT
@article{zhou2021ibot,
  title={ibot: Image bert pre-training with online tokenizer},
  author={Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  journal={arXiv preprint arXiv:2111.07832},
  year={2021}
}


% BERT
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
% Roberta
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
% Transformer
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
% BYOL
@article{grill2020bootstrap,
  title={Bootstrap your own latent-a new approach to self-supervised learning},
  author={Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
%  -------------------------------------------------------------------------------------------------------------------------------------
% SSL Analysis
% dark secret of MIM
@article{xie2022revealing,
  title={Revealing the Dark Secrets of Masked Image Modeling},
  author={Xie, Zhenda and Geng, Zigang and Hu, Jingcheng and Zhang, Zheng and Hu, Han and Cao, Yue},
  journal={arXiv preprint arXiv:2205.13543},
  year={2022}
}
%  what makes good for cl transfer learning
@article{zhao2020makes,
  title={What makes instance discrimination good for transfer learning?},
  author={Zhao, Nanxuan and Wu, Zhirong and Lau, Rynson WH and Lin, Stephen},
  journal={arXiv preprint arXiv:2006.06606},
  year={2020}
}
% high fidelity visualization
@article{bordes2021high,
  title={High Fidelity Visualization of What Your Self-Supervised Representation Knows About},
  author={Bordes, Florian and Balestriero, Randall and Vincent, Pascal},
  journal={arXiv preprint arXiv:2112.09164},
  year={2021}
}
% broad study of contrastive transfer
@inproceedings{islam2021broad,
  title={A broad study on the transferability of visual representations with contrastive learning},
  author={Islam, Ashraful and Chen, Chun-Fu Richard and Panda, Rameswar and Karlinsky, Leonid and Radke, Richard and Feris, Rogerio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}
% how well do ssl transfer
@inproceedings{ericsson2021well,
  title={How well do self-supervised models transfer?},
  author={Ericsson, Linus and Gouk, Henry and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2021}
}
% what do ssl vit learn?
@article{park2023self,
  title={What Do Self-Supervised Vision Transformers Learn?},
  author={Park, Namuk and Kim, Wonjae and Heo, Byeongho and Kim, Taekyung and Yun, Sangdoo},
  journal={arXiv preprint arXiv:2305.00729},
  year={2023}
}
%  -------------------------------------------------------------------------------------------------------------------------------------
% Tuning methods
% VPT
@article{jia2022visual,
  title={Visual prompt tuning},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  journal={arXiv preprint arXiv:2203.12119},
  year={2022}
}
%  Adapter
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  year={2019},
  organization={PMLR}
}
% Prefix Tuning
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
% prompt tuning
@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}
% WARP: Word-level Adversarial ReProgramming
@article{hambardzumyan2021warp,
  title={Warp: Word-level adversarial reprogramming},
  author={Hambardzumyan, Karen and Khachatrian, Hrant and May, Jonathan},
  journal={arXiv preprint arXiv:2101.00121},
  year={2021}
}
% Learning how to ask
@article{qin2021learning,
  title={Learning how to ask: Querying lms with mixtures of soft prompts},
  author={Qin, Guanghui and Eisner, Jason},
  journal={arXiv preprint arXiv:2104.06599},
  year={2021}
}
% Visual Query Tuning
@article{tu2022visual,
  title={Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning},
  author={Tu, Cheng-Hao and Mai, Zheda and Chao, Wei-Lun},
  journal={arXiv preprint arXiv:2212.03220},
  year={2022}
}
% Visual Prompting
@article{bahng2022visual,
  title={Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models},
  author={Bahng, Hyojin and Jahanian, Ali and Sankaranarayanan, Swami and Isola, Phillip},
  journal={arXiv preprint arXiv:2203.17274},
  year={2022}
}
% Bias
@article{cai2020tinytl,
  title={Tinytl: Reduce memory, not parameters for efficient on-device learning},
  author={Cai, Han and Gan, Chuang and Zhu, Ligeng and Han, Song},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
% HeadToToe
@inproceedings{evci2022head2toe,
  title={Head2toe: Utilizing intermediate representations for better transfer learning},
  author={Evci, Utku and Dumoulin, Vincent and Larochelle, Hugo and Mozer, Michael C},
  booktitle={International Conference on Machine Learning},
  year={2022},
  organization={PMLR}
}
% AdaptFormer
@article{chen2022adaptformer,
  title={Adaptformer: Adapting vision transformers for scalable visual recognition},
  author={Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
  journal={arXiv preprint arXiv:2205.13535},
  year={2022}
}
%  -------------------------------------------------------------------------------------------------------------------------------------
% Analysis
% DIP
@inproceedings{ulyanov2018deep,
  title={Deep image prior},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2018}
}
% Uniformity
@inproceedings{wang2020understanding,
  title={Understanding contrastive representation learning through alignment and uniformity on the hypersphere},
  author={Wang, Tongzhou and Isola, Phillip},
  booktitle={International Conference on Machine Learning},
  year={2020},
  organization={PMLR}
}

% gumbel
@article{geng2020does,
  title={How does selective mechanism improve self-attention networks?},
  author={Geng, Xinwei and Wang, Longyue and Wang, Xing and Qin, Bing and Liu, Ting and Tu, Zhaopeng},
  journal={arXiv preprint arXiv:2005.00979},
  year={2020}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}


@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@article{layernorm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

% PromptGating
@inproceedings{huang2023extensibleacl,
  title={An Extensible Plug-and-Play Method for Multi-Aspect Controllable Text Generation},
  author={Xuancheng Huang and Zijun Liu and Peng Li and Tao Li and Maosong Sun and Yang Liu},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  year={2023}
}