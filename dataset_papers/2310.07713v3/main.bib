@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={NeurIPS},
  year={2022}
}

@inproceedings{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={ICLR},
  year={2022}
}

@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@article{shi2023context,
  title={In-Context Pretraining: Language Modeling Beyond Document Boundaries},
  author={Shi, Weijia and Min, Sewon and Lomeli, Maria and Zhou, Chunting and Li, Margaret and Lin, Victoria and Smith, Noah A and Zettlemoyer, Luke and Yih, Scott and Lewis, Mike},
  journal={arXiv preprint arXiv:2310.10638},
  year={2023}
}

@inproceedings{petroni-etal-2021-kilt,
    title = "{KILT}: a Benchmark for Knowledge Intensive Language Tasks",
    author = {Petroni, Fabio  and
      Piktus, Aleksandra  and
      Fan, Angela  and
      Lewis, Patrick  and
      Yazdani, Majid  and
      De Cao, Nicola  and
      Thorne, James  and
      Jernite, Yacine  and
      Karpukhin, Vladimir  and
      Maillard, Jean  and
      Plachouras, Vassilis  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian},
    booktitle = "NAACL",
    year = "2021",
    abstract = "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at \-url{https://github.com/facebookresearch/KILT}.",
}

@article{wang2023far,
  title   = {How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources},
  author  = {Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2306.04751},
  url     = {https://arxiv.org/abs/2306.04751v1},
  pdf     = {https://arxiv.org/pdf/2306.04751.pdf}
}

@inproceedings{
lin2024radit,
title={{RA}-{DIT}: Retrieval-Augmented Dual Instruction Tuning},
author={Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Richard James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=22OTbutug9}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{bai2022training,
  title   = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author  = {Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
  year    = {2022},
  journal = {arXiv preprint arXiv: 2204.05862},
  url     = {https://arxiv.org/abs/2204.05862v1},
  pdf     = {https://arxiv.org/pdf/2204.05862.pdf}
}

@inproceedings{sanh2022multitask,
  title     = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  author    = {Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=9Vrb9D0WI4},
  pdf       = {https://openreview.net/pdf?id=9Vrb9D0WI4}
}

@article{chung2022scaling,
  title   = {Scaling Instruction-Finetuned Language Models},
  author  = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
  year    = {2022},
  journal = {arXiv preprint arXiv: 2210.11416},
  url     = {https://arxiv.org/abs/2210.11416v5},
  pdf     = {https://arxiv.org/pdf/2210.11416.pdf}
}

@article{izacard2022atlas,
  title   = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
  author  = {Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
  year    = {2022},
  journal = {arXiv preprint arXiv: 2208.03299}
}

@inproceedings{wang2023shall,
  title={Shall we pretrain autoregressive language models with retrieval? a comprehensive study},
  author={Wang, Boxin and Ping, Wei and Xu, Peng and McAfee, Lawrence and Liu, Zihan and Shoeybi, Mohammad and Dong, Yi and Kuchaiev, Oleksii and Li, Bo and Xiao, Chaowei and others},
  booktitle={EMNLP},
  year={2023}
}

@article{du2021glam,
  title     = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
  author    = {Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and M. Krikun and Yanqi Zhou and A. Yu and Orhan Firat and Barret Zoph and L. Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and K. Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V. Le and Yonghui Wu and Z. Chen and Claire Cui},
  journal   = {International Conference on Machine Learning},
  year      = {2021},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/80d0116d77beeded0c23cf48946d9d10d4faee14},
  url       = {https://arxiv.org/abs/2112.06905v2},
  pdf       = {https://arxiv.org/pdf/2112.06905.pdf}
}

@article{huang2023raven,
  title={RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models},
  author={Huang, Jie and Ping, Wei and Xu, Peng and Shoeybi, Mohammad and Chang, Kevin Chen-Chuan and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2308.07922},
  year={2023}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={ICML},
  year={2022}
}

@article{llama2,
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author  = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.09288},
  url     = {https://arxiv.org/abs/2307.09288v2},
  pdf     = {https://arxiv.org/pdf/2307.09288.pdf}
}

@article{llama,
  title   = {LLaMA: Open and Efficient Foundation Language Models},
  author  = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year    = {2023},
  journal = {ARXIV},
  url     = {https://arxiv.org/abs/2302.13971v1},
  pdf     = {https://arxiv.org/pdf/2302.13971.pdf}
}


@article{gpt3,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {Advances in neural information processing systems},
  volume  = {33},
  pages   = {1877-1901},
  year    = {2020}
}

@article{hnsw,
  title     = {Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author    = {Malkov, Yu A and Yashunin, Dmitry A},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {42},
  number    = {4},
  pages     = {824-836},
  year      = {2018},
  publisher = {IEEE}
}

@ARTICLE{pq,
  author={Gray, R.M. and Neuhoff, D.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={Quantization}, 
  year={1998},
  volume={44},
  number={6},
  pages={2325-2383},
  doi={10.1109/18.720541}}

@ARTICLE{opq,
  author={Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Optimized Product Quantization}, 
  year={2014},
  volume={36},
  number={4},
  pages={744-755},
  doi={10.1109/TPAMI.2013.240}}


@article{lee2022factuality,
  title   = {Factuality Enhanced Language Models for Open-Ended Text Generation},
  author  = {Nayeon Lee and Wei Ping and Peng Xu and Mostofa Patwary and Pascale Fung and Mohammad Shoeybi and Bryan Catanzaro},
  year    = {2022},
  journal = {NeurIPS}
}

@article{nakano2021webgpt,
  title={Web{GPT}: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@software{lmharness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{lin2021truthfulqa,
  title     = {Truthful{QA}: Measuring How Models Mimic Human Falsehoods},
  author    = {Stephanie C. Lin and Jacob Hilton and Owain Evans},
  journal   = {ACL},
  year      = {2021},
}


@inproceedings{khandelwal2019generalization,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={ICLR},
  year={2020}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={EMNLP},
  year={2020}
}

@article{zhong2021qmsum,
  title={QMSum: A new benchmark for query-based multi-domain meeting summarization},
  author={Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and others},
  journal={arXiv preprint arXiv:2104.05938},
  year={2021}
}

@article{Chen2021SummScreenAD,
  title={SummScreen: A Dataset for Abstractive Screenplay Summarization},
  author={Mingda Chen and Zewei Chu and Sam Wiseman and Kevin Gimpel},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.07091},
  url={https://api.semanticscholar.org/CorpusID:233240744}
}

@article{Huang2021EfficientAF,
  title={Efficient Attentions for Long Document Summarization},
  author={Luyang Robby Huang and Shuyang Cao and Nikolaus Nova Parulian and Heng Ji and Lu Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.02112},
  url={https://api.semanticscholar.org/CorpusID:233033613}
}

@inproceedings{guu2020retrieval,
  title={{REALM}: Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={ICML},
  year={2020}
}

@inproceedings{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{lin2023train,
  title   = {How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval},
  author  = {Sheng-Chieh Lin and Akari Asai and Minghan Li and Barlas Oguz and Jimmy Lin and Yashar Mehdad and Wen-tau Yih and Xilun Chen},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2302.07452}
}

@article{faiss,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@article{nucleus,
  title     = {The Curious Case of Neural Text Degeneration},
  author    = {Ari Holtzman and Jan Buys and Maxwell Forbes and Yejin Choi},
  journal   = {International Conference On Learning Representations},
  year      = {2019},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/cf4aa38ae31b43fd07abe13b4ffdb265babb7be1}
}

@article{izacard2022few,
  title={Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={arXiv preprint arXiv:2208.03299},
  year={2022}
}

@article{piantadosi14zipfs,
  publisher = {Springer Science and Business Media LLC},
  author    = {Steven T. Piantadosi},
  title     = {Zipf’s word frequency law in natural language: A critical review and future directions},
  year      = {2014},
  doi       = {10.3758/s13423-014-0585-6},
  pages     = {1112-1130},
  volume    = {21},
  journal   = {Psychonomic Bulletin \& Review},
  pdf       = {https://link.springer.com/content/pdf/10.3758/s13423-014-0585-6.pdf},
  url       = {https://link.springer.com/article/10.3758/s13423-014-0585-6/fulltext.html}
}

@inproceedings{selfbleu,
author = {Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
title = {Texygen: A Benchmarking Platform for Text Generation Models},
year = {2018},
isbn = {9781450356572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209978.3210080},
doi = {10.1145/3209978.3210080},
abstract = {We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not only implemented a majority of text generation models, but also covered a set of metrics that evaluate the diversity, the quality and the consistency of the generated texts. The Texygen platform could help standardize the research on text generation and improve the reproductivity and reliability of future research work in text generation.},
booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
pages = {1097–1100},
numpages = {4},
keywords = {evaluation metrics, benchmarking, text generation},
location = {Ann Arbor, MI, USA},
series = {SIGIR '18}
}

@inproceedings{hans,
    title = "Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot {NLI}",
    author = "Zhou, Yangqiaoyu  and
      Tan, Chenhao",
    booktitle = "Proceedings of the Second Workshop on Insights from Negative Results in NLP",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.insights-1.17",
    doi = "10.18653/v1/2021.insights-1.17",
    pages = "117--124",
    abstract = "Although neural models have shown strong performance in datasets such as SNLI, they lack the ability to generalize out-of-distribution (OOD). In this work, we formulate a few-shot learning setup and examine the effects of natural language explanations on OOD generalization. We leverage the templates in the HANS dataset and construct templated natural language explanations for each template. Although generated explanations show competitive BLEU scores against ground truth explanations, they fail to improve prediction performance. We further show that generated explanations often hallucinate information and miss key elements that indicate the label.",
}

@article{izacardunsupervised,
  title={Unsupervised Dense Information Retrieval with Contrastive Learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={Transactions on Machine Learning Research}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020}
}

@misc{chatgpt,
  title={Chat{GPT}},
  author={OpenAI},
  year={2022},
  howpublished={\url{https://chat.openai.com}},
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={Journal of Machine Learning Research},
  year={2020}
}

% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{adapter,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  booktitle={ICML},
  year={2019}
}

@inproceedings{wang2021adversarial,
title={Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
booktitle={Advances in Neural Information Processing Systems},
year={2021}
}

@inproceedings{
wang2022semattack,
author = {Wang, Boxin and Xu, Chejian and Liu, Xiangyu and Cheng, Yu and Li, Bo},
title = {{S}em{A}ttack: Natural Textual Attacks via Different Semantic Spaces},
year = {2022},
booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}
}

@inproceedings{wang2020t3,
  title={T3: Tree-Autoencoder Regularized Adversarial Text Generation for Targeted Attack},
  author={Wang, Boxin and Pei, Hengzhi and Pan, Boyuan and Chen, Qian and Wang, Shuohang and Li, Bo},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={6134--6150},
  year={2020}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}


@inproceedings{anli,
  title={Adversarial NLI: A New Benchmark for Natural Language Understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  booktitle={ACL},
  year={2020}
}

@inproceedings{booq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{izacard2021leveraging,
  title={Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
  author={Izacard, Gautier and Grave, {\'E}douard},
  booktitle={Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  pages={874--880},
  year={2021}
}


@inproceedings{li-etal-2016-diversity,
    title = "A Diversity-Promoting Objective Function for Neural Conversation Models",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1014",
    doi = "10.18653/v1/N16-1014",
    pages = "110--119",
}

@inproceedings{baheti-etal-2021-just,
    title = "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts",
    author = "Baheti, Ashutosh  and
      Sap, Maarten  and
      Ritter, Alan  and
      Riedl, Mark",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.397",
    doi = "10.18653/v1/2021.emnlp-main.397",
    pages = "4846--4862",
    abstract = "Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42{\%} of human responses agree with toxic comments, whereas only 13{\%} agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19{\%} reduction in agreement with offensive comments and produces 29{\%} fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.",
}

@inproceedings{hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={ACL},
  year={2019}
}

@inproceedings{winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{wic,
  title={WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={NAACL},
  year={2016}
}

@inproceedings{dhamala2021bold,
  title={Bold: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={862--872},
  year={2021}
}

@inproceedings{race,
  title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  booktitle={EMNLP},
  year={2017}
}

@inproceedings{piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={AAAI},
  year={2020}
}

@article{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}

@article{welbl2021challenges,
  title={Challenges in detoxifying language models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  journal={Findings of EMNLP},
  year={2021}
}


@inproceedings{dathathri2019plug,
  title={Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
  author={Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  booktitle={ICLR},
  year={2019}
}

@article{selfdebiasing,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={TACL},
  year={2021}
}

@article{megatron,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Mohammad Shoeybi and Mostofa Ali Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  journal={ArXiv},
  year={2019}
}

@article{mcguffie2020radicalization,
  title={The radicalization risks of {GPT}-3 and advanced neural language models},
  author={McGuffie, Kris and Newhouse, Alex},
  journal={arXiv},
  year={2020}
}

@article{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{MegatronTuring,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model},
  author={Kharya, Paresh and Alvi, Ali},
  year={2021},
}

@article{krause2020gedi,
  title={Ge{D}i: Generative discriminator guided sequence generation},
  author={Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
  journal={arXiv},
  year={2020}
}

@article{solaiman2021process,
  title={Process for Adapting Language Models to Society ({PALMS}) with Values-Targeted Datasets},
  author={Solaiman, Irene and Dennison, Christy},
  journal={arXiv preprint arXiv:2106.10328},
  year={2021}
}

@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{welleck2019neural,
  title={Neural text generation with unlikelihood training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@inproceedings{transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    booktitle={ICLR},
}

@article{survey,
    title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
    author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
    year={2021},
    journal={arXiv},
}

@article{prompt1,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv},
  year={2020}
}

@inproceedings{li2021prefix,
  title={Prefix-{T}uning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={ACL},
  year={2021}
}


@article{prompt3,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv},
  year={2021}
}

@article{prompt4,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Tony Z and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={arXiv},
  year={2021}
}

@article{prompt5,
  title={Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference},
  author={Timo Schick and Hinrich Schütze},
  journal={arXiv},
  year={2020}
}

@article{prompt6,
  title={It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  author={Timo Schick and Hinrich Schütze},
  journal={arXiv},
  year={2020}
}

@inproceedings{liu2021dexperts,
  title={D{E}xperts: Decoding-time controlled text generation with experts and anti-experts},
  author={Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi, Yejin},
  booktitle={ACL},
  year={2021}
}

@article{basta2019evaluating,
  title={Evaluating the underlying gender bias in contextualized word embeddings},
  author={Basta, Christine and Costa-Juss{\`a}, Marta R and Casas, Noe},
  journal={arXiv preprint arXiv:1904.08783},
  year={2019}
}

@inproceedings{gehman2020realtoxicityprompts,
  title={Real{T}oxicity{P}rompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  booktitle={Findings in EMNLP},
  year={2020}
}

@inproceedings{gururangan2020don,
  title={Don't stop pretraining: adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={ACL},
  year={2020}
}

@inproceedings{may2019measuring,
  title={On measuring social biases in sentence encoders},
  author={May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R and Rudinger, Rachel},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  booktitle={EMNLP},
  year={2019}
}

@inproceedings{xu2021detoxifying,
  title={Detoxifying language models risks marginalizing minority voices},
  author={Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{zhao2019gender,
  title={Gender bias in contextualized word embeddings},
  author={Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Cotterell, Ryan and Ordonez, Vicente and Chang, Kai-Wei},
  booktitle={NAACL},
  year={2019}
}

@article{smith2022using,
  title={Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model},
  author={Shaden Smith and Mostofa Patwary and Brandon Norick and Patrick LeGresley and Samyam Rajbhandari and Jared Casper and  Zhun Liu and Shrimai Prabhumoye and George Zerveas and Vijay Korthikanti and Elton Zhang and Rewon Child and Reza Yazdani Aminabadi and Julie Bernauer and Xia Song and Mohammad Shoeybi and Yuxiong He and Michael Houston and Saurabh Tiwary and Bryan Catanzaro
},
  journal={arXiv},
  year={2022}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@inproceedings{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  booktitle={EMNLP},
  year={2016}
}

@inproceedings{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  booktitle={NIPS},
  year={2015}
}

@article{hinton2002training,
  title={Training products of experts by minimizing contrastive divergence},
  author={Hinton, Geoffrey E},
  journal={Neural computation},
  year={2002},
}

@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{kim2022soda,
  title   = {SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization},
  author  = {Hyunwoo Kim and Jack Hessel and Liwei Jiang and Peter West and Ximing Lu and Youngjae Yu and Pei Zhou and Ronan Le Bras and Malihe Alikhani and Gunhee Kim and Maarten Sap and Yejin Choi},
  year    = {2022},
  journal = {arXiv preprint arXiv: 2212.10465},
  url     = {https://arxiv.org/abs/2212.10465v2},
  pdf     = {https://arxiv.org/pdf/2212.10465.pdf}
}

@article{fan2019eli5,
  title     = {ELI5: Long Form Question Answering},
  author    = {Angela Fan and Yacine Jernite and Ethan Perez and David Grangier and J. Weston and Michael Auli},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2019},
  doi       = {10.18653/v1/P19-1346},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ebf59587f8f170ff4241c42263bbfb9da5bd2135}
}

@article{longpre2023flan,
  title     = {The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author    = {S. Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
  journal   = {International Conference on Machine Learning},
  year      = {2023},
  doi       = {10.48550/arXiv.2301.13688},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f2b0017ddd77fa38760a18145e63553105a1a236}
}

@article{joshi2017triviaqa,
  title     = {TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author    = {Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2017},
  doi       = {10.18653/v1/P17-1147},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/f010affab57b5fcf1cd6be23df79d8ec98c7289c}
}

@article{trischler2016newsqa,
  title     = {NewsQA: A Machine Comprehension Dataset},
  author    = {A. Trischler and Tong Wang and Xingdi Yuan and Justin Harris and Alessandro Sordoni and Philip Bachman and Kaheer Suleman},
  journal   = {REP4NLP@ACL},
  year      = {2016},
  doi       = {10.18653/v1/W17-2623},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/3eda43078ae1f4741f09be08c4ecab6229046a5c}
}

@article{rajpurkar2016squad,
  title     = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author    = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2016},
  doi       = {10.18653/v1/D16-1264},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/05dd7254b632376973f3a1b4d39485da17814df5}
}

@article{rajpurkar2018suqad2,
  title     = {Know What You Don't Know: Unanswerable Questions for SQuAD},
  author    = {Pranav Rajpurkar and Robin Jia and Percy Liang},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2018},
  doi       = {10.18653/v1/P18-2124},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/4d1c856275744c0284312a3a50efb6ca9dc4cd4c}
}

@article{dua2019drop,
  title     = {DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  author    = {Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
  journal   = {North American Chapter of the Association for Computational Linguistics},
  year      = {2019},
  doi       = {10.18653/v1/N19-1246},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/dda6fb309f62e2557a071522354d8c2c897a2805}
}


@article{laurenccon2022bigscience,
  title   = {The bigscience roots corpus: A 1.6 tb composite multilingual dataset},
  author  = {Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and Villanova del Moral, Albert and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Gonz{\'a}lez Ponferrada, Eduardo and Nguyen, Huu and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {31809-31826},
  year    = {2022}
}

@article{feng2020doc2dial,
  title     = {doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset},
  author    = {Song Feng and H. Wan and R. Chulaka Gunasekara and S. Patel and Sachindra Joshi and L. Lastras},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2020},
  doi       = {10.18653/v1/2020.emnlp-main.652},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/206dad0b40920a68d0223e3721626ae0e6e12faa}
}

@article{kovcisky2018narrativeqa,
  title     = {The narrativeqa reading comprehension challenge},
  author    = {Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {6},
  pages     = {317-328},
  year      = {2018},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{dasigi2019quoref,
  title     = {Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning},
  author    = {Pradeep Dasigi and Nelson F. Liu and Ana Marasović and Noah A. Smith and Matt Gardner},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2019},
  doi       = {10.18653/v1/D19-1606},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/3838387ea8dd1bb8c2306be5a63c1c120075c5a2}
}

@article{kwiatkowski-etal-2019-natural,
  title     = {Natural Questions: A Benchmark for Question Answering Research},
  author    = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  year      = {2019},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q19-1026},
  doi       = {10.1162/tacl_a_00276},
  pages     = {452-466},
  abstract  = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
  pdf       = {https://aclanthology.org/Q19-1026.pdf}
}

@article{köpf2023openassistant,
  title   = {OpenAssistant Conversations - Democratizing Large Language Model Alignment},
  author  = {Andreas Köpf and Yannic Kilcher and Dimitri von Rütte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Richárd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2304.07327}
}

@article{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    journal   = {databricks}
}

@article{wei2022chain,
  title   = {Chain-of-thought prompting elicits reasoning in large language models},
  author  = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {24824-24837},
  year    = {2022}
}

@article{honovich2022unnatural,
  title     = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
  author    = {Or Honovich and Thomas Scialom and Omer Levy and Timo Schick},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2022},
  doi       = {10.48550/arXiv.2212.09689},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/6f4cc536f9ed83d0dbf7e919dc609be12aa0848a}
}

@article{wang2022selfinstruct,
  title     = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author    = {Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
  journal   = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2022},
  doi       = {10.48550/arXiv.2212.10560},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/e65b346d442e9962a4276dc1c1af2956d9d5f1eb}
}

@article{kudo2018sentencepiece,
  title     = {SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author    = {Taku Kudo and John Richardson},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2018},
  doi       = {10.18653/v1/D18-2012},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/b5246fa284f86b544a7c31f050b3bd0defd053fd}
}

@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@inproceedings{petroni2019language,
  title={Language models as knowledge bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H and Riedel, Sebastian},
  booktitle={EMNLP},
  year={2019}
}


@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@inproceedings{meng2022locating,
  title={Locating and editing factual knowledge in {GPT}},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  booktitle={NeurIPS},
  year={2022}
}

@article{yogatama2021adaptive,
  title={Adaptive semiparametric language models},
  author={Yogatama, Dani and de Masson d’Autume, Cyprien and Kong, Lingpeng},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  publisher={MIT Press}
}

@inproceedings{bilotti2007structured,
  title={Structured retrieval for question answering},
  author={Bilotti, Matthew W and Ogilvie, Paul and Callan, Jamie and Nyberg, Eric},
  booktitle={Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval},
  year={2007}
}

@article{shuster2021retrieval,
  title={Retrieval augmentation reduces hallucination in conversation},
  author={Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe and Weston, Jason},
  journal={arXiv preprint arXiv:2104.07567},
  year={2021}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@inproceedings{zhang2018guiding,
  title={Guiding neural machine translation with retrieved translation pieces},
  author={Zhang, Jingyi and Utiyama, Masao and Sumita, Eiichro and Neubig, Graham and Nakamura, Satoshi},
  booktitle={NAACL},
  year={2018}
}

@article{komeili2021internet,
  title={Internet-augmented dialogue generation},
  author={Komeili, Mojtaba and Shuster, Kurt and Weston, Jason},
  journal={arXiv preprint arXiv:2107.07566},
  year={2021}
}

@article{izacard2022unsupervised,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
   journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{lewis2019bart,
  title={{BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  booktitle={ACL},
  year={2020}
}

@article{openai2023gpt4,
  title={G{PT-4} Technical Report},
  author={OpenAI},
   journal={arXiv},
  year={2023}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@inproceedings{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{mishra2021cross,
  title={Cross-task generalization via natural language crowdsourcing instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle={ACL},
  year={2022}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{wang2022self,
  title={Self-instruct: Aligning language model with self generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}