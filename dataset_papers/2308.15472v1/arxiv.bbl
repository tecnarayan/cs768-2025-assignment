\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{Int. Conf. Mach. Learn.}, pages 214--223, 2017.

\bibitem[Bai et~al.(2023)Bai, Yang, Xu, Liu, Yang, and Shen]{bai2022glead}
Q.~Bai, C.~Yang, Y.~Xu, X.~Liu, Y.~Yang, and Y.~Shen.
\newblock {GLeaD}: Improving {GAN}s with a generator-leading task.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2023.

\bibitem[Brock et~al.(2019{\natexlab{a}})Brock, Donahue, and Simonyan]{biggan}
A.~Brock, J.~Donahue, and K.~Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2019{\natexlab{a}}.

\bibitem[Brock et~al.(2019{\natexlab{b}})Brock, Donahue, and
  Simonyan]{brock2018biggan}
A.~Brock, J.~Donahue, and K.~Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2019{\natexlab{b}}.

\bibitem[Brooks et~al.(2022)Brooks, Hellsten, Aittala, Wang, Aila, Lehtinen,
  Liu, Efros, and Karras]{brooks2022generating}
T.~Brooks, J.~Hellsten, M.~Aittala, T.-C. Wang, T.~Aila, J.~Lehtinen, M.-Y.
  Liu, A.~A. Efros, and T.~Karras.
\newblock Generating long videos of dynamic scenes.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2022.

\bibitem[Chan et~al.(2021)Chan, Monteiro, Kellnhofer, Wu, and
  Wetzstein]{chan2021pi}
E.~R. Chan, M.~Monteiro, P.~Kellnhofer, J.~Wu, and G.~Wetzstein.
\newblock {$\pi$-GAN}: Periodic implicit generative adversarial networks for
  3d-aware image synthesis.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2021.

\bibitem[Chan et~al.(2022)Chan, Lin, Chan, Nagano, Pan, De~Mello, Gallo,
  Guibas, Tremblay, Khamis, et~al.]{chan2022efficient}
E.~R. Chan, C.~Z. Lin, M.~A. Chan, K.~Nagano, B.~Pan, S.~De~Mello, O.~Gallo,
  L.~J. Guibas, J.~Tremblay, S.~Khamis, et~al.
\newblock Efficient geometry-aware 3d generative adversarial networks.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2022.

\bibitem[Clark et~al.(2019)Clark, Donahue, and Simonyan]{clark2019adversarial}
A.~Clark, J.~Donahue, and K.~Simonyan.
\newblock Adversarial video generation on complex datasets.
\newblock \emph{arXiv preprint arXiv:1907.06571}, 2019.

\bibitem[Dai et~al.(2017)Dai, Qi, Xiong, Li, Zhang, Hu, and
  Wei]{dai2017deformable}
J.~Dai, H.~Qi, Y.~Xiong, Y.~Li, G.~Zhang, H.~Hu, and Y.~Wei.
\newblock Deformable convolutional networks.
\newblock In \emph{Int. Conf. Comput. Vis.}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2009.

\bibitem[Fox et~al.(2021)Fox, Tewari, Elgharib, and
  Theobalt]{fox2021stylevideogan}
G.~Fox, A.~Tewari, M.~Elgharib, and C.~Theobalt.
\newblock {StyleVideoGAN}: A temporal generative model using a pretrained
  {StyleGAN}.
\newblock In \emph{Brit. Mach. Vis. Conf.}, 2021.

\bibitem[Ge et~al.(2022)Ge, Hayes, Yang, Yin, Pang, Jacobs, Huang, and
  Parikh]{ge2022long}
S.~Ge, T.~Hayes, H.~Yang, X.~Yin, G.~Pang, D.~Jacobs, J.-B. Huang, and
  D.~Parikh.
\newblock Long video generation with time-agnostic {VQGAN} and time-sensitive
  transformer.
\newblock In \emph{Eur. Conf. Comput. Vis.}, 2022.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{gan}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2014.

\bibitem[Gu et~al.(2022)Gu, Liu, Wang, and Theobalt]{gu2021stylenerf}
J.~Gu, L.~Liu, P.~Wang, and C.~Theobalt.
\newblock Stylenerf: A style-based 3d-aware generator for high-resolution image
  synthesis.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2022.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~C. Courville.
\newblock Improved training of wasserstein {GAN}s.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2017.

\bibitem[He et~al.(2021)He, Kan, and Shan]{he2021eigengan}
Z.~He, M.~Kan, and S.~Shan.
\newblock {EigenGAN}: Layer-wise eigen-learning for {GAN}s.
\newblock In \emph{Int. Conf. Comput. Vis.}, 2021.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Adv. Neural Inform. Process. Syst.}, 33:\penalty0 6840--6851,
  2020.

\bibitem[Huang and Belongie(2017)]{huang2017arbitrary}
X.~Huang and S.~Belongie.
\newblock Arbitrary style transfer in real-time with adaptive instance
  normalization.
\newblock In \emph{Int. Conf. Comput. Vis.}, 2017.

\bibitem[Jaderberg et~al.(2015)Jaderberg, Simonyan, Zisserman,
  et~al.]{jaderberg2015spatial}
M.~Jaderberg, K.~Simonyan, A.~Zisserman, et~al.
\newblock Spatial transformer networks.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2015.

\bibitem[Kang et~al.(2023)Kang, Zhu, Zhang, Park, Shechtman, Paris, and
  Park]{kang2023scaling}
M.~Kang, J.-Y. Zhu, R.~Zhang, J.~Park, E.~Shechtman, S.~Paris, and T.~Park.
\newblock Scaling up {GAN}s for text-to-image synthesis.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2023.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and
  Lehtinen]{karras2017progressive}
T.~Karras, T.~Aila, S.~Laine, and J.~Lehtinen.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2018.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019stylegan}
T.~Karras, S.~Laine, and T.~Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2019.

\bibitem[Karras et~al.(2020{\natexlab{a}})Karras, Laine, Aittala, Hellsten,
  Lehtinen, and Aila]{karras2020stylegan2}
T.~Karras, S.~Laine, M.~Aittala, J.~Hellsten, J.~Lehtinen, and T.~Aila.
\newblock Analyzing and improving the image quality of {StyleGAN}.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2020{\natexlab{a}}.

\bibitem[Karras et~al.(2020{\natexlab{b}})Karras, Laine, Aittala, Hellsten,
  Lehtinen, and Aila]{stylegan2}
T.~Karras, S.~Laine, M.~Aittala, J.~Hellsten, J.~Lehtinen, and T.~Aila.
\newblock Analyzing and improving the image quality of stylegan.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2020{\natexlab{b}}.

\bibitem[Karras et~al.(2021)Karras, Aittala, Laine, H{\"a}rk{\"o}nen, Hellsten,
  Lehtinen, and Aila]{karras2021alias}
T.~Karras, M.~Aittala, S.~Laine, E.~H{\"a}rk{\"o}nen, J.~Hellsten, J.~Lehtinen,
  and T.~Aila.
\newblock Alias-free generative adversarial networks.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2021.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kynk{\"a}{\"a}nniemi et~al.(2023)Kynk{\"a}{\"a}nniemi, Karras,
  Aittala, Aila, and Lehtinen]{kynkaanniemi2022role}
T.~Kynk{\"a}{\"a}nniemi, T.~Karras, M.~Aittala, T.~Aila, and J.~Lehtinen.
\newblock The role of {ImageNet} classes in fr$\backslash$'echet inception
  distance.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2023.

\bibitem[Lee et~al.(2022)Lee, Kim, Kim, Kim, Ha, and Choi]{lee2022ggdr}
G.~Lee, H.~Kim, J.~Kim, S.~Kim, J.-W. Ha, and Y.~Choi.
\newblock Generator knows what discriminator should learn in unconditional
  {GAN}s.
\newblock In \emph{Eur. Conf. Comput. Vis.}, 2022.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{mescheder2018training}
L.~Mescheder, A.~Geiger, and S.~Nowozin.
\newblock Which training methods for {GAN}s do actually converge?
\newblock In \emph{Int. Conf. Mach. Learn.}, 2018.

\bibitem[Nash et~al.(2021)Nash, Menick, Dieleman, and
  Battaglia]{nash2021generating}
C.~Nash, J.~Menick, S.~Dieleman, and P.~W. Battaglia.
\newblock Generating images with sparse representations.
\newblock In \emph{Int. Conf. Mach. Learn.}, 2021.

\bibitem[Nguyen-Phuoc et~al.(2019)Nguyen-Phuoc, Li, Theis, Richardt, and
  Yang]{nguyen2019hologan}
T.~Nguyen-Phuoc, C.~Li, L.~Theis, C.~Richardt, and Y.-L. Yang.
\newblock {HoloGAN}: Unsupervised learning of 3d representations from natural
  images.
\newblock In \emph{Int. Conf. Comput. Vis.}, 2019.

\bibitem[Peebles et~al.(2020)Peebles, Peebles, Zhu, Efros, and
  Torralba]{peebles2020hessian}
W.~Peebles, J.~Peebles, J.-Y. Zhu, A.~Efros, and A.~Torralba.
\newblock The hessian penalty: A weak prior for unsupervised disentanglement.
\newblock In \emph{Eur. Conf. Comput. Vis.}, 2020.

\bibitem[Radford et~al.(2016)Radford, Metz, and
  Chintala]{radford2015unsupervised}
A.~Radford, L.~Metz, and S.~Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2016.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{Int. Conf. Mach. Learn.}, 2021.

\bibitem[Saito et~al.(2017)Saito, Matsumoto, and Saito]{saito2017tgan}
M.~Saito, E.~Matsumoto, and S.~Saito.
\newblock Temporal generative adversarial nets with singular value clipping.
\newblock In \emph{Int. Conf. Comput. Vis.}, 2017.

\bibitem[Saito et~al.(2020)Saito, Saito, Koyama, and Kobayashi]{saito2020train}
M.~Saito, S.~Saito, M.~Koyama, and S.~Kobayashi.
\newblock Train sparsely, generate densely: Memory-efficient unsupervised
  training of high-resolution temporal gan.
\newblock \emph{Int. J. Comput. Vis.}, 2020.

\bibitem[Sauer et~al.(2022)Sauer, Schwarz, and Geiger]{sauer2022stylegan}
A.~Sauer, K.~Schwarz, and A.~Geiger.
\newblock {StyleGAN-XL}: Scaling {StyleGAN} to large diverse datasets.
\newblock In \emph{SIGGRAPH}, 2022.

\bibitem[Sauer et~al.(2023)Sauer, Karras, Laine, Geiger, and
  Aila]{sauer2023stylegan}
A.~Sauer, T.~Karras, S.~Laine, A.~Geiger, and T.~Aila.
\newblock {StyleGAN-T}: Unlocking the power of {GAN}s for fast large-scale
  text-to-image synthesis.
\newblock In \emph{Int. Conf. Mach. Learn.}, 2023.

\bibitem[Shi et~al.(2022)Shi, Shen, Zhu, Yeung, and Chen]{shi20223daware}
Z.~Shi, Y.~Shen, J.~Zhu, D.-Y. Yeung, and Q.~Chen.
\newblock 3d-aware indoor scene synthesis with depth priors.
\newblock In \emph{Eur. Conf. Comput. Vis.}, 2022.

\bibitem[Siarohin et~al.(2019)Siarohin, Lathuilière, Tulyakov, Ricci, and
  Sebe]{Siarohin_2019_NeurIPS}
A.~Siarohin, S.~Lathuilière, S.~Tulyakov, E.~Ricci, and N.~Sebe.
\newblock First order motion model for image animation.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2019.

\bibitem[Skorokhodov et~al.(2022)Skorokhodov, Tulyakov, and
  Elhoseiny]{skorokhodov2022stylegan}
I.~Skorokhodov, S.~Tulyakov, and M.~Elhoseiny.
\newblock {Stylegan-V}: A continuous video generator with the price, image
  quality and perks of {StyleGAN2}.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2022.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
J.~Sohl-Dickstein, E.~Weiss, N.~Maheswaranathan, and S.~Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{Int. Conf. Mach. Learn.}, pages 2256--2265. PMLR, 2015.

\bibitem[Tian et~al.(2021)Tian, Ren, Chai, Olszewski, Peng, Metaxas, and
  Tulyakov]{tian2021mocoganhd}
Y.~Tian, J.~Ren, M.~Chai, K.~Olszewski, X.~Peng, D.~N. Metaxas, and
  S.~Tulyakov.
\newblock A good image generator is what you need for high-resolution video
  synthesis.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2021.

\bibitem[Tulyakov et~al.(2018)Tulyakov, Liu, Yang, and
  Kautz]{tulyakov2018mocogan}
S.~Tulyakov, M.-Y. Liu, X.~Yang, and J.~Kautz.
\newblock {MoCoGAN}: Decomposing motion and content for video generation.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2018.

\bibitem[Unterthiner et~al.(2018)Unterthiner, van Steenkiste, Kurach, Marinier,
  Michalski, and Gelly]{unterthiner2018towards}
T.~Unterthiner, S.~van Steenkiste, K.~Kurach, R.~Marinier, M.~Michalski, and
  S.~Gelly.
\newblock Towards accurate generative models of video: A new metric \&
  challenges.
\newblock \emph{arXiv preprint arXiv:1812.01717}, 2018.

\bibitem[Van~den Oord et~al.(2016)Van~den Oord, Kalchbrenner, Espeholt,
  Vinyals, Graves, et~al.]{van2016conditional}
A.~Van~den Oord, N.~Kalchbrenner, L.~Espeholt, O.~Vinyals, A.~Graves, et~al.
\newblock Conditional image generation with pixelcnn decoders.
\newblock \emph{Adv. Neural Inform. Process. Syst.}, 29, 2016.

\bibitem[Van Den~Oord et~al.(2016)Van Den~Oord, Kalchbrenner, and
  Kavukcuoglu]{van2016pixel}
A.~Van Den~Oord, N.~Kalchbrenner, and K.~Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In \emph{Int. Conf. Mach. Learn.}, pages 1747--1756. PMLR, 2016.

\bibitem[Vondrick et~al.(2016)Vondrick, Pirsiavash, and
  Torralba]{vondrick2016generating}
C.~Vondrick, H.~Pirsiavash, and A.~Torralba.
\newblock Generating videos with scene dynamics.
\newblock \emph{Adv. Neural Inform. Process. Syst.}, 2016.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Yang, Xu, Shen, Li, and
  Zhou]{wang2022improving}
J.~Wang, C.~Yang, Y.~Xu, Y.~Shen, H.~Li, and B.~Zhou.
\newblock Improving {GAN} equilibrium by raising spatial awareness.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2023)Wang, Dai, Chen, Huang, Li, Zhu, Hu, Lu, Lu, Li,
  et~al.]{wang2022internimage}
W.~Wang, J.~Dai, Z.~Chen, Z.~Huang, Z.~Li, X.~Zhu, X.~Hu, T.~Lu, L.~Lu, H.~Li,
  et~al.
\newblock {InternImage}: Exploring large-scale vision foundation models with
  deformable convolutions.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2023.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Yang, Bremond, and
  Dantcheva]{wang2022latent}
Y.~Wang, D.~Yang, F.~Bremond, and A.~Dantcheva.
\newblock Latent image animator: Learning to animate images via latent space
  navigation.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2022{\natexlab{b}}.

\bibitem[Xiong et~al.(2018)Xiong, Luo, Ma, Liu, and Luo]{xiong2018learning}
W.~Xiong, W.~Luo, L.~Ma, W.~Liu, and J.~Luo.
\newblock Learning to generate time-lapse videos using multi-stage dynamic
  generative adversarial networks.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2018.

\bibitem[Xu et~al.(2022)Xu, Peng, Yang, Shen, and Zhou]{xu2022volumeGAN}
Y.~Xu, S.~Peng, C.~Yang, Y.~Shen, and B.~Zhou.
\newblock 3d-aware image synthesis via learning structural and textural
  representations.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2022.

\bibitem[Xu et~al.(2023)Xu, Chai, Shi, Peng, Skorokhodov, Siarohin, Yang, Shen,
  Lee, Zhou, et~al.]{xu2022discoscene}
Y.~Xu, M.~Chai, Z.~Shi, S.~Peng, I.~Skorokhodov, A.~Siarohin, C.~Yang, Y.~Shen,
  H.-Y. Lee, B.~Zhou, et~al.
\newblock {DisCoScene}: Spatially disentangled generative radiance fields for
  controllable 3d-aware scene synthesis.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2023.

\bibitem[Yang et~al.(2021)Yang, Shen, and Zhou]{yang2021semantic}
C.~Yang, Y.~Shen, and B.~Zhou.
\newblock Semantic hierarchy emerges in deep generative representations for
  scene synthesis.
\newblock \emph{Int. J. Comput. Vis.}, 2021.

\bibitem[Yang et~al.(2022)Yang, Shen, Xu, Zhao, Dai, and
  Zhou]{yang2022improving}
C.~Yang, Y.~Shen, Y.~Xu, D.~Zhao, B.~Dai, and B.~Zhou.
\newblock Improving {GAN}s with a dynamic discriminator.
\newblock In \emph{Adv. Neural Inform. Process. Syst.}, 2022.

\bibitem[Yu et~al.(2015)Yu, Seff, Zhang, Song, Funkhouser, and
  Xiao]{yu2015lsun}
F.~Yu, A.~Seff, Y.~Zhang, S.~Song, T.~Funkhouser, and J.~Xiao.
\newblock {LSUN}: Construction of a large-scale image dataset using deep
  learning with humans in the loop.
\newblock \emph{arXiv preprint arXiv:1506.03365}, 2015.

\bibitem[Yu et~al.(2022)Yu, Tack, Mo, Kim, Kim, Ha, and Shin]{yu2022dign}
S.~Yu, J.~Tack, S.~Mo, H.~Kim, J.~Kim, J.-W. Ha, and J.~Shin.
\newblock Generating videos with dynamics-aware implicit generative adversarial
  networks.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Peng, and Zhou]{zhang2022learning}
Q.~Zhang, Z.~Peng, and B.~Zhou.
\newblock Learning to drive by watching youtube videos: Action-conditioned
  contrastive policy pretraining.
\newblock In \emph{Eur. Conf. Comput. Vis.}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Yang, Shen, Xu, and Zhou]{zhang2022towards}
Q.~Zhang, C.~Yang, Y.~Shen, Y.~Xu, and B.~Zhou.
\newblock Towards smooth video composition.
\newblock In \emph{Int. Conf. Learn. Represent.}, 2023.

\bibitem[Zhao et~al.(2022)Zhao, Ma, G{\"u}era, Ren, Schwing, and
  Colburn]{zhao2022generative}
X.~Zhao, F.~Ma, D.~G{\"u}era, Z.~Ren, A.~G. Schwing, and A.~Colburn.
\newblock Generative multiplane images: Making a 2d {GAN} 3d-aware.
\newblock In \emph{Eur. Conf. Comput. Vis.}, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Yang, Shen, Shi, Zhao, and Chen]{zhu2023linkgan}
J.~Zhu, C.~Yang, Y.~Shen, Z.~Shi, D.~Zhao, and Q.~Chen.
\newblock {LinkGAN}: Linking {GAN} latents to pixels for controllable image
  synthesis.
\newblock \emph{arXiv preprint arXiv:2301.04604}, 2023.

\bibitem[Zhu et~al.(2019)Zhu, Hu, Lin, and Dai]{zhu2019deformable}
X.~Zhu, H.~Hu, S.~Lin, and J.~Dai.
\newblock Deformable convnets v2: More deformable, better results.
\newblock In \emph{IEEE Conf. Comput. Vis. Pattern Recog.}, 2019.

\end{thebibliography}
