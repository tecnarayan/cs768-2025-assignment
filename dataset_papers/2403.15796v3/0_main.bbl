\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainslie et~al.(2023)Ainslie, Lee{-}Thorp, de~Jong, Zemlyanskiy, Lebr{\'{o}}n, and Sanghai]{GQA:AinslieLJZLS23}
Joshua Ainslie, James Lee{-}Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'{o}}n, and Sumit Sanghai.
\newblock {GQA:} training generalized multi-query transformer models from multi-head checkpoints.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pages 4895--4901. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.298}.

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{open-llm-leaderboard}
Edward Beeching, Cl√©mentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf.
\newblock Open llm leaderboard.
\newblock \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}, 2023.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal]{Pythia:BidermanSABOHKP23}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der Wal.
\newblock Pythia: {A} suite for analyzing large language models across training and scaling.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 2397--2430. {PMLR}, 2023.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{PIQA:BiskZLGC20}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pages 7432--7439. {AAAI} Press, 2020.
\newblock \doi{10.1609/AAAI.V34I05.6239}.
\newblock URL \url{https://doi.org/10.1609/aaai.v34i05.6239}.

\bibitem[Brier(1950)]{BrierScore}
Glenn~W. Brier.
\newblock Verification of forecasts expressed in terms of probability.
\newblock \emph{Monthly Weather Review}, 78\penalty0 (1):\penalty0 1 -- 3, 1950.
\newblock \doi{10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2}.
\newblock URL \url{https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT-3:BrownMRSKDNSSAA20}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Cao and Gu(2020)]{CaoG20}
Yuan Cao and Quanquan Gu.
\newblock Generalization error bounds of gradient descent for learning over-parameterized deep relu networks.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pages 3349--3356. {AAAI} Press, 2020.
\newblock \doi{10.1609/AAAI.V34I04.5736}.
\newblock URL \url{https://doi.org/10.1609/aaai.v34i04.5736}.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur{-}Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier{-}Hellstern, Eck, Dean, Petrov, and Fiedel]{PaLM:ChowdheryNDBMRBCSGSSTMRBTSPRDHPBAI23}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur{-}Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier{-}Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{J. Mach. Learn. Res.}, 24:\penalty0 240:1--240:113, 2023.
\newblock URL \url{http://jmlr.org/papers/v24/22-1144.html}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei]{ScalingFlan:abs-2210-11416}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent~Y. Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{CoRR}, abs/2210.11416, 2022.
\newblock \doi{10.48550/ARXIV.2210.11416}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2210.11416}.

\bibitem[Clark et~al.(2022)Clark, de~Las~Casas, Guy, Mensch, Paganini, Hoffmann, Damoc, Hechtman, Cai, Borgeaud, van~den Driessche, Rutherford, Hennigan, Johnson, Cassirer, Jones, Buchatskaya, Budden, Sifre, Osindero, Vinyals, Ranzato, Rae, Elsen, Kavukcuoglu, and Simonyan]{ScalingRouted:ClarkCGMPHDHCB022}
Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake~A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van~den Driessche, Eliza Rutherford, Tom Hennigan, Matthew~J. Johnson, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Marc'Aurelio Ranzato, Jack~W. Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan.
\newblock Unified scaling laws for routed language models.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato, editors, \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 4057--4086. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/clark22a.html}.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{ARC:abs-1803-05457}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the {AI2} reasoning challenge.
\newblock \emph{CoRR}, abs/1803.05457, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.05457}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{GSM8K:abs-2110-14168}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock \emph{CoRR}, abs/2110.14168, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Computer(2023)]{together2023redpajama}
Together Computer.
\newblock Redpajama: An open source recipe to reproduce llama training dataset, April 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Dar et~al.(2021)Dar, Muthukumar, and Baraniuk]{TOPML:abs-2109-02355}
Yehuda Dar, Vidya Muthukumar, and Richard~G. Baraniuk.
\newblock A farewell to the bias-variance tradeoff? an overview of the theory of overparameterized machine learning.
\newblock \emph{CoRR}, abs/2109.02355, 2021.
\newblock URL \url{https://arxiv.org/abs/2109.02355}.

\bibitem[Duan(2016)]{NLPCC-KBQA}
Nan Duan.
\newblock Overview of the nlpcc-iccpol 2016 shared task: Open domain chinese question answering.
\newblock In \emph{Natural Language Understanding and Intelligent Applications}, pages 942--948. Springer International Publishing, 2016.
\newblock ISBN 978-3-319-50496-4.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{SwitchTransformer:FedusZS22}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{J. Mach. Learn. Res.}, 23:\penalty0 120:1--120:39, 2022.
\newblock URL \url{http://jmlr.org/papers/v23/21-0998.html}.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and R{\'{e}}]{H3:FuDSTRR23}
Daniel~Y. Fu, Tri Dao, Khaled~Kamal Saab, Armin~W. Thomas, Atri Rudra, and Christopher R{\'{e}}.
\newblock Hungry hungry hippos: Towards language modeling with state space models.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=COZDy0WYGg}.

\bibitem[Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao, Mercat, Fang, Li, Keh, Xin, Nezhurina, Vasiljevic, Jitsev, Dimakis, Ilharco, Song, Kollar, Carmon, Dave, Heckel, Muennighoff, and Schmidt]{Overtrain:abs-2403-08540}
Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros~G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt.
\newblock Language models scale reliably with over-training and on downstream tasks.
\newblock \emph{CoRR}, abs/2403.08540, 2024.

\bibitem[Ganguli et~al.(2022)Ganguli, Hernandez, Lovitt, Askell, Bai, Chen, Conerly, DasSarma, Drain, Elhage, Showk, Fort, Hatfield{-}Dodds, Henighan, Johnston, Jones, Joseph, Kernian, Kravec, Mann, Nanda, Ndousse, Olsson, Amodei, Brown, Kaplan, McCandlish, Olah, Amodei, and Clark]{Surprise:GanguliHLABCCDD22}
Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer~El Showk, Stanislav Fort, Zac Hatfield{-}Dodds, Tom Henighan, Scott Johnston, Andy Jones, Nicholas Joseph, Jackson Kernian, Shauna Kravec, Ben Mann, Neel Nanda, Kamal Ndousse, Catherine Olsson, Daniela Amodei, Tom~B. Brown, Jared Kaplan, Sam McCandlish, Christopher Olah, Dario Amodei, and Jack Clark.
\newblock Predictability and surprise in large generative models.
\newblock In \emph{FAccT '22: 2022 {ACM} Conference on Fairness, Accountability, and Transparency, Seoul, Republic of Korea, June 21 - 24, 2022}, pages 1747--1764. {ACM}, 2022.
\newblock \doi{10.1145/3531146.3533229}.
\newblock URL \url{https://doi.org/10.1145/3531146.3533229}.

\bibitem[Gao et~al.(2021)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{Pile:abs-2101-00027}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{CoRR}, abs/2101.00027, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.00027}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{MMLU:HendrycksBBZMSS21}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson, Jun, Brown, Dhariwal, Gray, Hallacy, Mann, Radford, Ramesh, Ryder, Ziegler, Schulman, Amodei, and McCandlish]{ScalingAuto:abs-2010-14701}
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom~B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel~M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{CoRR}, abs/2010.14701, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.14701}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre]{Chinchilla:abs-2203-15556}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models.
\newblock \emph{CoRR}, abs/2203.15556, 2022.
\newblock \doi{10.48550/ARXIV.2203.15556}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2203.15556}.

\bibitem[Hu et~al.(2023)Hu, Liu, Han, Zhang, He, Zhao, Lin, Ding, Ou, Zeng, et~al.]{hu2023predicting}
Shengding Hu, Xin Liu, Xu~Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, et~al.
\newblock Predicting emergent abilities with infinite resolution evaluation.
\newblock \emph{arXiv e-prints}, pages arXiv--2310, 2023.

\bibitem[Huang et~al.(2023)Huang, Bai, Zhu, Zhang, Zhang, Su, Liu, Lv, Zhang, Lei, Fu, Sun, and He]{CEval:abs-2305-08322}
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He.
\newblock C-eval: {A} multi-level multi-discipline chinese evaluation suite for foundation models.
\newblock \emph{CoRR}, abs/2305.08322, 2023.
\newblock \doi{10.48550/ARXIV.2305.08322}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.08322}.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{Mistral:abs-2310-06825}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~Las~Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L{\'{e}}lio~Renard Lavaud, Marie{-}Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timoth{\'{e}}e Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{CoRR}, abs/2310.06825, 2023.
\newblock \doi{10.48550/ARXIV.2310.06825}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.06825}.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{TriviaQA:JoshiCWZ17}
Mandar Joshi, Eunsol Choi, Daniel~S. Weld, and Luke Zettlemoyer.
\newblock Triviaqa: {A} large scale distantly supervised challenge dataset for reading comprehension.
\newblock In Regina Barzilay and Min{-}Yen Kan, editors, \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers}, pages 1601--1611. Association for Computational Linguistics, 2017.
\newblock \doi{10.18653/V1/P17-1147}.
\newblock URL \url{https://doi.org/10.18653/v1/P17-1147}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{ScalingNLM:abs-2001-08361}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{CoRR}, abs/2001.08361, 2020.
\newblock URL \url{https://arxiv.org/abs/2001.08361}.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{GPT-2:KojimaGRMI22}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.

\bibitem[Kudo and Richardson(2018)]{SentencePiece:KudoR18}
Taku Kudo and John Richardson.
\newblock Sentencepiece: {A} simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock In Eduardo Blanco and Wei Lu, editors, \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018}, pages 66--71. Association for Computational Linguistics, 2018.
\newblock \doi{10.18653/V1/D18-2012}.
\newblock URL \url{https://doi.org/10.18653/v1/d18-2012}.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{RACE:LaiXLYH17}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard~H. Hovy.
\newblock {RACE:} large-scale reading comprehension dataset from examinations.
\newblock In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September 9-11, 2017}, pages 785--794. Association for Computational Linguistics, 2017.
\newblock URL \url{https://doi.org/10.18653/v1/d17-1082}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Hall, Liang, and Ma]{Sophia:abs-2305-14342}
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma.
\newblock Sophia: {A} scalable stochastic second-order optimizer for language model pre-training.
\newblock \emph{CoRR}, abs/2305.14342, 2023{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2305.14342}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.14342}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Xie, Li, and Ma]{ImplicitBias:LiuXL023}
Hong Liu, Sang~Michael Xie, Zhiyuan Li, and Tengyu Ma.
\newblock Same pre-training loss, better downstream: Implicit bias matters for language models.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 22188--22214. {PMLR}, 2023{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v202/liu23ao.html}.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, and Roberts]{FlanCollection:LongpreHVWCTZLZ23}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V. Le, Barret Zoph, Jason Wei, and Adam Roberts.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 22631--22648. {PMLR}, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/longpre23a.html}.

\bibitem[Loshchilov and Hutter(2019)]{AdamW:LoshchilovH19}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[OpenAI(2023)]{GPT-4:abs-2303-08774}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.
\newblock \doi{10.48550/ARXIV.2303.08774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.08774}.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and Auli]{FairSeq:OttEBFGNGA19}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.
\newblock fairseq: {A} fast, extensible toolkit for sequence modeling.
\newblock In Waleed Ammar, Annie Louis, and Nasrin Mostafazadeh, editors, \emph{Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Demonstrations}, pages 48--53. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/N19-4009}.
\newblock URL \url{https://doi.org/10.18653/v1/n19-4009}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi, Pezzelle, Baroni, Boleda, and Fern{\'{a}}ndez]{LAMBADA:PapernoKLPBPBBF16}
Denis Paperno, Germ{\'{a}}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern{\'{a}}ndez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse context.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers}. The Association for Computer Linguistics, 2016.

\bibitem[Poli et~al.(2023)Poli, Massaroli, Nguyen, Fu, Dao, Baccus, Bengio, Ermon, and R{\'{e}}]{Heya:PoliMNFDBBER23}
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel~Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R{\'{e}}.
\newblock Hyena hierarchy: Towards larger convolutional language models.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 28043--28078. {PMLR}, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/poli23a.html}.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and Misra]{Grokking:abs-2201-02177}
Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra.
\newblock Grokking: Generalization beyond overfitting on small algorithmic datasets.
\newblock \emph{CoRR}, abs/2201.02177, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.02177}.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving]{Gopher:abs-2112-11446}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H.~Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po{-}Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean{-}Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy,
  Chris Jones, James Bradbury, Matthew~J. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis {\&} insights from training gopher.
\newblock \emph{CoRR}, abs/2112.11446, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{T5:RaffelSRLNMZLL20}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Roh et~al.(2020)Roh, Oh, and Lee]{NormalizedPerplexity:abs-2011-13220}
Jihyeon Roh, Sang{-}Hoon Oh, and Soo{-}Young Lee.
\newblock Unigram-normalized perplexity as a language model performance measure with different vocabulary sizes.
\newblock \emph{CoRR}, abs/2011.13220, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.13220}.

\bibitem[Sakaguchi et~al.(2020)Sakaguchi, Bras, Bhagavatula, and Choi]{WinoGrande:SakaguchiBBC20}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA, February 7-12, 2020}, pages 8732--8740. {AAAI} Press, 2020.
\newblock \doi{10.1609/AAAI.V34I05.6399}.
\newblock URL \url{https://doi.org/10.1609/aaai.v34i05.6399}.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey, Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'{e}}vry, Fries, Teehan, Scao, Biderman, Gao, Wolf, and Rush]{T0:SanhWRBSACSRDBX22}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta, Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Schaeffer et~al.(2023)Schaeffer, Miranda, and Koyejo]{Mirage:abs-2304-15004}
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo.
\newblock Are emergent abilities of large language models a mirage?
\newblock \emph{CoRR}, abs/2304.15004, 2023.
\newblock \doi{10.48550/ARXIV.2304.15004}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2304.15004}.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{BPE:SennrichHB16a}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers}. The Association for Computer Linguistics, 2016.
\newblock \doi{10.18653/V1/P16-1162}.
\newblock URL \url{https://doi.org/10.18653/v1/p16-1162}.

\bibitem[Shazeer and Stern(2018)]{Adafactor:ShazeerS18}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In Jennifer~G. Dy and Andreas Krause, editors, \emph{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages 4603--4611. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/shazeer18a.html}.

\bibitem[Shin et~al.(2022)Shin, Lee, Ahn, Kim, Kim, Kim, Cho, Lee, Park, Ha, and Sung]{ShinLAKKKCLPHS22}
Seongjin Shin, Sang{-}Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woo{-}Myoung Park, Jung{-}Woo Ha, and Nako Sung.
\newblock On the effect of pretraining corpora on in-context learning by a large-scale language model.
\newblock In Marine Carpuat, Marie{-}Catherine de~Marneffe, and Iv{\'{a}}n Vladimir~Meza Ru{\'{\i}}z, editors, \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022}, pages 5168--5186. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/V1/2022.NAACL-MAIN.380}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.naacl-main.380}.

\bibitem[Spearman(1904)]{spearman1904proof}
C.~Spearman.
\newblock The proof and measurement of association between two things.
\newblock \emph{The American Journal of Psychology}, 15\penalty0 (1):\penalty0 72--101, 1904.
\newblock ISSN 00029556.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga{-}Alonso, Kluska, Lewkowycz, Agarwal, Power, Ray, Warstadt, Kocurek, Safaya, Tazarv, Xiang, Parrish, Nie, Hussain, Askell, Dsouza, Rahane, Iyer, Andreassen, Santilli, Stuhlm{\"{u}}ller, Dai, La, Lampinen, Zou, Jiang, Chen, Vuong, Gupta, Gottardi, Norelli, Venkatesh, Gholamidavoodi, Tabassum, Menezes, Kirubarajan, Mullokandov, Sabharwal, Herrick, Efrat, Erdem, Karakas, and et~al.]{BIGBench:abs-2206-04615}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R. Brown, Adam Santoro, Aditya Gupta, Adri{\`{a}} Garriga{-}Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander~W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman~S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlm{\"{u}}ller, Andrew~M. Dai, Andrew La, Andrew~K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{CoRR}, abs/2206.04615, 2022.
\newblock \doi{10.48550/ARXIV.2206.04615}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2206.04615}.

\bibitem[Sun et~al.(2020)Sun, Yu, Yu, and Cardie]{C3:SunYYC20}
Kai Sun, Dian Yu, Dong Yu, and Claire Cardie.
\newblock Investigating prior knowledge for challenging chinese machine reading comprehension.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 8:\penalty0 141--155, 2020.
\newblock \doi{10.1162/TACL\_A\_00305}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00305}.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Rao, Fedus, Abnar, Chung, Narang, Yogatama, Vaswani, and Metzler]{ScaleEfficiently}
Yi~Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung~Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler.
\newblock Scale efficiently: Insights from pretraining and finetuning transformers.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.

\bibitem[Tay et~al.(2023)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran, Yogatama, and Metzler]{InductiveScaling:Tay0ACFRN0YM23}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung~Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh~Q. Tran, Dani Yogatama, and Donald Metzler.
\newblock Scaling laws vs model architectures: How does inductive bias influence scaling?
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Findings of the Association for Computational Linguistics: {EMNLP} 2023, Singapore, December 6-10, 2023}, pages 12342--12364. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.825}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{LLaMA:abs-2302-13971}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie{-}Anne Lachaux, Timoth{\'{e}}e Lacroix, Baptiste Rozi{\`{e}}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur{\'{e}}lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{CoRR}, abs/2302.13971, 2023{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2302.13971}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.13971}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{LLaMA2:abs-2307-09288}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton{-}Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie{-}Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur{\'{e}}lien Rodriguez, Robert Stojnic, Sergey Edunov,
  and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}, abs/2307.09288, 2023{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2307.09288}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.09288}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{FLAN:WeiBZGYLDDL22}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, Chi, Hashimoto, Vinyals, Liang, Dean, and Fedus]{Emergent:WeiTBRZBYBZMCHVLDF22}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
\newblock Emergent abilities of large language models.
\newblock \emph{Trans. Mach. Learn. Res.}, 2022, 2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=yzkSU5zdwD}.

\bibitem[Wei et~al.(2022{\natexlab{c}})Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{CoT:Wei0SBIXCLZ22}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~H. Chi, Quoc~V. Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022{\natexlab{c}}.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html}.

\bibitem[Welbl et~al.(2017)Welbl, Liu, and Gardner]{SciQ:WelblLG17}
Johannes Welbl, Nelson~F. Liu, and Matt Gardner.
\newblock Crowdsourcing multiple choice science questions.
\newblock In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, \emph{Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017}, pages 94--106. Association for Computational Linguistics, 2017.

\bibitem[Xia et~al.(2023)Xia, Artetxe, Zhou, Lin, Pasunuru, Chen, Zettlemoyer, and Stoyanov]{OptTraject:XiaAZLPCZS23}
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi~Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Training trajectories of language models across scales.
\newblock In Anna Rogers, Jordan~L. Boyd{-}Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pages 13711--13738. Association for Computational Linguistics, 2023.
\newblock \doi{10.18653/V1/2023.ACL-LONG.767}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.acl-long.767}.

\bibitem[Xu et~al.(2020)Xu, Hu, Zhang, Li, Cao, Li, Xu, Sun, Yu, Yu, Tian, Dong, Liu, Shi, Cui, Li, Zeng, Wang, Xie, Li, Patterson, Tian, Zhang, Zhou, Liu, Zhao, Zhao, Yue, Zhang, Yang, Richardson, and Lan]{CLUE:XuHZLCLXSYYTDLS20}
Liang Xu, Hai Hu, Xuanwei Zhang, Lu~Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo~Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He~Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and Zhenzhong Lan.
\newblock {CLUE:} {A} chinese language understanding evaluation benchmark.
\newblock In Donia Scott, N{\'{u}}ria Bel, and Chengqing Zong, editors, \emph{Proceedings of the 28th International Conference on Computational Linguistics, {COLING} 2020, Barcelona, Spain (Online), December 8-13, 2020}, pages 4762--4772. International Committee on Computational Linguistics, 2020.
\newblock URL \url{https://doi.org/10.18653/v1/2020.coling-main.419}.

\bibitem[Yao et~al.(2021)Yao, Dong, Guan, Cao, Zhang, Xiao, Wang, Qi, Bao, Nie, Zeng, Gu, Zhou, Huang, Li, Ren, Lu, Xu, Wang, Zeng, Zhou, Zhang, Li, Huang, Yan, He, Wan, Zhao, Sun, Liu, Liu, Han, Yang, Sui, and Sun]{CUGE:abs-2112-13610}
Yuan Yao, Qingxiu Dong, Jian Guan, Boxi Cao, Zhengyan Zhang, Chaojun Xiao, Xiaozhi Wang, Fanchao Qi, Junwei Bao, Jinran Nie, Zheni Zeng, Yuxian Gu, Kun Zhou, Xuancheng Huang, Wenhao Li, Shuhuai Ren, Jinliang Lu, Chengqiang Xu, Huadong Wang, Guoyang Zeng, Zile Zhou, Jiajun Zhang, Juanzi Li, Minlie Huang, Rui Yan, Xiaodong He, Xiaojun Wan, Xin Zhao, Xu~Sun, Yang Liu, Zhiyuan Liu, Xianpei Han, Erhong Yang, Zhifang Sui, and Maosong Sun.
\newblock {CUGE:} {A} chinese language understanding and generation evaluation benchmark.
\newblock \emph{CoRR}, abs/2112.13610, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.13610}.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{Hellaswag:ZellersHBFC19}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In Anna Korhonen, David~R. Traum, and Llu{\'{\i}}s M{\`{a}}rquez, editors, \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pages 4791--4800. Association for Computational Linguistics, 2019.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1472}.

\bibitem[Zeng et~al.(2023)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia, Tam, Ma, Xue, Zhai, Chen, Liu, Zhang, Dong, and Tang]{GLM-130B:ZengLDWL0YXZXTM23}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang.
\newblock {GLM-130B:} an open bilingual pre-trained model.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=-Aw0rrrPUF}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{OPT:abs-2205-01068}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona~T. Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock {OPT:} open pre-trained transformer language models.
\newblock \emph{CoRR}, abs/2205.01068, 2022.
\newblock \doi{10.48550/ARXIV.2205.01068}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.01068}.

\end{thebibliography}
