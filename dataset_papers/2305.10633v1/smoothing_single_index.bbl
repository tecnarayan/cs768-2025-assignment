\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ben~Arous et~al.(2021)Ben~Arous, Gheissari, and
  Jagannath]{arous2021online}
Gerard Ben~Arous, Reza Gheissari, and Aukosh Jagannath.
\newblock Online stochastic gradient descent on non-convex losses from
  high-dimensional inference.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 4788--4838, 2021.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Ma(2020)]{ma2020local}
Tengyu Ma.
\newblock Why do local methods solve nonconvex problems?, 2020.

\bibitem[Abbe et~al.(2023)Abbe, Boix-Adserà, and Misiakiewicz]{abbe2023leap}
Emmanuel Abbe, Enric Boix-Adserà, and Theodor Misiakiewicz.
\newblock Sgd learning on neural networks: leap complexity and saddle-to-saddle
  dynamics.
\newblock \emph{arXiv}, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.11055}.

\bibitem[Bietti et~al.(2022)Bietti, Bruna, Sanford, and
  Song]{bruna2022singleindex}
Alberto Bietti, Joan Bruna, Clayton Sanford, and Min~Jae Song.
\newblock Learning single-index models with shallow neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Damian et~al.(2022)Damian, Lee, and Soltanolkotabi]{damian2022neural}
Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi.
\newblock Neural networks can learn representations with gradient descent.
\newblock In \emph{Conference on Learning Theory}, pages 5413--5452. PMLR,
  2022.

\bibitem[Mei et~al.(2018)Mei, Bai, and Montanari]{mei2018}
Song Mei, Yu~Bai, and Andrea Montanari.
\newblock The landscape of empirical risk for nonconvex losses.
\newblock \emph{The Annals of Statistics}, 46:\penalty0 2747--2774, 2018.

\bibitem[Richard and Montanari(2014)]{richard2014tensor}
Emile Richard and Andrea Montanari.
\newblock A statistical model for tensor pca.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2897 -- 2905, 2014.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and Valiant]{BlancGVV20}
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock In \emph{Conference on Learning Theory}, pages 483--513, 2020.

\bibitem[Damian et~al.(2021)Damian, Ma, and Lee]{damian2021label}
Alex Damian, Tengyu Ma, and Jason~D. Lee.
\newblock Label noise {SGD} provably prefers flat global minimizers.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Li et~al.(2022)Li, Wang, and Arora]{li2022what}
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora.
\newblock What happens after {SGD} reaches zero loss? --a mathematical
  framework.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
Christopher~J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Wen et~al.(2019)Wen, Luk, Gazeau, Zhang, Chan, and
  Ba]{wen2019interplay}
Yeming Wen, Kevin Luk, Maxime Gazeau, Guodong Zhang, Harris Chan, and Jimmy Ba.
\newblock Interplay between optimization and generalization of stochastic
  gradient descent with covariance noise.
\newblock \emph{arXiv preprint arXiv:1902.08234}, 2019.

\bibitem[Kakade et~al.(2011)Kakade, Kanade, Shamir, and
  Kalai]{kakade2011efficient}
Sham~M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai.
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Soltanolkotabi(2017)]{soltanolkotabi2017}
Mahdi Soltanolkotabi.
\newblock Learning relus via gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Candès et~al.(2015)Candès, Li, and Soltanolkotabi]{candes2015phase}
Emmanuel~J. Candès, Xiaodong Li, and Mahdi Soltanolkotabi.
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (4):\penalty0 1985--2007, 2015.
\newblock \doi{10.1109/TIT.2015.2399924}.

\bibitem[Chen et~al.(2019)Chen, Chi, Fan, and Ma]{chen2019phase}
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma.
\newblock Gradient descent with random initialization: fast global convergence
  for nonconvex phase retrieval.
\newblock \emph{Mathematical Programming}, 176\penalty0 (1):\penalty0 5--37,
  2019.

\bibitem[Sun et~al.(2018)Sun, Qu, and Wright]{sun2018phase}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock \emph{Foundations of Computational Mathematics}, 18\penalty0
  (5):\penalty0 1131001198, 2018.

\bibitem[Dudeja and Hsu(2018)]{dudeja2018sim}
Rishabh Dudeja and Daniel Hsu.
\newblock Learning single-index models in gaussian space.
\newblock In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet,
  editors, \emph{Proceedings of the 31st Conference On Learning Theory},
  volume~75 of \emph{Proceedings of Machine Learning Research}, pages
  1887--1930. PMLR, 06--09 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v75/dudeja18a.html}.

\bibitem[Chen and Meka(2020)]{chen2020poly}
Sitan Chen and Raghu Meka.
\newblock Learning polynomials in few relevant dimensions.
\newblock In Jacob Abernethy and Shivani Agarwal, editors, \emph{Proceedings of
  Thirty Third Conference on Learning Theory}, volume 125 of \emph{Proceedings
  of Machine Learning Research}, pages 1161--1227. PMLR, 09--12 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v125/chen20a.html}.

\bibitem[Ba et~al.(2022)Ba, Erdogdu, Suzuki, Wang, Wu, and Yang]{ba2022onestep}
Jimmy Ba, Murat~A. Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg
  Yang.
\newblock High-dimensional asymptotics of feature learning: How one gradient
  step improves the representation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Abbe et~al.(2022)Abbe, Adsera, and Misiakiewicz]{abbe2022merged}
Emmanuel Abbe, Enric~Boix Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient
  condition for sgd learning of sparse functions on two-layer neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 4782--4887. PMLR,
  2022.

\bibitem[Hopkins et~al.(2016)Hopkins, Schramm, Shi, and
  Steurer]{hopkins2016partialtrace}
Samuel~B. Hopkins, Tselil Schramm, Jonathan Shi, and David Steurer.
\newblock Fast spectral algorithms from sum-of-squares proofs: Tensor
  decomposition and planted sparse vectors.
\newblock In \emph{Proceedings of the Forty-Eighth Annual ACM Symposium on
  Theory of Computing}, STOC '16, page 178–191, New York, NY, USA, 2016.
  Association for Computing Machinery.
\newblock ISBN 9781450341325.
\newblock \doi{10.1145/2897518.2897529}.
\newblock URL \url{https://doi.org/10.1145/2897518.2897529}.

\bibitem[Anandkumar et~al.(2017)Anandkumar, Deng, Ge, and
  Mobahi]{pmlr-v65-anandkumar17a}
Anima Anandkumar, Yuan Deng, Rong Ge, and Hossein Mobahi.
\newblock Homotopy analysis for tensor pca.
\newblock In Satyen Kale and Ohad Shamir, editors, \emph{Proceedings of the
  2017 Conference on Learning Theory}, volume~65 of \emph{Proceedings of
  Machine Learning Research}, pages 79--104. PMLR, 07--10 Jul 2017.
\newblock URL \url{https://proceedings.mlr.press/v65/anandkumar17a.html}.

\bibitem[Biroli et~al.(2020)Biroli, Cammarota, and
  Ricci-Tersenghi]{Biroli_2020}
Giulio Biroli, Chiara Cammarota, and Federico Ricci-Tersenghi.
\newblock How to iron out rough landscapes and get optimal performances:
  averaged gradient descent and its application to tensor pca.
\newblock \emph{Journal of Physics A: Mathematical and Theoretical},
  53\penalty0 (17):\penalty0 174003, apr 2020.
\newblock \doi{10.1088/1751-8121/ab7b1f}.
\newblock URL \url{https://dx.doi.org/10.1088/1751-8121/ab7b1f}.

\bibitem[Goel et~al.(2020)Goel, Gollakota, Jin, Karmalkar, and
  Klivans]{goel2020superpolynomial}
Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam
  Klivans.
\newblock Superpolynomial lower bounds for learning one-layer neural networks
  using gradient descent.
\newblock \emph{arXiv preprint arXiv:2006.12011}, 2020.

\bibitem[Diakonikolas et~al.(2020)Diakonikolas, Kane, Kontonis, and
  Zarifis]{diakonikolas2020algorithms}
Ilias Diakonikolas, Daniel~M Kane, Vasilis Kontonis, and Nikos Zarifis.
\newblock Algorithms and sq lower bounds for pac learning one-hidden-layer relu
  networks.
\newblock In \emph{Conference on Learning Theory}, pages 1514--1539, 2020.

\bibitem[Hopkins et~al.(2015)Hopkins, Shi, and Steurer]{hopkins2015SOS}
Samuel~B. Hopkins, Jonathan Shi, and David Steurer.
\newblock Tensor principal component analysis via sum-of-square proofs.
\newblock In Peter Grünwald, Elad Hazan, and Satyen Kale, editors,
  \emph{Proceedings of The 28th Conference on Learning Theory}, volume~40 of
  \emph{Proceedings of Machine Learning Research}, pages 956--1006, Paris,
  France, 03--06 Jul 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v40/Hopkins15.html}.

\bibitem[Kunisky et~al.(2019)Kunisky, Wein, and Bandeira]{kunisky2019notes}
Dmitriy Kunisky, Alexander~S. Wein, and Afonso~S. Bandeira.
\newblock Notes on computational hardness of hypothesis testing: Predictions
  using the low-degree likelihood ratio, 2019.

\bibitem[Bandeira et~al.(2022)Bandeira, El~Alaoui, Hopkins, Schramm, Wein, and
  Zadik]{bandeira2022parisi}
Afonso~S Bandeira, Ahmed El~Alaoui, Samuel Hopkins, Tselil Schramm, Alexander~S
  Wein, and Ilias Zadik.
\newblock The franz-parisi criterion and computational trade-offs in high
  dimensional statistics.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 33831--33844. Curran Associates, Inc., 2022.

\bibitem[Brennan et~al.(2021)Brennan, Bresler, Hopkins, Li, and
  Schramm]{brennan2021sq}
Matthew~S Brennan, Guy Bresler, Sam Hopkins, Jerry Li, and Tselil Schramm.
\newblock Statistical query algorithms and low degree tests are almost
  equivalent.
\newblock In Mikhail Belkin and Samory Kpotufe, editors, \emph{Proceedings of
  Thirty Fourth Conference on Learning Theory}, volume 134 of \emph{Proceedings
  of Machine Learning Research}, pages 774--774. PMLR, 15--19 Aug 2021.
\newblock URL \url{https://proceedings.mlr.press/v134/brennan21a.html}.

\bibitem[Dudeja and Hsu(2021)]{dudeja2021pca}
Rishabh Dudeja and Daniel Hsu.
\newblock Statistical query lower bounds for tensor pca.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (83):\penalty0 1--51, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-837.html}.

\bibitem[Dudeja and Hsu(2022)]{dudeja2022statisticalcomputational}
Rishabh Dudeja and Daniel Hsu.
\newblock Statistical-computational trade-offs in tensor pca and related
  problems via communication complexity, 2022.

\bibitem[Ben~Arous et~al.(2020)Ben~Arous, Gheissari, and
  Jagannath]{benarous2020thresholds}
G{\'e}rard Ben~Arous, Reza Gheissari, and Aukosh Jagannath.
\newblock {Algorithmic thresholds for tensor PCA}.
\newblock \emph{The Annals of Probability}, 48\penalty0 (4):\penalty0 2052 --
  2087, 2020.
\newblock \doi{10.1214/19-AOP1415}.
\newblock URL \url{https://doi.org/10.1214/19-AOP1415}.

\bibitem[Ros et~al.(2019)Ros, Ben~Arous, Biroli, and
  Cammarota]{ros2019landscape}
Valentina Ros, Gerard Ben~Arous, Giulio Biroli, and Chiara Cammarota.
\newblock Complex energy landscapes in spiked-tensor and simple glassy models:
  Ruggedness, arrangements of local minima, and phase transitions.
\newblock \emph{Phys. Rev. X}, 9:\penalty0 011003, Jan 2019.
\newblock \doi{10.1103/PhysRevX.9.011003}.
\newblock URL \url{https://link.aps.org/doi/10.1103/PhysRevX.9.011003}.

\bibitem[Ben~Arous et~al.(2019)Ben~Arous, Mei, Montanari, and
  Nica]{benarous2019landscape}
Gérard Ben~Arous, Song Mei, Andrea Montanari, and Mihai Nica.
\newblock The landscape of the spiked tensor model.
\newblock \emph{Communications on Pure and Applied Mathematics}, 72\penalty0
  (11):\penalty0 2282--2330, 2019.
\newblock \doi{https://doi.org/10.1002/cpa.21861}.
\newblock URL \url{https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21861}.

\bibitem[Sz{\"o}r{\'e}nyi(2009)]{Szrnyi2009CharacterizingSQ}
Bal{\'a}zs Sz{\"o}r{\'e}nyi.
\newblock Characterizing statistical query learning: Simplified notions and
  proofs.
\newblock In \emph{ALT}, 2009.

\bibitem[Pinelis(1994)]{pinelis1994}
Iosif Pinelis.
\newblock {Optimum Bounds for the Distributions of Martingales in Banach
  Spaces}.
\newblock \emph{The Annals of Probability}, 22\penalty0 (4):\penalty0 1679 --
  1706, 1994.
\newblock \doi{10.1214/aop/1176988477}.
\newblock URL \url{https://doi.org/10.1214/aop/1176988477}.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Biewald(2020)]{wandb}
Lukas Biewald.
\newblock Experiment tracking with weights and biases, 2020.
\newblock URL \url{https://www.wandb.com/}.
\newblock Software available from wandb.com.

\end{thebibliography}
