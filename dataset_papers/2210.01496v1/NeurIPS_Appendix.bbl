\begin{thebibliography}{10}

\bibitem{agarwal2017finding}
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima faster than gradient descent.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 1195--1199, 2017.

\bibitem{allen2018natasha}
Zeyuan Allen-Zhu.
\newblock Natasha 2: Faster non-convex optimization than sgd.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{allen2017faster}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Faster principal component regression and stable matrix chebyshev
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  107--115. PMLR, 2017.

\bibitem{allen2018neon2}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Neon2: Finding local minima via first-order oracles.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{balasubramanian2022zeroth}
Krishnakumar Balasubramanian and Saeed Ghadimi.
\newblock Zeroth-order nonconvex stochastic optimization: Handling constraints,
  high dimensionality, and saddle points.
\newblock {\em Foundations of Computational Mathematics}, 22(1):35--76, 2022.

\bibitem{bhagoji2018practical}
Arjun~Nitin Bhagoji, Warren He, Bo~Li, and Dawn Song.
\newblock Practical black-box attacks on deep neural networks using efficient
  query mechanisms.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 154--169, 2018.

\bibitem{carmon2016gradient}
Yair Carmon and John~C Duchi.
\newblock Gradient descent efficiently finds the cubic-regularized non-convex
  newton step.
\newblock {\em arXiv preprint arXiv:1612.00547}, 2016.

\bibitem{carmon2017convex}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock “convex until proven guilty”: Dimension-free acceleration of
  gradient descent on non-convex functions.
\newblock In {\em International Conference on Machine Learning}, pages
  654--663. PMLR, 2017.

\bibitem{carmon2018accelerated}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for nonconvex optimization.
\newblock {\em SIAM Journal on Optimization}, 28(2):1751--1772, 2018.

\bibitem{cartis2011adaptive1}
Coralia Cartis, Nicholas~IM Gould, and Philippe~L Toint.
\newblock Adaptive cubic regularisation methods for unconstrained optimization.
  part i: motivation, convergence and numerical results.
\newblock {\em Mathematical Programming}, 127(2):245--295, 2011.

\bibitem{cartis2011adaptive2}
Coralia Cartis, Nicholas~IM Gould, and Philippe~L Toint.
\newblock Adaptive cubic regularisation methods for unconstrained optimization.
  part ii: worst-case function-and derivative-evaluation complexity.
\newblock {\em Mathematical programming}, 130(2):295--319, 2011.

\bibitem{CC01a}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock {\em ACM Transactions on Intelligent Systems and Technology},
  2:27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem{chen2017zoo}
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock In {\em Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pages 15--26, 2017.

\bibitem{choromanski2018structured}
Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and
  Adrian Weller.
\newblock Structured evolution with compact architectures for scalable policy
  optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  970--978. PMLR, 2018.

\bibitem{du2017gradient}
Simon~S Du, Chi Jin, Jason~D Lee, Michael~I Jordan, Aarti Singh, and Barnabas
  Poczos.
\newblock Gradient descent can take exponential time to escape saddle points.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{gao2018information}
Xiang Gao, Bo~Jiang, and Shuzhong Zhang.
\newblock On the information-adaptive variants of the admm: an iteration
  complexity perspective.
\newblock {\em Journal of Scientific Computing}, 76(1):327--363, 2018.

\bibitem{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points-online stochastic gradient for tensor
  decomposition.
\newblock In {\em Conference on learning theory}, pages 797--842. PMLR, 2015.

\bibitem{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{ge2018learning}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{ge2017optimization}
Rong Ge and Tengyu Ma.
\newblock On the optimization landscape of tensor decompositions.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{JMLR:v19:16-465}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock {\em Journal of Machine Learning Research}, 19(29):1--44, 2018.

\bibitem{jain2017global}
Prateek Jain, Chi Jin, Sham Kakade, and Praneeth Netrapalli.
\newblock Global convergence of non-convex gradient descent for computing
  matrix squareroot.
\newblock In {\em Artificial Intelligence and Statistics}, pages 479--488.
  PMLR, 2017.

\bibitem{ji2019improved}
Kaiyi Ji, Zhe Wang, Yi~Zhou, and Yingbin Liang.
\newblock Improved zeroth-order variance reduced algorithms and analysis for
  nonconvex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  3100--3109. PMLR, 2019.

\bibitem{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem{jin2018local}
Chi Jin, Lydia~T Liu, Rong Ge, and Michael~I Jordan.
\newblock On the local minima of the empirical risk.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{jin2018accelerated}
Chi Jin, Praneeth Netrapalli, and Michael~I Jordan.
\newblock Accelerated gradient descent escapes saddle points faster than
  gradient descent.
\newblock In {\em Conference On Learning Theory}, pages 1042--1085. PMLR, 2018.

\bibitem{jing2021asynchronous}
Gangshan Jing, He~Bai, Jemin George, Aranya Chakrabortty, and Piyush~K Sharma.
\newblock Asynchronous distributed reinforcement learning for lqr control via
  zeroth-order block coordinate descent.
\newblock {\em arXiv preprint arXiv:2107.12416}, 2021.

\bibitem{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{kawaguchi2019every}
Kenji Kawaguchi, Jiaoyang Huang, and Leslie~Pack Kaelbling.
\newblock Every local minimum value is the global minimum value of induced
  model in nonconvex machine learning.
\newblock {\em Neural Computation}, 31(12):2293--2323, 2019.

\bibitem{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via scsg methods.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{li2022restarted}
Huan Li and Zhouchen Lin.
\newblock Restarted nonconvex accelerated gradient descent: No more
  polylogarithmic factor in the {$ \mathcal{O} (\epsilon^{-7/4}) $} complexity.
\newblock {\em arXiv preprint arXiv:2201.11411}, 2022.

\bibitem{liu2018adaptive}
Mingrui Liu, Zhe Li, Xiaoyu Wang, Jinfeng Yi, and Tianbao Yang.
\newblock Adaptive negative curvature descent with applications in non-convex
  optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{lucchi2021second}
Aurelien Lucchi, Antonio Orvieto, and Adamos Solomou.
\newblock On the second-order convergence properties of random search methods.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{nesterov2003introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem{nesterov2018lectures}
Yurii Nesterov et~al.
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock {\em Mathematical Programming}, 108(1):177--205, 2006.

\bibitem{nesterov2017random}
Yurii Nesterov and Vladimir Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock {\em Foundations of Computational Mathematics}, 17(2):527--566, 2017.

\bibitem{oja1982simplified}
Erkki Oja.
\newblock Simplified neuron model as a principal component analyzer.
\newblock {\em Journal of Mathematical Biology}, 15(3):267--273, 1982.

\bibitem{papernot2017practical}
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z~Berkay Celik,
  and Ananthram Swami.
\newblock Practical black-box attacks against machine learning.
\newblock In {\em Proceedings of the 2017 ACM on Asia Conference on Computer
  and Communications Security}, pages 506--519, 2017.

\bibitem{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  314--323. PMLR, 2016.

\bibitem{salimans2017evolution}
Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1703.03864}, 2017.

\bibitem{sun2018geometric}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock {\em Foundations of Computational Mathematics}, 18(5):1131--1198,
  2018.

\bibitem{tu2019autozoom}
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi,
  Cho-Jui Hsieh, and Shin-Ming Cheng.
\newblock Autozoom: Autoencoder-based zeroth order optimization method for
  attacking black-box neural networks.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  33:742--749, 07 2019.

\bibitem{vlatakis2019efficiently}
Emmanouil-Vasileios Vlatakis-Gkaragkounis, Lampros Flokas, and Georgios
  Piliouras.
\newblock Efficiently avoiding saddle points with zero order methods: No
  gradients required.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{azuma-hoeffding}
{Wikipedia contributors}.
\newblock Azuma's inequality --- {Wikipedia}{,} the free encyclopedia, 2021.
\newblock [Online; accessed 22-March-2022].

\bibitem{xu2017neon+}
Yi~Xu, Rong Jin, and Tianbao Yang.
\newblock Neon+: Accelerated gradient methods for extracting negative curvature
  for non-convex optimization.
\newblock {\em arXiv preprint arXiv:1712.01033}, 2017.

\bibitem{ye2018hessian}
Haishan Ye, Zhichao Huang, Cong Fang, Chris~Junchi Li, and Tong Zhang.
\newblock Hessian-aware zeroth-order optimization for black-box adversarial
  attack.
\newblock {\em arXiv preprint arXiv:1812.11377}, 2018.

\bibitem{zhang2021escape}
Chenyi Zhang and Tongyang Li.
\newblock Escape saddle points by a simple gradient-descent based algorithm.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\end{thebibliography}
