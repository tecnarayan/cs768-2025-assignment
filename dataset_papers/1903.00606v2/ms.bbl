\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{bacon2017option}
Bacon, P.-L., Harb, J., and Precup, D.
\newblock The option-critic architecture.
\newblock In \emph{AAAI}, pp.\  1726--1734, 2017.

\bibitem[Braess(1968)]{braess1968paradoxon}
Braess, D.
\newblock {\"U}ber ein paradoxon aus der verkehrsplanung.
\newblock \emph{Unternehmensforschung}, 12\penalty0 (1):\penalty0 258--268,
  1968.

\bibitem[Braess et~al.(2005)Braess, Nagurney, and
  Wakolbinger]{braess2005paradox}
Braess, D., Nagurney, A., and Wakolbinger, T.
\newblock On a paradox of traffic planning.
\newblock \emph{Transportation science}, 39\penalty0 (4):\penalty0 446--450,
  2005.

\bibitem[Broder \& Karlin(1989)Broder and Karlin]{broder1989bounds}
Broder, A.~Z. and Karlin, A.~R.
\newblock Bounds on the cover time.
\newblock \emph{Journal of Theoretical Probability}, 2\penalty0 (1):\penalty0
  101--120, 1989.

\bibitem[Chung(1996)]{chung1996spectral}
Chung, F.~R.
\newblock \emph{Spectral graph theory}.
\newblock American Mathematical Society, 1996.

\bibitem[Dietterich(2000)]{dietterich2000hierarchical}
Dietterich, T.~G.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function
  decomposition.
\newblock \emph{Journal of Artificial Intelligence Research}, 13:\penalty0
  227--303, 2000.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Fiedler(1973)]{fiedler1973algebraic}
Fiedler, M.
\newblock Algebraic connectivity of graphs.
\newblock \emph{Czechoslovak mathematical journal}, 23\penalty0 (2):\penalty0
  298--305, 1973.

\bibitem[Ghosh \& Boyd(2006)Ghosh and Boyd]{ghosh2006growing}
Ghosh, A. and Boyd, S.
\newblock Growing well-connected graphs.
\newblock In \emph{Decision and Control, 2006 45th IEEE Conference on}, pp.\
  6605--6611. IEEE, 2006.

\bibitem[Haemers(1995)]{haemers1995interlacing}
Haemers, W.~H.
\newblock Interlacing eigenvalues and graphs.
\newblock \emph{Linear Algebra and its applications}, 226:\penalty0 593--616,
  1995.

\bibitem[Harb et~al.(2017)Harb, Bacon, Klissarov, and Precup]{harb2017waiting}
Harb, J., Bacon, P.-L., Klissarov, M., and Precup, D.
\newblock When waiting is not an option: Learning options with a deliberation
  cost.
\newblock \emph{arXiv preprint arXiv:1709.04571}, 2017.

\bibitem[Iba(1989)]{iba1989heuristic}
Iba, G.~A.
\newblock A heuristic approach to the discovery of macro-operators.
\newblock \emph{Machine Learning}, 3\penalty0 (4):\penalty0 285--317, 1989.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jinnai et~al.(2018)Jinnai, Abel, Littman, and
  Konidaris]{jinnai2018finding}
Jinnai, Y., Abel, D., Littman, M., and Konidaris, G.
\newblock Finding options that minimize planning time.
\newblock \emph{arXiv preprint arXiv:1810.07311}, 2018.

\bibitem[Kompella et~al.(2017)Kompella, Stollenga, Luciw, and
  Schmidhuber]{kompella2017continual}
Kompella, V.~R., Stollenga, M., Luciw, M., and Schmidhuber, J.
\newblock Continual curiosity-driven skill acquisition from high-dimensional
  video inputs for humanoid robots.
\newblock \emph{Artificial Intelligence}, 247:\penalty0 313--335, 2017.

\bibitem[Konidaris \& Barto(2009)Konidaris and Barto]{konidaris2009skill}
Konidaris, G. and Barto, A.
\newblock Skill discovery in continuous reinforcement learning domains using
  skill chaining.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1015--1023, 2009.

\bibitem[Koren(2003)]{koren2003spectral}
Koren, Y.
\newblock On spectral graph drawing.
\newblock In \emph{International Computing and Combinatorics Conference}, pp.\
  496--508. Springer, 2003.

\bibitem[Koren et~al.(2002)Koren, Carmel, and Harel]{koren2002ace}
Koren, Y., Carmel, L., and Harel, D.
\newblock Ace: A fast multiscale eigenvectors computation for drawing huge
  graphs.
\newblock In \emph{Information Visualization, 2002. INFOVIS 2002. IEEE
  Symposium on}, pp.\  137--144. IEEE, 2002.

\bibitem[Machado \& Bowling(2016)Machado and Bowling]{machado2016learning}
Machado, M.~C. and Bowling, M.
\newblock Learning purposeful behaviour in the absence of rewards.
\newblock \emph{arXiv preprint arXiv:1605.07700}, 2016.

\bibitem[Machado et~al.(2017{\natexlab{a}})Machado, Bellemare, and
  Bowling]{machado2017laplacian}
Machado, M.~C., Bellemare, M.~G., and Bowling, M.
\newblock A {L}aplacian framework for option discovery in reinforcement
  learning.
\newblock In \emph{ICML}, 2017{\natexlab{a}}.

\bibitem[Machado et~al.(2017{\natexlab{b}})Machado, Rosenbaum, Guo, Liu,
  Tesauro, and Campbell]{machado2017eigenoption}
Machado, M.~C., Rosenbaum, C., Guo, X., Liu, M., Tesauro, G., and Campbell, M.
\newblock Eigenoption discovery through the deep successor representation.
\newblock \emph{arXiv preprint arXiv:1710.11089}, 2017{\natexlab{b}}.

\bibitem[Mankowitz et~al.(2016)Mankowitz, Mann, and
  Mannor]{mankowitz2016adaptive}
Mankowitz, D.~J., Mann, T.~A., and Mannor, S.
\newblock Adaptive skills adaptive partitions ({ASAP}).
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1588--1596, 2016.

\bibitem[McGovern \& Barto(2001)McGovern and Barto]{mcgovern2001automatic}
McGovern, A. and Barto, A.~G.
\newblock Automatic discovery of subgoals in reinforcement learning using
  diverse density.
\newblock In \emph{International Conference on Machine Learning}, 2001.

\bibitem[Menache et~al.(2002)Menache, Mannor, and Shimkin]{menache2002q}
Menache, I., Mannor, S., and Shimkin, N.
\newblock Q-cut - dynamic discovery of sub-goals in reinforcement learning.
\newblock In \emph{European Conference on Machine Learning}, pp.\  295--306.
  Springer, 2002.

\bibitem[Mosk-Aoyama(2008)]{mosk2008maximum}
Mosk-Aoyama, D.
\newblock Maximum algebraic connectivity augmentation is np-hard.
\newblock \emph{Operations Research Letters}, 36\penalty0 (6):\penalty0
  677--679, 2008.

\bibitem[Nair et~al.(2018)Nair, Pong, Dalal, Bahl, Lin, and
  Levine]{nair2018visual}
Nair, A.~V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S.
\newblock Visual reinforcement learning with imagined goals.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9209--9220, 2018.

\bibitem[Ortner \& Auer(2007)Ortner and Auer]{ortner2007logarithmic}
Ortner, P. and Auer, R.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \emph{Proceedings of the 2006 Conference on Advances in Neural
  Information Processing Systems}, volume~19, pp.\ ~49, 2007.

\bibitem[Parr \& Russell(1998)Parr and Russell]{parr1998reinforcement}
Parr, R. and Russell, S.
\newblock Reinforcement learning with hierarchies of machines.
\newblock \emph{Advances in neural information processing systems}, pp.\
  1043--1049, 1998.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave,
  van~de Wiele, Mnih, Heess, and Springenberg]{riedmiller2018kearning}
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., van~de Wiele,
  T., Mnih, V., Heess, N., and Springenberg, J.~T.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pp.\  4344--4353, 2018.

\bibitem[{\c S}im{\c s}ek \& Barto(2004){\c S}im{\c s}ek and Barto]{Simsek04}
{\c S}im{\c s}ek, {\" O}. and Barto, A.
\newblock Using relative novelty to identify useful temporal abstractions in
  reinforcement learning.
\newblock In \emph{Proceedings of the 21st International Conference on Machine
  Learning}, pp.\  751--758, 2004.

\bibitem[{\c{S}}im{\c{s}}ek \& Barto(2009){\c{S}}im{\c{s}}ek and
  Barto]{csimcsek2009skill}
{\c{S}}im{\c{s}}ek, {\"O}. and Barto, A.~G.
\newblock Skill characterization based on betweenness.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1497--1504, 2009.

\bibitem[Simsek et~al.(2005)Simsek, Wolfe, and Barto]{Simsek2005}
Simsek, O., Wolfe, A., and Barto, A.
\newblock Identifying useful subgoals in reinforcement learning by local graph
  partitioning.
\newblock In \emph{Proceedings of the Twenty Second International Conference},
  pp.\  816--823, 2005.

\bibitem[Stolle \& Precup(2002)Stolle and Precup]{stolle2002learning}
Stolle, M. and Precup, D.
\newblock Learning options in reinforcement learning.
\newblock In \emph{International Symposium on abstraction, reformulation, and
  approximation}, pp.\  212--223. Springer, 2002.

\bibitem[Sutton et~al.(1999)Sutton, , Precup, and Singh]{sutton1999between}
Sutton, R., , Precup, D., and Singh, S.
\newblock Between {MDP}s and semi-{MDP}s: A framework for temporal abstraction
  in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1):\penalty0 181--211,
  1999.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 1998.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 279--292, 1992.

\end{thebibliography}
