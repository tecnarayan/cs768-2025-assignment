On the Role of Attention Masks and LayerNorm in Transformers