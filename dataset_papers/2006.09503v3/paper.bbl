\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[meg()]{megatronrepository}
{Megatron Repository}.
\newblock \url{https://github.com/nvidia/megatron-lm}.

\bibitem[nvi()]{nvidiabenchmark}
{NVIDIA Deep Learning Examples, BERT}.
\newblock
  \url{https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/README.md#results}.

\bibitem[ope()]{openwebtext}
{OpenWebText Dataset}.
\newblock \url{https://github.com/jcpeterson/openwebtext}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, and et~al.]{gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., and et~al.
\newblock {Language Models are Few-Shot Learners}.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training {D}eep {N}ets with {S}ublinear {M}emory {C}ost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014adam}
Chilimbi, T.~M., Suzue, Y., Apacible, J., and Kalyanaraman, K.
\newblock Project {A}dam: {B}uilding an {E}fficient and {S}calable {D}eep
  {L}earning {T}raining {S}ystem.
\newblock In \emph{11th {USENIX} Symposium on Operating Systems Design and
  Implementation ({OSDI} '14)}, volume~14, pp.\  571--582, 2014.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{dean2012deepbelief}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A.,
  Tucker, P., Yang, K., Le, Q.~V., et~al.
\newblock Large {S}cale {D}istributed {D}eep {N}etworks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1223--1231, 2012.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: {P}re-training of {D}eep {B}idirectional {T}ransformers for
  {L}anguage {U}nderstanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Fan et~al.(2021)Fan, Rong, Meng, Cao, Wang, Zheng, Wu, Long, Yang,
  Xia, et~al.]{fan2021dapple}
Fan, S., Rong, Y., Meng, C., Cao, Z., Wang, S., Zheng, Z., Wu, C., Long, G.,
  Yang, J., Xia, L., et~al.
\newblock {DAPPLE: A Pipelined Data Parallel Approach for Training Large
  Models}.
\newblock In \emph{Proceedings of the 26th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, pp.\  431--445, 2021.

\bibitem[Griewank \& Walther(2000)Griewank and Walther]{griewank2000algorithm}
Griewank, A. and Walther, A.
\newblock {Revolve: An Implementation of Checkpointing for the Reverse or
  Adjoint Mode of Computational Differentiation}.
\newblock \emph{ACM Transactions on Mathematical Software (TOMS)}, 26\penalty0
  (1):\penalty0 19--45, 2000.

\bibitem[Harlap et~al.(2018)Harlap, Narayanan, Phanishayee, Seshadri, Devanur,
  Ganger, and Gibbons]{harlap2018pipedream}
Harlap, A., Narayanan, D., Phanishayee, A., Seshadri, V., Devanur, N., Ganger,
  G., and Gibbons, P.
\newblock Pipe{D}ream: {F}ast and {E}fficient {P}ipeline {P}arallel {DNN}
  {T}raining.
\newblock \emph{arXiv preprint arXiv:1806.03377}, 2018.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, et~al.]{huang2019gpipe}
Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam,
  J., Le, Q.~V., Wu, Y., et~al.
\newblock {GP}ipe: {E}fficient {T}raining of {G}iant {N}eural {N}etworks using
  {P}ipeline {P}arallelism.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  103--112, 2019.

\bibitem[Jain et~al.(2018)Jain, Phanishayee, Mars, Tang, and
  Pekhimenko]{jain2018gist}
Jain, A., Phanishayee, A., Mars, J., Tang, L., and Pekhimenko, G.
\newblock {Gist: Efficient Data Encoding for Deep Neural Network Training}.
\newblock In \emph{2018 ACM/IEEE 45th Annual International Symposium on
  Computer Architecture (ISCA)}, pp.\  776--789. IEEE, 2018.

\bibitem[Jain et~al.(2020)Jain, Jain, Nrusimha, Gholami, Abbeel, Gonzalez,
  Keutzer, and Stoica]{mlsys2020_196}
Jain, P., Jain, A., Nrusimha, A., Gholami, A., Abbeel, P., Gonzalez, J.,
  Keutzer, K., and Stoica, I.
\newblock {Breaking the Memory Wall with Optimal Tensor Rematerialization}.
\newblock In \emph{Proceedings of Machine Learning and Systems 2020}, pp.\
  497--511. 2020.

\bibitem[Jia et~al.(2018)Jia, Zaharia, and Aiken]{flexflow}
Jia, Z., Zaharia, M., and Aiken, A.
\newblock {B}eyond {D}ata and {M}odel {P}arallelism for {D}eep {N}eural
  {N}etworks.
\newblock In \emph{Proceedings of the 2nd Conference on Machine Learning and
  Systems (MLSys)}, 2018.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa,
  Bates, Bhatia, Boden, Borchers, et~al.]{jouppi2017datacenter}
Jouppi, N.~P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R.,
  Bates, S., Bhatia, S., Boden, N., Borchers, A., et~al.
\newblock {In-Datacenter Performance Analysis of a Tensor Processing Unit}.
\newblock In \emph{Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pp.\  1--12, 2017.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{lai2017race}
Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E.
\newblock {RACE: Large-scale ReAding Comprehension Dataset From Examinations}.
\newblock \emph{arXiv preprint arXiv:1704.04683}, 2017.

\bibitem[Narayanan et~al.(2019)Narayanan, Harlap, Phanishayee, Seshadri,
  Devanur, Ganger, Gibbons, and Zaharia]{narayanan2019pipedream}
Narayanan, D., Harlap, A., Phanishayee, A., Seshadri, V., Devanur, N.~R.,
  Ganger, G.~R., Gibbons, P.~B., and Zaharia, M.
\newblock Pipe{D}ream: {G}eneralized {P}ipeline {P}arallelism for {DNN}
  {T}raining.
\newblock In \emph{Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, pp.\  1--15, 2019.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving {L}anguage {U}nderstanding by {G}enerative {P}re-training.
\newblock \emph{URL https://s3-us-west-2. amazonaws.
  com/openai-assets/researchcovers/languageunsupervised/language understanding
  paper. pdf}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language {M}odels are {U}nsupervised {M}ultitask {L}earners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rajbhandari et~al.(2019)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2019zero}
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y.
\newblock {ZeRO: Memory Optimization Towards Training A Trillion Parameter
  Models}.
\newblock \emph{arXiv preprint arXiv:1910.02054}, 2019.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-{LM}: {T}raining {M}ulti-{B}illion {P}arameter {L}anguage
  {M}odels using {GPU} {M}odel {P}arallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2019glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural
  Language Understanding}.
\newblock 2019.
\newblock In the Proceedings of ICLR.

\bibitem[Xing et~al.(2015)Xing, Ho, Dai, Kim, Wei, Lee, Zheng, Xie, Kumar, and
  Yu]{xing2015petuum}
Xing, E.~P., Ho, Q., Dai, W., Kim, J.~K., Wei, J., Lee, S., Zheng, X., Xie, P.,
  Kumar, A., and Yu, Y.
\newblock {Petuum: A New Platform for Distributed Machine Learning on Big
  Data}.
\newblock \emph{IEEE Transactions on Big Data}, 1\penalty0 (2):\penalty0
  49--67, 2015.

\bibitem[Yang et~al.(2019)Yang, Zhang, Li, R{\'e}, Aberger, and
  De~Sa]{yang2019pipemare}
Yang, B., Zhang, J., Li, J., R{\'e}, C., Aberger, C.~R., and De~Sa, C.
\newblock {PipeMare: Asynchronous Pipeline Parallel DNN Training}.
\newblock \emph{arXiv preprint arXiv:1910.05124}, 2019.

\end{thebibliography}
