\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam and Sastry(2017)]{achiam17a}
J.~Achiam and S.~Sastry.
\newblock Surprise-based intrinsic motivation for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.01732}, 2017.

\bibitem[Ajay et~al.(2020)Ajay, Kumar, Agrawal, Levine, and
  Nachum]{ajay2020opal}
A.~Ajay, A.~Kumar, P.~Agrawal, S.~Levine, and O.~Nachum.
\newblock {OPAL}: {O}ffline primitive discovery for accelerating offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.13611}, 2020.

\bibitem[Amin et~al.(2020)Amin, Gomrokchi, Aboutalebi, Satija, and
  Precup]{amin2020locally}
S.~Amin, M.~Gomrokchi, H.~Aboutalebi, H.~Satija, and D.~Precup.
\newblock Locally persistent exploration in continuous control tasks with
  sparse rewards.
\newblock \emph{arXiv preprint arXiv:2012.13658}, 2020.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz17}
M.~Andrychowicz, F.~Wolski, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, P.~Abbeel, and W.~Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:1707.01495}, 2017.

\bibitem[Bacon et~al.(2017)Bacon, Harb, and Precup]{bacon17}
P.-L. Bacon, J.~Harb, and D.~Precup.
\newblock The option-critic architecture.
\newblock In \emph{Proceedings of the National Conference on Artificial
  Intelligence (AAAI)}, pages 1726--1734, 2017.

\bibitem[Bagatella et~al.(2022)Bagatella, Christen, and Hilliges]{bagatella22}
M.~Bagatella, S.~Christen, and O.~Hilliges.
\newblock {SFP}: {S}tate-free priors for exploration in off-policy
  reinforcement learning.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Bagatella et~al.(2024)Bagatella, Christen, and Hilliges.]{sfp-github}
M.~Bagatella, S.~Christen, and O.~Hilliges.
\newblock {SFP}: {S}tate-free priors for exploration in off-policy
  reinforcement learning---{GitHub}, 2024.
\newblock URL \url{https://github.com/eth-ait/sfp}.
\newblock Accessed on March 1, 2024.

\bibitem[Barto and Mahadevan(2003)]{barto03}
A.~G. Barto and S.~Mahadevan.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock \emph{Discrete Event Dynamic Systems}, 13:\penalty0 41--77, 2003.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare16}
M.~Bellemare, S.~Srinivasan, G.~Ostrovski, T.~Schaul, D.~Saxton, and R.~Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem[Biewald(2020)]{wandb}
L.~Biewald.
\newblock Experiment tracking with weights and biases, 2020.
\newblock URL \url{https://www.wandb.com/}.
\newblock Software available from wandb.com.

\bibitem[Boutilier et~al.(1997)Boutilier, Brafman, and Geib]{boutilier97}
C.~Boutilier, R.~I. Brafman, and C.~Geib.
\newblock Prioritized goal decomposition of {M}arkov decision processes:
  {T}oward a synthesis of classical and decision theoretic planning.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence (IJCAI)}, pages 1156--1162, 1997.

\bibitem[Burda et~al.(2018{\natexlab{a}})Burda, Edwards, Pathak, Storkey,
  Darrell, and Efros]{burda2018large}
Y.~Burda, H.~Edwards, D.~Pathak, A.~Storkey, T.~Darrell, and A.~A. Efros.
\newblock Large-scale study of curiosity-driven learning.
\newblock \emph{arXiv preprint arXiv:1808.04355}, 2018{\natexlab{a}}.

\bibitem[Burda et~al.(2018{\natexlab{b}})Burda, Edwards, Storkey, and
  Klimov]{burda18a}
Y.~Burda, H.~Edwards, A.~Storkey, and O.~Klimov.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen21}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,
  A.~Srinivas, and I.~Mordatch.
\newblock Decision transformer: {R}einforcement learning via sequence modeling.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 15084--15097, Dec. 2021.

\bibitem[Chentanez et~al.(2004)Chentanez, Barto, and Singh]{chentanez04}
N.~Chentanez, A.~Barto, and S.~Singh.
\newblock Intrinsically motivated reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2004.

\bibitem[Christodoulou(2019)]{christodoulou2019soft}
P.~Christodoulou.
\newblock Soft actor-critic for discrete action settings.
\newblock \emph{arXiv preprint arXiv:1910.07207}, 2019.

\bibitem[Cobbe et~al.(2019)Cobbe, Klimov, Hesse, Kim, and
  Schulman]{cobbe2019quantifying}
K.~Cobbe, O.~Klimov, C.~Hesse, T.~Kim, and J.~Schulman.
\newblock Quantifying generalization in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1282--1289, 2019.

\bibitem[Dadashi et~al.(2022)Dadashi, Hussenot, Vincent, Girgin, Raichuk,
  Geist, and Pietquin]{dadashi2022continuous}
R.~Dadashi, L.~Hussenot, D.~Vincent, S.~Girgin, A.~Raichuk, M.~Geist, and
  O.~Pietquin.
\newblock Continuous control with action quantization from demonstrations.
\newblock In \emph{International Conference on Machine Learning}, pages
  4537--4557, 2022.

\bibitem[Daniel et~al.(2012)Daniel, Neumann, and Peters]{daniel12}
C.~Daniel, G.~Neumann, and J.~Peters.
\newblock Hierarchical relative entropy policy search.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, pages 273--281, 2012.

\bibitem[Dayan and Hinton(1992)]{dayan92}
P.~Dayan and G.~E. Hinton.
\newblock Feudal reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 1992.

\bibitem[Dietterich(2000)]{dietterich00}
T.~G. Dietterich.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function
  decomposition.
\newblock \emph{Journal of Artificial Intelligence Research}, 13:\penalty0
  227--303, November 2000.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
B.~Eysenbach, A.~Gupta, J.~Ibarz, and S.~Levine.
\newblock Diversity is all you need: {L}earning skills without a reward
  function.
\newblock \emph{arXiv preprint arXiv:1802.06070}, 2018.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu20}
J.~Fu, A.~Kumar, O.~Nachum, G.~Tucker, and S.~Levine.
\newblock {D4RL}: {D}atasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Gage(1994)]{gage94}
P.~Gage.
\newblock A new algorithm for data compression.
\newblock \emph{C Users Journal}, 12\penalty0 (2):\penalty0 23--38, 1994.

\bibitem[Gregor et~al.(2016)Gregor, Rezende, and Wierstra]{gregor16}
K.~Gregor, D.~J. Rezende, and D.~Wierstra.
\newblock Variational intrinsic control.
\newblock \emph{arXiv preprint arXiv:1611.07507}, 2016.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu17}
S.~Gu, E.~Holly, T.~Lillicrap, and S.~Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{Proceedings of the IEEE International Conference on Robotics
  and Automation (ICRA)}, pages 3389--3396, 2017.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Ha, Zhou, Tan, Tucker,
  and Levine]{haarnoja18a}
T.~Haarnoja, S.~Ha, A.~Zhou, J.~Tan, G.~Tucker, and S.~Levine.
\newblock Learning to walk via deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.11103}, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, and Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, K.~Hartikainen, G.~Tucker, S.~Ha, J.~Tan, V.~Kumar,
  H.~Zhu, A.~Gupta, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Haber et~al.(2018)Haber, Mrowca, Wang, Fei-Fei, and Yamins]{haber18}
N.~Haber, D.~Mrowca, S.~Wang, L.~F. Fei-Fei, and D.~L. Yamins.
\newblock Learning to play with intrinsically-motivated, self-aware agents.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, Montr{\'e}al, Canada, Dec. 2018.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and Van~Soest]{hazan19}
E.~Hazan, S.~Kakade, K.~Singh, and A.~Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 2681--2691, 2019.

\bibitem[He et~al.(2020)He, Haffari, and Norouzi]{he2020dynamic}
X.~He, G.~Haffari, and M.~Norouzi.
\newblock Dynamic programming encoding for subword segmentation in neural
  machine translation.
\newblock In \emph{Proceedings of the Association for Computational Linguistics
  (ACL)}, pages 3042--3051, 2020.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner21}
M.~Janner, Q.~Li, and S.~Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 1273--1286, 2021.

\bibitem[Jiang et~al.(2022)Jiang, Zhang, Janner, Li, Rockt{\"a}schel,
  Grefenstette, and Tian]{jiang2022efficient}
Z.~Jiang, T.~Zhang, M.~Janner, Y.~Li, T.~Rockt{\"a}schel, E.~Grefenstette, and
  Y.~Tian.
\newblock Efficient planning in a compact latent action space.
\newblock \emph{arXiv preprint arXiv:2208.10291}, 2022.

\bibitem[Kaelbling(1993)]{kaelbling93a}
L.~P. Kaelbling.
\newblock Hierarchical learning in stochastic domains: {P}reliminary results.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 167--173, 1993.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Konidaris and Barto(2009)]{konidaris09a}
G.~Konidaris and A.~Barto.
\newblock Skill discovery in continuous reinforcement learning domains using
  skill chaining.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, December 2009.

\bibitem[Konidaris et~al.(2012)Konidaris, Kuindersma, Grupen, and
  Barto]{konidaris12}
G.~Konidaris, S.~R. Kuindersma, R.~A. Grupen, and A.~G. Barto.
\newblock Robot learning from demonstration by constructing skill trees.
\newblock \emph{International Journal of Robotics Research}, 31\penalty0
  (3):\penalty0 360--375, March 2012.

\bibitem[Kudo(2018)]{kudo2018subword}
T.~Kudo.
\newblock Subword regularization: {I}mproving neural network translation models
  with multiple subword candidates.
\newblock In \emph{Proceedings of the Association for Computational Linguistics
  (ACL)}, pages 66--75, 2018.
\newblock \doi{10.18653/v1/P18-1007}.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{kulkarni16}
T.~D. Kulkarni, K.~R. Narasimhan, A.~Saeedi, and J.~B. Tenenbaum.
\newblock Hierarchical deep reinforcement learning: {I}ngegrating temporal
  abstaction and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, Barcelona, Spain, December 2016.

\bibitem[Lange and Faisal(2019)]{lange2019semantic}
R.~T. Lange and A.~Faisal.
\newblock Semantic rl with action grammars: Data-efficient learning of
  hierarchical task abstractions.
\newblock \emph{arXiv preprint arXiv:1907.12477}, 2019.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee19}
L.~Lee, B.~Eysenbach, E.~Parisotto, E.~Xing, S.~Levine, and R.~Salakhutdinov.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Levy et~al.(2017)Levy, Konidaris, Platt, and Saenko]{levy17}
A.~Levy, G.~Konidaris, R.~Platt, and K.~Saenko.
\newblock Learning multi-level hierarchies with hindsight.
\newblock \emph{arXiv preprint arXiv:1712.00948}, 2017.

\bibitem[Liu and Abbeel(2021)]{liu2021behavior}
H.~Liu and P.~Abbeel.
\newblock Behavior from the void: {U}nsupervised active pre-training.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 18459--18473, 2021.

\bibitem[Lopes et~al.(2012)Lopes, Lang, Toussaint, and Oudeyer]{lopes12}
M.~Lopes, T.~Lang, M.~Toussaint, and P.-Y. Oudeyer.
\newblock Exploration in model-based reinforcement learning by empirically
  estimating learning progress.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2012.

\bibitem[Lynch et~al.(2020)Lynch, Khansari, Xiao, Kumar, Tompson, Levine, and
  Sermanet]{lynch2020learning}
C.~Lynch, M.~Khansari, T.~Xiao, V.~Kumar, J.~Tompson, S.~Levine, and
  P.~Sermanet.
\newblock Learning latent plans from play.
\newblock In \emph{Proceedings of the Conference on Robot Learning (CoRL)},
  pages 1113--1132, 2020.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih13}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~A. Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv:1312.5602}, 2013.

\bibitem[Mohamed and Jimenez~Rezende(2015)]{mohamed15}
S.~Mohamed and D.~Jimenez~Rezende.
\newblock Variational information maximisation for intrinsically motivated
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2015.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{nachum18a}
O.~Nachum, S.~S. Gu, H.~Lee, and S.~Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, Montr{\'e}al, Canada, 2018.

\bibitem[Park et~al.(2021)Park, Kim, and Kim]{park2021time}
S.~Park, J.~Kim, and G.~Kim.
\newblock Time discretization-invariant safe action repetition for policy
  gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 267--279, 2021.

\bibitem[Park et~al.(2022)Park, Choi, Kim, Lee, and Kim]{park2022lipschitz}
S.~Park, J.~Choi, J.~Kim, H.~Lee, and G.~Kim.
\newblock Lipschitz-constrained unsupervised skill discovery.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Park et~al.(2023)Park, Lee, Lee, and Abbeel]{park2023controllability}
S.~Park, K.~Lee, Y.~Lee, and P.~Abbeel.
\newblock Controllability-aware unsupervised skill discovery.
\newblock \emph{arXiv preprint arXiv:2302.05103}, 2023.

\bibitem[Parr and Russell(1997)]{parr97}
R.~Parr and S.~Russell.
\newblock Reinforcement learning with hierarchies of machines.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 1043--1049, 1997.

\bibitem[Parr(1998)]{parr98}
R.~E. Parr.
\newblock \emph{Hierarchical control and learning for {M}arkov decision
  processes}.
\newblock PhD thesis, University of California, Berkeley, Berkeley, CA, 1998.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock {PyTorch}: {A}n imperative style, high-performance deep learning
  library.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and Darrell]{pathak17}
D.~Pathak, P.~Agrawal, A.~A. Efros, and T.~Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 2778--2787, 2017.

\bibitem[Pertsch et~al.(2021)Pertsch, Lee, and Lim]{pertsch21}
K.~Pertsch, Y.~Lee, and J.~Lim.
\newblock Accelerating reinforcement learning with learned skill priors.
\newblock In \emph{Proceedings of the Conference on Robot Learning (CoRL)},
  pages 188--204, 2021.

\bibitem[Pertsch et~al.(2022)Pertsch, Lee, Wu, and Lim]{pertsch2022guided}
K.~Pertsch, Y.~Lee, Y.~Wu, and J.~J. Lim.
\newblock Guided reinforcement learning with learned skills.
\newblock In \emph{Conference on Robot Learning}, pages 729--739, 2022.

\bibitem[Pertsch et~al.(2024)Pertsch, Lee, and Lim]{spirl-ssp-github}
K.~Pertsch, Y.~Lee, and J.~Lim.
\newblock Accelerating reinforcement learning with learned skill
  priors---{GitHub}, 2024.
\newblock URL \url{https://github.com/clvrai/spirl}.
\newblock Accessed on March 1, 2024.

\bibitem[Pitis et~al.(2020)Pitis, Chan, Zhao, Stadie, and Ba]{pitis2020maximum}
S.~Pitis, H.~Chan, S.~Zhao, B.~Stadie, and J.~Ba.
\newblock Maximum entropy gain exploration for long horizon multi-goal
  reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 7750--7761, 2020.

\bibitem[Poupart et~al.(2006)Poupart, Vlassis, Hoey, and Regan]{poupart06}
P.~Poupart, N.~Vlassis, J.~Hoey, and K.~Regan.
\newblock An analytic solution to discrete {B}ayesian reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 697--704, Pittsburgh, PA, 2006.

\bibitem[Provilkov et~al.(2020)Provilkov, Emelianenko, and
  Voita]{provilkov2020bpe}
I.~Provilkov, D.~Emelianenko, and E.~Voita.
\newblock {BPE}-dropout: Simple and effective subword regularization.
\newblock In \emph{Proceedings of the Association for Computational Linguistics
  (ACL)}, pages 1882--1892, 2020.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and
  Dormann]{stable-baselines3}
A.~Raffin, A.~Hill, A.~Gleave, A.~Kanervisto, M.~Ernestus, and N.~Dormann.
\newblock {Stable-Baselines3}: {R}eliable reinforcement learning
  implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (268), 2021.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International conference on machine learning}, pages
  1278--1286. PMLR, 2014.

\bibitem[Schmidhuber(1991)]{schmidhuber1991possibility}
J.~Schmidhuber.
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock In \emph{Proceedings of the International Conference on Simulation of
  Adaptive Behavior: From Animals to Animats (SAB)}, pages 222--227, 1991.

\bibitem[Schuster and Nakajima(2012)]{schuster2012japanese}
M.~Schuster and K.~Nakajima.
\newblock Japanese and korean voice search.
\newblock In \emph{Proceedings of the IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 5149--5152, 2012.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich15}
R.~Sennrich, B.~Haddow, and A.~Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock \emph{arXiv preprint arXiv:1508.07909}, 2015.

\bibitem[Shafiullah et~al.(2022)Shafiullah, Cui, Altanzaya, and
  Pinto]{shafiullah22}
N.~M. Shafiullah, Z.~Cui, A.~A. Altanzaya, and L.~Pinto.
\newblock Behavior transformers: {C}loning $k$ modes with one stone.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 22955--22968, New Orleans, LA, 2022.

\bibitem[Sharma et~al.(2019)Sharma, Gu, Levine, Kumar, and
  Hausman]{sharma2019dynamics}
A.~Sharma, S.~Gu, S.~Levine, V.~Kumar, and K.~Hausman.
\newblock Dynamics-aware unsupervised discovery of skills.
\newblock \emph{arXiv preprint arXiv:1907.01657}, 2019.

\bibitem[Sharma et~al.(2017)Sharma, Srinivas, and
  Ravindran]{sharma2017learning}
S.~Sharma, A.~Srinivas, and B.~Ravindran.
\newblock Learning to repeat: Fine grained action repetition for deep
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{silver2017mastering}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, Y.~Chen, T.~Lillicrap, F.~Hui,
  L.~Sifre, G.~van~den Driessche, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of {Go} without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Singh et~al.(2020)Singh, Liu, Zhou, Yu, Rhinehart, and
  Levine]{singh2020parrot}
A.~Singh, H.~Liu, G.~Zhou, A.~Yu, N.~Rhinehart, and S.~Levine.
\newblock Parrot: Data-driven behavioral priors for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2011.10024}, 2020.

\bibitem[Skalse et~al.(2022)Skalse, Howe, Krasheninnikov, and
  Krueger]{skalse2022defining}
J.~Skalse, N.~Howe, D.~Krasheninnikov, and D.~Krueger.
\newblock Defining and characterizing reward gaming.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9460--9471, 2022.

\bibitem[Stadie et~al.(2015)Stadie, Levine, and Abbeel]{stadie15}
B.~C. Stadie, S.~Levine, and P.~Abbeel.
\newblock Incentivizing exploration in reinforcement learning with deep
  predictive models.
\newblock \emph{arXiv preprint arXiv:1507.00814}, 2015.

\bibitem[Sutton(1995)]{sutton95}
R.~S. Sutton.
\newblock {TD} models: {M}odeling the world at a mixture of time scales.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 531--539. 1995.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton99}
R.~S. Sutton, D.~Precup, and S.~Singh.
\newblock Between {MDP}s and semi-{MDP}s: {A} framework for temporal
  abstraction in reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1--2):\penalty0
  181--211, August 1999.

\bibitem[Vezhnevets et~al.(2017)Vezhnevets, Osindero, Schaul, Heess, Jaderberg,
  Silver, and Kavukcuoglu]{vezhnevets17}
A.~S. Vezhnevets, S.~Osindero, T.~Schaul, N.~Heess, M.~Jaderberg, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Fe{U}dal networks for hierarchical reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 3540--3549, Sydney, Australia, Aug. 2017.

\bibitem[Warde-Farley et~al.(2018)Warde-Farley, Van~de Wiele, Kulkarni,
  Ionescu, Hansen, and Mnih]{warde-farley18}
D.~Warde-Farley, T.~Van~de Wiele, T.~Kulkarni, C.~Ionescu, S.~Hansen, and
  V.~Mnih.
\newblock Unsupervised control through non-parametric discriminative rewards.
\newblock \emph{arXiv preprint arXiv:1811.11359}, 2018.

\bibitem[Yarats et~al.(2021)Yarats, Fergus, Lazaric, and Pinto]{yarats21}
D.~Yarats, R.~Fergus, A.~Lazaric, and L.~Pinto.
\newblock Reinforcement learning with prototypical representations.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 11920--11931, July 2021.

\bibitem[Zheng et~al.(2024)Zheng, Cheng, Daum{\'e}~III, Huang, and
  Kolobov]{zheng2024prise}
R.~Zheng, C.-A. Cheng, H.~Daum{\'e}~III, F.~Huang, and A.~Kolobov.
\newblock Prise: Learning temporal action abstractions as a sequence
  compression problem.
\newblock \emph{arXiv preprint arXiv:2402.10450}, 2024.

\end{thebibliography}
