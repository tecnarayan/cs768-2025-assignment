\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{bottou2018optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{{SAIM} Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Darlow et~al.(2018)Darlow, Crowley, Antoniou, and Storkey]{darlow2018cinic10}
Darlow, L.~N., Crowley, E.~J., Antoniou, A., and Storkey, A.~J.
\newblock {CINIC}-10 is not imagenet or {CIFAR}-10, 2018.

\bibitem[Das et~al.(2022)Das, Acharya, Hashemi, Sanghavi, Dhillon, and Topcu]{das2022faster}
Das, R., Acharya, A., Hashemi, A., Sanghavi, S., Dhillon, I.~S., and Topcu, U.
\newblock Faster non-convex federated learning via global and local momentum.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  496--506. PMLR, 2022.

\bibitem[{\relax Haddadpour, Farzin} et~al.(2019)]{NEURIPS2019_c17028c9}
{\relax Haddadpour, Farzin} et~al.
\newblock Local {SGD} with periodic averaging: Tighter analysis and adaptive synchronization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Hubbard \& Hubbard(2015)Hubbard and Hubbard]{hubbard2015vector}
Hubbard, J.~H. and Hubbard, B.~B.
\newblock \emph{Vector calculus, linear algebra, and differential forms: a unified approach}.
\newblock Matrix Editions, 2015.

\bibitem[Jiang \& Agrawal(2018)Jiang and Agrawal]{Peng2018}
Jiang, P. and Agrawal, G.
\newblock A linear speedup analysis of distributed deep learning with sparse and quantized communication.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji, Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2021advances}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 14\penalty0 (1--2):\penalty0 1--210, 2021.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5132--5143. PMLR, 2020.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and Richt{\'a}rik]{khaled2020tighter}
Khaled, A., Mishchenko, K., and Richt{\'a}rik, P.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  4519--4529. PMLR, 2020.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{CIFAR10}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and Smith]{LiT_2019}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0 50--60, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith]{li2020federated}
Li, T., Sahu, A.~K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smith, V.
\newblock Federated optimization in heterogeneous networks, 2020{\natexlab{b}}.

\bibitem[Lin et~al.(2020)Lin, Stich, Patel, and Jaggi]{Lin2020Don't}
Lin, T., Stich, S.~U., Patel, K.~K., and Jaggi, M.
\newblock Don't use large mini-batches, use local {SGD}.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=B1eyO1BFPr}.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and y~Arcas]{McMahan2017a}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock {Communication-Efficient Learning of Deep Networks from Decentralized Data}.
\newblock In \emph{Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)}, pp.\  1273--1282, 2017.

\bibitem[Niknam et~al.(2020)Niknam, Dhillon, and Reed]{niknam2020federated}
Niknam, S., Dhillon, H.~S., and Reed, J.~H.
\newblock Federated learning for wireless communications: Motivation, opportunities, and challenges.
\newblock \emph{IEEE Communications Magazine}, 58\penalty0 (6):\penalty0 46--51, 2020.

\bibitem[Reddi et~al.(2020)Reddi, Charles, Zaheer, Garrett, Rush, Konečný, Kumar, and McMahan]{reddi2020adaptive}
Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Konečný, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization, 2020.

\bibitem[Richt{\'a}rik et~al.(2021)Richt{\'a}rik, Sokolov, and Fatkhullin]{richtarik2021ef21}
Richt{\'a}rik, P., Sokolov, I., and Fatkhullin, I.
\newblock Ef21: A new, simpler, theoretically better, and practically faster error feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 4384--4396, 2021.

\bibitem[Rieke et~al.(2020)Rieke, Hancox, Li, Milletari, Roth, Albarqouni, Bakas, Galtier, Landman, Maier-Hein, et~al.]{rieke2020future}
Rieke, N., Hancox, J., Li, W., Milletari, F., Roth, H.~R., Albarqouni, S., Bakas, S., Galtier, M.~N., Landman, B.~A., Maier-Hein, K., et~al.
\newblock The future of digital health with federated learning.
\newblock \emph{NPJ digital medicine}, 3\penalty0 (1):\penalty0 1--7, 2020.

\bibitem[Wang \& Joshi(2019)Wang and Joshi]{CooperativeSGD}
Wang, J. and Joshi, G.
\newblock Cooperative {SGD:} {A} unified framework for the design and analysis of communication-efficient {SGD} algorithms.
\newblock In \emph{ICML}, 2019.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous federated optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 7611--7623, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Tantia, Ballas, and Rabbat]{wangslowmo}
Wang, J., Tantia, V., Ballas, N., and Rabbat, M.
\newblock Slowmo: Improving communication-efficient distributed {SGD} with slow momentum.
\newblock In \emph{International Conference on Learning Representations}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Das, Joshi, Kale, Xu, and Zhang]{wang2022unreasonable}
Wang, J., Das, R., Joshi, G., Kale, S., Xu, Z., and Zhang, T.
\newblock On the unreasonable effectiveness of federated averaging with heterogeneous data, 2022.

\bibitem[Woodworth et~al.(2020{\natexlab{a}})Woodworth, Patel, Stich, Dai, Bullins, Mcmahan, Shamir, and Srebro]{pmlr-v119-woodworth20a}
Woodworth, B., Patel, K.~K., Stich, S., Dai, Z., Bullins, B., Mcmahan, B., Shamir, O., and Srebro, N.
\newblock Is local {SGD} better than minibatch {SGD}?
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, pp.\  10334--10343, 2020{\natexlab{a}}.

\bibitem[Woodworth et~al.(2020{\natexlab{b}})Woodworth, Patel, and Srebro]{woodworth2020minibatch}
Woodworth, B.~E., Patel, K.~K., and Srebro, N.
\newblock Minibatch vs local {SGD} for heterogeneous distributed learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 6281--6292, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2020)Yang, Fang, and Liu]{yang2020achieving}
Yang, H., Fang, M., and Liu, J.
\newblock Achieving linear speedup with partial worker participation in non-iid federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yu et~al.(2019{\natexlab{a}})Yu, Jin, and Yang]{yu2019linear}
Yu, H., Jin, R., and Yang, S.
\newblock On the linear speedup analysis of communication efficient momentum {SGD} for distributed non-convex optimization.
\newblock In \emph{ICML}, pp.\  7184--7193, Jun. 2019{\natexlab{a}}.

\bibitem[Yu et~al.(2019{\natexlab{b}})Yu, Yang, and Zhu]{YuAAAI2019}
Yu, H., Yang, S., and Zhu, S.
\newblock Parallel restarted {SGD} with faster convergence and less communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{AAAI}, Jan.--Feb. 2019{\natexlab{b}}.

\bibitem[Zhang \& Zhou(2014)Zhang and Zhou]{6471714}
Zhang, M.-L. and Zhou, Z.-H.
\newblock A review on multi-label learning algorithms.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 26\penalty0 (8):\penalty0 1819--1837, 2014.
\newblock \doi{10.1109/TKDE.2013.39}.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and Chandra]{zhao2018federated}
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}, 2018.

\end{thebibliography}
