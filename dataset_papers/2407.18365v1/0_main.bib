@inproceedings{
reddi2021adaptive,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle={International Conference on Learning Representations},
year={2021},
}


@inproceedings{guo2021hybrid,
  title={Hybrid Local {SGD} for Federated Learning with Heterogeneous Communications},
  author={Guo, Yuanxiong and Sun, Ying and Hu, Rui and Gong, Yanmin},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

@inproceedings{
acar2021federated,
title={Federated Learning Based on Dynamic Regularization},
author={Durmus Alp Emre Acar and Yue Zhao and Ramon Matas and Matthew Mattina and Paul Whatmough and Venkatesh Saligrama},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=B7v4QMR6Z9w}
}

@article{khanduri2021stem,
  title={Stem: A stochastic two-sided momentum algorithm achieving near-optimal sample and communication complexities for federated learning},
  author={Khanduri, Prashant and Sharma, Pranay and Yang, Haibo and Hong, Mingyi and Liu, Jia and Rajawat, Ketan and Varshney, Pramod},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6050--6061},
  year={2021}
}

@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex sgd},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{xu2021fedcm,
  title={Fedcm: Federated learning with client-level momentum},
  author={Xu, Jing and Wang, Sen and Wang, Liwei and Yao, Andrew Chi-Chih},
  journal={arXiv preprint arXiv:2106.10874},
  year={2021}
}

@inproceedings{gao2022feddc,
  title={FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction},
  author={Gao, Liang and Fu, Huazhu and Li, Li and Chen, Yingwen and Xu, Ming and Xu, Cheng-Zhong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10112--10121},
  year={2022}
}

@article{caldas2018leaf,
  title={Leaf: A benchmark for federated settings},
  author={Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1812.01097},
  year={2018}
}

@inproceedings{liu2020client,
  title={Client-edge-cloud hierarchical federated learning},
  author={Liu, Lumin and Zhang, Jun and Song, SH and Letaief, Khaled B},
  booktitle={ICC 2020-2020 IEEE International Conference on Communications (ICC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@inproceedings{abad2020hierarchical,
  title={Hierarchical federated learning across heterogeneous cellular networks},
  author={Abad, Mehdi Salehi Heydar and Ozfatura, Emre and Gunduz, Deniz and Ercetin, Ozgur},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8866--8870},
  year={2020},
  organization={IEEE}
}

@inproceedings{castiglia2020multi,
  title={Multi-level local SGD: Distributed {SGD} for heterogeneous hierarchical networks},
  author={Castiglia, Timothy and Das, Anirban and Patterson, Stacy},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{gu2021fast,
  title={Fast federated learning in the presence of arbitrary device unavailability},
  author={Gu, Xinran and Huang, Kaixuan and Zhang, Jingzhao and Huang, Longbo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12052--12064},
  year={2021}
}

@article{stripelis2022semi,
  title={Semi-synchronous federated learning for energy-efficient training and accelerated convergence in cross-silo settings},
  author={Stripelis, Dimitris and Thompson, Paul M and Ambite, Jos{\'e} Luis},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={13},
  number={5},
  pages={1--29},
  year={2022},
  publisher={ACM New York, NY}
}

@inproceedings{
bornstein2023swift,
title={{SWIFT}: Rapid Decentralized Federated Learning via Wait-Free Model Communication},
author={Marco Bornstein and Tahseen Rabbani and Evan Z Wang and Amrit Bedi and Furong Huang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=jh1nCir1R3d}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{yan2020distributed,
  title={Distributed non-convex optimization with sublinear speedup under intermittent client availability},
  author={Yan, Yikai and Niu, Chaoyue and Ding, Yucheng and Zheng, Zhenzhe and Wu, Fan and Chen, Guihai and Tang, Shaojie and Wu, Zhihua},
  journal={arXiv preprint arXiv:2002.07399},
  year={2020}
}

@article{JMLR:v22:20-147,
  author  = {Jianyu Wang and Gauri Joshi},
  title   = {Cooperative {SGD}: A Unified Framework for the Design and Analysis of Local-Update {SGD} Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {213},
  pages   = {1-50},
  url     = {http://jmlr.org/papers/v22/20-147.html}
}

@article{haddadpour2019local,
  title={Local {SGD} with periodic averaging: Tighter analysis and adaptive synchronization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@article{le2015tiny,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}

@InProceedings{pmlr-v202-panchal23a,
  title = 	 {Flash: Concept Drift Adaptation in Federated Learning},
  author =       {Panchal, Kunjal and Choudhary, Sunav and Mitra, Subrata and Mukherjee, Koyel and Sarkhel, Somdeb and Mitra, Saayan and Guan, Hui},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26931--26962},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
}

@article{novikova2017e2e,
  title={The E2E dataset: New challenges for end-to-end generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  journal={arXiv preprint arXiv:1706.09254},
  year={2017}
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@inproceedings{toghani2022unbounded,
  title={Unbounded Gradients in Federated Learning with Buffered Asynchronous Aggregation},
  author={Toghani, Mohammad Taha and Uribe, C{\'e}sar A},
  booktitle={2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}

@article{yuan2020hierarchical,
  title={Hierarchical federated learning through LAN-WAN orchestration},
  author={Yuan, Jinliang and Xu, Mengwei and Ma, Xiao and Zhou, Ao and Liu, Xuanzhe and Wang, Shangguang},
  journal={arXiv preprint arXiv:2010.11612},
  year={2020}
}

@article{boyd2006randomized,
  title={Randomized gossip algorithms},
  author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
  journal={IEEE transactions on information theory},
  volume={52},
  number={6},
  pages={2508--2530},
  year={2006},
  publisher={IEEE}
}

@inproceedings{tang2018d,
  title={$D^2$: Decentralized training over decentralized data},
  author={Tang, Hanlin and Lian, Xiangru and Yan, Ming and Zhang, Ce and Liu, Ji},
  booktitle={International Conference on Machine Learning},
  pages={4848--4856},
  year={2018},
  organization={PMLR}
}

@inproceedings{lu2021optimal,
  title={Optimal complexity in decentralized training},
  author={Lu, Yucheng and De Sa, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={7111--7123},
  year={2021},
  organization={PMLR}
}

@misc{
ying2022communicate,
title={Communicate Then Adapt: An Effective Decentralized Adaptive Method for Deep Training},
author={Bicheng Ying and Kun Yuan and Yiming Chen and Hanbin Hu and Yingya Zhang and Pan Pan and Wotao Yin},
year={2022},
url={https://openreview.net/forum?id=m716e-0clj}
}

@article{nazari2019dadam,
  title={Dadam: A consensus-based distributed adaptive gradient method for online optimization},
  author={Nazari, Parvin and Tarzanagh, Davoud Ataee and Michailidis, George},
  journal={arXiv preprint arXiv:1901.09109},
  year={2019}
}

@inproceedings{yu2019parallel,
  title={Parallel restarted {SGD} with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={5693--5700},
  year={2019}
}

@article{stich2019error,
  title={The error-feedback framework: Better rates for {SGD} with delayed gradients and compressed communication},
  author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
  journal={arXiv preprint arXiv:1909.05350},
  year={2019}
}

@article{tong2020effective,
  title={Effective federated adaptive gradient methods with non-iid decentralized data},
  author={Tong, Qianqian and Liang, Guannan and Bi, Jinbo},
  journal={arXiv preprint arXiv:2009.06557},
  year={2020}
}

@inproceedings{
anonymous2022on,
title={On Distributed Adaptive Optimization with Gradient Compression},
author={Anonymous},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=CI-xXX9dg9l},
note={under review}
}

@inproceedings{reisizadeh2020fedpaq,
  title={Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization},
  author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Jadbabaie, Ali and Pedarsani, Ramtin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2021--2031},
  year={2020},
  organization={PMLR}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@inproceedings{
yang2021achieving,
title={Achieving Linear Speedup with Partial Worker Participation in Non-{IID} Federated Learning},
author={Haibo Yang and Minghong Fang and Jia Liu},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
year={2015},
}

@article{chen2018convergence,
  title={On the convergence of a class of adam-type algorithms for non-convex optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  journal={arXiv preprint arXiv:1808.02941},
  year={2018}
}

@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}

@article{chen2021convergence,
  title={On the Convergence of Decentralized Adaptive Gradient Methods},
  author={Chen, Xiangyi and Karimi, Belhal and Zhao, Weijie and Li, Ping},
  journal={arXiv preprint arXiv:2109.03194},
  year={2021}
}

@article{trockman2022patches,
  title={Patches Are All You Need?},
  author={Trockman, Asher and Kolter, J Zico},
  journal={arXiv preprint arXiv:2201.09792},
  year={2022}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{chen2020closing,
  title={Closing the generalization gap of adaptive gradient methods in training deep neural networks},
  author={Chen, Jinghui and Zhou, Dongruo and Tang, Yiqi and Yang, Ziyan and Cao, Yuan and Gu, Quanquan},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2020}
}

@article{li2019convergence,
  title={On the convergence of {FedAvg} on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}

@article{stich2018sparsified,
  title={Sparsified {SGD} with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  journal={arXiv preprint arXiv:1809.07599},
  year={2018}
}

@article{stich2018local,
  title={Local {SGD} converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1805.09767},
  year={2018}
}

@article{lin2018don,
  title={Don't use large mini-batches, use local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@article{li2020federated,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}

@article{hsu2019measuring,
  title={Measuring the effects of non-identical data distribution for federated visual classification},
  author={Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal={arXiv preprint arXiv:1909.06335},
  year={2019}
}

@article{wang2019slowmo,
  title={SlowMo: Improving communication-efficient distributed {SGD} with slow momentum},
  author={Wang, Jianyu and Tantia, Vinayak and Ballas, Nicolas and Rabbat, Michael},
  journal={arXiv preprint arXiv:1910.00643},
  year={2019}
}

@article{basu2019qsparse,
  title={Qsparse-local-SGD: Distributed {SGD} with quantization, sparsification, and local computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  journal={arXiv preprint arXiv:1906.02367},
  year={2019}
}

@article{alistarh2017qsgd,
  title={QSGD: Communication-efficient {SGD} via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={1709--1720},
  year={2017}
}

@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth annual conference of the international speech communication association},
  year={2014},
  organization={Citeseer}
}

@inproceedings{chen2021cada,
  title={Cada: Communication-adaptive distributed adam},
  author={Chen, Tianyi and Guo, Ziye and Sun, Yuejiao and Yin, Wotao},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={613--621},
  year={2021},
  organization={PMLR}
}

@article{tang20211,
  title={1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed},
  author={Tang, Hanlin and Gan, Shaoduo and Awan, Ammar Ahmad and Rajbhandari, Samyam and Li, Conglong and Lian, Xiangru and Liu, Ji and Zhang, Ce and He, Yuxiong},
  journal={arXiv preprint arXiv:2102.02888},
  year={2021}
}

@inproceedings{wang2022cdadam,
  title={Communication-Compressed Adaptive Gradient Method for Distributed Nonconvex Optimization},
  author={Wang, Yujia and Lin, Lu and Chen, Jinghui},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={6292--6320},
  year={2022},
  organization={PMLR}
}


@InProceedings{wang2022communication,
  title = 	 {Communication-Efficient Adaptive Federated Learning},
  author =       {Wang, Yujia and Lin, Lu and Chen, Jinghui},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {22802--22838},
  year = 	 {2022},
  publisher =    {PMLR},
}

@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@article{karimireddy2020mime,
  title={Mime: Mimicking centralized stochastic algorithms in federated learning},
  author={Karimireddy, Sai Praneeth and Jaggi, Martin and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:2008.03606},
  year={2020}
}

@article{xiao2017fashion,
  title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

@article{erdos59a,
  added-at = {2010-05-05T00:38:27.000+0200},
  author = {Erd\"{o}s, P. and R\'{e}nyi, A.},
  biburl = {https://www.bibsonomy.org/bibtex/25aab47a7be9ec47644735f8e0a4607b6/alex},
  interhash = {99061fc859ba540d4485abfbce44f298},
  intrahash = {5aab47a7be9ec47644735f8e0a4607b6},
  journal = {Publicationes Mathematicae Debrecen},
  keywords = {graph sna},
  pages = 290,
  timestamp = {2010-05-05T00:38:27.000+0200},
  title = {On Random Graphs I},
  volume = 6,
  year = 1959
}

@inproceedings{yu2019linear,
  title={On the linear speedup analysis of communication efficient momentum {SGD} for distributed non-convex optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  booktitle={International Conference on Machine Learning},
  pages={7184--7193},
  year={2019},
  organization={PMLR}
}

@article{gilbert1959random,
  title={Random graphs},
  author={Gilbert, Edgar N},
  journal={The Annals of Mathematical Statistics},
  volume={30},
  number={4},
  pages={1141--1144},
  year={1959},
  publisher={JSTOR}
}

@techreport{tsitsiklis1984problems,
  title={Problems in decentralized decision making and computation.},
  author={Tsitsiklis, John Nikolas},
  year={1984},
  institution={Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems}
}

@inproceedings{haddadpour2021federated,
  title={Federated learning with compression: Unified analysis and sharp guarantees},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2350--2358},
  year={2021},
  organization={PMLR}
}

@article{chen2021communication,
  title={Communication-efficient federated learning},
  author={Chen, Mingzhe and Shlezinger, Nir and Poor, H Vincent and Eldar, Yonina C and Cui, Shuguang},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={17},
  year={2021},
  publisher={National Acad Sciences}
}

@article{jin2020stochastic,
  title={Stochastic-sign {SGD} for federated learning with theoretical guarantees},
  author={Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu},
  journal={arXiv preprint arXiv:2002.10940},
  year={2020}
}

@inproceedings{jhunjhunwala2021adaptive,
  title={Adaptive quantization of model updates for communication-efficient federated learning},
  author={Jhunjhunwala, Divyansh and Gadhikar, Advait and Joshi, Gauri and Eldar, Yonina C},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3110--3114},
  year={2021},
  organization={IEEE}
}

@inproceedings{chen2020toward,
  title={Toward communication efficient adaptive gradient method},
  author={Chen, Xiangyi and Li, Xiaoyun and Li, Ping},
  booktitle={Proceedings of the 2020 ACM-IMS on Foundations of Data Science Conference},
  pages={119--128},
  year={2020}
}

@article{wang2020tackling,
  title={Tackling the objective inconsistency problem in heterogeneous federated optimization},
  author={Wang, Jianyu and Liu, Qinghua and Liang, Hao and Joshi, Gauri and Poor, H Vincent},
  journal={arXiv preprint arXiv:2007.07481},
  year={2020}
}

@article{gorbunov2021distributed,
  title={Distributed and Stochastic Optimization Methods with Gradient Compression and Local Steps},
  author={Gorbunov, Eduard},
  journal={arXiv preprint arXiv:2112.10645},
  year={2021}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey and others},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@article{luo2019adaptive,
  title={Adaptive gradient methods with dynamic bound of learning rate},
  author={Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  journal={arXiv preprint arXiv:1902.09843},
  year={2019}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{li2019fair,
  title={Fair resource allocation in federated learning},
  author={Li, Tian and Sanjabi, Maziar and Beirami, Ahmad and Smith, Virginia},
  journal={arXiv preprint arXiv:1905.10497},
  year={2019}
}

@article{yang2020energy,
  title={Energy efficient federated learning over wireless communication networks},
  author={Yang, Zhaohui and Chen, Mingzhe and Saad, Walid and Hong, Choong Seon and Shikh-Bahaei, Mohammad},
  journal={IEEE Transactions on Wireless Communications},
  volume={20},
  number={3},
  pages={1935--1949},
  year={2020},
  publisher={IEEE}
}

@inproceedings{
j.2018on,
title={On the Convergence of {Adam} and Beyond},
author={Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2018},
}

@article{zhang2023towards,
  title={Towards Building the Federated GPT: Federated Instruction Tuning},
  author={Zhang, Jianyi and Vahidian, Saeed and Kuo, Martin and Li, Chunyuan and Zhang, Ruiyi and Wang, Guoyin and Chen, Yiran},
  journal={arXiv preprint arXiv:2305.05644},
  year={2023}
}
@article{li2023fedda,
  title={FedDA: Faster Framework of Local Adaptive Gradient Methods via Restarted Dual Averaging},
  author={Li, Junyi and Huang, Feihu and Huang, Heng},
  journal={arXiv preprint arXiv:2302.06103},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{ghosh2019robust,
  title={Robust federated learning in a heterogeneous environment},
  author={Ghosh, Avishek and Hong, Justin and Yin, Dong and Ramchandran, Kannan},
  journal={arXiv preprint arXiv:1906.06629},
  year={2019}
}

@inproceedings{nishio2019client,
  title={Client selection for federated learning with heterogeneous resources in mobile edge},
  author={Nishio, Takayuki and Yonetani, Ryo},
  booktitle={ICC 2019-2019 IEEE International Conference on Communications (ICC)},
  pages={1--7},
  year={2019},
  organization={IEEE}
}

@article{li2019fedmd,
  title={Fedmd: Heterogenous federated learning via model distillation},
  author={Li, Daliang and Wang, Junpu},
  journal={arXiv preprint arXiv:1910.03581},
  year={2019}
}

@inproceedings{
Wang2020Federated,
title={Federated Learning with Matched Averaging},
author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BkluqlSFDS}
}

@article{li2020fedprox,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}

@inproceedings{chen2021accelerating,
  title={Accelerating gossip {SGD} with periodic global averaging},
  author={Chen, Yiming and Yuan, Kun and Zhang, Yingya and Pan, Pan and Xu, Yinghui and Yin, Wotao},
  booktitle={International Conference on Machine Learning},
  pages={1791--1802},
  year={2021},
  organization={PMLR}
}

@article{lian2017can,
  title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{li2019communication,
  title={Communication-efficient local decentralized {SGD} methods},
  author={Li, Xiang and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1910.09126},
  year={2019}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized {SGD} with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}


@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{teng2019leader,
  title={Leader stochastic gradient descent for distributed training of deep learning models},
  author={Teng, Yunfei and Gao, Wenbo and Chalus, Francois and Choromanska, Anna E and Goldfarb, Donald and Weller, Adrian},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{malinovsky2022variance,
  title={Variance reduced proxskip: Algorithm, theory and application to federated learning},
  author={Malinovsky, Grigory and Yi, Kai and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2207.04338},
  year={2022}
}

@article{long2022multi,
  title={Multi-center federated learning: clients clustering for better personalization},
  author={Long, Guodong and Xie, Ming and Shen, Tao and Zhou, Tianyi and Wang, Xianzhi and Jiang, Jing},
  journal={World Wide Web},
  pages={1--20},
  year={2022},
  publisher={Springer}
}


@article{long2022multi,
  title={Multi-center federated learning: clients clustering for better personalization},
  author={Long, Guodong and Xie, Ming and Shen, Tao and Zhou, Tianyi and Wang, Xianzhi and Jiang, Jing},
  journal={World Wide Web},
  pages={1--20},
  year={2022},
  publisher={Springer}
}

@inproceedings{
koloskova2022sharper,
title={Sharper Convergence Guarantees for Asynchronous {SGD} for Distributed and Federated Learning},
author={Anastasia Koloskova and Sebastian U Stich and Martin Jaggi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=4_oCZgBIVI}
}


@inproceedings{yang2022anarchic,
  title={Anarchic federated learning},
  author={Yang, Haibo and Zhang, Xin and Khanduri, Prashant and Liu, Jia},
  booktitle={International Conference on Machine Learning},
  pages={25331--25363},
  year={2022},
  organization={PMLR}
}

@inproceedings{nguyen2022federated,
  title={Federated learning with buffered asynchronous aggregation},
  author={Nguyen, John and Malik, Kshitiz and Zhan, Hongyuan and Yousefpour, Ashkan and Rabbat, Mike and Malek, Mani and Huba, Dzmitry},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3581--3607},
  year={2022},
  organization={PMLR}
}


@InProceedings{pmlr-v139-avdiukhin21a,
  title = 	 {Federated Learning under Arbitrary Communication Patterns},
  author =       {Avdiukhin, Dmitrii and Kasiviswanathan, Shiva},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {425--435},
  year = 	 {2021}
}

@article{mishchenko2022asynchronous,
  title={Asynchronous {SGD} beats minibatch {SGD} under arbitrary delays},
  author={Mishchenko, Konstantin and Bach, Francis and Even, Mathieu and Woodworth, Blake E},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={420--433},
  year={2022}
}

@article{xie2019asynchronous,
  title={Asynchronous federated optimization},
  author={Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  journal={arXiv preprint arXiv:1903.03934},
  year={2019}
}

@inproceedings{
wang2022accelerating,
title={Accelerating Adaptive Federated Optimization with Local Gossip Communications},
author={Yujia Wang and Pei Fang and Jinghui Chen},
booktitle={Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)},
year={2022},
url={https://openreview.net/forum?id=wwXb1qmcBuD}
}

@article{zhu2019deep,
  title={Deep leakage from gradients},
  author={Zhu, Ligeng and Liu, Zhijian and Han, Song},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{fredrikson2015model,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1322--1333},
  year={2015}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}


@InProceedings{pmlr-v162-zhang22b,
  title = 	 {Understanding Clipping for Federated Learning: Convergence and Client-Level Differential Privacy},
  author =       {Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Steven and Yi, Jinfeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {26048--26067},
  year = 	 {2022}
}

@inproceedings{lam2021gradient,
  title={Gradient disaggregation: Breaking privacy in federated learning by reconstructing the user participant matrix},
  author={Lam, Maximilian and Wei, Gu-Yeon and Brooks, David and Reddi, Vijay Janapa and Mitzenmacher, Michael},
  booktitle={International Conference on Machine Learning},
  pages={5959--5968},
  year={2021},
  organization={PMLR}
}

@inproceedings{chen2022fundamental,
  title={The fundamental price of secure aggregation in differentially private federated learning},
  author={Chen, Wei-Ning and Choo, Christopher A Choquette and Kairouz, Peter and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={3056--3089},
  year={2022},
  organization={PMLR}
}

@article{zizzo2020fat,
  title={Fat: Federated adversarial training},
  author={Zizzo, Giulio and Rawat, Ambrish and Sinn, Mathieu and Buesser, Beat},
  journal={arXiv preprint arXiv:2012.01791},
  year={2020}
}

@article{hong2021federated,
  title={Federated robustness propagation: Sharing adversarial robustness in federated learning},
  author={Hong, Junyuan and Wang, Haotao and Wang, Zhangyang and Zhou, Jiayu},
  journal={arXiv preprint arXiv:2106.10196},
  volume={1},
  year={2021}
}

@article{chen2022calfat,
  title={CalFAT: Calibrated federated adversarial training with label skewness},
  author={Chen, Chen and Liu, Yuchen and Ma, Xingjun and Lyu, Lingjuan},
  journal={arXiv preprint arXiv:2205.14926},
  year={2022}
}

@inproceedings{wang2022unified,
 author = {Wang, Shiqiang and Ji, Mingyue},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {19124-19137},
 title = {A Unified Analysis of Federated Learning with Arbitrary Client Participation},
 volume = {35},
 year = {2022}
}

@inproceedings{10.5555/2986459.2986537,
author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
title = {HOGWILD! A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},
year = {2011},
isbn = {9781618395993},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that {SGD} can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
booktitle = {Proceedings of the 24th International Conference on Neural Information Processing Systems},
pages = {693–701},
numpages = {9},
location = {Granada, Spain},
series = {NIPS'11}
}

@InProceedings{pmlr-v80-nguyen18c,
  title = 	 {{SGD} and Hogwild! {C}onvergence Without the Bounded Gradients Assumption},
  author =       {Nguyen, Lam and Nguyen, Phuong Ha and van Dijk, Marten and Richtarik, Peter and Scheinberg, Katya and Takac, Martin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3750--3758},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/nguyen18c/nguyen18c.pdf},
  url = 	 {https://proceedings.mlr.press/v80/nguyen18c.html},
  abstract = 	 {Stochastic gradient descent (SGD) is the optimization algorithm of choice in many machine learning applications such as regularized empirical risk minimization and training deep neural networks. The classical convergence analysis of {SGD} is carried out under the assumption that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for cases where the objective function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of {SGD} is performed under the assumption that stochastic gradients are bounded with respect to the true gradient norm. Here we show that for stochastic problems arising in machine learning such bound always holds; and we also propose an alternative convergence analysis of {SGD} with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous parallel setting, and prove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.}
}

@article{mania2017perturbed,
  title={Perturbed iterate analysis for asynchronous stochastic optimization},
  author={Mania, Horia and Pan, Xinghao and Papailiopoulos, Dimitris and Recht, Benjamin and Ramchandran, Kannan and Jordan, Michael I},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={4},
  pages={2202--2229},
  year={2017},
  publisher={SIAM}
}

@inproceedings{stich2021critical,
  title={Critical parameters for scalable distributed learning with large batches and asynchronous updates},
  author={Stich, Sebastian and Mohtashami, Amirkeivan and Jaggi, Martin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4042--4050},
  year={2021},
  organization={PMLR}
}

@article{JMLRasync,
  author  = {Remi Leblond and Fabian Pedregosa and Simon Lacoste-Julien},
  title   = {Improved Asynchronous Parallel Optimization Analysis for Stochastic Incremental Methods},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {19},
  number  = {81},
  pages   = {1--68},
}

@article{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{glasgow2022asynchronous,
  title={Asynchronous Distributed Optimization with Stochastic Delays},
  author={Glasgow, Margalit R and Wootters, Mary},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={9247--9279},
  year={2022},
  organization={PMLR}
}
@article{ortega2023asynchronous,
  title={Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation},
  author={Ortega, Tomas and Jafarkhani, Hamid},
  journal={arXiv preprint arXiv:2308.00263},
  year={2023}
}

@article{wang2023lightweight,
  title={A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging},
  author={Wang, Shiqiang and Ji, Mingyue},
  journal={arXiv preprint arXiv:2306.03401},
  year={2023}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{jhunjhunwala2023fedexp,
  title={{FedExP}: Speeding up Federated Averaging Via Extrapolation},
  author={Jhunjhunwala, Divyansh and Wang, Shiqiang and Joshi, Gauri},
  journal={arXiv preprint arXiv:2301.09604},
  year={2023}
}
@article{tan2022adafed,
  title={AdaFed: Optimizing Participation-Aware Federated Learning With Adaptive Aggregation Weights},
  author={Tan, Lei and Zhang, Xiaoxi and Zhou, Yipeng and Che, Xinkai and Hu, Miao and Chen, Xu and Wu, Di},
  journal={IEEE Transactions on Network Science and Engineering},
  volume={9},
  number={4},
  pages={2708--2720},
  year={2022},
  publisher={IEEE}
}
@article{sun2023fedlalr,
  title={FedLALR: Client-Specific Adaptive Learning Rates Achieve Linear Speedup for Non-IID Data},
  author={Sun, Hao and Shen, Li and Chen, Shixiang and Sun, Jingwei and Li, Jing and Sun, Guangzhong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2309.09719},
  year={2023}
}
@inproceedings{jin2022accelerated,
  title={Accelerated federated learning with decoupled adaptive optimization},
  author={Jin, Jiayin and Ren, Jiaxiang and Zhou, Yang and Lyu, Lingjuan and Liu, Ji and Dou, Dejing},
  booktitle={International Conference on Machine Learning},
  pages={10298--10322},
  year={2022},
  organization={PMLR}
}
@inproceedings{wu2023faster,
  title={Faster adaptive federated learning},
  author={Wu, Xidong and Huang, Feihu and Hu, Zhengmian and Huang, Heng},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={9},
  pages={10379--10387},
  year={2023}
}
@article{sun2023efficient,
  title={Efficient federated learning via local adaptive amended optimizer with linear speedup},
  author={Sun, Yan and Shen, Li and Sun, Hao and Ding, Liang and Tao, Dacheng},
  journal={arXiv preprint arXiv:2308.00522},
  year={2023}
}
@article{wang2021cooperative,
  title={Cooperative SGD: A unified framework for the design and analysis of local-update {SGD} algorithms},
  author={Wang, Jianyu and Joshi, Gauri},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={9709--9758},
  year={2021},
  publisher={JMLRORG}
}
@article{chen2020vafl,
  title={Vafl: a method of vertical asynchronous federated learning},
  author={Chen, Tianyi and Jin, Xiao and Sun, Yuejiao and Yin, Wotao},
  journal={arXiv preprint arXiv:2007.06081},
  year={2020}
}
@inproceedings{wang2023tackling,
  title={Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration},
  author={Wang, Yujia and Cao, Yuanpu and Wu, Jingcheng and Chen, Ruoyu and Chen, Jinghui},
  booktitle={Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities},
  year={2023}
}
@inproceedings{wu2022delay,
  title={Delay-adaptive step-sizes for asynchronous learning},
  author={Wu, Xuyang and Magnusson, Sindri and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  booktitle={International Conference on Machine Learning},
  pages={24093--24113},
  year={2022},
  organization={PMLR}
}
@inproceedings{jhunjhunwala2022fedvarp,
  title={Fedvarp: Tackling the variance due to partial client participation in federated learning},
  author={Jhunjhunwala, Divyansh and Sharma, Pranay and Nagarkatti, Aushim and Joshi, Gauri},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={906--916},
  year={2022},
  organization={PMLR}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}