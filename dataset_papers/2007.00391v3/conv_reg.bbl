\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2015)Abernethy, Lee, and
  Tewari]{abernethy2015fighting}
Abernethy, J., Lee, C., and Tewari, A.
\newblock Fighting bandits with a new kind of smoothness.
\newblock \emph{arXiv preprint arXiv:1512.04152}, 2015.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Auer, P., Cesa-Bianchi, N., and Fischer, P.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2-3):\penalty0 235--256, 2002.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  449--458. JMLR. org, 2017.

\bibitem[Bellman(1954)]{bellman1954theory}
Bellman, R.
\newblock The theory of dynamic programming.
\newblock Technical report, Rand corp santa monica ca, 1954.

\bibitem[Buesing et~al.(2020)Buesing, Heess, and Weber]{buesing2020approximate}
Buesing, L., Heess, N., and Weber, T.
\newblock Approximate inference in discrete distributions with monte carlo tree
  search and value functions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  624--634. PMLR, 2020.

\bibitem[Chaslot et~al.(2008)Chaslot, Winands, Herik, Uiterwijk, and
  Bouzy]{chaslot2008progressive}
Chaslot, G., Winands, M., Herik, J. V.~D., Uiterwijk, J., and Bouzy, B.
\newblock Progressive strategies for monte-carlo tree search.
\newblock \emph{New Mathematics and Natural Computation}, 4\penalty0
  (03):\penalty0 343--357, 2008.

\bibitem[Childs et~al.(2008)Childs, Brodeur, and
  Kocsis]{childs2008transpositions}
Childs, B.~E., Brodeur, J.~H., and Kocsis, L.
\newblock Transpositions and move groups in monte carlo tree search.
\newblock In \emph{2008 IEEE Symposium On Computational Intelligence and
  Games}. IEEE, 2008.

\bibitem[Coquelin \& Munos(2007)Coquelin and Munos]{coquelin2007bandit}
Coquelin, P.-A. and Munos, R.
\newblock Bandit algorithms for tree search.
\newblock \emph{arXiv preprint cs/0703062}, 2007.

\bibitem[Coulom(2006)]{coulom2006efficient}
Coulom, R.
\newblock Efficient selectivity and backup operators in monte-carlo tree
  search.
\newblock In \emph{International conference on computers and games}, pp.\
  72--83. Springer, 2006.

\bibitem[Geist \& Scherrer(2011)Geist and Scherrer]{geist:l1pbr}
Geist, M. and Scherrer, B.
\newblock L1-penalized projected bellman residual.
\newblock In \emph{Proceedings of the European Workshop on Reinforcement
  Learning (EWRL 2011)}, Lecture Notes in Computer Science (LNCS). Springer
  Verlag - Heidelberg Berlin, september 2011.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2160--2169, 2019.

\bibitem[Gelly \& Silver(2007)Gelly and Silver]{gelly2007combining}
Gelly, S. and Silver, D.
\newblock Combining online and offline knowledge in uct.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  273--280. ACM, 2007.

\bibitem[Gelly \& Wang(2006)Gelly and Wang]{gelly2006exploration}
Gelly, S. and Wang, Y.
\newblock Exploration exploitation in go: Uct for monte-carlo go.
\newblock In \emph{NIPS: Neural Information Processing Systems Conference
  On-line trading of Exploration and Exploitation Workshop}, 2006.

\bibitem[Grill et~al.(2020)Grill, Altch{\'e}, Tang, Hubert, Valko, Antonoglou,
  and Munos]{grill2020monte}
Grill, J.-B., Altch{\'e}, F., Tang, Y., Hubert, T., Valko, M., Antonoglou, I.,
  and Munos, R.
\newblock Monte-carlo tree search as regularized policy optimization.
\newblock \emph{arXiv preprint arXiv:2007.12509}, 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870, 2018.

\bibitem[Helmbold \& Parker-Wood(2009)Helmbold and
  Parker-Wood]{helmbold2009all}
Helmbold, D.~P. and Parker-Wood, A.
\newblock All-moves-as-first heuristics in monte-carlo go.
\newblock In \emph{IC-AI}, pp.\  605--610, 2009.

\bibitem[Hoock et~al.(2010)Hoock, Lee, Rimmel, Teytaud, Wang, and
  Teytaud]{hoock2010intelligent}
Hoock, J.-B., Lee, C.-S., Rimmel, A., Teytaud, F., Wang, M.-H., and Teytaud, O.
\newblock Intelligent agents for the game of go.
\newblock \emph{IEEE Computational Intelligence Magazine}, 2010.

\bibitem[Khandelwal et~al.(2016)Khandelwal, Liebman, Niekum, and
  Stone]{khandelwal2016analysis}
Khandelwal, P., Liebman, E., Niekum, S., and Stone, P.
\newblock On the analysis of complex backup strategies in monte carlo tree
  search.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Kocsis et~al.(2006)Kocsis, Szepesv{\'a}ri, and
  Willemson]{kocsis2006improved}
Kocsis, L., Szepesv{\'a}ri, C., and Willemson, J.
\newblock Improved monte-carlo search, 2006.

\bibitem[Kozelek(2009)]{kozelek2009methods}
Kozelek, T.
\newblock Methods of mcts and the game arimaa, 2009.

\bibitem[Lee et~al.(2018)Lee, Choi, and Oh]{lee2018sparse}
Lee, K., Choi, S., and Oh, S.
\newblock Sparse markov decision processes with causal sparse tsallis entropy
  regularization for reinforcement learning.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (3):\penalty0
  1466--1473, 2018.

\bibitem[Lorentz(2010)]{lorentz2010improving}
Lorentz, R.~J.
\newblock Improving monte--carlo tree search in havannah.
\newblock In \emph{International Conference on Computers and Games}, pp.\
  105--115. Springer, 2010.

\bibitem[Mei et~al.(2019)Mei, Xiao, Huang, Schuurmans, and
  M{\"u}ller]{mei2019principled}
Mei, J., Xiao, C., Huang, R., Schuurmans, D., and M{\"u}ller, M.
\newblock On principled entropy exploration in policy optimization.
\newblock In \emph{Proceedings of the 28th International Joint Conference on
  Artificial Intelligence}, pp.\  3130--3136. AAAI Press, 2019.

\bibitem[Mensch \& Blondel(2018)Mensch and Blondel]{mensch2018differentiable}
Mensch, A. and Blondel, M.
\newblock Differentiable dynamic programming for structured prediction and
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3462--3471, 2018.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937, 2016.

\bibitem[Montgomery \& Levine(2016)Montgomery and Levine]{montgomery2016guided}
Montgomery, W.~H. and Levine, S.
\newblock Guided policy search via approximate mirror descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4008--4016, 2016.

\bibitem[Nachum \& Dai(2020{\natexlab{a}})Nachum and Dai]{nachum2020duality}
Nachum, O. and Dai, B.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock \emph{CoRR}, abs/2001.01866, 2020{\natexlab{a}}.

\bibitem[Nachum \& Dai(2020{\natexlab{b}})Nachum and
  Dai]{nachum2020reinforcement}
Nachum, O. and Dai, B.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock \emph{arXiv preprint arXiv:2001.01866}, 2020{\natexlab{b}}.

\bibitem[Niculae \& Blondel(2017)Niculae and Blondel]{niculae2017regularized}
Niculae, V. and Blondel, M.
\newblock A regularized framework for sparse and structured neural attention.
\newblock \emph{arXiv preprint arXiv:1705.07704}, 2017.

\bibitem[Pavel(2007)]{pavel2007duality}
Pavel, L.
\newblock An extension of duality to a game-theoretic framework.
\newblock \emph{Automatica}, 43\penalty0 (2):\penalty0 226 -- 237, 2007.

\bibitem[Rummery(1995)]{rummery1995problem}
Rummery, G.~A.
\newblock \emph{Problem solving with reinforcement learning}.
\newblock PhD thesis, University of Cambridge Ph. D. dissertation, 1995.

\bibitem[Schrittwieser et~al.(2019)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and
  Silver]{schrittwieser2019mastering}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,
  Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap,
  T., and Silver, D.
\newblock Mastering atari, go, chess and shogi by planning with a learned
  model, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017{\natexlab{a}})Schulman, Chen, and
  Abbeel]{schulman2017equivalence}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017{\natexlab{a}}.

\bibitem[Schulman et~al.(2017{\natexlab{b}})Schulman, Wolski, Dhariwal,
  Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017{\natexlab{b}}.

\bibitem[Shalev-Shwartz \& Singer(2006)Shalev-Shwartz and
  Singer]{shalev2006convex}
Shalev-Shwartz, S. and Singer, Y.
\newblock Convex repeated games and fenchel duality.
\newblock \emph{Advances in neural information processing systems},
  19:\penalty0 1265--1272, 2006.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Silver et~al.(2017{\natexlab{a}})Silver, Hubert, Schrittwieser,
  Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel,
  et~al.]{silver2017bmastering}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017{\natexlab{a}}.

\bibitem[Silver et~al.(2017{\natexlab{b}})Silver, Schrittwieser, Simonyan,
  Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton,
  et~al.]{silver2017amastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359,
  2017{\natexlab{b}}.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998introduction}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Introduction to reinforcement learning}, volume 135.
\newblock MIT press Cambridge, 1998.

\bibitem[Tesauro et~al.(2012)Tesauro, Rajan, and Segal]{tesauro2012bayesian}
Tesauro, G., Rajan, V., and Segal, R.
\newblock Bayesian inference in monte-carlo tree search.
\newblock \emph{arXiv preprint arXiv:1203.3519}, 2012.

\bibitem[Teytaud \& Teytaud(2010)Teytaud and Teytaud]{teytaud2010huge}
Teytaud, F. and Teytaud, O.
\newblock On the huge benefit of decisive moves in monte-carlo tree search
  algorithms.
\newblock In \emph{Proceedings of the 2010 IEEE Conference on Computational
  Intelligence and Games}, pp.\  359--364. IEEE, 2010.

\bibitem[Tom(2010)]{tom2010investigating}
Tom, D.
\newblock Investigating uct and rave: Steps towards a more robust method, 2010.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Thirtieth AAAI conference on artificial intelligence}, 2016.

\bibitem[Vodopivec et~al.(2017)Vodopivec, Samothrakis, and
  Ster]{vodopivec2017monte}
Vodopivec, T., Samothrakis, S., and Ster, B.
\newblock On monte carlo tree search and reinforcement learning.
\newblock \emph{Journal of Artificial Intelligence Research}, 60:\penalty0
  881--936, 2017.

\bibitem[Wainwright(2019)]{wainwright2019high}
Wainwright, M.~J.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Xiao et~al.(2019)Xiao, Huang, Mei, Schuurmans, and
  M{\"u}ller]{xiao2019maximum}
Xiao, C., Huang, R., Mei, J., Schuurmans, D., and M{\"u}ller, M.
\newblock Maximum entropy monte-carlo planning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9516--9524, 2019.

\bibitem[Yee et~al.(2016)Yee, Lis{\`y}, Bowling, and Kambhampati]{yee2016monte}
Yee, T., Lis{\`y}, V., Bowling, M.~H., and Kambhampati, S.
\newblock Monte carlo tree search in continuous action spaces with execution
  uncertainty.
\newblock In \emph{IJCAI}, pp.\  690--697, 2016.

\bibitem[Zimmert \& Seldin(2019)Zimmert and Seldin]{zimmert2019optimal}
Zimmert, J. and Seldin, Y.
\newblock An optimal algorithm for stochastic and adversarial bandits.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  467--475. PMLR, 2019.

\end{thebibliography}
