\begin{thebibliography}{10}

\bibitem{artzner1999coherent}
Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath.
\newblock Coherent measures of risk.
\newblock {\em Mathematical finance}, 9(3):203--228, 1999.

\bibitem{blanchet2019unbiased}
Jose~H Blanchet, Peter~W Glynn, and Yanan Pei.
\newblock Unbiased multilevel monte carlo: Stochastic optimization,
  steady-state simulation, quantiles, and other applications.
\newblock {\em arXiv preprint arXiv:1904.09929}, 2019.

\bibitem{castro2007using}
Pablo~Samuel Castro and Doina Precup.
\newblock Using linear programming for bayesian exploration in markov decision
  processes.
\newblock In {\em IJCAI}, volume 24372442, 2007.

\bibitem{dearden2013model}
Richard Dearden, Nir Friedman, and David Andre.
\newblock Model-based bayesian exploration.
\newblock {\em arXiv preprint arXiv:1301.6690}, 2013.

\bibitem{delage2010percentile}
Erick Delage and Shie Mannor.
\newblock Percentile optimization for markov decision processes with parameter
  uncertainty.
\newblock {\em Operations research}, 58(1):203--213, 2010.

\bibitem{doob1949application}
Joseph~L Doob.
\newblock Application of the theory of martingales.
\newblock {\em Le calcul des probabilites et ses applications}, pages 23--27,
  1949.

\bibitem{duff2001monte}
Michael~O Duff.
\newblock Monte-carlo algorithms for the improvement of finite-state stochastic
  controllers: Application to bayes-adaptive markov decision processes.
\newblock In {\em International Workshop on Artificial Intelligence and
  Statistics}, pages 93--97. PMLR, 2001.

\bibitem{duff2002optimal}
Michael~O'Gordon Duff.
\newblock {\em Optimal Learning: Computational procedures for Bayes-adaptive
  Markov decision processes}.
\newblock University of Massachusetts Amherst, 2002.

\bibitem{engel2003bayes}
Yaakov Engel, Shie Mannor, and Ron Meir.
\newblock Bayes meets bellman: The gaussian process approach to temporal
  difference learning.
\newblock In {\em Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pages 154--161, 2003.

\bibitem{garcia2015comprehensive}
Javier Garc{\i}a and Fernando Fern{\'a}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 16(1):1437--1480, 2015.

\bibitem{ghavamzadeh2006bayesian}
Mohammad Ghavamzadeh and Yaakov Engel.
\newblock Bayesian policy gradient algorithms.
\newblock {\em Advances in neural information processing systems}, 19, 2006.

\bibitem{ghavamzadeh2015bayesian}
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et~al.
\newblock Bayesian reinforcement learning: A survey.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(5-6):359--483, 2015.

\bibitem{lin2022bayesian}
Yifan Lin, Yuxuan Ren, and Enlu Zhou.
\newblock Bayesian risk markov decision processes.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{liu2022distributionally}
Zijian Liu, Qinxun Bai, Jose Blanchet, Perry Dong, Wei Xu, Zhengqing Zhou, and
  Zhengyuan Zhou.
\newblock Distributionally robust $ q $-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  13623--13643. PMLR, 2022.

\bibitem{neufeld2022robust}
Ariel Neufeld and Julian Sester.
\newblock Robust $ q $-learning algorithm for markov decision processes under
  wasserstein uncertainty.
\newblock {\em arXiv preprint arXiv:2210.00898}, 2022.

\bibitem{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock {\em Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem{panaganti2022sample}
Kishan Panaganti and Dileep Kalathil.
\newblock Sample complexity of robust reinforcement learning with a generative
  model.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 9582--9602. PMLR, 2022.

\bibitem{poupart2006analytic}
Pascal Poupart, Nikos Vlassis, Jesse Hoey, and Kevin Regan.
\newblock An analytic solution to discrete bayesian reinforcement learning.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 697--704, 2006.

\bibitem{rigter2021risk}
Marc Rigter, Bruno Lacerda, and Nick Hawes.
\newblock Risk-averse bayes-adaptive reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:1142--1154, 2021.

\bibitem{ross2007bayes}
Stephane Ross, Brahim Chaib-draa, and Joelle Pineau.
\newblock Bayes-adaptive pomdps.
\newblock {\em Advances in neural information processing systems}, 20, 2007.

\bibitem{shapiro2017interchangeability}
Alexander Shapiro.
\newblock Interchangeability principle and dynamic equations in risk averse
  stochastic programming.
\newblock {\em Operations Research Letters}, 45(4):377--381, 2017.

\bibitem{sharma2019robust}
Apoorva Sharma, James Harrison, Matthew Tsao, and Marco Pavone.
\newblock Robust and adaptive planning under model uncertainty.
\newblock In {\em Proceedings of the International Conference on Automated
  Planning and Scheduling}, volume~29, pages 410--418, 2019.

\bibitem{si2020distributionally}
Nian Si, Fan Zhang, Zhengyuan Zhou, and Jose Blanchet.
\newblock Distributionally robust policy evaluation and learning in offline
  contextual bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  8884--8894. PMLR, 2020.

\bibitem{singh2000convergence}
Satinder Singh, Tommi Jaakkola, Michael~L Littman, and Csaba Szepesv{\'a}ri.
\newblock Convergence results for single-step on-policy reinforcement-learning
  algorithms.
\newblock {\em Machine learning}, 38(3):287--308, 2000.

\bibitem{strens2000bayesian}
Malcolm Strens.
\newblock A bayesian framework for reinforcement learning.
\newblock In {\em ICML}, volume 2000, pages 943--950, 2000.

\bibitem{sutton1984temporal}
Richard~Stuart Sutton.
\newblock {\em Temporal credit assignment in reinforcement learning}.
\newblock University of Massachusetts Amherst, 1984.

\bibitem{wang2005bayesian}
Tao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans.
\newblock Bayesian sparse sampling for on-line reward optimization.
\newblock In {\em Proceedings of the 22nd international conference on Machine
  learning}, pages 956--963, 2005.

\bibitem{watkins1989learning}
Christopher John Cornish~Hellaby Watkins.
\newblock Learning from delayed rewards.
\newblock 1989.

\bibitem{wu2018bayesian}
Di~Wu, Helin Zhu, and Enlu Zhou.
\newblock A bayesian risk approach to data-driven stochastic optimization:
  Formulations and asymptotics.
\newblock {\em SIAM Journal on Optimization}, 28(2):1588--1612, 2018.

\bibitem{yang2022toward}
Wenhao Yang, Liangyu Zhang, and Zhihua Zhang.
\newblock Toward theoretical understandings of robust {M}arkov decision
  processes: Sample complexity and asymptotics.
\newblock {\em The Annals of Statistics}, 50(6):3223--3248, 2022.

\bibitem{zhou2015simulation}
Enlu Zhou and Wei Xie.
\newblock Simulation optimization when facing input uncertainty.
\newblock In {\em 2015 Winter Simulation Conference (WSC)}, pages 3714--3724.
  IEEE, 2015.

\bibitem{zhou2021finite}
Zhengqing Zhou, Zhengyuan Zhou, Qinxun Bai, Linhai Qiu, Jose Blanchet, and
  Peter Glynn.
\newblock Finite-sample regret bound for distributionally robust offline
  tabular reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3331--3339. PMLR, 2021.

\end{thebibliography}
