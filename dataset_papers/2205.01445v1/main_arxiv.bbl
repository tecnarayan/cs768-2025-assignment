\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{ABAB{\etalchar{+}}21}

\bibitem[ABAB{\etalchar{+}}21]{abbe2021staircase}
Emmanuel Abbe, Enric Boix~Adsera, Matthew Brennan, Guy Bresler, and Dheeraj
  Nagaraj.
\newblock The staircase property: How hierarchical structure can guide deep
  learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[ABAM22]{abbe2022merged}
Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz.
\newblock The merged-staircase property: a necessary and nearly sufficient
  condition for sgd learning of sparse functions on two-layer neural networks.
\newblock {\em arXiv preprint arXiv:2202.08658}, 2022.

\bibitem[Ada15]{adamczak2015note}
Radoslaw Adamczak.
\newblock A note on the hanson-wright inequality for random vectors with
  dependencies.
\newblock {\em Electronic Communications in Probability}, 20:1--13, 2015.

\bibitem[ADH{\etalchar{+}}19]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[AGJ21]{arous2021online}
Gerard~Ben Arous, Reza Gheissari, and Aukosh Jagannath.
\newblock Online stochastic gradient descent on non-convex losses from
  high-dimensional inference.
\newblock {\em Journal of Machine Learning Research}, 22(106):1--51, 2021.

\bibitem[AP20]{adlam2020neural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In {\em International Conference on Machine Learning}, pages 74--84.
  PMLR, 2020.

\bibitem[AZL19]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[AZL20]{allen2020backward}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Backward feature correction: How deep learning performs deep
  learning.
\newblock {\em arXiv preprint arXiv:2001.04413}, 2020.

\bibitem[AZLL19]{allen2018learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem[Bac17]{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):629--681, 2017.

\bibitem[Bac23]{bach2021learning}
Francis Bach.
\newblock {\em Learning Theory from First Principles}.
\newblock MIT Press, 2023.

\bibitem[BAP05]{baik2005phase}
Jinho Baik, G{\'e}rard~Ben Arous, and Sandrine P{\'e}ch{\'e}.
\newblock Phase transition of the largest eigenvalue for nonnull complex sample
  covariance matrices.
\newblock {\em The Annals of Probability}, 33(5):1643--1697, 2005.

\bibitem[BGN11]{benaych2011eigenvalues}
Florent Benaych-Georges and Raj~Rao Nadakuditi.
\newblock The eigenvalues and eigenvectors of finite, low rank perturbations of
  large random matrices.
\newblock {\em Advances in Mathematics}, 227(1):494--521, 2011.

\bibitem[BGN12]{benaych2012singular}
Florent Benaych-Georges and Raj~Rao Nadakuditi.
\newblock The singular values and vectors of low rank perturbations of large
  rectangular random matrices.
\newblock {\em Journal of Multivariate Analysis}, 111:120--135, 2012.

\bibitem[BHMM19]{belkin2018reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem[BL20]{bai2019beyond}
Yu~Bai and Jason~D. Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[BLM13]{boucheron2013concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Pascal Massart.
\newblock {\em Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem[BM21]{bodin2021model}
Antoine Bodin and Nicolas Macris.
\newblock Model, sample, and epoch-wise descents: exact solution of gradient
  flow in the random feature model.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[BMR21]{bartlett2021deep}
Peter~L Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock {\em Acta numerica}, 30:87--201, 2021.

\bibitem[BP21]{benigni2019eigenvalue}
Lucas Benigni and Sandrine P{\'e}ch{\'e}.
\newblock Eigenvalue distribution of some nonlinear models of random matrices.
\newblock {\em Electronic Journal of Probability}, 26:1--37, 2021.

\bibitem[BP22]{benigni2022largest}
Lucas Benigni and Sandrine P{\'e}ch{\'e}.
\newblock Largest eigenvalues of the conjugate kernel of single-layered neural
  networks.
\newblock {\em arXiv preprint arXiv:2201.04753}, 2022.

\bibitem[BS98]{bai1998no}
Zhi-Dong Bai and Jack~W Silverstein.
\newblock No eigenvalues outside the support of the limiting spectral
  distribution of large-dimensional sample covariance matrices.
\newblock {\em The Annals of Probability}, 26(1):316--345, 1998.

\bibitem[BS10]{bai2010spectral}
Zhidong Bai and Jack~W Silverstein.
\newblock {\em Spectral analysis of large dimensional random matrices},
  volume~20.
\newblock Springer, 2010.

\bibitem[Cap18]{capitaine2018limiting}
Mireille Capitaine.
\newblock Limiting eigenvectors of outliers for spiked information-plus-noise
  type matrices.
\newblock In {\em S{\'e}minaire de Probabilit{\'e}s XLIX}, pages 119--164.
  Springer, 2018.

\bibitem[CB18]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In {\em Advances in neural information processing systems}, pages
  3036--3046, 2018.

\bibitem[CB20]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In {\em Conference on Learning Theory}, pages 1305--1338. PMLR, 2020.

\bibitem[Chi22]{chizat2022mean}
L{\'e}na{\"\i}c Chizat.
\newblock Mean-field langevin dynamics: Exponential convergence and annealing.
\newblock {\em arXiv preprint arXiv:2202.01009}, 2022.

\bibitem[CHS20]{chen2020label}
Shuxiao Chen, Hangfeng He, and Weijie Su.
\newblock Label-aware neural tangent kernel: Toward better generalization and
  local elasticity.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[CKL{\etalchar{+}}21]{cohen2021gradient}
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem[CLB21]{chatterji2020does}
Niladri~S Chatterji, Philip~M Long, and Peter~L Bartlett.
\newblock When does gradient descent with logistic loss find interpolating
  two-layer networks?
\newblock {\em Journal of Machine Learning Research}, 22(159):1--48, 2021.

\bibitem[COB19]{chizat2018note}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[CS13]{cheng2013spectrum}
Xiuyuan Cheng and Amit Singer.
\newblock The spectrum of random inner-product kernel matrices.
\newblock {\em Random Matrices: Theory and Applications}, 2(04):1350010, 2013.

\bibitem[CSTEK01]{cristianini2001kernel}
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz Kandola.
\newblock On kernel-target alignment.
\newblock {\em Advances in neural information processing systems}, 14, 2001.

\bibitem[DCLT18]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[DGA20]{dyer2019asymptotics}
Ethan Dyer and Guy Gur-Ari.
\newblock Asymptotics of wide networks from feynman diagrams.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[DL20]{dhifallah2020precise}
Oussama Dhifallah and Yue~M Lu.
\newblock A precise performance analysis of learning with random features.
\newblock {\em arXiv preprint arXiv:2008.11904}, 2020.

\bibitem[DM20]{daniely2020learning}
Amit Daniely and Eran Malach.
\newblock Learning parities with neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:20356--20365, 2020.

\bibitem[DV13]{do2013spectrum}
Yen Do and Van Vu.
\newblock The spectrum of random kernel matrices: universality results for
  rough and varying kernels.
\newblock {\em Random Matrices: Theory and Applications}, 2(03):1350005, 2013.

\bibitem[DW18]{dobriban2018high}
Edgar Dobriban and Stefan Wager.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock {\em The Annals of Statistics}, 46(1):247--279, 2018.

\bibitem[DWY21]{donhauser2021rotational}
Konstantin Donhauser, Mingqi Wu, and Fanny Yang.
\newblock How rotational invariance of common kernels prevents generalization
  in high dimensions.
\newblock In {\em International Conference on Machine Learning}, pages
  2804--2814. PMLR, 2021.

\bibitem[DZPS19]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem[EK10]{el2010spectrum}
Noureddine El~Karoui.
\newblock The spectrum of kernel random matrices.
\newblock {\em The Annals of Statistics}, 38(1):1--50, 2010.

\bibitem[EK18]{el2018impact}
Noureddine El~Karoui.
\newblock On the impact of predictor geometry on the performance on
  high-dimensional ridge-regularized generalized robust regression estimators.
\newblock {\em Probability Theory and Related Fields}, 170(1):95--175, 2018.

\bibitem[FCB22]{frei2022random}
Spencer Frei, Niladri~S Chatterji, and Peter~L Bartlett.
\newblock Random feature amplification: Feature learning and generalization in
  neural networks.
\newblock {\em arXiv preprint arXiv:2202.07626}, 2022.

\bibitem[FDP{\etalchar{+}}20]{fort2020deep}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
  Daniel~M Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock {\em Advances in Neural Information Processing Systems},
  33:5850--5861, 2020.

\bibitem[FM19]{fan2019spectral}
Zhou Fan and Andrea Montanari.
\newblock The spectral norm of random inner-product kernel matrices.
\newblock {\em Probability Theory and Related Fields}, 173(1-2):27--85, 2019.

\bibitem[FOBS06]{far2006spectra}
Reza~Rashidi Far, Tamer Oraby, Wlodzimierz Bryc, and Roland Speicher.
\newblock Spectra of large block matrices.
\newblock {\em arXiv preprint cs/0610045}, 2006.

\bibitem[FW20]{fan2020spectra}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock {\em Advances in neural information processing systems},
  33:7710--7721, 2020.

\bibitem[GAS19]{golatkar2019time}
Aditya~Sharad Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Time matters in regularizing deep networks: Weight decay and data
  augmentation affect early learning dynamics, matter little near convergence.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[GDDM14]{girshick2014rich}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 580--587, 2014.

\bibitem[GLK{\etalchar{+}}20]{gerace2020generalisation}
Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc M{\'e}zard, and Lenka
  Zdeborov{\'a}.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock In {\em International Conference on Machine Learning}, pages
  3452--3462. PMLR, 2020.

\bibitem[GLR{\etalchar{+}}21]{goldt2021gaussian}
Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc
  M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock The gaussian equivalence of generative models for learning with
  shallow neural networks.
\newblock {\em Proceedings of Machine Learning Research vol}, 145:1--46, 2021.

\bibitem[GMKZ20]{goldt2020modeling}
Sebastian Goldt, Marc M{\'e}zard, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock {\em Physical Review X}, 10(4):041044, 2020.

\bibitem[GMMM19]{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[GMMM20]{ghorbani2020neural}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock {\em Advances in Neural Information Processing Systems},
  33:14820--14830, 2020.

\bibitem[GMMM21]{ghorbani2019linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em The Annals of Statistics}, 49(2):1029--1054, 2021.

\bibitem[Gor88]{gordon1988milman}
Yehoram Gordon.
\newblock On milman's inequality and random subspaces which escape through a
  mesh in $\mathbb{R}^n$.
\newblock In {\em Geometric aspects of functional analysis}, pages 84--106.
  Springer, 1988.

\bibitem[GSJW20]{geiger2020disentangling}
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(11):113301, 2020.

\bibitem[HCG21]{hajjar2021training}
Karl Hajjar, L{\'e}na{\"\i}c Chizat, and Christophe Giraud.
\newblock Training integrable parameterizations of deep neural networks in the
  infinite-width limit.
\newblock {\em arXiv preprint arXiv:2110.15596}, 2021.

\bibitem[HFS07]{helton2007operator}
J~William Helton, Reza~Rashidi Far, and Roland Speicher.
\newblock Operator-valued semicircular elements: solving a quadratic matrix
  equation with positivity constraints.
\newblock {\em International Mathematics Research Notices},
  2007(9):rnm086--rnm086, 2007.

\bibitem[HL20]{hu2020universality}
Hong Hu and Yue~M Lu.
\newblock Universality laws for high-dimensional learning with random features.
\newblock {\em arXiv preprint arXiv:2009.07669}, 2020.

\bibitem[HMS18]{helton2018applications}
J~William Helton, Tobias Mai, and Roland Speicher.
\newblock Applications of realizations (aka linearizations) to free
  probability.
\newblock {\em Journal of Functional Analysis}, 274(1):1--79, 2018.

\bibitem[HY20]{huang2020dynamics}
Jiaoyang Huang and Horng-Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock In {\em International conference on machine learning}, pages
  4542--4551. PMLR, 2020.

\bibitem[IF19]{imaizumi2019deep}
Masaaki Imaizumi and Kenji Fukumizu.
\newblock Deep neural networks learn non-smooth functions effectively.
\newblock In {\em The 22nd international conference on artificial intelligence
  and statistics}, pages 869--878. PMLR, 2019.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[JSF{\etalchar{+}}20]{jastrzebski2020break}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
  Tabor, Kyunghyun Cho, and Krzysztof Geras.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[JT20]{ji2019polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[KWLS21]{karp2021local}
Stefani Karp, Ezra Winston, Yuanzhi Li, and Aarti Singh.
\newblock Local signal adaptivity: Provable feature learning in neural networks
  beyond kernels.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[LBD{\etalchar{+}}20]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock {\em arXiv preprint arXiv:2003.02218}, 2020.

\bibitem[LCM20]{liao2020random}
Zhenyu Liao, Romain Couillet, and Michael~W Mahoney.
\newblock A random matrix analysis of random fourier features: beyond the
  gaussian kernel, a precise phase transition, and the corresponding double
  descent.
\newblock {\em Advances in Neural Information Processing Systems},
  33:13939--13950, 2020.

\bibitem[LGC{\etalchar{+}}21]{loureiro2021learning}
Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala,
  Marc Mezard, and Lenka Zdeborov{\'a}.
\newblock Learning curves of generic features maps for realistic datasets with
  a teacher-student model.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[LLC18]{louart2018random}
Cosme Louart, Zhenyu Liao, and Romain Couillet.
\newblock A random matrix approach to neural networks.
\newblock {\em The Annals of Applied Probability}, 28(2):1190--1248, 2018.

\bibitem[LM20]{leclerc2020two}
Guillaume Leclerc and Aleksander Madry.
\newblock The two regimes of deep network training.
\newblock {\em arXiv preprint arXiv:2002.10376}, 2020.

\bibitem[LMZ20]{li2020learning}
Yuanzhi Li, Tengyu Ma, and Hongyang~R Zhang.
\newblock Learning over-parametrized two-layer neural networks beyond ntk.
\newblock In {\em Conference on learning theory}, pages 2613--2682. PMLR, 2020.

\bibitem[LR20]{liang2018just}
Tengyuan Liang and Alexander Rakhlin.
\newblock Just interpolate: Kernel “ridgeless” regression can generalize.
\newblock {\em The Annals of Statistics}, 48(3):1329--1347, 2020.

\bibitem[LWM19]{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11674--11685, 2019.

\bibitem[Mec19]{meckes2019random}
Elizabeth~S Meckes.
\newblock {\em The random matrix theory of the classical compact groups},
  volume 218.
\newblock Cambridge University Press, 2019.

\bibitem[MKAS21]{malach2021quantifying}
Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro.
\newblock Quantifying the benefit of using differentiable learning over tangent
  kernels.
\newblock In {\em International Conference on Machine Learning}, pages
  7379--7389. PMLR, 2021.

\bibitem[MM22]{mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock {\em Communications on Pure and Applied Mathematics}, 75(4):667--766,
  2022.

\bibitem[MMM21]{mei2021generalization}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Generalization error of random feature and kernel methods:
  hypercontractivity and kernel matrix concentration.
\newblock {\em Applied and Computational Harmonic Analysis}, 2021.

\bibitem[MMN18]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671, 2018.

\bibitem[MS17]{mingo2017free}
James~A Mingo and Roland Speicher.
\newblock {\em Free probability and random matrices}, volume~35.
\newblock Springer, 2017.

\bibitem[MS22]{montanari2022universality}
Andrea Montanari and Basil Saeed.
\newblock Universality of empirical risk minimization.
\newblock {\em arXiv preprint arXiv:2202.08832}, 2022.

\bibitem[MZ20]{montanari2020interpolation}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training.
\newblock {\em arXiv preprint arXiv:2007.12826v1}, 2020.

\bibitem[Nea95]{neal1995bayesian}
Radford~M Neal.
\newblock {\em Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 1995.

\bibitem[Ngu21]{nguyen2021analysis}
Phan-Minh Nguyen.
\newblock Analysis of feature learning in weight-tied autoencoders via the mean
  field lens.
\newblock {\em arXiv preprint arXiv:2102.08373}, 2021.

\bibitem[NS17]{nitanda2017stochastic}
Atsushi Nitanda and Taiji Suzuki.
\newblock Stochastic particle gradient descent for infinite ensembles.
\newblock {\em arXiv preprint arXiv:1712.05438}, 2017.

\bibitem[NWS22]{nitanda2022convex}
Atsushi Nitanda, Denny Wu, and Taiji Suzuki.
\newblock Convex analysis of the mean field langevin dynamics.
\newblock {\em arXiv preprint arXiv:2201.10469}, 2022.

\bibitem[OJMDF21]{ortiz2021can}
Guillermo Ortiz-Jim{\'e}nez, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
  Frossard.
\newblock What can linearized neural networks actually say about
  generalization?
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[OS20]{oymak2019towards}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock {\em IEEE Journal on Selected Areas in Information Theory},
  1(1):84--105, 2020.

\bibitem[P{\'e}c19]{peche2019note}
S~P{\'e}ch{\'e}.
\newblock A note on the pennington-worah distribution.
\newblock {\em Electronic Communications in Probability}, 24:1--7, 2019.

\bibitem[PPVF21]{pesme2021implicit}
Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion.
\newblock Implicit bias of sgd for diagonal linear networks: a provable benefit
  of stochasticity.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[PW17]{pennington2017nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2637--2646, 2017.

\bibitem[RGKZ21]{refinetti2021classifying}
Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborov{\'a}.
\newblock Classifying high-dimensional gaussian mixtures: Where kernel methods
  fail and neural networks succeed.
\newblock In {\em International Conference on Machine Learning}, pages
  8936--8947. PMLR, 2021.

\bibitem[RR08]{rahimi2008random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in neural information processing systems}, pages
  1177--1184, 2008.

\bibitem[SA20]{suzuki2020benefit}
Taiji Suzuki and Shunta Akiyama.
\newblock Benefit of deep learning with non-convex noisy gradient descent:
  Provable excess risk bound and superiority to kernel methods.
\newblock {\em arXiv preprint arXiv:2012.03224}, 2020.

\bibitem[SH20]{schmidt2020nonparametric}
Johannes Schmidt-Hieber.
\newblock Nonparametric regression using deep neural networks with relu
  activation function.
\newblock {\em The Annals of Statistics}, 48(4):1875--1897, 2020.

\bibitem[Ste90]{stewart1990matrix}
Gilbert~W Stewart.
\newblock Matrix perturbation theory.
\newblock 1990.

\bibitem[Suz18]{suzuki2018adaptivity}
Taiji Suzuki.
\newblock Adaptivity of deep relu network for learning in besov and mixed
  smooth besov spaces: optimal rate and curse of dimensionality.
\newblock {\em arXiv preprint arXiv:1810.08033}, 2018.

\bibitem[TAP21]{tripuraneni2021covariate}
Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington.
\newblock Covariate shift in high-dimensional random feature regression.
\newblock {\em arXiv preprint arXiv:2111.08234}, 2021.

\bibitem[TOH15]{thrampoulidis2015regularized}
Christos Thrampoulidis, Samet Oymak, and Babak Hassibi.
\newblock Regularized linear regression: A precise analysis of the estimation
  error.
\newblock In {\em Conference on Learning Theory}, pages 1683--1709. PMLR, 2015.

\bibitem[Ver18]{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[VSL{\etalchar{+}}22]{veiga2022phase}
Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka
  Zdeborov{\'a}.
\newblock Phase diagram of stochastic gradient descent in high-dimensional
  two-layer neural networks.
\newblock {\em arXiv preprint arXiv:2202.00293}, 2022.

\bibitem[WGL{\etalchar{+}}20]{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In {\em Conference on Learning Theory}, pages 3635--3673. PMLR, 2020.

\bibitem[WLLM19]{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9712--9724, 2019.

\bibitem[WX20]{wu2020optimal}
Denny Wu and Ji~Xu.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock {\em Advances in Neural Information Processing Systems},
  33:10112--10123, 2020.

\bibitem[WZ21]{wang2021deformed}
Zhichao Wang and Yizhe Zhu.
\newblock Deformed semicircle law and concentration of nonlinear random
  matrices for ultra-wide neural networks.
\newblock {\em arXiv preprint arXiv:2109.09304}, 2021.

\bibitem[Yan20]{yang2020tensor}
Greg Yang.
\newblock Tensor programs iii: Neural matrix laws.
\newblock {\em arXiv preprint arXiv:2009.10685}, 2020.

\bibitem[YH20]{yang2020feature}
Greg Yang and Edward~J Hu.
\newblock Feature learning in infinite-width neural networks.
\newblock {\em arXiv preprint arXiv:2011.14522}, 2020.

\bibitem[YS19]{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\end{thebibliography}
