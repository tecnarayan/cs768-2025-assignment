@inproceedings{ziemann2022single,
  title={Single trajectory nonparametric learning of nonlinear dynamics},
  author={Ziemann, Ingvar M and Sandberg, Henrik and Matni, Nikolai},
  booktitle={conference on Learning Theory},
  pages={3333--3364},
  year={2022},
  organization={PMLR}
}

@misc{chen2019informationtheoretic_coverage,
      title={Information-Theoretic Considerations in Batch Reinforcement Learning}, 
      author={Jinglin Chen and Nan Jiang},
      year={2019},
      eprint={1905.00360},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{xie2020q_coverage,
      title={Q* Approximation Schemes for Batch Reinforcement Learning: A Theoretical Comparison}, 
      author={Tengyang Xie and Nan Jiang},
      year={2020},
      eprint={2003.03924},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jin2022pessimism_coverage,
      title={Is Pessimism Provably Efficient for Offline RL?}, 
      author={Ying Jin and Zhuoran Yang and Zhaoran Wang},
      year={2022},
      eprint={2012.15085},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rashidinejad2023bridging_coverage,
      title={Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism}, 
      author={Paria Rashidinejad and Banghua Zhu and Cong Ma and Jiantao Jiao and Stuart Russell},
      year={2023},
      eprint={2103.12021},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{foster2022offline_coverage,
      title={Offline Reinforcement Learning: Fundamental Barriers for Value Function Approximation}, 
      author={Dylan J. Foster and Akshay Krishnamurthy and David Simchi-Levi and Yunzong Xu},
      year={2022},
      eprint={2111.10919},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhan2022offline_coverage,
      title={Offline Reinforcement Learning with Realizability and Single-policy Concentrability}, 
      author={Wenhao Zhan and Baihe Huang and Audrey Huang and Nan Jiang and Jason D. Lee},
      year={2022},
      eprint={2202.04634},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{ziemannlearning,
  title={Learning with little mixing},
  author={Ziemann, Ingvar and Tu, Stephen},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}
@inproceedings{kuznetsov2014generalization,
  title={Generalization bounds for time series prediction with non-stationary processes},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  booktitle={International conference on algorithmic learning theory},
  pages={},
  year={2014},
  organization={Springer}
}

@article{dean2020sample,
	author = {Dean, Sarah and Mania, Horia and Matni, Nikolai and Recht, Benjamin and Tu, Stephen},
	journal = {Foundations of Computational Mathematics},
	number = {4},
	pages = {633--679},
	publisher = {Springer},
	title = {On the sample complexity of the linear quadratic regulator},
	volume = {20},
	year = {2020}}

@article{mania2020active,
	author = {Mania, Horia and Jordan, Michael I and Recht, Benjamin},
	journal = {arXiv preprint arXiv:2006.10277},
	title = {Active learning for nonlinear system identification with guarantees},
	year = {2020}
}
@inproceedings{sarkar2019near,
	author = {Sarkar, Tuhin and Rakhlin, Alexander},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {5610--5618},
	title = {Near optimal finite time identification of arbitrary linear dynamical systems},
	year = {2019}
}
@inproceedings{kuznetsov2016time,
	author = {Kuznetsov, Vitaly and Mohri, Mehryar},
	booktitle = {Conference on Learning Theory},
	organization = {PMLR},
	pages = {1190--1213},
	title = {Time series prediction and online learning},
	year = {2016}
}
@inproceedings{simchowitz2018learning,
	author = {Simchowitz, Max and Mania, Horia and Tu, Stephen and Jordan, Michael I and Recht, Benjamin},
	booktitle = {Conference On Learning Theory},
	organization = {PMLR},
	pages = {439--473},
	title = {Learning without mixing: Towards a sharp analysis of linear system identification},
	year = {2018}
}

@article{mohri2008rademacher,
  title={Rademacher complexity bounds for non-iid processes},
  author={Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={Advances in Neural Information Processing Systems},
  volume={21},
  year={2008}
}

@inproceedings{foster2020learning,
	author = {Foster, Dylan and Sarkar, Tuhin and Rakhlin, Alexander},
	booktitle = {Learning for Dynamics and Control},
	organization = {PMLR},
	pages = {851--861},
	title = {Learning nonlinear dynamical systems from a single trajectory},
	year = {2020}}

@article{tsiamis2022statistical,
	author = {Tsiamis, Anastasios and Ziemann, Ingvar and Matni, Nikolai and Pappas, George J},
	journal = {arXiv preprint arXiv:2209.05423},
	title = {Statistical Learning Theory for Control: A Finite Sample Perspective},
	year = {2022}}

@article{sun2022finite,
  title={Finite sample identification of low-order LTI systems via nuclear norm regularization},
  author={Sun, Yue and Oymak, Samet and Fazel, Maryam},
  journal={IEEE Open Journal of Control Systems},
  volume={1},
  pages={237--254},
  year={2022},
  publisher={IEEE}
}

@inproceedings{matni2019tutorial,
	author = {Matni, Nikolai and Tu, Stephen},
	booktitle = {2019 IEEE 58th Conference on Decision and Control (CDC)},
	organization = {IEEE},
	pages = {3741--3749},
	title = {A tutorial on concentration bounds for system identification},
	year = {2019}}

@article{oymak2021revisiting,
	author = {Oymak, Samet and Ozay, Necmiye},
	journal = {IEEE Transactions on Automatic Control},
	number = {4},
	pages = {1914--1928},
	publisher = {IEEE},
	title = {Revisiting Ho--Kalman-Based System Identification: Robustness and Finite-Sample Analysis},
	volume = {67},
	year = {2021}}
@article{block2023smoothed,
  title={Smoothed Online Learning for Prediction in Piecewise Affine Systems},
  author={Block, Adam and Simchowitz, Max and Tedrake, Russ},
  journal={arXiv preprint arXiv:2301.11187},
  year={2023}
}


@article{tarzanagh2023transformers,
  title={Transformers as Support Vector Machines},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv preprint arXiv:2308.16898},
  year={2023}
}
@article{anonymous,
  title={Mechanics of Next Token Prediction with Self-Attention},
  author={Li, Yincong and Huang, Yixiao and Ildiz, M. Emrullah and Rawat, Ankit Singh and Oymak, Samet},
  journal={In International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2024}
}

@article{anonymous1,
  title={Anonymous Title},
  author={Anonymous Authors},
  journal={accepted to the International Conference on Artificial Intelligence and Statistics},
  year={2024}
}

@misc{press2017using,
      title={Using the Output Embedding to Improve Language Models}, 
      author={Ofir Press and Lior Wolf},
      year={2017},
      eprint={1608.05859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Bartlett_2005,
   title={Local Rademacher complexities},
   volume={33},
   ISSN={0090-5364},
   url={http://dx.doi.org/10.1214/009053605000000282},
   DOI={10.1214/009053605000000282},
   number={4},
   journal={The Annals of Statistics},
   publisher={Institute of Mathematical Statistics},
   author={Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
   year={2005},
   month=aug }


@article{see2017get,
  title={Get to the point: Summarization with pointer-generator networks},
  author={See, Abigail and Liu, Peter J and Manning, Christopher D},
  journal={arXiv preprint arXiv:1704.04368},
  year={2017}
}

@inproceedings{srebro_FastRate,
 author = {Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Smoothness, Low Noise and Fast Rates},
 url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf},
 volume = {23},
 year = {2010}
}

@misc{makkuva2024attention,
      title={Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains}, 
      author={Ashok Vardhan Makkuva and Marco Bondaschi and Adway Girish and Alliot Nagle and Martin Jaggi and Hyeji Kim and Michael Gastpar},
      year={2024},
      eprint={2402.04161},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{welleck2019neural,
  title={Neural text generation with unlikelihood training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  journal={arXiv preprint arXiv:1908.04319},
  year={2019}
}

@inproceedings{lin2021straight,
  title={Straight to the gradient: Learning to use novel tokens for neural text generation},
  author={Lin, Xiang and Han, Simeng and Joty, Shafiq},
  booktitle={International Conference on Machine Learning},
  pages={6642--6653},
  year={2021},
  organization={PMLR}
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{welleck2020consistency,
  title={Consistency of a recurrent language model with respect to incomplete decoding},
  author={Welleck, Sean and Kulikov, Ilia and Kim, Jaedeok and Pang, Richard Yuanzhe and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:2002.02492},
  year={2020}
}

@inproceedings{fu2021theoretical,
  title={A theoretical analysis of the repetition problem in text generation},
  author={Fu, Zihao and Lam, Wai and So, Anthony Man-Cho and Shi, Bei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  pages={12848--12856},
  year={2021}
}

@InProceedings{pmlr-v132-chan21a,
  title = 	 {Learning and Testing Irreducible {M}arkov Chains via the $k$-Cover Time},
  author =       {Chan, Siu On and Ding, Qinghua and Li, Sing Hei},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {458--480},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
}



@article{hao2018learning,
  title={On learning markov chains},
  author={Hao, Yi and Orlitsky, Alon and Pichapati, Venkatadheeraj},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{wolfer2021statistical,
  title={Statistical estimation of ergodic Markov chain kernel over discrete state space},
  author={Wolfer, Geoffrey and Kontorovich, Aryeh},
  journal={Bernoulli},
  volume={27},
  number={1},
  pages={532--553},
  year={2021}
}

@inproceedings{wolfer2019minimax,
  title={Minimax learning of ergodic markov chains},
  author={Wolfer, Geoffrey and Kontorovich, Aryeh},
  booktitle={Algorithmic Learning Theory},
  pages={904--930},
  year={2019},
  organization={PMLR}
}


@article{billingsley1961statistical,
  title={Statistical methods in Markov chains},
  author={Billingsley, Patrick},
  journal={The annals of mathematical statistics},
  pages={12--40},
  year={1961},
  publisher={JSTOR}
}


@article{stojanovic2023spectral,
  title={Spectral Entry-wise Matrix Estimation for Low-Rank Reinforcement Learning},
  author={Stojanovic, Stefan and Jedra, Yassir and Proutiere, Alexandre},
  journal={arXiv preprint arXiv:2310.06793},
  year={2023}
}



@article{zhang2019spectral,
  title={Spectral state compression of markov processes},
  author={Zhang, Anru and Wang, Mengdi},
  journal={IEEE transactions on information theory},
  volume={66},
  number={5},
  pages={3202--3231},
  year={2019},
  publisher={IEEE}
}

@article{bi2023low,
  title={A low-rank spectral method for learning Markov models},
  author={Bi, Shujun and Yin, Zhen and Weng, Yihong},
  journal={Optimization Letters},
  volume={17},
  number={1},
  pages={143--162},
  year={2023},
  publisher={Springer}
}

@inproceedings{li2018estimation,
  title={Estimation of Markov chain via rank-constrained likelihood},
  author={Li, Xudong and Wang, Mengdi and Zhang, Anru},
  booktitle={International Conference on Machine Learning},
  pages={3033--3042},
  year={2018},
  organization={PMLR}
}

@article{zhu2022learning,
  title={Learning Markov models via low-rank optimization},
  author={Zhu, Ziwei and Li, Xudong and Wang, Mengdi and Zhang, Anru},
  journal={Operations Research},
  volume={70},
  number={4},
  pages={2384--2398},
  year={2022},
  publisher={INFORMS}
}

@article{shah2020sample,
  title={Sample efficient reinforcement learning via low-rank matrix estimation},
  author={Shah, Devavrat and Song, Dogyoon and Xu, Zhi and Yang, Yuzhe},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12092--12103},
  year={2020}
}


@article{welton2005estimation,
  title={Estimation of Markov chain transition probabilities and rates from fully and partially observed data: uncertainty propagation, evidence synthesis, and model calibration},
  author={Welton, Nicky J and Ades, AE},
  journal={Medical Decision Making},
  volume={25},
  number={6},
  pages={633--645},
  year={2005},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@article{craig2002estimation,
  title={Estimation of the transition matrix of a discrete-time Markov chain},
  author={Craig, Bruce A and Sendi, Peter P},
  journal={Health economics},
  volume={11},
  number={1},
  pages={33--42},
  year={2002},
  publisher={Wiley Online Library}
}


@article{yan2023understanding,
  title={Understanding in-context learning from repetitions},
  author={Yan, Jianhao and Xu, Jin and Song, Chiyu and Wu, Chenming and Li, Yafu and Zhang, Yue},
  journal={arXiv preprint arXiv:2310.00297},
  year={2023}
}

@book{durrett1996pte,
  added-at = {2009-04-24T23:33:01.000+0200},
  address = {Belmont, CA},
  author = {Durrett, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/2ef848e83a27a6a88051aa9e77a375028/peter.ralph},
  description = {q-paper},
  edition = {Second},
  interhash = {7cd1cc941b60d755e6a1c4b030513cd1},
  intrahash = {ef848e83a27a6a88051aa9e77a375028},
  isbn = {0-534-24318-5},
  keywords = {probability_theory reference},
  mrclass = {60-01},
  mrnumber = {MR1609153 (98m:60001)},
  pages = {225},
  publisher = {Duxbury Press},
  timestamp = {2009-04-24T23:44:02.000+0200},
  title = {Probability: theory and examples},
  year = 1996,
    note = "pg. 225"
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@inproceedings{yun2020_universal,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank Reddi and Sanjiv Kumar},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr}
}

@article{yun2020_sparse,
  title={O (n) connections are expressive enough: Universal approximability of sparse transformers},
  author={Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13783--13794},
  year={2020}
}


@InProceedings{edelman2022_inductive,
  title = 	 {Inductive Biases and Variable Creation in Self-Attention Mechanisms},
  author =       {Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5793--5831},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/edelman22a/edelman22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/edelman22a.html},
  abstract = 	 {Self-attention, an architectural motif designed to model long-range interactions in sequential data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm Transformer networks "create sparse variables": a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers.}
}


@article{fu2023_randomfeatures,
  title={What can a Single Attention Layer Learn? A Study Through the Random Features Lens},
  author={Fu, Hengyu and Guo, Tianyu and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2307.11353},
  year={2023}
}

@article{baldi2023_quarks,
title = {The quarks of attention: Structure and capacity of neural attention building blocks},
journal = {Artificial Intelligence},
volume = {319},
pages = {103901},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103901},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000474},
author = {Pierre Baldi and Roman Vershynin},
keywords = {Neural networks, Deep learning, Attention, Gating, Synaptic modulation, Transformers, Capacity, Circuit complexity},
}


@article{tarzanagh2023margin,
  title={Margin Maximization in Attention Mechanism},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Zhang, Xuechen and Oymak, Samet},
  journal={arXiv preprint arXiv:2306.13596},
  year={2023}
}

@article{gpt4_techreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      journal={arXiv preprintarXiv:2303.08774},
      year={2023}
}


@InProceedings{dong2021_rank,
  title = 	 {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
  author =       {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2793--2803},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/dong21a/dong21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/dong21a.html},
}


@inproceedings{xie2022_incontext,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RdJVFCHjUMI}
}

@inproceedings{garg2022_incontext,
 author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {30583--30598},
 publisher = {Curran Associates, Inc.},
 title = {What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{akyurek2023_incontext,
title={What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0g0X4H8yN4I}
}

@InProceedings{oswald2023_incontext,
  title = 	 {Transformers Learn In-Context by Gradient Descent},
  author =       {Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Joao and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {35151--35174},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/von-oswald23a.html},
}


@InProceedings{li2023_incontext,
  title = 	 {Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author =       {Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {19565--19594},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/li23l/li23l.pdf},
  url = 	 {https://proceedings.mlr.press/v202/li23l.html},
}


@article{touvron2023_llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{chowdhery2022_palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{rosset2003margin,
  title={Margin maximizing loss functions},
  author={Rosset, Saharon and Zhu, Ji and Hastie, Trevor},
  journal={Advances in neural information processing systems},
  volume={16},
  year={2003}
}
@article{suggala2018connecting,
  title={Connecting optimization and regularization paths},
  author={Suggala, Arun and Prasad, Adarsh and Ravikumar, Pradeep K},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{ji2020gradient,
  title={Gradient descent follows the regularization path for general losses},
  author={Ji, Ziwei and Dud{\'\i}k, Miroslav and Schapire, Robert E and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={2109--2136},
  year={2020},
  organization={PMLR}
}

@inproceedings{chen2021_decision,
 author = {Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {15084--15097},
 publisher = {Curran Associates, Inc.},
 title = {Decision Transformer: Reinforcement Learning via Sequence Modeling},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf},
 volume = {34},
 year = {2021}
}


@InProceedings{oymak23a_prompt,
  title = 	 {On the Role of Attention in Prompt-tuning},
  author =       {Oymak, Samet and Rawat, Ankit Singh and Soltanolkotabi, Mahdi and Thrampoulidis, Christos},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {26724--26768},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/oymak23a/oymak23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/oymak23a.html},
}

@inproceedings{jelassi2022_transformer,
title={Vision Transformers provably learn spatial structure},
author={Samy Jelassi and Michael Eli Sander and Yuanzhi Li},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=eMW9AkXaREI}
}

@inproceedings{vaswani2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@inproceedings{li2023_transformer,
title={A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity},
author={Hongkang Li and Meng Wang and Sijia Liu and Pin-Yu Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=jClGv3Qjhb}
}

@article{radford2018_gpt1,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI},
journal={OpenAI blog}
}

@article{radford2019_gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{xu2022learning,
  title={Learning to break the loop: Analyzing and mitigating repetitions for neural text generation},
  author={Xu, Jin and Liu, Xiaojiang and Yan, Jianhao and Cai, Deng and Li, Huayang and Li, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3082--3095},
  year={2022}
}


@inproceedings{brown2020_gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{ferruz2022_protgpt2,
  title={ProtGPT2 is a deep unsupervised language model for protein design},
  author={Ferruz, Noelia and Schmidt, Steffen and H{\"o}cker, Birte},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={4348},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{dosovitskiy2021_vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@article{nijkamp2022_progen2,
  title={ProGen2: exploring the boundaries of protein language models},
  author={Nijkamp, Erik and Ruffolo, Jeffrey and Weinstein, Eli N and Naik, Nikhil and Madani, Ali},
  journal={arXiv preprint arXiv:2206.13517},
  year={2022}
}

@InProceedings{chen2020_pixels,
  title = 	 {Generative Pretraining From Pixels},
  author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1691--1703},
  year = 	 {2020},
  editor = 	 {III, Hal Daum√© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20s.html},
}

@article{tian2023_scan,
  title={Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer},
  author={Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon},
  journal={arXiv preprint arXiv:2305.16380},
  year={2023}
}

@inproceedings{devlin2019_bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}

@inproceedings{chung2020generative,
  title={Generative pre-training for speech with autoregressive predictive coding},
  author={Chung, Yu-An and Glass, James},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3497--3501},
  year={2020},
  organization={IEEE}
}

@article{soudry2018_implicit,
  author  = {Daniel Soudry and Elad Hoffer and Mor Shpigel Nacson and Suriya Gunasekar and Nathan Srebro},
  title   = {The Implicit Bias of Gradient Descent on Separable Data},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {19},
  number  = {70},
  pages   = {1--57},
  url     = {http://jmlr.org/papers/v19/18-188.html}
}

@InProceedings{gunasekar2018_implicit,
  title = 	 {Characterizing Implicit Bias in Terms of Optimization Geometry},
  author =       {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1832--1841},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/gunasekar18a/gunasekar18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/gunasekar18a.html},
}


@InProceedings{ji2021_characterizing,
  title = 	 {Characterizing the implicit bias via a primal-dual analysis},
  author =       {Ji, Ziwei and Telgarsky, Matus},
  booktitle = 	 {Proceedings of the 32nd International Conference on Algorithmic Learning Theory},
  pages = 	 {772--804},
  year = 	 {2021},
  editor = 	 {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
  volume = 	 {132},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--19 Mar},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v132/ji21a/ji21a.pdf},
  url = 	 {https://proceedings.mlr.press/v132/ji21a.html},
}

@article{kini2021_label,
  title={Label-imbalanced and group-sensitive classification under overparameterization},
  author={Kini, Ganesh Ramachandra and Paraskevas, Orestis and Oymak, Samet and Thrampoulidis, Christos},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18970--18983},
  year={2021}
}


@InProceedings{ji2020_gradient,
  title = 	 {Gradient descent follows the regularization path for general losses},
  author =       {Ji, Ziwei and Dud{\'i}k, Miroslav and Schapire, Robert E. and Telgarsky, Matus},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {2109--2136},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/ji20a/ji20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/ji20a.html},
}

@article{li2019_towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{blanc2020_implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{qian2019_implicit,
  title={The implicit bias of adagrad on separable data},
  author={Qian, Qian and Qian, Xiaoyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@InProceedings{wang2021_implicit,
  title = 	 {The Implicit Bias for Adaptive Optimization Algorithms on Homogeneous Neural Networks},
  author =       {Wang, Bohan and Meng, Qi and Chen, Wei and Liu, Tie-Yan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10849--10858},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wang21q/wang21q.pdf},
  url = 	 {https://proceedings.mlr.press/v139/wang21q.html},
}


@article{ji2019risk,
      title={Risk and parameter convergence of logistic regression}, 
      author={Ziwei Ji and Matus Telgarsky},
      year={2019},
      eprint={1803.07300},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{tarjan1972scc,
author = {Tarjan, Robert},
title = {Depth-First Search and Linear Graph Algorithms},
journal = {SIAM Journal on Computing},
volume = {1},
number = {2},
pages = {146-160},
year = {1972},
doi = {10.1137/0201010},
URL = { https://doi.org/10.1137/0201010
},
eprint = {https://doi.org/10.1137/0201010
}}

