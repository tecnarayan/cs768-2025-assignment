\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{abnar2020transferring}
Samira Abnar, Mostafa Dehghani, and Willem Zuidema.
\newblock Transferring inductive biases through knowledge distillation.
\newblock {\em arXiv preprint arXiv:2006.00555}, 2020.

\bibitem{Hu2017SENet}
Jie~Hu andLi Shen and Gang Sun.
\newblock Squeeze-and-excitation networks.
\newblock {\em arXiv preprint arXiv:1709.01507}, 2017.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{berman2019multigrain}
Maxim Berman, Herv{\'{e}} J{\'{e}}gou, Andrea Vedaldi, Iasonas Kokkinos, and
  Matthijs Douze.
\newblock Multigrain: a unified image embedding for classes and instances.
\newblock {\em arXiv preprint arXiv:1902.05509}, 2019.

\bibitem{Beyer2020ImageNetReal}
Lucas Beyer, Olivier~J. H{\'e}naff, Alexander Kolesnikov, Xiaohua Zhai, and
  Aaron van~den Oord.
\newblock Are we done with imagenet?
\newblock {\em arXiv preprint arXiv:2006.07159}, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European Conference on Computer Vision}, 2020.

\bibitem{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{Chen2020UNITERUI}
Yen-Chun Chen, Linjie Li, Licheng Yu, A.~E. Kholy, Faisal Ahmed, Zhe Gan, Y.
  Cheng, and Jing jing Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In {\em European Conference on Computer Vision}, 2020.

\bibitem{Cho2019OnTE}
J.~H. Cho and B. Hariharan.
\newblock On the efficacy of knowledge distillation.
\newblock {\em International Conference on Computer Vision}, 2019.

\bibitem{Chu2020FeatureSA}
P. Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling.
\newblock Feature space augmentation for long-tailed data.
\newblock {\em arXiv preprint arXiv:2008.03673}, 2020.

\bibitem{Ekin2018AutoAugment}
Ekin~Dogus Cubuk, Barret Zoph, Dandelion Man{\'{e}}, Vijay Vasudevan, and
  Quoc~V. Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock {\em arXiv preprint arXiv:1805.09501}, 2018.

\bibitem{Cubuk2019RandAugmentPA}
Ekin~D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock {\em arXiv preprint arXiv:1909.13719}, 2019.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, pages
  248--255, 2009.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fan2019reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock {\em arXiv preprint arXiv:1909.11556}, 2019.
\newblock ICLR 2020.

\bibitem{fan2020training}
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R{\'e}mi Gribonval,
  Herv{\'e} J{\'e}gou, and Armand Joulin.
\newblock Training with quantization noise for extreme model compression.
\newblock {\em arXiv preprint arXiv:2004.07320}, 2020.

\bibitem{ghering2017convseq2seq}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N. Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock {\em arXiv preprint arXiv:1705.03122}, 2017.

\bibitem{Goyal2017AccurateLM}
Priya Goyal, Piotr Doll{\'a}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock {\em arXiv preprint arXiv:1706.02677}, 2017.

\bibitem{hanin2018start}
Boris Hanin and David Rolnick.
\newblock How to start training: The effect of initialization and architecture.
\newblock {\em NIPS}, 31, 2018.

\bibitem{He2016ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, June
  2016.

\bibitem{he2019bag}
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li.
\newblock Bag of tricks for image classification with convolutional neural
  networks.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, 2019.

\bibitem{Hendrycks2016GaussianEL}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{Hinton2015DistillingTK}
Geoffrey~E. Hinton, Oriol Vinyals, and J. Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hoffer2020augment}
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel
  Soudry.
\newblock Augment your batch: Improving generalization through instance
  repetition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition}, 2020.

\bibitem{Horn2018INaturalist}
Grant~Van Horn, Oisin {Mac Aodha}, Yang Song, Alexander Shepard, Hartwig Adam,
  Pietro Perona, and Serge~J. Belongie.
\newblock The inaturalist challenge 2018 dataset.
\newblock {\em arXiv preprint arXiv:1707.06642}, 2018.

\bibitem{Horn2019INaturalist}
Grant~Van Horn, Oisin {Mac Aodha}, Yang Song, Alexander Shepard, Hartwig Adam,
  Pietro Perona, and Serge~J. Belongie.
\newblock The inaturalist challenge 2019 dataset.
\newblock {\em arXiv preprint arXiv:1707.06642}, 2019.

\bibitem{Hu2018RelationNF}
H. Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Y. Wei.
\newblock Relation networks for object detection.
\newblock {\em Conference on Computer Vision and Pattern Recognition}, 2018.

\bibitem{Huang2016DeepNW}
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em European Conference on Computer Vision}, 2016.

\bibitem{Cars2013}
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em 4th International IEEE Workshop on 3D Representation and
  Recognition (3dRR-13)}, 2013.

\bibitem{Krizhevsky2009LearningML}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, CIFAR, 2009.

\bibitem{Krizhevsky2012AlexNet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em NIPS}, 2012.

\bibitem{li2019visualbert}
Liunian~Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
\newblock {VisualBERT:} a simple and performant baseline for vision and
  language.
\newblock {\em arXiv preprint arXiv:1908.03557}, 2019.

\bibitem{Li2019SelectiveKN}
Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang.
\newblock Selective kernel networks.
\newblock {\em Conference on Computer Vision and Pattern Recognition}, 2019.

\bibitem{Locatello2020ObjectCentricLW}
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran,
  Georg Heigold, Jakob Uszkoreit, A. Dosovitskiy, and Thomas Kipf.
\newblock Object-centric learning with slot attention.
\newblock {\em arXiv preprint arXiv:2006.15055}, 2020.

\bibitem{Loshchilov2017AdamW}
I. Loshchilov and F. Hutter.
\newblock Fixing weight decay regularization in adam.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{Lu2019ViLBERTPT}
Jiasen Lu, Dhruv Batra, D. Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In {\em NIPS}, 2019.

\bibitem{Nilsback08}
M-E. Nilsback and A. Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em Proceedings of the Indian Conference on Computer Vision,
  Graphics and Image Processing}, 2008.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in neural information processing systems}, pages
  8026--8037, 2019.

\bibitem{Radosavovic2020RegNet}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross~B. Girshick, Kaiming He, and
  Piotr Doll{\'a}r.
\newblock Designing network design spaces.
\newblock {\em Conference on Computer Vision and Pattern Recognition}, 2020.

\bibitem{Recht2019ImageNetv2}
B. Recht, Rebecca Roelofs, L. Schmidt, and V. Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock {\em arXiv preprint arXiv:1902.10811}, 2019.

\bibitem{Russakovsky2015ImageNet12}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li Fei-Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of Computer Vision}, 2015.

\bibitem{shen2020global}
Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, and Ching-Hui Chen.
\newblock Global self-attention networks for image recognition.
\newblock {\em arXiv preprint arXiv:2010.03019}, 2020.

\bibitem{Simonyan2015VGG}
K. Simonyan and A. Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{Sun2019VideoBERTAJ}
C. Sun, A. Myers, Carl Vondrick, Kevin Murphy, and C. Schmid.
\newblock Videobert: A joint model for video and language representation
  learning.
\newblock {\em Conference on Computer Vision and Pattern Recognition}, 2019.

\bibitem{sun2017revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 843--852, 2017.

\bibitem{Szegedy2016RethinkingTI}
Christian Szegedy, V. Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock {\em Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc~V. Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock {\em arXiv preprint arXiv:1905.11946}, 2019.

\bibitem{Touvron2020GrafitLF}
Hugo Touvron, Alexandre Sablayrolles, M. Douze, M. Cord, and H. J{\'e}gou.
\newblock Grafit: Learning fine-grained image representations with coarse
  labels.
\newblock {\em arXiv preprint arXiv:2011.12982}, 2020.

\bibitem{Touvron2019FixRes}
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou.
\newblock Fixing the train-test resolution discrepancy.
\newblock {\em NIPS}, 2019.

\bibitem{Touvron2020FixingTT}
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Fixing the train-test resolution discrepancy: Fixefficientnet.
\newblock {\em arXiv preprint arXiv:2003.08237}, 2020.

\bibitem{Vaswani2017AttentionIA}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NIPS}, 2017.

\bibitem{Wang2018NonlocalNN}
X. Wang, Ross~B. Girshick, A. Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock {\em Conference on Computer Vision and Pattern Recognition}, 2018.

\bibitem{Wei2020CircumventingOO}
Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian.
\newblock Circumventing outliers of autoaugment with knowledge distillation.
\newblock {\em European Conference on Computer Vision}, 2020.

\bibitem{pytorchmodels}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{wu2020visual}
Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi
  Tomizuka, Kurt Keutzer, and Peter Vajda.
\newblock Visual transformers: Token-based image representation and processing
  for computer vision.
\newblock {\em arXiv preprint arXiv:2006.03677}, 2020.

\bibitem{Xie2019SelftrainingWN}
Qizhe Xie, Eduard~H. Hovy, Minh-Thang Luong, and Quoc~V. Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock {\em arXiv preprint arXiv:1911.04252}, 2019.

\bibitem{Yuan2019RevisitKD}
L. Yuan, F. Tay, G. Li, T. Wang, and Jiashi Feng.
\newblock Revisit knowledge distillation: a teacher-free framework.
\newblock {\em Conference on Computer Vision and Pattern Recognition}, 2020.

\bibitem{Yun2019CutMix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock {\em arXiv preprint arXiv:1905.04899}, 2019.

\bibitem{Zhang2017Mixup}
Hongyi Zhang, Moustapha Ciss{\'{e}}, Yann~N. Dauphin, and David Lopez{-}Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{zhang2020resnest}
Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue
  Sun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola.
\newblock Resnest: Split-attention networks.
\newblock {\em arXiv preprint arXiv:2004.08955}, 2020.

\bibitem{Zhong2020RandomED}
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang.
\newblock Random erasing data augmentation.
\newblock In {\em AAAI}, 2020.

\end{thebibliography}
