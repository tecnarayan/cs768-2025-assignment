
@inproceedings{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1200--1205},
  year={2017},
  organization={ACM}
}

@article{bergou2019stochastic,
  title={Stochastic Three Points Method for Unconstrained Smooth Minimization},
  author={Bergou, El Houcine and Gorbunov, Eduard and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1902.03591},
  year={2019}
}

@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1-2},
  pages={365--397},
  year={2012},
  publisher={Springer}
}
@article{gadat2018stochastic,
  title={Stochastic heavy ball},
  author={Gadat, S{\'e}bastien and Panloup, Fabien and Saadane, Sofiane and others},
  journal={Electronic Journal of Statistics},
  volume={12},
  number={1},
  pages={461--529},
  year={2018},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}
@article{bubeck2012regret,
  title={Regret analysis of stochastic and nonstochastic multi-armed bandit problems},
  author={Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={5},
  number={1},
  pages={1--122},
  year={2012},
  publisher={Now Publishers, Inc.}
}

@inproceedings{chen2017zoo,
  title={Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
  author={Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},
  booktitle={Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
  pages={15--26},
  year={2017},
  organization={ACM}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}

@inproceedings{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={689--699},
  year={2018}
}

@article{zhou2018nested,
  author  = {Dongruo Zhou and Pan Xu and Quanquan Gu},
  title   = {Stochastic Nested Variance Reduction for Nonconvex Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2020}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{ghadimi2018nonconvexopt,
       author = {{Balasubramanian}, Krishnakumar and {Ghadimi}, Saeed},
        title = "{Zeroth-order Nonconvex Stochastic Optimization: Handling Constraints, High-Dimensionality and Saddle-Points}",
      journal = {arXiv e-prints},
         year = {2018}
}

@article{gorbunov2018accelerated,
  title={An accelerated method for derivative-free smooth stochastic convex optimization},
  author={Gorbunov, Eduard and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:1802.09022},
  year={2018}
}

@article{gorbunov2019stochastic,
  title={A Stochastic Derivative Free Optimization Method with Momentum},
  author={Gorbunov, Eduard and Bibi, Adel and Sener, Ozan and Bergou, El Houcine and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1905.13278},
  year={2019}
}

@inproceedings{ji2019improved,
  title={Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization},
  author={Ji, Kaiyi and Wang, Zhe and Zhou, Yi and Liang, Yingbin},
  booktitle={International Conference on Machine Learning},
  pages={3100--3109},
  year={2019}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{lan2018optimal,
  title={An optimal randomized incremental gradient method},
  author={Lan, Guanghui and Zhou, Yi},
  journal={Mathematical programming},
  volume={171},
  number={1-2},
  pages={167--215},
  year={2018},
  publisher={Springer}
}

@article{lan2019unified,
  title={A unified variance-reduced accelerated gradient method for convex optimization},
  author={Lan, Guanghui and Li, Zhize and Zhou, Yi},
  journal={arXiv preprint arXiv:1905.12412},
  year={2019}
}

@inproceedings{lin2015universal,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  booktitle={Advances in neural information processing systems},
  pages={3384--3392},
  year={2015}
}

@inproceedings{liu2018stochastic,
title = {Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization},
author = {Liu, Sijia and Kailkhura, Bhavya and Chen, Pin-Yu and Ting, Paishun and Chang, Shiyu and Amini, Lisa},
booktitle = {Advances in Neural Information Processing Systems},
pages = {3727--3737},
year = {2018}
}

@article{liu2018stochasticb,
  title={Stochastic zeroth-order optimization via variance reduction method},
  author={Liu, Liu and Cheng, Minhao and Hsieh, Cho-Jui and Tao, Dacheng},
  journal={arXiv preprint arXiv:1805.11811},
  year={2018}
}

@article{matyas1965random,
  title={Random optimization},
  author={Matyas, J},
  journal={Automation and Remote control},
  volume={26},
  number={2},
  pages={246--253},
  year={1965}
}

@article{nelder1965simplex,
  title={A simplex method for function minimization},
  author={Nelder, John A and Mead, Roger},
  journal={The computer journal},
  volume={7},
  number={4},
  pages={308--313},
  year={1965},
  publisher={Oxford University Press}
}

@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. akad. nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}

@book{nesterov2004introbook,
	author = {Nesterov, Yurii},
	title = {Introductory Lectures on Convex Optimization: A Basic Course},
	year = {2014},
	isbn = {1461346916},
	publisher = {Springer Publishing Company, Incorporated},
	edition = {1}
}

@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  number={2},
  pages={527--566},
  year={2011},
  publisher={Springer}
}

@inproceedings{orvieto2019role,
  title={The role of memory in stochastic optimization},
  author={Orvieto, Antonio and Kohler, Jonas and Lucchi, Aurelien},
  booktitle={UAI},
  year={2019}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{polyak1987introduction,
  title={Introduction to Optimization. Optimization Software},
  author={Polyak, Boris T},
  journal={Inc., Publications Division, New York},
  volume={1},
  year={1987}
}

@article{salimans2017evolution,
  title={Evolution strategies as a scalable alternative to reinforcement learning},
  author={Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1703.03864},
  year={2017}
}

@article{su2016differential,
  title={A differential equation for modeling Nesterov's accelerated gradient method: theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel J},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={5312--5354},
  year={2016}
}

@article{fanhua2017simKatyusha,
       author = {{Shang}, Fanhua and {Liu}, Yuanyuan and {Cheng}, James and
         {Zhuo}, Jiacheng},
        title = "{Fast Stochastic Variance Reduced Gradient Method with Momentum Acceleration for Machine Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control, Statistics - Machine Learning},
         year = "2017",
        month = "Mar",
}

@misc{nguyen2017sarah,
    title={SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient},
    author={Lam M. Nguyen and Jie Liu and Katya Scheinberg and Martin Takáč},
    year={2017},
    eprint={1703.00102},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

