\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2018)Anil, Lucas, and Grosse]{Anil2018SortingOL}
C.~Anil, J.~Lucas, and R.~B. Grosse.
\newblock Sorting out lipschitz function approximation.
\newblock In \emph{ICML}, 2018.

\bibitem[Bunel et~al.(2020)Bunel, Lu, Turkaslan, Torr, Kohli, and
  Kumar]{Bunel2020BranchAB}
R.~Bunel, J.~Lu, I.~Turkaslan, P.~H.~S. Torr, P.~Kohli, and M.~P. Kumar.
\newblock Branch and bound for piecewise linear neural network verification.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 42:1--42:39, 2020.

\bibitem[Cao and Gong(2017)]{Cao2017MitigatingEA}
X.~Cao and N.~Z. Gong.
\newblock Mitigating evasion attacks to deep neural networks via region-based
  classification.
\newblock In \emph{Proceedings of the 33rd Annual Computer Security
  Applications Conference}, ACSAC 2017, page 278–287, New York, NY, USA,
  2017. Association for Computing Machinery.
\newblock ISBN 9781450353458.
\newblock \doi{10.1145/3134600.3134606}.
\newblock URL \url{https://doi.org/10.1145/3134600.3134606}.

\bibitem[Ciss{\'{e}} et~al.(2017)Ciss{\'{e}}, Bojanowski, Grave, Dauphin, and
  Usunier]{cisseparseval2017}
M.~Ciss{\'{e}}, P.~Bojanowski, E.~Grave, Y.~N. Dauphin, and N.~Usunier.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning, {ICML} 2017, Sydney, NSW,
  Australia, 6-11 August 2017}, volume~70 of \emph{Proceedings of Machine
  Learning Research}, pages 854--863. {PMLR}, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/cisse17a.html}.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{Cohen2019CertifiedAR}
J.~M. Cohen, E.~Rosenfeld, and J.~Z. Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{ICML}, 2019.

\bibitem[Croce et~al.(2019)Croce, Andriushchenko, and Hein]{croce2019provable}
F.~Croce, M.~Andriushchenko, and M.~Hein.
\newblock Provable robustness of relu networks via maximization of linear
  regions.
\newblock \emph{AISTATS 2019}, 2019.

\bibitem[Cybenko(1989)]{citeulike3561150}
G.~Cybenko.
\newblock {Approximation by superpositions of a sigmoidal function}.
\newblock \emph{Mathematics of Control, Signals, and Systems (MCSS)},
  2\penalty0 (4):\penalty0 303--314, Dec. 1989.
\newblock ISSN 0932-4194.
\newblock \doi{10.1007/BF02551274}.
\newblock URL \url{http://dx.doi.org/10.1007/BF02551274}.

\bibitem[Dugas et~al.(2000)Dugas, Bengio, B\'{e}lisle, Nadeau, and
  Garcia]{NIPS2000_44968aec}
C.~Dugas, Y.~Bengio, F.~B\'{e}lisle, C.~Nadeau, and R.~Garcia.
\newblock Incorporating second-order functional knowledge for better option
  pricing.
\newblock In T.~Leen, T.~Dietterich, and V.~Tresp, editors, \emph{Advances in
  Neural Information Processing Systems}, volume~13. MIT Press, 2000.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2000/file/44968aece94f667e4095002d140b5896-Paper.pdf}.

\bibitem[Dvijotham* et~al.(2018)Dvijotham*, Stanforth, Gowal, Mann, and
  Kohli]{Dvijotham18}
K.~Dvijotham*, R.~Stanforth, S.~Gowal, T.~Mann, and P.~Kohli.
\newblock A dual approach to scalable verification of deep networks.
\newblock In \emph{Proceedings of the Thirty-Fourth Conference Annual
  Conference on Uncertainty in Artificial Intelligence (UAI-18)}, Corvallis,
  Oregon, 2018. AUAI Press.

\bibitem[Dvijotham* et~al.(2020)Dvijotham*, Raghunathan, Uesato, Dathathri,
  Kurakin, Goodfellow, Kohli, Steinhardt, and Liang]{dj2020}
K.~D. Dvijotham*, A.~Raghunathan, J.~Uesato, S.~Dathathri, A.~Kurakin,
  I.~Goodfellow, P.~Kohli, J.~Steinhardt, and P.~Liang.
\newblock Enabling certification of verification-agnostic networks via
  memory-efficient semidefinite programming.
\newblock In \emph{Advances in Neural Information Processing Systems},
  pages~--, 2020.

\bibitem[Gouk et~al.(2020)Gouk, Frank, Pfahringer, and
  Cree]{gouk2020regularisation}
H.~Gouk, E.~Frank, B.~Pfahringer, and M.~J. Cree.
\newblock Regularisation of neural networks by enforcing lipschitz continuity,
  2020.

\bibitem[Gowal et~al.(2019)Gowal, Dvijotham*, Stanforth, Bunel, Qin, Uesato,
  Arandjelovic, Mann, and Kohli]{kulis2009kernelized}
S.~Gowal, K.~Dvijotham*, R.~Stanforth, R.~Bunel, C.~Qin, J.~Uesato,
  R.~Arandjelovic, T.~A. Mann, and P.~Kohli.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock In \emph{ICCV}, volume~9, pages~--, 2019.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~C. Courville.
\newblock Improved training of wasserstein gans.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30, pages 5767--5777. Curran
  Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf}.

\bibitem[Hoogeboom et~al.(2020)Hoogeboom, Satorras, Tomczak, and
  Welling]{Hoogeboom2020TheCE}
E.~Hoogeboom, V.~G. Satorras, J.~Tomczak, and M.~Welling.
\newblock The convolution exponential and generalized sylvester flows.
\newblock \emph{ArXiv}, abs/2006.01910, 2020.

\bibitem[Huang et~al.(2021)Huang, Zhang, Shi, Kolter, and
  Anandkumar]{huang2021training}
Y.~Huang, H.~Zhang, Y.~Shi, J.~Z. Kolter, and A.~Anandkumar.
\newblock Training certifiably robust neural networks with efficient local
  lipschitz bounds.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=FTt28RYj5Pc}.

\bibitem[Jacobsen et~al.(2018)Jacobsen, Smeulders, and
  Oyallon]{jacobsen2018irevnet}
J.-H. Jacobsen, A.~W. Smeulders, and E.~Oyallon.
\newblock i-revnet: Deep invertible networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HJsjkMb0Z}.

\bibitem[Kiani et~al.(2022)Kiani, Balestriero, Lecun, and Lloyd]{projunn2021}
B.~Kiani, R.~Balestriero, Y.~Lecun, and S.~Lloyd.
\newblock projunn: efficient method for training deep networks with unitary
  matrices, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.05483}.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Kumar et~al.(2020{\natexlab{a}})Kumar, Levine, Feizi, and
  Goldstein]{confidence_cert2020}
A.~Kumar, A.~Levine, S.~Feizi, and T.~Goldstein.
\newblock Certifying confidence via randomized smoothing.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 5165--5177. Curran Associates, Inc., 2020{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf}.

\bibitem[Kumar et~al.(2020{\natexlab{b}})Kumar, Levine, Goldstein, and
  Feizi]{cursedimensionalitykumar20}
A.~Kumar, A.~Levine, T.~Goldstein, and S.~Feizi.
\newblock Curse of dimensionality on randomized smoothing for certifiable
  robustness.
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 5458--5467. PMLR,
  13--18 Jul 2020{\natexlab{b}}.
\newblock URL \url{http://proceedings.mlr.press/v119/kumar20b.html}.

\bibitem[L{\'e}cuyer et~al.(2018)L{\'e}cuyer, Atlidakis, Geambasu, Hsu, and
  Jana]{Lcuyer2018CertifiedRT}
M.~L{\'e}cuyer, V.~Atlidakis, R.~Geambasu, D.~Hsu, and S.~K.~K. Jana.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In \emph{IEEE S\&P 2019}, 2018.

\bibitem[Leino and Fredrikson(2021)]{leino2021relaxing}
K.~Leino and M.~Fredrikson.
\newblock Relaxing local robustness.
\newblock In \emph{Neural Information Processing Systems (NIPS)}, 2021.

\bibitem[Leino et~al.(2021)Leino, Wang, and Fredrikson]{leino21gloro}
K.~Leino, Z.~Wang, and M.~Fredrikson.
\newblock Globally-robust neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Levine et~al.(2019)Levine, Singla, and Feizi]{levine2019certifiably}
A.~Levine, S.~Singla, and S.~Feizi.
\newblock Certifiably robust interpretation in deep learning, 2019.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Chen, Wang, and
  Carin]{Li2018CertifiedAR}
B.~Li, C.~Chen, W.~Wang, and L.~Carin.
\newblock Certified adversarial robustness with additive noise.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~Alche-Buc, E.~Fox,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~32, pages 9464--9474. Curran Associates, Inc.,
  2019{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Haque, Anil, Lucas, Grosse, and
  Jacobsen]{li2019lconvnet}
Q.~Li, S.~Haque, C.~Anil, J.~Lucas, R.~Grosse, and J.-H. Jacobsen.
\newblock Preventing gradient attenuation in lipschitz constrained
  convolutional networks.
\newblock \emph{Conference on Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[Liu et~al.(2018)Liu, Cheng, Zhang, and Hsieh]{Liu2018TowardsRN}
X.~Liu, M.~Cheng, H.~Zhang, and C.~Hsieh.
\newblock Towards robust neural networks via random self-ensemble.
\newblock In \emph{ECCV}, 2018.

\bibitem[Long and Sedghi(2020)]{long2019sizefree}
P.~M. Long and H.~Sedghi.
\newblock Generalization bounds for deep convolutional neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=r1e_FpNFDr}.

\bibitem[Lu and Kumar(2020)]{Lu2020Neural}
J.~Lu and M.~P. Kumar.
\newblock Neural network branching for neural network verification.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=B1evfa4tPB}.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1QRgziT-}.

\bibitem[M\"{u}ller et~al.(2021)M\"{u}ller, Serre, Singh, P\"{u}schel, and
  Vechev]{MLSYS2021_ca46c1b9}
C.~M\"{u}ller, F.~Serre, G.~Singh, M.~P\"{u}schel, and M.~Vechev.
\newblock Scaling polyhedral neural network verification on gpus.
\newblock In A.~Smola, A.~Dimakis, and I.~Stoica, editors, \emph{Proceedings of
  Machine Learning and Systems}, volume~3, pages 733--746, 2021.
\newblock URL
  \url{https://proceedings.mlsys.org/paper/2021/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf}.

\bibitem[Palma et~al.(2021)Palma, Behl, Bunel, Torr, and
  Kumar]{palma2021scaling}
A.~D. Palma, H.~Behl, R.~R. Bunel, P.~Torr, and M.~P. Kumar.
\newblock Scaling the convex barrier with active sets.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=uQfOy7LrlTR}.

\bibitem[Qian and Wegman(2019)]{qian2018lnonexpansive}
H.~Qian and M.~N. Wegman.
\newblock L2-nonexpansive neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ByxGSsR9FQ}.

\bibitem[Raghunathan et~al.(2018)Raghunathan, Steinhardt, and
  Liang]{Raghunathan2018SemidefiniteRF}
A.~Raghunathan, J.~Steinhardt, and P.~Liang.
\newblock Semidefinite relaxations for certifying robustness to adversarial
  examples.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Salman et~al.(2019{\natexlab{a}})Salman, Li, Razenshteyn, Zhang,
  Zhang, Bubeck, and Yang]{Salman2019ProvablyRD}
H.~Salman, J.~Li, I.~Razenshteyn, P.~Zhang, H.~Zhang, S.~Bubeck, and G.~Yang.
\newblock Provably robust deep learning via adversarially trained smoothed
  classifiers.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32, pages 11292--11303. Curran
  Associates, Inc., 2019{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf}.

\bibitem[Salman et~al.(2019{\natexlab{b}})Salman, Yang, Zhang, Hsieh, and
  Zhang]{NEURIPS2019_246a3c55}
H.~Salman, G.~Yang, H.~Zhang, C.-J. Hsieh, and P.~Zhang.
\newblock A convex relaxation barrier to tight robustness verification of
  neural networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.,
  2019{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/246a3c5544feb054f3ea718f61adfa16-Paper.pdf}.

\bibitem[Salman et~al.(2020)Salman, Sun, Yang, Kapoor, and Kolter]{salman2020}
H.~Salman, M.~Sun, G.~Yang, A.~Kapoor, and J.~Z. Kolter.
\newblock Denoised smoothing: A provable defense for pretrained classifiers.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Salman et~al.(2021)Salman, Jain, Wong, and Madry]{certpatchsalman2021}
H.~Salman, S.~Jain, E.~Wong, and A.~Madry.
\newblock Certified patch robustness via smoothed vision transformers.
\newblock \emph{CoRR}, abs/2110.07719, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.07719}.

\bibitem[Sedghi et~al.(2019)Sedghi, Gupta, and Long]{sedghi2018singular}
H.~Sedghi, V.~Gupta, and P.~M. Long.
\newblock The singular values of convolutional layers.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJevYoA9Fm}.

\bibitem[Singh et~al.(2017)Singh, P\"{u}schel, and Vechev]{polyhedradomain2017}
G.~Singh, M.~P\"{u}schel, and M.~Vechev.
\newblock Fast polyhedra abstract domain.
\newblock In \emph{Proceedings of the 44th ACM SIGPLAN Symposium on Principles
  of Programming Languages}, POPL 2017, page 46–59, New York, NY, USA, 2017.
  Association for Computing Machinery.
\newblock ISBN 9781450346603.
\newblock \doi{10.1145/3009837.3009885}.
\newblock URL \url{https://doi.org/10.1145/3009837.3009885}.

\bibitem[Singh et~al.(2018{\natexlab{a}})Singh, Gehr, Mirman, P\"{u}schel, and
  Vechev]{NEURIPS2018_f2f44698}
G.~Singh, T.~Gehr, M.~Mirman, M.~P\"{u}schel, and M.~Vechev.
\newblock Fast and effective robustness certification.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/f2f446980d8e971ef3da97af089481c3-Paper.pdf}.

\bibitem[Singh et~al.(2018{\natexlab{b}})Singh, Gehr, Mirman, Puschel, and
  Vechev]{Singh2018FastAE}
G.~Singh, T.~Gehr, M.~Mirman, M.~Puschel, and M.~T. Vechev.
\newblock Fast and effective robustness certification.
\newblock In \emph{NeurIPS}, 2018{\natexlab{b}}.

\bibitem[Singh et~al.(2019{\natexlab{a}})Singh, Ganvir, P\"{u}schel, and
  Vechev]{NEURIPS2019_0a9fdbb1}
G.~Singh, R.~Ganvir, M.~P\"{u}schel, and M.~Vechev.
\newblock Beyond the single neuron convex barrier for neural network
  certification.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.,
  2019{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf}.

\bibitem[Singh et~al.(2019{\natexlab{b}})Singh, Gehr, P\"{u}schel, and
  Vechev]{abstractdomain2019}
G.~Singh, T.~Gehr, M.~P\"{u}schel, and M.~Vechev.
\newblock An abstract domain for certifying neural networks.
\newblock \emph{Proc. ACM Program. Lang.}, 3\penalty0 (POPL), jan
  2019{\natexlab{b}}.
\newblock \doi{10.1145/3290354}.
\newblock URL \url{https://doi.org/10.1145/3290354}.

\bibitem[Singh et~al.(2019{\natexlab{c}})Singh, Gehr, Püschel, and
  Vechev]{singh2018robustness}
G.~Singh, T.~Gehr, M.~Püschel, and M.~Vechev.
\newblock Robustness certification with refinement.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=HJgeEh09KQ}.

\bibitem[Singh et~al.(2021)Singh, Kumar, Torr, and
  Dvijotham]{singh2021overcoming}
H.~Singh, M.~P. Kumar, P.~Torr, and K.~D. Dvijotham.
\newblock Overcoming the convex barrier for simplex inputs.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=JXREUkyHi7u}.

\bibitem[Singla and Feizi(2020)]{2020curvaturebased}
S.~Singla and S.~Feizi.
\newblock Second-order provable defenses against adversarial attacks.
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 8981--8991. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/singla20a.html}.

\bibitem[Singla and Feizi(2021)]{singlafeizi2021}
S.~Singla and S.~Feizi.
\newblock Skew orthogonal convolutions.
\newblock In \emph{ICML}, 2021.
\newblock URL \url{https://arxiv.org/abs/2105.11417}.

\bibitem[Singla et~al.(2022)Singla, Singla, and Feizi]{singla2022improved}
S.~Singla, S.~Singla, and S.~Feizi.
\newblock Improved deterministic l2 robustness on {CIFAR}-10 and {CIFAR}-100.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=tD7eCtaSkR}.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{sinha2018certifiable}
A.~Sinha, H.~Namkoong, and J.~Duchi.
\newblock Certifiable distributional robustness with principled adversarial
  training.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=Hk6kPgZA-}.

\bibitem[Su et~al.(2022)Su, Byeon, and Huang]{paraunitary2021}
J.~Su, W.~Byeon, and F.~Huang.
\newblock Scaling-up diverse orthogonal convolutional networks with a
  paraunitary framework.
\newblock In \emph{ICML}, 2022.
\newblock URL \url{https://arxiv.org/abs/2106.09121}.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{42503}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6199}.

\bibitem[Trockman and Kolter(2021)]{trockman2021orthogonalizing}
A.~Trockman and J.~Z. Kolter.
\newblock Orthogonalizing convolutional layers with the cayley transform.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Pbj8H_jEHYv}.

\bibitem[Tsipras et~al.(2018)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2019robustness}
D.~Tsipras, S.~Santurkar, L.~Engstrom, A.~Turner, and A.~Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{ICLR}, 2018.

\bibitem[Tsuzuku et~al.(2018)Tsuzuku, Sato, and
  Sugiyama]{Tsuzuku2018LipschitzMarginTS}
Y.~Tsuzuku, I.~Sato, and M.~Sugiyama.
\newblock Lipschitz-margin training: Scalable certification of perturbation
  invariance for deep neural networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Villani(2008)]{villani2012optimal}
C.~Villani.
\newblock Optimal transport, old and new, 2008.

\bibitem[Wang et~al.(2021)Wang, Zhang, Xu, Lin, Jana, Hsieh, and
  Kolter]{wang2021betacrown}
S.~Wang, H.~Zhang, K.~Xu, X.~Lin, S.~Jana, C.-J. Hsieh, and J.~Z. Kolter.
\newblock Beta-{CROWN}: Efficient bound propagation with per-neuron split
  constraints for neural network robustness verification.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, editors,
  \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ahYIlRBeCFw}.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Song, Hsieh, Daniel, Boning, and
  Dhillon]{weng2018CertifiedRobustness}
L.~Weng, H.~Zhang, H.~Chen, Z.~Song, C.-J. Hsieh, L.~Daniel, D.~Boning, and
  I.~Dhillon.
\newblock Towards fast computation of certified robustness for {R}e{LU}
  networks.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 5276--5285. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/weng18a.html}.

\bibitem[Wong and Kolter(2018)]{Wong2017ProvableDA}
E.~Wong and Z.~Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 5286--5295, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/wong18a.html}.

\bibitem[Wong et~al.(2018)Wong, Schmidt, Metzen, and Kolter]{Wong2018ScalingPA}
E.~Wong, F.~R. Schmidt, J.~H. Metzen, and J.~Z. Kolter.
\newblock Scaling provable adversarial defenses.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
L.~Xiao, Y.~Bahri, J.~Sohl-Dickstein, S.~Schoenholz, and J.~Pennington.
\newblock Dynamical isometry and a mean field theory of {CNN}s: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 5393--5402, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/xiao18a.html}.

\bibitem[Yu et~al.(2022)Yu, Li, CAI, and Li]{yu2022constructing}
T.~Yu, J.~Li, Y.~CAI, and P.~Li.
\newblock Constructing orthogonal convolutions in an explicit manner.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Zr5W2LSRhD}.

\bibitem[Zhang et~al.(2021)Zhang, Cai, Lu, He, and Wang]{linfty2021}
B.~Zhang, T.~Cai, Z.~Lu, D.~He, and L.~Wang.
\newblock Towards certifying linfinity robustness using neural networks with
  linfinity-dist neurons.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Jiang, He, and Wang]{zhang2022boosting}
B.~Zhang, D.~Jiang, D.~He, and L.~Wang.
\newblock Boosting the certified robustness of l-infinity distance nets.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Q76Y7wkiji}.

\bibitem[Zhang et~al.(2018)Zhang, Weng, Chen, Hsieh, and
  Daniel]{zhang2018crown}
H.~Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L.~Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS),
  arXiv preprint arXiv:1811.00866}, dec 2018.

\bibitem[Zhang et~al.(2019)Zhang, Zhang, and Hsieh]{zhang2018recurjac}
H.~Zhang, P.~Zhang, and C.-J. Hsieh.
\newblock Recurjac: An efficient recursive algorithm for bounding jacobian
  matrix of neural networks and its applications.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI), arXiv
  preprint arXiv:1810.11783}, dec 2019.

\end{thebibliography}
