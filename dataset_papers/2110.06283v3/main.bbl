\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2016)Agarwal, Podchiyska, Banda, Goel, Leung, Minty,
  Sweeney, Gyang, and Shah]{agarwal2016learning}
Agarwal, V., Podchiyska, T., Banda, J.~M., Goel, V., Leung, T.~I., Minty,
  E.~P., Sweeney, T.~E., Gyang, E., and Shah, N.~H.
\newblock Learning statistical models of phenotypes using noisy labeled
  training data.
\newblock \emph{Journal of the American Medical Informatics Association},
  23\penalty0 (6):\penalty0 1166--1173, 2016.

\bibitem[Amid et~al.(2019)Amid, Warmuth, Anil, and Koren]{amid2019robust}
Amid, E., Warmuth, M.~K., Anil, R., and Koren, T.
\newblock Robust bi-tempered logistic loss based on bregman divergences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  14987--14996, 2019.

\bibitem[Bahri et~al.(2020)Bahri, Jiang, and Gupta]{bahri2020deep}
Bahri, D., Jiang, H., and Gupta, M.
\newblock Deep k-nn for noisy labels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  540--550. PMLR, 2020.

\bibitem[Bai \& Liu(2021)Bai and Liu]{bai2021me}
Bai, Y. and Liu, T.
\newblock Me-momentum: Extracting hard confident examples from noisily labeled
  data.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9312--9321, 2021.

\bibitem[Bai et~al.(2021)Bai, Yang, Han, Yang, Li, Mao, Niu, and
  Liu]{bai2021understanding}
Bai, Y., Yang, E., Han, B., Yang, Y., Li, J., Mao, Y., Niu, G., and Liu, T.
\newblock Understanding and improving early stopping for learning with noisy
  labels.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and
  Raffel, C.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Buldygin \& Kozachenko(1980)Buldygin and Kozachenko]{buldygin1980sub}
Buldygin, V.~V. and Kozachenko, Y.~V.
\newblock Sub-gaussian random variables.
\newblock \emph{Ukrainian Mathematical Journal}, 32\penalty0 (6):\penalty0
  483--489, 1980.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Cheng et~al.(2021{\natexlab{a}})Cheng, Zhu, Li, Gong, Sun, and
  Liu]{cheng2021learningsieve}
Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., and Liu, Y.
\newblock Learning with instance-dependent label noise: A sample sieve
  approach.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=2VXyy9mIyU3}.

\bibitem[Cheng et~al.(2021{\natexlab{b}})Cheng, Zhu, Sun, and
  Liu]{cheng2021demystifying}
Cheng, H., Zhu, Z., Sun, X., and Liu, Y.
\newblock Demystifying how self-supervised features improve training from noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2110.09022}, 2021{\natexlab{b}}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics}, pp.\  4171--4186,
  June 2019.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Dua \& Graff(2017)Dua and Graff]{dua2017uci}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Feng et~al.(2021)Feng, Shu, Lin, Lv, Li, and An]{feng2021can}
Feng, L., Shu, S., Lin, Z., Lv, F., Li, L., and An, B.
\newblock Can cross entropy loss be robust to label noise?
\newblock In \emph{Proceedings of the Twenty-Ninth International Conference on
  International Joint Conferences on Artificial Intelligence}, pp.\
  2206--2212, 2021.

\bibitem[Gao et~al.(2016)Gao, Yang, and Zhou]{gao2016resistance}
Gao, W., Yang, B.-B., and Zhou, Z.-H.
\newblock On the resistance of nearest neighbor to random noisy labels.
\newblock \emph{arXiv preprint arXiv:1607.07526}, 2016.

\bibitem[Ghosh \& Lan(2021)Ghosh and Lan]{ghosh2021contrastive}
Ghosh, A. and Lan, A.
\newblock Contrastive learning improves model robustness under label noise.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  2703--2708, 2021.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Ghosh, A., Kumar, H., and Sastry, P.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\bibitem[Gong et~al.(2018)Gong, Li, Meng, Miao, and Liu]{gong2018decomposition}
Gong, M., Li, H., Meng, D., Miao, Q., and Liu, J.
\newblock Decomposition-based evolutionary multiobjective optimization to
  self-paced learning.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 23\penalty0
  (2):\penalty0 288--302, 2018.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8527--8537, 2018.

\bibitem[Han et~al.(2020)Han, Yao, Liu, Niu, Tsang, Kwok, and
  Sugiyama]{han2020survey}
Han, B., Yao, Q., Liu, T., Niu, G., Tsang, I.~W., Kwok, J.~T., and Sugiyama, M.
\newblock A survey of label-noise representation learning: Past, present and
  future.
\newblock \emph{arXiv preprint arXiv:2011.04406}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9729--9738, 2020.

\bibitem[Huang et~al.(2019)Huang, Qu, Jia, and Zhao]{huang2019o2u}
Huang, J., Qu, L., Jia, R., and Zhao, B.
\newblock O2u-net: A simple noisy label detection approach for deep neural
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  3326--3334, 2019.

\bibitem[Jaiswal et~al.(2021)Jaiswal, Babu, Zadeh, Banerjee, and
  Makedon]{jaiswal2021survey}
Jaiswal, A., Babu, A.~R., Zadeh, M.~Z., Banerjee, D., and Makedon, F.
\newblock A survey on contrastive self-supervised learning.
\newblock \emph{Technologies}, 9\penalty0 (1):\penalty0 2, 2021.

\bibitem[Ji et~al.(2019)Ji, Henriques, and Vedaldi]{ji2019invariant}
Ji, X., Henriques, J.~F., and Vedaldi, A.
\newblock Invariant information clustering for unsupervised image
  classification and segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9865--9874, 2019.

\bibitem[Jiang et~al.(2018)Jiang, Kim, Guan, and Gupta]{jiang2018trust}
Jiang, H., Kim, B., Guan, M., and Gupta, M.
\newblock To trust or not to trust a classifier.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jiang et~al.(2020)Jiang, Huang, Liu, and Yang]{jiang2020beyond}
Jiang, L., Huang, D., Liu, M., and Yang, W.
\newblock Beyond synthetic noise: Deep learning on controlled noisy labels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4804--4815. PMLR, 2020.

\bibitem[Jiang et~al.(2022)Jiang, Zhou, Liu, Li, Chen, Choi, and
  Hu]{jiang2022an}
Jiang, Z., Zhou, K., Liu, Z., Li, L., Chen, R., Choi, S.-H., and Hu, X.
\newblock An information fusion approach to learning with instance-dependent
  label noise.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=ecH2FKaARUp}.

\bibitem[Karger et~al.(2011)Karger, Oh, and Shah]{karger2011iterative}
Karger, D., Oh, S., and Shah, D.
\newblock Iterative learning for reliable crowdsourcing systems.
\newblock In \emph{Neural Information Processing Systems}, NIPS '11, 2011.

\bibitem[Karger et~al.(2013)Karger, Oh, and Shah]{karger2013efficient}
Karger, D.~R., Oh, S., and Shah, D.
\newblock Efficient crowdsourcing for multi-class labeling.
\newblock In \emph{Proceedings of the ACM SIGMETRICS/international conference
  on Measurement and modeling of computer systems}, pp.\  81--92, 2013.

\bibitem[Kong et~al.(2020)Kong, Li, Wang, Rezaei, and Zhou]{kong2020knn}
Kong, S., Li, Y., Wang, J., Rezaei, A., and Zhou, H.
\newblock Knn-enhanced deep learning against noisy labels.
\newblock \emph{arXiv preprint arXiv:2012.04224}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Socher, and Hoi]{Li2020DivideMix}
Li, J., Socher, R., and Hoi, S.~C.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=HJgExaVtwr}.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Zhang, Xu, Dickerson, and
  Ba]{li2021does}
Li, J., Zhang, M., Xu, K., Dickerson, J., and Ba, J.
\newblock How does a neural network's architecture impact its robustness to
  noisy labels?
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Soltanolkotabi, and
  Oymak]{li2020gradient}
Li, M., Soltanolkotabi, M., and Oymak, S.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pp.\  4313--4324. PMLR, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Liu, Han, Niu, and
  Sugiyama]{li2021provably}
Li, X., Liu, T., Han, B., Niu, G., and Sugiyama, M.
\newblock Provably end-to-end label-noise learning without anchor points.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6403--6413. PMLR, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Li, and Lee]{liu2021tera}
Liu, A.~T., Li, S.-W., and Lee, H.-y.
\newblock Tera: Self-supervised learning of transformer encoder representation
  for speech.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 29:\penalty0 2351--2366, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Haghgoo, Chen, Raghunathan, Koh,
  Sagawa, Liang, and Finn]{liu2021just}
Liu, E.~Z., Haghgoo, B., Chen, A.~S., Raghunathan, A., Koh, P.~W., Sagawa, S.,
  Liang, P., and Finn, C.
\newblock Just train twice: Improving group robustness without training group
  information.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6781--6792. PMLR, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2012)Liu, Peng, and Ihler]{liu2012variational}
Liu, Q., Peng, J., and Ihler, A.
\newblock Variational inference for crowdsourcing.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems-Volume 1}, pp.\  692--700, 2012.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and
  Fernandez-Granda]{liu2020early}
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20331--20342, 2020.

\bibitem[Liu \& Tao(2015)Liu and Tao]{liu2015classification}
Liu, T. and Tao, D.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2015.

\bibitem[Liu et~al.(2021{\natexlab{c}})Liu, Wang, Shen, and
  Tsang]{liu2021emerging}
Liu, W., Wang, H., Shen, X., and Tsang, I.
\newblock The emerging trends of multi-label learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2021{\natexlab{c}}.

\bibitem[Liu(2021)]{liu2021importance}
Liu, Y.
\newblock Understanding instance-level label noise: Disparate impacts and
  treatments.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6725--6735. PMLR, 2021.

\bibitem[Liu(2022)]{liu2022identifiability}
Liu, Y.
\newblock Identifiability of label noise transition matrix.
\newblock \emph{arXiv preprint arXiv:2202.02016}, 2022.

\bibitem[Liu \& Guo(2020)Liu and Guo]{liu2019peer}
Liu, Y. and Guo, H.
\newblock Peer loss functions: Learning from noisy labels without knowing noise
  rates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6226--6236. PMLR, 2020.

\bibitem[Liu \& Liu(2015)Liu and Liu]{liu2015online}
Liu, Y. and Liu, M.
\newblock An online learning approach to improving the quality of
  crowd-sourcing.
\newblock \emph{ACM SIGMETRICS Performance Evaluation Review}, 43\penalty0
  (1):\penalty0 217--230, 2015.

\bibitem[Liu \& Wang(2021)Liu and Wang]{liu2021can}
Liu, Y. and Wang, J.
\newblock Can less be more? when increasing-to-balancing label noise rates
  considered beneficial.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Luo et~al.(2021)Luo, Cheng, Meng, Gao, Li, Zhang, and
  Sun]{luo2021empirical}
Luo, H., Cheng, H., Meng, F., Gao, Y., Li, K., Zhang, M., and Sun, X.
\newblock An empirical study and analysis on open-set semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2101.08237}, 2021.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A.
\newblock Learning with noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1196--1204, 2013.

\bibitem[Northcutt et~al.(2021{\natexlab{a}})Northcutt, Jiang, and
  Chuang]{northcutt2021confident}
Northcutt, C., Jiang, L., and Chuang, I.
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock \emph{Journal of Artificial Intelligence Research}, 70:\penalty0
  1373--1411, 2021{\natexlab{a}}.

\bibitem[Northcutt et~al.(2021{\natexlab{b}})Northcutt, Athalye, and
  Mueller]{northcutt2021pervasive}
Northcutt, C.~G., Athalye, A., and Mueller, J.
\newblock Pervasive label errors in test sets destabilize machine learning
  benchmarks.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=XccDXrDNLek}.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1944--1952, 2017.

\bibitem[Pruthi et~al.(2020)Pruthi, Liu, Kale, and
  Sundararajan]{pruthi2020estimating}
Pruthi, G., Liu, F., Kale, S., and Sundararajan, M.
\newblock Estimating training data influence by tracing gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  19920--19930, 2020.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Reeve \& Kab{\'a}n(2019)Reeve and Kab{\'a}n]{reeve2019fast}
Reeve, H. and Kab{\'a}n, A.
\newblock Fast rates for a knn classifier robust to unknown asymmetric label
  noise.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5401--5409. PMLR, 2019.

\bibitem[Shu et~al.(2020)Shu, Zhao, Chen, Xu, and Meng]{shu2020learning}
Shu, J., Zhao, Q., Chen, K., Xu, Z., and Meng, D.
\newblock Learning adaptive loss for robust learning with noisy labels.
\newblock \emph{arXiv preprint arXiv:2002.06482}, 2020.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Xiao, Li, Feng, Niu, Chen, and
  Zhao]{wang2022pico}
Wang, H., Xiao, R., Li, Y., Feng, L., Niu, G., Chen, G., and Zhao, J.
\newblock Pi{CO}: Contrastive label disambiguation for partial label learning.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=EhYjZy6e1gJ}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Guo, Zhu, and
  Liu]{wang2021policy}
Wang, J., Guo, H., Zhu, Z., and Liu, Y.
\newblock Policy learning using weak supervision.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Liu, and Levy]{wang2021fair}
Wang, J., Liu, Y., and Levy, C.
\newblock Fair classification with group-dependent label noise.
\newblock In \emph{Proceedings of the 2021 ACM conference on fairness,
  accountability, and transparency}, pp.\  526--536, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Wang, and
  Liu]{wang2022understanding}
Wang, J., Wang, X.~E., and Liu, Y.
\newblock Understanding instance-level impact of fairness constraints.
\newblock In \emph{International Conference on Machine Learning}. PMLR,
  2022{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and
  Bailey]{wang2019symmetric}
Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., and Bailey, J.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  322--330, 2019.

\bibitem[Wang et~al.(2020)Wang, Jiang, Han, Feng, An, Niu, and
  Long]{wang2020seminll}
Wang, Z., Jiang, J., Han, B., Feng, L., An, B., Niu, G., and Long, G.
\newblock Seminll: A framework of noisy-label learning by semi-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2012.00925}, 2020.

\bibitem[Wei et~al.(2020)Wei, Feng, Chen, and An]{wei2020combating}
Wei, H., Feng, L., Chen, X., and An, B.
\newblock Combating noisy labels by agreement: A joint training method with
  co-regularization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13726--13735, 2020.

\bibitem[Wei et~al.(2021{\natexlab{a}})Wei, Tao, Xie, and An]{wei2021open}
Wei, H., Tao, L., Xie, R., and An, B.
\newblock Open-set label noise can improve robustness against inherent label
  noise.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Tao, Xie, Feng, and
  An]{wei2022open}
Wei, H., Tao, L., Xie, R., Feng, L., and An, B.
\newblock Open-sampling: Exploring out-of-distribution data for re-balancing
  long-tailed datasets.
\newblock In \emph{International Conference on Machine Learning (ICML)}. PMLR,
  2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Xie, Cheng, Feng, An, and
  Li]{wei2022mitigating}
Wei, H., Xie, R., Cheng, H., Feng, L., An, B., and Li, Y.
\newblock Mitigating neural network overconfidence with logit normalization.
\newblock \emph{arXiv preprint arXiv:2205.09310}, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2022{\natexlab{c}})Wei, Xie, Feng, Han, and
  An]{wei2022deep}
Wei, H., Xie, R., Feng, L., Han, B., and An, B.
\newblock Deep learning from multiple noisy annotators as a union.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022{\natexlab{c}}.

\bibitem[Wei \& Liu(2020)Wei and Liu]{wei2020optimizing}
Wei, J. and Liu, Y.
\newblock When optimizing $ f $-divergence is robust with label noise.
\newblock \emph{arXiv preprint arXiv:2011.03687}, 2020.

\bibitem[Wei et~al.(2021{\natexlab{b}})Wei, Liu, Liu, Niu, Sugiyama, and
  Liu]{wei2021understanding}
Wei, J., Liu, H., Liu, T., Niu, G., Sugiyama, M., and Liu, Y.
\newblock To smooth or not? when label smoothing meets noisy labels,
  2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2106.04149}.

\bibitem[Wei et~al.(2022{\natexlab{d}})Wei, Zhu, Cheng, Liu, Niu, and
  Liu]{wei2022learning}
Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., and Liu, Y.
\newblock Learning with noisy labels revisited: A study using real-world human
  annotations.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{d}}.
\newblock URL \url{https://openreview.net/forum?id=TBWA6PLJZQm}.

\bibitem[Wei et~al.(2022{\natexlab{e}})Wei, Zhu, Luo, Amid, Kumar, and
  Liu]{wei2022aggregate}
Wei, J., Zhu, Z., Luo, T., Amid, E., Kumar, A., and Liu, Y.
\newblock To aggregate or not? {L}earning with separate noisy labels,
  2022{\natexlab{e}}.
\newblock URL \url{https://arxiv.org/abs/2206.07181}.

\bibitem[Xia et~al.(2019)Xia, Liu, Wang, Han, Gong, Niu, and
  Sugiyama]{xia2019anchor}
Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6838--6849, 2019.

\bibitem[Xia et~al.(2020{\natexlab{a}})Xia, Liu, Han, Wang, Deng, Li, and
  Mao]{xia2020extended}
Xia, X., Liu, T., Han, B., Wang, N., Deng, J., Li, J., and Mao, Y.
\newblock Extended {T}: Learning with mixed closed-set and open-set noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2012.00932}, 2020{\natexlab{a}}.

\bibitem[Xia et~al.(2020{\natexlab{b}})Xia, Liu, Han, Wang, Gong, Liu, Niu,
  Tao, and Sugiyama]{xia2020parts}
Xia, X., Liu, T., Han, B., Wang, N., Gong, M., Liu, H., Niu, G., Tao, D., and
  Sugiyama, M.
\newblock Part-dependent label noise: Towards instance-dependent label noise.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7597--7610, 2020{\natexlab{b}}.

\bibitem[Xia et~al.(2021)Xia, Liu, Han, Gong, Wang, Ge, and
  Chang]{xia2021robust}
Xia, X., Liu, T., Han, B., Gong, C., Wang, N., Ge, Z., and Chang, Y.
\newblock Robust early-learning: Hindering the memorization of noisy labels.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2691--2699, 2015.

\bibitem[Xie et~al.(2019)Xie, Dai, Hovy, Luong, and Le]{xie2019unsupervised}
Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q.~V.
\newblock Unsupervised data augmentation.
\newblock \emph{arXiv preprint arXiv:1904.12848}, 2019.

\bibitem[Yao et~al.(2020)Yao, Yang, Han, Niu, and Kwok]{yao2020searching}
Yao, Q., Yang, H., Han, B., Niu, G., and Kwok, J.~T.
\newblock Searching to exploit memorization effect in learning with noisy
  labels.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML '20, 2020.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama, M.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pp.\  7164--7173. PMLR, 09--15 Jun 2019.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=r1Ddp1-Rb}.

\bibitem[Zhang et~al.(2017)Zhang, Sheng, Li, and Wu]{zhang2017improving}
Zhang, J., Sheng, V.~S., Li, T., and Wu, X.
\newblock Improving crowdsourced label quality using noise correction.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  29\penalty0 (5):\penalty0 1675--1688, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Lee, and Agarwal]{zhang2021learning}
Zhang, M., Lee, J., and Agarwal, S.
\newblock Learning from noisy labels with no change to the training process.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12468--12478. PMLR, 2021.

\bibitem[Zhang et~al.(2014)Zhang, Chen, Zhou, and Jordan]{zhang2014spectral}
Zhang, Y., Chen, X., Zhou, D., and Jordan, M.~I.
\newblock Spectral methods meet em: A provably optimal algorithm for
  crowdsourcing.
\newblock \emph{Advances in neural information processing systems},
  27:\penalty0 1260--1268, 2014.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8778--8788, 2018.

\bibitem[Zhu et~al.(2021{\natexlab{a}})Zhu, Liu, and Liu]{zhu2020second}
Zhu, Z., Liu, T., and Liu, Y.
\newblock A second-order approach to learning with instance-dependent label
  noise.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2021{\natexlab{a}}.

\bibitem[Zhu et~al.(2021{\natexlab{b}})Zhu, Song, and
  Liu]{zhu2021clusterability}
Zhu, Z., Song, Y., and Liu, Y.
\newblock Clusterability as an alternative to anchor points when learning with
  noisy labels.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, ICML '21, 2021{\natexlab{b}}.

\bibitem[Zhu et~al.(2021{\natexlab{c}})Zhu, Zhu, Liu, and
  Liu]{zhu2021federated}
Zhu, Z., Zhu, J., Liu, J., and Liu, Y.
\newblock Federated bandit: A gossiping approach.
\newblock In \emph{Abstract Proceedings of the 2021 ACM
  SIGMETRICS/International Conference on Measurement and Modeling of Computer
  Systems}, pp.\  3--4, 2021{\natexlab{c}}.

\bibitem[Zhu et~al.(2022{\natexlab{a}})Zhu, Luo, and Liu]{zhu2022the}
Zhu, Z., Luo, T., and Liu, Y.
\newblock The rich get richer: Disparate impact of semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=DXPftn5kjQK}.

\bibitem[Zhu et~al.(2022{\natexlab{b}})Zhu, Wang, and Liu]{zhu2022beyond}
Zhu, Z., Wang, J., and Liu, Y.
\newblock Beyond images: Label noise transition matrix estimation for tasks
  with lower-quality features.
\newblock \emph{arXiv preprint arXiv:2202.01273}, 2022{\natexlab{b}}.

\end{thebibliography}
