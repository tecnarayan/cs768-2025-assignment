\begin{thebibliography}{10}

\bibitem{amari2009divergence}
Shun-ichi Amari.
\newblock Divergence function, information monotonicity and information
  geometry.
\newblock In {\em Workshop on information theoretic methods in science and
  engineering (WITMSE)}. Citeseer, 2009.

\bibitem{arbel2020kale}
Michael Arbel, Liang Zhou, and Arthur Gretton.
\newblock Kale: When energy-based learning meets adversarial training.
\newblock {\em arXiv preprint arXiv:2003.05033}, 2020.

\bibitem{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein gan.
\newblock {\em arXiv preprint arXiv:1701.07875}, 2017.

\bibitem{belghazi2018mine}
Mohamed~Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua
  Bengio, Aaron Courville, and R~Devon Hjelm.
\newblock Mine: mutual information neural estimation.
\newblock {\em arXiv preprint arXiv:1801.04062}, 2018.

\bibitem{borgwardt2006integrating}
Karsten~M Borgwardt, Arthur Gretton, Malte~J Rasch, Hans-Peter Kriegel,
  Bernhard Sch{\"o}lkopf, and Alex~J Smola.
\newblock Integrating structured biological data by kernel maximum mean
  discrepancy.
\newblock {\em Bioinformatics}, 22(14):e49--e57, 2006.

\bibitem{bregman1967relaxation}
Lev~M Bregman.
\newblock The relaxation method of finding the common point of convex sets and
  its application to the solution of problems in convex programming.
\newblock {\em USSR computational mathematics and mathematical physics},
  7(3):200--217, 1967.

\bibitem{burda2015importance}
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov.
\newblock Importance weighted autoencoders.
\newblock {\em arXiv preprint arXiv:1509.00519}, 2015.

\bibitem{chen2018stochastic}
Jie Chen and Ronny Luss.
\newblock Stochastic gradient descent with biased but consistent gradient
  estimators.
\newblock {\em arXiv preprint arXiv:1807.11880}, 2018.

\bibitem{cimpoi2014describing}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3606--3613, 2014.

\bibitem{csiszar1964informationstheoretische}
Imre Csisz{\'a}r.
\newblock Eine informationstheoretische ungleichung und ihre anwendung auf
  beweis der ergodizitaet von markoffschen ketten.
\newblock {\em Magyer Tud. Akad. Mat. Kutato Int. Koezl.}, 8:85--108, 1964.

\bibitem{dai2018kernel}
Bo~Dai, Hanjun Dai, Arthur Gretton, Le~Song, Dale Schuurmans, and Niao He.
\newblock Kernel exponential family estimation via doubly dual embedding.
\newblock {\em arXiv preprint arXiv:1811.02228}, 2018.

\bibitem{dai2019exponential}
Bo~Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le~Song, and Dale
  Schuurmans.
\newblock Exponential family estimation via adversarial dynamics embedding.
\newblock {\em arXiv preprint arXiv:1904.12083}, 2019.

\bibitem{dawid1998coherent}
A~Philip Dawid.
\newblock Coherent measures of discrepancy, uncertainty and dependence, with
  applications to bayesian predictive experimental design.
\newblock {\em Department of Statistical Science, University College London.
  http://www. ucl. ac. uk/Stats/research/abs94. html, Tech. Rep}, 139, 1998.

\bibitem{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock Nice: Non-linear independent components estimation.
\newblock {\em arXiv preprint arXiv:1410.8516}, 2014.

\bibitem{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}, 2016.

\bibitem{du2019model}
Yilun Du, Toru Lin, and Igor Mordatch.
\newblock Model based planning with energy based models.
\newblock {\em arXiv preprint arXiv:1909.06878}, 2019.

\bibitem{du2019implicit}
Yilun Du and Igor Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  3603--3613, 2019.

\bibitem{finke2019importance}
Axel Finke and Alexandre~H Thiery.
\newblock On importance-weighted autoencoders.
\newblock {\em arXiv preprint arXiv:1907.10477}, 2019.

\bibitem{friedman1983effective}
Daniel Friedman.
\newblock Effective scoring rules for probabilistic forecasts.
\newblock {\em Management Science}, 29(4):447--454, 1983.

\bibitem{fujisawa2008robust}
Hironori Fujisawa and Shinto Eguchi.
\newblock Robust parameter estimation with a small bias against heavy
  contamination.
\newblock {\em Journal of Multivariate Analysis}, 99(9):2053--2081, 2008.

\bibitem{gao2020flow}
Ruiqi Gao, Erik Nijkamp, Diederik~P Kingma, Zhen Xu, Andrew~M Dai, and
  Ying~Nian Wu.
\newblock Flow contrastive estimation of energy-based models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7518--7528, 2020.

\bibitem{garthwaite2005statistical}
Paul~H Garthwaite, Joseph~B Kadane, and Anthony O'Hagan.
\newblock Statistical methods for eliciting probability distributions.
\newblock {\em Journal of the American Statistical Association},
  100(470):680--701, 2005.

\bibitem{germain2015made}
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle.
\newblock Made: Masked autoencoder for distribution estimation.
\newblock In {\em International Conference on Machine Learning}, pages
  881--889, 2015.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{gibbs2002choosing}
Alison~L Gibbs and Francis~Edward Su.
\newblock On choosing and bounding probability metrics.
\newblock {\em International statistical review}, 70(3):419--435, 2002.

\bibitem{gneiting2007strictly}
Tilmann Gneiting and Adrian~E Raftery.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock {\em Journal of the American Statistical Association},
  102(477):359--378, 2007.

\bibitem{good1971comment}
IJ~Good.
\newblock Comment on “measuring information and uncertainty” by robert j.
  buehler.
\newblock {\em Foundations of Statistical Inference}, pages 337--339, 1971.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{grathwohl2020no}
Will Grathwohl, Jacob Kelly, Milad Hashemi, Mohammad Norouzi, Kevin Swersky,
  and David Duvenaud.
\newblock No mcmc for me: Amortized sampling for fast and stable training of
  energy-based models.
\newblock {\em arXiv preprint arXiv:2010.04230}, 2020.

\bibitem{grathwohl2019your}
Will Grathwohl, Kuan-Chieh Wang, J{\"o}rn-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock {\em arXiv preprint arXiv:1912.03263}, 2019.

\bibitem{grenander2007pattern}
Ulf Grenander, Michael~I Miller, Michael Miller, et~al.
\newblock {\em Pattern theory: from representation to inference}.
\newblock Oxford university press, 2007.

\bibitem{gross2011recovering}
David Gross.
\newblock Recovering low-rank matrices from few coefficients in any basis.
\newblock {\em IEEE Transactions on Information Theory}, 57(3):1548--1566,
  2011.

\bibitem{gutmann2011bregman}
Michael~U Gutmann and Jun-ichiro Hirayama.
\newblock Bregman divergence as general framework to estimate unnormalized
  statistical models.
\newblock In {\em Proceedings of the Twenty-Seventh Conference on Uncertainty
  in Artificial Intelligence}, pages 283--290, 2011.

\bibitem{gutmann2012noise}
Michael~U Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation of unnormalized statistical models, with
  applications to natural image statistics.
\newblock {\em The journal of machine learning research}, 13(1):307--361, 2012.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In {\em Advances in neural information processing systems}, pages
  6626--6637, 2017.

\bibitem{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural computation}, 14(8):1771--1800, 2002.

\bibitem{homem2008rates}
Tito Homem-de Mello.
\newblock On rates of convergence for stochastic optimization problems under
  non--independent and identically distributed sampling.
\newblock {\em SIAM Journal on Optimization}, 19(2):524--551, 2008.

\bibitem{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(Apr):695--709, 2005.

\bibitem{jacob2017unbiased}
Pierre~E Jacob, John O'Leary, and Yves~F Atchad{\'e}.
\newblock Unbiased markov chain monte carlo with couplings.
\newblock {\em arXiv preprint arXiv:1708.03625}, 2017.

\bibitem{ji2015blackout}
Shihao Ji, SVN Vishwanathan, Nadathur Satish, Michael~J Anderson, and Pradeep
  Dubey.
\newblock Blackout: Speeding up recurrent neural network language models with
  very large vocabularies.
\newblock {\em arXiv preprint arXiv:1511.06909}, 2015.

\bibitem{jin2019local}
Chi Jin, Praneeth Netrapalli, and Michael~I Jordan.
\newblock What is local optimality in nonconvex-nonconcave minimax
  optimization?
\newblock {\em arXiv preprint arXiv:1902.00618}, 2019.

\bibitem{kanamori2015robust}
Takafumi Kanamori and Hironori Fujisawa.
\newblock Robust estimation under heavy contamination using unnormalized
  models.
\newblock {\em Biometrika}, 102(3):559--572, 2015.

\bibitem{kanamori2014affine}
Takafumi Kanamori, Hironori Fujisawa, et~al.
\newblock Affine invariant divergences associated with proper composite scoring
  rules and their applications.
\newblock {\em Bernoulli}, 20(4):2278--2304, 2014.

\bibitem{kingma2010regularized}
Durk~P Kingma and Yann~L Cun.
\newblock Regularized estimation of image statistics by score matching.
\newblock In {\em Advances in neural information processing systems}, pages
  1126--1134, 2010.

\bibitem{kingma2018glow}
Durk~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10215--10224, 2018.

\bibitem{kohler2017sub}
Jonas~Moritz Kohler and Aurelien Lucchi.
\newblock Sub-sampled cubic regularization for non-convex optimization.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1895--1904. JMLR. org, 2017.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{kumar2019maximum}
Rithesh Kumar, Sherjil Ozair, Anirudh Goyal, Aaron Courville, and Yoshua
  Bengio.
\newblock Maximum entropy generators for energy-based models.
\newblock {\em arXiv preprint arXiv:1901.08508}, 2019.

\bibitem{labeau2019experimenting}
Matthieu Labeau and Shay~B Cohen.
\newblock Experimenting with power divergences for language modeling.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 4095--4105, 2019.

\bibitem{lacoste2012simpler}
Simon Lacoste-Julien, Mark Schmidt, and Francis Bach.
\newblock A simpler approach to obtaining an o (1/t) convergence rate for the
  projected stochastic subgradient method.
\newblock {\em arXiv preprint arXiv:1212.2002}, 2012.

\bibitem{larochelle2011neural}
Hugo Larochelle and Iain Murray.
\newblock The neural autoregressive distribution estimator.
\newblock In {\em Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 29--37, 2011.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lecun2006tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, and F~Huang.
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting structured data}, 1(0), 2006.

\bibitem{li2016renyi}
Yingzhen Li and Richard~E Turner.
\newblock R{\'e}nyi divergence variational inference.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1073--1081, 2016.

\bibitem{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{martens2012estimating}
James Martens, Ilya Sutskever, and Kevin Swersky.
\newblock Estimating the hessian by back-propagating curvature.
\newblock {\em arXiv preprint arXiv:1206.6464}, 2012.

\bibitem{mescheder2018training}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for gans do actually converge?
\newblock {\em arXiv preprint arXiv:1801.04406}, 2018.

\bibitem{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1802.05957}, 2018.

\bibitem{naeem2020reliable}
Muhammad~Ferjad Naeem, Seong~Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun
  Yoo.
\newblock Reliable fidelity and diversity metrics for generative models.
\newblock In {\em International Conference on Machine Learning}, pages
  7176--7185. PMLR, 2020.

\bibitem{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock {\em Handbook of markov chain monte carlo}, 2(11):2, 2011.

\bibitem{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 19(4):1574--1609, 2009.

\bibitem{nesterov2013introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{nijkamp2019learning}
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying~Nian Wu.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5233--5243, 2019.

\bibitem{nowozin2016f}
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In {\em Advances in neural information processing systems}, pages
  271--279, 2016.

\bibitem{mcbook}
Art~B. Owen.
\newblock {\em Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem{parry2012proper}
Matthew Parry, A~Philip Dawid, Steffen Lauritzen, et~al.
\newblock Proper local scoring rules.
\newblock {\em The Annals of Statistics}, 40(1):561--592, 2012.

\bibitem{parry2016linear}
Matthew Parry et~al.
\newblock Linear scoring rules for probabilistic binary classification.
\newblock {\em Electronic Journal of Statistics}, 10(1):1596--1607, 2016.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8024--8035, 2019.

\bibitem{poon2011sum}
Hoifung Poon and Pedro Domingos.
\newblock Sum-product networks: A new deep architecture.
\newblock In {\em 2011 IEEE International Conference on Computer Vision
  Workshops (ICCV Workshops)}, pages 689--690. IEEE, 2011.

\bibitem{qiuunbiased}
Yixuan Qiu, Lingsong Zhang, and Xiao Wang.
\newblock Unbiased contrastive divergence algorithm for training energy-based
  latent variable models.

\bibitem{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex
  Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em International conference on machine learning}, pages
  314--323, 2016.

\bibitem{robert2013monte}
Christian Robert and George Casella.
\newblock {\em Monte Carlo statistical methods}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{roby1965belief}
Thornton~B Roby.
\newblock Belief states and the uses of evidence.
\newblock {\em Behavioral Science}, 10(3):255--270, 1965.

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In {\em Advances in neural information processing systems}, pages
  2234--2242, 2016.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{song2019bridging}
Jiaming Song and Stefano Ermon.
\newblock Bridging the gap between $ f $-gans and wasserstein gans.
\newblock {\em arXiv preprint arXiv:1910.09779}, 2019.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11895--11907, 2019.

\bibitem{song2020generative}
Yang Song and Stefano Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{song2019sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: {A} scalable approach to density and score
  estimation.
\newblock In {\em Proceedings of the Thirty-Fifth Conference on Uncertainty in
  Artificial Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  page 204, 2019.

\bibitem{song2020sliced}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon.
\newblock Sliced score matching: A scalable approach to density and score
  estimation.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 574--584.
  PMLR, 2020.

\bibitem{takenouchi2015empirical}
Takashi Takenouchi and Takafumi Kanamori.
\newblock Empirical localization of homogeneous divergences on discrete sample
  spaces.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  820--828, 2015.

\bibitem{uehara2020unified}
Masatoshi Uehara, Takafumi Kanamori, Takashi Takenouchi, and Takeru Matsuda.
\newblock A unified statistically efficient estimation framework for
  unnormalized models.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 809--819, 2020.

\bibitem{van2014renyi}
Tim Van~Erven and Peter Harremos.
\newblock R{\'e}nyi divergence and kullback-leibler divergence.
\newblock {\em IEEE Transactions on Information Theory}, 60(7):3797--3820,
  2014.

\bibitem{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural computation}, 23(7):1661--1674, 2011.

\bibitem{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem{yu2020training}
Lantao Yu, Yang Song, Jiaming Song, and Stefano Ermon.
\newblock Training deep energy-based models with f-divergence minimization.
\newblock In {\em International Conference on Machine Learning}, pages
  10957--10967. PMLR, 2020.

\bibitem{zhang2020deep}
Jun Zhang, Yaokun Lei, Yi~Isaac Yang, and Yi~Qin Gao.
\newblock Deep learning for multi-scale molecular modeling.
\newblock 2020.

\bibitem{zhao2018bias}
Shengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, and
  Stefano Ermon.
\newblock Bias and generalization in deep generative models: An empirical
  study.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10792--10801, 2018.

\end{thebibliography}
