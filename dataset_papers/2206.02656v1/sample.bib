@STRING{TVE = {{\VAN{Erven}{Van}{van}} Erven, Tim}}
@STRING{DVDH = {{\VAN{Hoeven}{Van der}{van der}} Hoeven, Dirk}}
@STRING{SdR = {{\VAN{Rooij}{De}{de}} Rooij, Steven}}


@inproceedings{beygelzimer2011contextual,
  title={Contextual bandit algorithms with supervised learning guarantees},
  author={Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={19--26},
  year={2011}
}

@article{lecue2014optimal,
  title={Optimal learning with {Q}-aggregation},
  author={Lecu{\'e}, Guillaume and Rigollet, Philippe},
  journal={The Annals of Statistics},
  volume={42},
  number={1},
  pages={211--224},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}

@article{lecue2009aggregation,
  title={Aggregation via empirical risk minimization},
  author={Lecu{\'e}, Guillaume and Mendelson, Shahar},
  journal={Probability theory and related fields},
  volume={145},
  number={3-4},
  pages={591--613},
  year={2009},
  publisher={Springer}
}

@article{audibert2007progressive,
  title={Progressive mixture rules are deviation suboptimal},
  author={Audibert, Jean-Yves},
  journal={Advances in Neural Information Processing Systems},
  year={2007}
}

@article{mourtada2021distribution,
  title={Distribution-free robust linear regression},
  author={Mourtada, Jaouad and Vaskevicius, Tomas and Zhivotovskiy, Nikita},
  journal={Mathematical Statistics and Learning},
  year={2022}
}




@article{kanade2022exponential,
  title={Exponential Tail Local {R}ademacher Complexity Risk Bounds Without the {B}ernstein Condition},
  author={Kanade, Varun and Rebeschini, Patrick and Vaskevicius, Tomas},
  journal={arXiv preprint arXiv:2202.11461},
  year={2022}
}

@article{juditsky2008learning,
  title={Learning by mirror averaging},
  author={Juditsky, Anatoli and Rigollet, Philippe and Tsybakov, Alexandre B},
  journal={The Annals of Statistics},
  volume={36},
  number={5},
  pages={2183--2206},
  year={2008},
  publisher={Institute of Mathematical Statistics}
}

@article{kakade2008generalization,
  title={On the generalization ability of online strongly convex programming algorithms},
  author={Kakade, Sham M and Tewari, Ambuj},
  journal={Advances in Neural Information Processing Systems},
  year={2008}
}

@article{shamir2015sample,
  title={The sample complexity of learning linear predictors with the squared loss.},
  author={Shamir, Ohad},
  journal={Journal of Machine Learning Research},
  volume={16},
  pages={3475--3486},
  year={2015}
}

@article{saad2021fast,
  title={Fast rates for prediction with limited expert advice},
  author={Saad, El Mehdi and Blanchard, Gilles},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{mourtada2019optimality,
  title={On the optimality of the {H}edge algorithm in the stochastic regime},
  author={Mourtada, Jaouad and Ga{\"\i}ffas, St{\'e}phane},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages={1--28},
  year={2019}
}

@InProceedings{rakhlin2014online,
  title = 	 {Online Non-Parametric Regression},
  author = 	 {Rakhlin, Alexander and Sridharan, Karthik},
  booktitle = 	 {Proceedings of The 27th Conference on Learning Theory},
  pages = 	 {1232--1264},
  year = 	 {2014},
  volume = 	 {35},
  series = 	 {Proceedings of Machine Learning Research}
}


@inproceedings{koolen2015second,
  title={Second-order quantile methods for experts and combinatorial games},
  author={Wouter M. Koolen and } # TVE,
  booktitle={Conference on Learning Theory},
  year={2015}
}

@article{wintenberger2017optimal,
  title={Optimal learning with {B}ernstein online aggregation},
  author={Wintenberger, Olivier},
  journal={Machine Learning},
  volume={106},
  number={1},
  pages={119--141},
  year={2017},
  publisher={Springer}
}

@article{nemirovski2000topics,
  title={Topics in non-parametric statistics},
  author={Nemirovski, Arkadi},
  journal={Ecole d’Et{\'e} de Probabilit{\'e}s de Saint-Flour},
  volume={28},
  pages={85},
  year={2000}
}

@incollection{tsybakov2003optimal,
  title={Optimal rates of aggregation},
  author={Tsybakov, Alexandre B},
  booktitle={Learning theory and kernel machines},
  pages={303--313},
  year={2003},
  publisher={Springer}
}

@inproceedings{foster2018logistic,
  title={Logistic regression: The importance of being improper},
  author={Foster, Dylan J and Kale, Satyen and Luo, Haipeng and Mohri, Mehryar and Sridharan, Karthik},
  booktitle={Conference On Learning Theory},
  pages={167--208},
  year={2018},
  organization={PMLR}
}

@inproceedings{jezequel2020efficient,
  title={Efficient improper learning for online logistic regression},
  author={J{\'e}z{\'e}quel, R{\'e}mi and Gaillard, Pierre and Rudi, Alessandro},
  booktitle={Conference on Learning Theory},
  pages={2085--2108},
  year={2020},
  organization={PMLR}
}

@article{mourtada2019improper,
  title={An improper estimator with optimal excess risk in misspecified density estimation and logistic regression},
  author={Mourtada, Jaouad and Ga{\"\i}ffas, St{\'e}phane},
  journal={arXiv preprint arXiv:1912.10784},
  year={2019}
}

@inproceedings{rakhlin2011making,
author = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
title = {Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization},
year = {2012},
booktitle = {Proceedings of the 29th International Conference on Machine Learning},
pages = {1571–1578},
series = {ICML'12}
}



@inproceedings{kakade2009generalization,
  title={On the generalization ability of online strongly convex programming algorithms},
  author={Kakade, Sham M and Tewari, Ambuj},
  booktitle={Advances in Neural Information Processing Systems},
  year={2009}
}

@article{vovk2001competitive,
  title={Competitive on-line statistics},
  author={Vovk, Volodimir G},
  journal={International Statistical Review},
  volume={69},
  number={2},
  pages={213--248},
  year={2001},
  publisher={Wiley Online Library}
}

@article{azoury2001relative,
  title={Relative loss bounds for on-line density estimation with the exponential family of distributions},
  author={Azoury, Katy S and Warmuth, Manfred K},
  journal={Machine Learning},
  volume={43},
  number={3},
  pages={211--246},
  year={2001},
  publisher={Springer}
}

@article{lecue2013optimality,
  title={On the optimality of the aggregate with exponential weights for low temperatures},
  author={Lecu{\'e}, Guillaume and Mendelson, Shahar},
  journal={Bernoulli},
  volume={19},
  number={2},
  pages={646--675},
  year={2013},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}


@inproceedings{gaillard2014second,
  title={A second-order bound with excess losses},
  author={Gaillard, Pierre and Stoltz, Gilles and} # TVE,
  booktitle={Conference on Learning Theory},
  year={2014}
}

@article{VanErven2021metagrad,
  author  = TVE # { and Wouter M. Koolen and } # DVDH,
  title   = {MetaGrad: Adaptation using Multiple Learning Rates in Online Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {161},
  pages   = {1-61}
}

@article{hazan2007logarithmic,
  title={Logarithmic regret algorithms for online convex optimization},
  author={Hazan, Elad and Agarwal, Amit and Kale, Satyen},
  journal={Machine Learning},
  volume={69},
  number={2-3},
  pages={169--192},
  year={2007},
  publisher={Springer}
}

@article{amir2020prediction,
  title={Prediction with Corrupted Expert Advice},
  author={Amir, Idan and Attias, Idan and Koren, Tomer and Mansour, Yishay and Livni, Roi},
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{ito2021optimal,
  title={On Optimal Robustness to Adversarial Corruption in Online Decision Problems},
  author={Ito, Shinji},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{derooij2014follow,
  author = SdR # { and } # TVE # { and Peter D. Gr{\"u}nwald and Wouter
	M. Koolen},
  title = {{F}ollow the {L}eader If You Can, {H}edge If You Must},
  journal = {Journal of Machine Learning Research},
  year = {2014},
  volume = {15},
  pages = {1281--1316}
}

@inproceedings{zimmert2019optimal,
  title={An optimal algorithm for stochastic and adversarial bandits},
  author={Zimmert, Julian and Seldin, Yevgeny},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={467--475},
  year={2019},
  organization={PMLR}
}


@InProceedings{cutkosky19anytime,
  title = 	 {Anytime Online-to-Batch, Optimism and Acceleration},
  author =       {Cutkosky, Ashok},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1446--1454},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/cutkosky19a/cutkosky19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/cutkosky19a.html},
  abstract = 	 {A standard way to obtain convergence guarantees in stochastic convex optimization is to run an online learning algorithm and then output the average of its iterates: the actual iterates of the online learning algorithm do not come with individual guarantees. We close this gap by introducing a black-box modification to any online learning algorithm whose iterates converge to the optimum in stochastic scenarios. We then consider the case of smooth losses, and show that combining our approach with optimistic online learning algorithms immediately yields a fast convergence rate of $O(L/T^{3/2}+\sigma/\sqrt{T})$ on $L$-smooth problems with $\sigma^2$ variance in the gradients. Finally, we provide a reduction that converts any adaptive online algorithm into one that obtains the optimal accelerated rate of $\tilde O(L/T^2 + \sigma/\sqrt{T})$, while still maintaining $\tilde O(1/\sqrt{T})$ convergence in the non-smooth setting. Importantly, our algorithms adapt to $L$ and $\sigma$ automatically: they do not need to know either to obtain these rates.}
}


@article{VanderHoeven2020exploiting,
  title={Exploiting the Surrogate Gap in Online Multiclass Classification},
  author= DVDH,
  journal={Advances in Neural Information Processing Systems},
  year={2020}
}

@article{luo2016efficient,
  title={Efficient second order online learning by sketching},
  author={Luo, Haipeng and Agarwal, Alekh and Cesa-Bianchi, Nicolo and Langford, John},
  journal={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{vanderhoeven2018many,
  title={The many faces of exponential weights in online learning},
  author= DVDH # { and } # TVE # { and Wojciech Kotlowski},
  booktitle={Conference On Learning Theory},
  year={2018}
}

@inproceedings{vovk1990aggregating,
  title={Aggregating strategies},
  author={Vovk, Volodimir G},
  booktitle={Proceedings of the 3rd Annual Workshop on Computational Learning Theory},
  year={1990}
}

@article{littlestone1994weighted,
  title={The weighted majority algorithm},
  author={Littlestone, Nick and Warmuth, Manfred K},
  journal={Information and computation},
  volume={108},
  number={2},
  pages={212--261},
  year={1994},
  publisher={Elsevier}
}


@inproceedings{mhammedi2019lipschitz,
  title={Lipschitz adaptivity with multiple learning rates in online learning},
  author={Mhammedi, Zakaria and Koolen, Wouter M and } # TVE,
  booktitle={Conference on Learning Theory},
  year={2019}
}

@article{orabona2019modern,
  title={A modern introduction to online learning},
  author={Orabona, Francesco},
  journal={arXiv preprint arXiv:1912.13213},
  year={2019}
}

@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge university press}
}


@inproceedings{wang2020adaptivity,
  title={Adaptivity and optimality: A universal algorithm for online convex optimization},
  author={Wang, Guanghui and Lu, Shiyin and Zhang, Lijun},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={659--668},
  year={2020},
  organization={PMLR}
}


@inproceedings{chen2021impossible,
  title={Impossible Tuning Made Possible: A New Expert Algorithm and Its Applications},
  author={Chen, Liyu and Luo, Haipeng and Wei, Chen-Yu},
  booktitle={Conference on Learning Theory},
  year={2021}
}



@article{saha2021optimal,
  title={Optimal Regret Algorithm for Pseudo-1d Bandit Convex Optimization},
  author={Saha, Aadirupa and Natarajan, Nagarajan and Netrapalli, Praneeth and Jain, Prateek},
  journal={ICML},
  year={2021}
}


@inproceedings{lykouris2018stochastic,
  title={Stochastic bandits robust to adversarial corruptions},
  author={Lykouris, Thodoris and Mirrokni, Vahab and Paes Leme, Renato},
  booktitle={Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={114--122},
  year={2018}
}


@article{Zimmert2021tsallis,
  author  = {Julian Zimmert and Yevgeny Seldin},
  title   = {Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {28},
  pages   = {1-49}
}


@article{vijaykumar2021localization,
  title={Localization, Convexity, and Star Aggregation},
  author={Vijaykumar, Suhas},
  journal={arXiv preprint arXiv:2105.08866},
  year={2021}
}

@inproceedings{alon2015online,
  title={Online learning with feedback graphs: Beyond bandits},
  author={Alon, Noga and Cesa-Bianchi, Nicolo and Dekel, Ofer and Koren, Tomer},
  booktitle={Conference on Learning Theory},
  pages={23--35},
  year={2015},
  organization={PMLR}
}


@article{cesa2006worst,
  title={Worst-Case Analysis of Selective Sampling for Linear Classification.},
  author={Cesa-Bianchi, Nicolo and Gentile, Claudio and Zaniboni, Luca and Warmuth, Manfred},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={7},
  year={2006}
}

@inproceedings{atlas1990training,
  title={Training connectionist networks with queries and selective sampling},
  author={Atlas, Les E and Cohn, David A and Ladner, Richard E},
  booktitle={Advances in neural information processing systems},
  year={1990}
}

@incollection{cesa2003learning,
  title={Learning probabilistic linear-threshold classifiers via selective sampling},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  booktitle={Learning Theory and Kernel Machines},
  pages={373--387},
  year={2003},
  publisher={Springer}
}

@article{freund1997selective,
  title={Selective sampling using the query by committee algorithm},
  author={Freund, Yoav and Seung, H Sebastian and Shamir, Eli and Tishby, Naftali},
  journal={Machine learning},
  volume={28},
  number={2},
  pages={133--168},
  year={1997},
  publisher={Springer}
}

@inproceedings{orabona2011better,
  title={Better algorithms for selective sampling},
  author={Orabona, Francesco and Cesa-Bianchi, Nicolo},
  booktitle={International conference on machine learning},
  year={2011}
}


@inproceedings{zhang2018dynamic,
  title={Dynamic regret of strongly adaptive methods},
  author={Zhang, Lijun and Yang, Tianbao and Zhou, Zhi-Hua and others},
  booktitle={International conference on machine learning},
  year={2018}
}

@inproceedings{neu2020fast,
  title={Fast rates for online prediction with abstention},
  author={Neu, Gergely and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  year={2020}
}

@inproceedings{VanderHoeven2021beyond,
 title={Beyond Bandit Feedback in Online Multiclass Classification
},
 author=DVDH # { and Federico Fusco} # { and Nicolo Cesa-Bianchi},
 booktitle={Advances in neural information processing systems},
 year={2021}
}

@article{bubeck2011introduction,
  title={Introduction to online optimization},
  author={Bubeck, S{\'e}bastien},
  journal={Lecture Notes},
  year={2011}
}

@article{hazan2016introduction,
  title={Introduction to Online Convex Optimization},
  author={Hazan, Elad},
  journal={Foundations and Trends in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@inproceedings{mhammedi2018constant,
  title={Constant regret, generalized mixability, and mirror descent},
  author={Mhammedi, Zakaria and Williamson, Robert C},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}

@InProceedings{cutkosky2019artificial,
  title = 	 {Artificial Constraints and Hints for Unbounded Online Learning},
  author =       {Cutkosky, Ashok},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {874--894},
  year = 	 {2019},
  volume = 	 {99},
  month = 	 {25--28 Jun},
  publisher =    {PMLR}
}

@ARTICLE{puchkin21a,
  author={Puchkin, Nikita and Zhivotovskiy, Nikita},
  journal={IEEE Transactions on Information Theory}, 
  title={Exponential Savings in Agnostic Active Learning through Abstention}, 
  year={2022},
  volume={},
  number={}
}


@book{meyer2000matrix,
  title={Matrix {A}nalysis and Applied Linear Algebra},
  author={Meyer, Carl D},
  volume={71},
  year={2000},
  publisher={Siam}
}