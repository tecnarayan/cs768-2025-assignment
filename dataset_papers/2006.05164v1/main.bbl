\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agakov \& Barber(2004)Agakov and Barber]{AgakovB04a/iconip}
Agakov, F.~V. and Barber, D.
\newblock An auxiliary variational method.
\newblock In \emph{ICONIP}, 2004.

\bibitem[Alain \& Bengio(2014)Alain and Bengio]{AlainB14/jmlr}
Alain, G. and Bengio, Y.
\newblock What regularized auto-encoders learn from the data-generating
  distribution.
\newblock \emph{J. Mach. Learn. Res.}, 2014.

\bibitem[Balaji et~al.(2019)Balaji, Hassani, Chellappa, and
  Feizi]{BalajiHCF19/icml}
Balaji, Y., Hassani, H., Chellappa, R., and Feizi, S.
\newblock Entropic gans meet vaes: {A} statistical approach to compute sample
  likelihoods in gans.
\newblock In \emph{ICML}, 2019.

\bibitem[Bertsekas(2016)]{bertsekas2016nonlinear}
Bertsekas, D.
\newblock Nonlinear programming. 3rd edn. massachussets: Athena scientific,
  2016.

\bibitem[Bigdeli et~al.(2020)Bigdeli, Lin, Portenier, Dunbar, and
  Zwicker]{bigdeli2020learning}
Bigdeli, S.~A., Lin, G., Portenier, T., Dunbar, L.~A., and Zwicker, M.
\newblock Learning generative models using denoising density estimators.
\newblock \emph{arXiv preprint arXiv:2001.02728}, 2020.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openaigym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Burda et~al.(2016)Burda, Grosse, and Salakhutdinov]{BurdaGS15/iclr}
Burda, Y., Grosse, R.~B., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock In \emph{ICLR}, 2016.

\bibitem[Chen et~al.(2016)Chen, Kingma, Salimans, Duan, Dhariwal, Schulman,
  Sutskever, and Abbeel]{chen2016variational}
Chen, X., Kingma, D.~P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J.,
  Sutskever, I., and Abbeel, P.
\newblock Variational lossy autoencoder.
\newblock In \emph{ICLR}, 2016.

\bibitem[Dieng et~al.(2019)Dieng, Ruiz, Blei, and Titsias]{Dieng2019pregan}
Dieng, A.~B., Ruiz, F. J.~R., Blei, D.~M., and Titsias, M.~K.
\newblock Prescribed generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1910.04302}, 2019.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{ICML}, 2016.

\bibitem[Durrett(2019)]{durrett2019probability}
Durrett, R.
\newblock \emph{Probability: theory and examples}, volume~49.
\newblock Cambridge university press, 2019.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{FujimotoHM18/icml}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{ICML}, 2018.

\bibitem[Geyer(1991)]{geyer1991reweighting}
Geyer, C.~J.
\newblock Reweighting monte carlo mixtures.
\newblock Technical report, University of Minnesota, 1991.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{HaDL17/iclr}
Ha, D., Dai, A.~M., and Le, Q.~V.
\newblock Hypernetworks.
\newblock In \emph{ICLR}, 2017.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{HaarnojaTAL17/icml}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{ICML}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{HaarnojaZAL18/icml}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{ICML}, 2018.

\bibitem[Hasselt(2010)]{hasselt2010double}
Hasselt, H.~V.
\newblock Double q-learning.
\newblock In \emph{NIPS}, 2010.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{HeZRS16/cvpr}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huang et~al.(2018)Huang, Krueger, Lacoste, and
  Courville]{HuangKLC18/icml}
Huang, C., Krueger, D., Lacoste, A., and Courville, A.~C.
\newblock Neural autoregressive flows.
\newblock In \emph{ICML}, 2018.

\bibitem[Huszár(2017)]{Huszar2019implicit}
Huszár, F.
\newblock Variational inference using implicit distributions.
\newblock \emph{arXiv preprint arXiv:1702.08235}, 2017.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
Hyv{\"a}rinen, A.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 695--709, 2005.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{ICLR}, 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, J{\'{o}}zefowicz, Chen,
  Sutskever, and Welling]{KingmaSJCCSW16/nips}
Kingma, D.~P., Salimans, T., J{\'{o}}zefowicz, R., Chen, X., Sutskever, I., and
  Welling, M.
\newblock Improving variational autoencoders with inverse autoregressive flow.
\newblock In \emph{NIPS}, 2016.

\bibitem[Kumar et~al.(2020)Kumar, Poole, and Murphy]{kumar2020regularized}
Kumar, A., Poole, B., and Murphy, K.
\newblock Regularized autoencoders via relaxed injective probability flow.
\newblock \emph{arXiv preprint arXiv:2002.08927}, 2020.

\bibitem[Larochelle \& Murray(2011)Larochelle and Murray]{larochelle2011neural}
Larochelle, H. and Murray, I.
\newblock The neural autoregressive distribution estimator.
\newblock In \emph{AISTATS}, 2011.

\bibitem[Li \& Turner(2018)Li and Turner]{LiT18/iclr}
Li, Y. and Turner, R.~E.
\newblock Gradient estimators for implicit models.
\newblock In \emph{ICLR}, 2018.

\bibitem[Li et~al.(2017)Li, Turner, and Liu]{li2017approximate}
Li, Y., Turner, R.~E., and Liu, Q.
\newblock Approximate inference with amortised mcmc.
\newblock \emph{arXiv preprint arXiv:1702.08343}, 2017.

\bibitem[Loaiza-Ganem et~al.(2017)Loaiza-Ganem, Gao, and
  Cunningham]{loaiza2017maximum}
Loaiza-Ganem, G., Gao, Y., and Cunningham, J.~P.
\newblock Maximum entropy flow networks.
\newblock \emph{arXiv preprint arXiv:1701.03504}, 2017.

\bibitem[Maal{\o}e et~al.(2016)Maal{\o}e, S{\o}nderby, S{\o}nderby, and
  Winther]{maaloe2016auxiliary}
Maal{\o}e, L., S{\o}nderby, C.~K., S{\o}nderby, S.~K., and Winther, O.
\newblock Auxiliary deep generative models.
\newblock \emph{arXiv preprint arXiv:1602.05473}, 2016.

\bibitem[Mazoure et~al.(2019)Mazoure, Doan, Durand, Hjelm, and
  Pineau]{MazoureDDHP19}
Mazoure, B., Doan, T., Durand, A., Hjelm, R.~D., and Pineau, J.
\newblock Leveraging exploration in off-policy algorithms via normalizing
  flows.
\newblock \emph{arXiv preprint arXiv:1905.06893}, 2019.

\bibitem[Mescheder et~al.(2017)Mescheder, Nowozin, and
  Geiger]{MeschederNG17/icml}
Mescheder, L.~M., Nowozin, S., and Geiger, A.
\newblock Adversarial variational bayes: Unifying variational autoencoders and
  generative adversarial networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Minka et~al.(2005)]{minka2005divergence}
Minka, T. et~al.
\newblock Divergence measures and message passing.
\newblock Technical report, Technical report, Microsoft Research, 2005.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and
  Hassabis]{MnihKSRVBGRFOPB15/nature}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M.~A., Fidjeland, A., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mohamed \& Lakshminarayanan(2016)Mohamed and
  Lakshminarayanan]{Mohamed2016learning}
Mohamed, S. and Lakshminarayanan, B.
\newblock Learning in implicit generative models.
\newblock \emph{arXiv preprint arXiv:1610.03483}, 2016.

\bibitem[Odena et~al.(2018)Odena, Buckman, Olsson, Brown, Olah, Raffel, and
  Goodfellow]{odena2018generator}
Odena, A., Buckman, J., Olsson, C., Brown, T.~B., Olah, C., Raffel, C., and
  Goodfellow, I.
\newblock Is generator conditioning causally related to gan performance?
\newblock \emph{arXiv preprint arXiv:1802.08768}, 2018.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{polyak1992acceleration}
Polyak, B.~T. and Juditsky, A.~B.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Ranganath et~al.(2016)Ranganath, Tran, and
  Blei]{ranganath2016hierarchical}
Ranganath, R., Tran, D., and Blei, D.
\newblock Hierarchical variational models.
\newblock In \emph{ICML}, 2016.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{RezendeM15/icml}
Rezende, D.~J. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock In \emph{ICML}, 2015.

\bibitem[Rezende et~al.(2014{\natexlab{a}})Rezende, Mohamed, and
  Wierstra]{RezendeMW14/icml}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{ICML}, 2014{\natexlab{a}}.

\bibitem[Rezende et~al.(2014{\natexlab{b}})Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:1401.4082}, 2014{\natexlab{b}}.

\bibitem[Roeder et~al.(2017)Roeder, Wu, and Duvenaud]{roeder2017sticking}
Roeder, G., Wu, Y., and Duvenaud, D.~K.
\newblock Sticking the landing: Simple, lower-variance gradient estimators for
  variational inference.
\newblock In \emph{NIPS}, 2017.

\bibitem[Saremi \& Hyvarinen(2019)Saremi and Hyvarinen]{saremi2019neural}
Saremi, S. and Hyvarinen, A.
\newblock Neural empirical bayes.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--23,
  2019.

\bibitem[Saremi et~al.(2018)Saremi, Mehrjou, Sch{\"o}lkopf, and
  Hyv{\"a}rinen]{saremi2018deep}
Saremi, S., Mehrjou, A., Sch{\"o}lkopf, B., and Hyv{\"a}rinen, A.
\newblock Deep energy estimator networks.
\newblock \emph{arXiv preprint arXiv:1805.08306}, 2018.

\bibitem[Savitzky \& Golay(1964)Savitzky and Golay]{savitzky1964smoothing}
Savitzky, A. and Golay, M.~J.
\newblock Smoothing and differentiation of data by simplified least squares
  procedures.
\newblock \emph{Analytical chemistry}, 36\penalty0 (8):\penalty0 1627--1639,
  1964.

\bibitem[Shalev-Shwartz et~al.(2017)Shalev-Shwartz, Shamir, and
  Shammah]{shalev2017failures}
Shalev-Shwartz, S., Shamir, O., and Shammah, S.
\newblock Failures of gradient-based deep learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Shi et~al.(2018)Shi, Sun, and Zhu]{shi2018spectral}
Shi, J., Sun, S., and Zhu, J.
\newblock A spectral approach to gradient estimation for implicit
  distributions.
\newblock In \emph{ICML}, 2018.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Song, Y. and Ermon, S.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Song et~al.(2019)Song, Garg, Shi, and Ermon]{SongGSE19}
Song, Y., Garg, S., Shi, J., and Ermon, S.
\newblock Sliced score matching: {A} scalable approach to density and score
  estimation.
\newblock In \emph{UAI}, 2019.

\bibitem[Sutton et~al.(1998)Sutton, Barto, et~al.]{sutton1998introduction}
Sutton, R.~S., Barto, A.~G., et~al.
\newblock \emph{Introduction to reinforcement learning}, volume~2.
\newblock MIT press Cambridge, 1998.

\bibitem[Tomczak \& Welling(2018)Tomczak and Welling]{tomczak2018vae}
Tomczak, J. and Welling, M.
\newblock Vae with a vampprior.
\newblock In \emph{AISTATS}, 2018.

\bibitem[Tran et~al.(2017)Tran, Ranganath, and Blei]{Tran2017hierarchical}
Tran, D., Ranganath, R., and Blei, D.
\newblock Hierarchical implicit models and likelihood-free variational
  inference.
\newblock In \emph{NIPS}, 2017.

\bibitem[Vincent(2011)]{vincent2011connection}
Vincent, P.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Vincent et~al.(2008)Vincent, Larochelle, Bengio, and
  Manzagol]{VincentLBM08/icml}
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock In \emph{ICML}, 2008.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, and
  Manzagol]{vincent2010stacked}
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock \emph{Journal of machine learning research}, 11\penalty0
  (Dec):\penalty0 3371--3408, 2010.

\bibitem[Wen et~al.(2020)Wen, Zhou, He, Zhou, and Xu]{wen2020mutual}
Wen, L., Zhou, Y., He, L., Zhou, M., and Xu, Z.
\newblock Mutual information gradient estimation for representation learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Ziebart, B.~D.
\newblock Modeling purposeful adaptive behavior with the principle of maximum
  causal entropy.
\newblock 2010.

\end{thebibliography}
