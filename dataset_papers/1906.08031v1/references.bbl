\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{aljundi2017expert}
R.~Aljundi, P.~Chakravarty, and T.~Tuytelaars.
\newblock Expert gate: Lifelong learning with a network of experts.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3366--3375, 2017.

\bibitem{cai2018proxylessnas}
H.~Cai, L.~Zhu, and S.~Han.
\newblock Proxylessnas: Direct neural architecture search on target task and
  hardware.
\newblock {\em arXiv preprint arXiv:1812.00332}, 2018.

\bibitem{casale2019probabilistic}
F.~P. Casale, J.~Gordon, and N.~Fusi.
\newblock Probabilistic neural architecture search.
\newblock {\em arXiv preprint arXiv:1902.05116}, 2019.

\bibitem{cesa2006prediction}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock {\em Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem{chen1999improved}
K.~Chen, L.~Xu, and H.~Chi.
\newblock Improved learning algorithms for mixture of experts in multiclass
  classification.
\newblock {\em Neural networks}, 12(9):1229--1252, 1999.

\bibitem{chen2019progressive}
X.~Chen, L.~Xie, J.~Wu, and Q.~Tian.
\newblock Progressive differentiable architecture search: Bridging the depth
  gap between search and evaluation.
\newblock {\em arXiv preprint arXiv:1904.12760}, 2019.

\bibitem{cubuk2018autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock {\em arXiv preprint arXiv:1805.09501}, 2018.

\bibitem{darlow2018cinic}
L.~N. Darlow, E.~J. Crowley, A.~Antoniou, and A.~J. Storkey.
\newblock Cinic-10 is not imagenet or cifar-10.
\newblock {\em arXiv preprint arXiv:1810.03505}, 2018.

\bibitem{devries2017improved}
T.~DeVries and G.~W. Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{eigen2013learning}
D.~Eigen, M.~Ranzato, and I.~Sutskever.
\newblock Learning factored representations in a deep mixture of experts.
\newblock {\em arXiv preprint arXiv:1312.4314}, 2013.

\bibitem{garmash2016ensemble}
E.~Garmash and C.~Monz.
\newblock Ensemble learning for multi-source neural machine translation.
\newblock In {\em Proceedings of COLING 2016, the 26th International Conference
  on Computational Linguistics: Technical Papers}, pages 1409--1418, 2016.

\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem{haussler1995tight}
D.~Haussler, J.~Kivinen, and M.~K. Warmuth.
\newblock Tight worst-case loss bounds for predicting with expert advice.
\newblock In {\em European Conference on Computational Learning Theory}, pages
  69--83. Springer, 1995.

\bibitem{hazan2016introduction}
E.~Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock {\em Foundations and Trends{\textregistered} in Optimization},
  2(3-4):157--325, 2016.

\bibitem{hoeffding1953lower}
W.~Hoeffding.
\newblock A lower bound for the average sample number of a sequential test.
\newblock {\em The Annals of Mathematical Statistics}, pages 127--130, 1953.

\bibitem{hu2018squeeze}
J.~Hu, L.~Shen, and G.~Sun.
\newblock Squeeze-and-excitation networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7132--7141, 2018.

\bibitem{hundt2019sharpdarts}
A.~Hundt, V.~Jain, and G.~D. Hager.
\newblock sharpdarts: Faster and more accurate differentiable architecture
  search.
\newblock {\em arXiv preprint arXiv:1903.09900}, 2019.

\bibitem{Jacobs:1991}
R.~A. Jacobs, M.~I. Jordan, S.~J. Nowlan, and G.~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock {\em Neural Comput.}, 3(1):79--87, Mar. 1991.

\bibitem{Freiburg}
P.~Jund, N.~Abdo, A.~Eitel, and W.~Burgard.
\newblock The freiburg groceries dataset.
\newblock {\em arXiv preprint arXiv:1611.05799}, 2016.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kivinen1997exponentiated}
J.~Kivinen and M.~K. Warmuth.
\newblock Exponentiated gradient versus gradient descent for linear predictors.
\newblock {\em information and computation}, 132(1):1--63, 1997.

\bibitem{larsson2016fractalnet}
G.~Larsson, M.~Maire, and G.~Shakhnarovich.
\newblock Fractalnet: Ultra-deep neural networks without residuals.
\newblock {\em arXiv preprint arXiv:1605.07648}, 2016.

\bibitem{li2019random}
L.~Li and A.~Talwalkar.
\newblock Random search and reproducibility for neural architecture search.
\newblock {\em arXiv preprint arXiv:1902.07638}, 2019.

\bibitem{PNAS}
C.~Liu, B.~Zoph, M.~Neumann, J.~Shlens, W.~Hua, L.-J. Li, L.~Fei-Fei,
  A.~Yuille, J.~Huang, and K.~Murphy.
\newblock Progressive neural architecture search.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 19--34, 2018.

\bibitem{liu2018darts}
H.~Liu, K.~Simonyan, and Y.~Yang.
\newblock Darts: Differentiable architecture search.
\newblock {\em arXiv preprint arXiv:1806.09055}, 2018.

\bibitem{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{NAO}
R.~Luo, F.~Tian, T.~Qin, E.~Chen, and T.-Y. Liu.
\newblock Neural architecture optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7827--7838, 2018.

\bibitem{SVHN}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning 2011}, 2011.

\bibitem{noy2019asap}
A.~Noy, N.~Nayman, T.~Ridnik, N.~Zamir, S.~Doveh, I.~Friedman, R.~Giryes, and
  L.~Zelnik-Manor.
\newblock Asap: Architecture search, anneal and prune.
\newblock {\em arXiv preprint arXiv:1904.04123}, 2019.

\bibitem{ENAS}
H.~Pham, M.~Y. Guan, B.~Zoph, Q.~V. Le, , and J.~Dean.
\newblock Efficient neural architecture search via parameter sharing.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{qian1999momentum}
N.~Qian.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock {\em Neural networks}, 12(1):145--151, 1999.

\bibitem{rasmussen2002infinite}
C.~E. Rasmussen and Z.~Ghahramani.
\newblock Infinite mixtures of gaussian process experts.
\newblock In {\em Advances in neural information processing systems}, pages
  881--888, 2002.

\bibitem{Real18Regularized}
E.~Real, A.~Aggarwal, Y.~Huang, and Q.~V. Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In {\em International Conference on Machine Learning - ICML AutoML
  Workshop}, 2018.

\bibitem{reddi2019convergence}
S.~J. Reddi, S.~Kale, and S.~Kumar.
\newblock On the convergence of adam and beyond.
\newblock {\em arXiv preprint arXiv:1904.09237}, 2019.

\bibitem{sciuto2019evaluating}
C.~Sciuto, K.~Yu, M.~Jaggi, C.~Musat, and M.~Salzmann.
\newblock Evaluating the search phase of neural architecture search.
\newblock {\em arXiv preprint arXiv:1902.08142}, 2019.

\bibitem{shalev2012online}
S.~Shalev-Shwartz et~al.
\newblock Online learning and online convex optimization.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  4(2):107--194, 2012.

\bibitem{shazeer2017outrageously}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock {\em arXiv preprint arXiv:1701.06538}, 2017.

\bibitem{smith2017cyclical}
L.~N. Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In {\em 2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\bibitem{szegedy2016rethinking_label_smooth}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{teja2018hydranets}
R.~Teja~Mullapudi, W.~R. Mark, N.~Shazeer, and K.~Fatahalian.
\newblock Hydranets: Specialized dynamic architectures for efficient inference.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8080--8089, 2018.

\bibitem{cifar100}
A.~Torralba, R.~Fergus, and W.~T. Freeman.
\newblock 80 million tiny images: A large data set for nonparametric object and
  scene recognition.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  30(11):1958--1970, 2008.

\bibitem{van2014follow}
T.~Van~Erven, W.~Kot{\l}owski, and M.~K. Warmuth.
\newblock Follow the leader with dropout perturbations.
\newblock In {\em Conference on Learning Theory}, pages 949--974, 2014.

\bibitem{wang2018deep}
X.~Wang, F.~Yu, R.~Wang, Y.-A. Ma, A.~Mirhoseini, T.~Darrell, and J.~E.
  Gonzalez.
\newblock Deep mixture of experts via shallow embedding.
\newblock {\em arXiv preprint arXiv:1806.01531}, 2018.

\bibitem{williams1992simple}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem{wu2018fbnet}
B.~Wu, X.~Dai, P.~Zhang, Y.~Wang, F.~Sun, Y.~Wu, Y.~Tian, P.~Vajda, Y.~Jia, and
  K.~Keutzer.
\newblock Fbnet: Hardware-aware efficient convnet design via differentiable
  neural architecture search, 2018.

\bibitem{fashionMnist}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{xie2019exploring}
S.~Xie, A.~Kirillov, R.~Girshick, and K.~He.
\newblock Exploring randomly wired neural networks for image recognition.
\newblock {\em arXiv preprint arXiv:1904.01569}, 2019.

\bibitem{xie2018snas}
S.~Xie, H.~Zheng, C.~Liu, and L.~Lin.
\newblock Snas: stochastic neural architecture search.
\newblock {\em arXiv preprint arXiv:1812.09926}, 2018.

\bibitem{snas}
S.~Xie, H.~Zheng, C.~Liu, and L.~Lin.
\newblock Snas: Stochastic neural architecture search.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{CondConv2019}
B.~Yang, G.~Bender, Q.~V. Le, and J.~Ngiam.
\newblock Soft conditional computation.
\newblock {\em CoRR}, abs/1904.04971, 2019.

\bibitem{yao2009hierarchical}
B.~Yao, D.~Walther, D.~Beck, and L.~Fei-Fei.
\newblock Hierarchical mixture of classification experts uncovers interactions
  between brain regions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2178--2186, 2009.

\bibitem{DSO}
X.~Zhang, Z.~Huang, and N.~Wang.
\newblock You only search once: Single shot neural architecture search via
  direct sparse optimization.
\newblock {\em arxiv 1811.01567}, 2018.

\bibitem{zhong2017random}
Z.~Zhong, L.~Zheng, G.~Kang, S.~Li, and Y.~Yang.
\newblock Random erasing data augmentation.
\newblock {\em arXiv preprint arXiv:1708.04896}, 2017.

\bibitem{zoph2016neural}
B.~Zoph and Q.~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.01578}, 2016.

\bibitem{NASNET}
B.~Zoph, V.~Vasudevan, J.~Shlens, and Q.~V. Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8697--8710, 2018.

\end{thebibliography}
