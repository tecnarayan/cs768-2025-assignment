\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlam et~al.(2020)Adlam, Lee, Xiao, Pennington, and
  Snoek]{adlam2020exploring}
Adlam, B., Lee, J., Xiao, L., Pennington, J., and Snoek, J.
\newblock Exploring the uncertainty properties of neural networks' implicit
  priors in the infinite-width limit.
\newblock \arXiv{2010.07355}, 2020.

\bibitem[{\'A}lvarez et~al.(2012){\'A}lvarez, Rosasco, and
  Lawrence]{mauricio:op-val-review}
{\'A}lvarez, M.~A., Rosasco, L., and Lawrence, N.~D.
\newblock Kernels for vector-valued functions: A review.
\newblock \emph{Foundations and Trends® in Machine Learning}, 4\penalty0
  (3):\penalty0 195--266, 2012.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{cntk2019arora}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{arora2019harnessing}
Arora, S., Du, S.~S., Li, Z., Salakhutdinov, R., Wang, R., and Yu, D.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Arpit \& Bengio(2019)Arpit and Bengio]{arpit2019overparameterization}
Arpit, D. and Bengio, Y.
\newblock The benefits of over-parameterization at initialization in deep
  {ReLU} networks.
\newblock \arXiv{1901.03611}, 2019.

\bibitem[Bachmann et~al.(2022)Bachmann, Hofmann, and
  Lucchi]{bachmann2022generalization}
Bachmann, G., Hofmann, T., and Lucchi, A.
\newblock Generalization through the lens of leave-one-out error.
\newblock In \emph{ICLR}, 2022.

\bibitem[Chen et~al.(2021)Chen, Hsieh, and Gong]{chen2021vision}
Chen, X., Hsieh, C.-J., and Gong, B.
\newblock When vision transformers outperform resnets without pre-training or
  strong data augmentations.
\newblock In \emph{ICLR}, 2021.

\bibitem[Craven \& Wahba(1978)Craven and Wahba]{craven1978smoothing}
Craven, P. and Wahba, G.
\newblock Smoothing noisy data with spline functions.
\newblock \emph{Numerische mathematik}, 31\penalty0 (4):\penalty0 377--403,
  1978.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Epperly(2022)]{hanson-wright}
Epperly, E.
\newblock Note to self: Hanson–wright inequality, 2022.
\newblock URL
  \url{https://www.ethanepperly.com/index.php/2022/10/04/note-to-self-hanson-wright-inequality/}.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fort2020deep}
Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli,
  S.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Franceschi et~al.(2022)Franceschi, de~B{\'e}zenac, Ayed, Chen,
  Lamprier, and Gallinari]{franceschi2021neural}
Franceschi, J.-Y., de~B{\'e}zenac, E., Ayed, I., Chen, M., Lamprier, S., and
  Gallinari, P.
\newblock A neural tangent kernel perspective of {GAN}s.
\newblock In \emph{ICML}, 2022.

\bibitem[Hazan \& Jaakkola(2015)Hazan and Jaakkola]{hazan2015steps}
Hazan, T. and Jaakkola, T.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \arXiv{1508.05133}, 2015.

\bibitem[He et~al.(2020)He, Lakshminarayanan, and Teh]{he2020bayesian}
He, B., Lakshminarayanan, B., and Teh, Y.~W.
\newblock Bayesian deep ensembles via the neural tangent kernel.
\newblock \emph{NeurIPS}, 2020.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2016kaiming}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  {ImageNet} classification.
\newblock In \emph{ICCV}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Holzm{\"u}ller et~al.(2023)Holzm{\"u}ller, Zaverkin, K{\"a}stner, and
  Steinwart]{holzmuller2022framework}
Holzm{\"u}ller, D., Zaverkin, V., K{\"a}stner, J., and Steinwart, I.
\newblock A framework and benchmark for deep batch active learning for
  regression.
\newblock \emph{JMLR}, 2023.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{ntk2018jacot}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Krizhevsky(2009)]{cifar102009krizhevsky}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{ICLR}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{linntk2019lee}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
Lee, J., Schoenholz, S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl-Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
Matthews, A. G. d.~G., Rowland, M., Hron, J., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Mohamadi et~al.(2022)Mohamadi, Bae, and
  Sutherland]{mohamadi:active-ntk}
Mohamadi, M.~A., Bae, W., and Sutherland, D.~J.
\newblock Making look-ahead active learning strategies feasible with neural
  tangent kernels.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Neal(1996)]{neal1996priors}
Neal, R.~M.
\newblock \emph{Priors for infinite networks}, pp.\  29--53.
\newblock Springer, 1996.

\bibitem[Nguyen et~al.(2021{\natexlab{a}})Nguyen, Chen, and
  Lee]{nguyen2020dataset}
Nguyen, T., Chen, Z., and Lee, J.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock In \emph{ICLR}, 2021{\natexlab{a}}.

\bibitem[Nguyen et~al.(2021{\natexlab{b}})Nguyen, Novak, Xiao, and
  Lee]{nguyen2021dataset}
Nguyen, T., Novak, R., Xiao, L., and Lee, J.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Novak et~al.(2019)Novak, Xiao, Lee, Bahri, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{novak2018bayesian}
Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl-Dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are
  {G}aussian processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Novak et~al.(2022)Novak, Sohl-Dickstein, and
  Schoenholz]{novak2021fast}
Novak, R., Sohl-Dickstein, J., and Schoenholz, S.~S.
\newblock Fast finite width neural tangent kernel.
\newblock In \emph{ICML}, 2022.

\bibitem[Park et~al.(2019)Park, Sohl-Dickstein, Le, and
  Smith]{park19paramterization}
Park, D., Sohl-Dickstein, J., Le, Q., and Smith, S.
\newblock The effect of network width on stochastic gradient descent and
  generalization: an empirical study.
\newblock In \emph{ICML}, 2019.

\bibitem[Park et~al.(2020)Park, Lee, Peng, Cao, and
  Sohl-Dickstein]{park2020towards}
Park, D.~S., Lee, J., Peng, D., Cao, Y., and Sohl-Dickstein, J.
\newblock Towards {NNGP}-guided neural architecture search.
\newblock \arXiv{2011.06006}, 2020.

\bibitem[Rudi et~al.(2017)Rudi, Carratino, and Rosasco]{falkon}
Rudi, A., Carratino, L., and Rosasco, L.
\newblock {FALKON}: An optimal large scale kernel method.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
  Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Vershynin(2018)]{vershynin2018high}
Vershynin, R.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}.
\newblock Cambridge University Press, 2018.

\bibitem[Wainwright(2019)]{wainwright_2019}
Wainwright, M.~J.
\newblock \emph{High-Dimensional Statistics: A Non-Asymptotic Viewpoint}.
\newblock Cambridge University Press, 2019.

\bibitem[Wang et~al.(2021)Wang, Huang, Margenot, Tong, and He]{wang2021deep}
Wang, H., Huang, W., Margenot, A., Tong, H., and He, J.
\newblock Deep active learning by leveraging training dynamics.
\newblock 2021.

\bibitem[Wei et~al.(2022)Wei, Hu, and Steinhardt]{wei2022more}
Wei, A., Hu, W., and Steinhardt, J.
\newblock More than a toy: Random matrix models predict how real-world neural
  representations generalize.
\newblock In \emph{ICML}, 2022.

\bibitem[Williams(1996)]{williams1996computing}
Williams, C.
\newblock Computing with infinite networks.
\newblock \emph{NeurIPS}, 9, 1996.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \arXiv{1708.07747}, 2017.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., and Pennington, J.
\newblock Dynamical isometry and a mean field theory of {CNN}s: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Xiao et~al.(2020)Xiao, Pennington, and
  Schoenholz]{xiao2020disentangling}
Xiao, L., Pennington, J., and Schoenholz, S.
\newblock Disentangling trainability and generalization in deep neural
  networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Yang(2019)]{yang2019wide}
Yang, G.
\newblock Wide feedforward or recurrent neural networks of any architecture are
  {G}aussian processes.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Yang(2020)]{yang2020tensor}
Yang, G.
\newblock Tensor {P}rograms {II}: Neural tangent kernel for any architecture.
\newblock \arXiv{2006.14548}, 2020.

\bibitem[Yang \& Littwin(2021)Yang and Littwin]{yang2021tensor}
Yang, G. and Littwin, E.
\newblock Tensor {P}rograms {IIb}: Architectural universality of neural tangent
  kernel training dynamics.
\newblock In \emph{ICML}, 2021.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{wide_resnet2016zagoruyko}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{BMVC}, 2016.

\bibitem[Zhou et~al.(2021)Zhou, Wang, Xian, Chen, and Xu]{zhou2021meta}
Zhou, Y., Wang, Z., Xian, J., Chen, C., and Xu, J.
\newblock Meta-learning with neural tangent kernels.
\newblock In \emph{ICLR}, 2021.

\end{thebibliography}
