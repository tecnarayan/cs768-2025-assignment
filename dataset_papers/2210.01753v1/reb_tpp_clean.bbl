\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alibaba(2018)]{tianchi-taobao-2018}
Alibaba.
\newblock \href {https://tianchi.aliyun.com/dataset/dataDetail?dataId=649}
  {User behavior data from taobao for recommendation}, 2018.

\bibitem[Bakhtin et~al.(2019)Bakhtin, Gross, Ott, Deng, Ranzato, and
  Szlam]{bakhtin2019learning}
Bakhtin, A., Gross, S., Ott, M., Deng, Y., Ranzato, M., and Szlam, A.
\newblock \href {http://arxiv.org/abs/1906.03351} {Real or fake? learning to
  discriminate machine from human generated text}.
\newblock 2019.

\bibitem[Boyd et~al.(2020)Boyd, Bamler, Mandt, and Smyth]{boyd-20-vae}
Boyd, A., Bamler, R., Mandt, S., and Smyth, P.
\newblock \href {https://arxiv.org/pdf/2011.03231.pdf} {User-dependent neural
  sequence models for continuous-time event data}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Brakel et~al.(2013)Brakel, Stroobandt, and
  Schrauwen]{brakel-imputation-2013}
Brakel, P., Stroobandt, D., and Schrauwen, B.
\newblock \href {https://jmlr.org/papers/v14/brakel13a.html} {Training
  energy-based models for time-series imputation}.
\newblock \emph{J. Mach. Learn. Res.}, 14\penalty0 (1):\penalty0 2771–2797,
  jan 2013.
\newblock ISSN 1532-4435.

\bibitem[Daley \& Vere-Jones(2007)Daley and Vere-Jones]{daley-07-poisson}
Daley, D.~J. and Vere-Jones, D.
\newblock \href {https://link.springer.com/book/10.1007/978-0-387-49835-5}
  {\emph{An Introduction to the Theory of Point Processes, Volume {II}: General
  Theory and Structure}}.
\newblock Springer, 2007.

\bibitem[Deng et~al.(2020)Deng, Bakhtin, Ott, Szlam, and
  Ranzato]{deng-2018-reb}
Deng, Y., Bakhtin, A., Ott, M., Szlam, A., and Ranzato, M.
\newblock \href {https://doi.org/10.48550/ARXIV.2004.11714} {Residual
  energy-based models for text generation}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Deshpande et~al.(2021)Deshpande, Marathe, De, and
  Sarawagi]{Deshpande_2021}
Deshpande, P., Marathe, K., De, A., and Sarawagi, S.
\newblock \href {https://doi.org/10.1145/3437963.3441740} {Long horizon
  forecasting with temporal point processes}.
\newblock In \emph{Proceedings of the 14th {ACM} International Conference on
  Web Search and Data Mining}. {ACM}, mar 2021.

\bibitem[Du et~al.(2016)Du, Dai, Trivedi, Upadhyay, Gomez-Rodriguez, and
  Song]{du-16-recurrent}
Du, N., Dai, H., Trivedi, R., Upadhyay, U., Gomez-Rodriguez, M., and Song, L.
\newblock \href {https://www.kdd.org/kdd2016/papers/files/rpp1081-duA.pdf}
  {Recurrent marked temporal point processes: Embedding event history to
  vector}.
\newblock In \emph{Proceedings of the ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining}, 2016.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du-ebm-2019}
Du, Y. and Mordatch, I.
\newblock \href
  {https://papers.nips.cc/paper/2019/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html}
  {Implicit generation and modeling with energy based models}.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Enguehard et~al.(2020)Enguehard, Busbridge, Bozson, Woodcock, and
  Hammerla]{enguehard-2020-neural}
Enguehard, J., Busbridge, D., Bozson, A., Woodcock, C., and Hammerla, N.
\newblock \href
  {http://proceedings.mlr.press/v136/enguehard20a/enguehard20a.pdf} {Neural
  temporal point processes [for] modelling electronic health records}.
\newblock In \emph{Proceedings of Machine Learning Research}, volume 136, pp.\
  85--113, 2020.
\newblock NeurIPS 2020 Workshop on Machine Learning for Health (ML4H).

\bibitem[Goyal(2021)]{goyal2021characterizing}
Goyal, K.
\newblock \href {https://arxiv.org/abs/2112.08914} {\emph{Characterizing and
  Overcoming the Limitations of Neural Autoregressive Models}}.
\newblock PhD thesis, Carnegie Mellon University, 2021.

\bibitem[Guan et~al.(2021)Guan, Mao, Fan, Liu, Ding, and Huang]{guan2021long}
Guan, J., Mao, X., Fan, C., Liu, Z., Ding, W., and Huang, M.
\newblock \href {https://arxiv.org/abs/2105.08963} {Long text generation by
  modeling sentence-level and discourse-level coherence}.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2021.

\bibitem[Guo et~al.(2018)Guo, Lu, Cai, Zhang, Yu, and Wang]{guo2018long}
Guo, J., Lu, S., Cai, H., Zhang, W., Yu, Y., and Wang, J.
\newblock \href {https://arxiv.org/abs/1709.08624} {Long text generation via
  adversarial training with leaked information}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem[Gutmann \& Hyv{\"a}rinen(2010)Gutmann and
  Hyv{\"a}rinen]{gutmann-10-nce}
Gutmann, M. and Hyv{\"a}rinen, A.
\newblock \href {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf}
  {Noise-contrastive estimation: {A} new estimation principle for unnormalized
  statistical models}.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2010.

\bibitem[Hawkes(1971)]{hawkes-71}
Hawkes, A.~G.
\newblock \href
  {https://pdfs.semanticscholar.org/c082/06b44dd1f0ea54bd073e4effaf2e4483169b.pdf}
  {Spectra of some self-exciting and mutually exciting point processes}.
\newblock \emph{Biometrika}, 1971.

\bibitem[Hinton(2002)]{hinton-2002}
Hinton, G.~E.
\newblock \href {https://doi.org/10.1162/089976602760128018} {Training products
  of experts by minimizing contrastive divergence}.
\newblock \emph{Neural Comput.}, 14\penalty0 (8):\penalty0 1771–1800, aug
  2002.
\newblock ISSN 0899-7667.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter-97-lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock \href
  {https://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735?casa_token=HqlHuDJ9dM4AAAAA:lItwxEYN6vz_nY6jxWrFjV4pqnFGncshlHoxZepDCPyMXBLLpxKxyxCizs8JDyMy896kaVIaYdOI}
  {Long short-term memory}.
\newblock \emph{Neural Computation}, 1997.

\bibitem[Hopfield(1982)]{hop-1982}
Hopfield, J.
\newblock \href {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/} {{Neural
  networks and physical systems with emergent collective
  computationalabilities}}.
\newblock \emph{National Academy of Sciences of the USA}, 79, 1982.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma-15}
Kingma, D. and Ba, J.
\newblock \href {https://arxiv.org/pdf/1412.6980.pdf} {{Adam}: A method for
  stochastic optimization}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2015.

\bibitem[Le~Guen \& Thome(2019)Le~Guen and Thome]{le2019shape}
Le~Guen, V. and Thome, N.
\newblock \href {https://arxiv.org/abs/1909.09020} {Shape and time distortion
  loss for training deep time series forecasting models}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun-2006}
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.-J.
\newblock \href {http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf} {A
  tutorial on energy-based learning}.
\newblock 2006.

\bibitem[Leskovec \& Krevl(2014)Leskovec and Krevl]{snapnets}
Leskovec, J. and Krevl, A.
\newblock \href {https://snap.stanford.edu/data/} {{SNAP} {D}atasets: Stanford
  large network dataset collection}, 2014.

\bibitem[Lewis \& Shedler(1979)Lewis and Shedler]{lewis-79-sim}
Lewis, P.~A. and Shedler, G.~S.
\newblock \href {https://onlinelibrary.wiley.com/doi/10.1002/nav.3800260304}
  {Simulation of nonhomogeneous {Poisson} processes by thinning}.
\newblock \emph{Naval Research Logistics Quarterly}, 1979.

\bibitem[Lin \& McCarthy(2022)Lin and McCarthy]{lin2022on}
Lin, C.-C. and McCarthy, A.~D.
\newblock \href {https://openreview.net/forum?id=SsPCtEY6yCl} {On the
  uncomputability of partition functions in energy-based sequence models}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem[Lin et~al.(2021)Lin, Jaech, Li, Gormley, and
  Eisner]{lin-et-al-2021-naacl}
Lin, C.-C., Jaech, A., Li, X., Gormley, M., and Eisner, J.
\newblock \href {http://cs.jhu.edu/~jason/papers/#lin-et-al-2021-naacl}
  {Limitations of autoregressive models and their alternatives}.
\newblock In \emph{Proceedings of the Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL)}, 2021.

\bibitem[Liniger(2009)]{liniger-09-hawkes}
Liniger, T.~J.
\newblock \href
  {https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/151886/eth-1112-02.pdf}
  {\emph{Multivariate {Hawkes} processes}}.
\newblock Diss., Eidgen{\"o}ssische Technische Hochschule ETH Z{\"u}rich, Nr.
  18403, 2009.

\bibitem[Ma \& Collins(2018)Ma and Collins]{ma-18-nce}
Ma, Z. and Collins, M.
\newblock \href {https://arxiv.org/abs/1809.01812} {Noise-contrastive
  estimation and negative sampling for conditional models: Consistency and
  statistical efficiency}.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, 2018.

\bibitem[Mei \& Eisner(2017)Mei and Eisner]{mei-17-neuralhawkes}
Mei, H. and Eisner, J.
\newblock \href {https://arxiv.org/abs/1612.09328} {The neural {H}awkes
  process: {A} neurally self-modulating multivariate point process}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Mei et~al.(2019)Mei, Qin, and Eisner]{mei-19-smoothing}
Mei, H., Qin, G., and Eisner, J.
\newblock \href {https://arxiv.org/pdf/1905.05570.pdf} {Imputing missing events
  in continuous-time event streams}.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem[Mei et~al.(2020{\natexlab{a}})Mei, Qin, Xu, and
  Eisner]{mei-2020-datalog}
Mei, H., Qin, G., Xu, M., and Eisner, J.
\newblock \href {https://www.cs.jhu.edu/~jason/papers/#mei-et-al-2020-icml}
  {Neural {D}atalog through time: Informed temporal modeling via logical
  specification}.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2020{\natexlab{a}}.

\bibitem[Mei et~al.(2020{\natexlab{b}})Mei, Wan, and Eisner]{mei-2020-nce}
Mei, H., Wan, T., and Eisner, J.
\newblock \href
  {https://papers.nips.cc/paper/2020/file/37e7897f62e8d91b1ce60515829ca282-Paper.pdf}
  {Noise-contrastive estimation for multivariate point processes}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{b}}.

\bibitem[Mnih \& Teh(2012)Mnih and Teh]{mnih-12-nce}
Mnih, A. and Teh, Y.~W.
\newblock \href {https://arxiv.org/abs/1206.6426} {A fast and simple algorithm
  for training neural probabilistic language models}.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2012.

\bibitem[Ngiam et~al.(2011)Ngiam, Chen, Koh, and Ng]{ng-learn-dem-2011}
Ngiam, J., Chen, Z., Koh, P.~W., and Ng, A.~Y.
\newblock \href
  {https://ai.stanford.edu/~ang/papers/icml11-DeepEnergyModels.pdf} {Learning
  deep energy models}.
\newblock ICML'11, pp.\  1105–1112, Madison, WI, USA, 2011. Omnipress.
\newblock ISBN 9781450306195.

\bibitem[O'Connor \& Andreas(2021)O'Connor and Andreas]{o2021context}
O'Connor, J. and Andreas, J.
\newblock \href {https://aclanthology.org/2021.acl-long.70.pdf} {What context
  features can transformer language models use?}
\newblock In \emph{Proceedings of the Annual Meeting of the Association for
  Computational Linguistics (ACL)}, 2021.

\bibitem[Omi et~al.(2019)Omi, Ueda, and Aihara]{omi-19-fully}
Omi, T., Ueda, N., and Aihara, K.
\newblock \href {https://arxiv.org/abs/1905.09690} {Fully neural network based
  model for general temporal point processes}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Oord et~al.(2016)Oord, Dieleman, Zen, Simonyan, Vinyals, Graves,
  Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
Oord, A. v.~d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A.,
  Kalchbrenner, N., Senior, A., and Kavukcuoglu, K.
\newblock \href {https://arxiv.org/abs/1609.03499} {Wavenet: A generative model
  for raw audio}.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[Pang et~al.(2021)Pang, Zhao, Xie, and Wu]{pang-2021}
Pang, B., Zhao, T., Xie, X., and Wu, Y.
\newblock \href {https://arxiv.org/abs/2104.03086} {Trajectory prediction with
  latent belief energy-based model}.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 04
  2021.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke-17-pytorch}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock \href {https://openreview.net/pdf?id=BJJsrmfCZ} {Automatic
  differentiation in {PyTorch}}.
\newblock 2017.

\bibitem[P{\'e}rez et~al.(2019)P{\'e}rez, Marinkovi{\'c}, and
  Barcel{\'o}]{perez2019turing}
P{\'e}rez, J., Marinkovi{\'c}, J., and Barcel{\'o}, P.
\newblock \href {https://arxiv.org/abs/1901.03429} {On the turing completeness
  of modern neural network architectures}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford-2019-gpt}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock \href {http://www.persagen.com/files/misc/radford2019language.pdf}
  {Language models are unsupervised multitask learners}.
\newblock 2019.

\bibitem[Ranzato et~al.(2007)Ranzato, Boureau, Chopra, and
  LeCun]{ranzato-2007-eb}
Ranzato, M., Boureau, Y.-L., Chopra, S., and LeCun, Y.
\newblock \href {https://proceedings.mlr.press/v2/ranzato07a.html} {A unified
  energy-based framework for unsupervised learning}.
\newblock In Meila, M. and Shen, X. (eds.), \emph{Proceedings of the Eleventh
  International Conference on Artificial Intelligence and Statistics}, volume~2
  of \emph{Proceedings of Machine Learning Research}, pp.\  371--379, San Juan,
  Puerto Rico, 21--24 Mar 2007. PMLR.

\bibitem[Ranzato et~al.(2016)Ranzato, Chopra, Auli, and Zaremba]{ranzato-2016}
Ranzato, M., Chopra, S., Auli, M., and Zaremba, W.
\newblock \href {https://arxiv.org/abs/1511.06732} {Sequence level training
  with recurrent neural networks}.
\newblock In \emph{International Conference on Learning Representation}, 2016.

\bibitem[Sharma et~al.(2021)Sharma, Zhang, Ferrara, and
  Liu]{sharma-2021-identifying}
Sharma, K., Zhang, Y., Ferrara, E., and Liu, Y.
\newblock \href {https://arxiv.org/abs/2008.11308} {Identifying coordinated
  accounts on social media through hidden influence and group behaviours}.
\newblock In \emph{Proceedings of the ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining}, 2021.

\bibitem[Shchur et~al.(2020)Shchur, Bilo{\v{s}}, and
  G{\"u}nnemann]{shchur-20-intensity}
Shchur, O., Bilo{\v{s}}, M., and G{\"u}nnemann, S.
\newblock \href {https://arxiv.org/abs/1909.12127} {Intensity-free learning of
  temporal point processes}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani-2017-transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock \href {https://arxiv.org/pdf/1706.03762.pdf} {Attention is all you
  need}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Whong(2014)]{whong-14-taxi}
Whong, C.
\newblock \href {https://chriswhong.com/open-data/foil_nyc_taxi/} {F{OIL}ing
  {NYC}’s taxi trip data}, 2014.

\bibitem[Xiao et~al.(2017{\natexlab{a}})Xiao, Yan, Farajtabar, Song, Yang, and
  Zha]{xiao-17-joint}
Xiao, S., Yan, J., Farajtabar, M., Song, L., Yang, X., and Zha, H.
\newblock \href {https://arxiv.org/pdf/1703.08524.pdf} {Joint modeling of event
  sequence and time series with attentional twin recurrent neural networks}.
\newblock \emph{arXiv preprint arXiv:1703.08524}, 2017{\natexlab{a}}.

\bibitem[Xiao et~al.(2017{\natexlab{b}})Xiao, Yan, Yang, Zha, and
  Chu]{xiao-17-modeling}
Xiao, S., Yan, J., Yang, X., Zha, H., and Chu, S.
\newblock \href {https://arxiv.org/abs/1705.08982} {Modeling the intensity
  function of point process via recurrent neural networks}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2017{\natexlab{b}}.

\bibitem[Xie et~al.(2019)Xie, Zhu, and Wu]{xie-2019}
Xie, J., Zhu, S.~C., and Wu, Y.~N.
\newblock \href {https://doi.org/10.1109/TPAMI.2019.2934852} {Learning
  energy-based spatial-temporal generative convnets for dynamic patterns}.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2019.

\bibitem[Yang et~al.(2022)Yang, Mei, and Eisner]{yang-2022-transformer}
Yang, C., Mei, H., and Eisner, J.
\newblock \href {https://arxiv.org/abs/2201.00044} {Transformer embeddings of
  irregularly spaced events and their participants}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2022.

\bibitem[Yu et~al.(2019)Yu, Zheng, Anandkumar, and Yue]{yu2019long}
Yu, R., Zheng, S., Anandkumar, A., and Yue, Y.
\newblock \href {https://arxiv.org/abs/1711.00073} {Long-term forecasting using
  higher order tensor rnns}.
\newblock 2019.

\bibitem[Zhang et~al.(2020)Zhang, Lipani, Kirnap, and Yilmaz]{zhang-2020-self}
Zhang, Q., Lipani, A., Kirnap, O., and Yilmaz, E.
\newblock \href {http://proceedings.mlr.press/v119/zhang20q/zhang20q.pdf}
  {Self-attentive {H}awkes process}.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Zhu et~al.(2021)Zhu, Zhang, Ding, and Xie]{zhu-2020-deep}
Zhu, S., Zhang, M., Ding, R., and Xie, Y.
\newblock \href {https://arxiv.org/abs/2002.07281} {Deep {F}ourier kernel for
  self-attentive point processes}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem[Zuo et~al.(2020)Zuo, Jiang, Li, Zhao, and Zha]{zuo2020transformer}
Zuo, S., Jiang, H., Li, Z., Zhao, T., and Zha, H.
\newblock \href {https://arxiv.org/pdf/2002.09291.pdf} {Transformer {H}awkes
  process}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11692--11702. PMLR, 2020.

\end{thebibliography}
