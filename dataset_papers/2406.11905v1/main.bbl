\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on Machine learning}, pp.\ ~1, 2004.

\bibitem[Barnes et~al.(2023)Barnes, Abueg, Lange, Deeds, Trader, Molitor, Wulfmeier, and O'Banion]{barnes2023massively}
Barnes, M., Abueg, M., Lange, O.~F., Deeds, M., Trader, J., Molitor, D., Wulfmeier, M., and O'Banion, S.
\newblock Massively scalable inverse reinforcement learning in google maps.
\newblock \emph{arXiv preprint arXiv:2305.11290}, 2023.

\bibitem[Beyer(2000)]{BEYER2000239}
Beyer, H.-G.
\newblock Evolutionary algorithms in noisy environments: theoretical issues and guidelines for practice.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering}, 186\penalty0 (2):\penalty0 239--267, 2000.
\newblock ISSN 0045-7825.
\newblock \doi{https://doi.org/10.1016/S0045-7825(99)00386-2}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0045782599003862}.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary, Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Cooke et~al.(2023)Cooke, Klyne, Zhang, Laidlaw, Tambe, and Doshi-Velez]{cooke2023toward}
Cooke, L.~H., Klyne, H., Zhang, E., Laidlaw, C., Tambe, M., and Doshi-Velez, F.
\newblock Toward computationally efficient inverse reinforcement learning via reward shaping.
\newblock \emph{arXiv preprint arXiv:2312.09983}, 2023.

\bibitem[Elfwing et~al.(2018)Elfwing, Uchibe, and Doya]{ELFWING20183}
Elfwing, S., Uchibe, E., and Doya, K.
\newblock Sigmoid-weighted linear units for neural network function approximation in reinforcement learning.
\newblock \emph{Neural Networks}, 107:\penalty0 3--11, 2018.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/j.neunet.2017.12.012}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0893608017302976}.
\newblock Special issue on deep reinforcement learning.

\bibitem[Finn et~al.(2016)Finn, Christiano, Abbeel, and Levine]{finn2016connection}
Finn, C., Christiano, P., Abbeel, P., and Levine, S.
\newblock A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models, 2016.

\bibitem[Flajolet et~al.(2022)Flajolet, Monroc, Beguir, and Pierrot]{flajolet2022fast}
Flajolet, A., Monroc, C.~B., Beguir, K., and Pierrot, T.
\newblock Fast population-based reinforcement learning on a single machine.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6533--6547. PMLR, 2022.

\bibitem[Freeman et~al.(2021)Freeman, Frey, Raichuk, Girgin, Mordatch, and Bachem]{brax2021github}
Freeman, C.~D., Frey, E., Raichuk, A., Girgin, S., Mordatch, I., and Bachem, O.
\newblock Brax - a differentiable physics engine for large scale rigid body simulation, 2021.
\newblock URL \url{http://github.com/google/brax}.

\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{fu2018learning}
Fu, J., Luo, K., and Levine, S.
\newblock Learning robust rewards with adversarial inverse reinforcement learning, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Gulino et~al.(2024)Gulino, Fu, Luo, Tucker, Bronstein, Lu, Harb, Pan, Wang, Chen, et~al.]{gulino2024waymax}
Gulino, C., Fu, J., Luo, W., Tucker, G., Bronstein, E., Lu, Y., Harb, J., Pan, X., Wang, Y., Chen, X., et~al.
\newblock Waymax: An accelerated, data-driven simulator for large-scale autonomous driving research.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and Courville]{gulrajani2017improved}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.
\newblock Improved training of wasserstein gans, 2017.

\bibitem[Hansen \& Ostermeier(2001)Hansen and Ostermeier]{cmaes}
Hansen, N. and Ostermeier, A.
\newblock Completely derandomized self-adaptation in evolution strategies.
\newblock \emph{Evolutionary Computation}, 9\penalty0 (2):\penalty0 159--195, 2001.
\newblock \doi{10.1162/106365601750190398}.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning, 2016.

\bibitem[Houthooft et~al.(2018)Houthooft, Chen, Isola, Stadie, Wolski, Ho, and Abbeel]{houthooft2018evolved}
Houthooft, R., Chen, R.~Y., Isola, P., Stadie, B.~C., Wolski, F., Ho, J., and Abbeel, P.
\newblock Evolved policy gradients, 2018.

\bibitem[Jackson et~al.(2023)Jackson, Lu, Kirsch, Lange, Whiteson, and Foerster]{jackson2023discovering}
Jackson, M.~T., Lu, C., Kirsch, L., Lange, R.~T., Whiteson, S., and Foerster, J.~N.
\newblock Discovering temporally-aware reinforcement learning algorithms.
\newblock In \emph{Second Agent Learning in Open-Endedness Workshop}, 2023.

\bibitem[Jaderberg et~al.(2019)Jaderberg, Czarnecki, Dunning, Marris, Lever, Castañeda, Beattie, Rabinowitz, Morcos, Ruderman, Sonnerat, Green, Deason, Leibo, Silver, Hassabis, Kavukcuoglu, and Graepel]{Jaderberg_2019}
Jaderberg, M., Czarnecki, W.~M., Dunning, I., Marris, L., Lever, G., Castañeda, A.~G., Beattie, C., Rabinowitz, N.~C., Morcos, A.~S., Ruderman, A., Sonnerat, N., Green, T., Deason, L., Leibo, J.~Z., Silver, D., Hassabis, D., Kavukcuoglu, K., and Graepel, T.
\newblock Human-level performance in 3d multiplayer games with population-based reinforcement learning.
\newblock \emph{Science}, 364\penalty0 (6443):\penalty0 859–865, May 2019.
\newblock ISSN 1095-9203.
\newblock \doi{10.1126/science.aau6249}.
\newblock URL \url{http://dx.doi.org/10.1126/science.aau6249}.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.~M.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock University of London, University College London (United Kingdom), 2003.

\bibitem[Khan et~al.(2023)Khan, Willi, Kwan, Tacchetti, Lu, Grefenstette, Rockt{\"a}schel, and Foerster]{khan2023scaling}
Khan, A., Willi, T., Kwan, N., Tacchetti, A., Lu, C., Grefenstette, E., Rockt{\"a}schel, T., and Foerster, J.
\newblock Scaling opponent shaping to high dimensional games.
\newblock \emph{arXiv preprint arXiv:2312.12568}, 2023.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 21810--21823, 2020.

\bibitem[Kitani et~al.(2012)Kitani, Ziebart, Bagnell, and Hebert]{kitani2012activity}
Kitani, K.~M., Ziebart, B.~D., Bagnell, J.~A., and Hebert, M.
\newblock Activity forecasting.
\newblock In \emph{Computer Vision--ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part IV 12}, pp.\  201--214. Springer, 2012.

\bibitem[Kolter et~al.(2008)Kolter, Rodgers, and Ng]{kolter2008control}
Kolter, J.~Z., Rodgers, M.~P., and Ng, A.~Y.
\newblock A control architecture for quadruped locomotion over rough terrain.
\newblock In \emph{2008 IEEE International Conference on Robotics and Automation}, pp.\  811--818. IEEE, 2008.

\bibitem[Laidlaw et~al.(2023)Laidlaw, Russell, and Dragan]{laidlaw2023bridging}
Laidlaw, C., Russell, S., and Dragan, A.
\newblock Bridging rl theory and practice with the effective horizon.
\newblock \emph{arXiv preprint arXiv:2304.09853}, 2023.

\bibitem[Lange(2023)]{lange2023evosax}
Lange, R.~T.
\newblock evosax: Jax-based evolution strategies.
\newblock In \emph{Proceedings of the Companion Conference on Genetic and Evolutionary Computation}, pp.\  659--662, 2023.

\bibitem[Likhachev et~al.(2003)Likhachev, Gordon, and Thrun]{likhachev2003ara}
Likhachev, M., Gordon, G.~J., and Thrun, S.
\newblock Ara*: Anytime a* with provable bounds on sub-optimality.
\newblock \emph{Advances in neural information processing systems}, 16, 2003.

\bibitem[Likhachev et~al.(2005)Likhachev, Stentz, and Thrun]{likhachev2005anytime}
Likhachev, M., Stentz, A., and Thrun, S.
\newblock Anytime dynamic a*: An anytime, replanning algorithm.
\newblock 2005.

\bibitem[Liu et~al.(2022)Liu, Feng, Ren, Mai, Zhu, Zhang, Wang, and Yang]{liu2022theoretical}
Liu, B., Feng, X., Ren, J., Mai, L., Zhu, R., Zhang, H., Wang, J., and Yang, Y.
\newblock A theoretical understanding of gradient bias in meta-reinforcement learning, 2022.

\bibitem[Lu et~al.(2022{\natexlab{a}})Lu, Kuba, Letcher, Metz, Schroeder~de Witt, and Foerster]{lu2022discovered}
Lu, C., Kuba, J., Letcher, A., Metz, L., Schroeder~de Witt, C., and Foerster, J.
\newblock Discovered policy optimisation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16455--16468, 2022{\natexlab{a}}.

\bibitem[Lu et~al.(2022{\natexlab{b}})Lu, Willi, De~Witt, and Foerster]{lu2022model}
Lu, C., Willi, T., De~Witt, C. A.~S., and Foerster, J.
\newblock Model-free opponent shaping.
\newblock In \emph{International Conference on Machine Learning}, pp.\  14398--14411. PMLR, 2022{\natexlab{b}}.

\bibitem[Lu et~al.(2023)Lu, Willi, Letcher, and Foerster]{lu2023adversarial}
Lu, C., Willi, T., Letcher, A., and Foerster, J.~N.
\newblock Adversarial cheap talk.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22917--22941. PMLR, 2023.

\bibitem[Lupu et~al.(2024)Lupu, Lu, Liesen, Lange, and Foerster]{lupu2024behaviour}
Lupu, A., Lu, C., Liesen, J.~L., Lange, R.~T., and Foerster, J.~N.
\newblock Behaviour distillation.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=qup9xD8mW4}.

\bibitem[McMahan(2011)]{mcmahan2011follow}
McMahan, B.
\newblock Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization.
\newblock In \emph{Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}, pp.\  525--533. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Metz et~al.(2022)Metz, Freeman, Schoenholz, and Kachman]{metz2022gradients}
Metz, L., Freeman, C.~D., Schoenholz, S.~S., and Kachman, T.
\newblock Gradients are not all you need, 2022.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and application to reward shaping.
\newblock In \emph{Icml}, volume~99, pp.\  278--287. Citeseer, 1999.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, pp.\ ~2, 2000.

\bibitem[Ng et~al.(2006)Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger, and Liang]{ng2006autonomous}
Ng, A.~Y., Coates, A., Diel, M., Ganapathi, V., Schulte, J., Tse, B., Berger, E., and Liang, E.
\newblock Autonomous inverted helicopter flight via reinforcement learning.
\newblock In \emph{Experimental robotics IX}, pp.\  363--372. Springer, 2006.

\bibitem[Niekum et~al.(2010)Niekum, Barto, and Spector]{niekum}
Niekum, S., Barto, A., and Spector, L.
\newblock Genetic programming for reward function search.
\newblock \emph{Autonomous Mental Development, IEEE Transactions on}, 2:\penalty0 83 -- 90, 07 2010.
\newblock \doi{10.1109/TAMD.2010.2051436}.

\bibitem[Pomerleau(1988)]{pomerleau1988alvinn}
Pomerleau, D.~A.
\newblock Alvinn: An autonomous land vehicle in a neural network.
\newblock \emph{Advances in neural information processing systems}, 1, 1988.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Ratliff et~al.(2009)Ratliff, Silver, and Bagnell]{ratliff2009learning}
Ratliff, N.~D., Silver, D., and Bagnell, J.~A.
\newblock Learning to search: Functional gradient techniques for imitation learning.
\newblock \emph{Autonomous Robots}, 27\penalty0 (1):\penalty0 25--53, 2009.

\bibitem[Real et~al.(2019)Real, Aggarwal, Huang, and Le]{Real_Aggarwal_Huang_Le_2019}
Real, E., Aggarwal, A., Huang, Y., and Le, Q.~V.
\newblock Regularized evolution for image classifier architecture search.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 33\penalty0 (01):\penalty0 4780--4789, Jul. 2019.
\newblock \doi{10.1609/aaai.v33i01.33014780}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/4405}.

\bibitem[Ren et~al.(2023)Ren, Swamy, Wu, Bagnell, and Choudhury]{ren2023hyrbid}
Ren, J., Swamy, G., Wu, Z.~S., Bagnell, J.~A., and Choudhury, S.
\newblock Hybrid inverse reinforcement learning.
\newblock 2023.
\newblock URL \url{https://www.robot-learning.ml/2023/files/paper42.pdf}.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
Ross, S., Gordon, G., and Bagnell, D.
\newblock A reduction of imitation learning and structured prediction to no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on artificial intelligence and statistics}, pp.\  627--635. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Russell \& Norvig(2010)Russell and Norvig]{russell2010artificial}
Russell, S.~J. and Norvig, P.
\newblock \emph{Artificial intelligence a modern approach}.
\newblock 2010.

\bibitem[Rutherford et~al.(2023)Rutherford, Ellis, Gallici, Cook, Lupu, Ingvarsson, Willi, Khan, de~Witt, Souly, et~al.]{rutherford2023jaxmarl}
Rutherford, A., Ellis, B., Gallici, M., Cook, J., Lupu, A., Ingvarsson, G., Willi, T., Khan, A., de~Witt, C.~S., Souly, A., et~al.
\newblock Jaxmarl: Multi-agent rl environments in jax.
\newblock \emph{arXiv preprint arXiv:2311.10090}, 2023.

\bibitem[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and Sutskever]{salimans2017evolution}
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I.
\newblock Evolution strategies as a scalable alternative to reinforcement learning, 2017.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms, 2017.

\bibitem[Silver et~al.(2010)Silver, Bagnell, and Stentz]{silver2010learning}
Silver, D., Bagnell, J.~A., and Stentz, A.
\newblock Learning from demonstration for autonomous navigation in complex unstructured terrain.
\newblock \emph{The International Journal of Robotics Research}, 29\penalty0 (12):\penalty0 1565--1592, 2010.

\bibitem[Such et~al.(2018)Such, Madhavan, Conti, Lehman, Stanley, and Clune]{such2018deep}
Such, F.~P., Madhavan, V., Conti, E., Lehman, J., Stanley, K.~O., and Clune, J.
\newblock Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning, 2018.

\bibitem[Swamy et~al.(2021)Swamy, Choudhury, Bagnell, and Wu]{swamy2021moments}
Swamy, G., Choudhury, S., Bagnell, J.~A., and Wu, S.
\newblock Of moments and matching: A game-theoretic framework for closing the imitation gap.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10022--10032. PMLR, 2021.

\bibitem[Swamy et~al.(2022)Swamy, Rajaraman, Peng, Choudhury, Bagnell, Wu, Jiao, and Ramchandran]{swamy2022minimax}
Swamy, G., Rajaraman, N., Peng, M., Choudhury, S., Bagnell, J., Wu, S.~Z., Jiao, J., and Ramchandran, K.
\newblock Minimax optimal online imitation learning via replay estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 7077--7088, 2022.

\bibitem[Swamy et~al.(2023)Swamy, Choudhury, Bagnell, and Wu]{swamy2023inverse}
Swamy, G., Choudhury, S., Bagnell, J.~A., and Wu, Z.~S.
\newblock Inverse reinforcement learning without reinforcement learning, 2023.

\bibitem[Syed \& Schapire(2007)Syed and Schapire]{syed2007game}
Syed, U. and Schapire, R.~E.
\newblock A game-theoretic approach to apprenticeship learning.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Tiapkin et~al.(2023)Tiapkin, Belomestny, Calandriello, Moulines, Naumov, Perrault, Valko, and Menard]{tiapkin2023regularized}
Tiapkin, D., Belomestny, D., Calandriello, D., Moulines, E., Naumov, A., Perrault, P., Valko, M., and Menard, P.
\newblock Regularized rl.
\newblock \emph{arXiv preprint arXiv:2310.17303}, 2023.

\bibitem[Vinitsky et~al.(2022)Vinitsky, Lichtl{\'e}, Yang, Amos, and Foerster]{vinitsky2022nocturne}
Vinitsky, E., Lichtl{\'e}, N., Yang, X., Amos, B., and Foerster, J.
\newblock Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3962--3974, 2022.

\bibitem[Werbos(1990)]{58337}
Werbos, P.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0 1550--1560, 1990.
\newblock \doi{10.1109/5.58337}.

\bibitem[Wu \& Tian(2017)Wu and Tian]{Wu2017Doom}
Wu, Y. and Tian, Y.
\newblock Training agent for first-person shooter game with actor-critic curriculum learning.
\newblock 2017.

\bibitem[Ziebart et~al.(2012)Ziebart, Dey, and Bagnell]{ziebart2012probabilistic}
Ziebart, B., Dey, A., and Bagnell, J.~A.
\newblock Probabilistic pointing target prediction via inverse optimal control.
\newblock In \emph{Proceedings of the 2012 ACM international conference on Intelligent User Interfaces}, pp.\  1--10, 2012.

\bibitem[Ziebart et~al.(2008{\natexlab{a}})Ziebart, Maas, Bagnell, Dey, et~al.]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., Dey, A.~K., et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pp.\  1433--1438. Chicago, IL, USA, 2008{\natexlab{a}}.

\bibitem[Ziebart et~al.(2008{\natexlab{b}})Ziebart, Maas, Dey, and Bagnell]{ziebart2008navigate}
Ziebart, B.~D., Maas, A.~L., Dey, A.~K., and Bagnell, J.~A.
\newblock Navigate like a cabbie: Probabilistic reasoning from observed context-aware behavior.
\newblock In \emph{Proceedings of the 10th international conference on Ubiquitous computing}, pp.\  322--331, 2008{\natexlab{b}}.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient ascent.
\newblock In \emph{Proceedings of the 20th international conference on machine learning (icml-03)}, pp.\  928--936, 2003.

\bibitem[Zucker et~al.(2011)Zucker, Ratliff, Stolle, Chestnutt, Bagnell, Atkeson, and Kuffner]{zucker2011optimization}
Zucker, M., Ratliff, N., Stolle, M., Chestnutt, J., Bagnell, J.~A., Atkeson, C.~G., and Kuffner, J.
\newblock Optimization and learning for rough terrain legged locomotion.
\newblock \emph{The International Journal of Robotics Research}, 30\penalty0 (2):\penalty0 175--191, 2011.

\end{thebibliography}
