\begin{thebibliography}{10}

\bibitem{brack2023sega}
M.~Brack, F.~Friedrich, D.~Hintersdorf, L.~Struppek, P.~Schramowski, and
  K.~Kersting.
\newblock Sega: Instructing diffusion using semantic dimensions.
\newblock {\em arXiv preprint arXiv:2301.12247}, 2023.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{caron2021emerging}
M.~Caron, H.~Touvron, I.~Misra, H.~J{\'e}gou, J.~Mairal, P.~Bojanowski, and
  A.~Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 9650--9660, 2021.

\bibitem{chefer2023attend}
H.~Chefer, Y.~Alaluf, Y.~Vinker, L.~Wolf, and D.~Cohen-Or.
\newblock Attend-and-excite: Attention-based semantic guidance for
  text-to-image diffusion models.
\newblock {\em arXiv preprint arXiv:2301.13826}, 2023.

\bibitem{choi2020stargan}
Y.~Choi, Y.~Uh, J.~Yoo, and J.-W. Ha.
\newblock Stargan v2: Diverse image synthesis for multiple domains.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 8188--8197, 2020.

\bibitem{demircigil2017model}
M.~Demircigil, J.~Heusel, M.~L{\"o}we, S.~Upgang, and F.~Vermet.
\newblock On a model of associative memory with huge storage capacity.
\newblock {\em Journal of Statistical Physics}, 168:288--299, 2017.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{du2020compositional}
Y.~Du, S.~Li, and I.~Mordatch.
\newblock Compositional visual generation with energy based models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6637--6647, 2020.

\bibitem{du2020improved}
Y.~Du, S.~Li, J.~Tenenbaum, and I.~Mordatch.
\newblock Improved contrastive divergence training of energy based models.
\newblock {\em arXiv preprint arXiv:2012.01316}, 2020.

\bibitem{du2019implicit}
Y.~Du and I.~Mordatch.
\newblock Implicit generation and generalization in energy-based models.
\newblock {\em arXiv preprint arXiv:1903.08689}, 2019.

\bibitem{feng2022training}
W.~Feng, X.~He, T.-J. Fu, V.~Jampani, A.~Akula, P.~Narayana, S.~Basu, X.~E.
  Wang, and W.~Y. Wang.
\newblock Training-free structured diffusion guidance for compositional
  text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2212.05032}, 2022.

\bibitem{hertz2022prompt}
A.~Hertz, R.~Mokady, J.~Tenenbaum, K.~Aberman, Y.~Pritch, and D.~Cohen-Or.
\newblock Prompt-to-prompt image editing with cross attention control.
\newblock {\em arXiv preprint arXiv:2208.01626}, 2022.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{hoover2023energy}
B.~Hoover, Y.~Liang, B.~Pham, R.~Panda, H.~Strobelt, D.~H. Chau, M.~J. Zaki,
  and D.~Krotov.
\newblock Energy transformer.
\newblock {\em arXiv preprint arXiv:2302.07253}, 2023.

\bibitem{huang2018introvae}
H.~Huang, R.~He, Z.~Sun, T.~Tan, et~al.
\newblock Introvae: Introspective variational autoencoders for photographic
  image synthesis.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{karras2022elucidating}
T.~Karras, M.~Aittala, T.~Aila, and S.~Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock In {\em Proc. NeurIPS}, 2022.

\bibitem{karras2019style}
T.~Karras, S.~Laine, and T.~Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 4401--4410, 2019.

\bibitem{kingma2021variational}
D.~P. Kingma, T.~Salimans, B.~Poole, and J.~Ho.
\newblock Variational diffusion models.
\newblock {\em arXiv preprint arXiv:2107.00630}, 2021.

\bibitem{krotov2018dense}
D.~Krotov and J.~Hopfield.
\newblock Dense associative memory is robust to adversarial inputs.
\newblock {\em Neural computation}, 30(12):3151--3167, 2018.

\bibitem{li2022blip}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em International Conference on Machine Learning}, pages
  12888--12900. PMLR, 2022.

\bibitem{liu2022pseudo}
L.~Liu, Y.~Ren, Z.~Lin, and Z.~Zhao.
\newblock Pseudo numerical methods for diffusion models on manifolds.
\newblock {\em arXiv preprint arXiv:2202.09778}, 2022.

\bibitem{liu2022compositional}
N.~Liu, S.~Li, Y.~Du, A.~Torralba, and J.~B. Tenenbaum.
\newblock Compositional visual generation with composable diffusion models.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XVII}, pages 423--439.
  Springer, 2022.

\bibitem{lugmayr2022repaint}
A.~Lugmayr, M.~Danelljan, A.~Romero, F.~Yu, R.~Timofte, and L.~Van~Gool.
\newblock Repaint: Inpainting using denoising diffusion probabilistic models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11461--11471, 2022.

\bibitem{meng2021sdedit}
C.~Meng, Y.~Song, J.~Song, J.~Wu, J.-Y. Zhu, and S.~Ermon.
\newblock Sdedit: Image synthesis and editing with stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2108.01073}, 2021.

\bibitem{mokady2022null}
R.~Mokady, A.~Hertz, K.~Aberman, Y.~Pritch, and D.~Cohen-Or.
\newblock Null-text inversion for editing real images using guided diffusion
  models.
\newblock {\em arXiv preprint arXiv:2211.09794}, 2022.

\bibitem{nie2021controllable}
W.~Nie, A.~Vahdat, and A.~Anandkumar.
\newblock Controllable and compositional generation with latent-space
  energy-based models.
\newblock {\em Advances in Neural Information Processing Systems},
  34:13497--13510, 2021.

\bibitem{nijkamp2019learning}
E.~Nijkamp, M.~Hill, S.-C. Zhu, and Y.~N. Wu.
\newblock Learning non-convergent non-persistent short-run mcmc toward
  energy-based model.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{parmar2023zero}
G.~Parmar, K.~K. Singh, R.~Zhang, Y.~Li, J.~Lu, and J.-Y. Zhu.
\newblock Zero-shot image-to-image translation.
\newblock {\em arXiv preprint arXiv:2302.03027}, 2023.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{ramsauer2020hopfield}
H.~Ramsauer, B.~Sch{\"a}fl, J.~Lehner, P.~Seidl, M.~Widrich, T.~Adler,
  L.~Gruber, M.~Holzleitner, M.~Pavlovi{\'c}, G.~K. Sandve, et~al.
\newblock Hopfield networks is all you need.
\newblock {\em arXiv preprint arXiv:2008.02217}, 2020.

\bibitem{rombach2022high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10684--10695, 2022.

\bibitem{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2015: 18th International Conference, Munich, Germany,
  October 5-9, 2015, Proceedings, Part III 18}, pages 234--241. Springer, 2015.

\bibitem{saharia2022photorealistic}
C.~Saharia, W.~Chan, S.~Saxena, L.~Li, J.~Whang, E.~L. Denton, K.~Ghasemipour,
  R.~Gontijo~Lopes, B.~Karagol~Ayan, T.~Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock {\em Advances in Neural Information Processing Systems},
  35:36479--36494, 2022.

\bibitem{schuhmann2022laion}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti,
  T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock {\em arXiv preprint arXiv:2210.08402}, 2022.

\bibitem{song2020denoising}
J.~Song, C.~Meng, and S.~Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{song2020score}
Y.~Song, J.~Sohl-Dickstein, D.~P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{tumanyan2022splicing}
N.~Tumanyan, O.~Bar-Tal, S.~Bagon, and T.~Dekel.
\newblock Splicing vit features for semantic appearance transfer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10748--10757, 2022.

\bibitem{tumanyan2022plug}
N.~Tumanyan, M.~Geyer, S.~Bagon, and T.~Dekel.
\newblock Plug-and-play diffusion features for text-driven image-to-image
  translation.
\newblock {\em arXiv preprint arXiv:2211.12572}, 2022.

\bibitem{wu2022unifying}
C.~H. Wu and F.~De~la Torre.
\newblock Unifying diffusion models' latent space, with applications to
  cyclediffusion and guidance.
\newblock {\em arXiv preprint arXiv:2210.05559}, 2022.

\bibitem{xie2022smartbrush}
S.~Xie, Z.~Zhang, Z.~Lin, T.~Hinz, and K.~Zhang.
\newblock Smartbrush: Text and shape guided object inpainting with diffusion
  model.
\newblock {\em arXiv preprint arXiv:2212.05034}, 2022.

\bibitem{ye2022geometry}
J.~C. Ye.
\newblock {\em Geometry of Deep Learning}.
\newblock Springer, 2022.

\bibitem{yuille2001concave}
A.~L. Yuille and A.~Rangarajan.
\newblock The concave-convex procedure (cccp).
\newblock {\em Advances in neural information processing systems}, 14, 2001.

\end{thebibliography}
