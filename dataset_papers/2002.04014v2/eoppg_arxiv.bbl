\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Agarwal, Kakade, Lee, and Mahajan}{Agarwal
  et~al.}{2019}]{AgarwalAlekh2019OaAw}
Agarwal, A., S.~Kakade, J.~Lee, and G.~Mahajan (2019).
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock {\em arXiv preprint arXiv: 1908.00261\/}.

\bibitem[\protect\citeauthoryear{Antos, Szepesv{\'a}ri, and Munos}{Antos
  et~al.}{2008}]{antos2008learning}
Antos, A., C.~Szepesv{\'a}ri, and R.~Munos (2008).
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock {\em Machine Learning\/}~{\em 71}, 89--129.

\bibitem[\protect\citeauthoryear{Athey and Wager}{Athey and
  Wager}{2017}]{AtheySusan2017EPL}
Athey, S. and S.~Wager (2017).
\newblock Efficient policy learning.
\newblock {\em arXiv preprint arXiv:1702.02896\/}.

\bibitem[\protect\citeauthoryear{Bartlett, Bousquet, and Mendelson}{Bartlett
  et~al.}{2005}]{BartlettPeterL.2005LRc}
Bartlett, P.~L., O.~Bousquet, and S.~Mendelson (2005).
\newblock Local rademacher complexities.
\newblock {\em Annals of Statistics\/}~{\em 33}, 1497--1537.

\bibitem[\protect\citeauthoryear{Baxter and Bartlett}{Baxter and
  Bartlett}{2001}]{baxter2001infinite}
Baxter, J. and P.~L. Bartlett (2001).
\newblock {Infinite-Horizon Policy-Gradient Estimation}.
\newblock {\em Journal of Artificial Intelligence Research\/}~{\em 15},
  319--350.

\bibitem[\protect\citeauthoryear{Bhandari and Russo}{Bhandari and
  Russo}{2019}]{BhandariJalaj2019GOGF}
Bhandari, J. and D.~Russo (2019).
\newblock Global optimality guarantees for policy gradient methods.
\newblock {\em arXiv preprint arXiv:1906.01786\/}.

\bibitem[\protect\citeauthoryear{Bickel, Klaassen, Ritov, and Wellner}{Bickel
  et~al.}{1993}]{bickel1993efficient}
Bickel, P.~J., C.~A. Klaassen, Y.~Ritov, and J.~A. Wellner (1993).
\newblock {\em Efficient and adaptive estimation for semiparametric models}.
\newblock Baltimore: Johns Hopkins University Press.

\bibitem[\protect\citeauthoryear{Chen and Jiang}{Chen and
  Jiang}{2019}]{ChenJinglin2019ICiB}
Chen, J. and N.~Jiang (2019).
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, Volume~97, pp.\  1042--1051.

\bibitem[\protect\citeauthoryear{Chen, Beutel, Covington, Jain, Belletti, and
  Chi}{Chen et~al.}{2019}]{ChenMinmin2019TOCf}
Chen, M., A.~Beutel, P.~Covington, S.~Jain, F.~Belletti, and E.~Chi (2019).
\newblock Top-k off-policy correction for a reinforce recommender system.
\newblock In {\em Proceedings of the Twelfth ACM International Conference on
  web search and data mining}, WSDM '19, pp.\  456--464.

\bibitem[\protect\citeauthoryear{Chen}{Chen}{2007}]{ChenXiaohong2007C7LS}
Chen, X. (2007).
\newblock Chapter 76 large sample sieve estimation of semi-nonparametric
  models.
\newblock {\em Handbook of Econometrics\/}~{\em 6}, 5549--5632.

\bibitem[\protect\citeauthoryear{Cheng, Yan, and Boots}{Cheng
  et~al.}{2019}]{ChengChing-An2019TCVf}
Cheng, C.-A., X.~Yan, and B.~Boots (2019).
\newblock Trajectory-wise control variates for variance reduction in policy
  gradient methods.
\newblock {\em arXiv preprint arxiv:1908.03263\/}.

\bibitem[\protect\citeauthoryear{Chernozhukov, Chetverikov, Demirer, Duflo,
  Hansen, Newey, and Robins}{Chernozhukov
  et~al.}{2018}]{ChernozhukovVictor2018Dmlf}
Chernozhukov, V., D.~Chetverikov, M.~Demirer, E.~Duflo, C.~Hansen, W.~Newey,
  and J.~Robins (2018).
\newblock Double/debiased machine learning for treatment and structural
  parameters.
\newblock {\em Econometrics Journal\/}~{\em 21}, C1--C68.

\bibitem[\protect\citeauthoryear{Dai, Kostrikov, Chow, Li, and Schuurmans}{Dai
  et~al.}{2019}]{DaiBo2019APGf}
Dai, B., I.~Kostrikov, Y.~Chow, L.~Li, and D.~Schuurmans (2019).
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock {\em arXiv preprint arXiv:1912.02074\/}.

\bibitem[\protect\citeauthoryear{Degris, White, and Sutton}{Degris
  et~al.}{2012}]{DegrisThomas2013OA}
Degris, T., M.~White, and R.~Sutton (2012).
\newblock Off-policy actor-critic.
\newblock {\em Proceedings of the 29th International Coference on International
  Conference on Machine Learning\/}, 179--186.

\bibitem[\protect\citeauthoryear{Deisenroth, Neumann, and Peters}{Deisenroth
  et~al.}{2013}]{DeisenrothMarc2013ASoP}
Deisenroth, M., G.~Neumann, and J.~Peters (2013).
\newblock A survey on policy search for robotics.
\newblock {\em Foundations and Trends in Robotics - volume 1\/}~{\em 2},
  1--142.

\bibitem[\protect\citeauthoryear{Foster and Syrgkanis}{Foster and
  Syrgkanis}{2019}]{FosterDylanJ.2019OSL}
Foster, D.~J. and V.~Syrgkanis (2019).
\newblock Orthogonal statistical learning.
\newblock {\em arXiv preprint arXiv:1901.09036\/}.

\bibitem[\protect\citeauthoryear{Fujimoto, Meger, and Precup}{Fujimoto
  et~al.}{2019}]{fujimoto19a}
Fujimoto, S., D.~Meger, and D.~Precup (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, Volume~97, pp.\  2052--2062.

\bibitem[\protect\citeauthoryear{Gottesman, Johansson, Komorowski, Faisal,
  Sontag, Doshi-Velez, and Celi}{Gottesman
  et~al.}{2019}]{gottesman2019guidelines}
Gottesman, O., F.~Johansson, M.~Komorowski, A.~Faisal, D.~Sontag,
  F.~Doshi-Velez, and L.~A. Celi (2019).
\newblock Guidelines for reinforcement learning in healthcare.
\newblock {\em Nat Med\/}~{\em 25}, 16--18.

\bibitem[\protect\citeauthoryear{Greensmith, Bartlett, and Baxter}{Greensmith
  et~al.}{2004}]{Greensmit2004}
Greensmith, E., P.~L. Bartlett, and J.~Baxter (2004).
\newblock Variance reduction techniques for gradient estimates in reinforcement
  learning.
\newblock {\em Journal of Machine Learning Research\/}, 1471--1530.

\bibitem[\protect\citeauthoryear{Gu, Lillicrap, Turner, Ghahramani,
  Sch\"{o}lkopf, and Levine}{Gu et~al.}{2017}]{gu2017}
Gu, S.~S., T.~Lillicrap, R.~E. Turner, Z.~Ghahramani, B.~Sch\"{o}lkopf, and
  S.~Levine (2017).
\newblock Interpolated policy gradient: Merging on-policy and off-policy
  gradient estimation for deep reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems 30}, pp.\
  3846--3855.

\bibitem[\protect\citeauthoryear{H{\'a}jek}{H{\'a}jek}{1970}]{hajek1970characterization}
H{\'a}jek, J. (1970).
\newblock A characterization of limiting distributions of regular estimates.
\newblock {\em Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte
  Gebiete\/}~{\em 14}, 323--330.

\bibitem[\protect\citeauthoryear{Hanna and Stone}{Hanna and
  Stone}{2018}]{AAAISSS2018-Hanna}
Hanna, J. and P.~Stone (2018).
\newblock Towards a data efficient off-policy policy gradient.
\newblock In {\em AAAI Spring Symposium on Data Efficient Reinforcement
  Learning}.

\bibitem[\protect\citeauthoryear{Hazan}{Hazan}{2015}]{HazanElad2015ItOC}
Hazan, E. (2015).
\newblock Introduction to online convex optimization.
\newblock {\em Foundations and Trends® in Optimization\/}~{\em 2}, 157--325.

\bibitem[\protect\citeauthoryear{Huang and Jiang}{Huang and
  Jiang}{2019}]{HuangJiawei2019FISt}
Huang, J. and N.~Jiang (2019).
\newblock From importance sampling to doubly robust policy gradient.
\newblock {\em arXiv preprint arXiv:1910.09066\/}.

\bibitem[\protect\citeauthoryear{Imani, Graves, and White}{Imani
  et~al.}{2018}]{Imani2018}
Imani, E., E.~Graves, and M.~White (2018).
\newblock An off-policy policy gradient theorem using emphatic weightings.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pp.\
  96--106.

\bibitem[\protect\citeauthoryear{Jain and Kar}{Jain and
  Kar}{2017}]{JainPrateek2017NOfM}
Jain, P. and P.~Kar (2017, December).
\newblock Non-convex optimization for machine learning.
\newblock {\em Found. Trends Mach. Learn.\/}~{\em 10}, 142–336.

\bibitem[\protect\citeauthoryear{Jiang and Agarwal}{Jiang and
  Agarwal}{2018}]{pmlr-v75-jiang18a}
Jiang, N. and A.~Agarwal (2018).
\newblock Open problem: The dependence of sample complexity lower bounds on
  planning horizon.
\newblock In {\em Proceedings of the 31st Conference On Learning Theory},
  Volume~75, pp.\  3395--3398.

\bibitem[\protect\citeauthoryear{Jiang and Li}{Jiang and Li}{2016}]{jiang}
Jiang, N. and L.~Li (2016).
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock {\em In Proceedings of the 33rd International Conference on
  International Conference on Machine Learning-Volume\/}, 652--661.

\bibitem[\protect\citeauthoryear{Kallus}{Kallus}{2018}]{kallus2018balanced}
Kallus, N. (2018).
\newblock Balanced policy evaluation and learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pp.\
  8895--8906.

\bibitem[\protect\citeauthoryear{Kallus and Uehara}{Kallus and
  Uehara}{2019a}]{KallusUehara2019}
Kallus, N. and M.~Uehara (2019a).
\newblock Double reinforcement learning for efficient off-policy evaluation in
  markov decision processes.
\newblock {\em arXiv preprint arXiv:1908.08526\/}.

\bibitem[\protect\citeauthoryear{Kallus and Uehara}{Kallus and
  Uehara}{2019b}]{KallusNathan2019EBtC}
Kallus, N. and M.~Uehara (2019b).
\newblock Efficiently breaking the curse of horizon: Double reinforcement
  learning in infinite-horizon processes.
\newblock {\em arXiv preprint arXiv:1909.05850\/}.

\bibitem[\protect\citeauthoryear{Kallus and Zhou}{Kallus and
  Zhou}{2018}]{kallus2018confounding}
Kallus, N. and A.~Zhou (2018).
\newblock Confounding-robust policy improvement.
\newblock In {\em Advances in neural information processing systems}, pp.\
  9269--9279.

\bibitem[\protect\citeauthoryear{Khamaru and Wainwright}{Khamaru and
  Wainwright}{2018}]{khamaru18a}
Khamaru, K. and M.~Wainwright (2018).
\newblock Convergence guarantees for a class of non-convex and non-smooth
  optimization problems.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  2601--2610.

\bibitem[\protect\citeauthoryear{Klaassen}{Klaassen}{1987}]{klaassen1987consistent}
Klaassen, C.~A. (1987).
\newblock Consistent estimation of the influence function of locally
  asymptotically linear estimators.
\newblock {\em The Annals of Statistics\/}, 1548--1562.

\bibitem[\protect\citeauthoryear{Liu, Li, Tang, and Zhou}{Liu
  et~al.}{2018}]{liu2018breaking}
Liu, Q., L.~Li, Z.~Tang, and D.~Zhou (2018).
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pp.\
  5361--5371.

\bibitem[\protect\citeauthoryear{Liu, Swaminathan, Agarwal, and Brunskill}{Liu
  et~al.}{2019}]{LiuYao2019OPGw}
Liu, Y., A.~Swaminathan, A.~Agarwal, and E.~Brunskill (2019).
\newblock Off-policy policy gradient with state distribution correction.
\newblock {\em In Proccedings of the Conference on Uncertainty in Artificial
  Intelligence (UAI 2019)\/}.

\bibitem[\protect\citeauthoryear{Metelli, Papini, Faccio, and Restelli}{Metelli
  et~al.}{2018}]{metelli2018}
Metelli, A.~M., M.~Papini, F.~Faccio, and M.~Restelli (2018).
\newblock Policy optimization via importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pp.\
  5442--5454.

\bibitem[\protect\citeauthoryear{Morimura, Uchibe, Yoshimoto, Peters, and
  Doya}{Morimura et~al.}{2010}]{MorimuraTetsuro2010DoLS}
Morimura, T., E.~Uchibe, J.~Yoshimoto, J.~Peters, and K.~Doya (2010).
\newblock Derivatives of logarithmic stationary distributions for policy
  gradient reinforcement learning.
\newblock {\em Neural Computation\/}~{\em 22}, 342--376.

\bibitem[\protect\citeauthoryear{Munos and Szepesv{\'a}ri}{Munos and
  Szepesv{\'a}ri}{2008}]{munos2008finite}
Munos, R. and C.~Szepesv{\'a}ri (2008).
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research\/}~{\em 9}, 815--857.

\bibitem[\protect\citeauthoryear{Nesterov and Polyak}{Nesterov and
  Polyak}{2006}]{NesterovYurii2006CroN}
Nesterov, Y. and B.~Polyak (2006).
\newblock Cubic regularization of newton method and its global performance.
\newblock {\em Mathematical Programming\/}~{\em 108}, 177--205.

\bibitem[\protect\citeauthoryear{Nie, Brunskill, and Wager}{Nie
  et~al.}{2019}]{NieXinkun2019LWP}
Nie, X., E.~Brunskill, and S.~Wager (2019).
\newblock Learning when-to-treat policies.
\newblock {\em arXiv preprint arXiv:1905.09751\/}.

\bibitem[\protect\citeauthoryear{Papini, Binaghi, Canonaco, Pirotta, and
  Restelli}{Papini et~al.}{2018}]{papini2018stochastic}
Papini, M., D.~Binaghi, G.~Canonaco, M.~Pirotta, and M.~Restelli (2018).
\newblock Stochastic variance-reduced policy gradient.
\newblock In {\em International Conference on Machine Learning}, pp.\
  4026--4035.

\bibitem[\protect\citeauthoryear{Precup, Sutton, and Singh}{Precup
  et~al.}{2000}]{precup2000eligibility}
Precup, D., R.~S. Sutton, and S.~P. Singh (2000).
\newblock {Eligibility Traces for Off-Policy Policy Evaluation}.
\newblock In {\em Proceedings of the 17th International Conference on Machine
  Learning}, pp.\  759--766.

\bibitem[\protect\citeauthoryear{Schulman, Heess, Weber, and Abbeel}{Schulman
  et~al.}{2015}]{Schulmanetal_NIPS2015}
Schulman, J., N.~Heess, T.~Weber, and P.~Abbeel (2015).
\newblock Gradient estimation using stochastic computation graphs.
\newblock In {\em Neural Information Processing Systems (NIPS), Montreal,
  Canada}.

\bibitem[\protect\citeauthoryear{Schulman, Moritz, Levine, Jordan, and
  Abbeel}{Schulman et~al.}{2016}]{Schulmanetal_ICLR2016}
Schulman, J., P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel (2016).
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR)}.

\bibitem[\protect\citeauthoryear{Sen}{Sen}{2018}]{empirical}
Sen, B. (2018).
\newblock A gentle introduction to empirical process theoryand applications.
\newblock
  \url{http://www.stat.columbia.edu/~bodhi/Talks/Emp-Proc-Lecture-Notes.pdf}.
\newblock Accessed: 2020--1-1.

\bibitem[\protect\citeauthoryear{Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller}{Silver et~al.}{2014}]{silver14}
Silver, D., G.~Lever, N.~Heess, T.~Degris, D.~Wierstra, and M.~Riedmiller
  (2014).
\newblock Deterministic policy gradient algorithms.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning}, pp.\  387--395.

\bibitem[\protect\citeauthoryear{Sutton and Barto}{Sutton and
  Barto}{2018}]{sutton2018reinforcement}
Sutton, R.~S. and A.~G. Barto (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[\protect\citeauthoryear{Sutton, Precup, and Singh}{Sutton
  et~al.}{1998}]{sutton1998intra}
Sutton, R.~S., D.~Precup, and S.~P. Singh (1998).
\newblock {Intra-option Learning about Temporally Abstract Actions}.
\newblock In {\em Proceedings of the 15th International Conference on Machine
  Learning}, pp.\  556--564.

\bibitem[\protect\citeauthoryear{Swaminathan and Joachims}{Swaminathan and
  Joachims}{2015}]{swaminathan2015counterfactual}
Swaminathan, A. and T.~Joachims (2015).
\newblock Counterfactual risk minimization: Learning from logged bandit
  feedback.
\newblock In {\em International Conference on Machine Learning}, pp.\
  814--823.

\bibitem[\protect\citeauthoryear{Tang and Abbeel}{Tang and
  Abbeel}{2010}]{jie2010}
Tang, J. and P.~Abbeel (2010).
\newblock On a connection between importance sampling and the likelihood ratio
  policy gradient.
\newblock In {\em Proceedings of the 23rd International Conference on Neural
  Information Processing Systems - Volume 1}, pp.\  1000–1008.

\bibitem[\protect\citeauthoryear{Thomas and Brunskill}{Thomas and
  Brunskill}{2016}]{thomas2016}
Thomas, P. and E.~Brunskill (2016).
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock {\em In Proceedings of the 33rd International Conference on Machine
  Learning\/}, 2139--2148.

\bibitem[\protect\citeauthoryear{Tosatto, Carvalho, Abdulsamad, and
  Peters}{Tosatto et~al.}{2020}]{TosattoSamuele2020ANOP}
Tosatto, S., J.~Carvalho, H.~Abdulsamad, and J.~Peters (2020).
\newblock A nonparametric offpolicy policy gradient.
\newblock {\em arXiv preprint arXiv:2001.02435\/}.

\bibitem[\protect\citeauthoryear{Tsiatis}{Tsiatis}{2006}]{TsiatisAnastasiosA2006STaM}
Tsiatis, A.~A. (2006).
\newblock {\em Semiparametric Theory and Missing Data}.
\newblock Springer Series in Statistics. New York, NY: Springer New York.

\bibitem[\protect\citeauthoryear{van Der~Laan and Robins}{van Der~Laan and
  Robins}{2003}]{LaanMarkJ.vanDer2003UMfC}
van Der~Laan, M.~J. and J.~M. Robins (2003).
\newblock {\em Unified Methods for Censored Longitudinal Data and Causality}.
\newblock Springer Series in Statistics,. New York, NY: Springer New York.

\bibitem[\protect\citeauthoryear{van~der Laan and Rose}{van~der Laan and
  Rose}{2018}]{vanderLaanMarkJ2011TLCI}
van~der Laan, M.~J. and S.~Rose (2018).
\newblock {\em Targeted Learning :Causal Inference for Observational and
  Experimental Data}.
\newblock Springer Series in Statistics. New York, NY: Springer New York :
  Imprint: Springer.

\bibitem[\protect\citeauthoryear{van~der Vaart}{van~der
  Vaart}{1998}]{VaartA.W.vander1998As}
van~der Vaart, A.~W. (1998).
\newblock {\em Asymptotic statistics}.
\newblock Cambridge, UK: Cambridge University Press.

\bibitem[\protect\citeauthoryear{Wainwright}{Wainwright}{2019}]{WainwrightMartinJ2019HS:A}
Wainwright, M.~J. (2019).
\newblock {\em High-Dimensional Statistics : A Non-Asymptotic Viewpoint}.
\newblock New York: Cambridge University Press.

\bibitem[\protect\citeauthoryear{Williams}{Williams}{1992}]{williams1992simple}
Williams, R.~J. (1992).
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning\/}~{\em 8\/}(3-4), 229--256.

\bibitem[\protect\citeauthoryear{Wu, Rajeswaran, Duan, Kumar, Bayen, Kakade,
  Mordatch, and Abbeel}{Wu et~al.}{2018}]{WuCathy2018VRfP}
Wu, C., A.~Rajeswaran, Y.~Duan, V.~Kumar, A.~Bayen, S.~Kakade, I.~Mordatch, and
  P.~Abbeel (2018).
\newblock Variance reduction for policy gradient with action-dependent
  factorized baselines.
\newblock {\em Proceedings of the International Conference on Learning
  Representations (ICLR)\/}.

\bibitem[\protect\citeauthoryear{Xie, Ma, and Wang}{Xie
  et~al.}{2019}]{XieTengyang2019OOEf}
Xie, T., Y.~Ma, and Y.-X. Wang (2019).
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pp.\
  9665--9675.

\bibitem[\protect\citeauthoryear{Zhou, Athey, and Wager}{Zhou
  et~al.}{2018}]{ZhouZhengyuan2018OMPL}
Zhou, Z., S.~Athey, and S.~Wager (2018).
\newblock Offline multi-action policy learning: Generalization and
  optimization.
\newblock {\em arXiv preprint arXiv:1810.04778\/}.

\end{thebibliography}
