\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani and Saxe(2017)]{advani2017high}
M.~S. Advani and A.~M. Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.03667}, 2017.

\bibitem[Advani et~al.(2020)Advani, Saxe, and Sompolinsky]{advani2020high}
M.~S. Advani, A.~M. Saxe, and H.~Sompolinsky.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{Neural Networks}, 132:\penalty0 428--446, 2020.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252, 2019.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8141--8150, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{ICML}, pages 477--502, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Du, Li, Salakhutdinov, Wang,
  and Yu]{arora2019harnessing}
S.~Arora, S.~S. Du, Z.~Li, R.~Salakhutdinov, R.~Wang, and D.~Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{c}}.

\bibitem[Baksalary and Baksalary(2007)]{baksalary2007particular}
J.~K. Baksalary and O.~M. Baksalary.
\newblock Particular formulae for the moore--penrose inverse of a columnwise
  partitioned matrix.
\newblock \emph{Linear algebra and its applications}, 421\penalty0
  (1):\penalty0 16--23, 2007.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
P.~L. Bartlett, P.~M. Long, G.~Lugosi, and A.~Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2020.

\bibitem[Belkin et~al.(2018{\natexlab{a}})Belkin, Hsu, Ma, and
  Mandal]{belkin2018reconciling}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine learning and the bias-variance trade-off.
\newblock \emph{stat}, 1050:\penalty0 28, 2018{\natexlab{a}}.

\bibitem[Belkin et~al.(2018{\natexlab{b}})Belkin, Ma, and
  Mandal]{belkin2018understand}
M.~Belkin, S.~Ma, and S.~Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  541--549, 2018{\natexlab{b}}.

\bibitem[Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, and Xu]{belkin2019two}
M.~Belkin, D.~Hsu, and J.~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{arXiv preprint arXiv:1903.07571}, 2019{\natexlab{a}}.

\bibitem[Belkin et~al.(2019{\natexlab{b}})Belkin, Rakhlin, and
  Tsybakov]{belkin2019does}
M.~Belkin, A.~Rakhlin, and A.~B. Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1611--1619, 2019{\natexlab{b}}.

\bibitem[Bengio et~al.(2003)Bengio, Ducharme, Vincent, and
  Jauvin]{bengio2003neural}
Y.~Bengio, R.~Ducharme, P.~Vincent, and C.~Jauvin.
\newblock A neural probabilistic language model.
\newblock \emph{Journal of machine learning research}, 3\penalty0
  (Feb):\penalty0 1137--1155, 2003.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Y.~Cao and Q.~Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10836--10846, 2019.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007optimal}
A.~Caponnetto and E.~De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Caron and Chretien(2020)]{caron2020finite}
E.~Caron and S.~Chretien.
\newblock A finite sample analysis of the double descent phenomenon for ridge
  function estimation.
\newblock \emph{arXiv preprint arXiv:2007.12882}, 2020.

\bibitem[Caron et~al.(2018)Caron, Bojanowski, Joulin, and Douze]{caron2018deep}
M.~Caron, P.~Bojanowski, A.~Joulin, and M.~Douze.
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 132--149, 2018.

\bibitem[Chen and Xu(2021)]{chen2020deep}
L.~Chen and S.~Xu.
\newblock Deep neural tangent kernel and laplace kernel have the same rkhs.
\newblock In \emph{ICLR}, 2021.

\bibitem[Chen et~al.(2020)Chen, Min, Zhang, and Karbasi]{chen2020more}
L.~Chen, Y.~Min, M.~Zhang, and A.~Karbasi.
\newblock More data can expand the generalization gap between adversarially
  robust and standard models.
\newblock In \emph{International Conference on Machine Learning}, pages
  1670--1680. PMLR, 2020.

\bibitem[Dar et~al.(2020)Dar, Mayer, Luzi, and Baraniuk]{dar2020subspace}
Y.~Dar, P.~Mayer, L.~Luzi, and R.~G. Baraniuk.
\newblock Subspace fitting meets regression: The effects of supervision and
  orthonormality constraints on double descent of generalization errors.
\newblock In \emph{ICML}, 2020.

\bibitem[d'Ascoli et~al.(2020)d'Ascoli, Sagun, and Biroli]{d2020triple}
S.~d'Ascoli, L.~Sagun, and G.~Biroli.
\newblock Triple descent and the two kinds of overfitting: Where \& why do they
  appear?
\newblock \emph{arXiv preprint arXiv:2006.03509}, 2020.

\bibitem[De~Vito et~al.(2005)De~Vito, Caponnetto, and Rosasco]{de2005model}
E.~De~Vito, A.~Caponnetto, and L.~Rosasco.
\newblock Model selection for regularized least-squares algorithm in learning
  theory.
\newblock \emph{Foundations of Computational Mathematics}, 5\penalty0
  (1):\penalty0 59--85, 2005.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1675--1685, 2019.

\bibitem[Fei and Chen(2018{\natexlab{a}})]{fei2018exponential}
Y.~Fei and Y.~Chen.
\newblock Exponential error rates of sdp for block models: Beyond
  grothendieck’s inequality.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (1):\penalty0 551--571, 2018{\natexlab{a}}.

\bibitem[Fei and Chen(2018{\natexlab{b}})]{fei2018hidden}
Y.~Fei and Y.~Chen.
\newblock Hidden integrality of sdp relaxations for sub-gaussian mixture
  models.
\newblock In \emph{Conference On Learning Theory}, pages 1931--1965. PMLR,
  2018{\natexlab{b}}.

\bibitem[Fei and Chen(2019)]{fei2019achieving}
Y.~Fei and Y.~Chen.
\newblock Achieving the bayes error rate in stochastic block model by sdp,
  robustly.
\newblock In \emph{Conference on Learning Theory}, pages 1235--1269. PMLR,
  2019.

\bibitem[Fei and Chen(2020)]{fei2020achieving}
Y.~Fei and Y.~Chen.
\newblock Achieving the bayes error rate in synchronization and block models by
  sdp, robustly.
\newblock \emph{IEEE Transactions on Information Theory}, 66\penalty0
  (6):\penalty0 3929--3953, 2020.

\bibitem[Fei et~al.(2020)Fei, Yang, Chen, Wang, and Xie]{fei2020risk}
Y.~Fei, Z.~Yang, Y.~Chen, Z.~Wang, and Q.~Xie.
\newblock Risk-sensitive reinforcement learning: Near-optimal risk-sample
  tradeoff in regret.
\newblock \emph{arXiv preprint arXiv:2006.13827}, 2020.

\bibitem[Geiger et~al.(2019)Geiger, Spigler, d'Ascoli, Sagun, Baity-Jesi,
  Biroli, and Wyart]{geiger2019jamming}
M.~Geiger, S.~Spigler, S.~d'Ascoli, L.~Sagun, M.~Baity-Jesi, G.~Biroli, and
  M.~Wyart.
\newblock Jamming transition as a paradigm to understand the loss landscape of
  deep neural networks.
\newblock \emph{Physical Review E}, 100\penalty0 (1):\penalty0 012115, 2019.

\bibitem[Geiger et~al.(2020)Geiger, Jacot, Spigler, Gabriel, Sagun, d’Ascoli,
  Biroli, Hongler, and Wyart]{geiger2020scaling}
M.~Geiger, A.~Jacot, S.~Spigler, F.~Gabriel, L.~Sagun, S.~d’Ascoli,
  G.~Biroli, C.~Hongler, and M.~Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (2):\penalty0 023401, 2020.

\bibitem[Geman et~al.(1992)Geman, Bienenstock, and Doursat]{geman1992neural}
S.~Geman, E.~Bienenstock, and R.~Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock \emph{Neural computation}, 4\penalty0 (1):\penalty0 1--58, 1992.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019linearized}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{arXiv preprint arXiv:1904.12191}, 2019.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and
  Friedman]{hastie2009elements}
T.~Hastie, R.~Tibshirani, and J.~Friedman.
\newblock \emph{The elements of statistical learning: data mining, inference,
  and prediction}.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2019surprises}
T.~Hastie, A.~Montanari, S.~Rosset, and R.~J. Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Huang et~al.(2019)Huang, Cheng, Bapna, Firat, Chen, Chen, Lee, Ngiam,
  Le, Wu, et~al.]{huang2019gpipe}
Y.~Huang, Y.~Cheng, A.~Bapna, O.~Firat, D.~Chen, M.~Chen, H.~Lee, J.~Ngiam,
  Q.~V. Le, Y.~Wu, et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock In \emph{Advances in neural information processing systems}, pages
  103--112, 2019.

\bibitem[Javanmard et~al.(2020)Javanmard, Soltanolkotabi, and
  Hassani]{javanmard2020precise}
A.~Javanmard, M.~Soltanolkotabi, and H.~Hassani.
\newblock Precise tradeoffs in adversarial training for linear regression.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Li and Wei(2021)]{li2021minimum}
Y.~Li and Y.~Wei.
\newblock Minimum $\ell_1$-norm interpolators: Precise asymptotics and multiple
  descent.
\newblock \emph{arXiv preprint arXiv:2110.09502}, 2021.

\bibitem[Liang and Rakhlin(2019)]{liang2018just}
T.~Liang and A.~Rakhlin.
\newblock Just interpolate: Kernel ``ridgeles'' regression can generalize.
\newblock \emph{Annals of Statistics}, page to appear, 2019.

\bibitem[Liang et~al.(2020)Liang, Rakhlin, and Zhai]{liang2019multiple}
T.~Liang, A.~Rakhlin, and X.~Zhai.
\newblock On the multiple descent of minimum-norm interpolants and restricted
  lower isometry of kernels.
\newblock In \emph{COLT}, 2020.

\bibitem[Liu et~al.(2021)Liu, Liao, and Suykens]{liu2021kernel}
F.~Liu, Z.~Liao, and J.~Suykens.
\newblock Kernel regression in high dimensions: Refined analysis beyond double
  descent.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 649--657. PMLR, 2021.

\bibitem[Loog et~al.(2019)Loog, Viering, and Mey]{loog2019minimizers}
M.~Loog, T.~Viering, and A.~Mey.
\newblock Minimizers of the empirical risk and risk monotonicity.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7478--7487, 2019.

\bibitem[Mei and Montanari(2019)]{mei2019generalization}
S.~Mei and A.~Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock \emph{arXiv preprint arXiv:1908.05355}, 2019.

\bibitem[Min et~al.(2020)Min, Chen, and Karbasi]{min2020curious}
Y.~Min, L.~Chen, and A.~Karbasi.
\newblock The curious case of adversarially robust models: More data can help,
  double descend, or hurt generalization.
\newblock \emph{arXiv preprint arXiv:2002.11080}, 2020.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2019deep}
P.~Nakkiran, G.~Kaplun, Y.~Bansal, T.~Yang, B.~Barak, and I.~Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{arXiv preprint arXiv:1912.02292}, 2019.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Venkat, Kakade, and
  Ma]{nakkiran2020optimal}
P.~Nakkiran, P.~Venkat, S.~Kakade, and T.~Ma.
\newblock Optimal regularization can mitigate double descent.
\newblock \emph{arXiv preprint arXiv:2003.01897}, 2020.

\bibitem[Neal et~al.(2018)Neal, Mittal, Baratin, Tantia, Scicluna,
  Lacoste-Julien, and Mitliagkas]{neal2018modern}
B.~Neal, S.~Mittal, A.~Baratin, V.~Tantia, M.~Scicluna, S.~Lacoste-Julien, and
  I.~Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock \emph{arXiv preprint arXiv:1810.08591}, 2018.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In \emph{ICLR (Workshop)}, 2015.

\bibitem[Rahimi and Recht(2008)]{rahimi2008random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pages
  1177--1184, 2008.

\bibitem[Rakhlin and Zhai(2019)]{rakhlin2019consistency}
A.~Rakhlin and X.~Zhai.
\newblock Consistency of interpolation with laplace kernels is a
  high-dimensional phenomenon.
\newblock In \emph{Conference on Learning Theory}, pages 2595--2623, 2019.

\bibitem[Richards et~al.(2020)Richards, Mourtada, and
  Rosasco]{richards2020asymptotics}
D.~Richards, J.~Mourtada, and L.~Rosasco.
\newblock Asymptotics of ridge (less) regression under general source
  condition.
\newblock \emph{arXiv preprint arXiv:2006.06386}, 2020.

\bibitem[Rudi and Rosasco(2017)]{rudi2017generalization}
A.~Rudi and L.~Rosasco.
\newblock Generalization properties of learning with random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3215--3225, 2017.

\bibitem[Song et~al.(2021)Song, Xu, and Lafferty]{song2021convergence}
G.~Song, R.~Xu, and J.~Lafferty.
\newblock Convergence and alignment of gradient descent with random back
  propagation weights.
\newblock \emph{arXiv preprint arXiv:2106.06044}, 2021.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\bibitem[Tsigler and Bartlett(2020)]{tsigler2020benign}
A.~Tsigler and P.~L. Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock \emph{arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[von Rosen(1988)]{von1988moments}
D.~von Rosen.
\newblock Moments for the inverted wishart distribution.
\newblock \emph{Scandinavian Journal of Statistics}, pages 97--109, 1988.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
C.~Wei, J.~D. Lee, Q.~Liu, and T.~Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9712--9724, 2019.

\bibitem[Wyner et~al.(2017)Wyner, Olson, Bleich, and
  Mease]{wyner2017explaining}
A.~J. Wyner, M.~Olson, J.~Bleich, and D.~Mease.
\newblock Explaining the success of adaboost and random forests as
  interpolating classifiers.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 1558--1590, 2017.

\bibitem[Xu and Hsu(2019)]{xu2019number}
J.~Xu and D.~J. Hsu.
\newblock On the number of variables to use in principal component regression.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5094--5103, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
D.~Zou, Y.~Cao, D.~Zhou, and Q.~Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
