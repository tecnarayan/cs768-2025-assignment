\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Martin Abadi, Andy Chu, Ian Goodfellow, H.~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC Conference on Computer
  and Communications Security}, page 308–318, 2016.

\bibitem[Andrew et~al.(2019)Andrew, Thakkar, McMahan, and
  Ramaswamy]{andrew2019differentially}
Galen Andrew, Om~Thakkar, H~Brendan McMahan, and Swaroop Ramaswamy.
\newblock Differentially private learning with adaptive clipping.
\newblock \emph{arXiv preprint arXiv:1905.03871}, 2019.

\bibitem[Authors(2019{\natexlab{a}})]{stackoverflow}
The TensorFlow~Federated Authors.
\newblock Tensor{F}low {F}ederated {Stack Overflow} dataset,
  2019{\natexlab{a}}.
\newblock URL
  \url{https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data}.

\bibitem[Authors(2019{\natexlab{b}})]{tff}
The~TFF Authors.
\newblock Tensor{F}low {F}ederated, 2019{\natexlab{b}}.
\newblock URL \url{https://www.tensorflow.org/federated}.

\bibitem[Basu et~al.(2019)Basu, Data, Karakus, and Diggavi]{basu2019qsparse}
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi.
\newblock Qsparse-local-{SGD}: Distributed {SGD} with quantization,
  sparsification and local computations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Bittau et~al.(2017)Bittau, Erlingsson, Maniatis, Mironov, Raghunathan,
  Lie, Rudominer, Kode, Tinnes, and Seefeld]{bittau2017prochlo}
Andrea Bittau, {\'U}lfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth
  Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and
  Bernhard Seefeld.
\newblock {PROCHLO}: Strong privacy for analytics in the crowd.
\newblock In \emph{Proceedings of the 26th Symposium on Operating Systems
  Principles}, pages 441--459, 2017.

\bibitem[Bonawitz et~al.(2019)Bonawitz, Eichner, Grieskamp, Huba, Ingerman,
  Ivanov, Kiddon, Kone\v{c}n\'{y}, Mazzocchi, McMahan, Van~Overveldt, Petrou,
  Ramage, and Roselander]{bonawitz2019towards}
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chlo\'{e} Kiddon, Jakub Kone\v{c}n\'{y}, Stefano
  Mazzocchi, Brendan McMahan, Timon Van~Overveldt, David Petrou, Daniel Ramage,
  and Jason Roselander.
\newblock Towards federated learning at scale: System design.
\newblock In \emph{Proceedings of Machine Learning and Systems}. Proceedings of
  MLSys, 2019.

\bibitem[Caldas et~al.(2018)Caldas, Wu, Li, Kone{\v{c}}n{\'y}, McMahan, Smith,
  and Talwalkar]{caldas2018leaf}
Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone{\v{c}}n{\'y}, H~Brendan
  McMahan, Virginia Smith, and Ameet Talwalkar.
\newblock {LEAF}: A benchmark for federated settings.
\newblock \emph{arXiv preprint arXiv:1812.01097}, 2018.

\bibitem[Charles and Kone\v{c}n\'y(2021)]{charles2021convergence}
Zachary Charles and Jakub Kone\v{c}n\'y.
\newblock Convergence and accuracy trade-offs in federated learning and
  meta-learning.
\newblock In \emph{Proceedings of The 24th International Conference on
  Artificial Intelligence and Statistics}, 2021.

\bibitem[Chen et~al.(2020)Chen, Horvath, and Richtarik]{chen2020optimal}
Wenlin Chen, Samuel Horvath, and Peter Richtarik.
\newblock Optimal client sampling for federated learning.
\newblock \emph{arXiv preprint arXiv:2010.13723}, 2020.

\bibitem[Cheu et~al.(2019)Cheu, Smith, Ullman, Zeber, and
  Zhilyaev]{cheu2019distributed}
Albert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev.
\newblock Distributed differential privacy via shuffling.
\newblock In \emph{Annual International Conference on the Theory and
  Applications of Cryptographic Techniques}, pages 375--403, 2019.

\bibitem[Cho et~al.(2020)Cho, Wang, and Joshi]{cho2020client}
Yae~Jee Cho, Jianyu Wang, and Gauri Joshi.
\newblock Client selection in federated learning: Convergence analysis and
  power-of-choice selection strategies.
\newblock \emph{arXiv preprint arXiv:2010.01243}, 2020.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and
  Van~Schaik]{cohen2017emnist}
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van~Schaik.
\newblock {EMNIST}: Extending {MNIST} to handwritten letters.
\newblock In \emph{2017 International Joint Conference on Neural Networks
  (IJCNN)}, pages 2921--2926. IEEE, 2017.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, Le, and Ng]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang, Quoc Le, and
  Andrew Ng.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Erlingsson et~al.(2020)Erlingsson, Feldman, Mironov, Raghunathan,
  Song, Talwar, and Thakurta]{erlingsson2020encode}
{\'U}lfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Shuang
  Song, Kunal Talwar, and Abhradeep Thakurta.
\newblock Encode, shuffle, analyze privacy revisited: Formalizations and
  empirical evaluation.
\newblock \emph{arXiv preprint arXiv:2001.03618}, 2020.

\bibitem[Girgis et~al.(2020)Girgis, Data, Diggavi, Kairouz, and
  Suresh]{girgis2020shuffled}
Antonious~M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and
  Ananda~Theertha Suresh.
\newblock Shuffled model of federated learning: Privacy, communication and
  accuracy trade-offs.
\newblock \emph{arXiv preprint arXiv:2008.07180}, 2020.

\bibitem[Goetz et~al.(2019)Goetz, Malik, Bui, Moon, Liu, and
  Kumar]{goetz2019active}
Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj
  Kumar.
\newblock Active federated learning.
\newblock \emph{arXiv preprint arXiv:1909.12641}, 2019.

\bibitem[Golmant et~al.(2018)Golmant, Vemuri, Yao, Feinberg, Gholami, Rothauge,
  Mahoney, and Gonzalez]{golmant2018computational}
Noah Golmant, Nikita Vemuri, Zhewei Yao, Vladimir Feinberg, Amir Gholami, Kai
  Rothauge, Michael~W Mahoney, and Joseph Gonzalez.
\newblock On the computational inefficiency of large batch sizes for stochastic
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1811.12941}, 2018.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training {I}mage{N}et in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Hsieh et~al.(2020)Hsieh, Phanishayee, Mutlu, and
  Gibbons]{hsieh2019non}
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons.
\newblock The non-{IID} data quagmire of decentralized machine learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Tzu-Ming~Harry Hsu, Hang Qi, and Matthew Brown.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Hu et~al.(2020)Hu, Shaloudegi, Zhang, and Yu]{hu2020fedmgda+}
Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu.
\newblock {FedMGDA}+: Federated learning meets multi-objective optimization.
\newblock \emph{arXiv preprint arXiv:2006.11489}, 2020.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, and
  contributors]{kairouz2019advances}
Peter Kairouz, H.~Brendan McMahan, and contributors.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends® in Machine Learning}, 14\penalty0 (1),
  2021.
\newblock ISSN 1935-8237.

\bibitem[Karimireddy et~al.(2020{\natexlab{a}})Karimireddy, Jaggi, Kale, Mohri,
  Reddi, Stich, and Suresh]{karimireddy2020mime}
Sai~Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank~J
  Reddi, Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Mime: {M}imicking centralized stochastic algorithms in federated
  learning.
\newblock \emph{arXiv preprint arXiv:2008.03606}, 2020{\natexlab{a}}.

\bibitem[Karimireddy et~al.(2020{\natexlab{b}})Karimireddy, Kale, Mohri, Reddi,
  Stich, and Suresh]{karimireddy2019scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD}: Stochastic controlled averaging for federated learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020{\natexlab{b}}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017iclr}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Khaled et~al.(2019)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2019first}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock First analysis of local {GD} on heterogeneous data.
\newblock \emph{arXiv preprint arXiv:1909.04715}, 2019.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richtarik]{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{Proceedings of the Twenty Third International Conference on
  Artificial Intelligence and Statistics}, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Ramage, and
  Richt{\'a}rik]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Krizhevsky(2014)]{krizhevsky2014one}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1404.5997}, 2014.

\bibitem[Laguel et~al.(2021)Laguel, Pillutla, Malick, and
  Harchaoui]{laguel2020device}
Yassine Laguel, Krishna Pillutla, Jerôme Malick, and Zaid Harchaoui.
\newblock A superquantile approach to federated learning with heterogeneous
  devices.
\newblock In \emph{2021 55th Annual Conference on Information Sciences and
  Systems (CISS)}, 2021.

\bibitem[Lee et~al.(2017)Lee, Lam, Pedarsani, Papailiopoulos, and
  Ramchandran]{lee2017speeding}
Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, and
  Kannan Ramchandran.
\newblock Speeding up distributed machine learning using codes.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (3):\penalty0 1514--1529, 2017.

\bibitem[Li and Wang(2019)]{li2019fedmd}
Daliang Li and Junpu Wang.
\newblock Fed{MD}: Heterogenous federated learning via model distillation.
\newblock \emph{arXiv preprint arXiv:1910.03581}, 2019.

\bibitem[Li et~al.(2019)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smithy]{li2019feddane}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smithy.
\newblock Fed{DANE}: A federated newton-type method.
\newblock In \emph{2019 53rd Asilomar Conference on Signals, Systems, and
  Computers}, pages 1227--1231. IEEE, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Sahu, Talwalkar, and
  Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: {C}hallenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  50--60, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2018federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock In \emph{Proceedings of Machine Learning and Systems 2020}, pages
  429--450, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Sanjabi, Beirami, and
  Smith]{li2019fair}
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.
\newblock Fair resource allocation in federated learning.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{c}}.

\bibitem[Li and McCallum(2006)]{li2006pachinko}
Wei Li and Andrew McCallum.
\newblock Pachinko allocation: {DAG}-structured mixture models of topic
  correlations.
\newblock In \emph{Proceedings of the 23rd International Conference on Machine
  Learning}, 2006.

\bibitem[Liang and Kozat(2014)]{liang2014tofec}
Guanfeng Liang and Ula{\c{s}}~C Kozat.
\newblock Tofec: Achieving optimal throughput-delay trade-off of cloud storage
  using erasure codes.
\newblock In \emph{IEEE INFOCOM 2014-IEEE Conference on Computer
  Communications}, 2014.

\bibitem[Lin et~al.(2019)Lin, Stich, Patel, and Jaggi]{lin2019don}
Tao Lin, Sebastian~U Stich, Kumar~Kshitij Patel, and Martin Jaggi.
\newblock Don't use large mini-batches, use local {SGD}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020extrapolation}
Tao Lin, Lingjing Kong, Sebastian Stich, and Martin Jaggi.
\newblock Extrapolation for large-batch training in deep learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma2018power}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of {SGD}
  in modern over-parametrized learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018.

\bibitem[Malinovskiy et~al.(2020)Malinovskiy, Kovalev, Gasanov, Condat, and
  Richtarik]{malinovsky2020local}
Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter
  Richtarik.
\newblock From local {SGD} to local fixed-point methods for federated learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem[Masters and Luschi(2018)]{masters2018revisiting}
Dominic Masters and Carlo Luschi.
\newblock Revisiting small batch training for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.07612}, 2018.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish2018empirical}
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI~Dota Team.
\newblock An empirical model of large-batch training.
\newblock \emph{arXiv preprint arXiv:1812.06162}, 2018.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and {Aguera y
  Arcas}]{mcmahan2017aistats}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise {Aguera y
  Arcas}.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, volume~54, 2017.

\bibitem[McMahan and Streeter(2010)]{mcmahan2010adaptive}
H.~Brendan McMahan and Matthew~J. Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock In \emph{{COLT} The 23rd Conference on Learning Theory}, 2010.

\bibitem[McMahan et~al.(2018)McMahan, Ramage, Talwar, and
  Zhang]{mcmahan2017learning}
H.~Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li~Zhang.
\newblock Learning differentially private recurrent language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, {\v{C}}ernock{\`y},
  and Khudanpur]{mikolov2010recurrent}
Tom{\'a}{\v{s}} Mikolov, Martin Karafi{\'a}t, Luk{\'a}{\v{s}} Burget, Jan
  {\v{C}}ernock{\`y}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Eleventh Annual Conference of the International Speech
  Communication Association}, 2010.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mehryar Mohri, Gary Sivek, and Ananda~Theertha Suresh.
\newblock Agnostic federated learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, 2019.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{nacson2019convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique~Pamplona
  Savarese, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, 2019.

\bibitem[Nishio and Yonetani(2019)]{nishio2019client}
Takayuki Nishio and Ryo Yonetani.
\newblock Client selection for federated learning with heterogeneous resources
  in mobile edge.
\newblock In \emph{IEEE International Conference on Communications (ICC)},
  2019.

\bibitem[Pathak and Wainwright(2020)]{pathak2020fedsplit}
Reese Pathak and Martin~J Wainwright.
\newblock Fed{S}plit: {A}n algorithmic framework for fast federated
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7057--7066, 2020.

\bibitem[Paulik et~al.(2021)Paulik, Seigel, Mason, Telaar, Kluivers, van Dalen,
  Lau, Carlson, Granqvist, Vandevelde, et~al.]{paulik2021federated}
Matthias Paulik, Matt Seigel, Henry Mason, Dominic Telaar, Joris Kluivers,
  Rogier van Dalen, Chi~Wai Lau, Luke Carlson, Filip Granqvist, Chris
  Vandevelde, et~al.
\newblock Federated evaluation and tuning for on-device personalization: System
  design \& applications.
\newblock \emph{arXiv preprint arXiv:2102.08503}, 2021.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Ribero and Vikalo(2020)]{ribero2020communication}
Monica Ribero and Haris Vikalo.
\newblock Communication-efficient federated learning via optimal client
  sampling.
\newblock \emph{arXiv preprint arXiv:2007.15197}, 2020.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2019measuring}
Christopher~J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--49,
  2019.

\bibitem[Shi et~al.(2011)Shi, Chan, Rieffel, Chow, and Song]{shi2011privacy}
Elaine Shi, TH~Hubert Chan, Eleanor Rieffel, Richard Chow, and Dawn Song.
\newblock Privacy-preserving aggregation of time-series data.
\newblock In \emph{Proc. NDSS}, volume~2, pages 1--17, 2011.

\bibitem[Smith et~al.(2018)Smith, Kindermans, and Le]{smith2017don}
Samuel~L. Smith, Pieter-Jan Kindermans, and Quoc~V. Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni]{wang2020federated}
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
  Yasaman Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 3--19, 2018.

\bibitem[Yang et~al.(2021)Yang, Fang, and Liu]{yang2021achieving}
Haibo Yang, Minghong Fang, and Jia Liu.
\newblock Achieving linear speedup with partial worker participation in
  non-{IID} federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yin et~al.(2018)Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and
  Bartlett]{yin2018gradient}
Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan
  Ramchandran, and Peter Bartlett.
\newblock Gradient diversity: a key ingredient for scalable distributed
  learning.
\newblock In \emph{Proceedings of the Twenty-First International Conference on
  Artificial Intelligence and Statistics}, 2018.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Hong, Dhople, Yin, and Liu]{zhang2020fedpd}
Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu.
\newblock Fed{PD}: A federated learning framework with optimal rates and
  adaptivity to non-{IID} data.
\newblock \emph{arXiv preprint arXiv:2005.11418}, 2020.

\end{thebibliography}
