\begin{thebibliography}{10}

\bibitem{anciukevicius2020object}
Titas Anciukevicius, Christoph~H Lampert, and Paul Henderson.
\newblock Object-centric image generation with factored depths, locations, and appearances.
\newblock {\em arXiv preprint arXiv:2004.00642}, 2020.

\bibitem{layernorm}
Jimmy Ba, Jamie~Ryan Kiros, and Geoffrey Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{monet}
Christopher~P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner.
\newblock Monet: Unsupervised scene decomposition and representation.
\newblock {\em arXiv preprint arXiv:1901.11390}, 2019.

\bibitem{transdreamer}
Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn.
\newblock {TransDreamer}: Reinforcement learning with {Transformer} world models.
\newblock In {\em Deep RL Workshop NeurIPS 2021}, 2021.

\bibitem{gru}
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence modeling.
\newblock {\em arXiv preprint arXiv:1412.3555}, 2014.

\bibitem{silot}
Eric Crawford and Joelle Pineau.
\newblock Exploiting spatial invariance for scalable unsupervised object tracking.
\newblock {\em arXiv preprint arXiv:1911.09033}, 2019.

\bibitem{spair}
Eric Crawford and Joelle Pineau.
\newblock Spatially invariant unsupervised object detection with convolutional neural networks.
\newblock In {\em Proceedings of AAAI}, 2019.

\bibitem{decision_s4}
Shmuel~Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf.
\newblock Decision {S4}: Efficient sequence-based {RL} via state spaces layers.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{de2024griffin}
Soham De, Samuel~L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et~al.
\newblock Griffin: Mixing gated linear recurrences with local attention for efficient language models.
\newblock {\em arXiv preprint arXiv:2402.19427}, 2024.

\bibitem{s4wm}
Fei Deng, Junyeong Park, and Sungjin Ahn.
\newblock Facing off world model backbones: {RNNs}, {Transformers}, and {S4}.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{deng2021generative}
Fei {Deng}, Zhuo {Zhi}, Donghun {Lee}, and Sungjin {Ahn}.
\newblock Generative scene graph networks.
\newblock In {\em ICLR 2021: The Ninth International Conference on Learning Representations}, 2021.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: {Transformers} for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Elsayed2022SAViTE}
Gamaleldin~F. Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael~Curtis Mozer, and Thomas Kipf.
\newblock Savi++: Towards end-to-end object-centric learning from real-world videos.
\newblock {\em ArXiv}, abs/2206.07764, 2022.

\bibitem{engelcke2021genesis}
Martin {Engelcke}, Oiwi~Parker {Jones}, and Ingmar {Posner}.
\newblock {GENESIS-V2}: Inferring unordered object representations without iterative refinement.
\newblock {\em arXiv preprint arXiv:2104.09958}, 2021.

\bibitem{engelcke2020genesis}
Martin {Engelcke}, Adam~R. {Kosiorek}, Oiwi~Parker {Jones}, and Ingmar {Posner}.
\newblock {GENESIS}: Generative scene inference and sampling with object-centric latent representations.
\newblock In {\em ICLR 2020 : Eighth International Conference on Learning Representations}, 2020.

\bibitem{air}
SM~Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, and Geoffrey~E Hinton.
\newblock Attend, infer, repeat: Fast scene understanding with generative models.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 3225--3233, 2016.

\bibitem{h3}
Daniel~Y Fu, Tri Dao, Khaled~Kamal Saab, Armin~W Thomas, Atri Rudra, and Christopher R{\'e}.
\newblock {Hungry Hungry Hippos}: Towards language modeling with state space models.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{cater}
Rohit Girdhar and Deva Ramanan.
\newblock {CATER: A diagnostic dataset for Compositional Actions and TEmporal Reasoning}.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{sashimi}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It’s raw! {A}udio generation with state-space models.
\newblock In {\em International Conference on Machine Learning}, 2022.

\bibitem{rims}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Sch{\"o}lkopf.
\newblock Recurrent independent mechanisms.
\newblock {\em ArXiv}, abs/1909.10893, 2021.

\bibitem{kubric}
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam~H. Laradji, Hsueh-Ti Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S.~M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang~Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi.
\newblock Kubric: A scalable dataset generator.
\newblock {\em arXiv preprint arXiv:2203.03570}, 2022.

\bibitem{iodine}
Klaus Greff, Rapha{\"e}l~Lopez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner.
\newblock Multi-object representation learning with iterative variational inference.
\newblock {\em arXiv preprint arXiv:1903.00450}, 2019.

\bibitem{greff2017neural}
Klaus Greff, Sjoerd van Steenkiste, and J{\"u}rgen Schmidhuber.
\newblock Neural expectation maximization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 6691--6701, 2017.

\bibitem{mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock {\em arXiv preprint arXiv:2312.00752}, 2023.

\bibitem{hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\'{e}.
\newblock {HiPPO}: Recurrent memory with optimal polynomial projections.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{s4d}
Albert Gu, Karan Goel, Ankit Gupta, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{s4}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{lssl}
Albert Gu, Isys Johnson, Karan Goel, Khaled~Kamal Saab, Tri Dao, Atri Rudra, and Christopher R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with linear state space layers.
\newblock In {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{dss}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{dlr}
Ankit Gupta, Harsh Mehta, and Jonathan Berant.
\newblock Simplifying and understanding state space models with diagonal linear {RNN}s.
\newblock {\em arXiv preprint arXiv:2212.00768}, 2022.

\bibitem{animate_anyone}
Li~Hu.
\newblock Animate anyone: Consistent and controllable image-to-video synthesis for character animation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8153--8163, 2024.

\bibitem{vis4mer}
Md~Mohaiminul Islam and Gedas Bertasius.
\newblock Long movie clip classification with state-space video models.
\newblock In {\em ECCV}, 2022.

\bibitem{tiktok_dataset}
Yasamin Jafarian and Hyun~Soo Park.
\newblock Learning high fidelity depths of dressed humans by watching social media dance videos.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 12753--12762, 2021.

\bibitem{jia2023improving}
Baoxiong Jia, Yu~Liu, and Siyuan Huang.
\newblock Improving {Object}-centric {Learning} with {Query} {Optimization}.
\newblock In {\em International {Conference} on {Learning} {Representations} ({ICLR})}, 2023.

\bibitem{gnm}
Jindong Jiang and Sungjin Ahn.
\newblock Generative neurosymbolic machines.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{jiang2024object}
Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin Ahn.
\newblock Object-centric slot diffusion.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{scalor}
Jindong Jiang, Sepehr Janghorbani, Gerard De~Melo, and Sungjin Ahn.
\newblock Scalor: Generative world models with scalable object representations.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{kabra2021simone}
Rishabh {Kabra}, Daniel {Zoran}, Goker {Erdogan}, Loic {Matthey}, Antonia {Creswell}, Matthew {Botvinick}, Alexander {Lerchner}, and Christopher~P. {Burgess}.
\newblock Simone: View-invariant, temporally-abstracted object representations via unsupervised video decomposition.
\newblock {\em arXiv preprint arXiv:2106.03849}, 2021.

\bibitem{savi}
Thomas Kipf, Gamaleldin~F. Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff.
\newblock {Conditional Object-Centric Learning from Video}.
\newblock {\em arXiv preprint arXiv:2111.12594}, 2021.

\bibitem{ccnn}
David~M Knigge, David~W Romero, Albert Gu, Efstratios Gavves, Erik~J Bekkers, Jakub~Mikolaj Tomczak, Mark Hoogendoorn, and {Jan-jakob} Sonke.
\newblock Modelling long range dependencies in {$N$D}: From task-specific to a general purpose {CNN}.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{sqair}
Adam Kosiorek, Hyunjik Kim, Yee~Whye Teh, and Ingmar Posner.
\newblock Sequential attend, infer, repeat: Generative modelling of moving objects.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 8606--8616, 2018.

\bibitem{utego}
Yong~Jae Lee, Joydeep Ghosh, and Kristen Grauman.
\newblock Discovering important people and objects for egocentric video summarization.
\newblock In {\em 2012 IEEE conference on computer vision and pattern recognition}, pages 1346--1353. IEEE, 2012.

\bibitem{bevformer}
Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu~Qiao, and Jifeng Dai.
\newblock Bevformer: Learning bird’s-eye-view representation from multi-camera images via spatiotemporal transformers.
\newblock In {\em European conference on computer vision}, pages 1--18. Springer, 2022.

\bibitem{gswm}
Zhixuan Lin, Yi-Fu Wu, Skand~Vishwanath Peri, Jindong Jiang, and Sungjin Ahn.
\newblock Improving generative imagination in object-centric world models.
\newblock In {\em International Conference on Machine Learning}, pages 4114--4124, 2020.

\bibitem{space}
Zhixuan Lin, Yi-Fu Wu, Skand~Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn.
\newblock Space: Unsupervised object-oriented scene representation via spatial attention and decomposition.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{slotattention}
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf.
\newblock Object-centric learning with slot attention, 2020.

\bibitem{lu2023structured}
Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani.
\newblock Structured state space models for in-context reinforcement learning.
\newblock {\em arXiv preprint arXiv:2303.03982}, 2023.

\bibitem{gss}
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{s4nd}
Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R{\'e}.
\newblock {S4ND}: Modeling images and videos as multidimensional signals with state spaces.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{orvieto2023resurrecting}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, 2013.

\bibitem{samsami2024mastering}
Mohammad~Reza Samsami, Artem Zholus, Janarthanan Rajendran, and Sarath Chandar.
\newblock Mastering memory tasks with world models.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{dinosaur}
Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Scholkopf, Thomas Brox, and Francesco Locatello.
\newblock Bridging the gap to real-world object-centric learning.
\newblock {\em arXiv preprint arXiv:2209.14860}, 2022.

\bibitem{slate}
Gautam Singh, Fei Deng, and Sungjin Ahn.
\newblock Illiterate dall-e learns to compose.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{sysbinder}
Gautam Singh, Yeongbin Kim, and Sungjin Ahn.
\newblock Neural {Systematic} {Binder}.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{psb}
Gautam Singh, Yue Wang, Jiawei Yang, Boris Ivanovic, Sungjin Ahn, Marco Pavone, and Tong Che.
\newblock Parallelized spatiotemporal binding, 2024.

\bibitem{steve}
Gautam Singh, Yi-Fu Wu, and Sungjin Ahn.
\newblock Simple unsupervised object-centric learning for complex and naturalistic videos.
\newblock {\em arXiv preprint arXiv:2205.14065}, 2022.

\bibitem{s5}
Jimmy~T.H. Smith, Andrew Warrington, and Scott Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{waymo_open_dataset}
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu~Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov.
\newblock Scalability in perception for autonomous driving: Waymo open dataset.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2020.

\bibitem{longrangearena}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock {Long Range Arena}: A benchmark for efficient {Transformers}.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Tsai2020Capsules}
Yao-Hung~Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov.
\newblock Capsules with inverted dot-product attention routing.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{rnem}
Sjoerd Van~Steenkiste, Michael Chang, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Relational neural expectation maximization: Unsupervised discovery of objects and their interactions.
\newblock {\em arXiv preprint arXiv:1802.10353}, 2018.

\bibitem{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages 5998--6008, 2017.

\bibitem{kugelgen2020towards}
Julius von {Kügelgen}, Ivan {Ustyuzhaninov}, Peter~V. {Gehler}, Matthias {Bethge}, and Bernhard {Schölkopf}.
\newblock Towards causal generative scene models via competition of experts.
\newblock {\em arXiv preprint arXiv:2004.12906}, 2020.

\bibitem{selective_s4}
Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid.
\newblock Selective structured state-spaces for long-form video understanding.
\newblock In {\em CVPR}, 2023.

\bibitem{watters2019spatial}
Nicholas {Watters}, Loic {Matthey}, Christopher~P. {Burgess}, and Alexander {Lerchner}.
\newblock Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes.
\newblock {\em arXiv preprint arXiv:1901.07017}, 2019.

\bibitem{wu2023invertedattention}
Yi-Fu Wu, Klaus Greff, Gamaleldin~Fathy Elsayed, Michael~Curtis Mozer, Thomas Kipf, and Sjoerd van Steenkiste.
\newblock Inverted-attention transformers can learn object representations: Insights from slot attention.
\newblock In {\em UniReps: the First Workshop on Unifying Representations in Neural Models}, 2023.

\bibitem{ocvt}
Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn.
\newblock Generative video transformer: Can objects be the words?
\newblock In {\em International Conference on Machine Learning}, pages 11307--11318. PMLR, 2021.

\bibitem{slotformer}
Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, and Animesh Garg.
\newblock Slotformer: Unsupervised visual dynamics simulation with object-centric models.
\newblock {\em arXiv preprint arXiv:2210.05861}, 2022.

\bibitem{wu2023slotdiffusion}
Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, and Animesh Garg.
\newblock Slotdiffusion: Object-centric generative modeling with diffusion models.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{yan2023diffusion}
Jing~Nathan Yan, Jiatao Gu, and Alexander~M. Rush.
\newblock Diffusion models without attention, 2023.

\bibitem{mmgeo}
Hanrong Ye, Haotian Zhang, Erik Daxberger, Lin Chen, Zongyu Lin, Yanghao Li, Bowen Zhang, Haoxuan You, Dan Xu, Zhe Gan, et~al.
\newblock Mm-ego: Towards building egocentric multimodal llms.
\newblock {\em arXiv preprint arXiv:2410.07177}, 2024.

\bibitem{ls4}
Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon.
\newblock Deep latent state space models for time-series generation.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{zhu2024vision}
Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang.
\newblock Vision mamba: Efficient visual representation learning with bidirectional state space model.
\newblock {\em arXiv preprint arXiv:2401.09417}, 2024.

\bibitem{spade}
Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao.
\newblock Efficient long sequence modeling via state space augmented {Transformer}.
\newblock {\em arXiv preprint arXiv:2212.08136}, 2022.

\end{thebibliography}
