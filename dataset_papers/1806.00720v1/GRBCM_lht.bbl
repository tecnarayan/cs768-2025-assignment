\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alvarez et~al.(2012)Alvarez, Rosasco, Lawrence,
  et~al.]{alvarez2012kernels}
Alvarez, Mauricio~A, Rosasco, Lorenzo, Lawrence, Neil~D, et~al.
\newblock Kernels for vector-valued functions: {A} review.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (3):\penalty0 195--266, 2012.

\bibitem[Bauer et~al.(2016)Bauer, van~der Wilk, and
  Rasmussen]{bauer2016understanding}
Bauer, Matthias, van~der Wilk, Mark, and Rasmussen, Carl~Edward.
\newblock Understanding probabilistic sparse {G}aussian process approximations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1533--1541. Curran Associates, Inc., 2016.

\bibitem[Bertin-Mahieux et~al.(2011)Bertin-Mahieux, Ellis, Whitman, and
  Lamere]{bertin2011million}
Bertin-Mahieux, Thierry, Ellis, Daniel~PW, Whitman, Brian, and Lamere, Paul.
\newblock The million song dataset.
\newblock In \emph{ISMIR}, pp.\  1--6, 2011.

\bibitem[Bui \& Turner(2014)Bui and Turner]{bui2014tree}
Bui, Thang~D and Turner, Richard~E.
\newblock Tree-structured {G}aussian process approximations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2213--2221. Curran Associates, Inc., 2014.

\bibitem[Cao \& Fleet(2014)Cao and Fleet]{cao2014generalized}
Cao, Yanshuai and Fleet, David~J.
\newblock Generalized product of experts for automatic and principled fusion of
  {G}aussian process predictions.
\newblock \emph{arXiv preprint arXiv:1410.7827}, 2014.

\bibitem[Chalupka et~al.(2013)Chalupka, Williams, and
  Murray]{chalupka2013framework}
Chalupka, Krzysztof, Williams, Christopher~KI, and Murray, Iain.
\newblock A framework for evaluating approximation methods for {G}aussian
  process regression.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Feb):\penalty0 333--350, 2013.

\bibitem[Choi \& Schervish(2004)Choi and Schervish]{choi2004posterior}
Choi, Taeryon and Schervish, Mark~J.
\newblock Posterior consistency in nonparametric regression problems under
  {G}aussian process priors.
\newblock Technical report, Carnegie Mellon University, 2004.

\bibitem[Deisenroth \& Ng(2015)Deisenroth and Ng]{deisenroth2015distributed}
Deisenroth, Marc~Peter and Ng, Jun~Wei.
\newblock Distributed {G}aussian processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1481--1490. PMLR, 2015.

\bibitem[Fu et~al.(2013)Fu, Zhu, and Li]{fu2013survey}
Fu, Yifan, Zhu, Xingquan, and Li, Bin.
\newblock A survey on instance selection for active learning.
\newblock \emph{Knowledge and Information Systems}, 35\penalty0 (2):\penalty0
  249--283, 2013.

\bibitem[Gal et~al.(2014)Gal, van~der Wilk, and Rasmussen]{gal2014distributed}
Gal, Yarin, van~der Wilk, Mark, and Rasmussen, Carl~Edward.
\newblock Distributed variational inference in sparse {G}aussian process
  regression and latent variable models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3257--3265. Curran Associates, Inc., 2014.

\bibitem[Genest \& Zidek(1986)Genest and Zidek]{genest1986combining}
Genest, Christian and Zidek, James~V.
\newblock Combining probability distributions: {A} critique and an annotated
  bibliography.
\newblock \emph{Statistical Science}, 1\penalty0 (1):\penalty0 114--135, 1986.

\bibitem[Hensman et~al.(2013)Hensman, Fusi, and Lawrence]{hensman2013gaussian}
Hensman, James, Fusi, Nicol{\`o}, and Lawrence, Neil~D.
\newblock Gaussian processes for big data.
\newblock In \emph{Proceedings of the 29th Conference on Uncertainty in
  Artificial Intelligence}, pp.\  282--290. AUAI Press, 2013.

\bibitem[Hinton(2002)]{hinton2002training}
Hinton, Geoffrey~E.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural Computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[Hoang et~al.(2016)Hoang, Hoang, and Low]{hoang2016distributed}
Hoang, Trong~Nghia, Hoang, Quang~Minh, and Low, Bryan Kian~Hsiang.
\newblock A distributed variational inference framework for unifying parallel
  sparse {G}aussian process regression models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  382--391. PMLR, 2016.

\bibitem[Ionescu(2015)]{ionescu2015revisiting}
Ionescu, Radu~Cristian.
\newblock Revisiting large scale distributed machine learning.
\newblock \emph{arXiv preprint arXiv:1507.01461}, 2015.

\bibitem[Lawrence(2005)]{lawrence2005probabilistic}
Lawrence, Neil.
\newblock Probabilistic non-linear principal component analysis with {G}aussian
  process latent variable models.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Nov):\penalty0 1783--1816, 2005.

\bibitem[Liu et~al.(2017)Liu, Cai, and Ong]{liu2017adaptive}
Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon.
\newblock An adaptive sampling approach for {K}riging metamodeling by
  maximizing expected prediction error.
\newblock \emph{Computers \& Chemical Engineering}, 106\penalty0
  (Nov):\penalty0 171--182, 2017.

\bibitem[Liu et~al.(2018)Liu, Cai, and Ong]{liu2018remarks}
Liu, Haitao, Cai, Jianfei, and Ong, Yew-Soon.
\newblock Remarks on multi-output {G}aussian process regression.
\newblock \emph{Knowledge-Based Systems}, 144\penalty0 (March):\penalty0
  102--121, 2018.

\bibitem[Moore \& Russell(2015)Moore and Russell]{moore2015gaussian}
Moore, David and Russell, Stuart~J.
\newblock Gaussian process random fields.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3357--3365. Curran Associates, Inc., 2015.

\bibitem[Park et~al.(2011)Park, Huang, and Ding]{park2011domain}
Park, Chiwoo, Huang, Jianhua~Z, and Ding, Yu.
\newblock Domain decomposition approach for fast {G}aussian process regression
  of large spatial data sets.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (May):\penalty0 1697--1728, 2011.

\bibitem[Peng et~al.(2017)Peng, Zhe, Zhang, and Qi]{peng2017asynchronous}
Peng, Hao, Zhe, Shandian, Zhang, Xiao, and Qi, Yuan.
\newblock Asynchronous distributed variational {G}aussian process for
  regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2788--2797. PMLR, 2017.

\bibitem[Qui{\~n}onero-Candela \& Rasmussen(2005)Qui{\~n}onero-Candela and
  Rasmussen]{quinonero2005unifying}
Qui{\~n}onero-Candela, Joaquin and Rasmussen, Carl~Edward.
\newblock A unifying view of sparse approximate {G}aussian process regression.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Dec):\penalty0 1939--1959, 2005.

\bibitem[Ranjan \& Gneiting(2010)Ranjan and Gneiting]{ranjan2010combining}
Ranjan, Roopesh and Gneiting, Tilmann.
\newblock Combining probability forecasts.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 72\penalty0 (1):\penalty0 71--91, 2010.

\bibitem[Rasmussen \& Ghahramani(2002)Rasmussen and
  Ghahramani]{rasmussen2002infinite}
Rasmussen, Carl~E and Ghahramani, Zoubin.
\newblock Infinite mixtures of {G}aussian process experts.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  881--888. Curran Associates, Inc., 2002.

\bibitem[Rasmussen \& Williams(2006)Rasmussen and
  Williams]{rasmussen2006gaussian}
Rasmussen, Carl~Edward and Williams, Christopher K.~I.
\newblock \emph{Gaussian processes for machine learning}.
\newblock MIT Press, 2006.

\bibitem[Rulli{\`e}re et~al.(2017)Rulli{\`e}re, Durrande, Bachoc, and
  Chevalier]{rulliere2017nested}
Rulli{\`e}re, Didier, Durrande, Nicolas, Bachoc, Fran{\c{c}}ois, and Chevalier,
  Cl{\'e}ment.
\newblock Nested {K}riging predictions for datasets with a large number of
  observations.
\newblock \emph{Statistics and Computing}, pp.\  1--19, 2017.

\bibitem[Seeger et~al.(2003)Seeger, Williams, and Lawrence]{seeger2003fast}
Seeger, Matthias, Williams, Christopher, and Lawrence, Neil.
\newblock Fast forward selection to speed up sparse {G}aussian process
  regression.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\
  EPFL--CONF--161318. PMLR, 2003.

\bibitem[Shahriari et~al.(2016)Shahriari, Swersky, Wang, Adams, and
  de~Freitas]{shahriari2016taking}
Shahriari, Bobak, Swersky, Kevin, Wang, Ziyu, Adams, Ryan~P, and de~Freitas,
  Nando.
\newblock Taking the human out of the loop: {A} review of {B}ayesian
  optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2016.

\bibitem[Snelson \& Ghahramani(2006)Snelson and Ghahramani]{snelson2006sparse}
Snelson, Edward and Ghahramani, Zoubin.
\newblock Sparse {G}aussian processes using pseudo-inputs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1257--1264. MIT Press, 2006.

\bibitem[Snelson \& Ghahramani(2007)Snelson and Ghahramani]{snelson2007local}
Snelson, Edward and Ghahramani, Zoubin.
\newblock Local and global sparse {G}aussian process approximations.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  524--531.
  PMLR, 2007.

\bibitem[Tavassolipour et~al.(2017)Tavassolipour, Motahari, and
  Shalmani]{tavassolipour2017learning}
Tavassolipour, Mostafa, Motahari, Seyed~Abolfazl, and Shalmani,
  Mohammad-Taghi~Manzuri.
\newblock Learning of {G}aussian processes in distributed and communication
  limited systems.
\newblock \emph{arXiv preprint arXiv:1705.02627}, 2017.

\bibitem[Titsias(2009)]{titsias2009variational}
Titsias, Michalis~K.
\newblock Variational learning of inducing variables in sparse {G}aussian
  processes.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  567--574.
  PMLR, 2009.

\bibitem[Tresp(2000)]{tresp2000bayesian}
Tresp, Volker.
\newblock A {B}ayesian committee machine.
\newblock \emph{Neural Computation}, 12\penalty0 (11):\penalty0 2719--2741,
  2000.

\bibitem[Vazquez \& Bect(2010)Vazquez and Bect]{vazquez2010pointwise}
Vazquez, Emmanuel and Bect, Julien.
\newblock Pointwise consistency of the {K}riging predictor with known mean and
  covariance functions.
\newblock In \emph{9th International Workshop in Model-Oriented Design and
  Analysis}, pp.\  221--228. Springer, 2010.

\bibitem[Wilson \& Nickisch(2015)Wilson and Nickisch]{wilson2015kernel}
Wilson, Andrew and Nickisch, Hannes.
\newblock Kernel interpolation for scalable structured {G}aussian processes
  ({KISS-GP}).
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1775--1784. PMLR, 2015.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016deep}
Wilson, Andrew~Gordon, Hu, Zhiting, Salakhutdinov, Ruslan, and Xing, Eric~P.
\newblock Deep kernel learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  370--378.
  PMLR, 2016.

\bibitem[Yuan \& Neubauer(2009)Yuan and Neubauer]{yuan2009variational}
Yuan, Chao and Neubauer, Claus.
\newblock Variational mixture of {G}aussian process experts.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1897--1904. Curran Associates, Inc., 2009.

\end{thebibliography}
