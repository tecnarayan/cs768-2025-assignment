\begin{thebibliography}{10}

\bibitem{applegate2022faster}
David Applegate, Oliver Hinder, Haihao Lu, and Miles Lubin.
\newblock Faster first-order primal-dual methods for linear programming using
  restarts and sharpness.
\newblock {\em Mathematical Programming}, pages 1--52, 2022.

\bibitem{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International conference on machine learning}, pages
  214--223. PMLR, 2017.

\bibitem{bauschke2011convex}
Heinz~H Bauschke, Patrick~L Combettes, et~al.
\newblock {\em Convex analysis and monotone operator theory in Hilbert spaces},
  volume 408.
\newblock Springer, 2011.

\bibitem{blondel2018smooth}
Mathieu Blondel, Vivien Seguy, and Antoine Rolet.
\newblock Smooth and sparse optimal transport.
\newblock In {\em International conference on artificial intelligence and
  statistics}, pages 880--889. PMLR, 2018.

\bibitem{burke1988identification}
James~V Burke and Jorge~J Mor{\'e}.
\newblock On the identification of active constraints.
\newblock {\em SIAM Journal on Numerical Analysis}, 25(5):1197--1211, 1988.

\bibitem{chambolle2022accelerated}
Antonin Chambolle and Juan~Pablo Contreras.
\newblock Accelerated bregman primal-dual methods applied to optimal transport
  and wasserstein barycenter problems.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 4(4):1369--1395,
  2022.

\bibitem{courty2017joint}
Nicolas Courty, R{\'e}mi Flamary, Amaury Habrard, and Alain Rakotomamonjy.
\newblock Joint distribution optimal transportation for domain adaptation.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{courty2014domain}
Nicolas Courty, R{\'e}mi Flamary, and Devis Tuia.
\newblock Domain adaptation with regularized optimal transport.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 274--289. Springer, 2014.

\bibitem{courty2016optimal}
Nicolas Courty, R{\'e}mi Flamary, Devis Tuia, and Alain Rakotomamonjy.
\newblock Optimal transport for domain adaptation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  39(9):1853--1865, 2016.

\bibitem{cuturi2013sinkhorn}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock {\em Advances in neural information processing systems}, 26, 2013.

\bibitem{NEURIPS2019_d8c24ca8}
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert.
\newblock Differentiable ranking and sorting using optimal transport.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In {\em International conference on machine learning}, pages
  933--941. PMLR, 2017.

\bibitem{di2020optimal}
Simone Di~Marino and Augusto Gerolin.
\newblock Optimal transport losses and {S}inkhorn algorithm with general convex
  regularization.
\newblock {\em arXiv preprint arXiv:2007.00976}, 2020.

\bibitem{feydy2019interpolating}
Jean Feydy, Thibault S{\'e}journ{\'e}, Fran{\c{c}}ois-Xavier Vialard, Shun-ichi
  Amari, Alain Trouv{\'e}, and Gabriel Peyr{\'e}.
\newblock Interpolating between optimal transport and mmd using {S}inkhorn
  divergences.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2681--2690. PMLR, 2019.

\bibitem{flamary2021pot}
R{\'e}mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar~Z. Alaya,
  Aur{\'e}lie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos,
  Kilian Fatras, Nemo Fournier, L{\'e}o Gautheron, Nathalie~T.H. Gayraud,
  Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony
  Schutz, Vivien Seguy, Danica~J. Sutherland, Romain Tavenard, Alexander Tong,
  and Titouan Vayer.
\newblock Pot: Python optimal transport.
\newblock {\em Journal of Machine Learning Research}, 22(78):1--8, 2021.

\bibitem{genevay2018learning}
Aude Genevay, Gabriel Peyr{\'e}, and Marco Cuturi.
\newblock Learning generative models with {S}inkhorn divergences.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1608--1617. PMLR, 2018.

\bibitem{giselsson2016linear}
Pontus Giselsson and Stephen Boyd.
\newblock Linear convergence and metric selection for {D}ouglas-{R}achford
  splitting and {ADMM}.
\newblock {\em IEEE Transactions on Automatic Control}, 62(2):532--544, 2016.

\bibitem{gu2022keypoint}
Xiang Gu, Yucheng Yang, Wei Zeng, Jian Sun, and Zongben Xu.
\newblock Keypoint-guided optimal transport with applications in heterogeneous
  domain adaptation.
\newblock {\em Advances in Neural Information Processing Systems},
  35:14972--14985, 2022.

\bibitem{hare2004identifying}
Warren~L Hare and Adrian~S Lewis.
\newblock Identifying active constraints via partial smoothness and
  prox-regularity.
\newblock {\em Journal of Convex Analysis}, 11(2):251--266, 2004.

\bibitem{he20121}
Bingsheng He and Xiaoming Yuan.
\newblock On the {O}(1/n) convergence rate of the {D}ouglas--{R}achford
  alternating direction method.
\newblock {\em SIAM Journal on Numerical Analysis}, 50(2):700--709, 2012.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{liang2017local}
Jingwei Liang, Jalal Fadili, and Gabriel Peyr{\'e}.
\newblock Local convergence properties of {D}ouglas--{R}achford and alternating
  direction method of multipliers.
\newblock {\em Journal of Optimization Theory and Applications},
  172(3):874--913, 2017.

\bibitem{lin2019efficient}
Tianyi Lin, Nhat Ho, and Michael Jordan.
\newblock On efficient optimal transport: An analysis of greedy and accelerated
  mirror descent algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  3982--3991. PMLR, 2019.

\bibitem{liu2022sparsity}
Tianlin Liu, Joan Puigcerver, and Mathieu Blondel.
\newblock Sparsity-constrained optimal transport.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{lorenz2021quadratically}
Dirk~A Lorenz, Paul Manns, and Christian Meyer.
\newblock Quadratically regularized optimal transport.
\newblock {\em Applied Mathematics \& Optimization}, 83(3):1919--1949, 2021.

\bibitem{mai2021fast}
Vien~V Mai, Jacob Lindb{\"a}ck, and Mikael Johansson.
\newblock A fast and accurate splitting method for optimal transport: Analysis
  and implementation.
\newblock {\em International Conference on Learning Representations}, 2022.

\bibitem{peng2022optimal}
Hanyu Peng, Mingming Sun, and Ping Li.
\newblock Optimal transport for long-tailed recognition with learnable cost
  matrix.
\newblock 2022.

\bibitem{peyre2019computational}
Gabriel Peyr{\'e}, Marco Cuturi, et~al.
\newblock Computational optimal transport: With applications to data science.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  11(5-6):355--607, 2019.

\bibitem{poon2019trajectory}
Clarice Poon and Jingwei Liang.
\newblock Trajectory of alternating direction method of multipliers and
  adaptive acceleration.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{roberts2017gini}
Lucas Roberts, Leo Razoumov, Lin Su, and Yuyang Wang.
\newblock Gini-regularized optimal transport with an application to
  spatio-temporal forecasting.
\newblock {\em arXiv preprint arXiv:1712.02512}, 2017.

\bibitem{rockafellar2009variational}
R~Tyrrell Rockafellar and Roger J-B Wets.
\newblock {\em Variational analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{salimans2018improving}
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas.
\newblock Improving gans using optimal transport.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{schmitzer2019stabilized}
Bernhard Schmitzer.
\newblock Stabilized sparse scaling algorithms for entropy regularized
  transport problems.
\newblock {\em SIAM Journal on Scientific Computing}, 41(3):A1443--A1481, 2019.

\bibitem{seguy2017large}
Vivien Seguy, Bharath~Bhushan Damodaran, R{\'e}mi Flamary, Nicolas Courty,
  Antoine Rolet, and Mathieu Blondel.
\newblock Large-scale optimal transport and mapping estimation.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{shang2016understanding}
Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee.
\newblock Understanding and improving convolutional neural networks via
  concatenated rectified linear units.
\newblock In {\em international conference on machine learning}, pages
  2217--2225. PMLR, 2016.

\bibitem{sinkhorn1967concerning}
Richard Sinkhorn and Paul Knopp.
\newblock Concerning nonnegative matrices and doubly stochastic matrices.
\newblock {\em Pacific Journal of Mathematics}, 21(2):343--348, 1967.

\bibitem{thornton2023rethinking}
James Thornton and Marco Cuturi.
\newblock Rethinking initialization of the {S}inkhorn algorithm.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 8682--8698. PMLR, 2023.

\bibitem{wang2017new}
Sinong Wang and Ness Shroff.
\newblock A new alternating direction method for linear programming.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{yu2013decomposing}
Yao-Liang Yu.
\newblock On decomposing the proximal map.
\newblock {\em Advances in neural information processing systems}, 26, 2013.

\end{thebibliography}
