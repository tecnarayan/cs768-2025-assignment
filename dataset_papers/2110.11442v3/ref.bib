@article{li2020second,
  title={A second look at exponential and cosine step sizes: Simplicity, convergence, and performance},
  author={Li, Xiaoyu and Zhuang, Zhenxun and Orabona, Francesco},
  journal={arXiv preprint arXiv:2002.05273},
  year={2020}
}

@inproceedings{loizou2021stochastic,
  title={Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence},
  author={Loizou, Nicolas and Vaswani, Sharan and Laradji, Issam Hadj and Lacoste-Julien, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1306--1314},
  year={2021},
  organization={PMLR}
}

@article{vaswani2020adaptive,
  title={Adaptive gradient methods converge faster with over-parameterization (and you can do a line-search)},
  author={Vaswani, Sharan and Laradji, Issam H and Kunstner, Frederik and Meng, Si Yi and Schmidt, Mark and Lacoste-Julien, Simon},
  year={2020}
}

@article{vaswani2019painless,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={3732--3745},
  year={2019}
}

@article{allen2016optimal,
  title={Optimal black-box reductions between optimization objectives},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  pages={1614--1622},
  year={2016}
}

@article{polyak1964gradient,
  title={Gradient methods for solving equations and inequalities},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={6},
  pages={17--32},
  year={1964},
  publisher={Elsevier}
}

@article{stich2019unified,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@article{lacoste2012simpler,
  title={A simpler approach to obtaining an O (1/t) convergence rate for the projected stochastic subgradient method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@article{armijo1966minimization,
  title={Minimization of functions having {L}ipschitz continuous first partial derivatives},
  author={Armijo, Larry},
  journal={Pacific Journal of mathematics},
  year={1966},
  publisher={Mathematical Sciences Publishers}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@inproceedings{vaswani2019fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1195--1204},
  year={2019},
  organization={PMLR}
}

@article{schmidt2013fast,
  title={Fast convergence of stochastic gradient descent under a strong growth condition},
  author={Schmidt, Mark and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:1308.6370},
  year={2013}
}

@article{solodov1998incremental,
  title={Incremental gradient algorithms with stepsizes bounded away from zero},
  author={Solodov, Mikhail V},
  journal={Computational Optimization and Applications},
  year={1998}
}

@article{tseng1998incremental,
  title={An incremental gradient (-projection) method with momentum term and adaptive stepsize rule},
  author={Tseng, Paul},
  journal={SIAM Journal on Optimization},
  year={1998}
}

@inproceedings{zhang2019stochastic,
  title={Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the $ O (1/T) $ Convergence Rate},
  author={Zhang, Lijun and Zhou, Zhi-Hua},
  booktitle={Conference on Learning Theory},
  pages={3160--3179},
  year={2019},
  organization={PMLR}
}

@inproceedings{ma2018power,
  title = {The Power of Interpolation: Understanding the Effectiveness of {SGD} in Modern Over-parametrized Learning},
  author = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning, {ICML}},
  year = {2018},
}

@article{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={24},
  pages={451--459},
  year={2011}
}

@inproceedings{hinder2020near,
  title={Near-optimal methods for minimizing star-convex functions and beyond},
  author={Hinder, Oliver and Sidford, Aaron and Sohoni, Nimit},
  booktitle={Conference on Learning Theory},
  pages={1894--1938},
  year={2020},
  organization={PMLR}
}

@inproceedings{gower2021sgd,
  title={Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation},
  author={Gower, Robert and Sebbouh, Othmane and Loizou, Nicolas},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1315--1323},
  year={2021},
  organization={PMLR}
}

@article{lucas2021analyzing,
  title={Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes},
  author={Lucas, James and Bae, Juhan and Zhang, Michael R and Fort, Stanislav and Zemel, Richard and Grosse, Roger},
  journal={arXiv preprint arXiv:2104.11044},
  year={2021}
}

@inproceedings{kleinberg2018alternative,
  title={An alternative view: When does SGD escape local minima?},
  author={Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang},
  booktitle={International Conference on Machine Learning},
  pages={2698--2707},
  year={2018},
  organization={PMLR}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John C. and Hazan, Elad and Singer, Yoram},
  journal={The Journal of Machine Learning Research},
  volume    = {12},
  pages     = {2121--2159},
  year      = {2011},
}

@inproceedings{levy2018online,
  author    = {Kfir Y. Levy and
               Alp Yurtsever and
               Volkan Cevher},
  title     = {Online Adaptive Methods, Universality and Acceleration},
  booktitle = {Advances in Neural Information Processing Systems, {NeurIPS}},
  year      = {2018},
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}

@article{khaled2020better,
  title={Better theory for SGD in the nonconvex world},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}

@Article{robbins1951stochastic,
  Title                    = {A stochastic approximation method},
  Author                   = {Robbins, H. and Monro, S.},
  Journal                  = {The Annals of Mathematical Statistics},
  Year                     = {1951},
  Pages                    = {400--407},
  Publisher                = {JSTOR}
}

@Book{NemYudin1983book,
  title     = {Problem complexity and method efficiency in optimization},
  publisher = {Wiley Interscience},
  year      = {1983},
  author    = {Nemirovski, Arkadi and Yudin, David B.},
}


@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  year={2004},
  publisher={Springer Science \& Business Media}
}


@inproceedings{gower2019sgd,
  title={{SGD}: General Analysis and Improved Rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={ICML},
  year={2019}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@inproceedings{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={ICLR},
  year={2017}
}

@InProceedings{belkin2019does,
  title = 	 {Does data interpolation contradict statistical optimality?},
  author = 	 {Belkin, Mikhail and Rakhlin, Alexander and Tsybakov, Alexandre B.},
  booktitle = 	 {AISTATS},
  year = 	 {2019}
}


@article{liang2018just,
  title={Just interpolate: Kernel" ridgeless" regression can generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:1808.00387},
  year={2018}
}

@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={ICML},
  year={2019}
}

@inproceedings{li2018convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={AISTATS},
  year={2019}
}

@article{bengio2015rmsprop,
  title={Rmsprop and equilibrated adaptive learning rates for nonconvex optimization},
  author={Bengio, Yoshua},
  journal={corr abs/1502.04390},
  year={2015}
}

@inproceedings{kingma2014adam,
  author    = {Diederik Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {{ICLR}},
  year      = {2015},
}

@phdthesis{mishkin2020interpolation,
  title={Interpolation, growth conditions, and stochastic gradient descent},
  author={Mishkin, Aaron},
  year={2020},
  school={University of British Columbia}
}

@inproceedings{cohen2018acceleration,
  title={On acceleration with noise-corrupted gradients},
  author={Cohen, Michael and Diakonikolas, Jelena and Orecchia, Lorenzo},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2018},
  organization={PMLR}
}

@book{lohr2019sampling,
  title={{Sampling: Design and Analysis: Design and Analysis}},
  author={Lohr, Sharon L},
  year={2019},
  publisher={Chapman and Hall/CRC}
}

@article{malitsky2019adaptive,
  title={Adaptive gradient descent without descent},
  author={Malitsky, Yura and Mishchenko, Konstantin},
  journal={arXiv preprint arXiv:1910.09529},
  year={2019}
}

@inproceedings{berrada2020training,
  title={Training neural networks for and by interpolation},
  author={Berrada, Leonard and Zisserman, Andrew and Kumar, M Pawan},
  booktitle={International Conference on Machine Learning},
  pages={799--809},
  year={2020},
  organization={PMLR}
}

@article{libsvm,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 number = {3},
 pages = {1--27},
 year = {2011},
 note = {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}

@article{nguyen2018tight,
  title={Tight dimension independent lower bound on the expected convergence rate for diminishing step sizes in SGD},
  author={Nguyen, Phuong Ha and Nguyen, Lam M and van Dijk, Marten},
  journal={arXiv preprint arXiv:1810.04723},
  year={2018}
}

@inproceedings{kovalev2020don,
  title={Don’t jump through hoops and remove those loops: {SVRG} and {K}atyusha are better without the outer loop},
  author={Kovalev, Dmitry and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={Algorithmic Learning Theory},
  pages={451--467},
  year={2020},
  organization={PMLR}
}

@article{aybat2019universally,
  title={A universally optimal multistage accelerated stochastic gradient method},
  author={Aybat, Necdet Serhat and Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asuman},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8525--8536},
  year={2019}
}

@inproceedings{arjevani2020tight,
  title={A tight convergence analysis for stochastic gradient descent with delayed updates},
  author={Arjevani, Yossi and Shamir, Ohad and Srebro, Nathan},
  booktitle={Algorithmic Learning Theory},
  pages={111--132},
  year={2020},
  organization={PMLR}
}

@article{ghadimi2013optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, II: shrinking procedures and optimal algorithms},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2061--2089},
  year={2013},
  publisher={SIAM}
}

@inproceedings{kulunchakov2019estimate,
  title={Estimate sequences for variance-reduced stochastic composite optimization},
  author={Kulunchakov, Andrei and Mairal, Julien},
  booktitle={International Conference on Machine Learning},
  pages={3541--3550},
  year={2019},
  organization={PMLR}
}

@inproceedings{arjevani2016iteration,
  title={On the iteration complexity of oblivious first-order optimization algorithms},
  author={Arjevani, Yossi and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={908--916},
  year={2016},
  organization={PMLR}
}

@inproceedings{barre2020complexity,
  title={Complexity guarantees for polyak steps with momentum},
  author={Barr{\'e}, Mathieu and Taylor, Adrien and d’Aspremont, Alexandre},
  booktitle={Conference on Learning Theory},
  pages={452--478},
  year={2020},
  organization={PMLR}
}

@inproceedings{jain2018accelerating,
  title={Accelerating stochastic gradient descent for least squares regression},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  booktitle={Conference On Learning Theory},
  pages={545--604},
  year={2018},
  organization={PMLR}
}

@article{liu2018accelerating,
  title={Accelerating sgd with momentum for over-parameterized learning},
  author={Liu, Chaoyue and Belkin, Mikhail},
  journal={arXiv preprint arXiv:1810.13395},
  year={2018}
}