\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Boob et~al.(2022)Boob, Deng, and Lan]{boob2022stochastic}
Boob, D., Deng, Q., and Lan, G.
\newblock Stochastic first-order methods for convex and nonconvex functional
  constrained optimization.
\newblock \emph{Mathematical Programming}, pp.\  1--65, 2022.

\bibitem[Chambolle \& Pock(2011)Chambolle and Pock]{chambolle2011first}
Chambolle, A. and Pock, T.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock \emph{Journal of mathematical imaging and vision}, 40\penalty0
  (1):\penalty0 120--145, 2011.

\bibitem[Chambolle \& Pock(2016)Chambolle and Pock]{chambolle2016ergodic}
Chambolle, A. and Pock, T.
\newblock On the ergodic convergence rates of a first-order primal--dual
  algorithm.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1):\penalty0 253--287,
  2016.

\bibitem[Chen et~al.(2014)Chen, Lan, and Ouyang]{chen2014optimal}
Chen, Y., Lan, G., and Ouyang, Y.
\newblock Optimal primal-dual methods for a class of saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  1779--1814, 2014.

\bibitem[Chen et~al.(2017)Chen, Lan, and Ouyang]{chen2017accelerated}
Chen, Y., Lan, G., and Ouyang, Y.
\newblock Accelerated schemes for a class of variational inequalities.
\newblock \emph{Mathematical Programming}, 165\penalty0 (1):\penalty0 113--149,
  2017.

\bibitem[Condat(2013)]{Condat2013APS}
Condat, L.
\newblock A primal–dual splitting method for convex optimization involving
  lipschitzian, proximable and linear composite terms.
\newblock \emph{Journal of Optimization Theory and Applications}, 158:\penalty0
  460--479, 2013.

\bibitem[Dang \& Lan(2014)Dang and Lan]{dang2014randomized}
Dang, C. and Lan, G.
\newblock Randomized first-order methods for saddle point optimization.
\newblock \emph{arXiv preprint arXiv:1409.8625}, 2014.

\bibitem[Hamedani \& Aybat(2021)Hamedani and Aybat]{hamedani2021primal}
Hamedani, E.~Y. and Aybat, N.~S.
\newblock A primal-dual algorithm with line search for general convex-concave
  saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (2):\penalty0
  1299--1329, 2021.

\bibitem[Hamedani et~al.(2018)Hamedani, Jalilzadeh, Aybat, and
  Shanbhag]{hamedani2018iteration}
Hamedani, E.~Y., Jalilzadeh, A., Aybat, N.~S., and Shanbhag, U.~V.
\newblock Iteration complexity of randomized primal-dual methods for
  convex-concave saddle point problems.
\newblock \emph{arXiv preprint arXiv:1806.04118}, 2018.

\bibitem[Juditsky et~al.(2011)Juditsky, Nemirovski, and
  Tauvel]{Juditsky:2011svi}
Juditsky, A., Nemirovski, A.~S., and Tauvel, C.
\newblock {Solving variational inequalities with Stochastic Mirror-Prox
  algorithm}.
\newblock \emph{{Stochastic Systems}}, 1\penalty0 (1):\penalty0 17--58, 2011.
\newblock \doi{10.1214/10-SSY011}.
\newblock URL \url{https://hal.archives-ouvertes.fr/hal-00318043}.

\bibitem[Korpelevich(1976)]{korpelevich1976extragradient}
Korpelevich, G.~M.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock \emph{Matecon}, 12:\penalty0 747--756, 1976.

\bibitem[Kovalev et~al.(2022)Kovalev, Gasnikov, and
  Richtárik]{kovalev2022accelerated}
Kovalev, D., Gasnikov, A., and Richtárik, P.
\newblock Accelerated primal-dual gradient method for smooth and convex-concave
  saddle-point problems with bilinear coupling, 2022.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{lin2020near}
Lin, T., Jin, C., and Jordan, M.~I.
\newblock Near-optimal algorithms for minimax optimization.
\newblock In \emph{Conference on Learning Theory}, pp.\  2738--2779. PMLR,
  2020.

\bibitem[Monteiro \& Svaiter(2010)Monteiro and Svaiter]{monteiro2010complexity}
Monteiro, R.~D. and Svaiter, B.~F.
\newblock On the complexity of the hybrid proximal extragradient method for the
  iterates and the ergodic mean.
\newblock \emph{SIAM Journal on Optimization}, 20\penalty0 (6):\penalty0
  2755--2787, 2010.

\bibitem[Nemirovski(2004)]{Nemirovski:2004}
Nemirovski, A.
\newblock Prox-method with rate of convergence o(1/t) for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{{SIAM} Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.
\newblock \doi{10.1137/S1052623403425629}.
\newblock URL \url{https://doi.org/10.1137/S1052623403425629}.

\bibitem[Nemirovski \& Yudin(1983)Nemirovski and Yudin]{Nemirovski:1983problem}
Nemirovski, A. and Yudin, D.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley-Interscience publication in Discrete Mathematics. John Wiley,
  XV, 1983.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{Nemirovski:2009robust}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.
\newblock \doi{10.1137/070704277}.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov(2005)]{Nesterov:2005}
Nesterov, Y.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical Programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Nesterov(2007)]{nesterov2007dual}
Nesterov, Y.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock \emph{Mathematical Programming}, 109\penalty0 (2):\penalty0 319--344,
  2007.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Y.~E.
\newblock A method for solving the convex programming problem with convergence
  rate {$O\big(\frac{1}{k^2}\big)$}.
\newblock In \emph{Dokl. Akad. Nauk SSSR,}, volume 269, pp.\  543--547, 1983.

\bibitem[Thekumparampil et~al.(2019)Thekumparampil, Jain, Netrapalli, and
  Oh]{thekumparampil2019efficient}
Thekumparampil, K.~K., Jain, P., Netrapalli, P., and Oh, S.
\newblock Efficient algorithms for smooth minimax optimization, 2019.

\bibitem[Thekumparampil et~al.(2022)Thekumparampil, He, and
  Oh]{thekumparampil2022lifted}
Thekumparampil, K.~K., He, N., and Oh, S.
\newblock Lifted primal-dual method for bilinearly coupled smooth minimax
  optimization, 2022.

\bibitem[Tseng(2008)]{tseng2008accelerated}
Tseng, P.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock \emph{submitted to SIAM Journal on Optimization}, 2\penalty0 (3),
  2008.

\bibitem[Vu(2011)]{Vu2011ASA}
Vu, B.~C.
\newblock A splitting algorithm for dual monotone inclusions involving
  cocoercive operators.
\newblock \emph{Advances in Computational Mathematics}, 38:\penalty0 667--681,
  2011.

\bibitem[Yu et~al.(2015)Yu, Lin, and Yang]{yu2015doubly}
Yu, A.~W., Lin, Q., and Yang, T.
\newblock Doubly stochastic primal-dual coordinate method for regularized
  empirical risk minimization with factorized data.
\newblock \emph{CoRR, abs/1508.03390}, 2015.

\bibitem[Zhang \& Lin(2015)Zhang and Lin]{zhang2015stochastic}
Zhang, Y. and Lin, X.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  353--361. PMLR, 2015.

\bibitem[Zhu \& Storkey(2015)Zhu and Storkey]{zhu2015adaptive}
Zhu, Z. and Storkey, A.~J.
\newblock Adaptive stochastic primal-dual coordinate descent for separable
  saddle point problems.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  645--658. Springer, 2015.

\end{thebibliography}
