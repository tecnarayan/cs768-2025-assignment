@article{benigni2019eigenvalue,
author = {Lucas Benigni and Sandrine P{\'e}ch{\'e}},
title = {{Eigenvalue distribution of some nonlinear models of random matrices}},
volume = {26},
journal = {Electronic Journal of Probability},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {1 -- 37},
keywords = {machine learning, neural networks, random matrices},
year = {2021},
}

@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{chen2020much,
  title={How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?},
  author={Chen, Zixiang and Cao, Yuan and Zou, Difan and Gu, Quanquan},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}


@inbook{vershrandmat, place={Cambridge}, title={Introduction to the non-asymptotic analysis of random matrices}, DOI={10.1017/CBO9780511794308.006}, booktitle={Compressed Sensing: Theory and Applications}, publisher={Cambridge University Press}, author={Vershynin, Roman}, editor={Eldar, Yonina C. and Kutyniok, Gitta}, year={2012}, pages={210–268}}


@article{theoreticalinsghts,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={2},
  pages={742--769},
  year={2018},
  publisher={IEEE}
}

@inproceedings{bubeck2020network,
  title={Network size and weights size for memorization with two-layers neural networks},
  author={S{\'e}bastien Bubeck and Ronen Eldan and Yin Tat Lee and Dan Mikulincer},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{QuynhICML2019,
  title     = {On Connected Sublevel Sets in Deep Learning},
  author    = {Q. Nguyen},
  booktitle = {International Conference on Machine Learning (ICML)},
  pages = {4790-4799},
  year      = {2019}
}

@inproceedings{han2017deep,
  title={Deep pyramidal residual networks},
  author={Han, Dongyoon and Kim, Jiwhan and Kim, Junmo},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {5927-5935},
  year={2017}
}


@inproceedings{ghorbani2020neural,
  title={When do neural networks outperform kernel methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{mei2021generalization,
  title={Generalization error of random feature and kernel methods: hypercontractivity and kernel matrix concentration},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Applied and Computational Harmonic Analysis},
  year={2021},
  publisher={Elsevier}
}

@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}

@article{ghorbani2021linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={49},
  number={2},
  pages={1029--1054},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}


@inproceedings{VGG,
  author    = {Karen Simonyan and Andrew Zisserman},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2015},
}

@inproceedings{QuynhICML2018,
  title     = {Optimization Landscape and Expressivity of Deep {CNN}s},
  author    = {Nguyen, Quynh and Hein, Matthias},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2018}
}

@inproceedings{QuynhICML2017,
  title     = {The loss surface of deep and wide neural networks},
  author    = {Nguyen, Quynh and Hein, Matthias},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2017}
}

@inproceedings{Zhang2017,
  author    = {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
  title     = {Understanding Deep Learning Requires Re-Thinking Generalization},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2017}
}

@inproceedings{seddik2020random,
  title={Random matrix theory proves that deep learning representations of {GAN-}data behave as gaussian mixtures},
  author={Mohamed El Amine Seddik and Cosme Louart and Mohamed Tamaazousti and Romain Couillet},
  booktitle = {International Conference on Machine Learning (ICML)},
  year={2020}
}

@inproceedings{SafranShamir2018,
  title	    = {Spurious Local Minima are Common in Two-Layer {ReLU} Neural Networks},
  author    = {Safran, Itay and Shamir, Ohad},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2018}
}


@inproceedings{Auer96,
  title	    = {Exponentially many local minima for single neurons},
  author    = {P. Auer and M. Herbster and M. Warmuth},
  booktitle = {Neural Information Processing Systems (NIPS)},
  year      = {1996}
}


@article{Blum1989,
  title     = {Training a 3-node neural network is {NP}-complete},
  author    = {Blum, Avrim L and Rivest, Ronald L},
  journal   = {Neural Networks},
  volume    = {5},
  number    = {1},
  pages     = {117--127},
  year      = {1992},
  publisher = {Elsevier}
}
@inproceedings{Yun2019,
  author    = {Chulhee Yun and Suvrit Sra and Ali Jadbabaie},
  title     = {Small nonlinearities in activation functions create bad local minima in neural networks},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2019},
}

@preprint{ge2019mildly,
  title={Mildly overparametrized neural nets can memorize training data efficiently},
  author={Rong Ge and Runzhe Wang and Haoyu Zhao},
  note={\texttt{arXiv:1909.11837}},
  year={2019}
}

@inproceedings{nguyen2021solutions,
  title={When Are Solutions Connected in Deep Networks?},
  author={Nguyen, Quynh and Br{\'e}chet, Pierre and Mondelli, Marco},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{yun2019small,
  title={Small {ReLU} networks are powerful memorizers: a tight analysis of memorization capacity},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{hammer,
  title={Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling},
  author={Adamczak, Radoslaw and Litvak, Alexander E and Pajor, Alain and Tomczak-Jaegermann, Nicole},
  journal={Constructive Approximation},
  volume={34},
  number={1},
  pages={61--88},
  year={2011},
  publisher={Springer}
}

@article{HWconvex,
  title={{A note on the Hanson-Wright inequality for random vectors with dependencies}},
  author={Adamczak, Radoslaw},
  journal={Electronic Communications in Probability},
  volume={20},
  pages={1--13},
  year={2015},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}


@inproceedings{tightbounds,
  title={Tight bounds on the smallest eigenvalue of the neural tangent kernel for deep {ReLU} networks},
  author={Nguyen, Quynh and Mondelli, Marco and Montufar, Guido},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}

@book{vershynin2018high,
  title     = {High-dimensional probability: An introduction with applications in data science},
  author    = {Roman Vershynin},
  year      = {2018},
  publisher = {Cambridge university press}
}

@inproceedings{song2021subquadratic,
  title={Subquadratic Overparameterization for Shallow Neural Networks},
  author={Song, Chaehwan and Ramezani-Kebrya, Ali and Pethick, Thomas and Eftekhari, Armin and Cevher, Volkan},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2021}
}
@inproceedings{JacotEtc2018,
  title     = {Neural tangent kernel: Convergence and generalization in neural networks},
  author    = {Arthur Jacot and Franck Gabriel and Clément Hongler},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2018}
}

@inproceedings{AroraEtal2019,
  title	    = {On Exact Computation with an Infinitely Wide Neural Net},
  author    = {Sanjeev Arora and Simon S. Du and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Rousong Wang},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}


@inproceedings{DuEtal2018_ICLR,
  author    = {Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
  title     = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year 	    = {2019},
}


@article{OymakMahdi2019,
  title={Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={84--105},
  year={2020},
  publisher={IEEE}
}

@inproceedings{oymak2019overparameterized,
  title={Overparameterized nonlinear learning: Gradient descent takes the shortest path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}

@article{bartlett2021deep,
  title={Deep learning: a statistical viewpoint},
  author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
  journal={Acta numerica},
  volume={30},
  pages={87--201},
  year={2021},
  publisher={Cambridge University Press}
}

@preprint{SongYang2020,
  author    = {Zhao Song and Xin Yang},
  title     = {Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound},
  note      = {\texttt{arXiv:1906.03593}},
  year 	    = {2020},
}

@preprint{wu2019global,
  title     = {Global convergence of adaptive gradient methods for an over-parameterized neural network},
  author    = {Xiaoxia Wu and Simon S. Du and Rachel Ward},
  note      = {\texttt{arXiv:1902.07111}},
  year      = {2019}
}

@inproceedings{DuEtal2019,
  title     = {Gradient Descent Finds Global Minima of Deep Neural Networks},
  author    = {Simon S. Du and Jason D. Lee and Haochuan Li and Liwei Wang and Xiyu Zhai},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}
@preprint{ZouEtal2018,
  author    = {Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  title     = {Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks},
  note      = {\texttt{arXiv:1811.08888}},
  year 	    = {2018},
}
@inproceedings{AllenZhuEtal2018,
  title     = {A Convergence Theory for Deep Learning via Over-Parameterization},
  author    = {Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2019}
}

@article{zou2020gradient,
  title={Gradient descent optimizes over-parameterized deep ReLU networks},
  author={Difan Zou and Yuan Cao and Dongruo Zhou and Quanquan Gu},
  journal={Machine Learning},
  volume={109},
  number={3},
  pages={467--492},
  year={2020},
  publisher={Springer}
}

@inproceedings{ZouGu2019,
  title     = {An improved analysis of training over-parameterized deep neural networks},
  author    = {Difan Zou and Quanquan Gu},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year      = {2019}
}

@inproceedings{QuynhMarco2020,
  author    = {Quynh Nguyen and Marco Mondelli},
  title     = {Global Convergence of Deep Networks with One Wide Layer Followed by Pyramidal Topology},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year 	    = {2020},
}
@preprint{Andrea2020,
  author    = {Andrea Montanari and Yiqiao Zhong},
  title     = {The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training},
  note      = {\texttt{arXiv:2007.12826}},
  year 	    = {2020},
}

@inproceedings{pennington2017nonlinear,
  title={Nonlinear random matrix theory for deep learning},
  author={Jeffrey Pennington and Pratik Worah},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@article{louart2018random,
  title={A random matrix approach to neural networks},
  author={Cosme Louart and Zhenyu Liao and Romain Couillet},
  journal={The Annals of Applied Probability},
  volume={28},
  number={2},
  pages={1190--1248},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@preprint{adlam2019random,
  title={A random matrix perspective on mixtures of nonlinearities for deep learning},
  author={Ben Adlam and Jake Levinson and Jeffrey Pennington},
  note={\texttt{arXiv:1912.00827}},
  year={2019}
}


@inproceedings{buchanan2021deep,
  title={Deep Networks and the Multiple Manifold Problem},
  author={Buchanan, Sam and Gilboa, Dar and Wright, John},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@preprint{wang2021deformed,
  title={Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks},
  author={Wang, Zhichao and Zhu, Yizhe},
  note={\texttt{arXiv:2109.09304}},
  year={2021}
}

@article{ge2021large,
  title={Large-dimensional random matrix theory and its applications in deep learning and wireless communications},
  author={Ge, Jungang and Liang, Ying-Chang and Bai, Zhidong and Pan, Guangming},
  journal={Random Matrices: Theory and Applications},
  pages={2230001},
  year={2021},
  publisher={World Scientific}
}

@article{peche2019note,
  title={A note on the {Pennington-Worah} distribution},
  author={P{\'e}ch{\'e}, Sandrine},
  journal={Electronic Communications in Probability},
  volume={24},
  pages={1--7},
  year={2019},
  publisher={Institute of Mathematical Statistics and Bernoulli Society}
}

@inproceedings{piccolo2021analysis,
  title={Analysis of one-hidden-layer neural networks via the resolvent method},
  author={Piccolo, Vanessa and Schr{\"o}der, Dominik},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{XavierBengio2010,
  author    = {Xavier Glorot and Yoshua Bengio},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  booktitle   = {International Conference on Machine Learning (ICML)},
  year      = {2010},	
}

@inproceedings{adlam2020neural,
  title={The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization},
  author={Adlam, Ben and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2020}
}

@inproceedings{avron2017random,
  title={Random Fourier features for kernel ridge regression: Approximation bounds and statistical guarantees},
  author={Avron, Haim and Kapralov, Michael and Musco, Cameron and Musco, Christopher and Velingker, Ameya and Zandieh, Amir},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}

@article{cover1965geometrical,
  title={Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
  author={Thomas M. Cover},
  journal={IEEE Transactions on Electronic Computers},
  number={3},
  pages={326--334},
  year={1965},
  publisher={IEEE}
}

@article{Baum1988,
  author    = {Eric B. Baum},
  title     = {On the capabilities of multilayer perceptrons},
  journal   = {Journal of Complexity},
  year 	    = {1988},
  volume    = {4},
}

@inproceedings{kowalczyk1994counting,
  title={Counting function theorem for multi-layer networks},
  author={Adam Kowalczyk},
  booktitle={Neural Information Processing Systems (NIPS)},
  year={1994}
}

@article{bartlett2019nearly,
  title={Nearly-tight VC-dimension and Pseudodimension Bounds for Piecewise Linear Neural Networks.},
  author={Peter Bartlett and Nick Harvey and Christopher Liaw and Abbas Mehrabian},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={20},
  number={63},
  pages={1--17},
  year={2019}
}

@inproceedings{sakurai1992nh,
  title={nh-1 networks store no less n* h+ 1 examples, but sometimes no more},
  author={Akito Sakurai},
  booktitle={IJCNN International Joint Conference on Neural Networks},
  year={1992},
}

@inproceedings{wang2021modular,
  title={A Modular Analysis of Provable Acceleration via {P}olyak’s Momentum: Training a Wide {ReLU} Network and a Deep Linear Network},
  author={Wang, Jun-Kun and Lin, Chi-Heng and Abernethy, Jacob D},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}

@inproceedings{huang2021fl,
  title={Fl-ntk: A neural tangent kernel-based framework for federated learning analysis},
  author={Huang, Baihe and Li, Xiaoxiao and Song, Zhao and Yang, Xin},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}

@inproceedings{he2015delving,
  title     = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}


@article{vershynin2020memory,
  title={Memory Capacity of Neural Networks with Threshold and Rectified Linear Unit Activations},
  author={Roman Vershynin},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1004--1033},
  year={2020},
  publisher={SIAM}
}

@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={The Annals of Statistics},
  volume={50},
  number={2},
  pages={949--986},
  year={2022},
  publisher={Institute of Mathematical Statistics}
}


@inproceedings{hu2020surprising,
  title={The surprising simplicity of the early-time learning dynamics of neural networks},
  author={Hu, Wei and Xiao, Lechao and Adlam, Ben and Pennington, Jeffrey},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@preprint{daniely2020memorizing,
  title={Memorizing gaussians with no over-parameterizaion via gradient decent on neural networks},
  author={Daniely, Amit},
  note={\texttt{arXiv:2003.12895}},
  year={2020}
}

@inproceedings{daniely2020neural,
  title={Neural networks learning and memorization with (almost) no over-parameterization},
  author={Daniely, Amit},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}


@inproceedings{liao2020random,
  title={{A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent}},
  author={Zhenyu Liao and Romain Couillet and Michael W. Mahoney},
  booktitle = {Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@inproceedings{pennington2018emergence,
  title={The emergence of spectral universality in deep networks},
  author={Jeffrey Pennington and Samuel Schoenholz and Surya Ganguli},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2018}
}

@inproceedings{pennington2017geometry,
  title={Geometry of neural network loss surfaces via random matrix theory},
  author={Jeffrey Pennington and Yasaman Bahri},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}

@inproceedings{pennington2018spectrum,
  title={The spectrum of the fisher information matrix of a single-hidden-layer neural network},
  author={Jeffrey Pennington and Pratik Worah},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2018}
}
@inproceedings{liao2018spectrum,
  title={On the Spectrum of Random Features Maps of High Dimensional Data},
  author={Zhenyu Liao and Romain Couillet},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

@inproceedings{fan2020spectra,
  title={Spectra of the conjugate kernel and neural tangent kernel for linear-width neural networks},
  author={Fan, Zhou and Wang, Zhichao},
  booktitle={Neural information processing systems (NeurIPS)},
  year={2020}
}

@article{liu2020linearity,
  title={On the linearity of large non-linear models: when and why the tangent kernel is constant},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Misha},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15954--15964},
  year={2020}
}

@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Sanjeev Arora and Simon Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}


@inproceedings{nguyen2021proof,
  title={On the proof of global convergence of gradient descent for deep relu networks with linear widths},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}












@inproceedings{deepmemorization,
title={Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization},
author={Bombari, Simone and Amani, Mohammad Hossein and Mondelli, Marco},
booktitle={Thirty-Sixth Conference on Neural Information Processing Systems},
year={2022}
}