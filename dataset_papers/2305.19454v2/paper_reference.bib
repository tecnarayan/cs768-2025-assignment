@article{dong2019network,
  title={Network pruning via transformable architecture search},
  author={Dong, Xuanyi and Yang, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{jaiswal2022training,
  title={Training your sparse neural network better with any mask},
  author={Jaiswal, Ajay Kumar and Ma, Haoyu and Chen, Tianlong and Ding, Ying and Wang, Zhangyang},
  booktitle={International Conference on Machine Learning},
  pages={9833--9844},
  year={2022},
  organization={PMLR}
}

@article{you2019gate,
  title={Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks},
  author={You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{liu2022more,
  title={More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2207.03620},
  year={2022}
}

@inproceedings{hou2022chex,
  title={CHEX: CHannel EXploration for CNN Model Compression},
  author={Hou, Zejiang and Qin, Minghai and Sun, Fei and Ma, Xiaolong and Yuan, Kun and Xu, Yi and Chen, Yen-Kuang and Jin, Rong and Xie, Yuan and Kung, Sun-Yuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12287--12298},
  year={2022}
}


@article{yu2019autoslim,
  title={Autoslim: Towards one-shot architecture search for channel numbers},
  author={Yu, Jiahui and Huang, Thomas},
  journal={arXiv preprint arXiv:1903.11728},
  year={2019}
}

@article{su2021locally,
  title={Locally free weight sharing for network width search},
  author={Su, Xiu and You, Shan and Huang, Tao and Wang, Fei and Qian, Chen and Zhang, Changshui and Xu, Chang},
  journal={arXiv preprint arXiv:2102.05258},
  year={2021}
}
@article{kang2020operation,
  title={Operation-Aware Soft Channel Pruning using Differentiable Masks},
  author={Kang, Minsoo and Han, Bohyung},
  journal={Proceedings of International Conference on Machine Learning},
  pages={5122--5131},
  year={2020},
}
@article{li2020group,
  title={Group sparsity: The hinge between filter pruning and decomposition for network compression},
  author={Li, Yawei and Gu, Shuhang and Mayer, Christoph and Gool, Luc Van and Timofte, Radu},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8018--8027},
  year={2020}
}

@article{zhuang2018discrimination,
  title={Discrimination-aware channel pruning for deep neural networks},
  author={Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jinhui},
  journal={Proceedings of Advances in Neural Information Processing Systems},
  pages={883--894},
  year={2018}
}

@article{zhuang2020neuron,
  title={Neuron-level Structured Pruning using Polarization Regularizer},
  author={Zhuang, Tao and Zhang, Zhixuan and Huang, Yuheng and Zeng, Xiaoyi and Shuang, Kai and Li, Xiang},
  journal={Proceedings of Advances in Neural Information Processing Systems},
  year={2020}
}
@article{li2021dynamic,
  title={Dynamic Slimmable Network},
  author={Li, Changlin and Wang, Guangrun and Wang, Bing and Liang, Xiaodan and Li, Zhihui and Chang, Xiaojun},
  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2021}
}

@article{luo2020neural,
  title={Neural Network Pruning with Residual-Connections and Limited-Data},
  author={Luo, Jian-Hao and Wu, Jianxin},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1458--1467},
  year={2020}
}


@article{guo2020dmcp,
  title={Dmcp: Differentiable markov channel pruning for neural networks},
  author={Guo, Shaopeng and Wang, Yujie and Li, Quanquan and Yan, Junjie},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1539--1547},
  year={2020}
}

@article{liu2019metapruning,
  title={Metapruning: Meta learning for automatic neural network channel pruning},
  author={Liu, Zechun and Mu, Haoyuan and Zhang, Xiangyu and Guo, Zichao and Yang, Xin and Cheng, Kwang-Ting and Sun, Jian},
  journal={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3296--3305},
  year={2019}
}

@article{li2020eagleeye,
  title={Eagleeye: Fast sub-net evaluation for efficient neural network pruning},
  author={Li, Bailin and Wu, Bowen and Su, Jiang and Wang, Guangrun},
  journal={Proceedings of European Conference on Computer Vision},
  pages={639--654},
  year={2020},
}


@article{ding2020lossless,
  title={Lossless CNN Channel Pruning via Decoupling Remembering and Forgetting},
  author={Ding, Xiaohan and Hao, Tianxiang and Tan, Jianchao and Liu, Ji and Han, Jungong and Guo, Yuchen and Ding, Guiguang},
  journal={Proceedings of the IEEE/CVF International Conference on Com-puter Vision},
  year={2021}
}


@inproceedings{rumi2020accelerating,
  title={Accelerating sparse CNN inference on GPUs with performance-aware weight pruning},
  author={Rumi, Masuma Akter and Ma, Xiaolong and Wang, Yanzhi and Jiang, Peng},
  booktitle={Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques},
  pages={267--278},
  year={2020}
}


@article{chetlur2014cudnn,
  title={cudnn: Efficient primitives for deep learning},
  author={Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal={arXiv preprint arXiv:1410.0759},
  year={2014}
}



@inproceedings{Peng2022exp,
  title={Exposing and Exploiting Fine-Grained Block Structures for Fast and Accurate Sparse Training},
  author={Jiang, Peng and Hu, Lihan and Song, Shihui},
  booktitle={Advances in Neural Information Processing Systems},
    year={2022}
}

@article{narang2017block,
  title={Block-sparse recurrent neural networks},
  author={Narang, Sharan and Undersander, Eric and Diamos, Gregory},
  journal={arXiv preprint arXiv:1711.02782},
  year={2017}
}

@article{gray2017gpu,
  title={Gpu kernels for block-sparse weights},
  author={Gray, Scott and Radford, Alec and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1711.09224},
  volume={3},
  pages={2},
  year={2017}
}

@article{chen2022coarsening,
  title={Coarsening the granularity: Towards structurally sparse lottery tickets},
  author={Chen, Tianlong and Chen, Xuxi and Ma, Xiaolong and Wang, Yanzhi and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2202.04736},
  year={2022}
}





@article{you2019drawing,
  title={Drawing early-bird tickets: Towards more efficient training of deep networks},
  author={You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G and Wang, Zhangyang and Lin, Yingyan},
  journal={arXiv preprint arXiv:1909.11957},
  year={2019}
}

@inproceedings{zhu2019sparse,
  title={Sparse tensor core: Algorithm and hardware co-design for vector-wise sparse neural networks on modern gpus},
  author={Zhu, Maohua and Zhang, Tao and Gu, Zhenyu and Xie, Yuan},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={359--371},
  year={2019}
}

@article{sun2021dominosearch,
  title={DominoSearch: Find layer-wise fine-grained N: M sparse schemes from dense neural networks},
  author={Sun, Wei and Zhou, Aojun and Stuijk, Sander and Wijnhoven, Rob and Nelson, Andrew O and Corporaal, Henk and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20721--20732},
  year={2021}
}



@article{yin2022superposing,
  title={Superposing Many Tickets into One: A Performance Booster for Sparse Neural Network Training},
  author={Yin, Lu and Menkovski, Vlado and Fang, Meng and Huang, Tianjin and Pei, Yulong and Pechenizkiy, Mykola and Mocanu, Decebal Constantin and Liu, Shiwei},
  journal={arXiv preprint arXiv:2205.15322},
  year={2022}
}
@inproceedings{evci2022gradient,
  title={Gradient flow in sparse neural networks and how lottery tickets win},
  author={Evci, Utku and Ioannou, Yani and Keskin, Cem and Dauphin, Yann},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={6},
  pages={6577--6586},
  year={2022}
}


@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}
@article{rame2022diverse,
  title={Diverse Weight Averaging for Out-of-Distribution Generalization},
  author={Rame, Alexandre and Kirchmeyer, Matthieu and Rahier, Thibaud and Rakotomamonjy, Alain and Gallinari, Patrick and Cord, Matthieu},
  journal={arXiv preprint arXiv:2205.09739},
  year={2022}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}
@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}


@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 volume = {33},
 year = {2020}
}


@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{srivastava2022beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@inproceedings{liu2022swin,
  title={Swin transformer v2: Scaling up capacity and resolution},
  author={Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12009--12019},
  year={2022}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{lecun1989handwritten,
  title={Handwritten digit recognition with a back-propagation network},
  author={LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, Richard and Hubbard, Wayne and Jackel, Lawrence},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@inproceedings{Alexnet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}


@INPROCEEDINGS{resnet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}


@inproceedings{wortsman2021learning,
  title={Learning neural network subspaces},
  author={Wortsman, Mitchell and Horton, Maxwell C and Guestrin, Carlos and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={11217--11227},
  year={2021},
  organization={PMLR}
}


@article{schwartz2020green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}



@article{zhou2019deconstructing,
  title={Deconstructing lottery tickets: Zeros, signs, and the supermask},
  author={Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{ramanujan2020s,
  title={What's hidden in a randomly weighted neural network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11893--11902},
  year={2020}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}


@article{morcos2019one,
  title={One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
  author={Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{chen2021gans,
  title={Gans can play lottery tickets too},
  author={Chen, Xuxi and Zhang, Zhenyu and Sui, Yongduo and Chen, Tianlong},
  journal={arXiv preprint arXiv:2106.00134},
  year={2021}
}

@article{mozer1989using,
  title={Using relevance to reduce network size automatically},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={3--16},
  year={1989},
  publisher={Taylor \& Francis}
}




@article{yu2019playing,
  title={Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp},
  author={Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S},
  journal={arXiv preprint arXiv:1906.02768},
  year={2019}
}

@techreport{ruppert1988efficient,
  title={Efficient estimations from a slowly convergent Robbins-Monro process},
  author={Ruppert, David},
  year={1988},
  institution={Cornell University Operations Research and Industrial Engineering}
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}



@article{zhang2019lookahead,
  title={Lookahead optimizer: k steps forward, 1 step back},
  author={Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{huang2017snapshot,
  title={Snapshot ensembles: Train 1, get m for free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:1704.00109},
  year={2017}
}

@inproceedings{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={8803--8812},
  year={2018}
}

@article{goodfellow2014qualitatively,
  title={Qualitatively characterizing neural network optimization problems},
  author={Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1412.6544},
  year={2014}
}

@article{evci2019difficulty,
  title={The difficulty of training sparse neural networks},
  author={Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
  journal={arXiv preprint arXiv:1906.10732},
  year={2019}
}

@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020}
}

@article{nagarajan2019uniform,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@misc{patching,
  doi = {10.48550/ARXIV.2208.05592},
  
  url = {https://arxiv.org/abs/2208.05592},
  
  author = {Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Patching open-vocabulary models by interpolating weights},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@article{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@techreport{perrone1992networks,
  title={When networks disagree: Ensemble methods for hybrid neural networks},
  author={Perrone, Michael P and Cooper, Leon N},
  year={1992},
  institution={BROWN UNIV PROVIDENCE RI INST FOR BRAIN AND NEURAL SYSTEMS}
}

@article{breiman1996bagging,
  title={Bagging predictors},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={2},
  pages={123--140},
  year={1996},
  publisher={Springer}
}

@inproceedings{Dietterich2000EnsembleMI,
  title={Ensemble Methods in Machine Learning},
  author={Thomas G. Dietterich},
  booktitle={Multiple Classifier Systems},
  year={2000}
}


@inproceedings{gustafsson2020evaluating,
  title={Evaluating scalable bayesian deep learning methods for robust computer vision},
  author={Gustafsson, Fredrik K and Danelljan, Martin and Schon, Thomas B},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={318--319},
  year={2020}
 }

@inproceedings{Ovadia2019CanYT,
 author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Can you trust your model\textquotesingle s uncertainty?  Evaluating predictive uncertainty under dataset shift},
 url = {https://proceedings.neurips.cc/paper/2019/file/8558cb408c1d76621371888657d2eb1d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{levin1990statistical,
  title={A statistical approach to learning and generalization in layered neural networks},
  author={Levin, Esther and Tishby, Naftali and Solla, Sara A},
  journal={Proceedings of the IEEE},
  volume={78},
  number={10},
  pages={1568--1574},
  year={1990},
  publisher={IEEE}
}

@article{hansen1990neural,
  title={Neural network ensembles},
  author={Hansen, Lars Kai and Salamon, Peter},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={12},
  number={10},
  pages={993--1001},
  year={1990},
  publisher={IEEE}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{karras2017progressive,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}

@article{fort2019deep,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1912.02757},
  year={2019}
}
@article{matena2021merging,
  title={Merging Models with Fisher-Weighted Averaging},
  author={Matena, Michael and Raffel, Colin},
  journal={arXiv preprint arXiv:2111.09832},
  year={2021}
}



@article{zhang2019lookahead,
  title={Lookahead optimizer: k steps forward, 1 step back},
  author={Zhang, Michael and Lucas, James and Ba, Jimmy and Hinton, Geoffrey E},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}


@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020}
}


@article{berger2014kolmogorov,
  title={Kolmogorov--smirnov test: Overview},
  author={Berger, Vance W and Zhou, YanYan},
  journal={Wiley statsref: Statistics reference online},
  year={2014},
  publisher={Wiley Online Library}
}



@article{lee2015m,
  title={Why m heads are better than one: Training a diverse ensemble of deep networks},
  author={Lee, Stefan and Purushwalkam, Senthil and Cogswell, Michael and Crandall, David and Batra, Dhruv},
  journal={arXiv preprint arXiv:1511.06314},
  year={2015}
}

@article{wen2020batchensemble,
  title={Batchensemble: an alternative approach to efficient ensemble and lifelong learning},
  author={Wen, Yeming and Tran, Dustin and Ba, Jimmy},
  journal={arXiv preprint arXiv:2002.06715},
  year={2020}
}


@inproceedings{wortsman2021learning,
  title={Learning neural network subspaces},
  author={Wortsman, Mitchell and Horton, Maxwell C and Guestrin, Carlos and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={11217--11227},
  year={2021},
  organization={PMLR}
}

@article{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Yitzhak and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  journal={arXiv preprint arXiv:2203.05482},
  year={2022}
}



@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}


@inproceedings{quinonero2005evaluating,
  title={Evaluating predictive uncertainty challenge},
  author={Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  booktitle={Machine Learning Challenges Workshop},
  pages={1--27},
  year={2005},
  organization={Springer}
}


@article{fort2019stiffness,
  title={Stiffness: A New Perspective on Generalization in Neural Networks},
  author={Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Narayanan, Srini},
  journal={arXiv preprint arXiv:1901.09491},
  year={2019}
}

@article{nagappan2020neuroregeneration,
  title={Neuroregeneration and plasticity: a review of the physiological mechanisms for achieving functional recovery postinjury},
  author={Nagappan, Palaniappan Ganesh and Chen, Hong and Wang, De-Yun},
  journal={Military Medical Research},
  volume={7},
  number={1},
  pages={1--16},
  year={2020},
  publisher={BioMed Central}
}

@article{liu2021SET,
  title={Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Matavalam, Amarsagar Reddy Ramapuram and Pei, Yulong and Pechenizkiy, Mykola},
  journal={Neural Computing and Applications},
  volume={33},
  number={7},
  pages={2589--2604},
  year={2021},
  publisher={Springer}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},  
  booktitle={Proceedings of the 36th International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@article{liu2022unreasonable,
  title={The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:2202.02643},
  year={2022}
}

@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}

@inproceedings{
lee2019signal,
title={A Signal Propagation Perspective for Pruning Neural Networks at Initialization},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Stephen Gould and Philip H. S. Torr},
booktitle={International Conference on Learning Representations},
year={2020.  arXiv:1906.06307},
url={https://openreview.net/forum?id=HJeTo2VFwH}
}


@article{zhang2019fast,
  title={Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}


@article{Krizhevsky09,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, A. and Hinton, G.},
  journal={Master's thesis, Department of Computer Science, University of Toronto},
  year={2009},
  publisher={Citeseer}
}

@article{atashgahi2020quick,
  title={Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders},
  author={Atashgahi, Zahra and Sokar, Ghada and van der Lee, Tim and Mocanu, Elena and Mocanu, Decebal Constantin and Veldhuis, Raymond and Pechenizkiy, Mykola},
  journal={arXiv:2012.00560},
  year={2020}
}

@article{mocanu2021sparse,
  title={Sparse Training Theory for Scalable and Efficient Agents},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Pinto, Tiago and Curci, Selima and Nguyen, Phuong H and Gibescu, Madeleine and Ernst, Damien and Vale, Zita A},
  journal={International Conference on Autonomous Agents and Multiagent Systems (AAMAS). arXiv:2103.01636},
  year={2021}
}





@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{schwartz2019green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={arXiv preprint arXiv:1907.10597},
  year={2019}
}



@inproceedings{
Lin2020Dynamic,
title={Dynamic Model Pruning with Feedback},
author={Tao Lin and Sebastian U. Stich and Luis Barba and Daniil Dmitriev and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJem8lSFwB}
}





@inproceedings{
zhou2021learning,
title={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},
author={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=K9bw7vqp_s}
}

@inproceedings{dao2022monarch,
  title={Monarch: Expressive structured matrices for efficient and accurate training},
  author={Dao, Tri and Chen, Beidi and Sohoni, Nimit S and Desai, Arjun and Poli, Michael and Grogan, Jessica and Liu, Alexander and Rao, Aniruddh and Rudra, Atri and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={4690--4721},
  year={2022},
  organization={PMLR}
}

@inproceedings{NEURIPS2021_6e8404c3,
 author = {Pool, Jeff and Yu, Chong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {13316--13327},
 publisher = {Curran Associates, Inc.},
 title = {Channel Permutations for N:M Sparsity},
 url = {https://proceedings.neurips.cc/paper/2021/file/6e8404c3b93a9527c8db241a1846599a-Paper.pdf},
 volume = {34},
 year = {2021}
}



@article{nvidia2020,
  title={NVIDIA A100 Tensor Core GPU Architecture},
  author={Nvidia},
  journal={https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf},
  year={2020}
}


@article{gale2020sparse,
  title={Sparse GPU kernels for deep learning},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  journal={arXiv preprint arXiv:2006.10901},
  year={2020}
}

@article{chen2019eyeriss,
  title={Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices},
  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={2},
  pages={292--308},
  year={2019},
  publisher={IEEE}
}

@inproceedings{wang2018snrram,
  title={Snrram: an efficient sparse neural network computation architecture based on resistive random-access memory},
  author={Wang, Peiqi and Ji, Yu and Hong, Chi and Lyu, Yongqiang and Wang, Dongsheng and Xie, Yuan},
  booktitle={2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@article{azarian2021cascade,
  title={Cascade Weight Shedding in Deep Neural Networks: Benefits and Pitfalls for Network Pruning},
  author={Azarian, Kambiz and Porikli, Fatih},
  journal={arXiv preprint arXiv:2103.10629},
  year={2021}
}


@misc{
zhu2017prune,
title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},
author={Michael H. Zhu and Suyog Gupta},
year={2018},
url={https://openreview.net/forum?id=S1lN69AT-},
}

@inproceedings{mittal2018recovering,
  title={Recovering from random pruning: On the plasticity of deep convolutional neural networks},
  author={Mittal, Deepak and Bhardwaj, Shweta and Khapra, Mitesh M and Ravindran, Balaraman},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={848--857},
  year={2018},
  organization={IEEE}
}

@article{hubara2021accelerated,
  title={Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks},
  author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Seffi and Soudry, Daniel},
  journal={arXiv preprint arXiv:2102.08124},
  year={2021}
}


@inproceedings{bartoldson2019generalization,
 author = {Bartoldson, Brian and Morcos, Ari and Barbu, Adrian and Erlebacher, Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {20852--20864},
 publisher = {Curran Associates, Inc.},
 title = {The Generalization-Stability Tradeoff In Neural Network Pruning},
 url = {https://proceedings.neurips.cc/paper/2020/file/ef2ee09ea9551de88bc11fd7eeea93b0-Paper.pdf},
 volume = {33},
 year = {2020}
}



@article{mahar2018intrinsic,
  title={Intrinsic mechanisms of neuronal axon regeneration},
  author={Mahar, Marcus and Cavalli, Valeria},
  journal={Nature Reviews Neuroscience},
  volume={19},
  number={6},
  pages={323--337},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{chen2020long,
  title={Long live the lottery: The existence of winning tickets in lifelong learning},
  author={Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
  booktitle={International Conference on Learning Representations},
  year={2021}
}



@article{yiu2006glial,
  title={Glial inhibition of CNS axon regeneration},
  author={Yiu, Glenn and He, Zhigang},
  journal={Nature Reviews Neuroscience},
  volume={7},
  number={8},
  pages={617--627},
  year={2006},
  publisher={Nature Publishing Group}
}

@book{kandel2000principles,
  title={Principles of neural science},
  author={Kandel, Eric R and Schwartz, James H and Jessell, Thomas M and Siegelbaum, Steven and Hudspeth, A James and Mack, Sarah},
  volume={4},
  year={2000},
  publisher={McGraw-hill New York}
}

@inproceedings{yang2020procrustes,
  title={Procrustes: a dataflow and accelerator for sparse deep neural network training},
  author={Yang, Dingqing and Ghasemazar, Amin and Ren, Xiaowei and Golub, Maximilian and Lemieux, Guy and Lis, Mieszko},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={711--724},
  year={2020},
  organization={IEEE}
}




@InProceedings{verma2021sparsifying,
  title = 	 {Sparsifying Networks via Subdifferential Inclusion},
  author =       {Verma, Sagar and Pesquet, Jean-Christophe},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10542--10552},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/verma21b/verma21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/verma21b.html},
  abstract = 	 {Sparsifying deep neural networks is of paramount interest in many areas, especially when those networks have to be implemented on low-memory devices. In this article, we propose a new formulation of the problem of generating sparse weights for a pre-trained neural network. By leveraging the properties of standard nonlinear activation functions, we show that the problem is equivalent to an approximate subdifferential inclusion problem. The accuracy of the approximation controls the sparsity. We show that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity whose convergence is guaranteed. Because of the algorithm flexibility, the sparsity can be ensured from partial training data in a minibatch manner. To demonstrate the effectiveness of our method, we perform experiments on various networks in different applicative contexts: image classification, speech recognition, natural language processing, and time-series forecasting.}
}


@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}


@inproceedings{liu2021selfish,
  title={Selfish sparse rnn training},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
  booktitle={International Conference on Machine Learning},
  pages={6893--6904},
  year={2021},
  organization={PMLR}
}



@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}







@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1902.06720},
  year={2019}
}

@article{natarajan1995sparse,
  title={Sparse approximate solutions to linear systems},
  author={Natarajan, Balas Kausik},
  journal={SIAM journal on computing},
  volume={24},
  number={2},
  pages={227--234},
  year={1995},
  publisher={SIAM}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on {imagenet} classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{shen2020cpot,
  title={Cpot: Channel pruning via optimal transport},
  author={Shen, Yucong and Shen, Li and Huang, Hao-Zhi and Wang, Xuan and Liu, Wei},
  journal={arXiv preprint arXiv:2005.10451},
  year={2020}
}

@article{an2020real,
  title={Real-time universal style transfer on high-resolution images via zero-channel pruning},
  author={An, Jie and Li, Tao and Huang, Haozhi and Shen, Li and Wang, Xuan and Tang, Yongyi and Ma, Jinwen and Liu, Wei and Luo, Jiebo},
  journal={arXiv preprint arXiv:2006.09029},
  year={2020}
}

@inproceedings{yuan2020block,
  title={A block decomposition algorithm for sparse optimization},
  author={Yuan, Ganzhao and Shen, Li and Zheng, Wei-Shi},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={275--285},
  year={2020}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}



@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex},
  year={2009},
  institution={Citeseer}
}

@inproceedings{zagoruyko2016wide,
  author={Sergey Zagoruyko and Nikos Komodakis},
  title={Wide Residual Networks},
  booktitle={BMVC},
  year={2016}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{neyshabur2018towards,
  title={The role of over-parametrization in generalization of neural networks},
  author={Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=BygfghAcYX},
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{narang2017exploring,
  title={Exploring sparsity in recurrent neural networks},
  author={Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  journal={arXiv preprint arXiv:1704.05119},
  year={2017}
}

@inproceedings{
You:2019tz,
title={Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks},
author={Haoran You and Chaojian Li and Pengfei Xu and Yonggan Fu and Yue Wang and Xiaohan Chen and Richard G. Baraniuk and Zhangyang Wang and Yingyan Lin},
booktitle={8th International Conference on Learning Representations},
year={2020}
}



@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={International Conference on Learning Representations},
  year={2016}
}



@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@inproceedings{mozer1989skeletonization,
  title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
  author={Mozer, Michael C and Smolensky, Paul},
  booktitle={Advances in neural information processing systems},
  pages={107--115},
  year={1989}
}


@inproceedings{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4857--4867},
  year={2017}
}

@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5058--5066},
  year={2017}
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1389--1397},
  year={2017}
}

@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={784--800},
  year={2018}
}


@inproceedings{huang2018data,
  title={Data-driven sparse structure selection for deep neural networks},
  author={Huang, Zehao and Wang, Naiyan},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={304--320},
  year={2018}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the {Hessian}},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{zhang2018noisy,
  title={Noisy Natural Gradient as Variational Inference},
  author={Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
  booktitle={International Conference on Machine Learning},
  pages={5847--5856},
  year={2018}
}


@InProceedings{wang2019eigen,
  title = 	 {{E}igen{D}amage: Structured Pruning in the {K}ronecker-Factored Eigenbasis},
  author = 	 {Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6566--6575},
  year = 	 {2019},
  volume = 	 {97},
  publisher = {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wang19g/wang19g.pdf},
  url = 	 {http://proceedings.mlr.press/v97/wang19g.html},
}



@article{frankle2019lottery,
  title={The Lottery Ticket Hypothesis at Scale},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@inproceedings{lin2017runtime,
  title={Runtime neural pruning},
  author={Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2181--2191},
  year={2017}
}

@inproceedings{wang2018skipnet,
  title={Skipnet: Learning dynamic routing in convolutional networks},
  author={Wang, Xin and Yu, Fisher and Dou, Zi-Yi and Darrell, Trevor and Gonzalez, Joseph E},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={409--424},
  year={2018}
}


@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2017-12-14T14:05:27.000+0100},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/slicside},
  description = {Nur für Referenzzwecke verwendet (MNIST)},
  groups = {public},}

@article{louizos2017learning,
  title={Learning Sparse Neural Networks through $ L\_0 $ Regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal={International Conference on Learning Representations},
  year={2018}
}




@inproceedings{touvron2020fixing,
 author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and Jegou, Herve},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fixing the train-test resolution discrepancy},
 url = {https://proceedings.neurips.cc/paper/2019/file/d03a857a23b5285736c4d55e0bb067c8-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{janowsky1989pruning,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, Steven A},
  journal={Physical Review A},
  volume={39},
  number={12},
  pages={6600},
  year={1989},
  publisher={APS}
}



@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={138--145},
  year={2017}
}

@inproceedings{
frankle2020pruning,
title={Pruning Neural Networks at Initialization: Why Are We Missing the Mark?},
author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel Roy and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2021. arXiv:2009.08576},
url={https://openreview.net/forum?id=Ig-VyQc-MLK}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems. arXiv:2006.05467},
  year={2020}
}

@inproceedings{
de2020progressive,
title={Progressive Skeletonization: Trimming more fat from a network at initialization},
author={Pau de Jorge and Amartya Sanyal and Harkirat Behl and Philip Torr and Gr{\'e}gory Rogez and Puneet K. Dokania},
booktitle={International Conference on Learning Representations},
year={2021. arXiv:cs.CV/2006.09081},
url={https://openreview.net/forum?id=9GsFOUyUPi}
}



@article{verdenius2020pruning,
  title={Pruning via iterative ranking of sensitivity statistics},
  author={Verdenius, Stijn and Stol, Maarten and Forr{\'e}, Patrick},
  journal={arXiv preprint arXiv:2006.00896},
  year={2020}
}

@article{xiao2019autoprune,
  title={Autoprune: Automatic network pruning by regularizing auxiliary parameters},
  author={Xiao, Xia and Wang, Zigeng and Rajasekaran, Sanguthevar},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{savarese2019winning,
  title={Winning the Lottery with Continuous Sparsification},
  author={Savarese, Pedro and Silva, Hugo and Maire, Michael},
  journal={arXiv preprint arXiv:1912.04427},
  year={2019}
}

@inproceedings{
LIU2020Dynamic,
title={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},
author={Junjie Liu and Zhe Xu and Runbin Shi and Ray C. C. Cheung and Hayden K.H. So},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJlbGJrtDB}
}

@inproceedings{NIPS2016_41bfd20a,
 author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Structured Sparsity in Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2016/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf},
 volume = {29},
 year = {2016}
}


@article{srinivas2016generalized,
  title={Generalized dropout},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1611.06791},
  year={2016}
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}


@article{gebhart2021unified,
  title={A Unified Paths Perspective for Pruning at Initialization},
  author={Gebhart, Thomas and Saxena, Udit and Schrater, Paul},
  journal={arXiv preprint arXiv:2101.10552},
  year={2021}
}

@inproceedings{liu2020finding,
  title={Finding trainable sparse networks through Neural Tangent Transfer},
  author={Liu, Tianlin and Zenke, Friedemann},
  booktitle={International Conference on Machine Learning},
  pages={6336--6347},
  year={2020},
  organization={PMLR}
}




@article{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  journal={International Conference on Machine Learning},
  year={2019}
}
@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={arXiv:1707.04780. Nature communications.},
  volume={9},
  number={1},
  pages={2383},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{shawahna2018fpga,
  title={Fpga-based accelerators of deep learning networks for learning and classification: A review},
  author={Shawahna, Ahmad and Sait, Sadiq M and El-Maleh, Aiman},
  journal={IEEE Access},
  volume={7},
  pages={7823--7859},
  year={2018},
  publisher={IEEE}
}

@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle={International Conference on Machine Learning},
  pages={5544--5555},
  year={2020},
  organization={PMLR}
}

@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={International Conference on Machine Learning},
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{xu2018trained,
  title={Trained rank pruning for efficient deep neural networks},
  author={Xu, Yuhui and Li, Yuxi and Zhang, Shuai and Wen, Wei and Wang, Botao and Qi, Yingyong and Chen, Yiran and Lin, Weiyao and Xiong, Hongkai},
  journal={arXiv preprint arXiv:1812.02402},
  year={2018}
}

@article{guo2018network,
  title={Network decoupling: From regular to depthwise separable convolutions},
  author={Guo, Jianbo and Li, Yuxi and Lin, Weiyao and Chen, Yurong and Li, Jianguo},
  journal={arXiv preprint arXiv:1808.05517},
  year={2018}
}

@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}

@article{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  journal={Twenty-eighth Conference on Neural Information Processing Systems. arXiv:1404.0736},
  year={2014}
}

@article{singh2020woodfisher,
  title={WoodFisher: Efficient Second-Order Approximation for Neural Network Compression},
  author={Singh, Sidak Pal and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@book{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  year={1993},
  publisher={Morgan Kaufmann}
}

@inproceedings{strom2015scalable,
  title={Scalable distributed DNN training using commodity GPU cloud computing},
  author={Strom, Nikko},
  booktitle={Sixteenth Annual Conference of the International Speech Communication Association},
  year={2015}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11264--11272},
  year={2019}
}

@article{evci2019difficulty,
  title={The Difficulty of Training Sparse Neural Networks},
  author={Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
  journal={arXiv preprint arXiv:1906.10732},
  year={2019}
}

@article{schraudolph2002fast,
  title={Fast curvature matrix-vector products for second-order gradient descent},
  author={Schraudolph, Nicol N},
  journal={Neural computation},
  volume={14},
  number={7},
  pages={1723--1738},
  year={2002},
  publisher={MIT Press}
}

@article{zhang2019all,
  title={Are All Layers Created Equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}

@inproceedings{han2016eie,
  title={EIE: efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  pages={243--254},
  year={2016},
  organization={IEEE}
}

@article{dey2019pre,
  title={Pre-Defined Sparse Neural Networks with Hardware Acceleration},
  author={Dey, Sourya and Huang, Kuan-Wen and Beerel, Peter A and Chugg, Keith M},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  year={2019},
  publisher={IEEE}
}


@inproceedings{
Wang2020Picking,
title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgsACVKPH}
}


@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@inproceedings{xiao2018dynamical,
  title={Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks},
  author={Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={5393--5402},
  year={2018}
}

@inproceedings{yang2017mean,
  title={Mean field residual networks: On the edge of chaos},
  author={Yang, Ge and Schoenholz, Samuel},
  booktitle={Advances in neural information processing systems},
  pages={7103--7114},
  year={2017}
}

@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances in neural information processing systems},
  pages={3360--3368},
  year={2016}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}


@article{sokar2021spacenet,
  title={Spacenet: Make free space for continual learning},
  author={Sokar, Ghada and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  journal={Neurocomputing},
  volume={439},
  pages={1--11},
  year={2021},
  publisher={Elsevier}
}

@article{bibikar2021federated,
  title={Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better},
  author={Bibikar, Sameer and Vikalo, Haris and Wang, Zhangyang and Chen, Xiaohan},
  journal={arXiv preprint arXiv:2112.09824},
  year={2021}
}

@article{zhu2019multi,
  title={Multi-objective evolutionary federated learning},
  author={Zhu, Hangyu and Jin, Yaochu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={4},
  pages={1310--1322},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ozdenizci2021training,
  title={Training adversarially robust sparse networks via Bayesian connectivity sampling},
  author={{\"O}zdenizci, Ozan and Legenstein, Robert},
  booktitle={International Conference on Machine Learning},
  pages={8314--8324},
  year={2021},
  organization={PMLR}
}
@misc{cuda,
  author={NVIDIA and Vingelmann, Péter and Fitzek, Frank H.P.},
  title={CUDA, release: 10.2.89},
  year={2020},
  url={https://developer.nvidia.com/cuda-toolkit},
}

@article{24gotErdos1959,
  author =        {Erd{\H{o}}s, Paul and R{\'e}nyi, Alfr{\'e}d},
  journal =       {Publicationes Mathematicae (Debrecen)},
  pages =         {290-297},
  title =         {On Random Graphs I.},
  volume =        {6},
  year =          {1959},
}

@article{liu2021sparse,
  title={Sparse training via boosting pruning plasticity with neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9908--9922},
  year={2021}
}


@inproceedings{liu2020topological,
  title={Topological insights into sparse neural networks},
  author={Liu, Shiwei and der Lee, Tim Van and Yaman, Anil and Atashgahi, Zahra and Ferraro, Davide and Sokar, Ghada and Pechenizkiy, Mykola and Mocanu, Decebal Constantin},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  pages={279--294},
  year={2020},
  organization={Springer}
}


@inproceedings{
huang2022fedspa,
title={On Heterogeneously Distributed Data, Sparsity Matters},
author={Huang, Tiansheng and Liu, Shiwei and Shen, L and He,  Fengxiang and Lin, Weiwei and Tao, Dacheng},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=AT0K-SZ3QGq},
}

@inproceedings{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={8803--8812},
  year={2018}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{fort2019large,
  title={Large scale structure of neural network loss landscapes},
  author={Fort, Stanislav and Jastrzebski, Stanislaw},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={6709--6717},
  year={2019}
}

@inproceedings{prabhu2018deep,
  title={Deep expander networks: Efficient deep networks from graph theory},
  author={Prabhu, Ameya and Varma, Girish and Namboodiri, Anoop},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={20--35},
  year={2018}
}

@inproceedings{kepner2019radix,
  title={Radix-net: Structured sparse matrices for deep neural networks},
  author={Kepner, Jeremy and Robinett, Ryan},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={268--274},
  year={2019},
  organization={IEEE}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={2017 IEEE winter conference on applications of computer vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}


@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{cheung2019superposition,
  title={Superposition of many models into one},
  author={Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  journal={arXiv preprint arXiv:1902.05522},
  year={2019}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{karras2017progressive,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}





@article{huang2017snapshot,
  title={Snapshot ensembles: Train 1, get m for free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:1704.00109},
  year={2017}
}



@article{yuan2021mest,
  title={Mest: Accurate and fast memory-economic sparse training framework on the edge},
  author={Yuan, Geng and Ma, Xiaolong and Niu, Wei and Li, Zhengang and Kong, Zhenglun and Liu, Ning and Gong, Yifan and Zhan, Zheng and He, Chaoyang and Jin, Qing and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20838--20850},
  year={2021}
}


@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}


@inproceedings{quinonero2005evaluating,
  title={Evaluating predictive uncertainty challenge},
  author={Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Sinz, Fabian and Bousquet, Olivier and Sch{\"o}lkopf, Bernhard},
  booktitle={Machine Learning Challenges Workshop},
  pages={1--27},
  year={2005},
  organization={Springer}
}


@article{fort2019stiffness,
  title={Stiffness: A New Perspective on Generalization in Neural Networks},
  author={Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Narayanan, Srini},
  journal={arXiv preprint arXiv:1901.09491},
  year={2019}
}


@article{nagappan2020neuroregeneration,
  title={Neuroregeneration and plasticity: a review of the physiological mechanisms for achieving functional recovery postinjury},
  author={Nagappan, Palaniappan Ganesh and Chen, Hong and Wang, De-Yun},
  journal={Military Medical Research},
  volume={7},
  number={1},
  pages={1--16},
  year={2020},
  publisher={BioMed Central}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},  
  booktitle={Proceedings of the 36th International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}


@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={International Conference on Learning Representations},
  year={2019}
}


@misc{
zeng2019mlprune,
title={{MLP}rune: Multi-Layer Pruning for Automated Neural Network Compression},
author={Wenyuan Zeng and Raquel Urtasun},
year={2019},
url={https://openreview.net/forum?id=r1g5b2RcKm},
}




@article{hoefler2021sparsity,
  author  = {Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
  title   = {Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {241},
  pages   = {1-124},
  url     = {http://jmlr.org/papers/v22/21-0366.html}
}



@article{garcia2019estimation,
  title={Estimation of energy consumption in machine learning},
  author={Garc{\'\i}a-Mart{\'\i}n, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, H{\aa}kan},
  journal={Journal of Parallel and Distributed Computing},
  volume={134},
  pages={75--88},
  year={2019},
  publisher={Elsevier}
}


@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}


@article{chen2020earlybert,
  title={Earlybert: Efficient bert training via early-bird lottery tickets},
  author={Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},
  journal={ACL-IJCNLP},
  year={2021}
}

@inproceedings{chen2020lottery,
 author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {15834--15846},
 publisher = {Curran Associates, Inc.},
 title = {The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf},
 volume = {33},
 year = {2020}
}



@InProceedings{chen2020lottery2,
    author    = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Carbin, Michael and Wang, Zhangyang},
    title     = {The Lottery Tickets Hypothesis for Supervised and Self-Supervised Pre-Training in Computer Vision Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {16306-16316}
}




@inproceedings{wortsman2019discovering,
 author = {Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Discovering Neural Wirings},
 url = {https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf},
 volume = {32},
 year = {2019}
}




@inproceedings{liu2021we,
 author = {Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
 booktitle = {Proceedings of the 39th International Conference on Machine Learning},
 pages = {6989--7000},
 title = {Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training},
 year = {2021},
 organization={PMLR}
}





@inproceedings{
renda2020comparing,
title={Comparing Rewinding and Fine-tuning in Neural Network Pruning},
author={Alex Renda and Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2020. arXiv:2003.02389},
url={https://openreview.net/forum?id=S1gSj0NKvB}
}


@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}


@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={International Conference on Learning Representations},
  year={2014}
}




@inproceedings{
frankle2018lottery,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019. arXiv:1803.03635},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}



@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations},
  year={2015}
}


@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}



@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2736--2744},
  year={2017}
}






@inproceedings{
bellec2018deep,
title={Deep Rewiring: Training very sparse deep networks},
author={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJ_wN01C-},
}



@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}

@article{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  journal={Twenty-eighth Conference on Neural Information Processing Systems. arXiv:1404.0736},
  year={2014}
}

@article{singh2020woodfisher,
  title={WoodFisher: Efficient Second-Order Approximation for Neural Network Compression},
  author={Singh, Sidak Pal and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@book{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  year={1993},
  publisher={Morgan Kaufmann}
}

@inproceedings{strom2015scalable,
  title={Scalable distributed DNN training using commodity GPU cloud computing},
  author={Strom, Nikko},
  booktitle={Sixteenth Annual Conference of the International Speech Communication Association},
  year={2015}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11264--11272},
  year={2019}
}

@article{evci2019difficulty,
  title={The Difficulty of Training Sparse Neural Networks},
  author={Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
  journal={arXiv preprint arXiv:1906.10732},
  year={2019}
}

@article{schraudolph2002fast,
  title={Fast curvature matrix-vector products for second-order gradient descent},
  author={Schraudolph, Nicol N},
  journal={Neural computation},
  volume={14},
  number={7},
  pages={1723--1738},
  year={2002},
  publisher={MIT Press}
}

@article{zhang2019all,
  title={Are All Layers Created Equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}

@inproceedings{han2016eie,
  title={EIE: efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  pages={243--254},
  year={2016},
  organization={IEEE}
}

@article{dey2019pre,
  title={Pre-Defined Sparse Neural Networks with Hardware Acceleration},
  author={Dey, Sourya and Huang, Kuan-Wen and Beerel, Peter A and Chugg, Keith M},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  year={2019},
  publisher={IEEE}
}




@article{dettmers2019sparse,
  title={Sparse Networks from Scratch: Faster Training without Losing Performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@inproceedings{xiao2018dynamical,
  title={Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks},
  author={Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={5393--5402},
  year={2018}
}

@inproceedings{yang2017mean,
  title={Mean field residual networks: On the edge of chaos},
  author={Yang, Ge and Schoenholz, Samuel},
  booktitle={Advances in neural information processing systems},
  pages={7103--7114},
  year={2017}
}

@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances in neural information processing systems},
  pages={3360--3368},
  year={2016}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}



@article{sokar2021spacenet,
  title={Spacenet: Make free space for continual learning},
  author={Sokar, Ghada and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  journal={Neurocomputing},
  volume={439},
  pages={1--11},
  year={2021},
  publisher={Elsevier}
}

@article{bibikar2021federated,
  title={Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better},
  author={Bibikar, Sameer and Vikalo, Haris and Wang, Zhangyang and Chen, Xiaohan},
  journal={arXiv preprint arXiv:2112.09824},
  year={2021}
}


@inproceedings{ozdenizci2021training,
  title={Training adversarially robust sparse networks via Bayesian connectivity sampling},
  author={{\"O}zdenizci, Ozan and Legenstein, Robert},
  booktitle={International Conference on Machine Learning},
  pages={8314--8324},
  year={2021},
  organization={PMLR}
}

@article{liu2021gan,
  title={Sparse Unbalanced GAN Training with In-Time Over-Parameterization},
  author={Liu, Shiwei and Tian, Yuesong and Chen, Tianlong and Shen, Li},
  year={2021}
}

@article{24gotErdos1959,
  author =        {Erd{\H{o}}s, Paul and R{\'e}nyi, Alfr{\'e}d},
  journal =       {Publicationes Mathematicae (Debrecen)},
  pages =         {290-297},
  title =         {On Random Graphs I.},
  volume =        {6},
  year =          {1959},
}



@inproceedings{liu2020topological,
  title={Topological Insights into Sparse Neural Networks},
  author={Liu, Shiwei and van der Lee, TT and Yaman, Anil and Atashgahi, Zahra and Ferrar, D and Sokar, Ghada and Pechenizkiy, Mykola and Mocanu, DC},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  year={2020}
}


@inproceedings{mariet2015diversity,
  title={Diversity networks: Neural network compression using determinantal point processes},
  author={Mariet, Zelda and Sra, Suvrit},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@inproceedings{
huang2022fedspa,
title={On Heterogeneously Distributed Data, Sparsity Matters},
author={Huang, Tiansheng and Liu, Shiwei and Shen, L and He,  Fengxiang and Lin, Weiwei and Tao, Dacheng},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=AT0K-SZ3QGq},
}

@inproceedings{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={8803--8812},
  year={2018}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{fort2019large,
  title={Large scale structure of neural network loss landscapes},
  author={Fort, Stanislav and Jastrzebski, Stanislaw},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={6709--6717},
  year={2019}
}

@article{evci2020gradient,
  title={Gradient flow in sparse neural networks and how lottery tickets win},
  author={Evci, Utku and Ioannou, Yani A and Keskin, Cem and Dauphin, Yann},
  journal={arXiv preprint arXiv:2010.03533},
  year={2020}
}

@inproceedings{prabhu2018deep,
  title={Deep expander networks: Efficient deep networks from graph theory},
  author={Prabhu, Ameya and Varma, Girish and Namboodiri, Anoop},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={20--35},
  year={2018}
}

@inproceedings{kepner2019radix,
  title={Radix-net: Structured sparse matrices for deep neural networks},
  author={Kepner, Jeremy and Robinett, Ryan},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={268--274},
  year={2019},
  organization={IEEE}
}

@inproceedings{smith2017cyclical,
  title={Cyclical learning rates for training neural networks},
  author={Smith, Leslie N},
  booktitle={2017 IEEE winter conference on applications of computer vision (WACV)},
  pages={464--472},
  year={2017},
  organization={IEEE}
}


@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{cheung2019superposition,
  title={Superposition of many models into one},
  author={Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  journal={arXiv preprint arXiv:1902.05522},
  year={2019}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{karras2017progressive,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}

@article{jia2019dissecting,
  title={Dissecting the graphcore ipu architecture via microbenchmarking},
  author={Jia, Zhe and Tillman, Blake and Maggioni, Marco and Scarpazza, Daniele Paolo},
  journal={arXiv preprint arXiv:1912.03413},
  year={2019}
}

@InProceedings{
    pmlr-v119-kurtz20a, 
    title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks}, 
    author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan}, 
    booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
    pages = {5533--5543}, 
    year = {2020}, 
    editor = {Hal Daumé III and Aarti Singh}, 
    volume = {119}, 
    series = {Proceedings of Machine Learning Research}, 
    address = {Virtual}, 
    month = {13--18 Jul}, 
    publisher = {PMLR}, 
    pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},
    url = {http://proceedings.mlr.press/v119/kurtz20a.html}
}


@article{dietrich2021towards,
  title={Towards Structured Dynamic Sparse Pre-Training of BERT},
  author={Dietrich, Anastasia and Gressmann, Frithjof and Orr, Douglas and Chelombiev, Ivan and Justus, Daniel and Luschi, Carlo},
  journal={arXiv preprint arXiv:2108.06277},
  year={2021}
}



@article{huang2017snapshot,
  title={Snapshot ensembles: Train 1, get m for free},
  author={Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:1704.00109},
  year={2017}
}


@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@article{sung2021training,
  title={Training neural networks with fixed sparse masks},
  author={Sung, Yi-Lin and Nair, Varun and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24193--24205},
  year={2021}
}

@article{evci2022gradmax,
  title={Gradmax: Growing neural networks using gradient information},
  author={Evci, Utku and Vladymyrov, Max and Unterthiner, Thomas and van Merri{\"e}nboer, Bart and Pedregosa, Fabian},
  journal={arXiv preprint arXiv:2201.05125},
  year={2022}
}




@inproceedings{he2019filter,
  title     = {Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration},
  author    = {He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019}
}

@article{he2018soft,
  title={Soft filter pruning for accelerating deep convolutional neural networks},
  author={He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
  journal={arXiv preprint arXiv:1808.06866},
  year={2018}
}

@inproceedings{yang2018netadapt,
  title={Netadapt: Platform-aware neural network adaptation for mobile applications},
  author={Yang, Tien-Ju and Howard, Andrew and Chen, Bo and Zhang, Xiao and Go, Alec and Sandler, Mark and Sze, Vivienne and Adam, Hartwig},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={285--300},
  year={2018}
}

@article{chin2019legr,
  title={Legr: Filter pruning via learned global ranking},
  author={Chin, Ting-Wu and Ding, Ruizhou and Zhang, Cha and Marculescu, Diana},
  year={2019}
}

@inproceedings{he2020learning,
  title={Learning filter pruning criteria for deep convolutional neural networks acceleration},
  author={He, Yang and Ding, Yuhang and Liu, Ping and Zhu, Linchao and Zhang, Hanwang and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2009--2018},
  year={2020}
}

@inproceedings{yu2018nisp,
  title={Nisp: Pruning networks using neuron importance score propagation},
  author={Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={9194--9203},
  year={2018}
}



@inproceedings{hou2020efficient,
  title={Efficient image super resolution via channel discriminative deep neural network pruning},
  author={Hou, Zejiang and Kung, Sun-Yuan},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3647--3651},
  year={2020},
  organization={IEEE}
}

@article{hou2020feature,
  title={A feature-map discriminant perspective for pruning deep neural networks},
  author={Hou, Zejiang and Kung, Sun-Yuan},
  journal={arXiv preprint arXiv:2005.13796},
  year={2020}
}

@inproceedings{ye2020good,
  title={Good subnetworks provably exist: Pruning via greedy forward selection},
  author={Ye, Mao and Gong, Chengyue and Nie, Lizhen and Zhou, Denny and Klivans, Adam and Liu, Qiang},
  booktitle={International Conference on Machine Learning},
  pages={10820--10830},
  year={2020},
  organization={PMLR}
}
@article{kung2020augment,
  title={Augment deep BP-parameter learning with local XAI-structural learning},
  author={Kung, Sun-Yuan and Hou, Zejiang},
  journal={Communications in Information and Systems},
  volume={20},
  number={3},
  pages={319--352},
  year={2020},
  publisher={International Press of Boston}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{schwarz2021powerpropagation,
  title={Powerpropagation: A sparsity inducing weight reparameterisation},
  author={Schwarz, Jonathan and Jayakumar, Siddhant and Pascanu, Razvan and Latham, Peter E and Teh, Yee},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28889--28903},
  year={2021}
}

@article{peste2021ac,
  title={Ac/dc: Alternating compressed/decompressed training of deep neural networks},
  author={Peste, Alexandra and Iofinova, Eugenia and Vladu, Adrian and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8557--8570},
  year={2021}
}



@article{jayakumar2020top,
  title={Top-kast: Top-k always sparse training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20744--20754},
  year={2020}
}



@inproceedings{elsen2020fast,
  title={Fast sparse convnets},
  author={Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14629--14638},
  year={2020}
}

@article{dietrichdynamic,
  title={Dynamic Sparse Pre-Training of BERT},
  author={Dietrich, Anastasia SD and Gressmann, Frithjof and Orr, Douglas and Chelombiev, Ivan and Justus, Daniel and Luschi, Carlo}
}



@inproceedings{iofinova2022well,
  title={How Well Do Sparse ImageNet Models Transfer?},
  author={Iofinova, Eugenia and Peste, Alexandra and Kurtz, Mark and Alistarh, Dan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12266--12276},
  year={2022}
}

@inproceedings{liu2021group,
  title={Group fisher pruning for practical network compression},
  author={Liu, Liyang and Zhang, Shilong and Kuang, Zhanghui and Zhou, Aojun and Xue, Jing-Hao and Wang, Xinjiang and Chen, Yimin and Yang, Wenming and Liao, Qingmin and Zhang, Wayne},
  booktitle={International Conference on Machine Learning},
  pages={7021--7032},
  year={2021},
  organization={PMLR}
}






@article{tang2020scop,
  title={Scop: Scientific control for reliable neural network pruning},
  author={Tang, Yehui and Wang, Yunhe and Xu, Yixing and Tao, Dacheng and Xu, Chunjing and Xu, Chao and Xu, Chang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={10936--10947},
  year={2020}
}
@inproceedings{lin2020hrank,
  title={Hrank: Filter pruning using high-rank feature map},
  author={Lin, Mingbao and Ji, Rongrong and Wang, Yan and Zhang, Yichen and Zhang, Baochang and Tian, Yonghong and Shao, Ling},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1529--1538},
  year={2020}
}
@article{sui2021chip,
  title={CHIP: CHannel independence-based pruning for compact neural networks},
  author={Sui, Yang and Yin, Miao and Xie, Yi and Phan, Huy and Aliari Zonouz, Saman and Yuan, Bo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24604--24616},
  year={2021}
}
