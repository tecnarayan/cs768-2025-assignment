\begin{thebibliography}{10}

\bibitem{akyurek2023what}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{arora2009computational}
Sanjeev Arora and Boaz Barak.
\newblock {\em Computational complexity: a modern approach}.
\newblock Cambridge University Press, 2009.

\bibitem{bai2023transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with
  in-context algorithm selection.
\newblock {\em arXiv preprint arXiv:2306.04637}, 2023.

\bibitem{barrington1986bounded}
David~A Barrington.
\newblock Bounded-width polynomial-size branching programs recognize exactly
  those languages in nc.
\newblock In {\em Proceedings of the eighteenth annual ACM symposium on Theory
  of computing}, pages 1--5, 1986.

\bibitem{barrington1988finite}
David A~Mix Barrington and Denis Therien.
\newblock Finite monoids and the fine structure of nc.
\newblock {\em Journal of the ACM (JACM)}, 35(4):941--952, 1988.

\bibitem{bellman1954theory}
Richard Bellman.
\newblock The theory of dynamic programming.
\newblock {\em Bulletin of the American Mathematical Society}, 60(6):503--515,
  1954.

\bibitem{bhattamishra2020ability}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the ability and limitations of transformers to recognize formal
  languages.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 7096--7116, 2020.

\bibitem{bhattamishra2020computational}
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
\newblock On the computational power of transformers and its implications in
  sequence modeling.
\newblock In {\em Proceedings of the 24th Conference on Computational Natural
  Language Learning}, pages 455--475, 2020.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in neural information processing systems},
  volume~33, pages 1877--1901, 2020.

\bibitem{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  {GPT}-4.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{burns2022discovering}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock Discovering latent knowledge in language models without supervision.
\newblock {\em arXiv preprint arXiv:2212.03827}, 2022.

\bibitem{buss1987boolean}
Samuel~R Buss.
\newblock The boolean formula value problem is in alogtime.
\newblock In {\em Proceedings of the nineteenth annual ACM symposium on Theory
  of computing}, pages 123--131, 1987.

\bibitem{chan2022data}
Stephanie~C.Y. Chan, Adam Santoro, Andrew~Kyle Lampinen, Jane~X Wang, Aaditya~K
  Singh, Pierre~Harvey Richemond, James McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{david2023tighter}
David Chiang, Peter Cholak, and Anand Pillay.
\newblock Tighter bounds on the expressivity of transformer encoders.
\newblock In {\em Proceedings of the 40th International Conference on Machine
  Learning}, pages 5544--5562, 2023.

\bibitem{chiu2001division}
Andrew Chiu, George Davida, and Bruce Litow.
\newblock Division in logspace-uniform nc1.
\newblock {\em RAIRO-Theoretical Informatics and Applications}, 35(3):259--275,
  2001.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{cormen2022introduction}
Thomas~H Cormen, Charles~E Leiserson, Ronald~L Rivest, and Clifford Stein.
\newblock {\em Introduction to algorithms}.
\newblock MIT press, 2022.

\bibitem{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of control, signals and systems}, 2(4):303--314,
  1989.

\bibitem{dai2022can}
Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Zhifang Sui, and Furu Wei.
\newblock Why can gpt learn in-context? language models secretly perform
  gradient descent as meta optimizers.
\newblock {\em arXiv preprint arXiv:2212.10559}, 2022.

\bibitem{dehghani2019universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock Universal transformers.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186.
  Association for Computational Linguistics, 2019.

\bibitem{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In {\em International Conference on Machine Learning}, pages
  5793--5831. PMLR, 2022.

\bibitem{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock {\em Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem{garg2022what}
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{giannou2023looped}
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason~D Lee, and
  Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock {\em arXiv preprint arXiv:2301.13196}, 2023.

\bibitem{hahn2020theoretical}
Michael Hahn.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock {\em Transactions of the Association for Computational Linguistics},
  8:156--171, 2020.

\bibitem{hao2022formal}
Yiding Hao, Dana Angluin, and Robert Frank.
\newblock Formal language recognition by hard attention transformers:
  Perspectives from circuit complexity.
\newblock {\em Transactions of the Association for Computational Linguistics},
  10:800--810, 2022.

\bibitem{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{hesse2001division}
William Hesse.
\newblock Division is in uniform tc0.
\newblock In {\em International Colloquium on Automata, Languages, and
  Programming}, pages 104--114. Springer, 2001.

\bibitem{hewitt2020rnns}
John Hewitt, Michael Hahn, Surya Ganguli, Percy Liang, and Christopher~D
  Manning.
\newblock Rnns can generate bounded hierarchical languages with optimal memory.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 1978--2010, 2020.

\bibitem{ieee754}
{IEEE Computer Society}.
\newblock Ieee standard for floating-point arithmetic.
\newblock {\em {IEEE Std 754-2019}}, 2019.

\bibitem{jiang2020can}
Zhengbao Jiang, Frank~F Xu, Jun Araki, and Graham Neubig.
\newblock How can we know what language models know?
\newblock {\em Transactions of the Association for Computational Linguistics},
  8:423--438, 2020.

\bibitem{jones1974complete}
Neil~D Jones and William~T Laaser.
\newblock Complete problems for deterministic polynomial time.
\newblock In {\em Proceedings of the sixth annual ACM symposium on Theory of
  computing}, pages 40--46, 1974.

\bibitem{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{leshno1993multilayer}
Moshe Leshno, Vladimir~Ya Lin, Allan Pinkus, and Shimon Schocken.
\newblock Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
\newblock {\em Neural networks}, 6(6):861--867, 1993.

\bibitem{lightman2023lets}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock {\em arXiv preprint arXiv:2305.20050}, 2023.

\bibitem{liu2023transformers}
Bingbin Liu, Jordan~T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 55(9):1--35, 2023.

\bibitem{loshchilov2017fixing}
Ilya Loshchilov and Frank Hutter.
\newblock Fixing weight decay regularization in adam.
\newblock 2017.

\bibitem{lu2017expressive}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{luo2022your}
Shengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di~He.
\newblock Your transformer may not be as powerful as you expect.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{merrill2023parallelism}
William Merrill and Ashish Sabharwal.
\newblock The parallelism tradeoff: Limitations of log-precision transformers.
\newblock {\em Transactions of the Association for Computational Linguistics},
  2023.

\bibitem{merrill2022saturated}
William Merrill, Ashish Sabharwal, and Noah~A Smith.
\newblock Saturated transformers are constant-depth threshold circuits.
\newblock {\em Transactions of the Association for Computational Linguistics},
  10:843--856, 2022.

\bibitem{nye2022show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, Charles Sutton, and Augustus Odena.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock In {\em Deep Learning for Code Workshop}, 2022.

\bibitem{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock {\em Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{perez2021attention}
Jorge P{\'e}rez, Pablo Barcel{\'o}, and Javier Marinkovic.
\newblock Attention is turing complete.
\newblock {\em The Journal of Machine Learning Research}, 22(1):3463--3497,
  2021.

\bibitem{press2022train}
Ofir Press, Noah Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input
  length extrapolation.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{pérez2019on}
Jorge Pérez, Javier Marinković, and Pablo Barceló.
\newblock On the turing completeness of modern neural network architectures.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock {\em arXiv preprint arXiv:2112.11446}, 2021.

\bibitem{sakai1961syntax}
Itiroo Sakai.
\newblock Syntax in universal translation.
\newblock In {\em Proceedings of the International Conference on Machine
  Translation and Applied Language Analysis}, 1961.

\bibitem{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock {\em arXiv preprint arXiv:2211.05100}, 2022.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock {\em arXiv preprint arXiv:2210.09261}, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems},
  volume~30, 2017.

\bibitem{von2022transformers}
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock {\em arXiv preprint arXiv:2212.07677}, 2022.

\bibitem{wei2022statistically}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35, pages 12071--12083, 2022.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
  Ed~H. Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{weiss2021thinking}
Gail Weiss, Yoav Goldberg, and Eran Yahav.
\newblock Thinking like transformers.
\newblock In {\em International Conference on Machine Learning}, pages
  11080--11090. PMLR, 2021.

\bibitem{wies2023subtask}
Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock Sub-task decomposition enables learning in sequence to sequence
  tasks.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{xie2022an}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{yao1989circuits}
Andrew~C Yao.
\newblock Circuits and local computation.
\newblock In {\em Proceedings of the twenty-first annual ACM symposium on
  Theory of computing}, pages 186--196, 1989.

\bibitem{yao2021self}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 3770--3785, 2021.

\bibitem{yun2020are}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{yun2020n}
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank
  Reddi, and Sanjiv Kumar.
\newblock O (n) connections are expressive enough: Universal approximability of
  sparse transformers.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 13783--13794, 2020.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhang2023automatic}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\end{thebibliography}
