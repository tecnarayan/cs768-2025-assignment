\begin{thebibliography}{}

\bibitem[Antoniou et~al., 2018]{antoniou2018data}
Antoniou, A., Storkey, A., and Edwards, H. (2018).
\newblock Augmenting image classifiers using data augmentation generative
  adversarial networks.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 594--603. Springer.

\bibitem[Arjovsky and Bottou, 2017]{arjovsky2017towards}
Arjovsky, M. and Bottou, L. (2017).
\newblock Towards principled methods for training generative adversarial
  networks.
\newblock {\em stat}, 1050:17.

\bibitem[Arjovsky et~al., 2017]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L. (2017).
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International Conference on Machine Learning}, pages
  214--223. PMLR.

\bibitem[Behrmann et~al., 2021]{behrmann2021understanding}
Behrmann, J., Vicol, P., Wang, K.-C., Grosse, R., and Jacobsen, J.-H. (2021).
\newblock Understanding and mitigating exploding inverses in invertible neural
  networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1792--1800. PMLR.

\bibitem[Blei et~al., 2017]{blei2017variational}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D. (2017).
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American Statistical Association},
  112(518):859--877.

\bibitem[Brock et~al., 2018]{brock2018large}
Brock, A., Donahue, J., and Simonyan, K. (2018).
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Cornish et~al., 2020]{cornish2020relaxing}
Cornish, R., Caterini, A., Deligiannidis, G., and Doucet, A. (2020).
\newblock Relaxing bijectivity constraints with continuously indexed
  normalising flows.
\newblock In {\em International Conference on Machine Learning}, pages
  2133--2143. PMLR.

\bibitem[Courty et~al., 2018]{courty2018learning}
Courty, N., Flamary, R., and Ducoffe, M. (2018).
\newblock Learning wasserstein embeddings.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Dhariwal and Nichol, 2021]{dhariwal2021diffusion}
Dhariwal, P. and Nichol, A. (2021).
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Dupuis and Ellis, 2011]{dupuis2011weak}
Dupuis, P. and Ellis, R.~S. (2011).
\newblock {\em A weak convergence approach to the theory of large deviations}.
\newblock John Wiley \& Sons.

\bibitem[Elfwing et~al., 2018]{elfwing2018sigmoid}
Elfwing, S., Uchibe, E., and Doya, K. (2018).
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock {\em Neural Networks}, 107:3--11.

\bibitem[Fazlyab et~al., 2019]{fazlyab2019efficient}
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G. (2019).
\newblock Efficient and accurate estimation of {L}ipschitz constants for deep
  neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Fedus et~al., 2018]{fedus2018many}
Fedus, W., Rosca, M., Lakshminarayanan, B., Dai, A.~M., Mohamed, S., and
  Goodfellow, I. (2018).
\newblock Many paths to equilibrium: Gans do not need to decrease a divergence
  at every step.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Goodfellow et~al., 2014]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial nets.
\newblock {\em Advances in Neural Information Processing Systems}, 27.

\bibitem[Goodfellow et~al., 2015]{goodfellow2015explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C. (2015).
\newblock Explaining and harnessing adversarial examples.
\newblock {\em Stat}, 1050:20.

\bibitem[Gulrajani et~al., 2017]{gulrajani2017improved}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.~C.
  (2017).
\newblock Improved training of wasserstein gans.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Gulrajani et~al., 2018]{gulrajani2018towards}
Gulrajani, I., Raffel, C., and Metz, L. (2018).
\newblock Towards gan benchmarks which require generalization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Gurumurthy et~al., 2017]{gurumurthy2017deligan}
Gurumurthy, S., Kiran~Sarvadevabhatla, R., and Venkatesh~Babu, R. (2017).
\newblock Deligan: Generative adversarial networks for diverse and limited
  data.
\newblock In {\em Proceedings of the IEEE Conference On Computer Vision and
  Pattern Recognition}, pages 166--174.

\bibitem[Hagemann and Neumayer, 2021]{hagemann2021stabilizing}
Hagemann, P. and Neumayer, S. (2021).
\newblock Stabilizing invertible neural networks using mixture models.
\newblock {\em Inverse Problems}, 37(8):085002.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778.

\bibitem[Ho et~al., 2020]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P. (2020).
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851.

\bibitem[Huang et~al., 2017]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q. (2017).
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4700--4708.

\bibitem[Hyv{\"a}rinen, 2005]{hyvarinen2005estimation}
Hyv{\"a}rinen, A. (2005).
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(4).

\bibitem[Isola et~al., 2017]{isola2018image}
Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A.~A. (2017).
\newblock Image-to-image translation with conditional adversarial networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1125--1134.

\bibitem[Issenhuth et~al., 2020]{issenhuth2020learning}
Issenhuth, T., Tanielian, U., Picard, D., and Mary, J. (2020).
\newblock Learning disconnected manifolds: Avoiding the no gan's land by latent
  rejection.

\bibitem[Karras et~al., 2020]{karras2020analyzing}
Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T.
  (2020).
\newblock Analyzing and improving the image quality of stylegan.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8110--8119.

\bibitem[Khayatkhoei et~al., 2018]{khayatkhoei2018disconnected}
Khayatkhoei, M., Elgammal, A., and Singh, M. (2018).
\newblock Disconnected manifold learning for generative adversarial networks.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 7354--7364.

\bibitem[Kingma and Ba, 2015]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2015).
\newblock Adam: A method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Kingma et~al., 2016]{kingma2016improved}
Kingma, D.~P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and
  Welling, M. (2016).
\newblock Improved variational inference with inverse autoregressive flow.
\newblock {\em Advances in neural information processing systems}, 29.

\bibitem[Kingma and Welling, 2014]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-{E}ncoding {V}ariational {B}ayes.
\newblock {\em Stat}, 1050:1.

\bibitem[Kodali et~al., 2017]{kodali2017convergence}
Kodali, N., Abernethy, J., Hays, J., and Kira, Z. (2017).
\newblock On convergence and stability of gans.
\newblock {\em arXiv preprint arXiv:1705.07215}.

\bibitem[Kumar and Poole, 2020]{kumar2020implicit}
Kumar, A. and Poole, B. (2020).
\newblock On implicit regularization in $\beta$-vaes.
\newblock In {\em International Conference on Machine Learning}, pages
  5480--5490. PMLR.

\bibitem[Kynk{\"a}{\"a}nniemi et~al., 2019]{kynkaanniemi2019improved}
Kynk{\"a}{\"a}nniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T.
  (2019).
\newblock Improved precision and recall metric for assessing generative models.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[LeCun et~al., 1998]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[Ledig et~al., 2017]{ledig2017photorealistic}
Ledig, C., Theis, L., Husz{\'a}r, F., Caballero, J., Cunningham, A., Acosta,
  A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et~al. (2017).
\newblock Photo-realistic single image super-resolution using a generative
  adversarial network.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4681--4690.

\bibitem[Lim and Ye, 2017]{lim2017geometric}
Lim, J.~H. and Ye, J.~C. (2017).
\newblock Geometric gan.
\newblock {\em arXiv preprint arXiv:1705.02894}.

\bibitem[Liu et~al., 2015]{liu2015deep}
Liu, Z., Luo, P., Wang, X., and Tang, X. (2015).
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 3730--3738.

\bibitem[Lu et~al., 2020]{lu2020implicit}
Lu, C., Chen, J., Li, C., Wang, Q., and Zhu, J. (2020).
\newblock Implicit normalizing flows.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Luise et~al., 2020]{luise2020generalization}
Luise, G., Pontil, M., and Ciliberto, C. (2020).
\newblock Generalization properties of optimal transport gans with latent
  distribution learning.
\newblock {\em arXiv preprint arXiv:2007.14641}.

\bibitem[Maas et~al., 2013]{maas2013rectifier}
Maas, A.~L., Hannun, A.~Y., Ng, A.~Y., et~al. (2013).
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em International Conference on Machine Learning}, volume~30,
  page~3. PMLR.

\bibitem[Mehr et~al., 2019]{mehr2019disconet}
Mehr, E., Jourdan, A., Thome, N., Cord, M., and Guitteny, V. (2019).
\newblock Disconet: Shapes learning on disconnected manifolds for 3d editing.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 3474--3483.

\bibitem[Metz et~al., 2017]{metz2017unrolled}
Metz, L., Poole, B., Pfau, D., and Sohl{-}Dickstein, J. (2017).
\newblock Unrolled generative adversarial networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Miyato et~al., 2018]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. (2018).
\newblock Spectral normalization for generative adversarial networks.
\newblock {\em International Conference on Learning Representations}, 6.

\bibitem[Mohajerin~Esfahani and Kuhn, 2018]{mohajerin2018data}
Mohajerin~Esfahani, P. and Kuhn, D. (2018).
\newblock Data-driven distributionally robust optimization using the
  wasserstein metric: Performance guarantees and tractable reformulations.
\newblock {\em Mathematical Programming}, 171(1):115--166.

\bibitem[Nagarajan et~al., 2018]{nagarajan2018theoretical}
Nagarajan, V., Raffel, C., and Goodfellow, I.~J. (2018).
\newblock Theoretical insights into memorization in gans.
\newblock In {\em Neural Information Processing Systems Workshop}, volume~1.

\bibitem[Odena et~al., 2018]{odena2018generator}
Odena, A., Buckman, J., Olsson, C., Brown, T., Olah, C., Raffel, C., and
  Goodfellow, I. (2018).
\newblock Is generator conditioning causally related to gan performance?
\newblock In {\em International Conference on Machine Learning}, pages
  3849--3858. PMLR.

\bibitem[Pennington et~al., 2017]{pennington2017resurrecting}
Pennington, J., Schoenholz, S., and Ganguli, S. (2017).
\newblock Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Peyr{\'e} and Cuturi, 2019]{OT}
Peyr{\'e}, G. and Cuturi, M. (2019).
\newblock Computational optimal transport: with applications to data science.
\newblock {\em Foundations and Trends in Machine Learning}, 11(5-6):355--607.

\bibitem[Pope et~al., 2020]{pope2020intrinsic}
Pope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. (2020).
\newblock The intrinsic dimension of images and its impact on learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Radford et~al., 2015]{radford2015unsupervised}
Radford, A., Metz, L., and Chintala, S. (2015).
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1511.06434}.

\bibitem[Ravuri et~al., 2021]{ravuri2021skilful}
Ravuri, S., Lenc, K., Willson, M., Kangin, D., Lam, R., Mirowski, P.,
  Fitzsimons, M., Athanassiadou, M., Kashem, S., Madge, S., et~al. (2021).
\newblock Skilful precipitation nowcasting using deep generative models of
  radar.
\newblock {\em Nature}, 597(7878):672--677.

\bibitem[Rezende and Mohamed, 2015]{rezende2015variational}
Rezende, D. and Mohamed, S. (2015).
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}, pages
  1530--1538. PMLR.

\bibitem[Ronneberger et~al., 2015]{ronneberger2015u}
Ronneberger, O., Fischer, P., and Brox, T. (2015).
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer.

\bibitem[Russakovsky et~al., 2015]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al. (2015).
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision}, 115(3):211--252.

\bibitem[Sajjadi et~al., 2018]{sajjadi2018assessing}
Sajjadi, M.~S., Bachem, O., Lucic, M., Bousquet, O., and Gelly, S. (2018).
\newblock Assessing generative models via precision and recall.
\newblock {\em Advances in Neural Information Processing Systems}, 31.

\bibitem[Sandfort et~al., 2019]{sandfort2019data}
Sandfort, V., Yan, K., Pickhardt, P.~J., and Summers, R.~M. (2019).
\newblock Data augmentation using generative adversarial networks
  ({C}ycle{GAN}) to improve generalizability in ct segmentation tasks.
\newblock {\em Scientific Reports}, 9(1):1--9.

\bibitem[Song and Ermon, 2019]{song2019generative}
Song, Y. and Ermon, S. (2019).
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Song et~al., 2020]{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole,
  B. (2020).
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[St{\'e}phanovitch et~al., 2022]{stephanovitch2022optimal}
St{\'e}phanovitch, A., Tanielian, U., Cadre, B., Klutchnikoff, N., and Biau, G.
  (2022).
\newblock Optimal 1-wasserstein distance for wgans.
\newblock {\em arXiv preprint arXiv:2201.02824}.

\bibitem[Sudakov and Tsirelson, 1978]{sudakov1978extremal}
Sudakov, V.~N. and Tsirelson, B.~S. (1978).
\newblock Extremal properties of half-spaces for spherically invariant
  measures.
\newblock {\em Journal of Soviet Mathematics}, 9(1):9--18.

\bibitem[Tanielian et~al., 2020]{tanielian2020learning}
Tanielian, U., Issenhuth, T., Dohmatob, E., and Mary, J. (2020).
\newblock Learning disconnected manifolds: a no gan’s land.
\newblock In {\em International Conference on Machine Learning}, pages
  9418--9427. PMLR.

\bibitem[Tran et~al., 2017]{tran2017deep}
Tran, D., Ranganath, R., and Blei, D. (2017).
\newblock Hierarchical implicit models and likelihood-free variational
  inference.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems}, 30.

\bibitem[Vincent, 2011]{vincent2011connection}
Vincent, P. (2011).
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural Computation}, 23(7):1661--1674.

\bibitem[Virmaux and Scaman, 2018]{scaman2019lipschitz}
Virmaux, A. and Scaman, K. (2018).
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 31.

\bibitem[Wang et~al., 2018]{wang2018non}
Wang, X., Girshick, R., Gupta, A., and He, K. (2018).
\newblock Non-local neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7794--7803.

\bibitem[Wenliang and Kanagawa, 2020]{wenliang2020blindness}
Wenliang, L.~K. and Kanagawa, H. (2020).
\newblock Blindness of score-based methods to isolated components and mixing
  proportions.
\newblock {\em arXiv preprint arXiv:2008.10087}.

\bibitem[Wu et~al., 2020]{wu2020stochastic}
Wu, H., K{\"o}hler, J., and No{\'e}, F. (2020).
\newblock Stochastic normalizing flows.
\newblock {\em Advances in Neural Information Processing Systems},
  33:5933--5944.

\bibitem[Wu and He, 2018]{wu2018group}
Wu, Y. and He, K. (2018).
\newblock Group normalization.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 3--19.

\bibitem[Yang et~al., 2018]{yang2018improving}
Yang, Z., Chen, W., Wang, F., and Xu, B. (2018).
\newblock Improving neural machine translation with conditional sequence
  generative adversarial nets.
\newblock {\em Conference of the North American Chapter of the Association for
  Computational Linguistics: Human Language Technologies}, 1:1346--1355.

\bibitem[Zhang et~al., 2019]{zhang2019self}
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A. (2019).
\newblock Self-attention generative adversarial networks.
\newblock In {\em International Conference on Machine Learning}, pages
  7354--7363. PMLR.

\end{thebibliography}
