\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arnab et~al.(2021)Arnab, Dehghani, Heigold, Sun, Lu{\v{c}}i{\'c}, and Schmid]{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c}, and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock In \emph{ICCV}, 2021.

\bibitem[Bao et~al.(2021)Bao, Dong, Piao, and Wei]{bao2021beit}
Hangbo Bao, Li~Dong, Songhao Piao, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{arXiv preprint arXiv:2106.08254}, 2021.

\bibitem[Bertasius et~al.(2021)Bertasius, Wang, and Torresani]{bertasius2021space}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock In \emph{ICML}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Carreira and Zisserman(2017)]{carreira2017quo}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In \emph{CVPR}, 2017.

\bibitem[Chen et~al.(2021)Chen, Fan, and Panda]{chen2021crossvit}
Chun-Fu~Richard Chen, Quanfu Fan, and Rameswar Panda.
\newblock Crossvit: Cross-attention multi-scale vision transformer for image classification.
\newblock In \emph{ICCV}, 2021.

\bibitem[Chen et~al.(2020)Chen, Radford, Child, Wu, Jun, Luan, and Sutskever]{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In \emph{ICML}, 2020.

\bibitem[Choi et~al.(2019)Choi, Gao, Messou, and Huang]{Choi-NeurIPS-2019}
Jinwoo Choi, Chen Gao, Joseph~CE Messou, and Jia-Bin Huang.
\newblock Why can't i dance in the mall? learning to mitigate scene bias in action recognition.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced search space.
\newblock In \emph{CVPR workshops}, 2020.

\bibitem[Damen et~al.(2022)Damen, Doughty, Farinella, , Furnari, Ma, Kazakos, Moltisanti, Munro, Perrett, Price, and Wray]{damen2022rescaling}
Dima Damen, Hazel Doughty, Giovanni~Maria Farinella, , Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray.
\newblock Rescaling egocentric vision.
\newblock \emph{IJCV}, 130\penalty0 (1):\penalty0 33--55, 2022.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Fan et~al.(2021)Fan, Xiong, Mangalam, Li, Yan, Malik, and Feichtenhofer]{fan2021multiscale}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Feichtenhofer(2020)]{feichtenhofer2020x3d}
Christoph Feichtenhofer.
\newblock X3d: Expanding architectures for efficient video recognition.
\newblock In \emph{CVPR}, 2020.

\bibitem[Feichtenhofer et~al.(2016)Feichtenhofer, Pinz, and Zisserman]{feichtenhofer2016convolutional}
Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.
\newblock Convolutional two-stream network fusion for video action recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Feichtenhofer et~al.(2019)Feichtenhofer, Fan, Malik, and He]{feichtenhofer2019slowfast}
Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He.
\newblock Slowfast networks for video recognition.
\newblock In \emph{ICCV}, 2019.

\bibitem[Girdhar et~al.(2022)Girdhar, Singh, Ravi, van~der Maaten, Joulin, and Misra]{girdhar2022omnivore}
Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van~der Maaten, Armand Joulin, and Ishan Misra.
\newblock {Omnivore: A Single Model for Many Visual Modalities}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin, and Misra]{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev Alwala, Armand Joulin, and Ishan Misra.
\newblock Imagebind: One embedding space to bind them all.
\newblock \emph{arXiv preprint arXiv:2305.05665}, 2023.

\bibitem[Gorti et~al.(2022)Gorti, Vouitsis, Ma, Golestan, Volkovs, Garg, and Yu]{gorti2022xpool}
Satya~Krishna Gorti, No{\"e}l Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu.
\newblock X-pool: Cross-modal language-video attention for text-video retrieval.
\newblock In \emph{CVPR}, 2022.

\bibitem[Goyal et~al.(2017)Goyal, Ebrahimi~Kahou, Michalski, Materzynska, Westphal, Kim, Haenel, Fruend, Yianilos, Mueller-Freitag, et~al.]{goyal2017something}
Raghav Goyal, Samira Ebrahimi~Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et~al.
\newblock The" something something" video database for learning and evaluating visual common sense.
\newblock In \emph{ICCV}, 2017.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Herzig et~al.(2022)Herzig, Ben-Avraham, Mangalam, Bar, Chechik, Rohrbach, Darrell, and Globerson]{herzig2022object}
Roei Herzig, Elad Ben-Avraham, Karttikeya Mangalam, Amir Bar, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir Globerson.
\newblock Object-region video transformers.
\newblock In \emph{CVPR}, 2022.

\bibitem[Hoffer et~al.(2020)Hoffer, Ben-Nun, Hubara, Giladi, Hoefler, and Soudry]{hoffer2020augment}
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry.
\newblock Augment your batch: Improving generalization through instance repetition.
\newblock In \emph{CVPR}, 2020.

\bibitem[Karimi~Mahabadi et~al.(2021)Karimi~Mahabadi, Henderson, and Ruder]{karimi2021compacter}
Rabeeh Karimi~Mahabadi, James Henderson, and Sebastian Ruder.
\newblock Compacter: Efficient low-rank hypercomplex adapter layers.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier, Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{kay2017kinetics}
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem[Kenton and Toutanova(2019)]{kenton2019bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Kim et~al.(2022)Kim, Yu, Yuan, and Tomasi]{Kim_2022_ACCV}
Hannah~Halin Kim, Shuzhi Yu, Shuai Yuan, and Carlo Tomasi.
\newblock Cross-attention transformer for video interpolation.
\newblock In \emph{ACCV}, 2022.

\bibitem[Kondratyuk et~al.(2021)Kondratyuk, Yuan, Li, Zhang, Tan, Brown, and Gong]{kondratyuk2021movinets}
Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li~Zhang, Mingxing Tan, Matthew Brown, and Boqing Gong.
\newblock Movinets: Mobile video networks for efficient video recognition.
\newblock In \emph{CVPR}, 2021.

\bibitem[Kowal et~al.(2022)Kowal, Siam, Islam, Bruce, Wildes, and Derpanis]{kowal2022deeper}
Matthew Kowal, Mennatullah Siam, Md~Amirul Islam, Neil~DB Bruce, Richard~P Wildes, and Konstantinos~G Derpanis.
\newblock A deeper dive into what deep spatiotemporal networks encode: Quantifying static vs. dynamic information.
\newblock In \emph{CVPR}, 2022.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and Hoi]{li2021align}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with momentum distillation.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Li et~al.(2022)Li, Wang, Gao, Song, Liu, Li, and Qiao]{li2022uniformer}
Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu~Liu, Hongsheng Li, and Yu~Qiao.
\newblock Uniformer: Unified transformer for efficient spatiotemporal representation learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Li et~al.(2018)Li, Li, and Vasconcelos]{li2018resound}
Yingwei Li, Yi~Li, and Nuno Vasconcelos.
\newblock Resound: Towards action recognition without representation bias.
\newblock In \emph{ECCV}, 2018.

\bibitem[Lin et~al.(2019)Lin, Gan, and Han]{lin2019tsm}
Ji~Lin, Chuang Gan, and Song Han.
\newblock Tsm: Temporal shift module for efficient video understanding.
\newblock In \emph{ICCV}, 2019.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Lei, Bansal, and Bertasius]{ECLIPSE_ECCV22}
Yan-Bo Lin, Jie Lei, Mohit Bansal, and Gedas Bertasius.
\newblock Eclipse: Efficient long-range video retrieval using sight and sound.
\newblock In \emph{ECCV}, 2022{\natexlab{a}}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Geng, Zhang, Gao, de~Melo, Wang, Dai, Qiao, and Li]{lin2022frozen}
Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de~Melo, Xiaogang Wang, Jifeng Dai, Yu~Qiao, and Hongsheng Li.
\newblock Frozen clip models are efficient video learners.
\newblock In \emph{ECCV}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Ma, Tian, He, and Kira]{liu2022polyhistor}
Yen-Cheng Liu, Chih-Yao Ma, Junjiao Tian, Zijian He, and Zsolt Kira.
\newblock Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks.
\newblock In \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Ning, Cao, Wei, Zhang, Lin, and Hu]{liu2022video}
Ze~Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu.
\newblock Video swin transformer.
\newblock In \emph{CVPR}, 2022{\natexlab{b}}.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Nagrani et~al.(2021)Nagrani, Yang, Arnab, Jansen, Schmid, and Sun]{nagrani2021mbt}
Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun.
\newblock Attention bottlenecks for multimodal fusion.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ni et~al.(2022)Ni, Peng, Chen, Zhang, Meng, Fu, Xiang, and Ling]{XCLIP}
Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling.
\newblock Expanding language-image pretrained models for general video recognition.
\newblock In \emph{ECCV}, 2022.

\bibitem[Pan et~al.(2022)Pan, Lin, Zhu, Shao, and Li]{pan2022st}
Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li.
\newblock St-adapter: Parameter-efficient image-to-video transfer learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Patrick et~al.(2021)Patrick, Campbell, Asano, Metze, Feichtenhofer, Vedaldi, and Henriques]{patrick2021keeping}
Mandela Patrick, Dylan Campbell, Yuki~M. Asano, Ishan Misra~Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and Jo{\~a}o~F. Henriques.
\newblock Keeping your eye on the ball: Trajectory attention in video transformers.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{ICML}, 2021.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Rebuffi et~al.(2018)Rebuffi, Bilen, and Vedaldi]{rebuffi2018efficient}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Efficient parametrization of multi-domain deep neural networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{CVPR}, 2022.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Sevilla-Lara et~al.(2021)Sevilla-Lara, Zha, Yan, Goswami, Feiszli, and Torresani]{sevilla2021only}
Laura Sevilla-Lara, Shengxin Zha, Zhicheng Yan, Vedanuj Goswami, Matt Feiszli, and Lorenzo Torresani.
\newblock Only time can tell: Discovering temporal data for temporal modeling.
\newblock In \emph{WACV}, 2021.

\bibitem[Simonyan and Zisserman(2014)]{Simonyan-NIPS-2014}
Karen Simonyan and Andrew Zisserman.
\newblock Two-stream convolutional networks for action recognition in videos.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Sudhakaran et~al.(2022)Sudhakaran, Escalera, and Lanz]{sudhakaran2022gate}
Swathikiran Sudhakaran, Sergio Escalera, and Oswald Lanz.
\newblock Gate-shift-fuse for video action recognition.
\newblock \emph{arXiv:2203.08897}, 2022.

\bibitem[Sun et~al.(2023)Sun, Fang, Wu, Wang, and Cao]{sun2023eva}
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
\newblock Eva-clip: Improved training techniques for clip at scale.
\newblock \emph{arXiv preprint arXiv:2303.15389}, 2023.

\bibitem[Sung et~al.(2022)Sung, Cho, and Bansal]{sung2022lst}
Yi-Lin Sung, Jaemin Cho, and Mohit Bansal.
\newblock Lst: Ladder side-tuning for parameter and memory efficient transfer learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Tong et~al.(2022)Tong, Song, Wang, and Wang]{tong2022videomae}
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
\newblock Video{MAE}: Masked autoencoders are data-efficient learners for self-supervised video pre-training.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Tran et~al.(2015)Tran, Bourdev, Fergus, Torresani, and Paluri]{tran2015learning}
Du~Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
\newblock Learning spatiotemporal features with 3d convolutional networks.
\newblock In \emph{ICCV}, 2015.

\bibitem[Tran et~al.(2018)Tran, Wang, Torresani, Ray, LeCun, and Paluri]{tran2018closer}
Du~Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri.
\newblock A closer look at spatiotemporal convolutions for action recognition.
\newblock In \emph{CVPR}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Xiong, Wang, Qiao, Lin, Tang, and Van~Gool]{wang2018temporal}
Limin Wang, Yuanjun Xiong, Zhe Wang, Yu~Qiao, Dahua Lin, Xiaoou Tang, and Luc Van~Gool.
\newblock Temporal segment networks for action recognition in videos.
\newblock \emph{TPAMI}, 41\penalty0 (11):\penalty0 2740--2755, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Chen, Wu, Chen, Dai, Liu, Jiang, Zhou, and Yuan]{wang2022bevt}
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou, and Lu~Yuan.
\newblock Bevt: Bert pretraining of video transformers.
\newblock In \emph{CVPR}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2023)Wang, Chen, Wu, Chen, Dai, Liu, Yuan, and Jiang]{wang2022masked}
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu~Yuan, and Yu-Gang Jiang.
\newblock Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning.
\newblock In \emph{CVPR}, 2023.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Girshick, Gupta, and He]{wang2018non}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In \emph{CVPR}, 2018{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Li, Li, He, Huang, Zhao, Zhang, Xu, Liu, Wang, Xing, Chen, Pan, Yu, Wang, Wang, and Qiao]{wang2022internvideo}
Yi~Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi~Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu~Qiao.
\newblock Internvideo: General video foundation models via generative and discriminative learning.
\newblock \emph{arXiv preprint arXiv:2212.03191}, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Zhang, Lee, Zhang, Sun, Ren, Su, Perot, Dy, and Pfister]{wang2022learning}
Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister.
\newblock Learning to prompt for continual learning.
\newblock In \emph{CVPR}, 2022{\natexlab{c}}.

\bibitem[Wei et~al.(2020)Wei, Zhang, Li, Zhang, and Wu]{wei2020multi}
Xi~Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, and Feng Wu.
\newblock Multi-modality cross attention network for image and sentence matching.
\newblock In \emph{CVPR}, 2020.

\bibitem[Wu et~al.(2022)Wu, Li, Mangalam, Fan, Xiong, Malik, and Feichtenhofer]{wu2022memvit}
Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo~Xiong, Jitendra Malik, and Christoph Feichtenhofer.
\newblock Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition.
\newblock In \emph{CVPR}, 2022.

\bibitem[Wu et~al.(2023)Wu, Sun, and Ouyang]{wu2023revisiting}
Wenhao Wu, Zhun Sun, and Wanli Ouyang.
\newblock Revisiting classifier: Transferring vision-language models for video recognition.
\newblock In \emph{AAAI}, 2023.

\bibitem[Xie et~al.(2018)Xie, Sun, Huang, Tu, and Murphy]{Xie-ECCV-2018}
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
\newblock Rethinking spatiotemporal feature learning for video understanding.
\newblock In \emph{ECCV}, 2018.

\bibitem[Yan et~al.(2022)Yan, Xiong, Arnab, Lu, Zhang, Sun, and Schmid]{yan2022multiview}
Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi~Zhang, Chen Sun, and Cordelia Schmid.
\newblock Multiview transformers for video recognition.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yang et~al.(2023)Yang, Zhu, Xie, Zhang, Chen, and Li]{yang2023aim}
Taojiannan Yang, Yi~Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu~Li.
\newblock Aim: Adapting image models for efficient video understanding.
\newblock In \emph{ICLR}, 2023.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Andonian, Oliva, and Torralba]{zhou2018temporal}
Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba.
\newblock Temporal relational reasoning in videos.
\newblock In \emph{ECCV}, 2018.

\bibitem[Zhu et~al.(2022)Zhu, Ke, Li, Liu, Tian, and Shan]{Zhu_2022_CVPR}
Haowei Zhu, Wenjing Ke, Dong Li, Ji~Liu, Lu~Tian, and Yi~Shan.
\newblock Dual cross-attention learning for fine-grained visual categorization and object re-identification.
\newblock In \emph{CVPR}, 2022.

\end{thebibliography}
