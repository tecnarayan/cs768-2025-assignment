\begin{thebibliography}{10}

\bibitem{benjamini2001control}
Yoav Benjamini and Daniel Yekutieli.
\newblock The control of the false discovery rate in multiple testing under
  dependency.
\newblock {\em The annals of statistics}, 29(4):1165--1188, 2001.

\bibitem{BerTho2004}
A.~Berlinet and C.~{Thomas-Agnan}.
\newblock {\em Reproducing Kernel Hilbert Spaces in Probability and
  Statistics}.
\newblock Kluwer, 2004.

\bibitem{BinSutArbGre2018}
M.~{Bi{\'n}kowski}, D.~J. {Sutherland}, M.~{Arbel}, and A.~{Gretton}.
\newblock Demystifying {MMD} {GANs}.
\newblock In {\em ICLR}. 2018.

\bibitem{bounliphone2015test}
Wacha Bounliphone, Eugene Belilovsky, Matthew~B. Blaschko, Ioannis Antonoglou,
  and Arthur Gretton.
\newblock A test of relative similarity for model selection in generative
  models.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{burkardt2014truncated}
John Burkardt.
\newblock The truncated normal distribution.
\newblock {\em Department of Scientific Computing Website, Florida State
  University}, 2014.

\bibitem{chwialkowski2016kernel}
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton.
\newblock A kernel test of goodness of fit.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2016.

\bibitem{chwialkowski2015fast}
Kacper~P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton.
\newblock Fast two-sample testing with analytic representations of probability
  measures.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1981--1989, 2015.

\bibitem{EriBacHar2008}
Moulines Eric, Francis~R Bach, and Za{\"\i}d Harchaoui.
\newblock Testing for homogeneity with kernel fisher discriminant analysis.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  609--616, 2008.

\bibitem{fithian2014optimal}
William Fithian, Dennis Sun, and Jonathan Taylor.
\newblock Optimal inference after model selection.
\newblock {\em arXiv preprint arXiv:1410.2597}, 2014.

\bibitem{FroLerReyoth2012}
Magalie Fromont, Matthieu Lerasle, Patricia Reynaud-Bouret, et~al.
\newblock Kernels based tests with non-asymptotic bootstrap approaches for
  two-sample problems.
\newblock In {\em Conference on Learning Theory}, pages 23--1, 2012.

\bibitem{fukumizu2008kernel}
Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Sch{\"o}lkopf.
\newblock Kernel measures of conditional dependence.
\newblock In {\em Advances in neural information processing systems}, pages
  489--496, 2008.

\bibitem{germain2015made}
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle.
\newblock {MADE}: Masked autoencoder for distribution estimation.
\newblock In {\em International Conference on Machine Learning}, pages
  881--889, 2015.

\bibitem{gorham2017measuring}
Jackson Gorham and Lester Mackey.
\newblock Measuring sample quality with kernels.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1292--1301. JMLR. org, 2017.

\bibitem{gretton2012kernel}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock {\em Journal of Machine Learning Research}, 13(Mar):723--773, 2012.

\bibitem{gretton2012optimal}
Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan,
  Massimiliano Pontil, Kenji Fukumizu, and Bharath~K Sriperumbudur.
\newblock Optimal kernel choice for large-scale two-sample tests.
\newblock In {\em Advances in neural information processing systems}, pages
  1205--1213, 2012.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6626--6637, 2017.

\bibitem{jitkrittum2018informative}
Wittawat Jitkrittum, Heishiro Kanagawa, Patsorn Sangkloy, James Hays, Bernhard
  Sch{\"o}lkopf, and Arthur Gretton.
\newblock Informative features for model comparison.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{JitSzaChwGre2016}
Wittawat Jitkrittum, Zolt\'{a}n Szab\'{o}, Kacper~P Chwialkowski, and Arthur
  Gretton.
\newblock Interpretable distribution features with maximum testing power.
\newblock In {\em NIPS}, pages 181--189. 2016.

\bibitem{jitkrittum2017linear}
Wittawat Jitkrittum, Wenkai Xu, Zolt{\'a}n Szab{\'o}, Kenji Fukumizu, and
  Arthur Gretton.
\newblock A linear-time kernel goodness-of-fit test.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  262--271, 2017.

\bibitem{lee2016exact}
Jason~D Lee, Dennis~L Sun, Yuekai Sun, and Jonathan~E Taylor.
\newblock Exact post-selection inference, with application to the {Lasso}.
\newblock {\em The Annals of Statistics}, 44(3):907--927, 2016.

\bibitem{leeb2005model}
Hannes Leeb and Benedikt~M P{\"o}tscher.
\newblock Model selection and inference: Facts and fiction.
\newblock {\em Econometric Theory}, 21(1):21--59, 2005.

\bibitem{leeb2006can}
Hannes Leeb, Benedikt~M P{\"o}tscher, et~al.
\newblock Can one estimate the conditional distribution of post-model-selection
  estimators?
\newblock {\em The Annals of Statistics}, 34(5):2554--2591, 2006.

\bibitem{liu2016kernelized}
Qiang Liu, Jason Lee, and Michael Jordan.
\newblock A kernelized {Stein} discrepancy for goodness-of-fit tests.
\newblock In {\em International Conference on Machine Learning}, pages
  276--284, 2016.

\bibitem{liu2018large}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Large-scale celebfaces attributes (celeba) dataset.
\newblock {\em Retrieved August}, 15:2018, 2018.

\bibitem{papamakarios2017masked}
George Papamakarios, Theo Pavlakou, and Iain Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2338--2347, 2017.

\bibitem{smola2007hilbert}
Alex Smola, Arthur Gretton, Le~Song, and Bernhard Sch{\"o}lkopf.
\newblock A {Hilbert} space embedding for distributions.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 13--31. Springer, 2007.

\bibitem{sriperumbudur2011universality}
Bharath~K Sriperumbudur, Kenji Fukumizu, and Gert~RG Lanckriet.
\newblock Universality, characteristic kernels and rkhs embedding of measures.
\newblock {\em Journal of Machine Learning Research}, 12(Jul):2389--2410, 2011.

\bibitem{steinwart2001influence}
Ingo Steinwart.
\newblock On the influence of the kernel on the consistency of support vector
  machines.
\newblock {\em Journal of machine learning research}, 2(Nov):67--93, 2001.

\bibitem{suzumura2017selective}
Shinya Suzumura, Kazuya Nakagawa, Yuta Umezu, Koji Tsuda, and Ichiro Takeuchi.
\newblock Selective inference for sparse high-order interaction models.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3338--3347. JMLR. org, 2017.

\bibitem{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{yamada2018post}
Makoto Yamada, Denny Wu, Yao-Hung~Hubert Tsai, Hirofumi Ohta, Ruslan
  Salakhutdinov, Ichiro Takeuchi, and Kenji Fukumizu.
\newblock Post selection inference with incomplete maximum mean discrepancy
  estimator.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{YanLiuRaoNev2018}
Jiasen Yang, Qiang Liu, Vinayak Rao, and Jennifer Neville.
\newblock Goodness-of-fit testing for discrete distributions via {Stein}
  discrepancy.
\newblock In {\em International Conference on Machine Learning}, pages
  5557--5566, 2018.

\bibitem{zaremba2013b}
Wojciech Zaremba, Arthur Gretton, and Matthew Blaschko.
\newblock B-test: A non-parametric, low variance kernel two-sample test.
\newblock In {\em Advances in neural information processing systems}, pages
  755--763, 2013.

\end{thebibliography}
