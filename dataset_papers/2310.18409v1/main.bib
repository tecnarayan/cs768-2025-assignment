@article{castro_kernel_2023,
title	= {A Kernel Perspective on Behavioural Metrics for Markov Decision Processes},
author	= {Pablo Samuel Castro and Tyler Kastner and Prakash Panangaden and Mark Rowland},
year	= {2023},
URL	= {https://openreview.net/forum?id=nHfPXl1ly7&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DTMLR%2FAuthors%23your-submissions)},
journal	= {TMLR}
}



@inproceedings{csaba_completeness_2005,
author = {Szepesv\'{a}ri, Csaba and Munos, R\'{e}mi},
title = {Finite Time Bounds for Sampling Based Fitted Value Iteration},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102351.1102462},
doi = {10.1145/1102351.1102462},
abstract = {In this paper we consider sampling based fitted value iteration for discounted, large (possibly infinite) state space, finite action Markovian Decision Problems where only a generative model of the transition probabilities and rewards is available. At each step the image of the current estimate of the optimal value function under a Monte-Carlo approximation to the Bellman-operator is projected onto some function space. PAC-style bounds on the weighted Lp-norm approximation error are obtained as a function of the covering number and the approximation power of the function space, the iteration number and the sample size.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {880–887},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}
@book{sutton_rlbook_2018,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}



@misc{fu_d4rl_2020,
    title={D4RL: Datasets for Deep Data-Driven Reinforcement Learning},
    author={Justin Fu and Aviral Kumar and Ofir Nachum and George Tucker and Sergey Levine},
    year={2020},
    eprint={2004.07219},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{kemertas_robustmetric_2021,
  author       = {Mete Kemertas and
                  Tristan Aumentado{-}Armstrong},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Towards Robust Bisimulation Metric Learning},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {4764--4777},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/256bf8e6923a52fda8ddf7dc050a1148-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:47 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/KemertasA21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{ba_layernorm_2016,
  title={Layer Normalization},
  author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@inproceedings{matthews_partial_1992,
  title={The Topology of Partial Metric Spaces},
  author={Steve Matthews},
  year={1992}
}

@article{ukaszyk_ld_2004,
  title={A new concept of probability metric and its applications in approximation of scattered data sets},
  author={Szymon Łukaszyk},
  journal={Computational Mechanics},
  year={2004},
  volume={33},
  pages={299-304}
}

@inproceedings{villani_wasser_2008,
  title={Optimal Transport: Old and New},
  author={C{\'e}dric Villani},
  year={2008}
}

@article{mnih_dqn_2015,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}



@misc{brockman_gym_2016,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
@article{raffin_sb3_2021,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@article{paine_hparam_2020,
  author       = {Tom Le Paine and
                  Cosmin Paduraru and
                  Andrea Michi and
                  {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
                  Konrad Zolna and
                  Alexander Novikov and
                  Ziyu Wang and
                  Nando de Freitas},
  title        = {Hyperparameter Selection for Offline Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/2007.09055},
  year         = {2020},
  url          = {https://arxiv.org/abs/2007.09055},
  eprinttype    = {arXiv},
  eprint       = {2007.09055},
  timestamp    = {Wed, 20 Jan 2021 16:29:33 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2007-09055.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{nachum_bestdice_2020,
 author = {Yang, Mengjiao and Nachum, Ofir and Dai, Bo and Li, Lihong and Schuurmans, Dale},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6551--6561},
 publisher = {Curran Associates, Inc.},
 title = {Off-Policy Evaluation via the Regularized Lagrangian},
 url = {https://proceedings.neurips.cc/paper/2020/file/488e4104520c6aab692863cc1dba45af-Paper.pdf},
 volume = {33},
 year = {2020}
}

@book{puterman_mdp_2014,
  added-at = {2017-04-07T12:13:11.000+0200},
  author = {Puterman, Martin L},
  biburl = {https://www.bibsonomy.org/bibtex/22e7ac99cd30c4892171e5a7cef1bc7a7/becker},
  interhash = {6cec8f775a265d8741171d17e4a4e7d0},
  intrahash = {2e7ac99cd30c4892171e5a7cef1bc7a7},
  keywords = {inthesis diss markov chain decision process citedby:scholar:count:9594 citedby:scholar:timestamp:2017-4-7},
  publisher = {John Wiley \& Sons},
  timestamp = {2017-04-07T12:13:11.000+0200},
  title = {Markov decision processes: discrete stochastic dynamic programming},
  year = 2014
}

@inproceedings{kumar_cql_2020,
author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
title = {Conservative Q-Learning for Offline Reinforcement Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {100},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}


@article{ferns_bisim_2011,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
title = {Bisimulation Metrics for Continuous Markov Decision Processes},
journal = {SIAM Journal on Computing},
volume = {40},
number = {6},
pages = {1662-1714},
year = {2011},
doi = {10.1137/10080484X},

URL = { 
    
        https://doi.org/10.1137/10080484X
    
    

},
eprint = { 
    
        https://doi.org/10.1137/10080484X
    
    

}
,
    abstract = { In recent years, various metrics have been developed for measuring the behavioral similarity of states in probabilistic transition systems [J. Desharnais et al., Proceedings of CONCUR'99, Springer-Verlag, London, 1999, pp. 258–273; F. van Breugel and J. Worrell, Proceedings of ICALP'01, Springer-Verlag, London, 2001, pp. 421–432]. In the context of finite Markov decision processes (MDPs), we have built on these metrics to provide a robust quantitative analogue of stochastic bisimulation [N. Ferns, P. Panangaden, and D. Precup, Proceedings of UAI-04, AUAI Press, Arlington, VA, 2004, pp. 162–169] and an efficient algorithm for its calculation [N. Ferns, P. Panangaden, and D. Precup, Proceedings of UAI-06, AUAI Press, Arlington, VA, 2006, pp. 174–181]. In this paper, we seek to properly extend these bisimulation metrics to MDPs with continuous state spaces. In particular, we provide the first distance-estimation scheme for metrics based on bisimulation for continuous probabilistic transition systems. Our work, based on statistical sampling and infinite dimensional linear programming, is a crucial first step in formally guiding real-world planning, where tasks are usually continuous and highly stochastic in nature, e.g., robot navigation, and often a substitution with a parametric model or crude finite approximation must be made. We show that the optimal value function associated with a discounted infinite-horizon planning task is continuous with respect to metric distances. Thus, our metrics allow one to reason about the quality of solution obtained by replacing one model with another. Alternatively, they may potentially be used directly for state aggregation. }
}

@inproceedings{ferns_bisim_2014,
author = {Ferns, Norm and Precup, Doina},
title = {Bisimulation Metrics Are Optimal Value Functions},
year = {2014},
isbn = {9780974903910},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Bisimulation is a notion of behavioural equivalence on the states of a transition system. Its definition has been extended to Markov decision processes, where it can be used to aggregate states. A bisimulation metric is a quantitative analog of bisimulation that measures how similar states are from a the perspective of long-term behavior. Bisimulation metrics have been used to establish approximation bounds for state aggregation and other forms of value function approximation. In this paper, we prove that a bisimulation metric defined on the state space of a Markov decision process is the optimal value function of an optimal coupling of two copies of the original model. We prove the result in the general case of continuous state spaces. This result has important implications in understanding the complexity of computing such metrics, and opens up the possibility of more efficient computational methods.},
booktitle = {Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence},
pages = {210–219},
numpages = {10},
location = {Quebec City, Quebec, Canada},
series = {UAI'14}
}

@inproceedings{ferns_bisim_2004,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
title = {Metrics for Finite Markov Decision Processes},
year = {2004},
isbn = {0974903906},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We present metrics for measuring the similarity of states in a finite Markov decision process (MDP). The formulation of our metrics is based on the notion of bisimulation for MDPs, with an aim towards solving discounted infinite horizon reinforcement learning tasks. Such metrics can be used to aggregate states, as well as to better structure other value function approximators (e.g., memory-based or nearest-neighbor approximators). We provide bounds that relate our metric distances to the optimal values of states in the given MDP.},
booktitle = {Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence},
pages = {162–169},
numpages = {8},
location = {Banff, Canada},
series = {UAI '04}
}

@InProceedings{dadashi_ploff_2021,
  title = 	 {Offline Reinforcement Learning with Pseudometric Learning},
  author =       {Dadashi, Robert and Rezaeifar, Shideh and Vieillard, Nino and Hussenot, L{\'e}onard and Pietquin, Olivier and Geist, Matthieu},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2307--2318},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/dadashi21a/dadashi21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/dadashi21a.html},
  abstract = 	 {Offline Reinforcement Learning methods seek to learn a policy from logged transitions of an environment, without any interaction. In the presence of function approximation, and under the assumption of limited coverage of the state-action space of the environment, it is necessary to enforce the policy to visit state-action pairs close to the support of logged transitions. In this work, we propose an iterative procedure to learn a pseudometric (closely related to bisimulation metrics) from logged transitions, and use it to define this notion of closeness. We show its convergence and extend it to the function approximation setting. We then use this pseudometric to define a new lookup based bonus in an actor-critic algorithm: PLOFF. This bonus encourages the actor to stay close, in terms of the defined pseudometric, to the support of logged transitions. Finally, we evaluate the method on hand manipulation and locomotion tasks.}
}

@inproceedings{
agarwal_psm_2021,
title={Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning},
author={Rishabh Agarwal and Marlos C. Machado and Pablo Samuel Castro and Marc G Bellemare},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qda7-sVg84}
}

@InProceedings{haarnoja_sac_2018,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}


@inproceedings{kozen_coinductive_2006,
author = {Kozen, Dexter},
title = {Coinductive Proof Principles for Stochastic Processes},
year = {2006},
isbn = {0769526314},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/LICS.2006.18},
doi = {10.1109/LICS.2006.18},
abstract = {We give an explicit coinduction principle for recursivelydefined stochastic processes. The principle applies to any closed property, not just equality, and works even when solutions are not unique. The rule encapsulates low-level analytic arguments, allowing reasoning about such processes at a higher algebraic level. We illustrate the use of the rule in deriving properties of a simple coin-flip process.},
booktitle = {Proceedings of the 21st Annual IEEE Symposium on Logic in Computer Science},
pages = {359–366},
numpages = {8},
series = {LICS '06}
}

@inproceedings{voloshin_opebench_2021,
title={Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning},
author={Cameron Voloshin and Hoang Minh Le and Nan Jiang and Yisong Yue},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=IsK8iKbL-I}
}

@inproceedings{fu_opebench_2021,
  author={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and Ziyu Wang and Alexander Novikov and Mengjiao Yang and Michael R. Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},
  title={Benchmarks for Deep Off-Policy Evaluation},
  year={2021},
  cdate={1609459200000},
  url={https://openreview.net/forum?id=kWSeGEeHvF8},
  booktitle={ICLR},
}

@article{agarwal_precipice_2021,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@misc{levine_offline_2020,
	title = {Offline {Reinforcement} {Learning}: {Tutorial}, {Review}, and {Perspectives} on {Open} {Problems}},
	shorttitle = {Offline {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2005.01643},
	abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
	month = nov,
	year = {2020},
	note = {arXiv:2005.01643 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{test,
author={Hello}, title={Test article}, journal={journal}, year={2019}}

@inproceedings{le_batch_2019,
	title = {Batch {Policy} {Learning} under {Constraints}},
	url = {http://arxiv.org/abs/1903.08738},
	abstract = {When learning policies for real-world domains, two important questions arise: (i) how to efficiently use pre-collected off-policy, non-optimal behavior data; and (ii) how to mediate among different competing objectives and constraints. We thus study the problem of batch policy learning under multiple constraints, and offer a systematic solution. We first propose a flexible meta-algorithm that admits any batch reinforcement learning and online learning procedure as subroutines. We then present a specific algorithmic instantiation and provide performance guarantees for the main objective and all constraints. To certify constraint satisfaction, we propose a new and simple method for off-policy policy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong empirical results in different domains, including in a challenging problem of simulated car driving subject to multiple constraints such as lane keeping and smooth driving. We also show experimentally that our OPE method outperforms other popular OPE techniques on a standalone basis, especially in a high-dimensional setting.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Le, Hoang M. and Voloshin, Cameron and Yue, Yisong},
	month = mar,
	year = {2019},
 booktitle={International Conference on Machine Learning (ICML)},
	note = {arXiv:1903.08738 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}



@article{castro_mico_2022,
	title = {{MICo}: {Improved} representations via sampling-based state similarity for {Markov} decision processes},
	shorttitle = {{MICo}},
	url = {http://arxiv.org/abs/2106.08229},
	abstract = {We present a new behavioural distance over the state space of a Markov decision process, and demonstrate the use of this distance as an effective means of shaping the learnt representations of deep reinforcement learning agents. While existing notions of state similarity are typically difficult to learn at scale due to high computational cost and lack of sample-based algorithms, our newly-proposed distance addresses both of these issues. In addition to providing detailed theoretical analysis, we provide empirical evidence that learning this distance alongside the value function yields structured and informative representations, including strong results on the Arcade Learning Environment benchmark.},
	urldate = {2022-02-07},
	journal = {arXiv:2106.08229 [cs]},
	author = {Castro, Pablo Samuel and Kastner, Tyler and Panangaden, Prakash and Rowland, Mark},
	month = jan,
	year = {2022},
	note = {arXiv: 2106.08229},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Published at NeurIPS 2021},
	annote = {Learn state representations based on a measure of state similarity. Show that this leads to good representation empirically and theoretical analysis given.},
}

@misc{castro_scalable_2019,
	title = {Scalable methods for computing state similarity in deterministic {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/1911.09291},
	doi = {10.48550/arXiv.1911.09291},
	abstract = {We present new algorithms for computing and approximating bisimulation metrics in Markov Decision Processes (MDPs). Bisimulation metrics are an elegant formalism that capture behavioral equivalence between states and provide strong theoretical guarantees on differences in optimal behaviour. Unfortunately, their computation is expensive and requires a tabular representation of the states, which has thus far rendered them impractical for large problems. In this paper we present a new version of the metric that is tied to a behavior policy in an MDP, along with an analysis of its theoretical properties. We then present two new algorithms for approximating bisimulation metrics in large, deterministic MDPs. The first does so via sampling and is guaranteed to converge to the true metric. The second is a differentiable loss which allows us to learn an approximation even for continuous state MDPs, which prior to this work had not been possible.},
	urldate = {2022-12-20},
	publisher = {arXiv},
	author = {Castro, Pablo Samuel},
	month = nov,
	year = {2019},
	note = {arXiv:1911.09291 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)},
}



@inproceedings{chang_learning_2022,
	title = {Learning {Bellman} {Complete} {Representations} for {Offline} {Policy} {Evaluation}},
	url = {https://proceedings.mlr.press/v162/chang22b.html},
	abstract = {We study representation learning for Offline Reinforcement Learning (RL), focusing on the important task of Offline Policy Evaluation (OPE). Recent work shows that, in contrast to supervised learning, realizability of the Q-function is not enough for learning it. Two sufficient conditions for sample-efficient OPE are Bellman completeness and coverage. Prior work often assumes that representations satisfying these conditions are given, with results being mostly theoretical in nature. In this work, we propose BCRL, which directly learns from data an approximately linear Bellman complete representation with good coverage. With this learned representation, we perform OPE using Least Square Policy Evaluation (LSPE) with linear functions in our learned representation. We present an end-to-end theoretical analysis, showing that our two-stage algorithm enjoys polynomial sample complexity provided some representation in the rich class considered is linear Bellman complete. Empirically, we extensively evaluate our algorithm on challenging, image-based continuous control tasks from the Deepmind Control Suite. We show our representation enables better OPE compared to previous representation learning methods developed for off-policy RL (e.g., CURL, SPR). BCRL achieve competitive OPE error with the state-of-the-art method Fitted Q-Evaluation (FQE), and beats FQE when evaluating beyond the initial state distribution. Our ablations show that both linear Bellman complete and coverage components of our method are crucial.},
	language = {en},
	urldate = {2022-10-10},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chang, Jonathan and Wang, Kaiwen and Kallus, Nathan and Sun, Wen},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {2938--2971},
}



@inproceedings{yang_representation_2021,
	title = {Representation {Matters}: {Offline} {Pretraining} for {Sequential} {Decision} {Making}},
	shorttitle = {Representation {Matters}},
	url = {https://proceedings.mlr.press/v139/yang21h.html},
	abstract = {The recent success of supervised learning methods on ever larger offline datasets has spurred interest in the reinforcement learning (RL) field to investigate whether the same paradigms can be translated to RL algorithms. This research area, known as offline RL, has largely focused on offline policy optimization, aiming to find a return-maximizing policy exclusively from offline data. In this paper, we consider a slightly different approach to incorporating offline data into sequential decision-making. We aim to answer the question, what unsupervised objectives applied to offline datasets are able to learn state representations which elevate performance on downstream tasks, whether those downstream tasks be online RL, imitation learning from expert demonstrations, or even offline policy optimization based on the same offline dataset? Through a variety of experiments utilizing standard offline RL datasets, we find that the use of pretraining with unsupervised learning objectives can dramatically improve the performance of policy learning algorithms that otherwise yield mediocre performance on their own. Extensive ablations further provide insights into what components of these unsupervised objectives \{–\} e.g., reward prediction, continuous or discrete representations, pretraining or finetuning \{–\} are most important and in which settings.},
	language = {en},
	urldate = {2023-01-03},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Mengjiao and Nachum, Ofir},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11784--11794},
}




@article{wang_fqedivergence_2020,
  author       = {Ruosong Wang and
                  Dean P. Foster and
                  Sham M. Kakade},
  title        = {What are the Statistical Limits of Offline {RL} with Linear Function
                  Approximation?},
  journal      = {CoRR},
  volume       = {abs/2010.11895},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.11895},
  eprinttype    = {arXiv},
  eprint       = {2010.11895},
  timestamp    = {Tue, 27 Oct 2020 11:22:08 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-11895.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{wang_instabilities_2021,
	title = {Instabilities of {Offline} {RL} with {Pre}-{Trained} {Neural} {Representation}},
	url = {https://proceedings.mlr.press/v139/wang21z.html},
	abstract = {In offline reinforcement learning (RL), we seek to utilize offline data to evaluate (or learn) policies in scenarios where the data are collected from a distribution that substantially differs from that of the target policy to be evaluated. Recent theoretical advances have shown that such sample-efficient offline RL is indeed possible provided certain strong representational conditions hold, else there are lower bounds exhibiting exponential error amplification (in the problem horizon) unless the data collection distribution has only a mild distribution shift relative to the target policy. This work studies these issues from an empirical perspective to gauge how stable offline RL methods are. In particular, our methodology explores these ideas when using features from pre-trained neural networks, in the hope that these representations are powerful enough to permit sample efficient offline RL. Through extensive experiments on a range of tasks, we see that substantial error amplification does occur even when using such pre-trained representations (trained on the same task itself); we find offline RL is stable only under extremely mild distribution shift. The implications of these results, both from a theoretical and an empirical perspective, are that successful offline RL (where we seek to go beyond the low distribution shift regime) requires substantially stronger conditions beyond those which suffice for successful supervised learning.},
	language = {en},
	urldate = {2022-06-29},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wang, Ruosong and Wu, Yifan and Salakhutdinov, Ruslan and Kakade, Sham},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10948--10960},
}


@inproceedings{pavse_scaling_2023,
	title = {Scaling {Marginalized} {Importance} {Sampling} to {High}-{Dimensional} {State}-{Spaces} via {State} {Abstraction}},
	abstract = {We consider the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of an evaluation policy, πe, using a fixed dataset, D, collected by one or more policies that may be different from πe. Current OPE algorithms may produce poor OPE estimates under policy distribution shift i.e., when the probability of a particular stateaction pair occurring under πe is very different from the probability of that same pair occurring in D (Voloshin et al. 2021; Fu et al. 2021). In this work, we propose to improve the accuracy of OPE estimators by projecting the high-dimensional state-space into a low-dimensional state-space using concepts from the state abstraction literature. Specifically, we consider marginalized importance sampling (MIS) OPE algorithms which compute state-action distribution correction ratios to produce their OPE estimate. In the original ground statespace, these ratios may have high variance which may lead to high variance OPE. However, we prove that in the lower-dimensional abstract state-space the ratios can have lower variance resulting in lower variance OPE. We then highlight the challenges that arise when estimating the abstract ratios from data, identify sufficient conditions to overcome these issues, and present a minimax optimization problem whose solution yields these abstract ratios. Finally, our empirical evaluation on difficult, high-dimensional state-space OPE tasks shows that the abstract ratios can make MIS OPE estimators achieve lower mean-squared error and more robust to hyperparameter tuning than the ground ratios.},
	language = {en},
	author = {Pavse, Brahma S and Hanna, Josiah P},
	year = {2023},
	file = {Pavse and Hanna - Scaling Marginalized Importance Sampling to High-D.pdf:/Users/jphanna/Zotero/storage/SY43SCYL/Pavse and Hanna - Scaling Marginalized Importance Sampling to High-D.pdf:application/pdf},
}


@misc{zhang_learning_2021,
	title = {Learning {Invariant} {Representations} for {Reinforcement} {Learning} without {Reconstruction}},
	url = {http://arxiv.org/abs/2006.10742},
	abstract = {We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Zhang, Amy and McAllister, Rowan and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
	month = apr,
	year = {2021},
	note = {arXiv:2006.10742 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted as an oral at ICLR 2021},
}


@techreport{gelada_deepmdp_2019,
	title = {{DeepMDP}: {Learning} {Continuous} {Latent} {Space} {Models} for {Representation} {Learning}},
	shorttitle = {{DeepMDP}},
	url = {http://arxiv.org/abs/1906.02736},
	abstract = {Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a DeepMDP, a parameterized latent space model that is trained via the minimization of two tractable losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the latent space as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. We connect these results to prior work in the bisimulation literature, and explore the use of a variety of metrics. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.},
	number = {arXiv:1906.02736},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Gelada, Carles and Kumar, Saurabh and Buckman, Jacob and Nachum, Ofir and Bellemare, Marc G.},
	month = jun,
	year = {2019},
	note = {arXiv:1906.02736 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 13 pages main text, 16 pages appendix. ICML 2019},
}



@article{li_towards_2006,
	title = {Towards a {Uniﬁed} {Theory} of {State} {Abstraction} for {MDPs}},
	abstract = {State abstraction (or state aggregation) has been extensively studied in the ﬁelds of artiﬁcial intelligence and operations research. Instead of working in the ground state space, the decision maker usually ﬁnds solutions in the abstract state space much faster by treating groups of states as a unit by ignoring irrelevant state information. A number of abstractions have been proposed and studied in the reinforcement-learning and planning literatures, and positive and negative results are known. We provide a uniﬁed treatment of state abstraction for Markov decision processes. We study ﬁve particular abstraction schemes, some of which have been proposed in the past in diﬀerent forms, and analyze their usability for planning and learning.},
	language = {en},
	author = {Li, Lihong and Walsh, Thomas J and Littman, Michael L},
	year = {2006},
	pages = {10},
	file = {Li et al. - Towards a Uniﬁed Theory of State Abstraction for M.pdf:/Users/jphanna/Zotero/storage/GLGS56F8/Li et al. - Towards a Uniﬁed Theory of State Abstraction for M.pdf:application/pdf},
}



@article{theocharous_personalized_2015,
	title = {Personalized {Ad} {Recommendation} {Systems} for {Life}-{Time} {Value} {Optimization} with {Guarantees}},
	abstract = {In this paper, we propose a framework for using reinforcement learning (RL) algorithms to learn good policies for personalized ad recommendation (PAR) systems. The RL algorithms take into account the long-term effect of an action, and thus, could be more suitable than myopic techniques like supervised learning and contextual bandit, for modern PAR systems in which the number of returning visitors is rapidly growing. However, while myopic techniques have been well-studied in PAR systems, the RL approach is still in its infancy, mainly due to two fundamental challenges: how to compute a good RL strategy and how to evaluate a solution using historical data to ensure its “safety” before deployment. In this paper, we propose to use a family of off-policy evaluation techniques with statistical guarantees to tackle both these challenges. We apply these methods to a real PAR problem, both for evaluating the ﬁnal performance and for optimizing the parameters of the RL algorithm. Our results show that a RL algorithm equipped with these offpolicy evaluation techniques outperforms the myopic approaches. Our results also give fundamental insights on the difference between the click through rate (CTR) and life-time value (LTV) metrics for evaluating the performance of a PAR algorithm.},
	language = {en},
	author = {Theocharous, Georgios and Thomas, Philip S and Ghavamzadeh, Mohammad},
	year = {2015},
	pages = {7},
	file = {Theocharous et al. - Personalized Ad Recommendation Systems for Life-Ti.pdf:/Users/jphanna/Zotero/storage/E8J94KBY/Theocharous et al. - Personalized Ad Recommendation Systems for Life-Ti.pdf:application/pdf},
}


@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00115009},
	doi = {10.1007/BF00115009},
	abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
	language = {en},
	number = {1},
	urldate = {2023-04-17},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	keywords = {connectionism, credit assignment, evaluation functions, Incremental learning, prediction},
	pages = {9--44},
}


@inproceedings{riedmiller_neural_2005,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Fitted} {Q} {Iteration} – {First} {Experiences} with a {Data} {Efficient} {Neural} {Reinforcement} {Learning} {Method}},
	isbn = {978-3-540-31692-3},
	doi = {10.1007/11564096_32},
	abstract = {This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.},
	language = {en},
	booktitle = {Machine {Learning}: {ECML} 2005},
	publisher = {Springer},
	author = {Riedmiller, Martin},
	editor = {Gama, João and Camacho, Rui and Brazdil, Pavel B. and Jorge, Alípio Mário and Torgo, Luís},
	year = {2005},
	pages = {317--328},
}



@article{ernst_tree-based_nodate,
	title = {Tree-{Based} {Batch} {Mode} {Reinforcement} {Learning}},
	abstract = {Reinforcement learning aims to determine an optimal control policy from interaction with a system or from observations gathered from a system. In batch mode, it can be achieved by approximating the so-called Q-function based on a set of four-tuples (xt , ut , rt , xt+1) where xt denotes the system state at time t, ut the control action taken, rt the instantaneous reward obtained and xt+1 the successor state of the system, and by determining the control policy from this Q-function. The Q-function approximation may be obtained from the limit of a sequence of (batch mode) supervised learning problems. Within this framework we describe the use of several classical tree-based supervised learning methods (CART, Kd-tree, tree bagging) and two newly proposed ensemble algorithms, namely extremely and totally randomized trees. We study their performances on several examples and ﬁnd that the ensemble methods based on regression trees perform well in extracting relevant information about the optimal control policy from sets of four-tuples. In particular, the totally randomized trees give good results while ensuring the convergence of the sequence, whereas by relaxing the convergence constraint even better accuracy results are provided by the extremely randomized trees.},
	language = {en},
	author = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
	file = {Ernst et al. - Tree-Based Batch Mode Reinforcement Learning.pdf:/Users/jphanna/Zotero/storage/EZHZKR99/Ernst et al. - Tree-Based Batch Mode Reinforcement Learning.pdf:application/pdf},
}



@inproceedings{antos_fitted_2007,
	title = {Fitted {Q}-iteration in continuous action-space {MDPs}},
	volume = {20},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/hash/da0d1111d2dc5d489242e60ebcbaf988-Abstract.html},
	abstract = {We consider continuous state, continuous action batch reinforcement learning where the goal is to learn a good policy from a sufficiently rich trajectory generated by another policy. We study a variant of fitted Q-iteration, where the greedy action selection is replaced by searching for a policy in a restricted set of candidate policies by maximizing the average action values. We provide a rigorous theoretical analysis of this algorithm, proving what we believe is the first finite-time bounds for value-function based algorithms for continuous state- and action-space problems.},
	urldate = {2023-04-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Antos, András and Szepesvári, Csaba and Munos, Rémi},
	year = {2007},
}


@article{liu_breaking_2018,
	title = {Breaking the {Curse} of {Horizon}: {Infinite}-{Horizon} {Off}-{Policy} {Estimation}},
	shorttitle = {Breaking the {Curse} of {Horizon}},
	url = {http://arxiv.org/abs/1810.12429},
	abstract = {We consider the off-policy estimation problem of estimating the expected reward of a target policy using samples collected by a different behavior policy. Importance sampling (IS) has been a key technique to derive (nearly) unbiased estimators, but is known to suffer from an excessively high variance in long-horizon problems. In the extreme case of in infinite-horizon problems, the variance of an IS-based estimator may even be unbounded. In this paper, we propose a new off-policy estimation method that applies IS directly on the stationary state-visitation distributions to avoid the exploding variance issue faced by existing estimators.Our key contribution is a novel approach to estimating the density ratio of two stationary distributions, with trajectories sampled from only the behavior distribution. We develop a mini-max loss function for the estimation problem, and derive a closed-form solution for the case of RKHS. We support our method with both theoretical and empirical analyses.},
	urldate = {2022-02-02},
	journal = {arXiv:1810.12429 [cs, stat]},
	author = {Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.12429},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: 21 pages, 5 figures, NIPS 2018 (spotlight)},
	annote = {Estimate state distribution visitations for OPE. Method relies on samples from the stationary distribution of the behavior policy. Extends OPE to long horizons.},
	file = {arXiv Fulltext PDF:/Users/jphanna/Zotero/storage/GYR8KJMF/Liu et al. - 2018 - Breaking the Curse of Horizon Infinite-Horizon Of.pdf:application/pdf;arXiv.org Snapshot:/Users/jphanna/Zotero/storage/N4P6JQ2V/1810.html:text/html},
}


@article{yang_off-policy_2020,
	title = {Off-{Policy} {Evaluation} via the {Regularized} {Lagrangian}},
	url = {http://arxiv.org/abs/2007.03438},
	abstract = {The recently proposed distribution correction estimation (DICE) family of estimators has advanced the state of the art in off-policy evaluation from behavior-agnostic data. While these estimators all perform some form of stationary distribution correction, they arise from different derivations and objective functions. In this paper, we unify these estimators as regularized Lagrangians of the same linear program. The unification allows us to expand the space of DICE estimators to new alternatives that demonstrate improved performance. More importantly, by analyzing the expanded space of estimators both mathematically and empirically we find that dual solutions offer greater flexibility in navigating the tradeoff between optimization stability and estimation bias, and generally provide superior estimates in practice.},
	urldate = {2022-03-22},
	journal = {arXiv:2007.03438 [cs, math, stat]},
	author = {Yang, Mengjiao and Nachum, Ofir and Dai, Bo and Li, Lihong and Schuurmans, Dale},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.03438},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}


@misc{uehara_minimax_2020,
	title = {Minimax {Weight} and {Q}-{Function} {Learning} for {Off}-{Policy} {Evaluation}},
	url = {http://arxiv.org/abs/1910.12809},
	abstract = {We provide theoretical investigations into off-policy evaluation in reinforcement learning using function approximators for (marginalized) importance weights and value functions. Our contributions include: (1) A new estimator, MWL, that directly estimates importance ratios over the state-action distributions, removing the reliance on knowledge of the behavior policy as in prior work (Liu et al., 2018). (2) Another new estimator, MQL, obtained by swapping the roles of importance weights and value-functions in MWL. MQL has an intuitive interpretation of minimizing average Bellman errors and can be combined with MWL in a doubly robust manner. (3) Several additional results that offer further insights into these methods, including the sample complexity analyses of MWL and MQL, their asymptotic optimality in the tabular setting, how the learned importance weights depend the choice of the discriminator class, and how our methods provide a unified view of some old and new algorithms in RL.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
	month = oct,
	year = {2020},
	note = {Number: arXiv:1910.12809
arXiv:1910.12809 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}


@article{thomas_high-confidence_nodate,
	title = {High-{Confidence} {Off}-{Policy} {Evaluation}},
	abstract = {Many reinforcement learning algorithms use trajectories collected from the execution of one or more policies to propose a new policy. Because execution of a bad policy can be costly or dangerous, techniques for evaluating the performance of the new policy without requiring its execution have been of recent interest in industry. Such off-policy evaluation methods, which estimate the performance of a policy using trajectories collected from the execution of other policies, heretofore have not provided conﬁdences regarding the accuracy of their estimates. In this paper we propose an off-policy method for computing a lower conﬁdence bound on the expected return of a policy.},
	language = {en},
	author = {Thomas, Philip S and Theocharous, Georgios and Ghavamzadeh, Mohammad},
	pages = {7},
	file = {Thomas et al. - High-Confidence Off-Policy Evaluation.pdf:/Users/jphanna/Zotero/storage/VVSGCS8X/Thomas et al. - High-Confidence Off-Policy Evaluation.pdf:application/pdf},
}


@misc{liu_representation_2019,
	title = {Representation {Balancing} {MDPs} for {Off}-{Policy} {Policy} {Evaluation}},
	url = {http://arxiv.org/abs/1805.09044},
	doi = {10.48550/arXiv.1805.09044},
	abstract = {We study the problem of off-policy policy evaluation (OPPE) in RL. In contrast to prior work, we consider how to estimate both the individual policy value and average policy value accurately. We draw inspiration from recent work in causal reasoning, and propose a new finite sample generalization error bound for value estimates from MDP models. Using this upper bound as an objective, we develop a learning algorithm of an MDP model with a balanced representation, and show that our approach can yield substantially lower MSE in common synthetic benchmarks and a HIV treatment simulation domain.},
	urldate = {2022-08-11},
	publisher = {arXiv},
	author = {Liu, Yao and Gottesman, Omer and Raghu, Aniruddh and Komorowski, Matthieu and Faisal, Aldo and Doshi-Velez, Finale and Brunskill, Emma},
	month = apr,
	year = {2019},
	note = {arXiv:1805.09044 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: appeared at NeurIPS 18; updated style file},
}


@inproceedings{jiang_doubly_2016,
	title = {Doubly {Robust} {Off}-policy {Value} {Evaluation} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.03722},
	abstract = {We study the problem of off-policy value evaluation in reinforcement learning (RL), where one aims to estimate the value of a new policy based on data collected by a different policy. This problem is often a critical step when applying RL in real-world problems. Despite its importance, existing general methods either have uncontrolled bias or suffer high variance. In this work, we extend the doubly robust estimator for bandits to sequential decision-making problems, which gets the best of both worlds: it is guaranteed to be unbiased and can have a much lower variance than the popular importance sampling estimators. We demonstrate the estimator's accuracy in several benchmark problems, and illustrate its use as a subroutine in safe policy improvement. We also provide theoretical results on the hardness of the problem, and show that our estimator can match the lower bound in certain scenarios.},
	urldate = {2022-02-02},
	author = {Jiang, Nan and Li, Lihong},
	month = may,
	year = {2016},
	note = {arXiv: 1511.03722},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Methodology},
	annote = {Comment: 14 pages; 4 figures; ICML 2016},
	annote = {Extend doubly robust estimator from bandits to MDPs. Lower bound on MSE of unbiased estimators for MDPs for tree and DAG MDPs},
	file = {arXiv Fulltext PDF:/Users/jphanna/Zotero/storage/7WFMRD6S/Jiang and Li - 2016 - Doubly Robust Off-policy Value Evaluation for Rein.pdf:application/pdf;arXiv.org Snapshot:/Users/jphanna/Zotero/storage/23WRR8HM/1511.html:text/html},
}


@article{hanna_importance_2021,
	title = {Importance {Sampling} in {Reinforcement} {Learning} with an {Estimated} {Behavior} {Policy}},
	volume = {110},
	abstract = {In reinforcement learning, importance sampling is a widely used method for evaluating an expectation under the distribution of data of one policy when the data has in fact been generated by a different policy. Importance sampling requires computing the likelihood ratio between the action probabilities of a target policy and those of the data-producing behavior policy. In this article, we study importance sampling where the behavior policy action probabilities are replaced by their maximum likelihood estimate of these probabilities under the observed data. We show this general technique reduces variance due to sampling error in Monte Carlo style estimators. We introduce two novel estimators that use this technique to estimate expected values that arise in the RL literature. We find that these general estimators reduce the variance of Monte Carlo sampling methods, leading to faster learning for policy gradient algorithms and more accurate off-policy policy evaluation. We also provide theoretical analysis showing that our new estimators are consistent and have asymptotically lower variance than Monte Carlo estimators.},
	number = {6},
	journal = {Machine Learning (MLJ)},
	author = {Hanna, Josiah P. and Niekum, Scott and Stone, Peter},
	month = may,
	year = {2021},
	pages = {1267--1317},
}


@inproceedings{hanna_bootstrapping_2017,
	title = {Bootstrapping with {Models}: {Confidence} {Intervals} for {Off}-{Policy} {Evaluation}},
	abstract = {For an autonomous agent, executing a poor policy may be costly or even dangerous. For such agents, it is desirable to determine confidence interval lower bounds on the performance of any given policy without executing said policy. Current methods for exact high confidence off-policy evaluation that use importance sampling require a substantial amount of data to achieve a tight lower bound. Existing model-based methods only address the problem in discrete state spaces. Since exact bounds are intractable for many domains we trade off strict guarantees of safety for more data-efficient approximate bounds. In this context, we propose two bootstrapping off-policy evaluation methods which use learned MDP transition models in order to estimate lower confidence bounds on policy performance with limited data in both continuous and discrete state spaces. Since direct use of a model may introduce bias, we derive a theoretical upper bound on model bias for when the model transition function is estimated with i.i.d. trajectories. This bound broadens our understanding of the conditions under which model-based methods have high bias. Finally, we empirically evaluate our proposed methods and analyze the settings in which different bootstrapping off-policy confidence interval methods succeed and fail.},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems} ({AAMAS})},
	author = {Hanna, Josiah and Stone, Peter and Niekum, Scott},
	month = may,
	year = {2017},
	note = {event-place: Sao Paolo, Brazil},
}


@inproceedings{farajtabar_more_2018,
	title = {More {Robust} {Doubly} {Robust} {Off}-policy {Evaluation}},
	url = {https://proceedings.mlr.press/v80/farajtabar18a.html},
	abstract = {We study the problem of off-policy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the performance of a policy from the data generated by another policy(ies). In particular, we focus on the doubly robust (DR) estimators that consist of an importance sampling (IS) component and a performance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge impact on the overall performance of DR, most of the work on using the DR estimators in OPE has been focused on improving the IS part, and not much on how to learn the model. In this paper, we propose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model parameter by minimizing the variance of the DR estimator. We first present a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t. the model parameters can be estimated from the samples, and propose methods to efficiently minimize the variance. We prove that the MRDR estimators are strongly consistent and asymptotically optimal. Finally, we evaluate MRDR in bandits and RL benchmark problems, and compare its performance with the existing methods.},
	language = {en},
	urldate = {2023-04-17},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1447--1456},
}


@misc{thomas_data-efficient_2016,
	title = {Data-{Efficient} {Off}-{Policy} {Policy} {Evaluation} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1604.00923},
	doi = {10.48550/arXiv.1604.00923},
	abstract = {In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods---it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang and Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.},
	urldate = {2023-04-17},
	publisher = {arXiv},
	author = {Thomas, Philip S. and Brunskill, Emma},
	month = apr,
	year = {2016},
	note = {arXiv:1604.00923 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}


@article{zhang_autoregressive_2021,
	title = {{AUTOREGRESSIVE} {DYNAMICS} {MODELS} {FOR} {OFFLINE} {POLICY} {EVALUATION} {AND} {OPTIMIZATION}},
	abstract = {Standard dynamics models for continuous control make use of feedforward computation to predict the conditional distribution of next state and reward given current state and action using a multivariate Gaussian with a diagonal covariance structure. This modeling choice assumes that different dimensions of the next state and reward are conditionally independent given the current state and action and may be driven by the fact that fully observable physics-based simulation environments entail deterministic transition dynamics. In this paper, we challenge this conditional independence assumption and propose a family of expressive autoregressive dynamics models that generate different dimensions of the next state and reward sequentially conditioned on previous dimensions. We demonstrate that autoregressive dynamics models indeed outperform standard feedforward models in log-likelihood on heldout transitions. Furthermore, we compare different model-based and model-free off-policy evaluation (OPE) methods on RL Unplugged, a suite of ofﬂine MuJoCo datasets, and ﬁnd that autoregressive dynamics models consistently outperform all baselines, achieving a new state-of-the-art. Finally, we show that autoregressive dynamics models are useful for ofﬂine policy optimization by serving as a way to enrich the replay buffer through data augmentation and improving performance using model-based planning.},
	language = {en},
	author = {Zhang, Michael R and Paine, Tom Le and Nachum, Oﬁr and Paduraru, Cosmin and Tucker, George and Wang, Ziyu and Norouzi, Mohammad},
	year = {2021},
	file = {Zhang et al. - 2021 - AUTOREGRESSIVE DYNAMICS MODELS FOR OFFLINE POLICY .pdf:/Users/jphanna/Zotero/storage/4BICXY24/Zhang et al. - 2021 - AUTOREGRESSIVE DYNAMICS MODELS FOR OFFLINE POLICY .pdf:application/pdf},
}



@article{precup_off-policy_nodate,
	title = {Off-{Policy} {Temporal}-{Difference} {Learning} with {Function} {Approximation}},
	abstract = {We introduce the ﬁrst algorithm for off-policy temporal-difference learning that is stable with linear function approximation. Off-policy learning is of interest because it forms the basis for popular reinforcement learning methods such as Q-learning, which has been known to diverge with linear function approximation, and because it is critical to the practical utility of multi-scale, multi-goal, learning frameworks such as options, HAMs, and MAXQ. Our new algorithm combines TD(λ) over state–action pairs with importance sampling ideas from our previous work. We prove that, given training under any ǫ-soft policy, the algorithm converges w.p.1 to a close approximation (as in Tsitsiklis and Van Roy, 1997; Tadic, 2001) to the action-value function for an arbitrary target policy. Variations of the algorithm designed to reduce variance introduce additional bias but are also guaranteed convergent. We also illustrate our method empirically on a small policy evaluation problem. Our current results are limited to episodic tasks with episodes of bounded length.},
	language = {en},
	author = {Precup, Doina and Sutton, Richard S and Dasgupta, Sanjoy},
	file = {Precup et al. - Off-Policy Temporal-Difference Learning with Funct.pdf:/Users/jphanna/Zotero/storage/KT363HTE/Precup et al. - Off-Policy Temporal-Difference Learning with Funct.pdf:application/pdf},
}
