\begin{thebibliography}{10}

\bibitem{anandkumar2014tensor}
A.~Anandkumar, R.~Ge, D.~Hsu, S.~M. Kakade, and M.~Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock {\em Journal of Machine Learning Research}, 15(1):2773--2832, 2014.

\bibitem{anandkumar2015sample}
A.~Anandkumar, R.~Ge, and M.~Janzamin.
\newblock Learning overcomplete latent variable models through tensor methods.
\newblock In {\em Proc. of COLT}, 2015.

\bibitem{anandkumar2012spectral}
A.~Anandkumar, Y.-k. Liu, D.~J. Hsu, D.~P. Foster, and S.~M. Kakade.
\newblock A spectral algorithm for latent dirichlet allocation.
\newblock In {\em NIPS}, 2012.

\bibitem{DBLP:journals/corr/Azizzadenesheli16}
K.~Azizzadenesheli, A.~Lazaric, and A.~Anandkumar.
\newblock Reinforcement learning of {POMDP}'s using spectral methods.
\newblock In {\em COLT}, 2016.

\bibitem{bader2006algorithm}
B.~W. Bader and T.~G. Kolda.
\newblock Algorithm 862: Matlab tensor classes for fast algorithm prototyping.
\newblock {\em ACM Transactions on Mathematical Software}, 32(4):635--653,
  2006.

\bibitem{balcan2016improved}
M.-F. Balcan, S.~Du, Y.~Wang, and A.~W. Yu.
\newblock An improved gap-dependency analysis of the noisy power method.
\newblock In {\em COLT}, 2016.

\bibitem{birge2001alternative}
L.~Birg{\'e}.
\newblock An alternative point of view on lepski's method.
\newblock {\em Lecture Notes-Monograph Series}, pages 113--133, 2001.

\bibitem{cirel1976norms}
B.~Cirel'soN, I.~Ibragimov, and V.~Sudakov.
\newblock Norms of gaussian sample functions.
\newblock {\em Lecture Notes in Mathematics}, 550:20--41, 1976.

\bibitem{dwork2014algorithmic}
C.~Dwork and A.~Roth.
\newblock The algorithmic foundations of differential privacy.
\newblock {\em Foundations and Trends in Theoretical Computer Science},
  9(3-4):211--407, 2014.

\bibitem{dwork2014analyze}
C.~Dwork, K.~Talwar, A.~Thakurta, and L.~Zhang.
\newblock Analyze gauss: optimal bounds for privacy-preserving principal
  component analysis.
\newblock In {\em STOC}, 2014.

\bibitem{ge2015escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle points---online stochastic gradient for tensor
  decomposition.
\newblock In {\em COLT}, 2015.

\bibitem{hardt2014noisy}
M.~Hardt and E.~Price.
\newblock The noisy power method: A meta algorithm with applications.
\newblock In {\em NIPS}, 2014.

\bibitem{hillar2013most}
C.~J. Hillar and L.-H. Lim.
\newblock Most tensor problems are np-hard.
\newblock {\em Journal of the ACM (JACM)}, 60(6):45, 2013.

\bibitem{hopkins2015tensor}
S.~B. Hopkins, J.~Shi, and D.~Steurer.
\newblock Tensor principal component analysis via sum-of-squares proofs.
\newblock In {\em COLT}, 2015.

\bibitem{hsu2012tail}
D.~Hsu, S.~M. Kakade, and T.~Zhang.
\newblock A tail inequality for quadratic forms of subgaussian random vectors.
\newblock {\em Electron. Commun. Probab}, 17(52):1--6, 2012.

\bibitem{huang2015online}
F.~Huang, U.~Niranjan, M.~U. Hakeem, and A.~Anandkumar.
\newblock Online tensor methods for learning latent variable models.
\newblock {\em Journal of Machine Learning Research}, 16:2797--2835, 2015.

\bibitem{huang2014scalable}
F.~Huang, I.~Perros, R.~Chen, J.~Sun, A.~Anandkumar, et~al.
\newblock Scalable latent tree model and its application to health analytics.
\newblock {\em arXiv preprint arXiv:1406.4566}, 2014.

\bibitem{janzamin2015generalization}
M.~Janzamin, H.~Sedghi, and A.~Anandkumar.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock {\em arXiv preprint arXiv:1506.08473}, 2015.

\bibitem{kamathbounds}
G.~Kamath.
\newblock Bounds on the expectation of the maximum of samples from a gaussian.
\newblock [Online; accessed April, 2016].

\bibitem{kolda2011shifted}
T.~G. Kolda and J.~R. Mayo.
\newblock Shifted power method for computing tensor eigenpairs.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  32(4):1095--1124, 2011.

\bibitem{kuleshov2015tensor}
V.~Kuleshov, A.~T. Chaganty, and P.~Liang.
\newblock Tensor factorization via matrix factorization.
\newblock In {\em AISTATS}, 2015.

\bibitem{laurent2000adaptive}
B.~Laurent and P.~Massart.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock {\em Annals of Statistics}, pages 1302--1338, 2000.

\bibitem{massart2007concentration}
P.~Massart.
\newblock {\em Concentration inequalities and model selection}, volume~6.
\newblock Springer, 2007.

\bibitem{montanari2014statistical}
A.~Montanari and E.~Richard.
\newblock A statistical model for tensor {PCA}.
\newblock In {\em NIPS}, 2014.

\bibitem{mu2015successive}
C.~Mu, D.~Hsu, and D.~Goldfarb.
\newblock Successive rank-one approximations for nearly orthogonally
  decomposable symmetric tensors.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  36(4):1638--1659, 2015.

\bibitem{stewart1990matrix}
G.~W. Stewart, J.-g. Sun, and H.~B. Jovanovich.
\newblock {\em Matrix perturbation theory}.
\newblock Academic press New York, 1990.

\bibitem{tomioka2014spectral}
R.~Tomioka and T.~Suzuki.
\newblock Spectral norm of random tensors.
\newblock {\em arXiv:1407.1870}, 2014.

\bibitem{wang2015fast}
Y.~Wang, H.-Y. Tung, A.~J. Smola, and A.~Anandkumar.
\newblock Fast and guaranteed tensor decomposition via sketching.
\newblock In {\em NIPS}, 2015.

\bibitem{wang2014spectral}
Y.~Wang and J.~Zhu.
\newblock Spectral methods for supervised topic models.
\newblock In {\em NIPS}, 2014.

\bibitem{zemel2013learning}
R.~Zemel, Y.~Wu, K.~Swersky, T.~Pitassi, and C.~Dwork.
\newblock Learning fair representations.
\newblock In {\em ICML}, 2013.

\end{thebibliography}
