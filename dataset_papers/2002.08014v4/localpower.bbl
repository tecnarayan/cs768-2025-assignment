\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu \& Li(2016)Allen-Zhu and Li]{allen2016lazysvd}
Allen-Zhu, Z. and Li, Y.
\newblock Lazysvd: even faster svd decomposition yet without agonizing pain.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  974--982, 2016.

\bibitem[Arbenz et~al.(2012)Arbenz, Kressner, and
  Z{\"u}rich]{arbenz2012lecture}
Arbenz, P., Kressner, D., and Z{\"u}rich, D.-M.~E.
\newblock Lecture notes on solving large scale eigenvalue problems.
\newblock \emph{D-MATH, EHT Zurich}, 2012.

\bibitem[Arora et~al.(2013)Arora, Cotter, and Srebro]{arora2013stochastic}
Arora, R., Cotter, A., and Srebro, N.
\newblock Stochastic optimization of pca with capped msg.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1815--1823, 2013.

\bibitem[Balcan et~al.(2016{\natexlab{a}})Balcan, Du, Wang, and
  Yu]{balcan2016improved}
Balcan, M.-F., Du, S.~S., Wang, Y., and Yu, A.~W.
\newblock An improved gap-dependency analysis of the noisy power method.
\newblock In \emph{Conference on Learning Theory}, pp.\  284--309,
  2016{\natexlab{a}}.

\bibitem[Balcan et~al.(2016{\natexlab{b}})Balcan, Liang, Song, Woodruff, and
  Xie]{balcan2016communication}
Balcan, M.~F., Liang, Y., Song, L., Woodruff, D., and Xie, B.
\newblock Communication efficient distributed kernel principal component
  analysis.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  725--734, 2016{\natexlab{b}}.

\bibitem[Bhaskara \& Wijewardena(2019)Bhaskara and
  Wijewardena]{bhaskara2019distributed}
Bhaskara, A. and Wijewardena, P.~M.
\newblock On distributed averaging for stochastic k-pca.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11024--11033, 2019.

\bibitem[Boutsidis et~al.(2016)Boutsidis, Woodruff, and
  Zhong]{boutsidis2016optimal}
Boutsidis, C., Woodruff, D.~P., and Zhong, P.
\newblock Optimal principal component analysis in distributed and streaming
  models.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pp.\  236--249. ACM, 2016.

\bibitem[Cand{\`e}s \& Recht(2009)Cand{\`e}s and Recht]{candes2009exact}
Cand{\`e}s, E.~J. and Recht, B.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Foundations of Computational mathematics}, 9\penalty0
  (6):\penalty0 717, 2009.

\bibitem[Cape(2020)]{cape2020orthogonal}
Cape, J.
\newblock Orthogonal procrustes and norm-dependent optimality.
\newblock \emph{The Electronic Journal of Linear Algebra}, 36\penalty0
  (36):\penalty0 158--168, 2020.

\bibitem[Charisopoulos et~al.(2020)Charisopoulos, Benson, and
  Damle]{charisopoulos2020communication}
Charisopoulos, V., Benson, A.~R., and Damle, A.
\newblock Communication-efficient distributed eigenspace estimation.
\newblock \emph{arXiv preprint arXiv:2009.02436}, 2020.

\bibitem[Chen et~al.(2020)Chen, Horvath, and Richtarik]{chen2020optimal}
Chen, W., Horvath, S., and Richtarik, P.
\newblock Optimal client sampling for federated learning.
\newblock \emph{arXiv preprint arXiv:2010.13723}, 2020.

\bibitem[Chen et~al.(2021)Chen, Lee, Li, and Yang]{chen2021distributed}
Chen, X., Lee, J.~D., Li, H., and Yang, Y.
\newblock Distributed estimation for principal component analysis: An enlarged
  eigenspace analysis.
\newblock \emph{Journal of the American Statistical Association}, pp.\  1--12,
  2021.

\bibitem[De~Sa et~al.(2018)De~Sa, He, Mitliagkas, R{\'e}, and
  Xu]{de2018accelerated}
De~Sa, C., He, B., Mitliagkas, I., R{\'e}, C., and Xu, P.
\newblock Accelerated stochastic power iteration.
\newblock \emph{Proceedings of machine learning research}, 84:\penalty0 58,
  2018.

\bibitem[Deerwester et~al.(1990)Deerwester, Dumais, Furnas, Landauer, and
  Harshman]{deerwester1990indexing}
Deerwester, S., Dumais, S.~T., Furnas, G.~W., Landauer, T.~K., and Harshman, R.
\newblock Indexing by latent semantic analysis.
\newblock \emph{Journal of the American society for information science},
  41\penalty0 (6):\penalty0 391--407, 1990.

\bibitem[Fan et~al.(2019)Fan, Wang, Wang, Zhu, et~al.]{fan2019distributed}
Fan, J., Wang, D., Wang, K., Zhu, Z., et~al.
\newblock Distributed estimation of principal eigenspaces.
\newblock \emph{The Annals of Statistics}, 47\penalty0 (6):\penalty0
  3009--3031, 2019.

\bibitem[Gang et~al.(2019)Gang, Raja, and Bajwa]{gang2019fast}
Gang, A., Raja, H., and Bajwa, W.~U.
\newblock Fast and communication-efficient distributed pca.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  7450--7454. IEEE, 2019.

\bibitem[Garber \& Hazan(2015)Garber and Hazan]{garber2015fast}
Garber, D. and Hazan, E.
\newblock Fast and simple pca via convex optimization.
\newblock \emph{arXiv preprint arXiv:1509.05647}, 2015.

\bibitem[Garber et~al.(2016)Garber, Hazan, Jin, Kakade, Musco, Netrapalli, and
  Sidford]{garber2016faster}
Garber, D., Hazan, E., Jin, C., Kakade, S.~M., Musco, C., Netrapalli, P., and
  Sidford, A.
\newblock Faster eigenvector computation via shift-and-invert preconditioning.
\newblock In \emph{ICML}, pp.\  2626--2634, 2016.

\bibitem[Garber et~al.(2017)Garber, Shamir, and
  Srebro]{garber2017communication}
Garber, D., Shamir, O., and Srebro, N.
\newblock Communication-efficient algorithms for distributed stochastic
  principal component analysis.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1203--1212. JMLR. org, 2017.

\bibitem[Gittens \& Mahoney(2016)Gittens and Mahoney]{gittens2013revisiting}
Gittens, A. and Mahoney, M.~W.
\newblock Revisiting the {N}ystr{\"o}m method for improved large-scale machine
  learning.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 3977--4041, 2016.

\bibitem[Gittens et~al.(2016)Gittens, Devarakonda, Racah, Ringenburg, Gerhardt,
  Kottalam, Liu, Maschhoff, Canon, and Chhugani]{gittens2016matrix}
Gittens, A., Devarakonda, A., Racah, E., Ringenburg, M., Gerhardt, L.,
  Kottalam, J., Liu, J., Maschhoff, K., Canon, S., and Chhugani, J.
\newblock Matrix factorizations at scale: a comparison of scientific data
  analytics in spark and {C+ MPI} using three case studies.
\newblock In \emph{IEEE International Conference on Big Data}, 2016.

\bibitem[Golub \& Van~Loan(2012)Golub and Van~Loan]{golub2012matrix}
Golub, G.~H. and Van~Loan, C.~F.
\newblock \emph{Matrix computations}, volume~3.
\newblock JHU Press, 2012.

\bibitem[Grammenos et~al.(2019)Grammenos, Mendoza-Smith, Mascolo, and
  Crowcroft]{grammenos2019federated}
Grammenos, A., Mendoza-Smith, R., Mascolo, C., and Crowcroft, J.
\newblock Federated principal component analysis.
\newblock \emph{arXiv preprint arXiv:1907.08059}, 2019.

\bibitem[Guo et~al.(2021)Guo, Li, Chang, Wang, and Zhang]{guo2021privacy}
Guo, X., Li, X., Chang, X., Wang, S., and Zhang, Z.
\newblock Privacy-preserving distributed svd via federated power.
\newblock \emph{arXiv preprint arXiv:2103.00704}, 2021.

\bibitem[Halko et~al.(2011)Halko, Martinsson, and Tropp]{halko2011finding}
Halko, N., Martinsson, P.-G., and Tropp, J.~A.
\newblock Finding structure with randomness: Probabilistic algorithms for
  constructing approximate matrix decompositions.
\newblock \emph{SIAM review}, 53\penalty0 (2):\penalty0 217--288, 2011.

\bibitem[Hardt \& Price(2014)Hardt and Price]{hardt2014noisy}
Hardt, M. and Price, E.
\newblock The noisy power method: A meta algorithm with applications.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2861--2869, 2014.

\bibitem[Ji-guang(1987)]{ji1987perturbation}
Ji-guang, S.
\newblock Perturbation of angles between linear subspaces.
\newblock \emph{Journal of Computational Mathematics}, pp.\  58--61, 1987.

\bibitem[Khaled et~al.(2019)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2019first}
Khaled, A., Mishchenko, K., and Richt{\'a}rik, P.
\newblock First analysis of local gd on heterogeneous data.
\newblock \emph{arXiv preprint arXiv:1909.04715}, 2019.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Huang, Yang, Wang, and
  Zhang]{li2019convergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Yang, Wang, and
  Zhang]{li2019communication}
Li, X., Yang, W., Wang, S., and Zhang, Z.
\newblock Communication efficient decentralized training with multiple local
  updates.
\newblock \emph{arXiv preprint arXiv:1910.09126}, 2019{\natexlab{b}}.

\bibitem[Liang et~al.(2014)Liang, Balcan, Kanchanapally, and
  Woodruff]{liang2014improved}
Liang, Y., Balcan, M.-F.~F., Kanchanapally, V., and Woodruff, D.
\newblock Improved distributed principal component analysis.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3113--3121, 2014.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Musco \& Musco(2015)Musco and Musco]{musco2015randomized}
Musco, C. and Musco, C.
\newblock Randomized block {K}rylov methods for stronger and faster approximate
  singular value decomposition.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2015.

\bibitem[Oja \& Karhunen(1985)Oja and Karhunen]{oja1985stochastic}
Oja, E. and Karhunen, J.
\newblock On stochastic approximation of the eigenvectors and eigenvalues of
  the expectation of a random matrix.
\newblock \emph{Journal of mathematical analysis and applications},
  106\penalty0 (1):\penalty0 69--84, 1985.

\bibitem[Raja \& Bajwa(2020)Raja and Bajwa]{raja2020distributed}
Raja, H. and Bajwa, W.~U.
\newblock Distributed stochastic algorithms for high-rate streaming principal
  component analysis.
\newblock \emph{arXiv preprint arXiv:2001.01017}, 2020.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Mokhtari, Hassani, Jadbabaie, and
  Pedarsani]{reisizadeh2020fedpaq}
Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R.
\newblock Fedpaq: A communication-efficient federated learning method with
  periodic averaging and quantization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2021--2031. PMLR, 2020.

\bibitem[Saad(2011)]{saad2011numerical}
Saad, Y.
\newblock Numerical methods for large eigenvalue problems.
\newblock \emph{preparation. Available from: http://www-users. cs. umn.
  edu/saad/books. html}, 2011.

\bibitem[Sch{\"o}nemann(1966)]{schonemann1966generalized}
Sch{\"o}nemann, P.~H.
\newblock A generalized solution of the orthogonal procrustes problem.
\newblock \emph{Psychometrika}, 31\penalty0 (1):\penalty0 1--10, 1966.

\bibitem[Shamir(2015)]{shamir2015stochastic}
Shamir, O.
\newblock A stochastic pca and svd algorithm with an exponential convergence
  rate.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  144--152, 2015.

\bibitem[Shamir(2016)]{shamir2016convergence}
Shamir, O.
\newblock Convergence of stochastic gradient descent for pca.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  257--265, 2016.

\bibitem[Simchowitz et~al.(2017)Simchowitz, Alaoui, and
  Recht]{simchowitz2017gap}
Simchowitz, M., Alaoui, A.~E., and Recht, B.
\newblock On the gap between strict-saddles and true convexity: An omega (log
  d) lower bound for eigenvector approximation.
\newblock \emph{arXiv preprint arXiv:1704.04548}, 2017.

\bibitem[Stich(2018)]{stich2018local}
Stich, S.~U.
\newblock Local {SGD} converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Sun(1995)]{sun1995perturbation}
Sun, J.-g.
\newblock On perturbation bounds for the qr factorization.
\newblock \emph{Linear algebra and its applications}, 215:\penalty0 95--111,
  1995.

\bibitem[Tropp(2012)]{tropp2012user}
Tropp, J.~A.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of computational mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Tropp(2015)]{tropp2015introduction}
Tropp, J.~A.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{arXiv preprint arXiv:1501.01571}, 2015.

\bibitem[Vu et~al.(2013)Vu, Lei, et~al.]{vu2013minimax}
Vu, V.~Q., Lei, J., et~al.
\newblock Minimax sparse principal subspace estimation in high dimensions.
\newblock \emph{The Annals of Statistics}, 41\penalty0 (6):\penalty0
  2905--2947, 2013.

\bibitem[Wang \& Joshi(2018{\natexlab{a}})Wang and Joshi]{wang2018adaptive}
Wang, J. and Joshi, G.
\newblock Adaptive communication strategies to achieve the best error-runtime
  trade-off in local-update sgd.
\newblock \emph{arXiv preprint arXiv:1810.08313}, 2018{\natexlab{a}}.

\bibitem[Wang \& Joshi(2018{\natexlab{b}})Wang and Joshi]{wang2018cooperative}
Wang, J. and Joshi, G.
\newblock Cooperative {SGD}: A unified framework for the design and analysis of
  communication-efficient {SGD} algorithms.
\newblock \emph{arXiv preprint arXiv:1808.07576}, 2018{\natexlab{b}}.

\bibitem[Wang et~al.(2016)Wang, Luo, and Zhang]{wang2016spsd}
Wang, S., Luo, L., and Zhang, Z.
\newblock {SPSD} matrix approximation vis column selection: Theories,
  algorithms, and extensions.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (49):\penalty0 1--49, 2016.

\bibitem[Wang et~al.(2019)Wang, Gittens, and Mahoney]{wang2019scalable}
Wang, S., Gittens, A., and Mahoney, M.~W.
\newblock Scalable kernel k-means clustering with {N}ystrom approximation:
  Relative-error bounds.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (12):\penalty0 1--49, 2019.

\bibitem[Wold et~al.(1987)Wold, Esbensen, and Geladi]{wold1987principal}
Wold, S., Esbensen, K., and Geladi, P.
\newblock Principal component analysis.
\newblock \emph{Chemometrics and intelligent laboratory systems}, 2\penalty0
  (1-3):\penalty0 37--52, 1987.

\bibitem[Woodruff(2014)]{woodruff2014sketching}
Woodruff, D.~P.
\newblock Sketching as a tool for numerical linear algebra.
\newblock \emph{arXiv preprint arXiv:1411.4357}, 2014.

\bibitem[Wu et~al.(2018)Wu, Wai, Li, and Scaglione]{wu2018review}
Wu, S.~X., Wai, H.-T., Li, L., and Scaglione, A.
\newblock A review of distributed algorithms for principal component analysis.
\newblock \emph{Proceedings of the IEEE}, 106\penalty0 (8):\penalty0
  1321--1340, 2018.

\bibitem[Xu(2018)]{xu2018gradient}
Xu, Z.
\newblock Gradient descent meets shift-and-invert preconditioning for
  eigenvector computation.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 2825--2834, 2018.

\bibitem[Ye \& Lim(2014)Ye and Lim]{ye2014distance}
Ye, K. and Lim, L.-H.
\newblock Distance between subspaces of different dimensions.
\newblock \emph{arXiv preprint arXiv:1407.0900}, 4, 2014.

\bibitem[Yu et~al.(2019)Yu, Yang, and Zhu]{yu2019parallel}
Yu, H., Yang, S., and Zhu, S.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2019.

\bibitem[Zhou \& Cong(2017)Zhou and Cong]{zhou2017convergence}
Zhou, F. and Cong, G.
\newblock On the convergence properties of a k-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1708.01012}, 2017.

\end{thebibliography}
