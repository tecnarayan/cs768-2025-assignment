\begin{thebibliography}{10}

\bibitem{topopt88}
Erik Andreassen, Anders Clausen, Mattias Schevenels, Boyan Lazarov, and Ole
  Sigmund.
\newblock Efficient topology optimization in matlab using 88 lines of code.
\newblock {\em Structural and Multidisciplinary Optimization}, 43:1--16, 11
  2011.

\bibitem{Arora}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em CoRR}, abs/1904.11955, 2019.

\bibitem{CNN_topoptim}
Saurabh Banga, Harsh Gehani, Sanket Bhilare, Sagar Patel, and Levent Kara.
\newblock 3d topology optimization using convolutional neural networks.
\newblock {\em CoRR}, abs/1808.07440, 2018.

\bibitem{topology}
Bendsoe and Sigmund.
\newblock Topology optimization: Theory, methods and applications.
\newblock {\em Springer Science and Business}, April 2013.

\bibitem{Bendsoe}
Martin Bendsøe.
\newblock Bendsoe, m.p.: Optimal shape design as a material distribution
  problem. structural optimization 1, 193-202.
\newblock {\em Structural Optimization}, 1:193--202, 01 1989.

\bibitem{Fourier_TOUNN}
Aaditya Chandrasekhar and K.~Suresh.
\newblock Length scale control in topology optimization using fourier enhanced
  neural networks.
\newblock 2020.

\bibitem{TOuNN}
Aaditya Chandrasekhar and Krishnan Suresh.
\newblock Tounn: Topology optimization using neural networks.
\newblock {\em Structural and Multidisciplinary Optimization}, 2020.

\bibitem{cholmod2}
Yanqing Chen, Timothy~A. Davis, and William~W. Hager.
\newblock Algorithm 887: Cholmod, supernodal sparse cholesky factorization and
  update/downdate.
\newblock {\em ACM Transactions on Mathematical Software}, pages 1--14, 2008.

\bibitem{toward_deeper}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock {\em CoRR}, abs/1602.05897, 2016.

\bibitem{cholmod}
Timothy~A. Davis.
\newblock User guide for cholmod: a sparse cholesky factorization and
  modification package.
\newblock 2009.

\bibitem{implicit_diff}
Andreas Griewank and Christèle Faure.
\newblock Reduced functions, gradients and hessians from fixed-point iterations
  for state equations.
\newblock {\em Numerical Algorithms}, 30:113--139, 06 2002.

\bibitem{neural_param}
Stephan Hoyer, Jascha Sohl{-}Dickstein, and Sam Greydanus.
\newblock Neural reparameterization improves structural optimization.
\newblock {\em CoRR}, abs/1909.04240, 2019.

\bibitem{hierarchy}
Jiaoyang Huang and Horng{-}Tzer Yau.
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock {\em CoRR}, abs/1909.08156, 2019.

\bibitem{NTK}
Arthur Jacot, Franck Gabriel, and Cl{\'{e}}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em CoRR}, abs/1806.07572, 2018.

\bibitem{order_chao}
Arthur Jacot, Franck Gabriel, and Cl{\'{e}}ment Hongler.
\newblock Order and chaos: {NTK} views on {DNN} normalization, checkerboard and
  boundary artifacts.
\newblock {\em CoRR}, abs/1907.05715, 2019.

\bibitem{metasurfaces}
Jiaqi Jiang and Jonathan~A. Fan.
\newblock Global optimization of dielectric metasurfaces using a physics-driven
  neural network.
\newblock {\em Nano Letters}, 19(8):5366–5372, Jul 2019.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock 2017.

\bibitem{wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Roman Novak,
  Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(12):124002, Dec 2020.

\bibitem{topopt3D}
Kai Liu and Andres Tovar.
\newblock An efficient 3d topology optimization code written in matlab.
\newblock {\em Structural and Multidisciplinary Optimization}, 50, 12 2014.

\bibitem{heat}
Gilles Marck, Maroun Nemer, Jean-Luc Harion, Serge Russeil, and Daniel
  Bougeard.
\newblock Topology optimization using the simp method for multiobjective
  conductive problems.
\newblock {\em Numerical Heat Transfer Part B-fundamentals - NUMER HEAT
  TRANSFER PT B-FUND}, 61:439--470, 06 2012.

\bibitem{differentiable_image_param}
Alexander Mordvintsev, Nicola Pezzotti, Ludwig Schubert, and Chris Olah.
\newblock Differentiable image parameterizations.
\newblock {\em Distill}, 2018.
\newblock https://distill.pub/2018/differentiable-parameterizations.

\bibitem{topologyGAN}
Zhenguo Nie, Tong Lin, Haoliang Jiang, and Levent~Burak Kara.
\newblock Topologygan: Topology optimization using generative adversarial
  networks based on physical fields over the initial domain.
\newblock {\em CoRR}, abs/2003.04685, 2020.

\bibitem{Chaos_Poole2016}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  3360--3368. Curran Associates, Inc., 2016.

\bibitem{random_features}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis, editors, {\em
  Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2008.

\bibitem{Rprop}
M.~Riedmiller and H.~Braun.
\newblock A direct adaptive method for faster backpropagation learning: the
  rprop algorithm.
\newblock In {\em IEEE International Conference on Neural Networks}, pages
  586--591 vol.1, 1993.

\bibitem{Rprop_full_batch}
Martin Riedmiller and Heinrich Braun.
\newblock A direct adaptive method for faster backpropagation learning: The
  rprop algorithm.
\newblock pages 586--591, 1993.

\bibitem{bochner}
W.~Rudin.
\newblock {\em Fourier Analysis on Groups}.
\newblock Wiley Classics Library. Wiley, 1990.

\bibitem{Deep_Info_Prop_Schoenholz2017}
Samuel~S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock 2017.

\bibitem{CGAN_topoptim}
M.{-}H.~Herman Shen and Liang Chen.
\newblock A new {CGAN} technique for constrained topology design optimization.
\newblock {\em CoRR}, abs/1901.07675, 2019.

\bibitem{filtering}
Ole Sigmund.
\newblock Morphology-based black and white filters for topology optimization.
\newblock {\em Structural and Multidisciplinary Optimization}, 33:401--424, 04
  2007.

\bibitem{periodic_activations}
Vincent Sitzmann, Julien N.~P. Martel, Alexander~W. Bergman, David~B. Lindell,
  and Gordon Wetzstein.
\newblock Implicit neural representations with periodic activation functions.
\newblock {\em CoRR}, abs/2006.09661, 2020.

\bibitem{neural_topoptim}
Ivan Sosnovik and Ivan~V. Oseledets.
\newblock Neural networks for topology optimization.
\newblock {\em CoRR}, abs/1709.09578, 2017.

\bibitem{Fourier_features}
Matthew Tancik, Pratul~P. Srinivasan, Ben Mildenhall, Sara Fridovich{-}Keil,
  Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan~T. Barron, and
  Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock {\em CoRR}, abs/2006.10739, 2020.

\bibitem{bicgstab}
H.~Vorst.
\newblock Bi-cgstab: A fast and smoothly converging variant of bi-cg for the
  solution of nonsymmetric linear systems.
\newblock {\em SIAM J. Sci. Comput.}, 13:631--644, 1992.

\bibitem{disentangling}
Lechao Xiao, Jeffrey Pennington, and Samuel~S. Schoenholz.
\newblock Disentangling trainability and generalization in deep learning.
\newblock {\em CoRR}, abs/1912.13053, 2019.

\bibitem{optimality_criteria}
Luzhong Yin and Wei Yang.
\newblock Optimality criteria method for topology optimization under multiple
  constraints.
\newblock {\em Computers and Structures}, 79(20):1839--1850, 2001.

\end{thebibliography}
