\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2022)Agarwal, Jiang, Kakade, and
  Sun]{agarwal2022reinforcement}
Alekh Agarwal, Nan Jiang, Sham~M. Kakade, and Wen Sun.
\newblock \emph{Reinforcement learning: Theory and algorithms}.
\newblock 2022.

\bibitem[Alexanderian(2009)]{alexanderian2009some}
Alen Alexanderian.
\newblock Some notes on asymptotic theory in probability.
\newblock \emph{Notes. University of Maryland}, 2009.

\bibitem[Arnold et~al.(2022)Arnold, L'Ecuyer, Chen, Chen, and
  Sha]{arnold2022policy}
S\'ebastien M.~R. Arnold, Pierre L'Ecuyer, Liyu Chen, Yi-fan Chen, and Fei Sha.
\newblock Policy learning and evaluation with randomized quasi-monte carlo.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2022.

\bibitem[Bouchard et~al.(2016)Bouchard, Trouillon, Perez, and
  Gaidon]{bouchard2016online}
Guillaume Bouchard, Th{\'e}o Trouillon, Julien Perez, and Adrien Gaidon.
\newblock Online learning to sample.
\newblock \emph{arXiv preprint arXiv:1506.09016}, 2016.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI} {Gym}.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Ciosek and Whiteson(2017)]{ciosek2017offer}
Kamil Ciosek and Shimon Whiteson.
\newblock O{F}{F}{E}{R}: Off-environment reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2017.

\bibitem[Farajtabar et~al.(2018)Farajtabar, Chow, and
  Ghavamzadeh]{farajtabar2018more}
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Frank et~al.(2008)Frank, Mannor, and Precup]{frank2008reinforcement}
Jordan Frank, Shie Mannor, and Doina Precup.
\newblock Reinforcement learning in the presence of rare events.
\newblock In \emph{International Conference on Machine Learning}, 2008.

\bibitem[Hanna et~al.(2017)Hanna, Thomas, Stone, and
  Niekum]{hanna2017data-efficient}
Josiah~P. Hanna, Philip~S. Thomas, Peter Stone, and Scott Niekum.
\newblock Data-efficient policy evaluation through behavior policy search.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Hanna et~al.(2021)Hanna, Niekum, and Stone]{hanna2021importance}
Josiah~P. Hanna, Scott Niekum, and Peter Stone.
\newblock Importance {Sampling} in {Reinforcement} {Learning} with an
  {Estimated} {Behavior} {Policy}.
\newblock \emph{Machine Learning}, 110\penalty0 (6):\penalty0 1267--1317, May
  2021.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del R{\'{i}}o, Wiebe, Peterson, G{\'{e}}rard-Marchant,
  Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020array}
Charles~R. Harris, K.~Jarrod Millman, St{\'{e}}fan~J. van~der Walt, Ralf
  Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,
  Sebastian Berg, Nathaniel~J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,
  Marten~H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime~Fern{\'{a}}ndez
  del R{\'{i}}o, Mark Wiebe, Pearu Peterson, Pierre G{\'{e}}rard-Marchant,
  Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph
  Gohlke, and Travis~E. Oliphant.
\newblock Array programming with {NumPy}.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, 2020.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Konyushova et~al.(2021)Konyushova, Chen, Paine, Gulcehre, Paduraru,
  Mankowitz, Denil, and de~Freitas]{konyushkova2021active}
Ksenia Konyushova, Yutian Chen, Thomas Paine, Caglar Gulcehre, Cosmin Paduraru,
  Daniel~J. Mankowitz, Misha Denil, and Nando de~Freitas.
\newblock Active offline policy selection.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Le et~al.(2019)Le, Voloshin, and Yue]{le2019batch}
Hoang Le, Cameron Voloshin, and Yisong Yue.
\newblock Batch policy learning under constraints.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesv{\'a}ri]{li2015toward}
Lihong Li, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Toward minimax off-policy value estimation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2015.

\bibitem[Mardia et~al.(2019)Mardia, Jiao, Tánczos, Nowak, and
  Weissman]{10.1093/imaiai/iaz025}
Jay Mardia, Jiantao Jiao, Ervin Tánczos, Robert~D Nowak, and Tsachy Weissman.
\newblock Concentration inequalities for the empirical distribution of discrete
  distributions: beyond the method of types.
\newblock \emph{Information and Inference: A Journal of the IMA}, 9\penalty0
  (4):\penalty0 813--850, 2019.

\bibitem[Mukherjee et~al.(2022)Mukherjee, Hanna, and
  Nowak]{mukherjee_revar_2022}
Subhojyoti Mukherjee, Josiah~P. Hanna, and Robert Nowak.
\newblock {ReVar}: {Strengthening} {Policy} {Evaluation} via {Reduced}
  {Variance} {Sampling}.
\newblock In \emph{{International} {Conference} on {Uncertainty} in
  {Artificial} {Intelligence} ({UAI})}, August 2022.

\bibitem[Narita et~al.(2019)Narita, Yasui, and Yata]{narita2019efficient}
Yusuke Narita, Shota Yasui, and Kohei Yata.
\newblock Efficient counterfactual learning from bandit feedback.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2019.

\bibitem[O'Hagan(1987)]{ohagan1987monte}
Anthony O'Hagan.
\newblock Monte carlo is fundamentally unsound.
\newblock \emph{The Statistician}, pages 247--249, 1987.

\bibitem[Oosterhuis and de~Rijke(2020)]{oosterhuis2020taking}
Harrie Oosterhuis and Maarten de~Rijke.
\newblock Taking the counterfactual online: Efficient and unbiased online
  evaluation for ranking.
\newblock In \emph{International Conference on Theory of Information
  Retrieval}, 2020.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, Oord, and
  Munos]{ostrovski2017count}
Georg Ostrovski, Marc~G. Bellemare, A{\"a}ron Oord, and R{\'e}mi Munos.
\newblock Count-based exploration with neural density models.
\newblock In \emph{International conference on machine learning}, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In \emph{Advances in Neural Information Processing Systems}. 2019.

\bibitem[Pavse et~al.(2020)Pavse, Durugkar, Hanna, and
  Stone]{pavse2020reducing}
Brahma~S. Pavse, Ishan Durugkar, Josiah~P. Hanna, and Peter Stone.
\newblock Reducing sampling error in batch temporal difference learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L. Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rasmussen and Ghahramani(2003)]{rasmussen2003bayesian}
Carl~Edward Rasmussen and Zoubin Ghahramani.
\newblock Bayesian monte carlo.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2003.

\bibitem[Sch\"afer et~al.(2022)Sch\"afer, Christianos, Hanna, and
  Albrecht]{schaefer2022derl}
Lukas Sch\"afer, Filippos Christianos, Josiah~P. Hanna, and Stefano~V.
  Albrecht.
\newblock Decoupled reinforcement learning to stabilise intrinsically-motivated
  exploration.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, 2022.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sen and Singer(1993)]{sen1993large}
Pranab~K. Sen and Julio~M. Singer.
\newblock \emph{Large Sample Methods in Statistics: An Introduction with
  Applications}.
\newblock Chapman \& Hall, 1993.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 1998.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  De~Turck, and Abbeel]{tang2017exploration}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi~Chen, Yan Duan, John
  Schulman, Filip De~Turck, and Pieter Abbeel.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Thomas and Brunskill(2016)]{thomas2016data-efficient}
Philip~S. Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Tucker and Joachims(2022)]{tucker2022variance-optimal}
Aaron~David Tucker and Thorsten Joachims.
\newblock Variance-optimal augmentation logging for counterfactual evaluation
  in contextual bandits.
\newblock \emph{arXiv preprint arXiv:2202.01721}, 2022.

\bibitem[Williams(1992)]{reinforce}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Zinkevich et~al.(2006)Zinkevich, Bowling, Bard, Kan, and
  Billings]{Zinkevich2006}
Martin Zinkevich, Michael Bowling, Nolan Bard, Morgan Kan, and Darse Billings.
\newblock Optimal unbiased estimators for evaluating agent performance.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2006.

\end{thebibliography}
