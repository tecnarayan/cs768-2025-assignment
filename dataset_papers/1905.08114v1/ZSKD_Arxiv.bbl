\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balakrishnan \& Nevzorov(2004)Balakrishnan and
  Nevzorov]{balakrishnan2004primer}
Balakrishnan, N. and Nevzorov, V.~B.
\newblock \emph{A primer on statistical distributions}.
\newblock John Wiley \& Sons, 2004.

\bibitem[Buciluǎ et~al.(2006)Buciluǎ, Caruana, and
  Niculescu-Mizil]{bucilua2006model}
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A.
\newblock Model compression.
\newblock In \emph{International Conference on Knowledge discovery and Data
  mining}. ACM, 2006.

\bibitem[Dao et~al.(2018)Dao, Gu, Ratner, Smith, De~Sa, and
  R{\'e}]{dao2018kernel}
Dao, T., Gu, A., Ratner, A.~J., Smith, V., De~Sa, C., and R{\'e}, C.
\newblock A kernel theory of modern data augmentation.
\newblock \emph{arXiv preprint arXiv:1803.06084}, 2018.

\bibitem[Furlanello et~al.(2018)Furlanello, Lipton, Tschannen, Itti, and
  Anandkumar]{furlanello2018born}
Furlanello, T., Lipton, Z.~C., Tschannen, M., Itti, L., and Anandkumar, A.
\newblock Born again neural networks.
\newblock \emph{arXiv preprint arXiv:1805.04770}, 2018.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm-icml-2015}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Kimura et~al.(2018)Kimura, Ghahramani, Takeuchi, Iwata, and
  Ueda]{kimura2018few}
Kimura, A., Ghahramani, Z., Takeuchi, K., Iwata, T., and Ueda, N.
\newblock Few-shot learning of neural networks from scratch by pseudo example
  optimization.
\newblock In \emph{British Machine Vision Conference (BMVC)}, 2018.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Canadian Institute for Advanced Research, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lin(2016)]{lin2016dirichlet}
Lin, J.
\newblock On the dirichlet distribution.
\newblock Master's thesis, Department of Mathematics and Statistics, Queen’s
  University, Kingston, Ontario, Canada, 2016.

\bibitem[Lopes et~al.(2017)Lopes, Fenu, and Starner]{dfkd-nips-lld-17}
Lopes, R.~G., Fenu, S., and Starner, T.
\newblock Data-free knowledge distillation for deep neural networks.
\newblock In \emph{LLD Workshop at Neural Information Processing Systems (NIPS
  )}, 2017.

\bibitem[Mopuri et~al.(2018)Mopuri, Krishna, and Babu]{mopuri2018ask}
Mopuri, K.~R., Krishna, P., and Babu, R.~V.
\newblock Ask, acquire, and attack: Data-free uap generation using class
  impressions.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2018.

\bibitem[Mordvintsev et~al.(2015)Mordvintsev, Tyka, and Olah]{deep-dream-2015}
Mordvintsev, A., Tyka, M., and Olah, C.
\newblock Google deep dream.
\newblock 2015.
\newblock URL
  \url{https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}.

\bibitem[Olah et~al.(2017)Olah, Mordvintsev, and Schubert]{olah2017feature}
Olah, C., Mordvintsev, A., and Schubert, L.
\newblock Feature visualization.
\newblock \emph{Distill}, 2017.
\newblock URL \url{https://distill.pub/2017/feature-visualization}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet-ijcv-2015}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3), 2015.

\bibitem[Simonyan et~al.(2014)Simonyan, Vedaldi, and
  Zisserman]{backprop-iclrw-2014}
Simonyan, K., Vedaldi, A., and Zisserman, A.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock In \emph{International Conference on Learning Representations (ICLR)
  Workshops}, 2014.

\bibitem[Snelson \& Ghahramani(2006)Snelson and Ghahramani]{snelson2006sparse}
Snelson, E. and Ghahramani, Z.
\newblock Sparse gaussian processes using pseudo-inputs.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2006.

\bibitem[Springenberg et~al.(2015)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{guidedbackprop-iclrw-2015}
Springenberg, J., Dosovitskiy, A., Brox, T., and Riedmiller, M.
\newblock Striving for simplicity: The all convolutional net.
\newblock In \emph{International Conference on Learning Representations (ICLR)
  workshops}, 2015.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout-jmlr-2014}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\end{thebibliography}
