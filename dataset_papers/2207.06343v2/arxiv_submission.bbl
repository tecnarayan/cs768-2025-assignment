\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acar et~al.(2021)Acar, Zhao, Matas, Mattina, Whatmough, and
  Saligrama]{acar2021federated}
Durmus Alp~Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough,
  and Venkatesh Saligrama.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=B7v4QMR6Z9w}.

\bibitem[Achille et~al.(2021)Achille, Golatkar, Ravichandran, Polito, and
  Soatto]{achille2021lqf}
Alessandro Achille, Aditya Golatkar, Avinash Ravichandran, Marzia Polito, and
  Stefano Soatto.
\newblock Lqf: Linear quadratic fine-tuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15729--15739, 2021.

\bibitem[Afonin and Karimireddy(2021)]{afonin2021towards}
Andrei Afonin and Sai~Praneeth Karimireddy.
\newblock Towards model agnostic federated learning using knowledge
  distillation.
\newblock \emph{arXiv preprint arXiv:2110.15210}, 2021.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Avron et~al.()Avron, Clarkson, and Woodruff]{avron17sharper}
Haim Avron, Kenneth~L. Clarkson, and David~P. Woodruff.
\newblock Sharper bounds for regularized data fitting.
\newblock In \emph{Approximation, Randomization, and Combinatorial
  Optimization. Algorithms and Techniques, {}}.

\bibitem[Bagdasaryan et~al.(2020)Bagdasaryan, Veit, Hua, Estrin, and
  Shmatikov]{bagdasaryan2020backdoor}
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
  Shmatikov.
\newblock How to backdoor federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2938--2948. PMLR, 2020.

\bibitem[Blanchard et~al.(2017)Blanchard, El~Mhamdi, Guerraoui, and
  Stainer]{blanchard2017machine}
Peva Blanchard, El~Mahdi El~Mhamdi, Rachid Guerraoui, and Julien Stainer.
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Bonawitz et~al.(2021)Bonawitz, Kairouz, McMahan, and
  Ramage]{bonawitz2021federated}
Kallista Bonawitz, Peter Kairouz, Brendan McMahan, and Daniel Ramage.
\newblock Federated learning and privacy: Building privacy-preserving systems
  for machine learning and data science on decentralized data.
\newblock \emph{Queue}, 19\penalty0 (5):\penalty0 87--114, 2021.

\bibitem[Bonawitz et~al.(2017)Bonawitz, Ivanov, Kreuter, Marcedone, McMahan,
  Patel, Ramage, Segal, and Seth]{bonawitz2017practical}
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H~Brendan
  McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In \emph{Proceedings of the 2017 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 1175--1191, 2017.

\bibitem[Bonawitz et~al.(2019)Bonawitz, Eichner, Grieskamp, Huba, Ingerman,
  Ivanov, Kiddon, Kone{\v{c}}n{\`y}, Mazzocchi, McMahan,
  et~al.]{bonawitz2019towards}
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Kone{\v{c}}n{\`y}, Stefano
  Mazzocchi, Brendan McMahan, et~al.
\newblock Towards federated learning at scale: System design.
\newblock \emph{Proceedings of Machine Learning and Systems}, 1:\penalty0
  374--388, 2019.

\bibitem[Charles et~al.(2021)Charles, Garrett, Huo, Shmulyian, and
  Smith]{charles2021large}
Zachary Charles, Zachary Garrett, Zhouyuan Huo, Sergei Shmulyian, and Virginia
  Smith.
\newblock On large-cohort training for federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Chayti and Karimireddy(2022)]{chayti2022optimization}
El~Mahdi Chayti and Sai~Praneeth Karimireddy.
\newblock Optimization with access to auxiliary information.
\newblock \emph{arXiv preprint arXiv:2206.00395}, 2022.

\bibitem[Collins et~al.(2021)Collins, Hassani, Mokhtari, and
  Shakkottai]{collins2021exploiting}
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock Exploiting shared representations for personalized federated
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2089--2099. PMLR, 2021.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, et~al.]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang, et~al.
\newblock Large scale distributed deep networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 25, 2012.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Deng et~al.(2020)Deng, Kamani, and Mahdavi]{deng2020adaptive}
Yuyang Deng, Mohammad~Mahdi Kamani, and Mehrdad Mahdavi.
\newblock Adaptive personalized federated learning.
\newblock \emph{arXiv preprint arXiv:2003.13461}, 2020.

\bibitem[du~Terrail et~al.(2022)du~Terrail, Ayed, Cyffers, Grimberg, He, Loeb,
  Mangold, Marchand, Marfoq, Mushtaq, et~al.]{du2022flamby}
Jean~Ogier du~Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,
  Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq, Erum
  Mushtaq, et~al.
\newblock Flamby: Datasets and benchmarks for cross-silo federated learning in
  realistic settings.
\newblock 2022.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020personalized}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Personalized federated learning: A meta-learning approach.
\newblock \emph{arXiv preprint arXiv:2002.07948}, 2020.

\bibitem[Fang et~al.(2020)Fang, Cao, Jia, and Gong]{fang2020local}
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong.
\newblock Local model poisoning attacks to $\{$Byzantine-Robust$\}$ federated
  learning.
\newblock In \emph{29th USENIX Security Symposium (USENIX Security 20)}, pages
  1605--1622, 2020.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fortntk}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
  Daniel~M Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 5850--5861, 2020.

\bibitem[Fung et~al.(2018)Fung, Yoon, and Beschastnikh]{fung2018mitigating}
Clement Fung, Chris~JM Yoon, and Ivan Beschastnikh.
\newblock Mitigating sybils in federated learning poisoning.
\newblock \emph{arXiv preprint arXiv:1808.04866}, 2018.

\bibitem[Goldblum et~al.(2019)Goldblum, Geiping, Schwarzschild, Moeller, and
  Goldstein]{goldblum2019truth}
Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, and Tom
  Goldstein.
\newblock Truth or backpropaganda? an empirical investigation of deep learning
  theory.
\newblock \emph{arXiv preprint arXiv:1910.00359}, 2019.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training {I}magenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Haddadpour et~al.(2021)Haddadpour, Kamani, Mokhtari, and
  Mahdavi]{haddadpour2021federated}
Farzin Haddadpour, Mohammad~Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2350--2358. PMLR, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2022)He, Karimireddy, and Jaggi]{he2022byzantine}
Lie He, Sai~Praneeth Karimireddy, and Martin Jaggi.
\newblock Byzantine-robust decentralized learning via self-centered clipping.
\newblock \emph{arXiv preprint arXiv:2202.01545}, 2022.

\bibitem[Hsieh et~al.(2020)Hsieh, Phanishayee, Mutlu, and
  Gibbons]{hsieh2020non}
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons.
\newblock The non-iid data quagmire of decentralized machine learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4387--4398. PMLR, 2020.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Tzu-Ming~Harry Hsu, Hang Qi, and Matthew Brown.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Hui and Belkin(2020)]{hui2020evaluation}
Like Hui and Mikhail Belkin.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock \emph{arXiv preprint arXiv:2006.07322}, 2020.

\bibitem[Iandola et~al.(2016)Iandola, Moskewicz, Ashraf, and
  Keutzer]{iandola2016firecaffe}
Forrest~N Iandola, Matthew~W Moskewicz, Khalid Ashraf, and Kurt Keutzer.
\newblock Firecaffe: near-linear acceleration of deep neural network training
  on compute clusters.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2592--2600, 2016.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Jones and Tonetti(2020)]{jones2020nonrivalry}
Charles~I Jones and Christopher Tonetti.
\newblock Nonrivalry and the economics of data.
\newblock \emph{American Economic Review}, 110\penalty0 (9):\penalty0 2819--58,
  2020.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, et~al.]{kairouz2019federated}
Peter Kairouz, H.~Brendan McMahan, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  14\penalty0 (1--2):\penalty0 1--210, 2021.

\bibitem[Karimireddy et~al.(2020{\natexlab{a}})Karimireddy, Jaggi, Kale, Mohri,
  Reddi, Stich, and Suresh]{karimireddy2020mime}
Sai~Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank~J
  Reddi, Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Mime: Mimicking centralized stochastic algorithms in federated
  learning.
\newblock \emph{arXiv preprint arXiv:2008.03606}, 2020{\natexlab{a}}.

\bibitem[Karimireddy et~al.(2020{\natexlab{b}})Karimireddy, Kale, Mohri, Reddi,
  Stich, and Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020{\natexlab{b}}.

\bibitem[Karimireddy et~al.(2021)Karimireddy, He, and
  Jaggi]{karimireddy2022byzantine}
Sai~Praneeth Karimireddy, Lie He, and Martin Jaggi.
\newblock Byzantine-robust learning on heterogeneous datasets via bucketing.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kulkarni et~al.(2020)Kulkarni, Kulkarni, and Pant]{kulkarni2020survey}
Viraj Kulkarni, Milind Kulkarni, and Aniruddha Pant.
\newblock Survey of personalization techniques for federated learning.
\newblock In \emph{2020 Fourth World Conference on Smart Trends in Systems,
  Security and Sustainability (WorldS4)}, pages 794--797. IEEE, 2020.

\bibitem[Kulynych et~al.(2020)Kulynych, Madras, Milli, Raji, Zhou, and
  Zemel]{paml2020}
Bogdan Kulynych, David Madras, Smitha Milli, Inioluwa~Deborah Raji, Angela
  Zhou, and Richard Zemel.
\newblock Participatory approaches to machine learning.
\newblock International Conference on Machine Learning Workshop, 2020.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Diao, Chen, and He]{li2021federated}
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.
\newblock Federated learning on non-iid data silos: An experimental study.
\newblock \emph{arXiv preprint arXiv:2102.02079}, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, He, and Song]{li2021model}
Qinbin Li, Bingsheng He, and Dawn Song.
\newblock Model-contrastive federated learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10713--10722, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Sanjabi, Beirami, and Smith]{li2019fair}
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.
\newblock Fair resource allocation in federated learning.
\newblock \emph{arXiv preprint arXiv:1905.10497}, 2019.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  429--450, 2020.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020ensemble}
Tao Lin, Lingjing Kong, Sebastian~U Stich, and Martin Jaggi.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2351--2363, 2020.

\bibitem[Lin et~al.(2021)Lin, Karimireddy, Stich, and Jaggi]{lin2021quasi}
Tao Lin, Sai~Praneeth Karimireddy, Sebastian~U Stich, and Martin Jaggi.
\newblock Quasi-global momentum: Accelerating decentralized deep learning on
  heterogeneous data.
\newblock \emph{arXiv preprint arXiv:2102.04761}, 2021.

\bibitem[Long(2021)]{long2021properties}
Philip~M Long.
\newblock Properties of the after kernel.
\newblock \emph{arXiv preprint arXiv:2105.10585}, 2021.

\bibitem[Lyu et~al.(2020)Lyu, Yu, and Yang]{lyu2020threats}
Lingjuan Lyu, Han Yu, and Qiang Yang.
\newblock Threats to federated learning: A survey.
\newblock \emph{arXiv preprint arXiv:2003.02133}, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Malinovsky, Stich, and
  Richt{\'a}rik]{mishchenko2022proxskip}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter
  Richt{\'a}rik.
\newblock Proxskip: Yes! local gradient steps provably lead to communication
  acceleration! finally!
\newblock \emph{arXiv preprint arXiv:2202.09357}, 2022.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mehryar Mohri, Gary Sivek, and Ananda~Theertha Suresh.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4615--4625. PMLR, 2019.

\bibitem[Mothukuri et~al.(2021)Mothukuri, Parizi, Pouriyeh, Huang,
  Dehghantanha, and Srivastava]{mothukuri2021survey}
Viraaji Mothukuri, Reza~M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
  Dehghantanha, and Gautam Srivastava.
\newblock A survey on security and privacy of federated learning.
\newblock \emph{Future Generation Computer Systems}, 115:\penalty0 619--640,
  2021.

\bibitem[Mu et~al.(2020)Mu, Liang, and Li]{mu2020gradients}
Fangzhou Mu, Yingyu Liang, and Yin Li.
\newblock Gradients as features for deep representation learning.
\newblock \emph{arXiv preprint arXiv:2004.05529}, 2020.

\bibitem[Ozkara et~al.(2021)Ozkara, Singh, Data, and Diggavi]{ozkara2021quped}
Kaan Ozkara, Navjot Singh, Deepesh Data, and Suhas Diggavi.
\newblock Quped: Quantized personalization via distillation with applications
  to federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Pentland et~al.(2021)Pentland, Lipton, and
  Hardjono]{pentland2021building}
Alex Pentland, Alexander Lipton, and Thomas Hardjono.
\newblock \emph{Building the New Economy: Data as Capital}.
\newblock MIT Press, 2021.

\bibitem[Ramaswamy et~al.(2020)Ramaswamy, Thakkar, Mathews, Andrew, McMahan,
  and Beaufays]{ramaswamy2020training}
Swaroop Ramaswamy, Om~Thakkar, Rajiv Mathews, Galen Andrew, H~Brendan McMahan,
  and Fran{\c{c}}oise Beaufays.
\newblock Training production language models without memorizing user data.
\newblock \emph{arXiv preprint arXiv:2009.10031}, 2020.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=LkFG3lB13U5}.

\bibitem[Shi et~al.(2021)Shi, Yu, and Leung]{shi2021survey}
Yuxin Shi, Han Yu, and Cyril Leung.
\newblock A survey of fairness-aware federated learning.
\newblock \emph{arXiv preprint arXiv:2111.01872}, 2021.

\bibitem[Singh and Jaggi(2020)]{singh2020model}
Sidak~Pal Singh and Martin Jaggi.
\newblock Model fusion via optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 22045--22055, 2020.

\bibitem[So et~al.(2020)So, G{\"u}ler, and Avestimehr]{so2020byzantine}
Jinhyun So, Ba{\c{s}}ak G{\"u}ler, and A~Salman Avestimehr.
\newblock Byzantine-resilient secure federated learning.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 39\penalty0
  (7):\penalty0 2168--2181, 2020.

\bibitem[Stich and Karimireddy(2020)]{stich2020error}
Sebastian~U Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed updates.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--36,
  2020.

\bibitem[Sun et~al.(2019)Sun, Kairouz, Suresh, and McMahan]{sun2019can}
Ziteng Sun, Peter Kairouz, Ananda~Theertha Suresh, and H~Brendan McMahan.
\newblock Can you really backdoor federated learning?
\newblock \emph{arXiv preprint arXiv:1911.07963}, 2019.

\bibitem[Suresh et~al.(2017)Suresh, Felix, Kumar, and
  McMahan]{suresh2017distributed}
Ananda~Theertha Suresh, X~Yu Felix, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In \emph{International Conference on Machine Learning}, pages
  3329--3337. PMLR, 2017.

\bibitem[Tan et~al.(2021)Tan, Long, Liu, Zhou, Lu, Jiang, and
  Zhang]{tan2021fedproto}
Yue Tan, Guodong Long, Lu~Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi
  Zhang.
\newblock Fedproto: Federated prototype learning over heterogeneous devices.
\newblock \emph{arXiv preprint arXiv:2105.00243}, 2021.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Sreenivasan, Rajput, Vishwakarma,
  Agarwal, Sohn, Lee, and Papailiopoulos]{wang2020attack}
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
  Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock Attack of the tails: Yes, you really can backdoor federated learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16070--16084, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Tantia, Ballas, and
  Rabbat]{wang2019slowmo}
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat.
\newblock Slowmo: Improving communication-efficient distributed sgd with slow
  momentum.
\newblock \emph{arXiv preprint arXiv:1910.00643}, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Liu, Liang, Joshi, and
  Poor]{wang2020tackling}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7611--7623, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H~Brendan McMahan, Maruan
  Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
  et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wang et~al.(2022)Wang, Das, Joshi, Kale, Xu, and
  Zhang]{wang2022unreasonable}
Jianyu Wang, Rudrajit Das, Gauri Joshi, Satyen Kale, Zheng Xu, and Tong Zhang.
\newblock On the unreasonable effectiveness of federated averaging with
  heterogeneous data.
\newblock \emph{arXiv preprint arXiv:2206.04723}, 2022.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Tuor, Salonidis, Leung, Makaya,
  He, and Chan]{wang2019adaptive}
Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, Kin~K Leung, Christian
  Makaya, Ting He, and Kevin Chan.
\newblock Adaptive federated learning in resource constrained edge computing
  systems.
\newblock \emph{IEEE Journal on Selected Areas in Communications}, 37\penalty0
  (6):\penalty0 1205--1221, 2019{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Hu, and Steinhardt]{wei2022more}
Alexander Wei, Wei Hu, and Jacob Steinhardt.
\newblock More than a toy: Random matrix models predict how real-world neural
  representations generalize.
\newblock \emph{arXiv preprint arXiv:2203.06176}, 2022.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, and
  Srebro]{woodworth2020minibatch}
Blake~E Woodworth, Kumar~Kshitij Patel, and Nati Srebro.
\newblock Minibatch vs local sgd for heterogeneous distributed learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6281--6292, 2020.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs,
  Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair
  Carmon, Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock \emph{arXiv preprint arXiv:2203.05482}, 2022.

\bibitem[Wu et~al.(2020)Wu, He, and Chen]{wu2020personalized}
Qiong Wu, Kaiwen He, and Xu~Chen.
\newblock Personalized federated learning for intelligent {IoT} applications: A
  cloud-edge based framework.
\newblock \emph{IEEE Open Journal of the Computer Society}, 1:\penalty0 35--44,
  2020.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 3--19, 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yu et~al.(2021)Yu, Zhang, Qin, Xu, Wang, Liu, Tian, and
  Chen]{yu2021fed2}
Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di~Wang, Chenchen Liu, Zhi Tian,
  and Xiang Chen.
\newblock Fed2: Feature-aligned federated learning.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 2066--2074, 2021.

\bibitem[Zancato et~al.(2020)Zancato, Achille, Ravichandran, Bhotika, and
  Soatto]{zancato2020predicting}
Luca Zancato, Alessandro Achille, Avinash Ravichandran, Rahul Bhotika, and
  Stefano Soatto.
\newblock Predicting training time without training.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6136--6146, 2020.

\end{thebibliography}
