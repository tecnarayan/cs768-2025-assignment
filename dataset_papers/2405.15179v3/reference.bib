@inproceedings{
ortiz2024magnitude,
title={Magnitude Invariant Parametrizations Improve Hypernetwork Learning},
author={Jose Javier Gonzalez Ortiz and John Guttag and Adrian V Dalca},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}


@inproceedings{mazzawideep,
  title={Deep Fusion: Efficient Network Training via Pre-trained Initializations},
  author={Mazzawi, Hanna and Gonzalvo, Javier and Wunder, Michael and Jerome, Sammy and Dherin, Benoit},
  booktitle={Forty-first International Conference on Machine Learning}, 
  year={2024}
}


@article{kolda2009tensor,
  title={Tensor decompositions and applications},
  author={Kolda, Tamara G and Bader, Brett W},
  journal={SIAM review},
  volume={51},
  number={3},
  pages={455--500},
  year={2009},
  publisher={SIAM}
}

@inproceedings{arora2013practical,
  title={A practical algorithm for topic modeling with provable guarantees},
  author={Arora, Sanjeev and Ge, Rong and Halpern, Yonatan and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
  booktitle={International Conference on Machine Learning},
  pages={280--288},
  year={2013},
  organization={PMLR}
}

@article{chan2009convex,
  title={A convex analysis-based minimum-volume enclosing simplex algorithm for hyperspectral unmixing},
  author={Chan, Tsung-Han and Chi, Chong-Yung and Huang, Yu-Min and Ma, Wing-Kin},
  journal={IEEE Transactions on Signal Processing},
  volume={57},
  number={11},
  pages={4418--4432},
  year={2009},
  publisher={IEEE}
}

@inproceedings{reisinger2010spherical,
  title={Spherical topic models},
  author={Reisinger, Joseph and Waters, Austin and Silverthorn, Bryan and Mooney, Raymond J},
  booktitle={International Conference on Machine Learning },
  pages={903--910},
  year={2010},
  organization={Citeseer}
}


@inproceedings{le2013fastfood,
  title={Fastfood-approximating kernel expansions in loglinear time},
  author={Le, Quoc and Sarl{\'o}s, Tam{\'a}s and Smola, Alex and others},
  booktitle={International Conference on Machine Learning},
  volume={85},
  number={8},
  year={2013}
}

@article{aghajanyan2020intrinsic,
  title={Intrinsic dimensionality explains the effectiveness of language model fine-tuning},
  author={Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2012.13255},
  year={2020}
}

@article{borzunov2024distributed,
  title={Distributed Inference and Fine-tuning of Large Language Models Over The Internet},
  author={Borzunov, Alexander and Ryabinin, Max and Chumachenko, Artem and Baranchuk, Dmitry and Dettmers, Tim and Belkada, Younes and Samygin, Pavel and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{inouye2014admixture,
  title={Admixture of {P}oisson {M}{R}{F}s: A topic model with word dependencies},
  author={Inouye, David and Ravikumar, Pradeep and Dhillon, Inderjit},
  booktitle={International Conference on Machine Learning},
  pages={683--691},
  year={2014},
  organization={PMLR}
}

@article{pritchard2000inference,
  title={Inference of population structure using multilocus genotype data},
  author={Pritchard, Jonathan K and Stephens, Matthew and Donnelly, Peter},
  journal={Genetics},
  volume={155},
  number={2},
  pages={945--959},
  year={2000},
  publisher={Oxford University Press}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  year={2021},
  organization={Association for Computational Linguistics}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@inproceedings{he2021towards,
  title={Towards a Unified View of Parameter-Efficient Transfer Learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4582--4597},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{sheng2023s,
  title={S-{L}o{R}{A}: Serving thousands of concurrent {L}o{R}{A} adapters},
  author={Sheng, Ying and Cao, Shiyi and Li, Dacheng and Hooper, Coleman and Lee, Nicholas and Yang, Shuo and Chou, Christopher and Zhu, Banghua and Zheng, Lianmin and Keutzer, Kurt and others},
  journal={arXiv preprint arXiv:2311.03285},
  year={2023}
}

@article{fu2015blind,
  title={Blind separation of quasi-stationary sources: Exploiting convex geometry in covariance domain},
  author={Fu, Xiao and Ma, Wing-Kin and Huang, Kejun and Sidiropoulos, Nicholas D},
  journal={IEEE Transactions on Signal Processing},
  volume={63},
  number={9},
  pages={2306--2320},
  year={2015},
  publisher={IEEE}
}

@article{lin2015identifiability,
  title={Identifiability of the simplex volume minimization criterion for blind hyperspectral unmixing: The no-pure-pixel case},
  author={Lin, Chia-Hsiang and Ma, Wing-Kin and Li, Wei-Chiang and Chi, Chong-Yung and Ambikapathi, ArulMurugan},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={53},
  number={10},
  pages={5530--5546},
  year={2015},
  publisher={IEEE}
}

@inproceedings{li2008minimum,
  title={Minimum volume simplex analysis: A fast algorithm to unmix hyperspectral data},
  author={Li, Jun and Bioucas-Dias, Jos{\'e} M},
  booktitle={IGARSS 2008-2008 IEEE International Geoscience and Remote Sensing Symposium},
  volume={3},
  pages={III--250},
  year={2008},
  organization={IEEE}
}

@inproceedings{mahabadi2021parameter,
  title={Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks},
  author={Mahabadi, Rabeeh Karimi and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={565--576},
  year={2021}
}

@article{ponti2022combining,
  title={Combining modular skills in multitask learning},
  author={Ponti, Edoardo M and Sordoni, Alessandro and Bengio, Yoshua and Reddy, Siva},
  journal={arXiv preprint arXiv:2202.13914},
  year={2022}
}

@article{cichocki2014era,
  title={Era of big data processing: A new approach via tensor networks and tensor decompositions},
  author={Cichocki, Andrzej},
  journal={arXiv preprint arXiv:1403.2048},
  year={2014}
}

@article{qin2021exploring,
  title={Exploring universal intrinsic task subspace via prompt tuning},
  author={Qin, Yujia and Wang, Xiaozhi and Su, Yusheng and Lin, Yankai and Ding, Ning and Yi, Jing and Chen, Weize and Liu, Zhiyuan and Li, Juanzi and Hou, Lei and others},
  journal={arXiv preprint arXiv:2110.07867},
  year={2021}
}

@article{karimi2021compacter,
  title={Compacter: Efficient low-rank hypercomplex adapter layers},
  author={Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1022--1035},
  year={2021}
}

@article{feng2024mixture,
  title={Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models},
  author={Feng, Wenfeng and Hao, Chuzhan and Zhang, Yuewei and Han, Yu and Wang, Hao},
  journal={arXiv preprint arXiv:2403.03432},
  year={2024}
}

@article{zhao2023domain,
  title={Domain specialization as the key to make large language models disruptive: A comprehensive survey},
  author={Zhao, Xujiang and Lu, Jiaying and Deng, Chengyuan and Zheng, Can and Wang, Junxiang and Chowdhury, Tanmoy and Yun, Li and Cui, Hejie and Xuchao, Zhang and Zhao, Tianjiao and others},
  journal={arXiv preprint arXiv:2305.18703},
  year={2023}
}

@article{wang2023multilora,
  title={Multilora: Democratizing lora for better multi-task learning},
  author={Wang, Yiming and Lin, Yu and Zeng, Xiaodong and Zhang, Guannan},
  journal={arXiv preprint arXiv:2311.11501},
  year={2023}
}
@article{ding2008convex,
  title={Convex and semi-nonnegative matrix factorizations},
  author={Ding, Chris HQ and Li, Tao and Jordan, Michael I},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={32},
  number={1},
  pages={45--55},
  year={2008},
  publisher={IEEE}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

@article{huijben2022review,
  title={A review of the gumbel-max trick and its extensions for discrete stochasticity in machine learning},
  author={Huijben, Iris AM and Kool, Wouter and Paulus, Max B and Van Sloun, Ruud JG},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={2},
  pages={1353--1371},
  year={2022},
  publisher={IEEE}
}


@inproceedings{shazeer2016outrageously,
  title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2016}
}


@article{ding2022delta,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={3},
  pages={220--235},
  year={2023},
  publisher={Nature Publishing Group UK London}
}


@article{sukhbaatar2024branch,
  title={Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM},
  author={Sukhbaatar, Sainbayar and Golovneva, Olga and Sharma, Vasu and Xu, Hu and Lin, Xi Victoria and Rozi{\`e}re, Baptiste and Kahn, Jacob and Li, Daniel and Yih, Wen-tau and Weston, Jason and others},
  journal={arXiv preprint arXiv:2403.07816},
  year={2024}
}


@article{oseledets2010approximation,
  title={Approximation of $2^{}d\times2^{}d$ matrices using tensor decomposition},
  author={Oseledets, Ivan V},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={31},
  number={4},
  pages={2130--2145},
  year={2010},
  publisher={SIAM}
}

@misc{zhao2024galore,
      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, 
      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
      year={2024},
      eprint={2403.03507},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dou2023loramoe,
  title={Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment},
  author={Dou, Shihan and Zhou, Enyu and Liu, Yan and Gao, Songyang and Zhao, Jun and Shen, Wei and Zhou, Yuhao and Xi, Zhiheng and Wang, Xiao and Fan, Xiaoran and others},
  journal={arXiv preprint arXiv:2312.09979},
  year={2023}
}

@inproceedings{zadouri2023pushing,
  title={Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning},
  author={Zadouri, Ted and {\"U}st{\"u}n, Ahmet and Ahmadian, Arash and Ermis, Beyza and Locatelli, Acyr and Hooker, Sara},
  booktitle={International Conference on Learning Representations},
  year={2023}
}


@article{gao2024higher,
  title={Higher Layers Need More LoRA Experts},
  author={Gao, Chongyang and Chen, Kezhen and Rao, Jinmeng and Sun, Baochen and Liu, Ruibo and Peng, Daiyi and Zhang, Yawen and Guo, Xiaoyuan and Yang, Jie and Subrahmanian, VS},
  journal={arXiv preprint arXiv:2402.08562},
  year={2024}
}

@inproceedings{jie2023fact,
  title={Fact: Factor-tuning for lightweight adaptation on vision transformer},
  author={Jie, Shibo and Deng, Zhi-Hong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={1},
  pages={1060--1068},
  year={2023}
}

@inproceedings{caccia2023multi,
  title={Multi-Head Adapter Routing for Cross-Task Generalization},
  author={Caccia, Lucas and Ponti, Edoardo and Su, Zhan and Pereira, Matheus and Le Roux, Nicolas and Sordoni, Alessandro},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}


@inproceedings{renduchintala2024tied,
  title={Tied-{L}o{R}{A}: Enhancing parameter efficiency of {L}o{R}{A} with Weight Tying},
  author={Renduchintala, Adithya and Konuk, Tugrul and Kuchaiev, Oleksii},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={8686--8697},
  year={2024}
}


@inproceedings{
kopiczko2023vera,
title={Ve{RA}: Vector-based Random Matrix Adaptation},
author={Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki M Asano},
booktitle={International Conference on Learning Representations},
year={2024},
url={}
}

@inproceedings{li2018measuring,
  title={Measuring the Intrinsic Dimension of Objective Landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{hu2021lora,
  title={Lo{R}{A}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for {N}{L}{P}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{novikov2015tensorizing,
  title={Tensorizing neural networks},
  author={Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{pires2023one,
  title={One wide feedforward is all you need},
  author={Pires, Telmo Pessoa and Lopes, Ant{\'o}nio V and Assogba, Yannick and Setiawan, Hendra},
  journal={arXiv preprint arXiv:2309.01826},
  year={2023}
}

@inproceedings{chen2015compressing,
  title={Compressing neural networks with the hashing trick},
  author={Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
  booktitle={International conference on machine learning},
  pages={2285--2294},
  year={2015},
  organization={PMLR}
}

@article{khoromskij2011d,
  title={O (d log N)-quantics approximation of N-d tensors in high-dimensional numerical modeling},
  author={Khoromskij, Boris N},
  journal={Constructive Approximation},
  volume={34},
  pages={257--280},
  year={2011},
  publisher={Springer}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on learning theory},
  pages={698--728},
  year={2016},
  organization={PMLR}
}

@inproceedings{
ha2017hypernetworks,
title={HyperNetworks},
author={David Ha and Andrew M. Dai and Quoc V. Le},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkpACe1lx}
}

@article{ma2019tensorized,
  title={A tensorized transformer for language modeling},
  author={Ma, Xindian and Zhang, Peng and Zhang, Shuai and Duan, Nan and Hou, Yuexian and Zhou, Ming and Song, Dawei},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{wang2021exploring,
  title={Exploring extreme parameter compression for pre-trained language models},
  author={Wang, Benyou and Ren, Yuxin and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{liu2022tuformer,
  title={Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency},
  author={Liu, Xiaoyu and Su, Jiahao and Huang, Furong},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{pan2024reusing,
  title={Reusing Pretrained Models by Multi-linear Operators for Efficient Training},
  author={Pan, Yu and Yuan, Ye and Yin, Yichun and Xu, Zenglin and Shang, Lifeng and Jiang, Xin and Liu, Qun},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{han2016variational,
  title={Variational Gaussian copula inference},
  author={Han, Shaobo and Liao, Xuejun and Dunson, David and Carin, Lawrence},
  booktitle={Artificial Intelligence and Statistics},
  pages={829--838},
  year={2016},
  organization={PMLR}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{liu2019roberta,
  title={Ro{B}{E}{R}{T}a: A robustly optimized {B}{E}{R}{T} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}


@inproceedings{zhang2022adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{novikova-etal-2017-e2e,
    title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
    author = "Novikova, Jekaterina  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Rieser, Verena",
    editor = "Jokinen, Kristiina  and
      Stede, Manfred  and
      DeVault, David  and
      Louis, Annie",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "",
    url = "",
    doi = "",
    pages = "201--206",
    abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}

@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    editor = "Yarowsky, David  and
      Baldwin, Timothy  and
      Korhonen, Anna  and
      Livescu, Karen  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "",
    url = "",
    pages = "1631--1642",
}

@inproceedings{dolan-brockett-2005-automatically,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "",
}

@article{warstadt-etal-2019-neural,
    title = "Neural Network Acceptability Judgments",
    author = "Warstadt, Alex  and
      Singh, Amanpreet  and
      Bowman, Samuel R.",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "",
    doi = "",
    pages = "625--641",
    abstract = "This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.{'}s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.",
}

@inproceedings{rajpurkar-etal-2018-know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "",
    url = "",
    doi = "",
    pages = "784--789",
    abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.",
}

@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.  and
      Cer, Daniel  and
      Jurgens, David",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "",
    url = "",
    doi = "",
    pages = "1--14",
    abstract = "Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in \textit{all language tracks}. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the \textit{STS Benchmark} is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "",
    url = "",
    doi = "",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@inproceedings{ruckle-etal-2021-adapterdrop,
    title = "{AdapterDrop}: {O}n the Efficiency of Adapters in Transformers",
    author = {R{\"u}ckl{\'e}, Andreas  and
      Geigle, Gregor  and
      Glockner, Max  and
      Beck, Tilman  and
      Pfeiffer, Jonas  and
      Reimers, Nils  and
      Gurevych, Iryna},
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.626",
    doi = "10.18653/v1/2021.emnlp-main.626",
    pages = "7930--7946",
    abstract = "Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.",
}

@inproceedings{lin-etal-2020-exploring,
    title = "Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning",
    author = "Lin, Zhaojiang  and
      Madotto, Andrea  and
      Fung, Pascale",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.41",
    doi = "10.18653/v1/2020.findings-emnlp.41",
    pages = "441--459",
    abstract = "Fine-tuning pre-trained generative language models to down-stream language generation tasks has shown promising results. However, this comes with the cost of having a single, large model for each task, which is not ideal in low-memory/power scenarios (e.g., mobile). In this paper, we propose an effective way to fine-tune multiple down-stream generation tasks simultaneously using a single, large pretrained model. The experiments on five diverse language generation tasks show that by just using an additional 2-3{\%} parameters for each task, our model can maintain or even improve the performance of fine-tuning the whole model.",
}

@article{pfeiffer2020adapterfusion,
  title={Adapterfusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}

@inproceedings{ben-zaken-etal-2022-bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "",
    doi = "",
    pages = "1--9",
    abstract = "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
}

@inproceedings{Tim-2023-qlora,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {10088--10115},
 publisher = {},
 title = {{Q}{L}o{R}{A}: Efficient Finetuning of Quantized {L}{L}{M}s},
 volume = {36},
 year = {2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}


@article{zheng2023judging,
  title={Judging {L}{L}{M}-as-a-judge with {M}{T}-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}


@inproceedings{jang2016categorical,
  title={Categorical Reparametrization with {G}umble-Softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  year={2017},
  organization={}
}


@inproceedings{maddison2016concrete,
  title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
  author={Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
  booktitle={International Conference on Learning Representations},
  year={2016}
}


@article{tsne,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-{S}{N}{E}},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {}
}

@article{balazy2024lora,
  title={Lo{R}{A}-{X}{S}: Low-Rank Adaptation with Extremely Small Number of Parameters},
  author={Ba{\l}azy, Klaudia and Banaei, Mohammadreza and Aberer, Karl and Tabor, Jacek},
  journal={arXiv preprint arXiv:2405.17604},
  year={2024}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}

@article{yu2023metamath,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}