\begin{thebibliography}{10}

\bibitem{Agarwal11}
D.~Agarwal, L.~Li, and A.~J. Smola.
\newblock {Linear-Time Estimators for Propensity Scores}.
\newblock In {\em {$14^{th}$ Intl. Conference on Artificial Intelligence and
  Statistics (AISTATS)}}, pages 93--100, 2011.

\bibitem{confbad}
R.~B. Akshayvarun~Subramanya, Suraj~Srinivas.
\newblock Confidence estimation in deep neural networks via density modelling.
\newblock {\em arXiv preprint arXiv:1707.07013}, 2017.

\bibitem{probes}
G.~Alain and Y.~Bengio.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock {\em arXiv preprint arXiv:1610.01644}, 2016.

\bibitem{modelcompr2}
L.~J. Ba and R.~Caurana.
\newblock Do deep nets really need to be deep?
\newblock {\em CoRR}, abs/1312.6184, 2013.

\bibitem{bach2015pixel}
S.~Bach, A.~Binder, G.~Montavon, F.~Klauschen, K.-R. M{\"u}ller, and W.~Samek.
\newblock On pixel-wise explanations for non-linear classifier decisions by
  layer-wise relevance propagation.
\newblock {\em PloS one}, 10(7):e0130140, 2015.

\bibitem{bastani2017interpreting}
O.~Bastani, C.~Kim, and H.~Bastani.
\newblock Interpreting blackbox models via model extraction.
\newblock {\em arXiv preprint arXiv:1705.08504}, 2017.

\bibitem{curriculumL}
Y.~Bengio, J.~Louradour, R.~Collobert, and J.~Weston.
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th Annual International Conference on
  Machine Learning}, 2009.

\bibitem{boucheron2005theory}
S.~Boucheron, O.~Bousquet, and G.~Lugosi.
\newblock Theory of classification: A survey of some recent advances.
\newblock {\em ESAIM: probability and statistics}, 9:323--375, 2005.

\bibitem{modelcompr}
C.~Bucilu\v{a}, R.~Caruana, and A.~Niculescu-Mizil.
\newblock Model compression.
\newblock In {\em Proceedings of the 12th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, 2006.

\bibitem{facialex}
P.-L. Carrier and A.~Courville.
\newblock Challenges in representation learning: Facial expression recognition
  challenge.
\newblock {\em ICML}, 2013.

\bibitem{pc2}
Y.-H. Chen, J.~Emer, and V.~Sze.
\newblock Eyeriss: A spatial architecture for energy-efficient dataflow for
  convolutional neural networks.
\newblock In {\em 2016 ACM/IEEE 43rd Annual International Symposium on Computer
  Architecture}, 2016.

\bibitem{tip}
A.~Dhurandhar, V.~Iyengar, R.~Luss, and K.~Shanmugam.
\newblock Tip: Typifying the interpretability of procedures.
\newblock {\em arXiv preprint arXiv:1706.02952}, 2017.

\bibitem{boost}
Y.~Freund and R.~E. Schapire.
\newblock Decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock {\em Journal of Computer and System Sciences}, 55(1):119--139, 1997.

\bibitem{distill}
J.~D. Geoffrey~Hinton, Oriol~Vinyals.
\newblock Distilling the knowledge in a neural network.
\newblock In {\em https://arxiv.org/abs/1503.02531}, 2015.

\bibitem{gan}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock {\em Deep Learning.}
\newblock MIT Press, 2016.

\bibitem{Grippo}
L.~Grippo and M.~Sciandrone.
\newblock On the convergence of the block nonlinear gauss-seidel method under
  convex constraints.
\newblock {\em Oper. Res. Lett.}, 26(3):127--136, 2000.

\bibitem{calib}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Weinberger.
\newblock On calibration of modern neural networks.
\newblock {\em Intl. Conference on Machine Learning (ICML)}, 2017.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Intl. Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem{cifar}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock In {\em Tech. Report}. 2009.

\bibitem{smdata}
M.~Lindstrom.
\newblock {\em Small Data: The Tiny Clues that Uncover Huge Trends}.
\newblock St. Martin's Press, 2016.

\bibitem{liptondflaws}
Z.~C. Lipton.
\newblock (deep learning’s deep flaws)’s deep flaws.
\newblock In {\em kdnuggets}. 2015.
\newblock
  https://www.kdnuggets.com/2015/01/deep-learning-flaws-universal-machine-learning.html.

\bibitem{priv16}
D.~Lopez-Paz, L.~Bottou, B.~Sch\"{o}lkopf, and V.~Vapnik.
\newblock Unifying distillation and privileged information.
\newblock In {\em International Conference on Learning Representations (ICLR
  2016)}, 2016.

\bibitem{montavon2017methods}
G.~Montavon, W.~Samek, and K.-R. M{\"u}ller.
\newblock Methods for interpreting and understanding deep neural networks.
\newblock {\em Digital Signal Processing}, 2017.

\bibitem{pc1}
B.~Reagen, P.~Whatmough, R.~Adolf, S.~Rama, H.~Lee, S.~K. Lee, J.~Miguel,
  Hernandez-Lobato, G.-Y. Wei, and D.~Brooks.
\newblock Minerva: Enabling low-power, highly-accurate deep neural network
  accelerators.
\newblock {\em IEEE Explore}, 2016.

\bibitem{lime}
M.~Ribeiro, S.~Singh, and C.~Guestrin.
\newblock "why should i trust you?” explaining the predictions of any
  classifier.
\newblock In {\em ACM SIGKDD Intl. Conference on Knowledge Discovery and Data
  Mining}, 2016.

\bibitem{fitnet}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock {\em arXiv preprint arXiv:1412.6550}, 2015.

\bibitem{unifiedPI}
S.-I.~L. Scott~Lundberg.
\newblock Unified framework for interpretable methods.
\newblock In {\em In Advances of Neural Inf. Proc. Systems}, 2017.

\bibitem{selvaraju2016grad}
R.~R. Selvaraju, M.~Cogswell, A.~Das, R.~Vedantam, D.~Parikh, and D.~Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock {\em See https://arxiv. org/abs/1610.02391 v3}, 2016.

\bibitem{saliency}
K.~Simonyan, A.~Vedaldi, and A.~Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock {\em CoRR}, abs/1312.6034, 2013.

\bibitem{distillnew}
S.~Tan, R.~Caruana, G.~Hooker, and Y.~Lou.
\newblock Auditing black-box models using transparent model distillation with
  side information.
\newblock {\em CoRR}, 2017.

\bibitem{sholom}
S.~Weiss, A.~Dhurandhar, and R.~Baseman.
\newblock Improving quality control by early prediction of manufacturing
  outcomes.
\newblock In {\em ACM SIGKDD conference on Knowledge Discovery and Data Mining
  (KDD)}, 2013.

\end{thebibliography}
