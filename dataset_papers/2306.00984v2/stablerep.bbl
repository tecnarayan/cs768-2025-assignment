\begin{thebibliography}{10}

\bibitem{abu2018augmented}
Hassan Abu~Alhaija, Siva~Karthik Mustikovela, Lars Mescheder, Andreas Geiger,
  and Carsten Rother.
\newblock Augmented reality meets computer vision: Efficient data generation
  for urban driving scenes.
\newblock {\em IJCV}, 2018.

\bibitem{azizi2023synthetic}
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and
  David~J Fleet.
\newblock Synthetic data from diffusion models improves imagenet
  classification.
\newblock {\em arXiv preprint arXiv:2304.08466}, 2023.

\bibitem{ediffi}
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten
  Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et~al.
\newblock ediffi: Text-to-image diffusion models with an ensemble of expert
  denoisers.
\newblock {\em arXiv preprint arXiv:2211.01324}, 2022.

\bibitem{baradad2021learning}
Manel Baradad~Jurjo, Jonas Wulff, Tongzhou Wang, Phillip Isola, and Antonio
  Torralba.
\newblock Learning to see by looking at noise.
\newblock In {\em NeurIPS}, 2021.

\bibitem{bossard2014food}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In {\em ECCV}, 2014.

\bibitem{dino}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{muse}
Huiwen Chang, Han Zhang, Jarred Barber, AJ~Maschinot, Jose Lezama, Lu~Jiang,
  Ming-Hsuan Yang, Kevin Murphy, William~T Freeman, Michael Rubinstein, et~al.
\newblock Muse: Text-to-image generation via masked generative transformers.
\newblock {\em arXiv preprint arXiv:2301.00704}, 2023.

\bibitem{chang2022maskgit}
Huiwen Chang, Han Zhang, Lu~Jiang, Ce~Liu, and William~T Freeman.
\newblock Maskgit: Masked generative image transformer.
\newblock In {\em CVPR}, 2022.

\bibitem{cc12m}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em CVPR}, 2021.

\bibitem{simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em ICML}, 2020.

\bibitem{mocov3}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{chen2019learning}
Yuhua Chen, Wen Li, Xiaoran Chen, and Luc~Van Gool.
\newblock Learning semantic segmentation from synthetic data: A geometrically
  guided input-output adaptation approach.
\newblock In {\em CVPR}, 2019.

\bibitem{cimpoi2014describing}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea
  Vedaldi.
\newblock Describing textures in the wild.
\newblock In {\em CVPR}, 2014.

\bibitem{dan2020generative}
Yabo Dan, Yong Zhao, Xiang Li, Shaobo Li, Ming Hu, and Jianjun Hu.
\newblock Generative adversarial networks (gan) based efficient sampling of
  chemical composition space for inverse design of inorganic materials.
\newblock {\em NPJ Computational Materials}, 2020.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\bibitem{desai2021redcaps}
Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson.
\newblock Redcaps: Web-curated image-text data created by the people, for the
  people.
\newblock {\em arXiv preprint arXiv:2111.11431}, 2021.

\bibitem{dhillon2019baseline}
Guneet~S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto.
\newblock A baseline for few-shot image classification.
\newblock {\em arXiv preprint arXiv:1909.02729}, 2019.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{el2023learning}
Mohamed El~Banani, Karan Desai, and Justin Johnson.
\newblock Learning visual representations via language-guided sampling.
\newblock In {\em CVPR}, 2023.

\bibitem{ericsson2021well}
Linus Ericsson, Henry Gouk, and Timothy~M Hospedales.
\newblock How well do self-supervised models transfer?
\newblock In {\em CVPR}, 2021.

\bibitem{everingham2010pascal}
Mark Everingham, Luc Van~Gool, Christopher~KI Williams, John Winn, and Andrew
  Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em IJCV}, 2010.

\bibitem{fei2006one}
Li~Fei-Fei, Robert Fergus, and Pietro Perona.
\newblock One-shot learning of object categories.
\newblock {\em TPAMI}, 2006.

\bibitem{fischer2015flownet}
Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip H{\"a}usser, Caner
  Haz{\i}rba{\c{s}}, Vladimir Golkov, Patrick Van~der Smagt, Daniel Cremers,
  and Thomas Brox.
\newblock Flownet: Learning optical flow with convolutional networks.
\newblock {\em arXiv preprint arXiv:1504.06852}, 2015.

\bibitem{georghiades1998illumination}
Athinodoros~S Georghiades, David~J Kriegman, and PN~Belhurneur.
\newblock Illumination cones for recognition under variable lighting: Faces.
\newblock In {\em CVPR}, 1998.

\bibitem{byol}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{mae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em CVPR}, 2022.

\bibitem{he2019momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock {\em arXiv:1911.05722}, 2019.

\bibitem{he2022synthetic}
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
  Bai, and Xiaojuan Qi.
\newblock Is synthetic data from generative models ready for image recognition?
\newblock {\em arXiv preprint arXiv:2210.07574}, 2022.

\bibitem{he2022generate}
Xuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haffari, and Mohammad Norouzi.
\newblock Generate, annotate, and learn: Nlp with synthetic text.
\newblock {\em TACL}, 2022.

\bibitem{hinton1995wake}
Geoffrey~E Hinton, Peter Dayan, Brendan~J Frey, and Radford~M Neal.
\newblock The "wake-sleep" algorithm for unsupervised neural networks.
\newblock {\em Science}, 1995.

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In {\em NeurIPS}, 2020.

\bibitem{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock {\em arXiv preprint arXiv:2207.12598}, 2022.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{ionescu2013human3}
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu.
\newblock Human3. 6m: Large scale datasets and predictive methods for 3d human
  sensing in natural environments.
\newblock {\em TPAMI}, 2013.

\bibitem{jahanian2021generative}
Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.
\newblock Generative models as a data source for multiview representation
  learning.
\newblock {\em arXiv preprint arXiv:2106.05258}, 2021.

\bibitem{gigaGAN}
Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain
  Paris, and Taesung Park.
\newblock Scaling up gans for text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2303.05511}, 2023.

\bibitem{karkkainen2019fairface}
Kimmo K{\"a}rkk{\"a}inen and Jungseock Joo.
\newblock Fairface: Face attribute dataset for balanced race, gender, and age.
\newblock {\em arXiv preprint arXiv:1908.04913}, 2019.

\bibitem{kawar2022imagic}
Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar
  Mosseri, and Michal Irani.
\newblock Imagic: Text-based real image editing with diffusion models.
\newblock {\em arXiv preprint arXiv:2210.09276}, 2022.

\bibitem{supcon}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em arXiv:2004.11362}, 2020.

\bibitem{kornblith2019better}
Simon Kornblith, Jonathon Shlens, and Quoc~V Le.
\newblock Do better imagenet models transfer better?
\newblock In {\em CVPR}, 2019.

\bibitem{krause20133d}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em ICCV workshop}, 2013.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock {\em Tech Report}, 2009.

\bibitem{kumar2020data}
Varun Kumar, Ashutosh Choudhary, and Eunah Cho.
\newblock Data augmentation using pre-trained transformer models.
\newblock {\em arXiv preprint arXiv:2003.02245}, 2020.

\bibitem{li2018training}
Jason Li, Ravi Gadde, Boris Ginsburg, and Vitaly Lavrukhin.
\newblock Training neural speech recognition systems with synthetic speech
  augmentation.
\newblock {\em arXiv preprint arXiv:1811.00707}, 2018.

\bibitem{liu2022palm}
Hao Liu, Tom Zahavy, Volodymyr Mnih, and Satinder Singh.
\newblock Palm up: Playing in the latent manifold for unsupervised pretraining.
\newblock {\em arXiv preprint arXiv:2210.10913}, 2022.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{maji2013fine}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock {\em arXiv preprint arXiv:1306.5151}, 2013.

\bibitem{mayer2016large}
Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers,
  Alexey Dosovitskiy, and Thomas Brox.
\newblock A large dataset to train convolutional networks for disparity,
  optical flow, and scene flow estimation.
\newblock In {\em CVPR}, 2016.

\bibitem{meng2022generating}
Yu~Meng, Jiaxin Huang, Yu~Zhang, and Jiawei Han.
\newblock Generating training data with language models: Towards zero-shot
  language understanding.
\newblock {\em arXiv preprint arXiv:2202.04538}, 2022.

\bibitem{mimura2018leveraging}
Masato Mimura, Sei Ueno, Hirofumi Inaguma, Shinsuke Sakai, and Tatsuya
  Kawahara.
\newblock Leveraging sequence-to-sequence speech synthesis for enhancing
  acoustic-to-word speech recognition.
\newblock In {\em SLT}, 2018.

\bibitem{mu2022slip}
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
\newblock Slip: Self-supervision meets language-image pre-training.
\newblock In {\em ECCV}, 2022.

\bibitem{nene1996columbia}
Sameer~A Nene, Shree~K Nayar, Hiroshi Murase, et~al.
\newblock Columbia object image library (coil-20).
\newblock {\em Tech Report}, 1996.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em Indian Conference on Computer Vision, Graphics \& Image
  Processing}, 2008.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In {\em CVPR}, 2012.

\bibitem{pathak2016context}
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei~A
  Efros.
\newblock Context encoders: Feature learning by inpainting.
\newblock In {\em CVPR}, 2016.

\bibitem{peng2015learning}
Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko.
\newblock Learning deep object detectors from 3d models.
\newblock In {\em CVPR}, 2015.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, 2021.

\bibitem{dalle2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 2022.

\bibitem{ren2018cross}
Zhongzheng Ren and Yong~Jae Lee.
\newblock Cross-domain self-supervised multi-task feature learning using
  synthetic imagery.
\newblock In {\em CVPR}, 2018.

\bibitem{ldm}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em CVPR}, 2022.

\bibitem{ros2016synthia}
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio~M
  Lopez.
\newblock The synthia dataset: A large collection of synthetic images for
  semantic segmentation of urban scenes.
\newblock In {\em CVPR}, 2016.

\bibitem{rosenberg2019speech}
Andrew Rosenberg, Yu~Zhang, Bhuvana Ramabhadran, Ye~Jia, Pedro Moreno, Yonghui
  Wu, and Zelin Wu.
\newblock Speech recognition with augmented synthesized speech.
\newblock In {\em ASRU}, 2019.

\bibitem{rossenbach2020generating}
Nick Rossenbach, Albert Zeyer, Ralf Schl{\"u}ter, and Hermann Ney.
\newblock Generating synthetic audio data for attention-based speech
  recognition systems.
\newblock In {\em ICASSP}, 2020.

\bibitem{rozantsev2015rendering}
Artem Rozantsev, Vincent Lepetit, and Pascal Fua.
\newblock On rendering synthetic images for training an object detector.
\newblock {\em Computer Vision and Image Understanding}, 2015.

\bibitem{ruiz2022dreambooth}
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
  Kfir Aberman.
\newblock Dreambooth: Fine tuning text-to-image diffusion models for
  subject-driven generation.
\newblock {\em arXiv preprint arXiv:2208.12242}, 2022.

\bibitem{imagen}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L
  Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim
  Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock In {\em NeurIPS}, 2022.

\bibitem{samaria1994parameterisation}
Ferdinando~S Samaria and Andy~C Harter.
\newblock Parameterisation of a stochastic model for human face identification.
\newblock In {\em IEEE workshop on applications of computer vision}, 1994.

\bibitem{sariyildiz2023fake}
Mert~Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis.
\newblock Fake it till you make it: Learning transferable representations from
  synthetic imagenet clones.
\newblock In {\em CVPR}, 2023.

\bibitem{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{cc3m}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In {\em ACL}, 2018.

\bibitem{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 2017.

\bibitem{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em ICML}, 2015.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{sutton1991dyna}
Richard~S Sutton.
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock {\em ACM Sigart Bulletin}, 1991.

\bibitem{taori2023alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock {\em Stanford Center for Research on Foundation Models.}, 2023.

\bibitem{MoCLR}
Yonglong Tian, Olivier~J Henaff, and A{\"a}ron van~den Oord.
\newblock Divide and contrast: Self-supervised learning from uncurated data.
\newblock In {\em ICCV}, 2021.

\bibitem{tian2019contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive multiview coding.
\newblock {\em arXiv:1906.05849}, 2019.

\bibitem{tian2020makes}
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and
  Phillip Isola.
\newblock What makes for good views for contrastive learning?
\newblock In {\em NeurIPS}, 2020.

\bibitem{tian2020rethinking}
Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua~B Tenenbaum, and Phillip Isola.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock In {\em ECCV}, 2020.

\bibitem{tucker2020generating}
Allan Tucker, Zhenchen Wang, Ylenia Rotalinti, and Puja Myles.
\newblock Generating high-fidelity synthetic patient data for assessing machine
  learning healthcare software.
\newblock {\em NPJ digital medicine}, 2020.

\bibitem{varol2017learning}
Gul Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael~J Black, Ivan
  Laptev, and Cordelia Schmid.
\newblock Learning from synthetic humans.
\newblock In {\em CVPR}, 2017.

\bibitem{wang2019simpleshot}
Yan Wang, Wei-Lun Chao, Kilian~Q Weinberger, and Laurens van~der Maaten.
\newblock Simpleshot: Revisiting nearest-neighbor classification for few-shot
  learning.
\newblock {\em arXiv preprint arXiv:1911.04623}, 2019.

\bibitem{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em CVPR}, 2018.

\bibitem{xiao2010sun}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In {\em CVPR}, 2010.

\bibitem{xiao2018unified}
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
\newblock Unified perceptual parsing for scene understanding.
\newblock In {\em ECCV}, 2018.

\bibitem{yang2020generative}
Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan~Le
  Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey.
\newblock Generative data augmentation for commonsense reasoning.
\newblock {\em arXiv preprint arXiv:2004.11546}, 2020.

\bibitem{parti}
Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang,
  Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al.
\newblock Scaling autoregressive models for content-rich text-to-image
  generation.
\newblock {\em arXiv preprint arXiv:2206.10789}, 2022.

\bibitem{yuksekgonul2022and}
Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James
  Zou.
\newblock When and why vision-language models behave like bags-of-words, and
  what to do about it?
\newblock In {\em ICLR}, 2022.

\bibitem{zhang2023adding}
Lvmin Zhang and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock {\em arXiv preprint arXiv:2302.05543}, 2023.

\bibitem{zhou2019semantic}
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso,
  and Antonio Torralba.
\newblock Semantic understanding of scenes through the ade20k dataset.
\newblock {\em IJCV}, 2019.

\end{thebibliography}
