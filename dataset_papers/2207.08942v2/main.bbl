\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning~(ICML)}, pp.\
  244--253, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems~(NeurIPS)},
  2019.

\bibitem[De~Lathauwer et~al.(2000)De~Lathauwer, De~Moor, and
  Vandewalle]{de2000multilinear}
De~Lathauwer, L., De~Moor, B., and Vandewalle, J.
\newblock A multilinear singular value decomposition.
\newblock \emph{SIAM journal on Matrix Analysis and Applications}, 21\penalty0
  (4):\penalty0 1253--1278, 2000.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Du, S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock \emph{Advances in Neural Information Processing Systems~(NeurIPS)},
  31, 2018.

\bibitem[Gandy et~al.(2011)Gandy, Recht, and Yamada]{gandy2011tensor}
Gandy, S., Recht, B., and Yamada, I.
\newblock Tensor completion and low-n-rank tensor recovery via convex
  optimization.
\newblock \emph{Inverse problems}, 27\penalty0 (2):\penalty0 025010, 2011.

\bibitem[Ge et~al.(2021)Ge, Ren, Wang, and Zhou]{ge2021understanding}
Ge, R., Ren, Y., Wang, X., and Zhou, M.
\newblock Understanding deflation process in over-parametrized tensor
  decomposition.
\newblock \emph{Advances in Neural Information Processing Systems~(NeurIPS)},
  34, 2021.

\bibitem[Grasedyck et~al.(2013)Grasedyck, Kressner, and
  Tobler]{grasedyck2013literature}
Grasedyck, L., Kressner, D., and Tobler, C.
\newblock A literature survey of low-rank tensor approximation techniques.
\newblock \emph{GAMM-Mitteilungen}, 36\penalty0 (1):\penalty0 53--78, 2013.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekarimplicit}
Gunasekar, S., Woodworth, B., Bhojanapalli, S., Neyshabur, B., and Srebro, N.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems~(NeurIPS)},
  2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Jing et~al.(2020)Jing, Zbontar, et~al.]{jing2020implicit}
Jing, L., Zbontar, J., et~al.
\newblock Implicit rank-minimizing autoencoder.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Kolda \& Bader(2009)Kolda and Bader]{Kolda}
Kolda, T.~G. and Bader, B.~W.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM Review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[Kumar \& Poole(2020)Kumar and Poole]{kumar2020implicit}
Kumar, A. and Poole, B.
\newblock On implicit regularization in $\beta$-{VAE}s.
\newblock In \emph{International Conference on Machine Learning~(ICML)}, 2020.

\bibitem[Li et~al.(2021)Li, Luo, and Lyu]{li2021towards}
Li, Z., Luo, Y., and Lyu, K.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock In \emph{International Conference on Learning Representation~(ICLR)},
  2021.

\bibitem[Lozano et~al.(2009)Lozano, Li, Niculescu-Mizil, Liu, Perlich, Hosking,
  and Abe]{lozano2009spatial}
Lozano, A.~C., Li, H., Niculescu-Mizil, A., Liu, Y., Perlich, C., Hosking, J.,
  and Abe, N.
\newblock Spatial-temporal causal modeling for climate change attribution.
\newblock In \emph{Proceedings of the 15th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  587--596, 2009.

\bibitem[Milanesi et~al.(2021)Milanesi, Kadri, Ayache, and
  Arti{\`e}res]{milanesi2021implicit}
Milanesi, P., Kadri, H., Ayache, S., and Arti{\`e}res, T.
\newblock Implicit regularization in deep tensor factorization.
\newblock In \emph{International Joint Conference on Neural Networks~(IJCNN)},
  2021.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Oseledets(2011)]{oseledets2011tensor}
Oseledets, I.~V.
\newblock Tensor-train decomposition.
\newblock \emph{SIAM Journal on Scientific Computing}, 33\penalty0
  (5):\penalty0 2295--2317, 2011.

\bibitem[Razin \& Cohen(2020)Razin and Cohen]{razin2020implicit}
Razin, N. and Cohen, N.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock \emph{Advances in Neural Information Processing Systems~(NeurIPS)},
  2020.

\bibitem[Razin et~al.(2021)Razin, Maman, and Cohen]{razin2021implicit}
Razin, N., Maman, A., and Cohen, N.
\newblock Implicit regularization in tensor factorization.
\newblock In \emph{International Conference on Machine Learning~(ICML)}, 2021.

\bibitem[Razin et~al.(2022)Razin, Maman, and Cohen]{razin2022implicit}
Razin, N., Maman, A., and Cohen, N.
\newblock Implicit regularization in hierarchical tensor factorization and deep
  convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:2201.11729}, 2022.

\bibitem[Song et~al.(2019)Song, Ge, Caverlee, and Hu]{song2019tensor}
Song, Q., Ge, H., Caverlee, J., and Hu, X.
\newblock Tensor completion algorithms in big data analytics.
\newblock \emph{ACM Transactions on Knowledge Discovery from Data (TKDD)},
  13\penalty0 (1):\penalty0 1--48, 2019.

\bibitem[Zou et~al.(2021)Zou, Wu, Braverman, Gu, Foster, and
  Kakade]{zou2021benefits}
Zou, D., Wu, J., Braverman, V., Gu, Q., Foster, D.~P., and Kakade, S.
\newblock The benefits of implicit regularization from {SGD} in least squares
  problems.
\newblock \emph{Advances in Neural Information Processing Systems~(NeurIPS)},
  2021.

\end{thebibliography}
