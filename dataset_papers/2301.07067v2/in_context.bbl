\begin{thebibliography}{10}

\bibitem{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock {\em arXiv preprint arXiv:2211.15661}, 2022.

\bibitem{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock {\em Machine learning}, 79(1):151--175, 2010.

\bibitem{block2023smoothed}
Adam Block, Max Simchowitz, and Russ Tedrake.
\newblock Smoothed online learning for prediction in piecewise affine systems.
\newblock {\em arXiv preprint arXiv:2301.11187}, 2023.

\bibitem{bousquet2000algorithmic}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Algorithmic stability and generalization performance.
\newblock {\em Advances in Neural Information Processing Systems}, 13, 2000.

\bibitem{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em The Journal of Machine Learning Research}, 2:499--526, 2002.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{chen2022understanding}
Lisha Chen, Songtao Lu, and Tianyi Chen.
\newblock Understanding benign overfitting in gradient-based meta learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{cheng2022provable}
Yuan Cheng, Songtao Feng, Jing Yang, Hong Zhang, and Yingbin Liang.
\newblock Provable benefit of multitask representation learning in
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2206.05900}, 2022.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{collins2022maml}
Liam Collins, Aryan Mokhtari, Sewoong Oh, and Sanjay Shakkottai.
\newblock Maml and anil provably learn representations.
\newblock {\em arXiv preprint arXiv:2202.03483}, 2022.

\bibitem{dai2022can}
Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Zhifang Sui, and Furu Wei.
\newblock Why can gpt learn in-context? language models secretly perform
  gradient descent as meta optimizers.
\newblock {\em arXiv preprint arXiv:2212.10559}, 2022.

\bibitem{dean2020sample}
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock {\em Foundations of Computational Mathematics}, 20(4):633--679, 2020.

\bibitem{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock {\em arXiv preprint arXiv:2002.09434}, 2020.

\bibitem{faradonbeh2022joint}
Mohamad Kazem~Shirani Faradonbeh and Aditya Modi.
\newblock Joint learning-based stabilization of multiple unknown linear
  systems.
\newblock {\em arXiv preprint arXiv:2201.01387}, 2022.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem{foster2020learning}
Dylan Foster, Tuhin Sarkar, and Alexander Rakhlin.
\newblock Learning nonlinear dynamical systems from a single trajectory.
\newblock In {\em Learning for Dynamics and Control}, pages 851--861. PMLR,
  2020.

\bibitem{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock {\em Neural Information Processing Systems}, 2022.

\bibitem{hanneke2019value}
Steve Hanneke and Samory Kpotufe.
\newblock On the value of target data in transfer learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{hollmann2022tabpfn}
Noah Hollmann, Samuel M{\"u}ller, Katharina Eggensperger, and Frank Hutter.
\newblock Tabpfn: A transformer that solves small tabular classification
  problems in a second.
\newblock {\em arXiv preprint arXiv:2207.01848}, 2022.

\bibitem{hugface}
HuggingFace.
\newblock Huggingface pretrained models.

\bibitem{kong2020meta}
Weihao Kong, Raghav Somani, Zhao Song, Sham Kakade, and Sewoong Oh.
\newblock Meta-learning for mixed linear regression.
\newblock In {\em International Conference on Machine Learning}, pages
  5394--5404. PMLR, 2020.

\bibitem{kuznetsov2014generalization}
Vitaly Kuznetsov and Mehryar Mohri.
\newblock Generalization bounds for time series prediction with non-stationary
  processes.
\newblock In {\em International conference on algorithmic learning theory}.
  Springer, 2014.

\bibitem{kuznetsov2016time}
Vitaly Kuznetsov and Mehryar Mohri.
\newblock Time series prediction and online learning.
\newblock In {\em Conference on Learning Theory}, pages 1190--1213. PMLR, 2016.

\bibitem{laskin2022context}
Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer,
  Richie Steigerwald, DJ~Strouse, Steven Hansen, Angelos Filos, Ethan Brooks,
  et~al.
\newblock In-context reinforcement learning with algorithm distillation.
\newblock {\em arXiv preprint arXiv:2210.14215}, 2022.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock {\em arXiv preprint arXiv:2104.08691}, 2021.

\bibitem{li2022provable}
Yingcong Li, Mingchen Li, M~Salman Asif, and Samet Oymak.
\newblock Provable and efficient continual representation learning.
\newblock {\em arXiv preprint arXiv:2203.02026}, 2022.

\bibitem{lindley1972bayes}
Dennis~V Lindley and Adrian~FM Smith.
\newblock Bayes estimates for the linear model.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 34(1):1--18, 1972.

\bibitem{ljung1998system}
Lennart Ljung.
\newblock System identification.
\newblock In {\em Signal analysis and prediction}, pages 163--173. Springer,
  1998.

\bibitem{mania2020active}
Horia Mania, Michael~I Jordan, and Benjamin Recht.
\newblock Active learning for nonlinear system identification with guarantees.
\newblock {\em arXiv preprint arXiv:2006.10277}, 2020.

\bibitem{matni2019tutorial}
Nikolai Matni and Stephen Tu.
\newblock A tutorial on concentration bounds for system identification.
\newblock In {\em 2019 IEEE 58th Conference on Decision and Control (CDC)},
  pages 3741--3749. IEEE, 2019.

\bibitem{maurer2016vector}
Andreas Maurer.
\newblock A vector-contraction inequality for rademacher complexities.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 3--17. Springer, 2016.

\bibitem{maurer2016benefit}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock {\em Journal of Machine Learning Research}, 17(81):1--32, 2016.

\bibitem{mcdonald2017nonparametric}
Daniel~J McDonald, Cosma~Rohilla Shalizi, and Mark Schervish.
\newblock Nonparametric risk bounds for time-series forecasting.
\newblock {\em The Journal of Machine Learning Research}, 18(1):1044--1083,
  2017.

\bibitem{modi2021joint}
Aditya Modi, Mohamad Kazem~Shirani Faradonbeh, Ambuj Tewari, and George
  Michailidis.
\newblock Joint learning of linear time-invariant dynamical systems.
\newblock {\em arXiv preprint arXiv:2112.10955}, 2021.

\bibitem{mohri2008rademacher}
Mehryar Mohri and Afshin Rostamizadeh.
\newblock Rademacher complexity bounds for non-iid processes.
\newblock {\em Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem{mohri2010stability}
Mehryar Mohri and Afshin Rostamizadeh.
\newblock Stability bounds for stationary $\varphi$-mixing and $\beta$-mixing
  processes.
\newblock {\em Journal of Machine Learning Research}, 11(2), 2010.

\bibitem{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem{neyshabur2017geometry}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock {\em arXiv preprint arXiv:1705.03071}, 2017.

\bibitem{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock {\em arXiv preprint arXiv:2209.11895}, 2022.

\bibitem{oymak2021revisiting}
Samet Oymak and Necmiye Ozay.
\newblock Revisiting ho--kalman-based system identification: Robustness and
  finite-sample analysis.
\newblock {\em IEEE Transactions on Automatic Control}, 67(4):1914--1928, 2021.

\bibitem{pillonetto2016regularized}
Gianluigi Pillonetto, Tianshi Chen, Alessandro Chiuso, Giuseppe De~Nicolao, and
  Lennart Ljung.
\newblock Regularized linear system identification using atomic, nuclear and
  kernel-based norms: The role of the stability constraint.
\newblock {\em Automatica}, 69:137--149, 2016.

\bibitem{qin2022non}
Yuzhen Qin, Tommaso Menara, Samet Oymak, Shinung Ching, and Fabio Pasqualetti.
\newblock Non-stationary representation learning in sequential linear bandits.
\newblock {\em IEEE Open Journal of Control Systems}, 2022.

\bibitem{richards2021asymptotics}
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco.
\newblock Asymptotics of ridge (less) regression under general source
  condition.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3889--3897. PMLR, 2021.

\bibitem{sarkar2019near}
Tuhin Sarkar and Alexander Rakhlin.
\newblock Near optimal finite time identification of arbitrary linear dynamical
  systems.
\newblock In {\em International Conference on Machine Learning}, pages
  5610--5618. PMLR, 2019.

\bibitem{sattar2022non}
Yahya Sattar and Samet Oymak.
\newblock Non-asymptotic and accurate learning of nonlinear dynamical systems.
\newblock {\em Journal of Machine Learning Research}, 23(140):1--49, 2022.

\bibitem{simchowitz2018learning}
Max Simchowitz, Horia Mania, Stephen Tu, Michael~I Jordan, and Benjamin Recht.
\newblock Learning without mixing: Towards a sharp analysis of linear system
  identification.
\newblock In {\em Conference On Learning Theory}, pages 439--473. PMLR, 2018.

\bibitem{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em The Journal of Machine Learning Research}, 19(1):2822--2878,
  2018.

\bibitem{sun2021towards}
Yue Sun, Adhyyan Narang, Ibrahim Gulluk, Samet Oymak, and Maryam Fazel.
\newblock Towards sample-efficient overparameterized meta-learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:28156--28168, 2021.

\bibitem{sun2022finite}
Yue Sun, Samet Oymak, and Maryam Fazel.
\newblock Finite sample identification of low-order lti systems via nuclear
  norm regularization.
\newblock {\em IEEE Open Journal of Control Systems}, 1:237--254, 2022.

\bibitem{tripuraneni2021provable}
Nilesh Tripuraneni, Chi Jin, and Michael Jordan.
\newblock Provable meta-learning of linear representations.
\newblock In {\em International Conference on Machine Learning}, pages
  10434--10443. PMLR, 2021.

\bibitem{tripuraneni2020theory}
Nilesh Tripuraneni, Michael Jordan, and Chi Jin.
\newblock On the theory of transfer learning: The importance of task diversity.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7852--7862, 2020.

\bibitem{tsiamis2022statistical}
Anastasios Tsiamis, Ingvar Ziemann, Nikolai Matni, and George~J Pappas.
\newblock Statistical learning theory for control: A finite sample perspective.
\newblock {\em arXiv preprint arXiv:2209.05423}, 2022.

\bibitem{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem{von2022transformers}
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock {\em arXiv preprint arXiv:2212.07677}, 2022.

\bibitem{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock {\em arXiv preprint arXiv:2111.02080}, 2021.

\bibitem{yu1994rates}
Bin Yu.
\newblock Rates of convergence for empirical processes of stationary mixing
  sequences.
\newblock {\em The Annals of Probability}, pages 94--116, 1994.

\bibitem{zhang2022multi}
Thomas~T Zhang, Katie Kang, Bruce~D Lee, Claire Tomlin, Sergey Levine, Stephen
  Tu, and Nikolai Matni.
\newblock Multi-task imitation learning for linear dynamical systems.
\newblock {\em arXiv:2212.00186}, 2022.

\bibitem{ziemannlearning}
Ingvar Ziemann and Stephen Tu.
\newblock Learning with little mixing.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{ziemann2022single}
Ingvar~M Ziemann, Henrik Sandberg, and Nikolai Matni.
\newblock Single trajectory nonparametric learning of nonlinear dynamics.
\newblock In {\em conference on Learning Theory}, pages 3333--3364. PMLR, 2022.

\end{thebibliography}
