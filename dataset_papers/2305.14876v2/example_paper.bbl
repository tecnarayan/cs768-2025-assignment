\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barni et~al.(2019)Barni, Kallas, and Tondi]{barni2019new}
Barni, M., Kallas, K., and Tondi, B.
\newblock A new backdoor attack in cnns by training set corruption without
  label poisoning.
\newblock In \emph{ICIP}, 2019.

\bibitem[Chen et~al.(2019)Chen, Carvalho, Baracaldo, Ludwig, Edwards, Lee,
  Molloy, and Srivastava]{chen2018detecting}
Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T.,
  Molloy, I., and Srivastava, B.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock In \emph{AAAI Workshop}, 2019.

\bibitem[Chen et~al.(2022)Chen, Wu, and Wang]{chen2022effective}
Chen, W., Wu, B., and Wang, H.
\newblock Effective backdoor defense by exploiting sensitivity of poisoned
  samples.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{NeurIPS}, 2022.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{chen2017targeted}
Chen, X., Liu, C., Li, B., Lu, K., and Song, D.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017.

\bibitem[Cheng et~al.(2021)Cheng, Liu, Ma, and Zhang]{cheng2021deep}
Cheng, S., Liu, Y., Ma, S., and Zhang, X.
\newblock Deep feature space trojan attack of neural networks by controlled
  detoxification.
\newblock In \emph{AAAI}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Doan et~al.(2021)Doan, Lao, Zhao, and Li]{doan2021lira}
Doan, K., Lao, Y., Zhao, W., and Li, P.
\newblock Lira: Learnable, imperceptible and robust backdoor attacks.
\newblock In \emph{ICCV}, 2021.

\bibitem[Gao et~al.(2019)Gao, Xu, Wang, Chen, Ranasinghe, and
  Nepal]{gao2019strip}
Gao, Y., Xu, C., Wang, D., Chen, S., Ranasinghe, D.~C., and Nepal, S.
\newblock Strip: A defence against trojan attacks on deep neural networks.
\newblock In \emph{ACSAC}, 2019.

\bibitem[Garg et~al.(2020)Garg, Kumar, Goel, and Liang]{garg2020can}
Garg, S., Kumar, A., Goel, V., and Liang, Y.
\newblock Can adversarial weight perturbations inject neural backdoors.
\newblock In \emph{Proceedings of the 29th ACM International Conference on
  Information \& Knowledge Management}, 2020.

\bibitem[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Gu, T., Dolan-Gavitt, B., and Garg, S.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{arXiv preprint arXiv:1708.06733}, 2017.

\bibitem[Guan et~al.(2022)Guan, Tu, He, and Tao]{guan2022few}
Guan, J., Tu, Z., He, R., and Tao, D.
\newblock Few-shot backdoor defense using shapley estimation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Guo et~al.(2019)Guo, Wang, Xing, Du, and Song]{guo2019tabor}
Guo, W., Wang, L., Xing, X., Du, M., and Song, D.
\newblock Tabor: A highly accurate approach to inspecting and restoring trojan
  backdoors in ai systems.
\newblock \emph{arXiv preprint arXiv:1908.01763}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hu et~al.(2022)Hu, Lin, Cogswell, Yao, Jha, and Chen]{hu2022trigger}
Hu, X., Lin, X., Cogswell, M., Yao, Y., Jha, S., and Chen, C.
\newblock Trigger hunting with a topological prior for trojan detection.
\newblock In \emph{ICLR}, 2022.

\bibitem[Huang et~al.(2022)Huang, Li, Wu, Qin, and Ren]{huang2022backdoor}
Huang, K., Li, Y., Wu, B., Qin, Z., and Ren, K.
\newblock Backdoor defense via decoupling the training process.
\newblock In \emph{ICLR}, 2022.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li et~al.(2020)Li, Zhai, Wu, Jiang, Li, and Xia]{li2020rethinking}
Li, Y., Zhai, T., Wu, B., Jiang, Y., Li, Z., and Xia, S.
\newblock Rethinking the trigger of backdoor attack.
\newblock \emph{arXiv preprint arXiv:2004.04692}, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Li, Wu, Li, He, and
  Lyu]{li2021invisible}
Li, Y., Li, Y., Wu, B., Li, L., He, R., and Lyu, S.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In \emph{ICCV}, pp.\  16463--16472, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021anti}
Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., and Ma, X.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021neural}
Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., and Ma, X.
\newblock Neural attention distillation: Erasing backdoor triggers from deep
  neural networks.
\newblock In \emph{ICLR}, 2021{\natexlab{c}}.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Dolan-Gavitt, and
  Garg]{liu2018fine}
Liu, K., Dolan-Gavitt, B., and Garg, S.
\newblock Fine-pruning: Defending against backdooring attacks on deep neural
  networks.
\newblock In \emph{RAID}, 2018{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Ma, Aafer, Lee, Zhai, Wang, and
  Zhang]{liu2018trojaning}
Liu, Y., Ma, S., Aafer, Y., Lee, W.-C., Zhai, J., Wang, W., and Zhang, X.
\newblock Trojaning attack on neural networks.
\newblock In \emph{NDSS}, 2018{\natexlab{b}}.

\bibitem[Liu et~al.(2019)Liu, Lee, Tao, Ma, Aafer, and Zhang]{liu2019abs}
Liu, Y., Lee, W.-C., Tao, G., Ma, S., Aafer, Y., and Zhang, X.
\newblock Abs: Scanning neural networks for back-doors by artificial brain
  stimulation.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer
  and Communications Security}, pp.\  1265--1282, 2019.

\bibitem[Liu et~al.(2020)Liu, Ma, Bailey, and Lu]{liu2020reflection}
Liu, Y., Ma, X., Bailey, J., and Lu, F.
\newblock Reflection backdoor: A natural backdoor attack on deep neural
  networks.
\newblock In \emph{ECCV}, 2020.

\bibitem[Liu et~al.(2022)Liu, Shen, Tao, Wang, Ma, and Zhang]{liu2022complex}
Liu, Y., Shen, G., Tao, G., Wang, Z., Ma, S., and Zhang, X.
\newblock Complex backdoor detection by symmetric feature differencing.
\newblock In \emph{CVPR}, 2022.

\bibitem[Nguyen \& Tran(2020)Nguyen and Tran]{nguyen2020input}
Nguyen, A. and Tran, A.
\newblock Input-aware dynamic backdoor attack.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Nguyen \& Tran(2021)Nguyen and Tran]{nguyen2021wanet}
Nguyen, A. and Tran, A.
\newblock Wanet--imperceptible warping-based backdoor attack.
\newblock In \emph{ICLR}, 2021.

\bibitem[Qi et~al.(2022{\natexlab{a}})Qi, Xie, Mahloujifar, and
  Mittal]{qi2022circumventing}
Qi, X., Xie, T., Mahloujifar, S., and Mittal, P.
\newblock Circumventing backdoor defenses that are based on latent
  separability.
\newblock \emph{arXiv preprint arXiv:2205.13613}, 2022{\natexlab{a}}.

\bibitem[Qi et~al.(2022{\natexlab{b}})Qi, Xie, Pan, Zhu, Yang, and
  Bu]{qi2022towards}
Qi, X., Xie, T., Pan, R., Zhu, J., Yang, Y., and Bu, K.
\newblock Towards practical deployment-stage backdoor attack on deep neural
  networks.
\newblock In \emph{CVPR}, 2022{\natexlab{b}}.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Najibi, Suciu, Studer, Dumitras,
  and Goldstein]{shafahi2018poison}
Shafahi, A., Huang, W.~R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and
  Goldstein, T.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Shen et~al.(2021)Shen, Liu, Tao, An, Xu, Cheng, Ma, and
  Zhang]{shen2021backdoor}
Shen, G., Liu, Y., Tao, G., An, S., Xu, Q., Cheng, S., Ma, S., and Zhang, X.
\newblock Backdoor scanning for deep neural networks through k-arm
  optimization.
\newblock In \emph{ICML}, 2021.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, 2013.

\bibitem[Tang et~al.(2021)Tang, Wang, Tang, and Zhang]{tang2021demon}
Tang, D., Wang, X., Tang, H., and Zhang, K.
\newblock Demon in the variant: Statistical analysis of dnns for robust
  backdoor contamination detection.
\newblock In \emph{USENIX Security}, 2021.

\bibitem[Tran et~al.(2018)Tran, Li, and Madry]{tran2018spectral}
Tran, B., Li, J., and Madry, A.
\newblock Spectral signatures in backdoor attacks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Turner et~al.(2019)Turner, Tsipras, and Madry]{turner2019clean}
Turner, A., Tsipras, D., and Madry, A.
\newblock Clean-label backdoor attacks.
\newblock \emph{https://people.csail.mit.edu/madry/lab/}, 2019.

\bibitem[Wang et~al.(2019)Wang, Yao, Shan, Li, Viswanath, Zheng, and
  Zhao]{wang2019neural}
Wang, B., Yao, Y., Shan, S., Li, H., Viswanath, B., Zheng, H., and Zhao, B.~Y.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In \emph{S\&P}. IEEE, 2019.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Yao, Xu, An, Tong, and
  Wang]{wang2022invisible}
Wang, T., Yao, Y., Xu, F., An, S., Tong, H., and Wang, T.
\newblock An invisible black-box backdoor attack through frequency domain.
\newblock In \emph{ECCV}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Ding, Zhai, and
  Ma]{wang2022training}
Wang, Z., Ding, H., Zhai, J., and Ma, S.
\newblock Training with more confidence: Mitigating injected and natural
  backdoors during training.
\newblock In \emph{NeurIPS}, 2022{\natexlab{b}}.

\bibitem[Wu \& Wang(2021)Wu and Wang]{wu2021adversarial}
Wu, D. and Wang, Y.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Xu et~al.(2021)Xu, Wang, Li, Borisov, Gunter, and Li]{xu2021detecting}
Xu, X., Wang, Q., Li, H., Borisov, N., Gunter, C.~A., and Li, B.
\newblock Detecting ai trojans using meta neural analysis.
\newblock In \emph{S\&P}, 2021.

\bibitem[Zeng et~al.(2021)Zeng, Park, Mao, and Jia]{zeng2021rethinking}
Zeng, Y., Park, W., Mao, Z.~M., and Jia, R.
\newblock Rethinking the backdoor attacks' triggers: A frequency perspective.
\newblock In \emph{ICCV}, 2021.

\bibitem[Zeng et~al.(2022)Zeng, Chen, Park, Mao, Jin, and
  Jia]{zeng2021adversarial}
Zeng, Y., Chen, S., Park, W., Mao, Z.~M., Jin, M., and Jia, R.
\newblock Adversarial unlearning of backdoors via implicit hypergradient.
\newblock In \emph{ICLR}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Chen, Xuan, Dong, Wang, and
  Liang]{zhao2022defeat}
Zhao, Z., Chen, X., Xuan, Y., Dong, Y., Wang, D., and Liang, K.
\newblock Defeat: Deep hidden feature backdoor attacks by imperceptible
  perturbation and latent representation constraints.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zheng et~al.(2022)Zheng, Tang, Li, and Liu]{zheng2022data}
Zheng, R., Tang, R., Li, J., and Liu, L.
\newblock Data-free backdoor removal based on channel lipschitzness.
\newblock In \emph{ECCV}, 2022.

\end{thebibliography}
