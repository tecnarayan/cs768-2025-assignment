\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[AI@Meta(2024)]{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card, 2024.
\newblock URL \url{https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}.

\bibitem[Allen-Zhu and Li(2023{\natexlab{a}})]{allen2023physics}
Z.~Allen-Zhu and Y.~Li.
\newblock Physics of language models: Part 3.2, knowledge manipulation.
\newblock \emph{arXiv preprint arXiv:2309.14402}, 2023{\natexlab{a}}.

\bibitem[Allen-Zhu and Li(2023{\natexlab{b}})]{zhu2023physics}
Z.~Allen-Zhu and Y.~Li.
\newblock Physics of language models: Part 3.1, knowledge storage and extraction.
\newblock \emph{arXiv preprint arXiv:2309.14316}, 2023{\natexlab{b}}.

\bibitem[Anil et~al.(2022{\natexlab{a}})Anil, Pokle, Liang, Treutlein, Wu, Bai, Kolter, and Grosse]{anil2022path}
C.~Anil, A.~Pokle, K.~Liang, J.~Treutlein, Y.~Wu, S.~Bai, J.~Z. Kolter, and R.~B. Grosse.
\newblock Path independent equilibrium models can better exploit test-time computation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 7796--7809, 2022{\natexlab{a}}.

\bibitem[Anil et~al.(2022{\natexlab{b}})Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh, Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
C.~Anil, Y.~Wu, A.~Andreassen, A.~Lewkowycz, V.~Misra, V.~Ramasesh, A.~Slone, G.~Gur-Ari, E.~Dyer, and B.~Neyshabur.
\newblock Exploring length generalization in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 38546--38556, 2022{\natexlab{b}}.

\bibitem[Bengio et~al.(2023)Bengio, Hinton, Yao, Song, Abbeel, Harari, Zhang, Xue, Shalev-Shwartz, Hadfield, et~al.]{bengio2023managing}
Y.~Bengio, G.~Hinton, A.~Yao, D.~Song, P.~Abbeel, Y.~N. Harari, Y.-Q. Zhang, L.~Xue, S.~Shalev-Shwartz, G.~Hadfield, et~al.
\newblock Managing ai risks in an era of rapid progress.
\newblock \emph{arXiv preprint arXiv:2310.17688}, 2023.

\bibitem[Berglund et~al.(2023{\natexlab{a}})Berglund, Stickland, Balesni, Kaufmann, Tong, Korbak, Kokotajlo, and Evans]{berglund2023taken}
L.~Berglund, A.~C. Stickland, M.~Balesni, M.~Kaufmann, M.~Tong, T.~Korbak, D.~Kokotajlo, and O.~Evans.
\newblock Taken out of context: On measuring situational awareness in llms.
\newblock \emph{arXiv preprint arXiv:2309.00667}, 2023{\natexlab{a}}.

\bibitem[Berglund et~al.(2023{\natexlab{b}})Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans]{berglund2023reversal}
L.~Berglund, M.~Tong, M.~Kaufmann, M.~Balesni, A.~C. Stickland, T.~Korbak, and O.~Evans.
\newblock The reversal curse: Llms trained on" a is b" fail to learn" b is a".
\newblock \emph{arXiv preprint arXiv:2309.12288}, 2023{\natexlab{b}}.

\bibitem[Blum et~al.(2003)Blum, Kalai, and Wasserman]{blum2003noise}
A.~Blum, A.~Kalai, and H.~Wasserman.
\newblock Noise-tolerant learning, the parity problem, and the statistical query model.
\newblock \emph{Journal of the ACM (JACM)}, 50\penalty0 (4):\penalty0 506--519, 2003.

\bibitem[Cohen et~al.(2024)Cohen, Biran, Yoran, Globerson, and Geva]{cohen2024evaluating}
R.~Cohen, E.~Biran, O.~Yoran, A.~Globerson, and M.~Geva.
\newblock Evaluating the ripple effects of knowledge editing in language models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 283--298, 2024.

\bibitem[Cotra(2022)]{cotra2022without}
A.~Cotra.
\newblock Without specific countermeasures, the easiest path to transformative ai likely leads to ai takeover.
\newblock \url{https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to}, 2022.
\newblock Accessed: 2024-06-18.

\bibitem[GeoNames(2024)]{geonames}
GeoNames.
\newblock Geonames database, 2024.
\newblock URL \url{http://www.geonames.org}.
\newblock Licensed under CC BY 4.0.

\bibitem[Greenblatt and Shlegeris(2024)]{GreenblatShlegeris2024}
R.~Greenblatt and B.~Shlegeris.
\newblock The case for ensuring that powerful {AI}s are controlled.
\newblock AI Alignment Forum, 2024.
\newblock URL \url{https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled}.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Hubinger et~al.(2024)Hubinger, Denison, Mu, Lambert, Tong, MacDiarmid, Lanham, Ziegler, Maxwell, Cheng, et~al.]{hubinger2024sleeper}
E.~Hubinger, C.~Denison, J.~Mu, M.~Lambert, M.~Tong, M.~MacDiarmid, T.~Lanham, D.~M. Ziegler, T.~Maxwell, N.~Cheng, et~al.
\newblock Sleeper agents: Training deceptive llms that persist through safety training.
\newblock \emph{arXiv preprint arXiv:2401.05566}, 2024.

\bibitem[Krasheninnikov et~al.(2023)Krasheninnikov, Krasheninnikov, Mlodozeniec, Maharaj, and Krueger]{krasheninnikov2023meta}
D.~Krasheninnikov, E.~Krasheninnikov, B.~Mlodozeniec, T.~Maharaj, and D.~Krueger.
\newblock Implicit meta-learning may lead language models to trust more reliable sources.
\newblock \emph{arXiv preprint arXiv:2310.15047}, 2023.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
P.~Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal, H.~K{\"u}ttler, M.~Lewis, W.-t. Yih, T.~Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9459--9474, 2020.

\bibitem[Meinke and Evans(2023)]{meinke2023tell}
A.~Meinke and O.~Evans.
\newblock Tell, don't show: Declarative facts influence how llms generalize.
\newblock \emph{arXiv preprint arXiv:2312.07779}, 2023.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
K.~Meng, D.~Bau, A.~Andonian, and Y.~Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 17359--17372, 2022.

\bibitem[Misra et~al.(2022)Misra, Rayz, and Ettinger]{misra2022property}
K.~Misra, J.~T. Rayz, and A.~Ettinger.
\newblock A property induction framework for neural language models.
\newblock \emph{arXiv preprint arXiv:2205.06910}, 2022.

\bibitem[Mouton et~al.(2024)Mouton, Lucas, and Guest]{mouton2024operational}
C.~A. Mouton, C.~Lucas, and E.~Guest.
\newblock \emph{The Operational Risks of AI in Large-Scale Biological Attacks: Results of a Red-Team Study}.
\newblock RAND Corporation, Santa Monica, CA, 2024.
\newblock \doi{10.7249/RRA2977-2}.

\bibitem[Onoe et~al.(2023)Onoe, Zhang, Padmanabhan, Durrett, and Choi]{onoe2023can}
Y.~Onoe, M.~J. Zhang, S.~Padmanabhan, G.~Durrett, and E.~Choi.
\newblock Can lms learn new entities from descriptions? challenges in propagating injected knowledge.
\newblock \emph{arXiv preprint arXiv:2305.01651}, 2023.

\bibitem[{OpenAI}(2024)]{OpenAIDocumentation}
{OpenAI}.
\newblock Openai documentation.
\newblock \url{https://platform.openai.com/docs/overview}, 2024.
\newblock Accessed: 2024-05-30.

\bibitem[OpenAI(2024)]{openai2024api}
OpenAI.
\newblock Openai api, 2024.
\newblock URL \url{https://www.openai.com/api/}.

\bibitem[Raz(2018)]{raz2018fast}
R.~Raz.
\newblock Fast learning requires good memory: A time-space lower bound for parity learning.
\newblock \emph{Journal of the ACM (JACM)}, 66\penalty0 (1):\penalty0 1--18, 2018.

\bibitem[Saparov et~al.(2024)Saparov, Pang, Padmakumar, Joshi, Kazemi, Kim, and He]{saparov2024testing}
A.~Saparov, R.~Y. Pang, V.~Padmakumar, N.~Joshi, M.~Kazemi, N.~Kim, and H.~He.
\newblock Testing the general deductive reasoning capacity of large language models using ood examples.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
A.~Srivastava, A.~Rastogi, A.~Rao, A.~A.~M. Shoeb, A.~Abid, A.~Fisch, A.~R. Brown, A.~Santoro, A.~Gupta, A.~Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Von~Oswald et~al.(2023{\natexlab{a}})Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
J.~Von~Oswald, E.~Niklasson, E.~Randazzo, J.~Sacramento, A.~Mordvintsev, A.~Zhmoginov, and M.~Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 35151--35174. PMLR, 2023{\natexlab{a}}.

\bibitem[Von~Oswald et~al.(2023{\natexlab{b}})Von~Oswald, Niklasson, Schlegel, Kobayashi, Zucchet, Scherrer, Miller, Sandler, Vladymyrov, Pascanu, et~al.]{von2023uncovering}
J.~Von~Oswald, E.~Niklasson, M.~Schlegel, S.~Kobayashi, N.~Zucchet, N.~Scherrer, N.~Miller, M.~Sandler, M.~Vladymyrov, R.~Pascanu, et~al.
\newblock Uncovering mesa-optimization algorithms in transformers.
\newblock \emph{arXiv preprint arXiv:2309.05858}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, F.~Xia, E.~Chi, Q.~V. Le, D.~Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Yang et~al.(2024)Yang, Gribovskaya, Kassner, Geva, and Riedel]{yang2024large}
S.~Yang, E.~Gribovskaya, N.~Kassner, M.~Geva, and S.~Riedel.
\newblock Do large language models latently perform multi-hop reasoning?
\newblock \emph{arXiv preprint arXiv:2402.16837}, 2024.

\bibitem[Zheng et~al.(2024)Zheng, Zhang, Zhang, Ye, Luo, and Ma]{zheng2024llamafactory}
Y.~Zheng, R.~Zhang, J.~Zhang, Y.~Ye, Z.~Luo, and Y.~Ma.
\newblock Llamafactory: Unified efficient fine-tuning of 100+ language models.
\newblock \emph{arXiv preprint arXiv:2403.13372}, 2024.

\bibitem[Zhong et~al.(2023)Zhong, Wu, Manning, Potts, and Chen]{zhong2023mquake}
Z.~Zhong, Z.~Wu, C.~D. Manning, C.~Potts, and D.~Chen.
\newblock Mquake: Assessing knowledge editing in language models via multi-hop questions.
\newblock \emph{arXiv preprint arXiv:2305.14795}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Bradley, Littwin, Razin, Saremi, Susskind, Bengio, and Nakkiran]{zhou2023algorithms}
H.~Zhou, A.~Bradley, E.~Littwin, N.~Razin, O.~Saremi, J.~Susskind, S.~Bengio, and P.~Nakkiran.
\newblock What algorithms can transformers learn? a study in length generalization.
\newblock \emph{arXiv preprint arXiv:2310.16028}, 2023.

\end{thebibliography}
