\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Angelova et~al.(2005)Angelova, Abu-Mostafam, and
  Perona]{angelova2005pruning}
Anelia Angelova, Yaser Abu-Mostafam, and Pietro Perona.
\newblock Pruning training sets for learning of object categories.
\newblock In \emph{2005 IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition (CVPR'05)}, volume~1, pages 494--501. IEEE, 2005.

\bibitem[Angluin and Laird(1988)]{angluin1988learning}
Dana Angluin and Philip Laird.
\newblock Learning from noisy examples.
\newblock \emph{Machine Learning}, 2\penalty0 (4):\penalty0 343--370, 1988.

\bibitem[Arachie and Huang(2019)]{arachie2019adversarial}
Chidubem Arachie and Bert Huang.
\newblock Adversarial label learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3183--3190, 2019.

\bibitem[Bousquet et~al.(2003)Bousquet, Boucheron, and
  Lugosi]{bousquet2003introduction}
Olivier Bousquet, St{\'e}phane Boucheron, and G{\'a}bor Lugosi.
\newblock Introduction to statistical learning theory.
\newblock In \emph{Summer school on machine learning}, pages 169--207.
  Springer, 2003.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2022)Chen, Fu, Adila, Zhang, Sala, Fatahalian, and
  Re]{chen2022shoring}
Mayee~F Chen, Daniel~Yang Fu, Dyah Adila, Michael Zhang, Frederic Sala, Kayvon
  Fatahalian, and Christopher Re.
\newblock Shoring up the foundations: Fusing model embeddings and weak
  supervision.
\newblock In \emph{The 38th Conference on Uncertainty in Artificial
  Intelligence}, 2022.

\bibitem[Cheng et~al.(2021)Cheng, Zhu, Li, Gong, Sun, and
  Liu]{cheng2021learning}
Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu.
\newblock Learning with instance-dependent label noise: A sample sieve
  approach.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dawid and Skene(1979)]{dawid1979maximum}
Alexander~Philip Dawid and Allan~M Skene.
\newblock Maximum likelihood estimation of observer error-rates using the em
  algorithm.
\newblock \emph{Journal of the Royal Statistical Society: Series C (Applied
  Statistics)}, 28\penalty0 (1):\penalty0 20--28, 1979.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Fu et~al.(2020)Fu, Chen, Sala, Hooper, Fatahalian, and
  R{\'e}]{fu2020fast}
Daniel Fu, Mayee Chen, Frederic Sala, Sarah Hooper, Kayvon Fatahalian, and
  Christopher R{\'e}.
\newblock Fast and three-rious: Speeding up weak supervision with triplet
  methods.
\newblock In \emph{International Conference on Machine Learning}, pages
  3280--3291. PMLR, 2020.

\bibitem[Gu et~al.(2021)Gu, Tinn, Cheng, Lucas, Usuyama, Liu, Naumann, Gao, and
  Poon]{gu2021domain}
Yu~Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu,
  Tristan Naumann, Jianfeng Gao, and Hoifung Poon.
\newblock Domain-specific language model pretraining for biomedical natural
  language processing.
\newblock \emph{ACM Transactions on Computing for Healthcare (HEALTH)},
  3\penalty0 (1):\penalty0 1--23, 2021.

\bibitem[Horng(2022)]{horng2022}
Steven Horng.
\newblock {Machine Learning Core}.
\newblock 2 2022.
\newblock \doi{10.6084/m9.figshare.19104917.v2}.
\newblock URL
  \url{https://figshare.com/articles/preprint/Machine_Learning_Core/19104917}.

\bibitem[Irvin et~al.(2019)Irvin, Rajpurkar, Ko, Yu, Ciurea-Ilcus, Chute,
  Marklund, Haghgoo, Ball, Shpanskaya, et~al.]{irvin2019chexpert}
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus,
  Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya,
  et~al.
\newblock Chexpert: A large chest radiograph dataset with uncertainty labels
  and expert comparison.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pages 590--597, 2019.

\bibitem[Johnson et~al.(2019)Johnson, Pollard, Berkowitz, Greenbaum, Lungren,
  Deng, Mark, and Horng]{johnson2019mimic}
Alistair~EW Johnson, Tom~J Pollard, Seth~J Berkowitz, Nathaniel~R Greenbaum,
  Matthew~P Lungren, Chih-ying Deng, Roger~G Mark, and Steven Horng.
\newblock Mimic-cxr, a de-identified publicly available database of chest
  radiographs with free-text reports.
\newblock \emph{Scientific data}, 6\penalty0 (1):\penalty0 1--8, 2019.

\bibitem[Karamanolakis et~al.(2021)Karamanolakis, Mukherjee, Zheng, and
  Hassan]{karamanolakis2021self}
Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng, and Ahmed Hassan.
\newblock Self-training with weak supervision.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 845--863, 2021.

\bibitem[Lang et~al.(2022)Lang, Agrawal, Kim, and Sontag]{lang2022co}
Hunter Lang, Monica~N Agrawal, Yoon Kim, and David Sontag.
\newblock Co-training improves prompt-based learning for large language models.
\newblock In \emph{International Conference on Machine Learning}, pages
  11985--12003. PMLR, 2022.

\bibitem[Lee et~al.(2020)Lee, Yoon, Kim, Kim, Kim, So, and
  Kang]{lee2020biobert}
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan~Ho So,
  and Jaewoo Kang.
\newblock Biobert: a pre-trained biomedical language representation model for
  biomedical text mining.
\newblock \emph{Bioinformatics}, 36\penalty0 (4):\penalty0 1234--1240, 2020.

\bibitem[Li et~al.(2019)Li, Socher, and Hoi]{li2019dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li and Zhou(2005)]{li2005setred}
Ming Li and Zhi-Hua Zhou.
\newblock Setred: Self-training with editing.
\newblock In \emph{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining}, pages 611--621. Springer, 2005.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Maheshwari et~al.(2021)Maheshwari, Chatterjee, Killamsetty,
  Ramakrishnan, and Iyer]{maheshwari-etal-2021-semi}
Ayush Maheshwari, Oishik Chatterjee, Krishnateja Killamsetty, Ganesh
  Ramakrishnan, and Rishabh Iyer.
\newblock Semi-supervised data programming with subset selection.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 4640--4651, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.408}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.408}.

\bibitem[Mekala et~al.(2022)Mekala, Dong, and Shang]{mekala-etal-2022-lops}
Dheeraj Mekala, Chengyu Dong, and Jingbo Shang.
\newblock {LOPS}: Learning order inspired pseudo-label selection for weakly
  supervised text classification.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 4894--4908, Abu Dhabi, United Arab Emirates, December
  2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.findings-emnlp.360}.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
Aditya Menon, Brendan Van~Rooyen, Cheng~Soon Ong, and Bob Williamson.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{International conference on machine learning}, pages
  125--134. PMLR, 2015.

\bibitem[Muhlenbach et~al.(2004)Muhlenbach, Lallich, and
  Zighed]{muhlenbach2004identifying}
Fabrice Muhlenbach, St{\'e}phane Lallich, and Djamel~A Zighed.
\newblock Identifying and handling mislabelled instances.
\newblock \emph{Journal of Intelligent Information Systems}, 22\penalty0
  (1):\penalty0 89--109, 2004.

\bibitem[Mukherjee and Awadallah(2020)]{mukherjee2020uncertainty}
Subhabrata Mukherjee and Ahmed Awadallah.
\newblock Uncertainty-aware self-training for few-shot text classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 21199--21212, 2020.

\bibitem[Northcutt et~al.(2021)Northcutt, Jiang, and
  Chuang]{northcutt2021confident}
Curtis Northcutt, Lu~Jiang, and Isaac Chuang.
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock \emph{Journal of Artificial Intelligence Research}, 70:\penalty0
  1373--1411, 2021.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Giorgio Patrini, Alessandro Rozza, Aditya Krishna~Menon, Richard Nock, and
  Lizhen Qu.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1944--1952, 2017.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Ratner et~al.(2017)Ratner, Bach, Ehrenberg, Fries, Wu, and
  R{\'e}]{ratner2017snorkel}
Alexander Ratner, Stephen~H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and
  Christopher R{\'e}.
\newblock Snorkel: Rapid training data creation with weak supervision.
\newblock In \emph{Proceedings of the VLDB Endowment. International Conference
  on Very Large Data Bases}, volume~11, page 269. NIH Public Access, 2017.

\bibitem[Ratner et~al.(2019)Ratner, Hancock, Dunnmon, Sala, Pandey, and
  R{\'e}]{ratner2019training}
Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash
  Pandey, and Christopher R{\'e}.
\newblock Training complex models with multi-task weak supervision.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 4763--4771, 2019.

\bibitem[Ratner et~al.(2016)Ratner, De~Sa, Wu, Selsam, and
  R{\'e}]{ratner2016data}
Alexander~J Ratner, Christopher~M De~Sa, Sen Wu, Daniel Selsam, and Christopher
  R{\'e}.
\newblock Data programming: Creating large training sets, quickly.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 3567--3575, 2016.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Le~Scao, Raja, et~al.]{sanh2022multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid
  Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le~Scao, Arun Raja, et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{The Tenth International Conference on Learning
  Representations}, 2022.

\bibitem[Sauer(1972)]{sauer1972density}
Norbert Sauer.
\newblock On the density of families of sets.
\newblock \emph{Journal of Combinatorial Theory, Series A}, 13\penalty0
  (1):\penalty0 145--147, 1972.

\bibitem[Scott et~al.(2013)Scott, Blanchard, and
  Handy]{scott2013classification}
Clayton Scott, Gilles Blanchard, and Gregory Handy.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In Shai Shalev-Shwartz and Ingo Steinwart, editors, \emph{Proceedings
  of the 26th Annual Conference on Learning Theory}, volume~30 of
  \emph{Proceedings of Machine Learning Research}, pages 489--511, Princeton,
  NJ, USA, 12--14 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v30/Scott13.html}.

\bibitem[Scudder(1965)]{scudder1965probability}
Henry Scudder.
\newblock Probability of error of some adaptive pattern-recognition machines.
\newblock \emph{IEEE Transactions on Information Theory}, 11\penalty0
  (3):\penalty0 363--371, 1965.

\bibitem[Seyyed-Kalantari et~al.(2020)Seyyed-Kalantari, Liu, McDermott, Chen,
  and Ghassemi]{seyyed2020chexclusion}
Laleh Seyyed-Kalantari, Guanxiong Liu, Matthew McDermott, Irene~Y Chen, and
  Marzyeh Ghassemi.
\newblock Chexclusion: Fairness gaps in deep chest x-ray classifiers.
\newblock In \emph{BIOCOMPUTING 2021: Proceedings of the Pacific Symposium},
  pages 232--243. World Scientific, 2020.

\bibitem[Shelah(1972)]{shelah1972combinatorial}
Saharon Shelah.
\newblock A combinatorial problem; stability and order for models and theories
  in infinitary languages.
\newblock \emph{Pacific Journal of Mathematics}, 41\penalty0 (1):\penalty0
  247--261, 1972.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem[Vapnik(1971)]{vapnik1971uniform}
VN~Vapnik.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock \emph{Theory of Probability and its Applications}, 16\penalty0
  (2):\penalty0 264--281, 1971.

\bibitem[Woodworth et~al.(2017)Woodworth, Gunasekar, Ohannessian, and
  Srebro]{woodworth2017learning}
Blake Woodworth, Suriya Gunasekar, Mesrob~I Ohannessian, and Nathan Srebro.
\newblock Learning non-discriminatory predictors.
\newblock In \emph{Conference on Learning Theory}, pages 1920--1953. PMLR,
  2017.

\bibitem[Yarowsky(1995)]{yarowsky1995unsupervised}
David Yarowsky.
\newblock Unsupervised word sense disambiguation rivaling supervised methods.
\newblock In \emph{33rd Annual Meeting of the Association for Computational
  Linguistics}, pages 189--196, Cambridge, Massachusetts, USA, June 1995.
  Association for Computational Linguistics.
\newblock \doi{10.3115/981658.981684}.
\newblock URL \url{https://aclanthology.org/P95-1026}.

\bibitem[Yu et~al.(2021)Yu, Zuo, Jiang, Ren, Zhao, and Zhang]{yu2021fine}
Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang.
\newblock Fine-tuning pre-trained language model with weak supervision: A
  contrastive-regularized self-training approach.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1063--1077, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Yu, Li, Wang, Yang, Yang, and
  Ratner]{zhang2021wrench}
Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and
  Alexander Ratner.
\newblock Wrench: A comprehensive benchmark for weak supervision.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\bibitem[Zhang and Zhou(2011)]{zhang2011cotrade}
Min-Ling Zhang and Zhi-Hua Zhou.
\newblock Cotrade: Confident co-training with data editing.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 41\penalty0 (6):\penalty0 1612--1626, 2011.

\bibitem[Zhou et~al.(2012)Zhou, Kantarcioglu, and Thuraisingham]{zhou2012self}
Yan Zhou, Murat Kantarcioglu, and Bhavani Thuraisingham.
\newblock Self-training with selection-by-rejection.
\newblock In \emph{2012 IEEE 12th international conference on data mining},
  pages 795--803. IEEE, 2012.

\bibitem[Zhu et~al.(2021)Zhu, Dong, Cheng, and Liu]{zhu2021good}
Zhaowei Zhu, Zihao Dong, Hao Cheng, and Yang Liu.
\newblock A good representation detects noisy labels.
\newblock \emph{arXiv preprint arXiv:2110.06283}, 2021.

\end{thebibliography}
