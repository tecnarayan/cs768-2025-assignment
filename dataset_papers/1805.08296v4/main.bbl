\begin{thebibliography}{10}

\bibitem{andrychowicz2017hindsight}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI~Pieter Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5048--5058, 2017.

\bibitem{bacon2017option}
Pierre-Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture.
\newblock In {\em AAAI}, pages 1726--1734, 2017.

\bibitem{barth2018distributed}
Gabriel Barth-Maron, Matthew~W Hoffman, David Budden, Will Dabney, Dan Horgan,
  Alistair Muldal, Nicolas Heess, and Timothy Lillicrap.
\newblock Distributed distributional deterministic policy gradients.
\newblock {\em arXiv preprint arXiv:1804.08617}, 2018.

\bibitem{barto2003recent}
Andrew~G Barto and Sridhar Mahadevan.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock {\em Discrete Event Dynamic Systems}, 13(4):341--379, 2003.

\bibitem{chentanez2005intrinsically}
Nuttapong Chentanez, Andrew~G Barto, and Satinder~P Singh.
\newblock Intrinsically motivated reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1281--1288, 2005.

\bibitem{daniel2012hierarchical}
Christian Daniel, Gerhard Neumann, and Jan Peters.
\newblock Hierarchical relative entropy policy search.
\newblock In {\em Artificial Intelligence and Statistics}, pages 273--281,
  2012.

\bibitem{dayan1993feudal}
Peter Dayan and Geoffrey~E Hinton.
\newblock Feudal reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  271--278, 1993.

\bibitem{dietterich2000hierarchical}
Thomas~G Dietterich.
\newblock Hierarchical reinforcement learning with the maxq value function
  decomposition.
\newblock {\em Journal of Artificial Intelligence Research}, 13:227--303, 2000.

\bibitem{duan2016benchmarking}
Yan Duan, Xi~Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em International Conference on Machine Learning}, pages
  1329--1338, 2016.

\bibitem{florensa2017stochastic}
Carlos Florensa, Yan Duan, and Pieter Abbeel.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock {\em arXiv preprint arXiv:1704.03012}, 2017.

\bibitem{frans2017meta}
Kevin Frans, Jonathan Ho, Xi~Chen, Pieter Abbeel, and John Schulman.
\newblock Meta learning shared hierarchies.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{td3}
Scott Fujimoto, Herke van Hoof, and Dave Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock {\em arXiv preprint arXiv:1802.09477}, 2018.

\bibitem{gu2017deep}
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In {\em Robotics and Automation (ICRA), 2017 IEEE International
  Conference on}, pages 3389--3396. IEEE, 2017.

\bibitem{gu2017interpolated}
Shixiang Gu, Tim Lillicrap, Richard~E Turner, Zoubin Ghahramani, Bernhard
  Sch{\"o}lkopf, and Sergey Levine.
\newblock Interpolated policy gradient: Merging on-policy and off-policy
  gradient estimation for deep reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3849--3858, 2017.

\bibitem{gu2016q}
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard~E Turner, and Sergey
  Levine.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock {\em arXiv preprint arXiv:1611.02247}, 2016.

\bibitem{sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}, 2018.

\bibitem{harb2017waiting}
Jean Harb, Pierre-Luc Bacon, Martin Klissarov, and Doina Precup.
\newblock When waiting is not an option: Learning options with a deliberation
  cost.
\newblock {\em arXiv preprint arXiv:1709.04571}, 2017.

\bibitem{heess2017emergence}
Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval
  Tassa, Tom Erez, Ziyu Wang, Ali Eslami, Martin Riedmiller, et~al.
\newblock Emergence of locomotion behaviours in rich environments.
\newblock {\em arXiv preprint arXiv:1707.02286}, 2017.

\bibitem{heess2016learning}
Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller,
  and David Silver.
\newblock Learning and transfer of modulated locomotor controllers.
\newblock {\em arXiv preprint arXiv:1610.05182}, 2016.

\bibitem{held2017automatic}
David Held, Xinyang Geng, Carlos Florensa, and Pieter Abbeel.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock {\em arXiv preprint arXiv:1705.06366}, 2017.

\bibitem{houthooft2016vime}
Rein Houthooft, Xi~Chen, Yan Duan, John Schulman, Filip De~Turck, and Pieter
  Abbeel.
\newblock Vime: Variational information maximizing exploration.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1109--1117, 2016.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{konidaris2007building}
George Konidaris and Andrew~G Barto.
\newblock Building portable options: Skill transfer in reinforcement learning.
\newblock In {\em IJCAI}, volume~7, pages 895--900, 2007.

\bibitem{kulkarni2016hierarchical}
Tejas~D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock In {\em Advances in neural information processing systems}, pages
  3675--3683, 2016.

\bibitem{tdm}
Sergey Levine, Shane Gu, and Vitchyr Pong.
\newblock Temporal difference model learning: Model-free deep rl for
  model-based control.
\newblock 2018.

\bibitem{levy2017hierarchical}
Andrew Levy, Robert Platt, and Kate Saenko.
\newblock Hierarchical actor-critic.
\newblock {\em arXiv preprint arXiv:1712.00948}, 2017.

\bibitem{ddpg}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{mahadevan2007proto}
Sridhar Mahadevan and Mauro Maggioni.
\newblock Proto-value functions: A laplacian framework for learning
  representation and control in markov decision processes.
\newblock {\em Journal of Machine Learning Research}, 8(Oct):2169--2231, 2007.

\bibitem{mannor2004dynamic}
Shie Mannor, Ishai Menache, Amit Hoze, and Uri Klein.
\newblock Dynamic abstraction in reinforcement learning via clustering.
\newblock In {\em Proceedings of the twenty-first international conference on
  Machine learning}, page~71. ACM, 2004.

\bibitem{munos2016safe}
R{\'e}mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1054--1062, 2016.

\bibitem{tpcl}
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans.
\newblock Trust-pcl: An off-policy trust region method for continuous control.
\newblock {\em arXiv preprint arXiv:1707.01891}, 2017.

\bibitem{parr1998reinforcement}
Ronald Parr and Stuart~J Russell.
\newblock Reinforcement learning with hierarchies of machines.
\newblock In {\em Advances in neural information processing systems}, pages
  1043--1049, 1998.

\bibitem{plappert2018multi}
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker,
  Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder,
  et~al.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock {\em arXiv preprint arXiv:1802.09464}, 2018.

\bibitem{pong2018temporal}
Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine.
\newblock Temporal difference models: Model-free deep rl for model-based
  control.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{precup2000temporal}
Doina Precup.
\newblock {\em Temporal abstraction in reinforcement learning}.
\newblock University of Massachusetts Amherst, 2000.

\bibitem{rajeswaran}
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, John Schulman, Emanuel
  Todorov, and Sergey Levine.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock {\em arXiv preprint arXiv:1709.10087}, 2017.

\bibitem{rajeswaran2017towards}
Aravind Rajeswaran, Kendall Lowrey, Emanuel~V Todorov, and Sham~M Kakade.
\newblock Towards generalization and simplicity in continuous control.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6553--6564, 2017.

\bibitem{uvf}
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In {\em International Conference on Machine Learning}, pages
  1312--1320, 2015.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem{sigaud2018policy}
Olivier Sigaud and Freek Stulp.
\newblock Policy search in continuous action domains: an overview.
\newblock {\em arXiv preprint arXiv:1803.04706}, 2018.

\bibitem{stolle2002learning}
Martin Stolle and Doina Precup.
\newblock Learning options in reinforcement learning.
\newblock In {\em International Symposium on abstraction, reformulation, and
  approximation}, pages 212--223. Springer, 2002.

\bibitem{sutton2011horde}
Richard~S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick~M
  Pilarski, Adam White, and Doina Precup.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In {\em The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pages 761--768. International Foundation for
  Autonomous Agents and Multiagent Systems, 2011.

\bibitem{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock {\em Artificial intelligence}, 112(1-2):181--211, 1999.

\bibitem{tessler2017deep}
Chen Tessler, Shahar Givony, Tom Zahavy, Daniel~J Mankowitz, and Shie Mannor.
\newblock A deep hierarchical approach to lifelong learning in minecraft.
\newblock In {\em AAAI}, volume~3, page~6, 2017.

\bibitem{mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pages 5026--5033. IEEE, 2012.

\bibitem{vevcerik2017leveraging}
Matej Ve{\v{c}}er{\'\i}k, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier
  Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth{\"o}rl, Thomas Lampe, and
  Martin Riedmiller.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock {\em arXiv preprint arXiv:1707.08817}, 2017.

\bibitem{vezhnevets2016strategic}
Alexander Vezhnevets, Volodymyr Mnih, Simon Osindero, Alex Graves, Oriol
  Vinyals, John Agapiou, et~al.
\newblock Strategic attentive writer for learning macro-actions.
\newblock In {\em Advances in neural information processing systems}, pages
  3486--3494, 2016.

\bibitem{vezhnevets2017feudal}
Alexander~Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max
  Jaderberg, David Silver, and Koray Kavukcuoglu.
\newblock Feudal networks for hierarchical reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.01161}, 2017.

\bibitem{wang2017sample}
Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray
  Kavukcuoglu, and Nando de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock {\em International Conference on Learning Representations}, 2017.

\end{thebibliography}
