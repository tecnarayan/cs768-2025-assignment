\begin{thebibliography}{10}

\bibitem{Zhu2018Overparameterized}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In {\em Proceedings of the 33rd International Conference on Neural
  Information Processing Systems (NeurIPS)}, 2019.

\bibitem{Alon2006Biological}
Uri Alon.
\newblock {\em An Introduction to Systems Biology: Design Principles of
  Biological Circuits}.
\newblock Chapman and Hall/CRC, 2006.

\bibitem{anselmi2019symmetry}
Fabio Anselmi, Georgios Evangelopoulos, Lorenzo Rosasco, and Tomaso Poggio.
\newblock Symmetry-adapted representation learning.
\newblock {\em Pattern Recognition}, 86:201--208, 2019.

\bibitem{bekkers2020bspline}
Erik~J Bekkers.
\newblock B-spline cnns on lie groups.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{Bengio2007Greedy}
Y.~Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, and U.~Montreal.
\newblock Greedy layer-wise training of deep networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2007.

\bibitem{cohen16gcnn}
Taco Cohen and Max Welling.
\newblock Group equivariant convolutional networks.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning (ICML)}, 2016.

\bibitem{connor2020representing}
Marissa Connor and Christopher Rozell.
\newblock Representing closed transformation paths in encoded network latent
  space.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, 2020.

\bibitem{Cordonnier2020On}
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.
\newblock On the relationship between self-attention and convolutional layers.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{culpepper2009learning}
Benjamin Culpepper and Bruno Olshausen.
\newblock Learning transport operators for image manifolds.
\newblock {\em Advances in neural information processing systems}, 22, 2009.

\bibitem{dehmamy2021automatic}
Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, and Rose Yu.
\newblock Automatic symmetry discovery with lie algebra convolutional network.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{erhan2010does}
Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent.
\newblock Why does unsupervised pre-training help deep learning?
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 201--208. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{finzi2021residual}
Marc Finzi, Gregory Benton, and Andrew~G Wilson.
\newblock Residual pathway priors for soft equivariance constraints.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{finzi2021emlp}
Marc Finzi, Max Welling, and Andrew~Gordon Wilson.
\newblock A practical method for constructing equivariant multilayer
  perceptrons for arbitrary matrix groups.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{Goodfellow2009Measuring}
Ian Goodfellow, Honglak Lee, Quoc Le, Andrew Saxe, and Andrew Ng.
\newblock Measuring invariances in deep networks.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2009.

\bibitem{gruver2022lie}
Nate Gruver, Marc Finzi, Micah Goldblum, and Andrew~Gordon Wilson.
\newblock The lie derivative for measuring learned equivariance.
\newblock {\em arXiv preprint arXiv:2210.02984}, 2022.

\bibitem{he2019rethinking}
Kaiming He, Ross Girshick, and Piotr Doll{\'a}r.
\newblock Rethinking imagenet pre-training.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, 2019.

\bibitem{hendrycks2019using}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{JacobsenBMVC2017}
J-H. Jacobsen, B.~de~Brabandere, and A.~W.~M. Smeulders.
\newblock Dynamic steerable blocks in deep residual networks.
\newblock In {\em British Machine Vision Conference (BMVC)}, 2017.

\bibitem{jenner2022steerable}
Erik Jenner and Maurice Weiler.
\newblock Steerable partial differential operators for equivariant neural
  networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2022.

\bibitem{gupta2021implicit}
Naman Khetan, Tushar Arora, Samee~Ur Rehman, and Deepak~K Gupta.
\newblock Implicit equivariance in convolutional networks.
\newblock {\em arXiv preprint arXiv:2111.14157}, 2021.

\bibitem{Kidger2020uat}
Patrick Kidger and Terry Lyons.
\newblock {Universal Approximation with Deep Narrow Networks}.
\newblock In {\em Proceedings of Thirty Third Conference on Learning Theory},
  volume 125 of {\em Proceedings of Machine Learning Research}, pages
  2306--2327. PMLR, 09--12 Jul 2020.

\bibitem{Kingma2014Adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{Larochelle2007Empirical}
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua
  Bengio.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock In {\em Proceedings of the 24th International Conference on Machine
  Learning (ICML)}, 2007.

\bibitem{LeCun1999cnn}
Yann LeCun, Patrick Haffner, L{\'e}on Bottou, and Yoshua Bengio.
\newblock Object recognition with gradient-based learning.
\newblock In {\em Shape, contour and grouping in computer vision}. Springer,
  1999.

\bibitem{Lenc2014understanding}
Karel Lenc and Andrea Vedaldi.
\newblock Understanding image representations by measuring their equivariance
  and equivalence.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition (CVPR)}, 2015.

\bibitem{lu2017expressive}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  30, 2017.

\bibitem{nguyen2021do}
Thao Nguyen, Maithra Raghu, and Simon Kornblith.
\newblock Do wide and deep networks learn the same things? uncovering how
  neural network representations vary with width and depth.
\newblock In {\em International Conference on Learning Representations (ICML)},
  2021.

\bibitem{olah2020naturally}
Chris Olah, Nick Cammarata, Chelsea Voss, Ludwig Schubert, and Gabriel Goh.
\newblock Naturally occurring equivariance in neural networks.
\newblock {\em Distill}, 2020.
\newblock https://distill.pub/2020/circuits/equivariance.

\bibitem{olver2000applications}
Peter~J Olver.
\newblock {\em Applications of Lie groups to differential equations}, volume
  107.
\newblock Springer Science \& Business Media, 2000.

\bibitem{Paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}.
  2019.

\bibitem{Raghu2017Expressive}
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha
  Sohl-Dickstein.
\newblock On the expressive power of deep neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, 2017.

\bibitem{Ramachandran2017Searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V. Le.
\newblock Searching for activation functions.
\newblock {\em CoRR}, abs/1710.05941, 2017.

\bibitem{rao1998learning}
Rajesh Rao and Daniel Ruderman.
\newblock Learning lie groups for invariant visual perception.
\newblock {\em Advances in neural information processing systems}, 11, 1998.

\bibitem{sohl2010unsupervised}
Jascha Sohl-Dickstein, Ching~Ming Wang, and Bruno~A Olshausen.
\newblock An unsupervised algorithm for learning lie group transformations.
\newblock {\em arXiv preprint arXiv:1001.1027}, 2010.

\bibitem{sosnovik2021disco}
Ivan Sosnovik, Artem Moskalev, and Arnold Smeulders.
\newblock Disco: accurate discrete scale convolutions.
\newblock In {\em British Machine Vision Conference (BMVC)}, 2021.

\bibitem{sosnovik2021transform}
Ivan Sosnovik, Artem Moskalev, and Arnold Smeulders.
\newblock How to transform kernels for scale-convolutions.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV) Workshops}, 2021.

\bibitem{Sosnovik_2021_WACV}
Ivan Sosnovik, Artem Moskalev, and Arnold~W.M. Smeulders.
\newblock Scale equivariance improves siamese tracking.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision (WACV)}, pages 2765--2774, January 2021.

\bibitem{sosnovik2020sesn}
Ivan Sosnovik, Micha≈Ç Szmaja, and Arnold Smeulders.
\newblock Scale-equivariant steerable networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{tolstikhin2021mixer}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem{Vakili2021Uniform}
Sattar Vakili, Michael Bromberg, Jezabel Garcia, Da-shan Shiu, and Alberto
  Bernacchia.
\newblock Uniform generalization bounds for overparameterized neural networks.
\newblock {\em arXiv preprint arXiv:2109.06099}, 2021.

\bibitem{wang2022approximately}
Rui Wang, Robin Walters, and Rose Yu.
\newblock Approximately equivariant networks for imperfectly symmetric
  dynamics.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2022.

\bibitem{e2cnn}
Maurice Weiler and Gabriele Cesa.
\newblock {General E(2)-Equivariant Steerable CNNs}.
\newblock In {\em Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem{wojtowytsch2020emergence}
Stephan Wojtowytsch and E~Weinan.
\newblock On the emergence of tetrahedral symmetry in the final and penultimate
  layers of neural network classifiers.
\newblock {\em arXiv preprint arXiv:2012.05420}, 2020.

\bibitem{worrall2019deep}
Daniel Worrall and Max Welling.
\newblock Deep scale-spaces: Equivariance over scale.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{worrall2017harmonic}
Daniel~E. Worrall, Stephan~J. Garbin, Daniyar Turmukhambetov, and Gabriel~J.
  Brostow.
\newblock Harmonic networks: Deep translation and rotation equivariance.
\newblock {\em 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 7168--7177, 2017.

\bibitem{YAROTSKY2017103}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\bibitem{Zaheer2017deepsets}
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ~R
  Salakhutdinov, and Alexander~J Smola.
\newblock Deep sets.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock {\em Communications of the ACM}, 64(3):107--115, 2021.

\bibitem{zhou2020meta}
Allan Zhou, Tom Knowles, and Chelsea Finn.
\newblock Meta-learning symmetries by reparameterization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{zoph2020rethinking}
Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin~Dogus
  Cubuk, and Quoc Le.
\newblock Rethinking pre-training and self-training.
\newblock {\em Advances in neural information processing systems (NeurIPS)},
  2020.

\end{thebibliography}
