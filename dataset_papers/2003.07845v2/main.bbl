\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Baevski \& Auli(2019)Baevski and Auli]{baevski2018adaptive}
Baevski, A. and Auli, M.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{ICLR}, 2019.

\bibitem[Cooijmans et~al.(2017)Cooijmans, Ballas, Laurent, G{\"u}l{\c{c}}ehre,
  and Courville]{cooijmans2016recurrent}
Cooijmans, T., Ballas, N., Laurent, C., G{\"u}l{\c{c}}ehre, {\c{C}}., and
  Courville, A.
\newblock Recurrent batch normalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J.~G., Le, Q., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{ACL}, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Fan et~al.(2020)Fan, Grave, and Joulin]{fan2019reducing}
Fan, A., Grave, E., and Joulin, A.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{ICLR}, 2020.

\bibitem[Gao et~al.(2019)Gao, He, Tan, Qin, Wang, and
  Liu]{gao2019representation}
Gao, J., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.
\newblock Representation degeneration problem in training natural language
  generation models.
\newblock In \emph{ICLR}, 2019.

\bibitem[Goldberger et~al.(2005)Goldberger, Hinton, Roweis, and
  Salakhutdinov]{goldberger2005neighbourhood}
Goldberger, J., Hinton, G.~E., Roweis, S.~T., and Salakhutdinov, R.~R.
\newblock Neighbourhood components analysis.
\newblock In \emph{NeurIPS}, 2005.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 1997.

\bibitem[Hou et~al.(2019)Hou, Zhu, Kwok, Gao, Qin, and
  Liu]{hou2019normalization}
Hou, L., Zhu, J., Kwok, J., Gao, F., Qin, T., and Liu, T.-y.
\newblock Normalization helps training of quantized lstm.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Inan et~al.(2017)Inan, Khosravi, and Socher]{inan2016tying}
Inan, H., Khosravi, K., and Socher, R.
\newblock Tying word vectors and word classifiers: A loss framework for
  language modeling.
\newblock In \emph{ICLR}, 2017.

\bibitem[Ioffe(2017)]{ioffe2017batch}
Ioffe, S.
\newblock Batch renormalization: Towards reducing minibatch dependence in
  batch-normalized models.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Jarrett et~al.(2009)Jarrett, Kavukcuoglu, Ranzato, and
  LeCun]{jarrett2009best}
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y.
\newblock What is the best multi-stage architecture for object recognition?
\newblock In \emph{ICCV}, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NeurIPS}, 2012.

\bibitem[Li et~al.(2019)Li, Wu, Weinberger, and Belongie]{li2019positional}
Li, B., Wu, F., Weinberger, K.~Q., and Belongie, S.
\newblock Positional normalization.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Lin et~al.(2017)Lin, Goyal, Girshick, He, and
  Doll{\'a}r]{lin2017focal}
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Focal loss for dense object detection.
\newblock In \emph{ICCV}, 2017.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Ma et~al.(2019)Ma, Zhang, Zhang, Duan, Hou, Zhou, and
  Song]{ma2019tensorized}
Ma, X., Zhang, P., Zhang, S., Duan, N., Hou, Y., Zhou, M., and Song, D.
\newblock A tensorized transformer for language modeling.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock In \emph{ICLR}, 2017.

\bibitem[Mikolov et~al.(2011)Mikolov, Deoras, Kombrink, Burget, and
  {\v{C}}ernock{\`y}]{mikolov2011empirical}
Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and {\v{C}}ernock{\`y}, J.
\newblock Empirical evaluation and combination of advanced language modeling
  techniques.
\newblock In \emph{INTERSPEECH}, 2011.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Nguyen \& Salazar(2019)Nguyen and Salazar]{nguyen2019transformers}
Nguyen, T.~Q. and Salazar, J.
\newblock Transformers without tears: Improving the normalization of
  self-attention.
\newblock \emph{arXiv preprint arXiv:1910.05895}, 2019.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
Ott, M., Edunov, S., Grangier, D., and Auli, M.
\newblock Scaling neural machine translation.
\newblock In \emph{Machine Translation}, 2018.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{NAACL: Demonstrations}, 2019.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{ACL}, 2002.

\bibitem[Qiao et~al.(2019)Qiao, Wang, Liu, Shen, and Yuille]{qiao2019weight}
Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A.
\newblock Weight standardization.
\newblock \emph{arXiv preprint arXiv:1903.10520}, 2019.

\bibitem[Rahimi(2017)]{alirahimi}
Rahimi, A.
\newblock Nuerips 2017 test-of-time award presentation, December 2017.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016weight}
Salimans, T. and Kingma, D.~P.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A.
\newblock How does batch normalization help optimization?
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{sennrich2016neural}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{ACL}, 2016.

\bibitem[Singh \& Shrivastava(2019)Singh and Shrivastava]{singh2019evalnorm}
Singh, S. and Shrivastava, A.
\newblock Evalnorm: Estimating batch normalization statistics for evaluation.
\newblock In \emph{ICCV}, 2019.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and
  Lempitsky]{ulyanov2016instance}
Ulyanov, D., Vedaldi, A., and Lempitsky, V.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{arXiv preprint arXiv:1607.08022}, 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2020)Wang, Huang, Huang, Hu, Wang, and
  Gu]{wang2020improving}
Wang, L., Huang, J., Huang, K., Hu, Z., Wang, G., and Gu, Q.
\newblock Improving neural language generation with spectrum control.
\newblock In \emph{ICLR}, 2020.

\bibitem[Wang et~al.(2019)Wang, Li, Xiao, Zhu, Li, Wong, and
  Chao]{wang2019learning}
Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.~F., and Chao, L.~S.
\newblock Learning deep transformer models for machine translation.
\newblock In \emph{ACL}, 2019.

\bibitem[Wu et~al.(2019)Wu, Fan, Baevski, Dauphin, and Auli]{wu2019pay}
Wu, F., Fan, A., Baevski, A., Dauphin, Y., and Auli, M.
\newblock Pay less attention with lightweight and dynamic convolutions.
\newblock In \emph{ICLR}, 2019.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock In \emph{ECCV}, 2018.

\bibitem[Xu et~al.(2019)Xu, Sun, Zhang, Zhao, and Lin]{xu2019understanding}
Xu, J., Sun, X., Zhang, Z., Zhao, G., and Lin, J.
\newblock Understanding and improving layer normalization.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Yan et~al.(2020)Yan, Wan, Zhang, Zhang, Wei, and Sun]{yan2020towards}
Yan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y., and Sun, J.
\newblock Towards stabilizing batch statistics in backward propagation of batch
  normalization.
\newblock In \emph{ICLR}, 2020.

\bibitem[Yang et~al.(2018)Yang, Dai, Salakhutdinov, and
  Cohen]{yang2017breaking}
Yang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W.~W.
\newblock Breaking the softmax bottleneck: A high-rank rnn language model.
\newblock In \emph{ICLR}, 2018.

\bibitem[Yao et~al.(2019)Yao, Gholami, Keutzer, and Mahoney]{yao2019pyhessian}
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M.~W.
\newblock {PyHessian}: Neural networks through the lens of the {H}essian.
\newblock \emph{arXiv preprint arXiv:1912.07145}, 2019.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{zhang2019root}
Zhang, B. and Sennrich, R.
\newblock Root mean square layer normalization.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Titov, and
  Sennrich]{zhang2019improving}
Zhang, B., Titov, I., and Sennrich, R.
\newblock Improving deep transformer with depth-scaled initialization and
  merged attention.
\newblock In \emph{EMNLP}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Dauphin, and
  Ma]{zhang2019fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Residual learning without normalization via better initialization.
\newblock In \emph{ICLR}, 2019{\natexlab{b}}.

\end{thebibliography}
