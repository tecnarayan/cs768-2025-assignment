\begin{thebibliography}{10}

\bibitem{an2020repulsive}
Bang An, Jie Lyu, Zhenyi Wang, Chunyuan Li, Changwei Hu, Fei Tan, Ruiyi Zhang,
  Yifan Hu, and Changyou Chen.
\newblock Repulsive attention: Rethinking multi-head attention as bayesian
  inference.
\newblock {\em arXiv preprint arXiv:2009.09364}, 2020.

\bibitem{andreas2016neural}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Neural module networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 39--48, 2016.

\bibitem{anokhin2020image}
Ivan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor
  Lempitsky, and Denis Korzhenkov.
\newblock Image generators with conditionally-independent pixel synthesis,
  2020.

\bibitem{bahdanau2018systematic}
Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien~Huu Nguyen, Harm
  de~Vries, and Aaron Courville.
\newblock Systematic generalization: What is required and can it be learned?
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{barrett2018measuring}
David G.~T. Barrett, Felix Hill, Adam Santoro, Ari~S. Morcos, and Timothy
  Lillicrap.
\newblock Measuring abstract reasoning in neural networks, 2018.

\bibitem{bengio2019metatransfer}
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien
  Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal.
\newblock A meta-transfer objective for learning to disentangle causal
  mechanisms, 2019.

\bibitem{besserve2020theory}
Michel Besserve, R{\'e}my Sun, Dominik Janzing, and Bernhard Sch{\"o}lkopf.
\newblock A theory of independent mechanisms for extrapolation in generative
  models.
\newblock {\em arXiv preprint arXiv:2004.00184}, 2020.

\bibitem{buch2021nes}
Shyamal Buch, Li~Fei-Fei, and Noah~D. Goodman.
\newblock Neural event semantics for grounded language understanding.
\newblock In {\em Transactions of the Association for Computational Linguistics
  (TACL)}, 2021.

\bibitem{chang2018automatically}
Michael~B Chang, Abhishek Gupta, Sergey Levine, and Thomas~L Griffiths.
\newblock Automatically composing representation transformations as a means for
  generalization.
\newblock {\em arXiv preprint arXiv:1807.04640}, 2018.

\bibitem{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers,
  2021.

\bibitem{choromanska2015loss}
Anna Choromanska, Mikael Henaff, Michael Mathieu, G{\'e}rard~Ben Arous, and
  Yann LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In {\em Artificial intelligence and statistics}, pages 192--204.
  PMLR, 2015.

\bibitem{clanuwat2018deep}
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
  Yamamoto, and David Ha.
\newblock Deep learning for classical japanese literature, 2018.

\bibitem{cordonnier2019relationship}
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.
\newblock On the relationship between self-attention and convolutional layers.
\newblock {\em arXiv preprint arXiv:1911.03584}, 2019.

\bibitem{cubuk2019randaugment}
Ekin~D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space, 2019.

\bibitem{dehghani2019universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz
  Kaiser.
\newblock Universal transformers, 2019.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding, 2019.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em arXiv preprint arXiv:2101.03961}, 2021.

\bibitem{fernando2017pathnet}
Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha,
  Andrei~A Rusu, Alexander Pritzel, and Daan Wierstra.
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks.
\newblock {\em arXiv preprint arXiv:1701.08734}, 2017.

\bibitem{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock {\em The journal of machine learning research}, 17(1):2096--2030,
  2016.

\bibitem{goyal2020inductive}
Anirudh Goyal and Yoshua Bengio.
\newblock Inductive biases for deep learning of higher-level cognition.
\newblock {\em arXiv preprint arXiv:2011.15091}, 2020.

\bibitem{goyal2021neural}
Anirudh Goyal, Aniket Didolkar, Nan~Rosemary Ke, Charles Blundell, Philippe
  Beaudoin, Nicolas Heess, Michael Mozer, and Yoshua Bengio.
\newblock Neural production systems.
\newblock {\em arXiv preprint arXiv:2103.01937}, 2021.

\bibitem{goyal2020object}
Anirudh Goyal, Alex Lamb, Phanideep Gampa, Philippe Beaudoin, Sergey Levine,
  Charles Blundell, Yoshua Bengio, and Michael Mozer.
\newblock Object files and schemata: Factorizing declarative and procedural
  knowledge in dynamical systems.
\newblock {\em arXiv preprint arXiv:2006.16225}, 2020.

\bibitem{goyal2019recurrent}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine,
  Yoshua Bengio, and Bernhard Sch{\"o}lkopf.
\newblock Recurrent independent mechanisms.
\newblock {\em arXiv preprint arXiv:1909.10893}, 2019.

\bibitem{gupta2018shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization, 2018.

\bibitem{hendrycks2020gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus), 2020.

\bibitem{hudson2018compositional}
Drew~A Hudson and Christopher~D Manning.
\newblock Compositional attention networks for machine reasoning.
\newblock {\em arXiv preprint arXiv:1803.03067}, 2018.

\bibitem{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and
  Joao Carreira.
\newblock Perceiver: General perception with iterative attention, 2021.

\bibitem{jo2017measuring}
Jason Jo and Yoshua Bengio.
\newblock Measuring the tendency of cnns to learn surface statistical
  regularities, 2017.

\bibitem{kendal2007introduction}
Simon~L Kendal and Malcolm Creen.
\newblock {\em An introduction to knowledge engineering}.
\newblock Springer, 2007.

\bibitem{keysers2019measuring}
Daniel Keysers, Nathanael Sch{\"a}rli, Nathan Scales, Hylke Buisman, Daniel
  Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz
  Stafiniak, Tibor Tihon, et~al.
\newblock Measuring compositional generalization: A comprehensive method on
  realistic data.
\newblock {\em arXiv preprint arXiv:1912.09713}, 2019.

\bibitem{kirsch2018modular}
Louis Kirsch, Julius Kunze, and David Barber.
\newblock Modular networks: Learning to decompose neural computation.
\newblock {\em arXiv preprint arXiv:1811.05249}, 2018.

\bibitem{lake2018generalization}
Brenden Lake and Marco Baroni.
\newblock Generalization without systematicity: On the compositional skills of
  sequence-to-sequence recurrent networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2873--2882. PMLR, 2018.

\bibitem{lamb2021transformers}
Alex Lamb, Di~He, Anirudh Goyal, Guolin Ke, Chien-Feng Liao, Mirco Ravanelli,
  and Yoshua Bengio.
\newblock Transformers with competitive ensembles of independent mechanisms.
\newblock {\em arXiv preprint arXiv:2103.00336}, 2021.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock {\em ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  2, 2010.

\bibitem{liu2019variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng
  Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock {\em arXiv preprint arXiv:1908.03265}, 2019.

\bibitem{locatello2020objectcentric}
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran,
  Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf.
\newblock Object-centric learning with slot attention, 2020.

\bibitem{logeswaran2020few}
Lajanugen Logeswaran, Ann Lee, Myle Ott, Honglak Lee, Marc'Aurelio Ranzato, and
  Arthur Szlam.
\newblock Few-shot sequence learning with transformers.
\newblock {\em arXiv preprint arXiv:2012.09543}, 2020.

\bibitem{loula2018rearranging}
Joao Loula, Marco Baroni, and Brenden~M Lake.
\newblock Rearranging the familiar: Testing compositional generalization in
  recurrent networks.
\newblock {\em arXiv preprint arXiv:1807.07545}, 2018.

\bibitem{mao2019neuro}
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua~B Tenenbaum, and Jiajun Wu.
\newblock The neuro-symbolic concept learner: Interpreting scenes, words, and
  sentences from natural supervision.
\newblock {\em arXiv preprint arXiv:1904.12584}, 2019.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{ParKilRojSch18}
G.~Parascandolo, N.~Kilbertus, M.~Rojas-Carulla, and B.~Sch{\"o}lkopf.
\newblock Learning independent causal mechanisms.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, volume~80 of {\em Proceedings of Machine Learning
  Research}, pages 4033--4041. PMLR, 2018.

\bibitem{peters2017elements}
Jonas Peters, Dominik Janzing, and Bernhard Sch{\"o}lkopf.
\newblock {\em Elements of causal inference: foundations and learning
  algorithms}.
\newblock The MIT Press, 2017.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rahaman2019spectral}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred~A.
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks, 2019.

\bibitem{rahaman2020s2rms}
Nasim Rahaman, Anirudh Goyal, Muhammad~Waleed Gondal, Manuel Wuthrich, Stefan
  Bauer, Yash Sharma, Yoshua Bengio, and Bernhard Schölkopf.
\newblock S2rms: Spatially structured recurrent modules, 2020.

\bibitem{raven1998raven}
John~C Raven and John~Hugh Court.
\newblock {\em Raven's progressive matrices and vocabulary scales}, volume 759.
\newblock Oxford pyschologists Press Oxford, England, 1998.

\bibitem{rosenbaum2019routing}
Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger.
\newblock Routing networks and the challenges of modular and compositional
  computation.
\newblock {\em arXiv preprint arXiv:1904.12774}, 2019.

\bibitem{rosenbaum2017routing}
Clemens Rosenbaum, Tim Klinger, and Matthew Riemer.
\newblock Routing networks: Adaptive selection of non-linear functions for
  multi-task learning.
\newblock {\em arXiv preprint arXiv:1711.01239}, 2017.

\bibitem{sabour2017dynamic}
Sara Sabour, Nicholas Frosst, and Geoffrey~E Hinton.
\newblock Dynamic routing between capsules.
\newblock {\em arXiv preprint arXiv:1710.09829}, 2017.

\bibitem{Scholkopfetal21}
B.~Sch{\"o}lkopf, F.~Locatello, S.~Bauer, R.~Nan~Ke, N.~Kalchbrenner, A.~Goyal,
  and Y.~Bengio.
\newblock Towards causal representation learning.
\newblock {\em Proceedings of the IEEE}, 2021.

\bibitem{scholkopf2012causal}
Bernhard Sch{\"o}lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun
  Zhang, and Joris Mooij.
\newblock On causal and anticausal learning.
\newblock {\em arXiv preprint arXiv:1206.6471}, 2012.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock {\em arXiv preprint arXiv:1701.06538}, 2017.

\bibitem{steenbrugge2018improving}
Xander Steenbrugge, Sam Leroux, Tim Verbelen, and Bart Dhoedt.
\newblock Improving generalization for abstract reasoning tasks using
  disentangled feature representations.
\newblock {\em arXiv preprint arXiv:1811.04784}, 2018.

\bibitem{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Hervé Jégou.
\newblock Training data-efficient image transformers and distillation through
  attention, 2021.

\bibitem{touvron2021going}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Hervé Jégou.
\newblock Going deeper with image transformers, 2021.

\bibitem{van2018relational}
Sjoerd Van~Steenkiste, Michael Chang, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Relational neural expectation maximization: Unsupervised discovery of
  objects and their interactions.
\newblock {\em arXiv preprint arXiv:1802.10353}, 2018.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em arXiv preprint arXiv:1706.03762}, 2017.

\bibitem{wang2020abstract}
Duo Wang, Mateja Jamnik, and Pietro Lio.
\newblock Abstract diagrammatic reasoning with multiplex graph networks.
\newblock {\em arXiv preprint arXiv:2006.11197}, 2020.

\bibitem{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem{wu2020scattering}
Yuhuai Wu, Honghua Dong, Roger Grosse, and Jimmy Ba.
\newblock The scattering compositional learner: Discovering objects,
  attributes, relationships in analogical reasoning.
\newblock {\em arXiv preprint arXiv:2007.04212}, 2020.

\bibitem{yi2019clevrer}
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba,
  and Joshua~B Tenenbaum.
\newblock Clevrer: Collision events for video representation and reasoning.
\newblock {\em arXiv preprint arXiv:1910.01442}, 2019.

\bibitem{zilberstein1996using}
Shlomo Zilberstein.
\newblock Using anytime algorithms in intelligent systems.
\newblock {\em AI magazine}, 17(3):73--73, 1996.

\end{thebibliography}
