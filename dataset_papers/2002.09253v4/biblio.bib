@misc{karch2020deep,
      title={Deep Sets for Generalization in RL}, 
      author={Tristan Karch and Cédric Colas and Laetitia Teodorescu and Clément Moulin-Frier and Pierre-Yves Oudeyer},
      year={2020},
      eprint={2003.09443},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{asada2009cognitive,
  title={Cognitive developmental robotics: A survey},
  author={Asada, Minoru and Hosoda, Koh and Kuniyoshi, Yasuo and Ishiguro, Hiroshi and Inui, Toshio and Yoshikawa, Yuichiro and Ogino, Masaki and Yoshida, Chisato},
  journal={IEEE transactions on autonomous mental development},
  volume={1},
  number={1},
  pages={12--34},
  year={2009},
  publisher={IEEE}
}

@inproceedings{nguyen2019vision,
  title={Vision-based navigation with language-based assistance via imitation learning with indirect intervention},
  author={Nguyen, Khanh and Dey, Debadeepta and Brockett, Chris and Dolan, Bill},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={12527--12537},
  year={2019}
}


@inproceedings{shridhar2020alfred,
  title={Alfred: A benchmark for interpreting grounded instructions for everyday tasks},
  author={Shridhar, Mohit and Thomason, Jesse and Gordon, Daniel and Bisk, Yonatan and Han, Winson and Mottaghi, Roozbeh and Zettlemoyer, Luke and Fox, Dieter},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10740--10749},
  year={2020}
}


@article{schmidhuber2010formal,
  title={Formal theory of creativity, fun, and intrinsic motivation (1990--2010)},
  author={Schmidhuber, J{\"u}rgen},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={2},
  number={3},
  pages={230--247},
  year={2010},
  publisher={IEEE}
}


@article{racaniere2019automated,
  title={Automated curricula through setter-solver interactions},
  author={Racaniere, Sebastien and Lampinen, Andrew K and Santoro, Adam and Reichert, David P and Firoiu, Vlad and Lillicrap, Timothy P},
  journal={arXiv preprint arXiv:1909.12892},
  year={2019}
}


@book{tomasello2009constructing,
  title={Constructing a language},
  author={Tomasello, Michael},
  year={2009},
  publisher={Harvard university press}
}
@article{nair2019contextual,
  title={Contextual Imagined Goals for Self-Supervised Robotic Learning},
  author={Nair, Ashvin and Bahl, Shikhar and Khazatsky, Alexander and Pong, Vitchyr and Berseth, Glen and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.11670},
  year={2019}
}

@article{florensa2017automatic,
  title={Automatic goal generation for reinforcement learning agents},
  author={Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
  journal={ICML},
  year={2018}
}

@article{venkattaramanujam2019self,
  title={Self-supervised Learning of Distance Functions for Goal-Conditioned Reinforcement Learning},
  author={Venkattaramanujam, Srinivas and Crawford, Eric and Doan, Thang and Precup, Doina},
  journal={arXiv preprint arXiv:1907.02998},
  year={2019}
}

@inproceedings{andreas2019goodenough,
    title = "Good-Enough Compositional Data Augmentation",
    author = "Andreas, Jacob",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.676",
    doi = "10.18653/v1/2020.acl-main.676",
    pages = "7556--7566",
    abstract = "We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models. Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment. The protocol is model-agnostic and useful for a variety of tasks. Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87{\%} on diagnostic tasks from the SCAN dataset and 16{\%} on a semantic parsing task. Applied to n-gram language models, it reduces perplexity by roughly 1{\%} on small corpora in several languages.",
}

@article{ecoffet2019go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}


@inproceedings{schaul2015universal,
  title={Universal value function approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={1312--1320},
  year={2015}
}
@misc{zaremba2014recurrent,
    title={Recurrent Neural Network Regularization},
    author={Wojciech Zaremba and Ilya Sutskever and Oriol Vinyals},
    year={2014},
    eprint={1409.2329},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}
@misc{coreyes2018guiding,
    title={Guiding Policies with Language via Meta-Learning},
    author={John D. Co-Reyes and Abhishek Gupta and Suvansh Sanjeev and Nick Altieri and Jacob Andreas and John DeNero and Pieter Abbeel and Sergey Levine},
    year={2018},
    eprint={1811.07882},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{chan2019actrce,
    title={ACTRCE: Augmenting Experience via Teacher's Advice For Multi-Goal Reinforcement Learning},
    author={Harris Chan and Yuhuai Wu and Jamie Kiros and Sanja Fidler and Jimmy Ba},
    year={2019},
    eprint={1902.04546},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{goldberg2003constructions,
  title={Constructions: A new theoretical approach to language},
  author={Goldberg, Adele E},
  journal={Trends in cognitive sciences},
  volume={7},
  number={5},
  pages={219--224},
  year={2003},
  publisher={Elsevier}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@article{collobert2011natural,
  title={Natural language processing (almost) from scratch},
  author={Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  journal={Journal of machine learning research},
  volume={12},
  number={Aug},
  pages={2493--2537},
  year={2011}
}


@article{hinaut2013real,
  title={Real-time parallel processing of grammatical structure in the fronto-striatal system: A recurrent network simulation study using reservoir computing},
  author={Hinaut, Xavier and Dominey, Peter Ford},
  journal={PloS one},
  volume={8},
  number={2},
  year={2013},
  publisher={Public Library of Science}
}

@article{tomasello2000item,
  title={The item-based nature of children’s early syntactic development},
  author={Tomasello, Michael},
  journal={Trends in cognitive sciences},
  volume={4},
  number={4},
  pages={156--163},
  year={2000},
  publisher={Elsevier}
}

@article{tomasello1993twenty,
  title={Twenty-three-month-old children have a grammatical category of noun},
  author={Tomasello, Michael and Olguin, Raquel},
  journal={Cognitive development},
  volume={8},
  number={4},
  pages={451--464},
  year={1993},
  publisher={Elsevier}
}

@article{mintz2003frequent,
  title={Frequent frames as a cue for grammatical categories in child directed speech},
  author={Mintz, Toben H},
  journal={Cognition},
  volume={90},
  number={1},
  pages={91--117},
  year={2003},
  publisher={Elsevier}
}


@article{spelke1992origins,
  title={Origins of knowledge.},
  author={Spelke, Elizabeth S and Breinlinger, Karen and Macomber, Janet and Jacobson, Kristen},
  journal={Psychological review},
  volume={99},
  number={4},
  pages={605},
  year={1992},
  publisher={American Psychological Association}
}


@article{johnson2003development,
  title={Development of object concepts in infancy: Evidence for early learning in an eye-tracking paradigm},
  author={Johnson, Scott P and Amso, Dima and Slemmer, Jonathan A},
  journal={Proceedings of the National Academy of Sciences},
  volume={100},
  number={18},
  pages={10568--10573},
  year={2003},
  publisher={National Acad Sciences}
}


@article{hochreiter1997lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@article{mcclelland2019extending,
  title={Extending Machine Language Models toward Human-Level Language Understanding},
  author={McClelland, James L and Hill, Felix and Rudolph, Maja and Baldridge, Jason and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:1912.05877},
  year={2019}
}

@article{johnson2013object,
  title={Object perception},
  author={Johnson, Scott P},
  journal={Handbook of developmental psychology},
  pages={371--379},
  year={2013}
}

@article{green2017object,
  title={What is an object file?},
  author={Green, Edwin James and Quilty-Dunn, Jake},
  journal={The British Journal for the Philosophy of Science},
  year={2017}
}

@article{burgess2019monet,
  title={Monet: Unsupervised scene decomposition and representation},
  author={Burgess, Christopher P and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1901.11390},
  year={2019}
}

@article{greff2019multi,
  title={Multi-object representation learning with iterative variational inference},
  author={Greff, Klaus and Kaufmann, Rapha{\"e}l Lopez and Kabra, Rishab and Watters, Nick and Burgess, Chris and Zoran, Daniel and Matthey, Loic and Botvinick, Matthew and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1903.00450},
  year={2019}
}



@article{wagstaff2019limitations,
  title={On the limitations of representing functions on sets},
  author={Wagstaff, Edward and Fuchs, Fabian B and Engelcke, Martin and Posner, Ingmar and Osborne, Michael},
  journal={arXiv preprint arXiv:1901.09006},
  year={2019}
}


@book{baldassarre2013intrinsically,
  title={Intrinsically motivated learning in natural and artificial systems},
  author={Baldassarre, Gianluca and Mirolli, Marco},
  year={2013},
  publisher={Springer}
}

@article{pong2019skew,
  title={Skew-fit: State-covering self-supervised reinforcement learning},
  author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.03698},
  year={2019}
}


@inproceedings{nair2018visual,
  title={Visual reinforcement learning with imagined goals},
  author={Nair, Ashvin V and Pong, Vitchyr and Dalal, Murtaza and Bahl, Shikhar and Lin, Steven and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9191--9200},
  year={2018}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={16--17},
  year={2017}
}

@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in neural information processing systems},
  pages={1471--1479},
  year={2016}
}

@article{kidd2015psychology,
  title={The psychology and neuroscience of curiosity},
  author={Kidd, Celeste and Hayden, Benjamin Y},
  journal={Neuron},
  volume={88},
  number={3},
  pages={449--460},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{chentanez2005intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Chentanez, Nuttapong and Barto, Andrew G and Singh, Satinder P},
  booktitle={Advances in neural information processing systems},
  pages={1281--1288},
  year={2005}
}

@book{gopnik1999scientist,
  title={The scientist in the crib: Minds, brains, and how children learn.},
  author={Gopnik, Alison and Meltzoff, Andrew N and Kuhl, Patricia K},
  year={1999},
  publisher={William Morrow \& Co}
}

@book{cangelosi2015developmental,
  title={Developmental robotics: From babies to robots},
  author={Cangelosi, Angelo and Schlesinger, Matthew},
  year={2015},
  publisher={MIT press}
}

@article{kaplan2007search,
  title={In search of the neural circuits of intrinsic motivation},
  author={Kaplan, Frederic and Oudeyer, Pierre-Yves},
  journal={Frontiers in neuroscience},
  volume={1},
  pages={17},
  year={2007},
  publisher={Frontiers}
}

@InProceedings{laversanne2018curiosity,
title = {Curiosity Driven Exploration of Learned Disentangled Goal Spaces}, author = {Laversanne-Finot, Adrien and Pere, Alexandre and Oudeyer, Pierre-Yves}, pages = {487--504}, year = {2018}, editor = {Aude Billard and Anca Dragan and Jan Peters and Jun Morimoto}, volume = {87}, series = {Proceedings of Machine Learning Research}, address = {}, month = {29--31 Oct}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v87/laversanne-finot18a/laversanne-finot18a.pdf}, url = {http://proceedings.mlr.press/v87/laversanne-finot18a.html}, abstract = {Intrinsically motivated goal exploration processes enable agents to explore efficiently complex environments with high-dimensional continuous actions. They have been applied successfully to real world robots to discover repertoires of policies producing a wide diversity of effects. Often these algorithms relied on engineered goal spaces but it was recently shown that one can use deep representation learning algorithms to learn an adequate goal space in simple environments. In this paper we show that using a disentangled goal space (i.e. a representation where each latent variable is sensitive to a single degree of freedom) leads to better exploration performances than an entangled one. We further show that when the representation is disentangled, one can leverage it by sampling goals that maximize learning progress in a modular manner. Finally, we show that the measure of learning progress, used to drive curiosity-driven exploration, can be used simultaneously to discover abstract independently controllable features of the environment. } }
  

@misc{chaplot2017gatedattention,
    title={Gated-Attention Architectures for Task-Oriented Language Grounding},
    author={Devendra Singh Chaplot and Kanthashree Mysore Sathyendra and Rama Kumar Pasumarthi and Dheeraj Rajagopal and Ruslan Salakhutdinov},
    year={2017},
    eprint={1706.07230},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{colas2019hitchhiker,
  title={A Hitchhiker's Guide to Statistical Comparisons of Reinforcement Learning Algorithms},
  author={Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1904.06979},
  year={2019}
}


@inproceedings{he,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}


@inproceedings{bahdanau2018systematic,
    title={Systematic Generalization: What Is Required and Can It Be Learned?},
    author={Dzmitry Bahdanau and Shikhar Murty and Michael Noukhovitch and Thien Huu Nguyen and Harm de Vries and Aaron Courville},
    year={2019},
    booktitle={ICLR},
    eprint={1811.12889},
    archivePrefix={arXiv},
    primaryClass={cs.CL}}

@misc{hill2019emergent,
    title={Emergent Systematic Generalization in a Situated Agent},
    author={Felix Hill and Andrew Lampinen and Rosalia Schneider and Stephen Clark and Matthew Botvinick and James L. McClelland and Adam Santoro},
    year={2019},
    eprint={1910.00571},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{pugh2016quality,
  title={Quality diversity: A new frontier for evolutionary computation},
  author={Pugh, Justin K. and Soros, Lisa B. and Stanley, Kenneth O.},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016},
  publisher={Frontiers}
}

@inproceedings{deepset,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  booktitle={Advances in neural information processing systems},
  pages={3391--3401},
  year={2017}
}



@article{steels2006semiotic,
  title={Semiotic dynamics for embodied agents},
  author={Steels, Luc},
  journal={IEEE Intelligent Systems},
  volume={21},
  number={3},
  pages={32--38},
  year={2006},
  publisher={IEEE}
}

@article{pugh2016quality,
  title={Quality diversity: A new frontier for evolutionary computation},
  author={Pugh, Justin K and Soros, Lisa B and Stanley, Kenneth O},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016},
  publisher={Frontiers}
}




@article{plappert2018multi,
  title={Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research},
  author={Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and others},
  journal={arXiv preprint arXiv:1802.09464},
  year={2018}
}

@article{fujimoto2018addressing,
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  author={Fujimoto, Scott and van Hoof, Herke and Meger, Dave},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}

@article{DBLP:journals/tnn/SuttonB98,
  author    = {Richard S. Sutton and
               Andrew G. Barto},
  title     = {Reinforcement Learning: An Introduction},
  journal   = {{IEEE} Trans. Neural Networks},
  volume    = {9},
  number    = {5},
  pages     = {1054--1054},
  year      = {1998},
  url       = {https://doi.org/10.1109/TNN.1998.712192},
  doi       = {10.1109/TNN.1998.712192},
  timestamp = {Sun, 28 May 2017 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tnn/SuttonB98},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5048--5058},
  year={2017}
}


@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}


@article{veeriah2018many,
  title={Many-Goals Reinforcement Learning},
  author={Veeriah, Vivek and Oh, Junhyuk and Singh, Satinder},
  journal={arXiv preprint arXiv:1806.09605},
  year={2018}
}

@article{mankowitz2018unicorn,
  title={Unicorn: Continual Learning with a Universal, Off-policy Agent},
  author={Mankowitz, Daniel J and {\v{Z}}{\'\i}dek, Augustin and Barreto, Andr{\'e} and Horgan, Dan and Hessel, Matteo and Quan, John and Oh, Junhyuk and van Hasselt, Hado and Silver, David and Schaul, Tom},
  journal={arXiv preprint arXiv:1802.08294},
  year={2018}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@inproceedings{forestier2016modular,
  title={Modular active curiosity-driven discovery of tool use},
  author={Forestier, S{\'e}bastien and Oudeyer, Pierre-Yves},
  booktitle={Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on},
  pages={3965--3972},
  year={2016},
  organization={IEEE}
}

@article{riedmiller2018learning,
  title={Learning by Playing-Solving Sparse Reward Tasks from Scratch},
  author={Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Van de Wiele, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
  journal={arXiv preprint arXiv:1802.10567},
  year={2018}
}


@incollection{kaplan2004maximizing,
  title={Maximizing learning progress: an internal reward system for development},
  author={Kaplan, Fr{\'e}d{\'e}ric and Oudeyer, Pierre-Yves},
  booktitle={Embodied artificial intelligence},
  pages={259--270},
  year={2004},
  publisher={Springer}
}

@article{oudeyer2016evolution,
  title={How evolution may work through curiosity-driven developmental process},
  author={Oudeyer, Pierre-Yves and Smith, Linda B},
  journal={Topics in Cognitive Science},
  volume={8},
  number={2},
  pages={492--502},
  year={2016},
  publisher={Wiley Online Library}
}

@article{fujimoto2018off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  journal={arXiv preprint arXiv:1812.02900},
  year={2018}
}


@inproceedings{baranes2011interaction,
  title={The interaction of maturational constraints and intrinsic motivations in active motor development},
  author={Baranes, Adrien and Oudeyer, Pierre-Yves},
  booktitle={Development and Learning (ICDL), 2011 IEEE International Conference on},
  volume={2},
  pages={1--8},
  year={2011},
  organization={IEEE}
}

@article{fournier2018accuracy,
  title={Accuracy-based curriculum learning in deep reinforcement learning},
  author={Fournier, Pierre and Sigaud, Olivier and Chetouani, Mohamed and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1806.09614},
  year={2018}
}


@inproceedings{baranes2010intrinsically,
  title={Intrinsically motivated goal exploration for active motor learning in robots: A case study},
  author={Baranes, Adrien and Oudeyer, Pierre-Yves},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010)},
  year={2010}
}

@article{baranes2013active,
  title={Active learning of inverse models with intrinsically motivated goal exploration in robots},
  author={Baranes, Adrien and Oudeyer, Pierre-Yves},
  journal={Robotics and Autonomous Systems},
  volume={61},
  number={1},
  pages={49--73},
  year={2013},
  publisher={Elsevier}
}

@article{forestier2017intrinsically,
  title={Intrinsically motivated goal exploration processes with automatic curriculum learning},
  author={Forestier, S{\'e}bastien and Mollard, Yoan and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1708.02190},
  year={2017}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{colas2018gep,
  title={{GEP-PG}: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms},
  author={Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1802.05054},
  year={2018}
}

@article{nair2017overcoming,
  title={Overcoming exploration in reinforcement learning with demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1709.10089},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@article{Luketina2019,
abstract = {To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.},
archivePrefix = {arXiv},
arxivId = {1906.03926},
author = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"{a}}schel, Tim},
eprint = {1906.03926},
journal = {IJCAI'19},
mendeley-groups = {Review IML},
month = {jun},
title = {{A Survey of Reinforcement Learning Informed by Natural Language}},
url = {http://arxiv.org/abs/1906.03926},
year = {2019}
}

@inproceedings{bahdanau2018learning,
abstract = {Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.},
archivePrefix = {arXiv},
arxivId = {1806.01946},
author = {Bahdanau, Dzmitry and Hill, Felix and Leike, Jan and Hughes, Edward and Kohli, Pushmeet and Grefenstette, Edward},
booktitle = {International Conference on Learning Representations},
eprint = {1806.01946},
month = {jun},
title = {{Learning to Understand Goal Specifications by Modelling Reward}},
year = {2019}
}

@inproceedings{Jiang2019,
abstract = {Solving complex, temporally-extended tasks is a long-standing problem in reinforcement learning (RL). We hypothesize that one critical element of solving such problems is the notion of compositionality. With the ability to learn concepts and sub-skills that can be composed to solve longer tasks, i.e. hierarchical RL, we can acquire temporally-extended behaviors. However, acquiring effective yet general abstractions for hierarchical RL is remarkably challenging. In this paper, we propose to use language as the abstraction, as it provides unique compositional structure, enabling fast learning and combinatorial generalization, while retaining tremendous flexibility, making it suitable for a variety of problems. Our approach learns an instruction-following low-level policy and a high-level policy that can reuse abstractions across tasks, in essence, permitting agents to reason using structured language. To study compositional task learning, we introduce an open-source object interaction environment built using the MuJoCo physics engine and the CLEVR engine. We find that, using our approach, agents can learn to solve to diverse, temporally-extended tasks such as object sorting and multi-object rearrangement, including from raw pixel observations. Our analysis find that the compositional nature of language is critical for learning diverse sub-skills and systematically generalizing to new sub-skills in comparison to non-compositional abstractions that use the same supervision.},
archivePrefix = {arXiv},
arxivId = {1906.07343},
author = {Jiang, Yiding and Gu, Shixiang and Murphy, Kevin and Finn, Chelsea},
booktitle = { Workshop on “Structure {\&} Priors in Reinforcement Learning”at ICLR 2019},
eprint = {1906.07343},
month = {jun},
title = {{Language as an Abstraction for Hierarchical Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1906.07343},
year = {2019}
}

@article{Hermann2017,
abstract = {We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world.},
archivePrefix = {arXiv},
arxivId = {1706.06551},
author = {Hermann, Karl Moritz and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and Wainwright, Marcus and Apps, Chris and Hassabis, Demis and Blunsom, Phil},
eprint = {1706.06551},
month = {jun},
title = {{Grounded Language Learning in a Simulated 3D World}},
url = {http://arxiv.org/abs/1706.06551},
year = {2017}
}
@inproceedings{Goyal2019,
abstract = {Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60{\%} more often on average, compared to learning without language.},
archivePrefix = {arXiv},
arxivId = {1903.02020},
author = {Goyal, Prasoon and Niekum, Scott and Mooney, Raymond J.},
booktitle = {IJCAI 2019},
eprint = {1903.02020},
month = {mar},
title = {{Using Natural Language for Reward Shaping in Reinforcement Learning}},
url = {http://arxiv.org/abs/1903.02020},
year = {2019}
}
@inproceedings{Chaplot2018,
abstract = {To perform tasks specified by natural language instructions, autonomous agents need to extract semantically meaningful representations of language and map it to visual elements and actions in the environment. This problem is called task-oriented language grounding. We propose an end-to-end trainable neural architecture for task-oriented language grounding in 3D environments which assumes no prior linguistic or perceptual knowledge and requires only raw pixels from the environment and the natural language instruction as input. The proposed model combines the image and text representations using a Gated-Attention mechanism and learns a policy to execute the natural language instruction using standard reinforcement and imitation learning methods. We show the effectiveness of the proposed model on unseen instructions as well as unseen maps, both quantitatively and qualitatively. We also introduce a novel environment based on a 3D game engine to simulate the challenges of task-oriented language grounding over a rich set of instructions and environment states.},
archivePrefix = {arXiv},
arxivId = {1706.07230},
author = {Chaplot, Devendra Singh and Sathyendra, Kanthashree Mysore and Pasumarthi, Rama Kumar and Rajagopal, Dheeraj and Salakhutdinov, Ruslan},
booktitle = {AAAI 2018},
eprint = {1706.07230},
month = {jun},
title = {{Gated-Attention Architectures for Task-Oriented Language Grounding}},
url = {http://arxiv.org/abs/1706.07230},
year = {2018}
}
@inproceedings{chevalier-boisvert2018babyai,
author = {Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
booktitle = {International Conference on Learning Representations},
title = {{Baby{\{}AI{\}}: First Steps Towards Grounded Language Learning With a Human In the Loop}},
year = {2019}
}
@inproceedings{fu2018from,
author = {Fu, Justin and Korattikara, Anoop and Levine, Sergey and Guadarrama, Sergio},
booktitle = {International Conference on Learning Representations},
title = {{From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following}},
year = {2019}
}
@inproceedings{Branavan2010,
author = {{R. K. Branavan}, S and {S. Zettlemoyer}, Luke and Barzilay, Regina},
booktitle = {ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
pages = {1268--1277},
title = {{Reading Between the Lines: Learning to Map High-level Instructions to Commands}},
year = {2010}
}
@inproceedings{Chen2011,
author = {Chen, David L. and Mooney, Raymond J.},
booktitle = {AAAI Conference on Artificial Intelligence (AAAI), 2011 },
title = {{Learning to Interpret Natural Language Navigation Instructions from Observations}},
year = {2011}
}
@inproceedings{Kuhlmann04,
abstract = {We describe our current efforts towards creating a reinforcement learner that learns both from reinforcements provided by its environment and from human-generated advice. Our research involves two complementary components: (a) mapping advice expressed in English to a formal advice language and (b) using advice expressed in a formal notation in a reinforcement learner. We use a subtask of the challenging RoboCup simulated soccer task (Noda et al. 1998) as our testbed.},
author = "Kuhlmann, Gregory and Stone, Peter H and Mooney, Raymond J and Shavlik, Jude",
booktitle = {AAAI Workshop - Technical Report},
pages = {30--35},
title = {{Guiding a reinforcement learner with natural language advice: Initial results in RoboCup soccer}},
volume = {WS-04-10},
year = {2004}
}

@inproceedings{Silberer2012,
author = {Silberer, Carina and Lapata, Mirella},
booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
pages = {1423--1433},
publisher = {Association for Computational Linguistics},
title = {{Grounded Models of Semantic Representation}},
year = {2012}
}

@article{Zwaan05,
author = {Zwaan, Rolf and Madden, Carol},
doi = {10.1017/CBO9780511499968.010},
journal = {Grounding Cognition: The Role of Perception and Action in Memory, Language, and Thinking},
pages = {224--245},
publisher = {Cambridge University Press},
title = {{Embodied sentence comprehension}},
year = {2005}
}

@article{Glenberg2002,
author = {Glenberg, Arthur M. and Kaschak, Michael P.},
doi = {10.3758/BF03196313},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
month = {sep},
number = {3},
pages = {558--565},
publisher = {Springer-Verlag},
title = {{Grounding language in action}},
volume = {9},
year = {2002}
}

@article{Madden2010,
abstract = {This article addresses issues in embodied sentence processing from a “cognitive neural systems” approach that combines analysis of the behavior in question, analysis of the known neurophysiological bases of this behavior, and the synthesis of a neuro-computational model of embodied sentence processing that can be applied to and tested in the context of human–robot cooperative interaction. We propose a Hybrid Comprehension Model that links compact propositional representations of sentences and discourse with their temporal unfolding in situated simulations, under the control of grammar. The starting point is a model of grammatical construction processing which specifies the neural mechanisms by which language is a structured inventory of mappings from sentence to meaning. This model is then “embodied” in a perceptual-motor system (robot) which allows it access to sentence-perceptual representation pairs, and interaction with the world providing the basis for language acquisition. We then introduce a “simulation” capability, such that the robot has an internal representation of its interaction with the world. The control of this simulator and the associated representations present a number of interesting “neuro-technical” issues. First, the “simulator” has been liberated from real-time. It can run without being connected to current sensory motor experience. Second, “simulations” appear to be represented at different levels of detail. Our paper provides a framework for beginning to address the questions: how does language and its grammar control these aspects of simulation, what are the neurophysiological bases, and how can this be demonstrated in an artificial yet embodied cognitive system.},
author = {Madden, Carol and Hoen, Michel and Dominey, Peter Ford},
doi = {10.1016/J.BANDL.2009.07.001},
issn = {0093-934X},
journal = {Brain and Language},
month = {mar},
number = {3},
pages = {180--188},
publisher = {Academic Press},
title = {{A cognitive neuroscience perspective on embodied language for human–robot cooperation}},
volume = {112},
year = {2010}
}

@article{Dominey2005,
author = {Dominey, Peter Ford},
doi = {10.1080/09540090500270714},
issn = {0954-0091},
journal = {Connection Science},
month = {sep},
number = {3-4},
pages = {289--306},
title = {{Emergence of grammatical constructions: evidence from simulation and grounded agent experiments}},
volume = {17},
year = {2005}
}


% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{Calandra2016,
  Title                    = {Bayesian Optimization for Learning Gaits under Uncertainty},
  Author                   = {Calandra, Roberto and Andr\'e Seyfarth and Peters, Jan and Marc P. Deisenroth},
  Journal                  = {Annals of Mathematics and Artificial Intelligence (AMAI)},
  Year                     = {2016},
  Number                   = {1},
  Pages                    = {5--23},
  Volume                   = {76},

  Doi                      = {10.1007/s10472-015-9463-9},
  ISSN                     = {1573-7470},
  Keywords                 = {Gait Optimization, Bayesian Optimization, Bipedal Locomotion, Optimization},
  Owner                    = {Roberto Calandra},
  Timestamp                = {2015.05.06}
}

@Article{tscl,
  author    = {Tambet Matiisen and
               Avital Oliver and
               Taco Cohen and
               John Schulman},
  title     = {Teacher-Student Curriculum Learning},
  journal   = {CoRR},
  volume    = {abs/1707.00183},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.00183},
  archivePrefix = {arXiv},
  eprint    = {1707.00183},
  timestamp = {Mon, 13 Aug 2018 16:48:57 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MatiisenOCS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zpdes,
author = {Clement, Benjamin and Roy, Didier and Oudeyer, Pierre-Yves and Lopes, Manuel},
year = {2014},
month = {12},
pages = {},
title = {Developmental Learning for Intelligent Tutoring Systems},
journal = {IEEE ICDL-EPIROB 2014 - 4th Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics},
doi = {10.1109/DEVLRN.2014.6983019}
}

@inproceedings{DBLP:conf/icml/HaarnojaZAL18,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  pages     = {1856--1865},
  year      = {2018},
  crossref  = {DBLP:conf/icml/2018},
  url       = {http://proceedings.mlr.press/v80/haarnoja18b.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/icml/HaarnojaZAL18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:journals/corr/LillicrapHPHETS15,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  crossref  = {DBLP:conf/iclr/2016},
  url       = {http://arxiv.org/abs/1509.02971},
  timestamp = {Fri, 29 Mar 2019 10:35:07 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LillicrapHPHETS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{curious,
  author    = {C{\'{e}}dric Colas and
               Pierre{-}Yves Oudeyer and
               Olivier Sigaud and
               Pierre Fournier and
               Mohamed Chetouani},
  title     = {{CURIOUS:} Intrinsically Motivated Modular Multi-Goal Reinforcement
               Learning},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  pages     = {1331--1340},
  year      = {2019},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/icml/ColasOSFC19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{goalgan,
  author    = {Carlos Florensa and
               David Held and
               Xinyang Geng and
               Pieter Abbeel},
  title     = {Automatic Goal Generation for Reinforcement Learning Agents},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  pages     = {1514--1523},
  year      = {2018},
  crossref  = {DBLP:conf/icml/2018},
  url       = {http://proceedings.mlr.press/v80/florensa18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/icml/FlorensaHGA18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{unicorn,
  author    = {Daniel J. Mankowitz and
               Augustin Z{\'{\i}}dek and
               Andr{\'{e}} Barreto and
               Dan Horgan and
               Matteo Hessel and
               John Quan and
               Junhyuk Oh and
               Hado van Hasselt and
               David Silver and
               Tom Schaul},
  title     = {Unicorn: Continual Learning with a Universal, Off-policy Agent},
  journal   = {CoRR},
  volume    = {abs/1802.08294},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.08294},
  archivePrefix = {arXiv},
  eprint    = {1802.08294},
  timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-08294},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{poet,
  author    = {Rui Wang and
               Joel Lehman and
               Jeff Clune and
               Kenneth O. Stanley},
  title     = {Paired Open-Ended Trailblazer {(POET):} Endlessly Generating Increasingly
               Complex and Diverse Learning Environments and Their Solutions},
  journal   = {CoRR},
  volume    = {abs/1901.01753},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.01753},
  archivePrefix = {arXiv},
  eprint    = {1901.01753},
  timestamp = {Thu, 31 Jan 2019 13:52:49 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-01753},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{riac,
  author    = {Adrien Baranes and
               Pierre{-}Yves Oudeyer},
  title     = {{R-IAC:} Robust Intrinsically Motivated Exploration and Active Learning},
  journal   = {{IEEE} Trans. Autonomous Mental Development},
  volume    = {1},
  number    = {3},
  pages     = {155--169},
  year      = {2009},
  url       = {https://doi.org/10.1109/TAMD.2009.2037513},
  doi       = {10.1109/TAMD.2009.2037513},
  timestamp = {Sat, 27 May 2017 14:23:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tamd/BaranesO09},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{saggriac,
  author    = {Adrien Baranes and
               Pierre{-}Yves Oudeyer},
  title     = {Active Learning of Inverse Models with Intrinsically Motivated Goal
               Exploration in Robots},
  journal   = {CoRR},
  volume    = {abs/1301.4862},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.4862},
  archivePrefix = {arXiv},
  eprint    = {1301.4862},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1301-4862},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{moulinfriergmm,
	Author = {Moulin-Frier, Cl{\'e}ment and Nguyen, Sao Mai and Oudeyer, Pierre-Yves},
	Date-Modified = {2014-08-15 15:49:29 +0100},
	Doi = {10.3389/fpsyg.2013.01006},
	Issn = {1664-1078},
	Journal = {Frontiers in Psychology (Cognitive Science)},
	Number = {1006},
	Title = {Self-Organization of Early Vocal Development in Infants and Machines: The Role of Intrinsic Motivation},
	Url = {http://www.frontiersin.org/cognitive_science/10.3389/fpsyg.2013.01006/abstract},
	Volume = {4},
	Year = {2014}}
	
	@article{kdtree,
  author    = {Jon Louis Bentley},
  title     = {Multidimensional Binary Search Trees Used for Associative Searching},
  journal   = {Commun. {ACM}},
  volume    = {18},
  number    = {9},
  pages     = {509--517},
  year      = {1975},
  url       = {http://doi.acm.org/10.1145/361002.361007},
  doi       = {10.1145/361002.361007},
  timestamp = {Wed, 14 Nov 2018 10:22:32 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/cacm/Bentley75},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{imgep,
  author    = {S{\'{e}}bastien Forestier and
               Yoan Mollard and
               Pierre{-}Yves Oudeyer},
  title     = {Intrinsically Motivated Goal Exploration Processes with Automatic
               Curriculum Learning},
  journal   = {CoRR},
  volume    = {abs/1708.02190},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.02190},
  archivePrefix = {arXiv},
  eprint    = {1708.02190},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-02190},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@article{agentdesignha,
  author    = {David Ha},
  title     = {Reinforcement Learning for Improving Agent Design},
  journal   = {CoRR},
  volume    = {abs/1810.03779},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.03779},
  archivePrefix = {arXiv},
  eprint    = {1810.03779},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-03779},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{tscllike,
  author    = {Mysore, S., Platt, R., Saenko, K.},
  title     = {Reward-guided Curriculum for Robust Reinforcement Learning},
  year      = {2018}
}

@inproceedings{bengiocl,
  author    = {Yoshua Bengio and
               J{\'{e}}r{\^{o}}me Louradour and
               Ronan Collobert and
               Jason Weston},
  title     = {Curriculum learning},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine
               Learning, {ICML} 2009, Montreal, Quebec, Canada, June 14-18, 2009},
  pages     = {41--48},
  year      = {2009},
  url       = {https://doi.org/10.1145/1553374.1553380},
  doi       = {10.1145/1553374.1553380},
  timestamp = {Wed, 14 Nov 2018 10:58:56 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/icml/BengioLCW09},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{elman,
title = "Learning and development in neural networks: the importance of starting small",
journal = "Cognition",
volume = "48",
number = "1",
pages = "71 - 99",
year = "1993",
issn = "0010-0277",
doi = "https://doi.org/10.1016/0010-0277(93)90058-4",
url = "http://www.sciencedirect.com/science/article/pii/0010027793900584",
author = "Jeffrey L. Elman"
}

@article{taylortransfer,
 author = {Taylor, Matthew E. and Stone, Peter},
 title = {Transfer Learning for Reinforcement Learning Domains: A Survey},
 journal = {J. Mach. Learn. Res.},
 issue_date = {12/1/2009},
 volume = {10},
 month = dec,
 year = {2009},
 issn = {1532-4435},
 pages = {1633--1685},
 numpages = {53},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1755839},
 acmid = {1755839},
 publisher = {JMLR.org},
} 
[download]

@inproceedings{her,
  author    = {Marcin Andrychowicz and
               Dwight Crow and
               Alex Ray and
               Jonas Schneider and
               Rachel Fong and
               Peter Welinder and
               Bob McGrew and
               Josh Tobin and
               Pieter Abbeel and
               Wojciech Zaremba},
  title     = {Hindsight Experience Replay},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, 4-9 December 2017,
               Long Beach, CA, {USA}},
  pages     = {5055--5065},
  year      = {2017},
  crossref  = {DBLP:conf/nips/2017},
  url       = {http://papers.nips.cc/paper/7090-hindsight-experience-replay},
  timestamp = {Mon, 27 Nov 2017 12:38:48 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/nips/AndrychowiczCRS17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{krueger,
title = "Flexible shaping: How learning in small steps helps",
journal = "Cognition",
volume = "110",
number = "3",
pages = "380 - 394",
year = "2009",
issn = "0010-0277",
doi = "https://doi.org/10.1016/j.cognition.2008.11.014",
url = "http://www.sciencedirect.com/science/article/pii/S0010027708002850",
author = "Kai A. Krueger and Peter Dayan",
keywords = "PFC, Gating, Shaping, Sequence learning, Computational modeling"
}

@article{playgroundexp,
author = "P. Oudeyer and F. Kaplan and V. Hafner and A. Whyte",
title = "The playground experiment: Task-independent development of a curious robot",
journal = "Proceedings
of the AAAI Spring Symposium on Developmental Robotics",
pages = "42 - 47",
year = "2005"
}

@article{devrobbook,
author = {Blank, Douglas and Kumar, Deepak and Meeden, Lisa and Marshall, James},
year = {2003},
month = {12},
pages = {},
title = {Bringing up robot: Fundamental mechanisms for creating a self-motivated, self-organizing architecture},
journal = {Cybernetics & Systems},
doi = {10.1080/01969720590897107}
}
 

@book{intrinsicbook ,
author = {E.L. Deci and M. Ryan},
title = {Intrinsic Motivation and self-determination in
human behavior},
publisher = {Plenum Press},
address = {New York},
year = "1985",
}

@inproceedings{strategicstudent,
  TITLE = {{The Strategic Student Approach for Life-Long Exploration and Learning}},
  AUTHOR = {Lopes, Manuel and Oudeyer, Pierre-Yves},
  URL = {https://hal.inria.fr/hal-00755216},
  BOOKTITLE = {{IEEE Conference on Development and Learning / EpiRob 2012}},
  ADDRESS = {San Diego, United States},
  YEAR = {2012},
  MONTH = Nov,
  PDF = {https://hal.inria.fr/hal-00755216/file/PID2563983.pdf},
  HAL_ID = {hal-00755216},
  HAL_VERSION = {v1},
}

@article{sac,
  author    = {Tuomas Haarnoja and
               Aurick Zhou and
               Pieter Abbeel and
               Sergey Levine},
  title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
               with a Stochastic Actor},
  journal   = {CoRR},
  volume    = {abs/1801.01290},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.01290},
  archivePrefix = {arXiv},
  eprint    = {1801.01290},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-01290},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bic,
  added-at = {2009-08-24T15:07:31.000+0200},
  author = {Schwarz, G.},
  biburl = {https://www.bibsonomy.org/bibtex/215b8ffe9f50cb4fca87e30b578ac8f2e/neongod},
  interhash = {6b480223b0df6bd10504863cede240c5},
  intrahash = {15b8ffe9f50cb4fca87e30b578ac8f2e},
  journal = {The Annals of Statistics},
  keywords = {bayesian bic criterion information},
  pages = {461--464},
  timestamp = {2009-08-24T15:07:32.000+0200},
  title = {Estimating the dimension of a model},
  volume = 6,
  year = 1978
}

@Article{aic,
author="Bozdogan, Hamparsum",
title="Model selection and Akaike's Information Criterion (AIC): The general theory and its analytical extensions",
journal="Psychometrika",
year="1987",
month="Sep",
day="01",
volume="52",
number="3",
pages="345--370",
issn="1860-0980",
doi="10.1007/BF02294361",
url="https://doi.org/10.1007/BF02294361"
}

@article{bornstein1992maternal,
  title={Maternal responsiveness to infants in three societies: The United States, France, and Japan},
  author={Bornstein, Marc H and Tamis-LeMonda, Catherine S and Tal, Joseph and Ludemann, Pamela and Toda, Sueko and Rahn, Charles W and P{\^e}cheux, Marie-Germaine and Azuma, Hiroshi and Vardi, Danya},
  journal={Child development},
  volume={63},
  number={4},
  pages={808--821},
  year={1992},
  publisher={Wiley Online Library}
}

@inproceedings{malmo,
  author    = {Matthew Johnson and
               Katja Hofmann and
               Tim Hutton and
               David Bignell},
  title     = {The Malmo Platform for Artificial Intelligence Experimentation},
  booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on
               Artificial Intelligence, {IJCAI} 2016, New York, NY, USA, 9-15 July
               2016},
  pages     = {4246--4247},
  year      = {2016},
  crossref  = {DBLP:conf/ijcai/2016},
  url       = {http://www.ijcai.org/Abstract/16/643},
  timestamp = {Fri, 15 Jul 2016 15:58:28 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/ijcai/JohnsonHHB16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{keysers2019measuring,
    title={Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
    author={Daniel Keysers and Nathanael Schärli and Nathan Scales and Hylke Buisman and Daniel Furrer and Sergii Kashubin and Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and Olivier Bousquet},
    year={2019},
    eprint={1912.09713},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@INPROCEEDINGS{modularforestier, 
author={S. {Forestier} and P. {Oudeyer}}, 
booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Modular active curiosity-driven discovery of tool use}, 
year={2016}, 
volume={}, 
number={}, 
pages={3965-3972}, 
keywords={inverse problems;learning systems;manipulators;motion control;modular active curiosity-driven discovery;high-dimensional structured sensorimotor space;tool use discovery;goal babbling architecture;solution learning;inverse model;parameterized sensorimotor problem;parameterized motor primitives;space exploration;exploration architecture;model babbling;modular space representation;MACOB architecture;object movement;structured high-dimensional continuous motor space;sensory space;Robot sensing systems;Databases;Inverse problems;Silicon;Aerospace electronics;Data models}, 
doi={10.1109/IROS.2016.7759584}, 
ISSN={2153-0866}, 
month={Oct},}


@incollection{gmm,
title = {The Infinite Gaussian Mixture Model},
author = {Carl Edward Rasmussen},
booktitle = {Advances in Neural Information Processing Systems 12},
editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},
pages = {554--560},
year = {2000},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1745-the-infinite-gaussian-mixture-model.pdf}
}

@ARTICLE{EMalgo,
    author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
    title = {Maximum likelihood from incomplete data via the EM algorithm},
    journal = {JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B},
    year = {1977},
    volume = {39},
    number = {1},
    pages = {1--38}
}


@inproceedings{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014}
}


@incollection{Vygotskii1978,
abstract = {Vygotsky, L. S. (1978). Mind in Society. The development of higher psychological processes. Cambridge, MA: Harvard University Press.},
author = {Vygotsky, L. S.},
booktitle = {Mind in Society},
chapter = {Tool and Symbol in Child Development},
doi = {10.2307/j.ctvjf9vz4.6},
isbn = {0674576292},
pages = {19--30},
publisher = {Harvard University Press},
title = {{Tool and Symbol in Child Development}},
year = {1978}
}

@article{fujimoto2018addressing,
  title={Addressing Function Approximation Error in Actor-Critic Methods},
  author={Fujimoto, Scott and van Hoof, Herke and Meger, Dave},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}

@article{DBLP:journals/tnn/SuttonB98,
  author    = {Richard S. Sutton and
               Andrew G. Barto},
  title     = {Reinforcement Learning: An Introduction},
  journal   = {{IEEE} Trans. Neural Networks},
  volume    = {9},
  number    = {5},
  pages     = {1054--1054},
  year      = {1998},
  url       = {https://doi.org/10.1109/TNN.1998.712192},
  doi       = {10.1109/TNN.1998.712192},
  timestamp = {Sun, 28 May 2017 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tnn/SuttonB98},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lin1992self,
  title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
  author={Lin, Long-Ji},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={293--321},
  year={1992},
  publisher={Springer}
}



@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}


@article{veeriah2018many,
  title={Many-Goals Reinforcement Learning},
  author={Veeriah, Vivek and Oh, Junhyuk and Singh, Satinder},
  journal={arXiv preprint arXiv:1806.09605},
  year={2018}
}



@article{mann1947test,
  title={On a test of whether one of two random variables is stochastically larger than the other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The annals of mathematical statistics},
  pages={50--60},
  year={1947},
  publisher={JSTOR}
}


@article{riedmiller2018learning,
  title={Learning by Playing-Solving Sparse Reward Tasks from Scratch},
  author={Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Van de Wiele, Tom and Mnih, Volodymyr and Heess, Nicolas and Springenberg, Jost Tobias},
  journal={arXiv preprint arXiv:1802.10567},
  year={2018}
}

@article{oudeyer2007intrinsic,
  title={Intrinsic motivation systems for autonomous mental development},
  author={Oudeyer, Pierre-Yves and Kaplan, Frdric and Hafner, Verena V},
  journal={IEEE transactions on evolutionary computation},
  volume={11},
  number={2},
  pages={265--286},
  year={2007},
  publisher={IEEE}
}

@incollection{kaplan2004maximizing,
  title={Maximizing learning progress: an internal reward system for development},
  author={Kaplan, Fr{\'e}d{\'e}ric and Oudeyer, Pierre-Yves},
  booktitle={Embodied artificial intelligence},
  pages={259--270},
  year={2004},
  publisher={Springer}
}

@article{espeholt2018impala,
  title={IMPALA: Scalable distributed Deep-RL with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}


@article{mai2012active,
  title={Active choice of teachers, learning strategies and goals for a socially guided intrinsic motivation learner},
  author={Nguyen, Sao Mai and Oudeyer, Pierre-Yves},
  journal={Paladyn},
  volume={3},
  number={3},
  pages={136--146},
  year={2012},
  publisher={Springer}
}

@article{zhao2018energy,
  title={Energy-Based Hindsight Experience Prioritization},
  author={Zhao, Rui and Tresp, Volker},
  journal={arXiv preprint arXiv:1810.01363},
  year={2018}
}

@article{nachum2018data,
  title={Data-Efficient Hierarchical Reinforcement Learning},
  author={Nachum, Ofir and Gu, Shane and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1805.08296},
  year={2018}
}

@article{chu2020exploratory,
  title={Exploratory play, rational action, and efficient search},
  author={Chu, Junyi and Schulz, Laura},
  year={2020},
  publisher={PsyArXiv}
}


@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}


@article{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1703.01161},
  year={2017}
}

@article{levy2018hierarchical,
  title={Hierarchical Reinforcement Learning with Hindsight},
  author={Levy, Andrew and Platt, Robert and Saenko, Kate},
  journal={arXiv preprint arXiv:1805.08180},
  year={2018}
}


@article{hessel2018multi,
  title={Multi-task deep reinforcement learning with popart},
  author={Hessel, Matteo and Soyer, Hubert and Espeholt, Lasse and Czarnecki, Wojciech and Schmitt, Simon and van Hasselt, Hado},
  journal={arXiv preprint arXiv:1809.04474},
  year={2018}
}

@inproceedings{teh2017distral,
  title={Distral: Robust multitask reinforcement learning},
  author={Teh, Yee and Bapst, Victor and Czarnecki, Wojciech M and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4496--4506},
  year={2017}
}


@article{oudeyer2016evolution,
  title={How evolution may work through curiosity-driven developmental process},
  author={Oudeyer, Pierre-Yves and Smith, Linda B},
  journal={Topics in Cognitive Science},
  volume={8},
  number={2},
  pages={492--502},
  year={2016},
  publisher={Wiley Online Library}
}

@inproceedings{schmidhuber1991curious,
  title={Curious model-building control systems},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={Neural Networks, 1991. 1991 IEEE International Joint Conference on},
  pages={1458--1463},
  year={1991},
  organization={IEEE}
}


@inproceedings{lopes2012strategic,
  title={The strategic student approach for life-long exploration and learning},
  author={Lopes, Manuel and Oudeyer, Pierre-Yves},
  booktitle={Development and Learning and Epigenetic Robotics (ICDL), 2012 IEEE International Conference on},
  pages={1--8},
  year={2012},
  organization={IEEE}
}

@article{vevcerik2017leveraging,
  title={Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards},
  author={Ve{\v{c}}er{\'\i}k, Matej and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
} 

@article{held2017automatic,
  title={Automatic goal generation for reinforcement learning agents},
  author={Held, David and Geng, Xinyang and Florensa, Carlos and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1705.06366},
  year={2017}
}

@inproceedings{baranes2011interaction,
  title={The interaction of maturational constraints and intrinsic motivations in active motor development},
  author={Baranes, Adrien and Oudeyer, Pierre-Yves},
  booktitle={Development and Learning (ICDL), 2011 IEEE International Conference on},
  volume={2},
  pages={1--8},
  year={2011},
  organization={IEEE}
}

@inproceedings{baranes2010intrinsically,
  title={Intrinsically motivated goal exploration for active motor learning in robots: A case study},
  author={Baranes, Adrien and Oudeyer, Pierre-Yves},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010)},
  year={2010}
}

@article{mathys2011bayesian,
  title={A Bayesian foundation for individual learning under uncertainty},
  author={Mathys, Christoph and Daunizeau, Jean and Friston, Karl J and Stephan, Klaas Enno},
  journal={Frontiers in human neuroscience},
  volume={5},
  pages={39},
  year={2011},
  publisher={Frontiers}
}

@article{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@article{colas2018gep,
  title={{GEP-PG}: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms},
  author={Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1802.05054},
  year={2018}
}

@article{nair2017overcoming,
  title={Overcoming exploration in reinforcement learning with demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1709.10089},
  year={2017}
}

@article{ther,
  title={Self-Educated Language Agent With Hindsight Experience Replay For Instruction Following},
  author={Cideron, Geoffrey and Seurin, Mathieu and Strub, Florian and Pietquin, Olivier},
  journal={arXiv preprint arXiv:1910.09451},
  year={2019}
}



@article{Bruner1991,
abstract = {Surely since the Enlightenment, if not before, the study of mind has centered principally on how man achieves a "true" knowledge of the world. Emphasis in this pursuit has varied, of course: empiricists have con- centrated on the mind's interplay with an external world of nature, hop- ing to find the key in the association of sensations and ideas, while rationalists have looked inward to the powers of mind itself for the princi- ples of right reason. The objective, in either case, has been to discover how we achieve "reality," that is to say, how we get a reliable fix on the world, a world that is, as it were, assumed to be immutable and, as it were, "there to be observed."},
author = {Bruner, Jerome},
doi = {10.1086/448619},
issn = {0093-1896},
journal = {Critical Inquiry},
mendeley-groups = {Psychology,Cognitive neuroscience},
month = {oct},
number = {1},
pages = {1--21},
publisher = {University of Chicago Press},
title = {{The Narrative Construction of Reality}},
url = {},
volume = {18},
year = {1991}
}

@book{Piaget1926,
abstract = {3rd ed. This book is for anyone who has ever wondered how a child develops language, thought, and knowledge. Before this classic appeared, little was known of the way children think. In 1923, however, Jean Piaget, the most important developmental psychologist of the twentieth century, took the psychological world by storm with The Language and Thought of the Child. Applying for the first time the insights of social psychology and psychoanalysis to the observation of children, he uncovered the ways in which a child actively constructs his or her understanding of the world through language. The book has since been a source of inspiration and guidance to generations of parents and teachers. While its conclusions remain contentious to this very day, few can deny the huge debt we owe to this pioneering work in our continuing attempts to understand the minds of the child -- publisher's description. The functions of language in two children of six -- Types and stages in the conversation of children between the ages of four and seven -- Understanding and verbal explanation between children of the same age between the years of six and eight -- Some peculiarities of verbal understanding in the child between the ages of nine and eleven -- The questions of a child of six -- The measure of ego-centric language in verbal communication between the adult and the child and in verbal exchanges between children.},
author = {Piaget, Jean},
isbn = {0415267501},
mendeley-groups = {Psychology,Cognitive neuroscience},
pages = {294},
publisher = {Routledge},
title = {{The language and thought of the child}},
year = {1926}
}

@book{Tomasello1999,
abstract = {"Many current theories of human cognition stress its biological roots, while others stress its cultural roots. Tomasello demonstrates that both of these perspectives are essential in creating a unified account of the evolution, history, and development of human cognition. He makes a powerful case that while human cognition is biologically based, this biological adaptation's key contribution is that it permits the flowering of the cultural-historical and ontogenetic processes that have actually made the varieties of human cognition what they are today."--Jacket. 1. A Puzzle and a Hypothesis; 2. Biological and Cultural Inheritance; 3. Joint Attention and Cultural Learning; 4. Linguistic Communication and Symbolic Representation; 5. Linguistic Constructions and Event Cognition; 6. Discourse and Representational Redescription; 7. Cultural Cognition; References; Index.},
author = {Tomasello, Michael},
publisher={Harvard University Press},
isbn = {9780674005822},
mendeley-groups = {Psychology,Cognitive neuroscience},
title = {{The cultural origins of human cognition}},
year = {1999}
}

@book{Chomsky1957,
abstract = {Noam Chomsky's first book on syntactic structures is one of the first serious attempts on the part of a linguist to construct within the tradition of scientific theory-construction a comprehensive theory of language which may be understood in the same sense that a chemical, biological theory is understood by experts in those fields. It is not a mere reorganization of the data into a new kind of library catalogue, nor another specualtive philosophy about the nature of man and language, but rather a rigorus explication of our intuitions about our language in terms of an overt axiom system, the theorems derivable from it, explicit results which may be compared with new data and other intuitions, all based plainly on an overt theory of the internal structure of languages; and it may well provide an opportunity for the application of explicity measures of simplicity to decide preference of one form over another form of grammar. Introduction --- The Independence of Grammar --- An Elementary Linguistic Theory --- Phrase Structure --- Limitations of Phrase Structure Description --- On the Goals of Linguistic Theory --- Some Transformations in English --- The Explanatory Power of Linguistic Theory --- Syntax and Semantics --- Summary --- Appendix I: Notations and Terminology --- Appendix II: Examples of English Phrase Structure and Transformational Rules.},
author = {Chomsky, Noam.},
isbn = {9789027933850},
mendeley-groups = {Psychology,Cognitive neuroscience},
publisher = {Mouton},
title = {{Syntactic structures}},
year = {1957}
}
