\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andukuri et~al.(2024)Andukuri, Fr{\"a}nken, Gerstenberg, and Goodman]{andukuri2024star}
Chinmaya Andukuri, Jan-Philipp Fr{\"a}nken, Tobias Gerstenberg, and Noah~D Goodman.
\newblock Star-gate: Teaching language models to ask clarifying questions.
\newblock \emph{arXiv preprint arXiv:2403.19154}, 2024.

\bibitem[Anthony et~al.(2017)Anthony, Tian, and Barber]{anthony2017thinking}
Thomas Anthony, Zheng Tian, and David Barber.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones, Joseph, Mann, DasSarma, et~al.]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos]{azar2023general}
Mohammad~Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and R{\'e}mi Munos.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock \emph{arXiv preprint arXiv:2310.12036}, 2023.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao, Aschenbrenner, Chen, Ecoffet, Joglekar, Leike, et~al.]{burns2023weak}
Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et~al.
\newblock Weak-to-strong generalization: Eliciting strong capabilities with weak supervision.
\newblock \emph{arXiv preprint arXiv:2312.09390}, 2023.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Deshpande et~al.(2023)Deshpande, Murahari, Rajpurohit, Kalyan, and Narasimhan]{deshpande2023toxicity}
Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan.
\newblock Toxicity in chatgpt: Analyzing persona-assigned language models.
\newblock \emph{arXiv preprint arXiv:2304.05335}, 2023.

\bibitem[Dubois et~al.(2023)Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin, Liang, and Hashimoto]{dubois2023alpacafarm}
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Alpacafarm: A simulation framework for methods that learn from human feedback, 2023.

\bibitem[Durmus et~al.(2023)Durmus, Nyugen, Liao, Schiefer, Askell, Bakhtin, Chen, Hatfield-Dodds, Hernandez, Joseph, et~al.]{durmus2023towards}
Esin Durmus, Karina Nyugen, Thomas~I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et~al.
\newblock Towards measuring the representation of subjective global opinions in language models.
\newblock \emph{arXiv preprint arXiv:2306.16388}, 2023.

\bibitem[Ethayarajh et~al.(2024)Ethayarajh, Xu, Muennighoff, Jurafsky, and Kiela]{ethayarajh2024kto}
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela.
\newblock Kto: Model alignment as prospect theoretic optimization.
\newblock \emph{arXiv preprint arXiv:2402.01306}, 2024.

\bibitem[Fr{\"a}nken et~al.(2023)Fr{\"a}nken, Kwok, Ye, Gandhi, Arumugam, Moore, Tamkin, Gerstenberg, and Goodman]{franken2023social}
Jan-Philipp Fr{\"a}nken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, and Noah~D Goodman.
\newblock Social contract ai: Aligning ai assistants with implicit group norms.
\newblock \emph{arXiv preprint arXiv:2310.17769}, 2023.

\bibitem[Grice(1975)]{grice1975logic}
Herbert~P Grice.
\newblock Logic and conversation.
\newblock In \emph{Speech acts}, pages 41--58. Brill, 1975.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kundu et~al.(2023)Kundu, Bai, Kadavath, Askell, Callahan, Chen, Goldie, Balwit, Mirhoseini, McLean, et~al.]{kundu2023specific}
Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen, Anna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et~al.
\newblock Specific versus general principles for constitutional ai.
\newblock \emph{arXiv preprint arXiv:2310.13798}, 2023.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph~E. Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem[Lee et~al.(2023)Lee, Phatale, Mansoor, Lu, Mesnard, Bishop, Carbune, and Rastogi]{lee2023rlaif}
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi.
\newblock Rlaif: Scaling reinforcement learning from human feedback with ai feedback.
\newblock \emph{arXiv preprint arXiv:2309.00267}, 2023.

\bibitem[Li et~al.(2023)Li, Zhang, Dubois, Taori, Gulrajani, Guestrin, Liang, and Hashimoto]{li2023alpacaeval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models, 2023.

\bibitem[Lin et~al.(2024)Lin, Ravichander, Lu, Dziri, Sclar, Chandu, Bhagavatula, and Choi]{Lin2024ReAlign}
Bill~Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi.
\newblock The unlocking spell on base llms: Rethinking alignment via in-context learning.
\newblock In \emph{International Conference on Learning Representations}, 2024.
\newblock URL \url{https://arxiv.org/abs/2312.01552}.

\bibitem[{Meta}(2024)]{meta_llama_3_2024}
{Meta}.
\newblock Introducing meta llama 3: The most capable openly available llm to date.
\newblock \url{https://www.meta.com/blog/meta-llama-3-introduction}, April 2024.
\newblock Accessed: 2024-05-12.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[{OpenAI}(2023)]{openai_gpt4_2023}
{OpenAI}.
\newblock {GPT-4 Technical Report}.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Park et~al.(2024)Park, Rafailov, Ermon, and Finn]{park2024disentangling}
Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn.
\newblock Disentangling length from quality in direct preference optimization.
\newblock \emph{arXiv preprint arXiv:2403.19159}, 2024.

\bibitem[Poole et~al.(2019)Poole, Ozair, Van Den~Oord, Alemi, and Tucker]{poole2019variational}
Ben Poole, Sherjil Ozair, Aaron Van Den~Oord, Alex Alemi, and George Tucker.
\newblock On variational bounds of mutual information.
\newblock In \emph{International Conference on Machine Learning}, pages 5171--5180. PMLR, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Stiennon et~al.(2022)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2022learning}
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano.
\newblock Learning to summarize from human feedback, 2022.

\bibitem[Sun et~al.(2023)Sun, Shen, Zhang, Zhou, Chen, Cox, Yang, and Gan]{sun2023salmon}
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.
\newblock Salmon: Self-alignment with principle-following reward models.
\newblock \emph{arXiv preprint arXiv:2310.05910}, 2023.

\bibitem[Sun et~al.(2024)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan]{sun2024principle}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Tang et~al.(2024)Tang, Guo, Zheng, Calandriello, Munos, Rowland, Richemond, Valko, Pires, and Piot]{tang2024generalized}
Yunhao Tang, Zhaohan~Daniel Guo, Zeyu Zheng, Daniele Calandriello, R{\'e}mi Munos, Mark Rowland, Pierre~Harvey Richemond, Michal Valko, Bernardo~{\'A}vila Pires, and Bilal Piot.
\newblock Generalized preference optimization: A unified approach to offline alignment.
\newblock \emph{arXiv preprint arXiv:2402.05749}, 2024.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Wang et~al.(2023)Wang, Li, Chen, Zhu, Lin, Cao, Liu, Liu, and Sui]{wang2023large}
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu, Tianyu Liu, and Zhifang Sui.
\newblock Large language models are not fair evaluators.
\newblock \emph{arXiv preprint arXiv:2305.17926}, 2023.

\bibitem[Williams and Nadel(1989)]{williams1989style}
Joseph~M Williams and Ira~Bruce Nadel.
\newblock \emph{Style: Ten lessons in clarity and grace}.
\newblock Scott, Foresman Glenview, IL, 1989.

\bibitem[Yang et~al.(2023)Yang, Klein, Celikyilmaz, Peng, and Tian]{yang2023rlcd}
Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian.
\newblock Rlcd: Reinforcement learning from contrast distillation for language model alignment.
\newblock \emph{arXiv preprint arXiv:2307.12950}, 2023.

\bibitem[Yin et~al.(2024)Yin, Wang, Gu, Huang, Chen, and Zhou]{yin2024relative}
Yueqin Yin, Zhendong Wang, Yi~Gu, Hai Huang, Weizhu Chen, and Mingyuan Zhou.
\newblock Relative preference optimization: Enhancing llm alignment through contrasting responses across identical and diverse prompts.
\newblock \emph{arXiv preprint arXiv:2402.10958}, 2024.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 15476--15488, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{zhao2023slic}
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter~J Liu.
\newblock Slic-hf: Sequence likelihood calibration with human feedback.
\newblock \emph{arXiv preprint arXiv:2305.10425}, 2023.

\bibitem[Zheng et~al.(2024)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhou et~al.(2024)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu, et~al.]{zhou2024lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\end{thebibliography}
