\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ash \& Adams(2020)Ash and Adams]{ash2020warm}
Ash, J. and Adams, R.~P.
\newblock On warm-starting neural network training.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 3884--3894, 2020.

\bibitem[Buzzega et~al.(2020)Buzzega, Boschini, Porrello, Abati, and Calderara]{buzzega2020dark}
Buzzega, P., Boschini, M., Porrello, A., Abati, D., and Calderara, S.
\newblock Dark experience for general continual learning: a strong, simple baseline.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 15920--15930, 2020.

\bibitem[Caccia et~al.(2021)Caccia, Aljundi, Asadi, Tuytelaars, Pineau, and Belilovsky]{caccia2021new}
Caccia, L., Aljundi, R., Asadi, N., Tuytelaars, T., Pineau, J., and Belilovsky, E.
\newblock New insights on reducing abrupt representation change in online continual learning.
\newblock \emph{arXiv preprint arXiv:2104.05025}, 2021.

\bibitem[Caccia et~al.(2020)Caccia, Rodriguez, Ostapenko, Normandin, Lin, Caccia, Laradji, Rish, Lacoste, Vazquez, et~al.]{caccia2020online}
Caccia, M., Rodriguez, P., Ostapenko, O., Normandin, F., Lin, M., Caccia, L., Laradji, I., Rish, I., Lacoste, A., Vazquez, D., et~al.
\newblock Online fast adaptation and knowledge accumulation: a new approach to continual learning.
\newblock \emph{arXiv preprint arXiv:2003.05856}, 2020.

\bibitem[Carta et~al.(2023{\natexlab{a}})Carta, Pellegrini, Cossu, Hemati, and Lomonaco]{carta2023avalanche}
Carta, A., Pellegrini, L., Cossu, A., Hemati, H., and Lomonaco, V.
\newblock Avalanche: A pytorch library for deep continual learning.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (363):\penalty0 1--6, 2023{\natexlab{a}}.

\bibitem[Carta et~al.(2023{\natexlab{b}})Carta, Van~de Weijer, et~al.]{carta2023improving}
Carta, A., Van~de Weijer, J., et~al.
\newblock Improving online continual learning performance and stability with temporal ensembles.
\newblock \emph{arXiv preprint arXiv:2306.16817}, 2023{\natexlab{b}}.

\bibitem[Censor \& Zenios(1992)Censor and Zenios]{censor1992proximal}
Censor, Y. and Zenios, S.~A.
\newblock Proximal minimization algorithm with d-functions.
\newblock \emph{Journal of Optimization Theory and Applications}, 73\penalty0 (3):\penalty0 451–464, June 1992.
\newblock ISSN 1573-2878.
\newblock \doi{10.1007/bf00940051}.
\newblock URL \url{http://dx.doi.org/10.1007/BF00940051}.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Ranzato, Rohrbach, and Elhoseiny]{chaudhry2018efficient}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with a-gem.
\newblock \emph{arXiv preprint arXiv:1812.00420}, 2018.

\bibitem[Chaudhry et~al.(2019)Chaudhry, Rohrbach, Elhoseiny, Ajanthan, Dokania, Torr, and Ranzato]{chaudhry2019continual}
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P., Torr, P., and Ranzato, M.
\newblock Continual learning with tiny episodic memories.
\newblock In \emph{Workshop on Multi-Task and Lifelong Reinforcement Learning}, 2019.

\bibitem[De~Lange et~al.(2022)De~Lange, van~de Ven, and Tuytelaars]{de2022continual}
De~Lange, M., van~de Ven, G., and Tuytelaars, T.
\newblock Continual evaluation for lifelong learning: Identifying the stability gap.
\newblock \emph{arXiv preprint arXiv:2205.13452}, 2022.

\bibitem[Deng et~al.(2021)Deng, Chen, Hao, Wang, and Heng]{deng2021flattening}
Deng, D., Chen, G., Hao, J., Wang, Q., and Heng, P.-A.
\newblock Flattening sharpness for dynamic gradient projection memory benefits continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 18710--18721, 2021.

\bibitem[Drusvyatskiy(2017)]{drusvyatskiy2017proximal}
Drusvyatskiy, D.
\newblock The proximal point method revisited, 2017.

\bibitem[Duncker et~al.(2020)Duncker, Driscoll, Shenoy, Sahani, and Sussillo]{duncker2020organizing}
Duncker, L., Driscoll, L., Shenoy, K.~V., Sahani, M., and Sussillo, D.
\newblock Organizing recurrent network dynamics by task-computation to enable continual learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 14387--14397, 2020.

\bibitem[Farajtabar et~al.(2020)Farajtabar, Azizan, Mott, and Li]{farajtabar2020orthogonal}
Farajtabar, M., Azizan, N., Mott, A., and Li, A.
\newblock Orthogonal gradient descent for continual learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  3762--3773. PMLR, 2020.

\bibitem[French(1999)]{french1999catastrophic}
French, R.~M.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in cognitive sciences}, 3\penalty0 (4):\penalty0 128--135, 1999.

\bibitem[Guo et~al.(2022)Guo, Hu, Zhao, and Liu]{guo2022adaptive}
Guo, Y., Hu, W., Zhao, D., and Liu, B.
\newblock Adaptive orthogonal projection for batch and online continual learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  6783--6791, 2022.

\bibitem[Hess et~al.(2023)Hess, Tuytelaars, and van~de Ven]{hess2023two}
Hess, T., Tuytelaars, T., and van~de Ven, G.~M.
\newblock Two complementary perspectives to continual learning: Ask not only what to optimize, but also how.
\newblock \emph{arXiv preprint arXiv:2311.04898}, 2023.

\bibitem[Kao et~al.(2021)Kao, Jensen, van~de Ven, Bernacchia, and Hennequin]{kao2021natural}
Kao, T.-C., Jensen, K., van~de Ven, G., Bernacchia, A., and Hennequin, G.
\newblock Natural continual learning: success is a journey, not (just) a destination.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 28067--28079, 2021.

\bibitem[Konishi et~al.(2023)Konishi, Kurokawa, Ono, Ke, Kim, and Liu]{konishi2023parameter}
Konishi, T., Kurokawa, M., Ono, C., Ke, Z., Kim, G., and Liu, B.
\newblock Parameter-level soft-masking for continual learning.
\newblock \emph{arXiv preprint arXiv:2306.14775}, 2023.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Liang \& Li(2023)Liang and Li]{liang2023loss}
Liang, Y.-S. and Li, W.-J.
\newblock Loss decoupling for task-agnostic continual learning.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Lin et~al.(2022)Lin, Yang, Fan, and Zhang]{lin2022trgp}
Lin, S., Yang, L., Fan, D., and Zhang, J.
\newblock Trgp: Trust region gradient projection for continual learning.
\newblock \emph{arXiv preprint arXiv:2202.02931}, 2022.

\bibitem[Lin et~al.(2021)Lin, Shi, Pathak, and Ramanan]{lin2021clear}
Lin, Z., Shi, J., Pathak, D., and Ramanan, D.
\newblock The clear benchmark: Continual learning on real-world imagery.
\newblock In \emph{Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 2)}, 2021.

\bibitem[Lomonaco \& Maltoni(2017)Lomonaco and Maltoni]{lomonaco2017core50}
Lomonaco, V. and Maltoni, D.
\newblock Core50: a new dataset and benchmark for continuous object recognition.
\newblock In \emph{Conference on robot learning}, pp.\  17--26. PMLR, 2017.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate curvature.
\newblock In \emph{International conference on machine learning}, pp.\  2408--2417. PMLR, 2015.

\bibitem[Parikh \& Boyd(2014)Parikh and Boyd]{parikh2019proximal}
Parikh, N. and Boyd, S.
\newblock Proximal algorithms.
\newblock \emph{Found. Trends Optim.}, 1\penalty0 (3):\penalty0 127–239, jan 2014.
\newblock ISSN 2167-3888.
\newblock URL \url{https://doi.org/10.1561/2400000003}.

\bibitem[Saha \& Roy(2023)Saha and Roy]{saha2023continual}
Saha, G. and Roy, K.
\newblock Continual learning with scaled gradient projection.
\newblock \emph{arXiv preprint arXiv:2302.01386}, 2023.

\bibitem[Saha et~al.(2021{\natexlab{a}})Saha, Garg, Ankit, and Roy]{saha2021space}
Saha, G., Garg, I., Ankit, A., and Roy, K.
\newblock Space: Structured compression and sharing of representational space for continual learning.
\newblock \emph{IEEE Access}, 9:\penalty0 150480--150494, 2021{\natexlab{a}}.

\bibitem[Saha et~al.(2021{\natexlab{b}})Saha, Garg, and Roy]{saha2021gradient}
Saha, G., Garg, I., and Roy, K.
\newblock Gradient projection memory for continual learning.
\newblock \emph{arXiv preprint arXiv:2103.09762}, 2021{\natexlab{b}}.

\bibitem[Shu et~al.(2022)Shu, Li, Cheng, Guo, Leng, Liao, Hu, and Liu]{shu2022replay}
Shu, K., Li, H., Cheng, J., Guo, Q., Leng, L., Liao, J., Hu, Y., and Liu, J.
\newblock Replay-oriented gradient projection memory for continual learning in medical scenarios.
\newblock In \emph{2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, pp.\  1724--1729, 2022.
\newblock \doi{10.1109/BIBM55620.2022.9995580}.

\bibitem[Soutif-Cormerais et~al.(2023)Soutif-Cormerais, Carta, Cossu, Hurtado, Hemati, Lomonaco, and de~Weijer]{soutifcormerais2023comprehensive}
Soutif-Cormerais, A., Carta, A., Cossu, A., Hurtado, J., Hemati, H., Lomonaco, V., and de~Weijer, J.~V.
\newblock A comprehensive empirical evaluation on online continual learning, 2023.

\bibitem[van~de Ven et~al.(2022)van~de Ven, Tuytelaars, and Tolias]{van2022three}
van~de Ven, G.~M., Tuytelaars, T., and Tolias, A.~S.
\newblock Three types of incremental learning, December 2022.
\newblock URL \url{http://dx.doi.org/10.1038/s42256-022-00568-3}.

\bibitem[Zeng et~al.(2019)Zeng, Chen, Cui, and Yu]{zeng2019continual}
Zeng, G., Chen, Y., Cui, B., and Yu, S.
\newblock Continual learning of context-dependent processing in neural networks.
\newblock \emph{Nature Machine Intelligence}, 1\penalty0 (8):\penalty0 364--372, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Pfahringer, Frank, Bifet, Lim, and Jia]{zhang2022simple}
Zhang, Y., Pfahringer, B., Frank, E., Bifet, A., Lim, N. J.~S., and Jia, Y.
\newblock A simple but strong baseline for online continual learning: Repeated augmented rehearsal.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 14771--14783, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Zhang, Tan, Liu, Qu, Xie, and Ma]{zhao2023rethinking}
Zhao, Z., Zhang, Z., Tan, X., Liu, J., Qu, Y., Xie, Y., and Ma, L.
\newblock Rethinking gradient projection continual learning: Stability/plasticity feature space decoupling.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  3718--3727, 2023.

\end{thebibliography}
