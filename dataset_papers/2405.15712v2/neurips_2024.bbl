\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem[Han et~al.(2022)Han, Wang, Chen, Chen, Guo, Liu, Tang, Xiao, Xu, Xu, et~al.]{han2022survey}
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An~Xiao, Chunjing Xu, Yixing Xu, et~al.
\newblock A survey on vision transformer.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 45\penalty0 (1):\penalty0 87--110, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dehghani et~al.(2022)Dehghani, Gritsenko, Arnab, Minderer, and Tay]{dehghani2021scenic}
Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi~Tay.
\newblock Scenic: A jax library for computer vision research and beyond.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 21393--21398, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Yang et~al.(2021)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2021tuning}
Ge~Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17084--17097, 2021.

\bibitem[Bordelon et~al.(2024{\natexlab{a}})Bordelon, Noci, Li, Hanin, and Pehlevan]{bordelon2024depthwise}
Blake Bordelon, Lorenzo Noci, Mufan~Bill Li, Boris Hanin, and Cengiz Pehlevan.
\newblock Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=KZJehvRKGD}.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Yu, Zhu, and Hayou]{yang2023feature}
Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou.
\newblock Feature learning in infinite depth neural networks.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023{\natexlab{a}}.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pages 2388--2464. PMLR, 2019.

\bibitem[Yang and Hu(2021)]{yang2021tensor}
Greg Yang and Edward~J Hu.
\newblock Tensor programs iv: Feature learning in infinite-width neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages 11727--11737. PMLR, 2021.

\bibitem[Bordelon and Pehlevan(2022{\natexlab{a}})]{bordelon2022self}
Blake Bordelon and Cengiz Pehlevan.
\newblock Self-consistent dynamical field theory of kernel evolution in wide neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 32240--32256, 2022{\natexlab{a}}.

\bibitem[Vyas et~al.(2023)Vyas, Atanasov, Bordelon, Morwani, Sainathan, and Pehlevan]{vyas2023featurelearning}
Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan.
\newblock Feature-learning networks are consistent across widths at realistic scales, 2023.

\bibitem[Bordelon and Pehlevan(2023)]{bordelon2023dynamics}
Blake Bordelon and Cengiz Pehlevan.
\newblock Dynamics of finite width kernel and prediction fluctuations in mean field neural networks.
\newblock \emph{arXiv preprint arXiv:2304.03408}, 2023.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl-Dickstein, and Novak]{hron2020infinite}
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak.
\newblock Infinite attention: Nngp and ntk for deep attention networks.
\newblock In \emph{International Conference on Machine Learning}, pages 4376--4386. PMLR, 2020.

\bibitem[Dinan et~al.(2023)Dinan, Yaida, and Zhang]{dinan2023effective}
Emily Dinan, Sho Yaida, and Susan Zhang.
\newblock Effective theory of transformers at initialization, 2023.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly exponentially with depth.
\newblock In \emph{International Conference on Machine Learning}, pages 2793--2803. PMLR, 2021.

\bibitem[Noci et~al.(2022)Noci, Anagnostidis, Biggio, Orvieto, Singh, and Lucchi]{noci2022signal}
Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak~Pal Singh, and Aurelien Lucchi.
\newblock Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27198--27211, 2022.

\bibitem[He and Hofmann(2023)]{he2023simplifying}
Bobby He and Thomas Hofmann.
\newblock Simplifying transformer blocks.
\newblock \emph{arXiv preprint arXiv:2311.01906}, 2023.

\bibitem[Cowsik et~al.(2024)Cowsik, Nebabu, Qi, and Ganguli]{cowsik2024geometric}
Aditya Cowsik, Tamra Nebabu, Xiao-Liang Qi, and Surya Ganguli.
\newblock Geometric dynamics of signal propagation predict trainability of transformers, 2024.

\bibitem[Noci et~al.(2024)Noci, Li, Li, He, Hofmann, Maddison, and Roy]{noci2024shaped}
Lorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris~J Maddison, and Dan Roy.
\newblock The shaped transformer: Attention models in the infinite depth-and-width limit.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Hayou(2023)]{hayou2023on}
Soufiane Hayou.
\newblock On the infinite-depth limit of finite-width neural networks.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=RbLsYz1Az9}.

\bibitem[Cirone et~al.(2023)Cirone, Lemercier, and Salvi]{cirone2023neural}
Nicola~Muca Cirone, Maud Lemercier, and Cristopher Salvi.
\newblock Neural signature kernels as infinite-width-depth-limits of controlled resnets.
\newblock \emph{arXiv preprint arXiv:2303.17671}, 2023.

\bibitem[Chizat and Netrapalli(2024)]{chizat2024featurespeedformulaflexible}
Lénaïc Chizat and Praneeth Netrapalli.
\newblock The feature speed formula: a flexible approach to scale hyper-parameters of deep neural networks, 2024.
\newblock URL \url{https://arxiv.org/abs/2311.18718}.

\bibitem[Bernstein et~al.(2020)Bernstein, Vahdat, Yue, and Liu]{bernstein2020distance}
Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu.
\newblock On the distance between two neural networks and the stability of learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 21370--21381, 2020.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Simon, and Bernstein]{yang2023spectral}
Greg Yang, James~B Simon, and Jeremy Bernstein.
\newblock A spectral condition for feature learning.
\newblock \emph{arXiv preprint arXiv:2310.17813}, 2023{\natexlab{b}}.

\bibitem[Bernstein et~al.(2023)Bernstein, Mingard, Huang, Azizan, and Yue]{bernstein2023automatic}
Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, and Yisong Yue.
\newblock Automatic gradient descent: Deep learning without hyperparameters.
\newblock \emph{arXiv preprint arXiv:2304.05187}, 2023.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Sompolinsky and Zippelius(1981)]{sompolinsky1981dynamic}
Haim Sompolinsky and Annette Zippelius.
\newblock Dynamic theory of the spin-glass phase.
\newblock \emph{Physical Review Letters}, 47\penalty0 (5):\penalty0 359, 1981.

\bibitem[Helias and Dahmen(2020)]{helias2020statistical}
Moritz Helias and David Dahmen.
\newblock \emph{Statistical field theory for neural networks}, volume 970.
\newblock Springer, 2020.

\bibitem[Mannelli et~al.(2019)Mannelli, Krzakala, Urbani, and Zdeborova]{mannelli2019passed}
Stefano~Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova.
\newblock Passed \& spurious: Descent algorithms and local minima in spiked matrix-tensor models.
\newblock In \emph{international conference on machine learning}, pages 4333--4342. PMLR, 2019.

\bibitem[Mignacco et~al.(2020)Mignacco, Krzakala, Urbani, and Zdeborov{\'a}]{mignacco2020dynamical}
Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborov{\'a}.
\newblock Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9540--9550, 2020.

\bibitem[Gerbelot et~al.(2022)Gerbelot, Troiani, Mignacco, Krzakala, and Zdeborova]{gerbelot2022rigorous}
Cedric Gerbelot, Emanuele Troiani, Francesca Mignacco, Florent Krzakala, and Lenka Zdeborova.
\newblock Rigorous dynamical mean field theory for stochastic gradient descent methods.
\newblock \emph{arXiv preprint arXiv:2210.06591}, 2022.

\bibitem[Bordelon et~al.(2024{\natexlab{b}})Bordelon, Atanasov, and Pehlevan]{bordelon2024dynamical}
Blake Bordelon, Alexander Atanasov, and Cengiz Pehlevan.
\newblock A dynamical model of neural scaling laws, 2024{\natexlab{b}}.

\bibitem[Martin et~al.(1973)Martin, Siggia, and Rose]{martin1973statistical}
Paul~Cecil Martin, ED~Siggia, and HA~Rose.
\newblock Statistical dynamics of classical systems.
\newblock \emph{Physical Review A}, 8\penalty0 (1):\penalty0 423, 1973.

\bibitem[Bordelon and Pehlevan(2022{\natexlab{b}})]{bordelon2022influence}
Blake Bordelon and Cengiz Pehlevan.
\newblock The influence of learning rule on representation dynamics in wide neural networks.
\newblock \emph{arXiv preprint arXiv:2210.02157}, 2022{\natexlab{b}}.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Neyshabur, and Sedghi]{nakkiran2020deep}
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi.
\newblock The deep bootstrap framework: Good online learners are good offline generalizers.
\newblock \emph{arXiv preprint arXiv:2010.08127}, 2020.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Alabdulmohsin et~al.(2024)Alabdulmohsin, Zhai, Kolesnikov, and Beyer]{alabdulmohsin2024getting}
Ibrahim~M Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer.
\newblock Getting vit in shape: Scaling laws for compute-optimal model design.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Littwin and Yang(2022)]{littwin2022adaptive}
Etai Littwin and Greg Yang.
\newblock Adaptive optimization in the $\infty$-width limit.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Li et~al.(2022)Li, Nica, and Roy]{li2022neural}
Mufan Li, Mihai Nica, and Dan Roy.
\newblock The neural covariance sde: Shaped infinite depth-and-width networks at initialization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 10795--10808, 2022.

\end{thebibliography}
