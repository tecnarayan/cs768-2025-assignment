\begin{thebibliography}{10}

\bibitem{energy_1}
Francisco~Martinez Alvarez, Alicia Troncoso, Jose~C Riquelme, and Jesus S~Aguilar Ruiz.
\newblock Energy time series forecasting based on pattern sequence similarity.
\newblock {\em IEEE Transactions on Knowledge and Data Engineering}, 23(8):1230--1243, 2010.

\bibitem{energy_2}
Irena Koprinska, Dengsong Wu, and Zheng Wang.
\newblock Convolutional neural networks for energy time series forecasting.
\newblock In {\em 2018 international joint conference on neural networks (IJCNN)}, pages 1--8. IEEE, 2018.

\bibitem{Weather}
Rafal~A Angryk, Petrus~C Martens, Berkay Aydin, Dustin Kempton, Sushant~S Mahajan, Sunitha Basodi, Azim Ahmadzadeh, Xumin Cai, Soukaina Filali~Boubrahimi, Shah~Muhammad Hamdi, et~al.
\newblock Multivariate time series dataset for space weather data analytics.
\newblock {\em Scientific data}, 7(1):1--13, 2020.

\bibitem{Graphcast}
Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet, Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et~al.
\newblock Learning skillful medium-range global weather forecasting.
\newblock {\em Science}, 382(6677):1416--1421, 2023.

\bibitem{traffic_time_series}
Li~Li, Xiaonan Su, Yi~Zhang, Yuetong Lin, and Zhiheng Li.
\newblock Trend modeling for traffic time series analysis: An integrated study.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems}, 16(6):3430--3439, 2015.

\bibitem{traffice_forecasting}
Yi~Yin and Pengjian Shang.
\newblock Forecasting traffic time series with multivariate predicting method.
\newblock {\em Applied Mathematics and Computation}, 291:266--278, 2016.

\bibitem{chen2023tsmixer}
Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan~O Arik, and Tomas Pfister.
\newblock Tsmixer: An all-mlp architecture for time series forecasting.
\newblock {\em arXiv preprint arXiv:2303.06053}, 2023.

\bibitem{LinearModel}
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.
\newblock Are transformers effective for time series forecasting?
\newblock {\em arXiv preprint arXiv:2205.13504}, 2022.

\bibitem{transformers-survey}
Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun.
\newblock Transformers in time series: A survey.
\newblock {\em arXiv preprint arXiv:2202.07125}, 2022.

\bibitem{Fedformer}
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
\newblock Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock {\em arXiv preprint arXiv:2201.12740}, 2022.

\bibitem{Informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 11106--11115, 2021.

\bibitem{Autoformer}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock {\em Advances in Neural Information Processing Systems}, 34:22419--22430, 2021.

\bibitem{TimeNet}
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock {\em arXiv preprint arXiv:2210.02186}, 2022.

\bibitem{liu2022scinet}
Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu.
\newblock Scinet: Time series modeling and forecasting with sample convolution and interaction.
\newblock {\em Advances in Neural Information Processing Systems}, 35:5816--5828, 2022.

\bibitem{li2023revisiting}
Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu.
\newblock Revisiting long-term time series forecasting: An investigation on linear mapping.
\newblock {\em arXiv preprint arXiv:2305.10721}, 2023.

\bibitem{is_channel}
Yuan Peiwen and Zhu Changsheng.
\newblock Is channel independent strategy optimal for time series forecasting?
\newblock {\em arXiv preprint arXiv:2310.17658}, 2023.

\bibitem{CICD}
Lu~Han, Han-Jia Ye, and De-Chuan Zhan.
\newblock The capacity and robustness trade-off: Revisiting the channel independent strategy for multivariate time series forecasting.
\newblock {\em arXiv preprint arXiv:2304.05206}, 2023.

\bibitem{ni2023forecasting}
Jian Ni and Yue Xu.
\newblock Forecasting the dynamic correlation of stock indices based on deep learning method.
\newblock {\em Computational Economics}, 61(1):35--55, 2023.

\bibitem{yin2021forecasting}
Xingkun Yin, Da~Yan, Abdullateef Almudaifer, Sibo Yan, and Yang Zhou.
\newblock Forecasting stock prices using stock correlation graph: A graph convolutional network approach.
\newblock In {\em 2021 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8. IEEE, 2021.

\bibitem{jang2023spatial}
Phillip~A Jang and David~S Matteson.
\newblock Spatial correlation in weather forecast accuracy: a functional time series approach.
\newblock {\em Computational Statistics}, pages 1--15, 2023.

\bibitem{patchtst}
Yuqi Nie, Nam~H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock {\em arXiv preprint arXiv:2211.14730}, 2022.

\bibitem{prophet}
Sean~J Taylor and Benjamin Letham.
\newblock Forecasting at scale.
\newblock {\em The American Statistician}, 72(1):37--45, 2018.

\bibitem{triebe2021neuralprophet}
Oskar Triebe, Hansika Hewamalage, Polina Pilyugina, Nikolay Laptev, Christoph Bergmeir, and Ram Rajagopal.
\newblock Neuralprophet: Explainable forecasting at scale, 2021.

\bibitem{arima}
G~Peter Zhang.
\newblock Time series forecasting using a hybrid arima and neural network model.
\newblock {\em Neurocomputing}, 50:159--175, 2003.

\bibitem{ahmed2010empirical}
Nesreen~K Ahmed, Amir~F Atiya, Neamat~El Gayar, and Hisham El-Shishiny.
\newblock An empirical comparison of machine learning models for time series forecasting.
\newblock {\em Econometric reviews}, 29(5-6):594--621, 2010.

\bibitem{torres2021deep}
Jos{\'e}~F Torres, Dalil Hadjout, Abderrazak Sebaa, Francisco Mart{\'\i}nez-{\'A}lvarez, and Alicia Troncoso.
\newblock Deep learning for time series forecasting: a survey.
\newblock {\em Big Data}, 9(1):3--21, 2021.

\bibitem{lim2021time}
Bryan Lim and Stefan Zohren.
\newblock Time-series forecasting with deep learning: a survey.
\newblock {\em Philosophical Transactions of the Royal Society A}, 379(2194):20200209, 2021.

\bibitem{Tcn}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock {\em arXiv preprint arXiv:1803.01271}, 2018.

\bibitem{convolutional_survey}
Renzhuo Wan, Shuping Mei, Jun Wang, Min Liu, and Fan Yang.
\newblock Multivariate temporal convolutional network: A deep neural networks approach for multivariate time series forecasting.
\newblock {\em Electronics}, 8(8):876, 2019.

\bibitem{sen2019think}
Rajat Sen, Hsiang-Fu Yu, and Inderjit~S Dhillon.
\newblock Think globally, act locally: A deep neural network approach to high-dimensional time series forecasting.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{hewamalage2021recurrent}
Hansika Hewamalage, Christoph Bergmeir, and Kasun Bandara.
\newblock Recurrent neural networks for time series forecasting: Current status and future directions.
\newblock {\em International Journal of Forecasting}, 37(1):388--427, 2021.

\bibitem{rangapuram2018deep}
Syama~Sundar Rangapuram, Matthias~W Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski.
\newblock Deep state space models for time series forecasting.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{exponential_smmooth_rnn}
Slawek Smyl.
\newblock A hybrid method of exponential smoothing and recurrent neural networks for time series forecasting.
\newblock {\em International Journal of Forecasting}, 36(1):75--85, 2020.

\bibitem{Deepar}
David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent networks.
\newblock {\em International Journal of Forecasting}, 36(3):1181--1191, 2020.

\bibitem{liu2022non}
Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long.
\newblock Non-stationary transformers: Rethinking the stationarity in time series forecasting.
\newblock {\em arXiv preprint arXiv:2205.14415}, 2022.

\bibitem{zhang2022crossformer}
Yunhao Zhang and Junchi Yan.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{itransformer}
Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock {\em arXiv preprint arXiv:2310.06625}, 2023.

\bibitem{probabilistic_transformer}
Binh Tang and David~S Matteson.
\newblock Probabilistic transformer for time series analysis.
\newblock {\em Advances in Neural Information Processing Systems}, 34:23592--23608, 2021.

\bibitem{Pyraformer}
Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex~X Liu, and Schahram Dustdar.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{feng2024efficient}
Aosong Feng, Jialin Chen, Juan Garza, Brooklyn Berry, Francisco Salazar, Yifeng Gao, Rex Ying, and Leandros Tassiulas.
\newblock Efficient high-resolution time series classification via attention kronecker decomposition.
\newblock {\em arXiv preprint arXiv:2403.04882}, 2024.

\bibitem{Attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{nbeats}
Boris~N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio.
\newblock N-beats: Neural basis expansion analysis for interpretable time series forecasting.
\newblock {\em arXiv preprint arXiv:1905.10437}, 2019.

\bibitem{zhang2022less}
Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li.
\newblock Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures.
\newblock {\em arXiv preprint arXiv:2207.01186}, 2022.

\bibitem{das2023long}
Abhimanyu Das, Weihao Kong, Andrew Leach, Rajat Sen, and Rose Yu.
\newblock Long-term forecasting with tide: Time-series dense encoder.
\newblock {\em arXiv preprint arXiv:2304.08424}, 2023.

\bibitem{STL}
Robert~B Cleveland, William~S Cleveland, Jean~E McRae, and Irma Terpenning.
\newblock Stl: A seasonal-trend decomposition.
\newblock {\em J. Off. Stat}, 6(1):3--73, 1990.

\bibitem{RobustSTL}
Qingsong Wen, Zhe Zhang, Yan Li, and Liang Sun.
\newblock Fast robuststl: Efficient and robust seasonal-trend decomposition for time series with complex patterns.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 2203--2213, 2020.

\bibitem{benaouda2006wavelet}
Djamel Benaouda, Fionn Murtagh, J-L Starck, and Olivier Renaud.
\newblock Wavelet-based nonlinear multiscale decomposition model for electricity load forecasting.
\newblock {\em Neurocomputing}, 70(1-3):139--154, 2006.

\bibitem{percival2000wavelet}
Donald~B Percival and Andrew~T Walden.
\newblock {\em Wavelet methods for time series analysis}, volume~4.
\newblock Cambridge university press, 2000.

\bibitem{wang2022micn}
Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao.
\newblock Micn: Multi-scale local and global context modeling for long-term series forecasting.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{zhang2024multi}
Yifan Zhang, Rui Wu, Sergiu~M Dascalu, and Frederick~C Harris.
\newblock Multi-scale transformer pyramid networks for multivariate time series forecasting.
\newblock {\em IEEE Access}, 2024.

\bibitem{yi2023frequency}
Kun Yi, Qi~Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Defu Lian, Ning An, Longbing Cao, and Zhendong Niu.
\newblock Frequency-domain mlps are more effective learners in time series forecasting.
\newblock {\em arXiv preprint arXiv:2311.06184}, 2023.

\bibitem{zhou2022film}
Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin, et~al.
\newblock Film: Frequency improved legendre memory model for long-term time series forecasting.
\newblock {\em Advances in Neural Information Processing Systems}, 35:12677--12690, 2022.

\bibitem{montero2021principles}
Pablo Montero-Manso and Rob~J Hyndman.
\newblock Principles and algorithms for forecasting groups of time series: Locality and globality.
\newblock {\em International Journal of Forecasting}, 37(4):1632--1653, 2021.

\bibitem{wang2023card}
Xue Wang, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, and Rong Jin.
\newblock Card: Channel aligned robust blend transformer for time series forecasting.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{jiang2010fuzzy}
Jung-Yi Jiang, Ren-Jia Liou, and Shie-Jue Lee.
\newblock A fuzzy self-constructing feature clustering algorithm for text classification.
\newblock {\em IEEE transactions on knowledge and data engineering}, 23(3):335--349, 2010.

\bibitem{marin2023token}
Dmitrii Marin, Jen-Hao~Rick Chang, Anurag Ranjan, Anish Prabhu, Mohammad Rastegari, and Oncel Tuzel.
\newblock Token pooling in vision transformers for image classification.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 12--21, 2023.

\bibitem{george2023integrated}
Lijimol George and P~Sumathy.
\newblock An integrated clustering and bert framework for improved topic modeling.
\newblock {\em International Journal of Information Technology}, pages 1--9, 2023.

\bibitem{li2012unsupervised}
Hao Li, Alin Achim, and D~Bull.
\newblock Unsupervised video anomaly detection using feature clustering.
\newblock {\em IET signal processing}, 6(5):521--533, 2012.

\bibitem{syarif2012unsupervised}
Iwan Syarif, Adam Prugel-Bennett, and Gary Wills.
\newblock Unsupervised clustering approach for network anomaly detection.
\newblock In {\em Networked Digital Technologies: 4th International Conference, NDT 2012, Dubai, UAE, April 24-26, 2012. Proceedings, Part I 4}, pages 135--145. Springer, 2012.

\bibitem{gunupudi2017clapp}
Rajesh~Kumar Gunupudi, Mangathayaru Nimmala, Narsimha Gugulothu, and Suresh~Reddy Gali.
\newblock Clapp: A self constructing feature clustering approach for anomaly detection.
\newblock {\em Future Generation Computer Systems}, 74:417--429, 2017.

\bibitem{ji2023spatio}
Jiahao Ji, Jingyuan Wang, Chao Huang, Junjie Wu, Boren Xu, Zhenhe Wu, Junbo Zhang, and Yu~Zheng.
\newblock Spatio-temporal self-supervised learning for traffic flow prediction.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~37, pages 4356--4364, 2023.

\bibitem{liu2023self}
Gang Liu, Silu He, Xing Han, Qinyao Luo, Ronghua Du, Xinsha Fu, and Ling Zhao.
\newblock Self-supervised spatiotemporal masking strategy-based models for traffic flow forecasting.
\newblock {\em Symmetry}, 15(11):2002, 2023.

\bibitem{li2023mts}
Zhe Li, Zhongwen Rao, Lujia Pan, and Zenglin Xu.
\newblock Mts-mixers: Multivariate time series forecasting via factorized temporal and channel mixing.
\newblock {\em arXiv preprint arXiv:2302.04501}, 2023.

\bibitem{RevIN}
Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.
\newblock Reversible instance normalization for accurate time-series forecasting against distribution shift.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{SAN}
Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi~Liu, Yanhu Xie, and Enhong Chen.
\newblock Adaptive normalization for non-stationary time series forecasting: A temporal slice perspective.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{zhang2004compactly}
Hao~Helen Zhang, Mark~G Genton, and Peng Liu.
\newblock Compactly supported radial basis function kernels.
\newblock Technical report, North Carolina State University. Dept. of Statistics, 2004.

\bibitem{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock {\em arXiv preprint arXiv:1611.01144}, 2016.

\bibitem{Reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{shen2021efficient}
Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li.
\newblock Efficient attention: Attention with linear complexities.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pages 3531--3539, 2021.

\bibitem{exchange}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In {\em The 41st international ACM SIGIR conference on research \& development in information retrieval}, pages 95--104, 2018.

\bibitem{makridakis2018m4}
Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos.
\newblock The m4 competition: Results, findings, conclusion and way forward.
\newblock {\em International Journal of Forecasting}, 34(4):802--808, 2018.

\bibitem{jin2023time}
Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James~Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et~al.
\newblock Time-llm: Time series forecasting by reprogramming large language models.
\newblock {\em arXiv preprint arXiv:2310.01728}, 2023.

\bibitem{chen2004marriage}
Lei Chen and Raymond Ng.
\newblock On the marriage of lp-norms and edit distance.
\newblock In {\em Proceedings of the Thirtieth international conference on Very large data bases-Volume 30}, pages 792--803, 2004.

\bibitem{ding2008querying}
Hui Ding, Goce Trajcevski, Peter Scheuermann, Xiaoyue Wang, and Eamonn Keogh.
\newblock Querying and mining of time series data: experimental comparison of representations and distance measures.
\newblock {\em Proceedings of the VLDB Endowment}, 1(2):1542--1552, 2008.

\bibitem{gold2018dynamic}
Omer Gold and Micha Sharir.
\newblock Dynamic time warping and geometric edit distance: Breaking the quadratic barrier.
\newblock {\em ACM Transactions on Algorithms (TALG)}, 14(4):1--17, 2018.

\bibitem{kljun2020review}
Ma{\v{s}}a Kljun and M~Tersˇek.
\newblock A review and comparison of time series similarity measures.
\newblock In {\em 29th International Electrotechnical and Computer Science Conference (ERK 2020). Portorozˇ}, pages 21--22, 2020.

\bibitem{li2023mlinear}
Wei Li, Xiangxu Meng, Chuhao Chen, and Jianing Chen.
\newblock Mlinear: Rethink the linear model for time-series forecasting.
\newblock {\em arXiv preprint arXiv:2305.04800}, 2023.

\bibitem{electricity_dataset}
Artur Trindade.
\newblock {ElectricityLoadDiagrams20112014}.
\newblock UCI Machine Learning Repository, 2015.
\newblock {DOI}: https://doi.org/10.24432/C58C86.

\bibitem{exchange_dataset}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks. corr abs/1703.07015 (2017).
\newblock {\em arXiv preprint arXiv:1703.07015}, 2017.

\bibitem{m42018m4}
M4~Team et~al.
\newblock M4 competitor’s guide: prizes and rules.
\newblock {\em See https://www. m4. unic. ac. cy/wpcontent/uploads/2018/03/M4-CompetitorsGuide. pdf}, 2018.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
