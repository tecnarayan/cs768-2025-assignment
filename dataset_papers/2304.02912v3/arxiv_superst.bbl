\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adler et~al.(1998)Adler, Feldman, and Taqqu]{Adler1998}
R.J. Adler, R.E. Feldman, and M.S. Taqqu, editors.
\newblock \emph{A Practical Guide to Heavy Tails: Statistical Techniques and
  Applications}.
\newblock Birkhauser Boston Inc., USA, 1998.
\newblock ISBN 0817639519.

\bibitem[Alspach and Sorenson(1972)]{Alspach}
D.~Alspach and H.~Sorenson.
\newblock {Nonlinear Bayesian estimation using Gaussian sum approximations}.
\newblock \emph{IEEE Trans. Autom. Control}, 17\penalty0 (4):\penalty0
  439--448, 1972.

\bibitem[Baldassi et~al.(2020)Baldassi, Malatesta, Negri, and
  Zecchina]{Baldassi2020}
C.~Baldassi, E.M. Malatesta, M.~Negri, and R.~Zecchina.
\newblock Wide flat minima and optimal generalization in classifying
  high-dimensional gaussian mixtures.
\newblock \emph{J. Stat. Mech.: Theory Exp.}, 2020\penalty0 (12):\penalty0
  124012, 2020.

\bibitem[Bayati and Montanari(2011)]{bayati2011dynamics}
M.~Bayati and A.~Montanari.
\newblock The dynamics of message passing on dense graphs, with applications to
  compressed sensing.
\newblock \emph{IEEE Trans. Inf. Theory}, 57\penalty0 (2):\penalty0 764--785,
  2011.

\bibitem[Beck(2003)]{Beck2003}
C.~Beck.
\newblock Superstatistics: Theory and applications.
\newblock \emph{Contin. Mech. Thermodyn.}, 16, 2003.

\bibitem[Beck(2008)]{Beck2008RecentDI}
C.~Beck.
\newblock Recent developments in superstatistics.
\newblock \emph{Braz. J. Phys.}, 39:\penalty0 357--363, 2008.

\bibitem[Beck and Cohen(2003)]{Beck2003b}
C.~Beck and E.G.D. Cohen.
\newblock Superstatistics.
\newblock \emph{Physica A}, 322:\penalty0 267--275, 2003.

\bibitem[Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, Ma, and
  Mandal]{Belkin2019}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias–variance trade-off.
\newblock \emph{Proc. Natl. Acad. Sci. U.S.A.}, 116\penalty0 (32):\penalty0
  15849--15854, 2019{\natexlab{a}}.

\bibitem[Belkin et~al.(2019{\natexlab{b}})Belkin, Rakhlin, and
  Tsybakov]{pmlr-v89-belkin19a}
M.~Belkin, A.~Rakhlin, and A.B. Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock In K.~Chaudhuri and M.~Sugiyama, editors, \emph{Proceedings of the
  22nd International Conference on Artificial Intelligence and Statistics},
  volume~89 of \emph{Proc. Mach. Learn. Res.}, pages 1611--1619. PMLR,
  2019{\natexlab{b}}.

\bibitem[Berthier et~al.(2020)Berthier, Montanari, and
  Nguyen]{berthier2020state}
R.~Berthier, A.~Montanari, and P.-M. Nguyen.
\newblock State evolution for approximate message passing with non-separable
  functions.
\newblock \emph{Inf. Inference}, 9\penalty0 (1):\penalty0 33--79, 2020.

\bibitem[Brunel et~al.(1992)Brunel, Nadal, and Toulouse]{Brunel1992}
N.~Brunel, J.-P. Nadal, and G.~Toulouse.
\newblock Information capacity of a perceptron.
\newblock \emph{JJ. Phys. A: Math. Gen.}, 25\penalty0 (19):\penalty0 5017,
  1992.

\bibitem[Cover(1965)]{Cover1965}
T.M. Cover.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock \emph{IEEE Trans. Comput.}, EC-14\penalty0 (3):\penalty0 326--334,
  1965.

\bibitem[Dandi et~al.(2023)Dandi, Stephan, Krzakala, Loureiro, and
  Zdeborov\'a']{dandi2023}
Y.~Dandi, L.~Stephan, F.~Krzakala, B.~Loureiro, and L.~Zdeborov\'a'.
\newblock Universality laws for gaussian mixtures in generalized linear models.
\newblock \emph{arXiv:2302.08933}, 2023.

\bibitem[del Giudice et~al.(1989)del Giudice, Franz, and
  Virasoro]{percep_repl_89}
P.~del Giudice, S.~Franz, and M.~Virasoro.
\newblock Perceptron beyond the limit of capacity.
\newblock \emph{J. Physique}, 50:\penalty0 121--134, 1989.

\bibitem[Delpini and Bormetti(2011)]{Delpini2011}
D.~Delpini and G.~Bormetti.
\newblock Minimal model of financial stylized facts.
\newblock \emph{Phys. Rev. E}, 83:\penalty0 041111, 2011.

\bibitem[Deng and Thrampoulidis(2021)]{Deng2021}
A.~Deng, Z.and~Kammoun and C.~Thrampoulidis.
\newblock A model of double descent for high-dimensional binary linear
  classification.
\newblock \emph{Inf. Inference}, 11, 2021.

\bibitem[Donoho et~al.(2009)Donoho, Maleki, and Montanari]{Donoho2009}
D.L. Donoho, A.~Maleki, and A.~Montanari.
\newblock Message-passing algorithms for compressed sensing.
\newblock \emph{Proc. Natl. Acad. Sci. U.S.A.}, 106\penalty0 (45):\penalty0
  18914–18919, Nov 2009.

\bibitem[El~Karoui(2018)]{ElKaroui2018}
N.~El~Karoui.
\newblock On the impact of predictor geometry on the performance on
  high-dimensional ridge-regularized generalized robust regression estimators.
\newblock \emph{Probab. Theory Relat. Fields}, 170, 2018.

\bibitem[Franz et~al.(1990)Franz, Amit, and Virasoro]{Prosopagnosia90}
S.~Franz, D.J. Amit, and M.A. Virasoro.
\newblock {Prosopagnosia in high capacity neural networks storing uncorrelated
  classes}.
\newblock \emph{{J. Physique}}, 51\penalty0 (5):\penalty0 387--408, 1990.

\bibitem[Gardner(1988)]{Gardner_1988_alpha2}
E~Gardner.
\newblock The space of interactions in neural network models.
\newblock \emph{J. Phys. A: Math. Gen.}, 21\penalty0 (1):\penalty0 257, 1988.

\bibitem[Gardner and Derrida(1989)]{Gardner_1989_unfinished}
E.~Gardner and B.~Derrida.
\newblock Three unfinished works on the optimal storage capacity of networks.
\newblock \emph{J. Phys. A: Math. Gen.}, 22\penalty0 (12):\penalty0 1983, 1989.

\bibitem[Gelman and Hill(2006)]{gelman_hill_2006}
A.~Gelman and J.1/ Hill.
\newblock \emph{Data Analysis Using Regression and Multilevel/Hierarchical
  Models}.
\newblock Analytical Methods for Social Research. Cambridge University Press,
  2006.

\bibitem[Gelman et~al.(2013)Gelman, Carlin, Stern, Dunson, Vehtari, and
  Rubin]{gelman2013bayesian}
A.~Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A.~Vehtari, and D.B. Rubin.
\newblock \emph{Bayesian Data Analysis, Third Edition}.
\newblock Chapman \& Hall/CRC Texts in Statistical Science. Taylor \& Francis,
  2013.

\bibitem[Gerace et~al.(2020)Gerace, Loureiro, Krzakala, M{\'e}zard, and
  Zdeborov{\'a}]{gerace2020}
F.~Gerace, B.~Loureiro, F.~Krzakala, M.~M{\'e}zard, and L.~Zdeborov{\'a}.
\newblock Generalisation error in learning with random features and the hidden
  manifold model.
\newblock In \emph{ICML}, pages 3452--3462, 2020.

\bibitem[Gerace et~al.(2023)Gerace, Krzakala, Loureiro, Stephan, and
  Zdeborov\'a]{gerace2023}
F.~Gerace, F.~Krzakala, B.~Loureiro, L.~Stephan, and L.~Zdeborov\'a.
\newblock Gaussian universality of perceptrons with random labels.
\newblock \emph{arXiv:2205.13303}, 2023.

\bibitem[Gerbelot and Berthier(2021)]{gerbelot2021graph}
C.~Gerbelot and R.~Berthier.
\newblock Graph-based approximate message passing iterations.
\newblock \emph{arXiv preprint arXiv:2109.11905}, 2021.

\bibitem[Gerbelot et~al.(2023)Gerbelot, Abbara, and Krzakala]{Gerbelot_2023}
C.~Gerbelot, A.~Abbara, and F.~Krzakala.
\newblock Asymptotic errors for teacher-student convex generalized linear
  models (or: How to prove kabashima's replica formula).
\newblock \emph{IEEE Trans. Inf. Theory}, 69\penalty0 (3):\penalty0 1824--1852,
  2023.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{2Layer2019}
B.~Ghorbani, S.~Mei, T.~Misiakiewicz, and A.~Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32, 2019.

\bibitem[Ghosh and Ramamoorthi(2006)]{ghosh2006bayesian}
J.K. Ghosh and R.V. Ramamoorthi.
\newblock \emph{Bayesian Nonparametrics}.
\newblock Springer Series in Statistics. Springer New York, 2006.

\bibitem[Goldt et~al.(2020)Goldt, M\'ezard, Krzakala, and
  Zdeborov\'a]{Goldt2020}
S.~Goldt, M.~M\'ezard, F.~Krzakala, and L.~Zdeborov\'a.
\newblock Modeling the influence of data structure on learning in neural
  networks: The hidden manifold model.
\newblock \emph{Phys. Rev. X}, 10:\penalty0 041044, 2020.

\bibitem[Gordon(1985)]{Gordon1985}
Y.~Gordon.
\newblock Some inequalities for gaussian processes and applications.
\newblock \emph{Isr. J. Math.}, 50:\penalty0 265--289, 1985.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and Friedman]{hastie2009}
T.~Hastie, R.~Tibshirani, and J.H. Friedman.
\newblock \emph{The elements of statistical learning: data mining, inference,
  and prediction}, volume~2.
\newblock Springer, 2009.

\bibitem[Hastie et~al.(2022)Hastie, Montanari, Rosset, and
  Tibshirani]{Surprises2019}
T.~Hastie, A.~Montanari, S.~Rosset, and R.J. Tibshirani.
\newblock {Surprises in high-dimensional ridgeless least squares
  interpolation}.
\newblock \emph{Ann. Stat.}, 50\penalty0 (2):\penalty0 949 -- 986, 2022.

\bibitem[Hu and Lu(2022)]{Hu2022}
H.~Hu and Y.M. Lu.
\newblock Universality laws for high-dimensional learning with random features.
\newblock \emph{arXiv:2009.07669}, 2022.

\bibitem[Huber(1964)]{Huber1964}
P.J. Huber.
\newblock Robust estimation of a location parameter.
\newblock \emph{Ann. Math. Stat.}, 35\penalty0 (1):\penalty0 73–101, 1964.

\bibitem[Javanmard and Montanari(2013)]{javanmard2013state}
A.~Javanmard and A.~Montanari.
\newblock State evolution for general approximate message passing algorithms,
  with applications to spatial coupling.
\newblock \emph{Inf. Inference}, 2\penalty0 (2):\penalty0 115--144, 2013.

\bibitem[Kini and Thrampoulidis(2021)]{KiniTh2021}
G.R. Kini and C.~Thrampoulidis.
\newblock Phase transitions for one-vs-one and one-vs-all linear separability
  in multiclass gaussian mixtures.
\newblock In \emph{ICASSP 2021}, pages 4020--4024, 2021.

\bibitem[Kothapalli et~al.(2022)Kothapalli, Rasromani, and
  Awatramani]{neur_coll_review_NYU_2022}
V.~Kothapalli, E.~Rasromani, and V.~Awatramani.
\newblock Neural collapse: A review on modelling principles and generalization.
\newblock \emph{arXiv:2206.04041}, 2022.

\bibitem[Krauth and M{\'e}zard(1989)]{KrauthMezard89_cap}
W.~Krauth and M.~M{\'e}zard.
\newblock {Storage capacity of memory networks with binary couplings}.
\newblock \emph{{J. Physique}}, 50\penalty0 (20):\penalty0 3057--3066, 1989.

\bibitem[Langren\'e et~al.(2015)Langren\'e, Lee, and Zhu]{Langrene_2015}
N.~Langren\'e, G.~Lee, and Z.~Zhu.
\newblock Switching to non-affine stochastic volatility: A closed-form
  expansion for the inverse gamma model.
\newblock \emph{Int. J. Theor. Appl. Finance}, 19, 2015.

\bibitem[Lelarge and Miolane(2019{\natexlab{a}})]{Lelarge2019}
M.~Lelarge and L.~Miolane.
\newblock Asymptotic bayes risk for gaussian mixture in a semi-supervised
  setting.
\newblock In \emph{2019 IEEE 8th International Workshop on Computational
  Advances in Multi-Sensor Adaptive Processing}, pages 639--643,
  2019{\natexlab{a}}.

\bibitem[Lelarge and Miolane(2019{\natexlab{b}})]{LelargeMiolane2019}
M.~Lelarge and L.~Miolane.
\newblock Asymptotic bayes risk for gaussian mixture in a semi-supervised
  setting.
\newblock In \emph{2019 IEEE 8th International Workshop on Computational
  Advances in Multi-Sensor Adaptive Processing (CAMSAP)}, pages 639--643,
  2019{\natexlab{b}}.

\bibitem[Loureiro et~al.(2021)Loureiro, Sicuro, Gerbelot, Pacco, Krzakala, and
  Zdeborov{\'a}]{loureiro2021}
B.~Loureiro, G.~Sicuro, C.~Gerbelot, A.~Pacco, F.~Krzakala, and
  L.~Zdeborov{\'a}.
\newblock {Learning Gaussian Mixtures with Generalized Linear Models: Precise
  Asymptotics in High-dimensions}.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10144--10157, 2021.

\bibitem[Loureiro et~al.(2022)Loureiro, Gerbelot, Cui, Goldt, Krzakala,
  Mézard, and Zdeborov\'a]{Loureiro_2022}
B.~Loureiro, C.~Gerbelot, H.~Cui, S.~Goldt, F.~Krzakala, M.~Mézard, and
  L.~Zdeborov\'a.
\newblock {Learning curves of generic features maps for realistic datasets with
  a teacher-student model}.
\newblock \emph{J. Stat. Mech.: Theory Exp.}, 2022\penalty0 (11):\penalty0
  114001, 2022.

\bibitem[Maennel et~al.(2020)Maennel, Alabdulmohsin, Tolstikhin, Baldock,
  Bousquet, Gelly, and Keysers]{Maennel2020}
H.~Maennel, I.M. Alabdulmohsin, I.O. Tolstikhin, R.~Baldock, O.~Bousquet,
  S.~Gelly, and D.~Keysers.
\newblock What do neural networks learn when trained with random labels?
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 19693--19704. Curran Associates, Inc., 2020.

\bibitem[Mai and Liao(2020)]{squarebest_mai2020}
X.~Mai and Z.~Liao.
\newblock High dimensional classification via regularized and unregularized
  empirical risk minimization: Precise error and optimal loss.
\newblock \emph{arXiv:1905.13742}, 2020.

\bibitem[Mei and Montanari(2019)]{MeiMontanari2019}
S.~Mei and A.~Montanari.
\newblock {The Generalization Error of Random Features Regression: Precise
  Asymptotics and the Double Descent Curve}.
\newblock \emph{Commun. Pure Appl. Math.}, 75, 2019.

\bibitem[M{\'e}zard et~al.(1987)M{\'e}zard, Parisi, and
  Virasoro]{mezard1987spin}
M.~M{\'e}zard, G.~Parisi, and M.A. Virasoro.
\newblock \emph{Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem[Mignacco et~al.(2020)Mignacco, Krzakala, Lu, Urbani, and
  Zdeborov\'a]{mignacco20a}
F.~Mignacco, F.~Krzakala, Y.~Lu, P.~Urbani, and L.~Zdeborov\'a.
\newblock The role of regularization in classification of high-dimensional
  noisy {G}aussian mixture.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of \emph{Proc.
  Mach. Learn. Res.}, pages 6874--6883. PMLR, 2020.

\bibitem[Montanari and Nguyen(2017)]{montanari2017}
A.~Montanari and P.-M. Nguyen.
\newblock Universality of the elastic net error.
\newblock In \emph{2017 IEEE ISIT}, pages 2338--2342. IEEE, 2017.

\bibitem[Montanari and Saeed(2022)]{Montanari2022}
A.~Montanari and B.N. Saeed.
\newblock Universality of empirical risk minimization.
\newblock In Po-Ling Loh and Maxim Raginsky, editors, \emph{Proceedings of 35th
  Conference on Learning Theory}, volume 178 of \emph{Proceedings of Machine
  Learning Research}, pages 4310--4312. PMLR, 2022.

\bibitem[Nelson(1990)]{Nelson1990}
D.B. Nelson.
\newblock Arch models as diffusion approximations.
\newblock \emph{J. Econom.}, 45\penalty0 (1-2):\penalty0 7--38, 1990.

\bibitem[Nestoridis et~al.(2011)Nestoridis, Schmutzhard, and
  Stefanopoulos]{Nestodoris}
V.~Nestoridis, S.~Schmutzhard, and V.~Stefanopoulos.
\newblock Universal series induced by approximate identities and some relevant
  applications.
\newblock \emph{J. Approx. Theory}, 163\penalty0 (12):\penalty0 1783--1797,
  2011.

\bibitem[Opper et~al.(1990)Opper, Kinzel, Kleinz, and Nehl]{Opper1990}
M.~Opper, W.~Kinzel, J.~Kleinz, and R.~Nehl.
\newblock On the ability of the optimal perceptron to generalise.
\newblock \emph{J. Phys. A: Math. Gen.}, 23\penalty0 (11):\penalty0 L581, 1990.

\bibitem[Panahi and Hassibi(2017)]{NIPS2017_136f9513}
A.~Panahi and B.~Hassibi.
\newblock A universal analysis of large-scale regularized least squares
  solutions.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{Papyan2020}
V.~Papyan, X.Y. Han, and D.L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proc. Natl. Acad. Sci. U.S.A.}, 117\penalty0 (40):\penalty0
  24652--24663, 2020.

\bibitem[Parikh and Boyd(2014)]{Parikh14prox}
N.~Parikh and S.~Boyd.
\newblock Proximal algorithms.
\newblock \emph{Found. Trends Optim.}, 1\penalty0 (3):\penalty0 127–239,
  2014.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{J. Mach. Learn. Res.}, 12:\penalty0 2825--2830, 2011.

\bibitem[Pesce et~al.(2023)Pesce, Krzakala, Loureiro, and Stephan]{pesce2023}
L.~Pesce, F.~Krzakala, B.~Loureiro, and L.~Stephan.
\newblock {Are Gaussian data all you need? Extents and limits of universality
  in high-dimensional generalized linear estimation}.
\newblock \emph{arXiv:2302.08923}, 2023.

\bibitem[Pinsky and Karlin(2010)]{Pinsky_stochintro}
{M.A.} Pinsky and S.~Karlin.
\newblock \emph{An Introduction to Stochastic Modeling: Fourth Edition}, pages
  1--563.
\newblock Elsevier Inc, December 2010.
\newblock ISBN 9780123814166.

\bibitem[Rahimi and Recht(2007)]{Rahimi2007}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~20. Curran
  Associates, Inc., 2007.

\bibitem[Reed and Hughes(2002)]{Reed2002}
W.J. Reed and B.D. Hughes.
\newblock From gene families and genera to incomes and internet file sizes: Why
  power laws are so common in nature.
\newblock \emph{Phys. Rev. E}, 66:\penalty0 067103, 2002.

\bibitem[Robbins(1985)]{Robbins1985_compound}
H.E. Robbins.
\newblock Asymptotically subminimax solutions of compound statistical decision
  problems.
\newblock 1985.

\bibitem[Rosset et~al.(2003)Rosset, Zhu, and Hastie]{rosset2003margin}
S.~Rosset, J.~Zhu, and T.~Hastie.
\newblock Margin maximizing loss functions.
\newblock In S.~Thrun, L.~Saul, and B.~Sch\"{o}lkopf, editors, \emph{Advances
  in Neural Information Processing Systems}, volume~16. MIT Press, 2003.

\bibitem[Rossi(2014)]{Rossi2014}
P.E. Rossi.
\newblock \emph{Bayesian Non- and Semi-parametric Methods and Applications}.
\newblock Princeton University Press, 2014.

\bibitem[Schnoerr et~al.(2016)Schnoerr, Grima, and
  Sanguinetti]{Schnoerr_coxprocess}
D.~Schnoerr, R.~Grima, and G.~Sanguinetti.
\newblock Cox process representation and inference for stochastic
  reaction-diffusion processes.
\newblock \emph{Nat. Comm.}, 7:\penalty0 11729, 05 2016.

\bibitem[Seddik et~al.(2020)Seddik, Louart, Tamaazousti, and
  Couillet]{Gaussmix_GAN_Couillet2020}
M.E.A. Seddik, C.~Louart, M.~Tamaazousti, and R.~Couillet.
\newblock Random matrix theory proves that deep learning representations of
  {GAN}-data behave as {G}aussian mixtures.
\newblock In H.D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of \emph{Proc.
  Mach. Learn. Res.}, pages 8573--8582. PMLR, 2020.

\bibitem[Seung et~al.(1992)Seung, Sompolinsky, and Tishby]{Seung1992}
H.~S. Seung, H.~Sompolinsky, and N.~Tishby.
\newblock Statistical mechanics of learning from examples.
\newblock \emph{Phys. Rev. A}, 45:\penalty0 6056--6091, 1992.

\bibitem[Steinberger and Leeb(2018)]{Steinberger2018}
L.~Steinberger and H.~Leeb.
\newblock {Conditional predictive inference for stable algorithms}.
\newblock \emph{Ann. Stat.}, 51\penalty0 (1):\penalty0 290 -- 311, 2018.

\bibitem[Sur and Candès(2019)]{SurCandes2019}
P.~Sur and E.J. Candès.
\newblock A modern maximum-likelihood theory for high-dimensional logistic
  regression.
\newblock \emph{Proc. Natl. Acad. Sci. U.S.A.}, 116\penalty0 (29):\penalty0
  14516--14525, 2019.

\bibitem[Thrampoulidis et~al.(2015)Thrampoulidis, Oymak, and
  Hassibi]{Thrampoulidis15}
C.~Thrampoulidis, S.~Oymak, and B.~Hassibi.
\newblock Regularized linear regression: A precise analysis of the estimation
  error.
\newblock In Peter Grünwald, Elad Hazan, and Satyen Kale, editors,
  \emph{Proceedings of The 28th Conference on Learning Theory}, volume~40 of
  \emph{Proc. Mach. Learn. Res.}, pages 1683--1709, Paris, France, 2015. PMLR.

\bibitem[Thrampoulidis et~al.(2018)Thrampoulidis, Abbasi, and
  Hassibi]{Thrampoulidis2018}
C.~Thrampoulidis, E.~Abbasi, and B.~Hassibi.
\newblock Precise error analysis of regularized $m$ -estimators in high
  dimensions.
\newblock \emph{IEEE Trans. Inf. Theory}, 64\penalty0 (8):\penalty0 5592--5628,
  2018.

\bibitem[Watkin et~al.(1993)Watkin, Rau, and Biehl]{Watkin1993}
T.L.H. Watkin, A.~Rau, and M.~Biehl.
\newblock The statistical mechanics of learning a rule.
\newblock \emph{Rev. Mod. Phys.}, 65:\penalty0 499--556, 1993.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and Vinyals]{Zhang2021}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Commun. ACM}, 64\penalty0 (3):\penalty0 107–115, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Sur, and Cand{\`e}s]{ZhaoSurCandes2022}
Q.~Zhao, P.~Sur, and E.J. Cand{\`e}s.
\newblock {The asymptotic distribution of the MLE in high-dimensional logistic
  models: Arbitrary covariance}.
\newblock \emph{Bernoulli}, 28\penalty0 (3):\penalty0 1835 -- 1861, 2022.

\end{thebibliography}
