\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Daneshmand, and Sra]{Ahn23a}
K.~Ahn, X.~Cheng, H.~Daneshmand, and S.~Sra.
\newblock {Transformers learn to implement preconditioned gradient descent for in-context learning}.
\newblock \emph{arXiv preprint arXiv:2306.00297}, 2023.

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{Akyrek23}
E.~Aky{\"u}rek, D.~Schuurmans, J.~Andreas, T.~Ma, and D.~Zhou.
\newblock {What learning algorithm is in-context learning? Investigations with linear models}.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{Bai23}
Y.~Bai, F.~Chen, H.~Wang, C.~Xiong, and S.~Mei.
\newblock {Transformers as statisticians: provable in-context learning with in-context algorithm selection}.
\newblock In \emph{ICML Workshop on Efficient Systems for Foundation Models}, 2023.

\bibitem[Bhojanapalli et~al.(2020)Bhojanapalli, Yun, Rawat, Reddi, and Kumar]{Bho20}
S.~Bhojanapalli, C.~Yun, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Low-rank bottleneck in multi-head attention models.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{Brown20}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Chen et~al.(2021)Chen, Dao, Winsor, Song, Rudra, and R{\'e}]{Chen21}
B.~Chen, T.~Dao, E.~Winsor, Z.~Song, A.~Rudra, and C.~R{\'e}.
\newblock Scatterbrain: unifying sparse and low-rank attention approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Chen et~al.(2024)Chen, Sheen, Wang, and Yang]{Chen24}
S.~Chen, H.~Sheen, T.~Wang, and Z.~Yang.
\newblock Training dynamics of multi-head softmax attention for in-context learning: emergence, convergence, and optimality.
\newblock \emph{arXiv preprint arXiv:2402.19442}, 2024.

\bibitem[De{V}ore and Popov(1988)]{Devore88}
R.~A. De{V}ore and V.~A. Popov.
\newblock Interpolation of {B}esov spaces.
\newblock \emph{Transactions of the American Mathematical Society}, 305\penalty0 (1):\penalty0 397--414, 1988.

\bibitem[Donoho and Johnstone(1998)]{Donoho98}
D.~L. Donoho and I.~M. Johnstone.
\newblock Minimax estimation via wavelet shrinkage.
\newblock \emph{The Annals of Statistics}, 26\penalty0 (3):\penalty0 879--921, 1998.

\bibitem[Du et~al.(2021)Du, Hu, Kakade, Lee, and Lei]{Du21}
S.~Du, W.~Hu, S.~Kakade, J.~Lee, and Q.~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[D{\~u}ng(2011{\natexlab{a}})]{Dung11}
D.~D{\~u}ng.
\newblock Optimal adaptive sampling recovery.
\newblock \emph{Advances in Computational Mathematics}, 34:\penalty0 1--41, 2011{\natexlab{a}}.

\bibitem[D{\~u}ng(2011{\natexlab{b}})]{Dung11b}
D.~D{\~u}ng.
\newblock B-spline quasi-interpolant representations and sampling recovery of functions with mixed smoothness.
\newblock \emph{Journal of Complexity}, 27\penalty0 (6):\penalty0 541--567, 2011{\natexlab{b}}.

\bibitem[Garc{\'i}a et~al.(2023)Garc{\'i}a, Bansal, Cherry, Foster, Krikun, Feng, Johnson, and Firat]{Garcia23}
X.~Garc{\'i}a, Y.~Bansal, C.~Cherry, G.~F. Foster, M.~Krikun, F.~Feng, M.~Johnson, and O.~Firat.
\newblock The unreasonable effectiveness of few-shot learning for machine translation.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{Garg22}
S.~Garg, D.~Tsipras, P.~Liang, and G.~Valiant.
\newblock {What can Transformers learn in-context? A case study of simple function classes}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Gin\'{e} and Nickl(2015)]{Nickl15}
E.~Gin\'{e} and R.~Nickl.
\newblock \emph{Mathematical foundations of infinite-dimensional statistical models}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.

\bibitem[Guo et~al.(2023)Guo, Hu, Mei, Wang, Xiong, Savarese, and Bai]{Guo23}
T.~Guo, W.~Hu, S.~Mei, H.~Wang, C.~Xiong, S.~Savarese, and Y.~Bai.
\newblock {How do Transformers learn in-context beyond simple functions? A case study on learning with representations}.
\newblock \emph{arXiv preprint arXiv:2310.10616}, 2023.

\bibitem[Hayakawa and Suzuki(2020)]{Hayakawa20}
S.~Hayakawa and T.~Suzuki.
\newblock On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces.
\newblock \emph{Neural Networks}, 123:\penalty0 343--361, 2020.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{Huang23}
Y.~Huang, Y.~Cheng, and Y.~Liang.
\newblock {In-context convergence of Transformers}.
\newblock \emph{arXiv preprint arXiv:2310.05249}, 2023.

\bibitem[Kim and Suzuki(2024)]{Kim24}
J.~Kim and T.~Suzuki.
\newblock Transformers learn nonlinear features in context: nonconvex mean-field dynamics on the attention landscape.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Li et~al.(2024)Li, Wang, Lu, Cui, and Chen]{Li24}
H.~Li, M.~Wang, S.~Lu, X.~Cui, and P.-Y. Chen.
\newblock {Training nonlinear Transformers for efficient in-context learning: a theoretical learning and generalization analysis}.
\newblock \emph{arXiv preprint arXiv:2402.15607}, 2024.

\bibitem[Mahankali et~al.(2023)Mahankali, Hashimoto, and Ma]{Mahankali23}
A.~Mahankali, T.~B. Hashimoto, and T.~Ma.
\newblock {One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention}.
\newblock \emph{arXiv preprint arXiv:2307.03576}, 2023.

\bibitem[Meunier et~al.(2023)Meunier, Li, Gretton, and Kpotufe]{Meunier23}
D.~Meunier, Z.~Li, A.~Gretton, and S.~Kpotufe.
\newblock Nonlinear meta-learning can guarantee faster rates.
\newblock \emph{arXiv preprint arXiv:2307.10870}, 2023.

\bibitem[Nikol'skii(1975)]{Nikolskii75}
S.~M. Nikol'skii.
\newblock \emph{Approximation of functions of several variables and imbedding theorems}, volume 205 of \emph{Grundlehren der mathematischen Wissenschaften}.
\newblock Springer Berlin, 1975.

\bibitem[Nishimura and Suzuki(2024)]{nishimura2024minimax}
Y.~Nishimura and T.~Suzuki.
\newblock Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=EW8ZExRZkJ}.

\bibitem[Okumoto and Suzuki(2022)]{Okumoto22}
S.~Okumoto and T.~Suzuki.
\newblock Learnability of convolutional neural networks for infinite dimensional input via mixed and anisotropic smoothness.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Raventos et~al.(2023)Raventos, Paul, Chen, and Ganguli]{Raventos23}
A.~Raventos, M.~Paul, F.~Chen, and S.~Ganguli.
\newblock Pretraining task diversity and the emergence of non-{B}ayesian in-context learning for regression.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Schmidt-Hieber(2020)]{Schmidt20}
J.~Schmidt-Hieber.
\newblock Nonparametric regression using deep neural networks with {ReLU} activation function.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (4), 2020.

\bibitem[Suzuki(2019)]{Suzuki19}
T.~Suzuki.
\newblock {Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces: optimal rate and curse of dimensionality}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Suzuki and Nitanda(2021)]{Suzuki21}
T.~Suzuki and A.~Nitanda.
\newblock Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic {B}esov space.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Szarek(1981)]{Szarek81}
S.~J. Szarek.
\newblock Nets of {G}rassmann manifold and orthogonal group.
\newblock \emph{Proceedings of Banach Space Workshop}, pages 169--186, 1981.

\bibitem[Takakura and Suzuki(2023)]{Takakura23}
S.~Takakura and T.~Suzuki.
\newblock Approximation and estimation ability of {T}ransformers for sequence-to-sequence functions with infinite dimensional input.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Triebel(1983)]{Triebel83}
H.~Triebel.
\newblock \emph{Theory of function spaces}.
\newblock Monographs in mathematics. Birkh\"{a}user Verlag, 1983.

\bibitem[Triebel(2011)]{Triebel11}
H.~Triebel.
\newblock Entropy numbers in function spaces with mixed integrability.
\newblock \emph{Revista Matematica Complutense}, 24:\penalty0 169--188, 2011.

\bibitem[Tripuraneni et~al.(2020)Tripuraneni, Jin, and Jordan]{Tri20}
N.~Tripuraneni, C.~Jin, and M.~Jordan.
\newblock Provable meta-learning of linear representations.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Tropp(2015)]{Tropp15}
J.~A. Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{Foundations and Trends in Machine Learning}, 8\penalty0 (1–2):\penalty0 1–230, May 2015.
\newblock ISSN 1935-8237.

\bibitem[van~der Vaart and Wellner(1996)]{Book:VanDerVaart:WeakConvergence}
A.~W. van~der Vaart and J.~A. Wellner.
\newblock \emph{Weak convergence and empirical processes: with applications to statistics}.
\newblock Springer, New York, 1996.

\bibitem[von Oswald et~al.(2023)von Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{Oswald23}
J.~von Oswald, E.~Niklasson, E.~Randazzo, J.~Sacramento, A.~Mordvintsev, A.~Zhmoginov, and M.~Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Vyb\'{i}ral(2006)]{Vybiral06}
J.~Vyb\'{i}ral.
\newblock \emph{Function spaces with dominating mixed smoothness}, volume~30 of \emph{Lectures in Mathematics}.
\newblock European Mathematical Society, 2006.

\bibitem[Vyb\'{i}ral(2008)]{Vybiral08}
J.~Vyb\'{i}ral.
\newblock Widths of embeddings in function spaces.
\newblock \emph{Journal of Complexity}, 24\penalty0 (4):\penalty0 545--570, 2008.

\bibitem[Wu et~al.(2024)Wu, Zou, Chen, Braverman, Gu, and Bartlett]{Wu24}
J.~Wu, D.~Zou, Z.~Chen, V.~Braverman, Q.~Gu, and P.~L. Bartlett.
\newblock How many pretraining tasks are needed for in-context learning of linear regression?
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Yang and Barron(1999)]{Yang99}
Y.~Yang and A.~Barron.
\newblock Information-theoretic determination of minimax rates of convergence.
\newblock \emph{The Annals of Statistics}, 27\penalty0 (5):\penalty0 1564--1599, 1999.

\bibitem[Yarotsky(2016)]{Yarotsky16}
D.~Yarotsky.
\newblock {Error bounds for approximations with deep ReLU networks}.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2016.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{Zhang23}
R.~Zhang, S.~Frei, and P.~L. Bartlett.
\newblock {Trained Transformers learn linear models in-context}.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Wu, and Bartlett]{Zhang24}
R.~Zhang, J.~Wu, and P.~L. Bartlett.
\newblock {In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization}.
\newblock \emph{arXiv preprint arXiv:2402.14951}, 2024.

\end{thebibliography}
