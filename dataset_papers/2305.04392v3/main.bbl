\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brevault et~al.(2020)Brevault, Balesdent, and
  Hebbal]{brevault2020overview}
Brevault, L., Balesdent, M., and Hebbal, A.
\newblock Overview of gaussian process based multi-fidelity techniques with
  variable relationship between fidelities, application to aerospace systems.
\newblock \emph{Aerospace Science and Technology}, 107:\penalty0 106339, 2020.

\bibitem[Chaloner \& Verdinelli(1995)Chaloner and
  Verdinelli]{chaloner1995bayesian}
Chaloner, K. and Verdinelli, I.
\newblock Bayesian experimental design: A review.
\newblock \emph{Statistical Science}, pp.\  273--304, 1995.

\bibitem[Cohn et~al.(1996)Cohn, Ghahramani, and Jordan]{cohn1996active}
Cohn, D.~A., Ghahramani, Z., and Jordan, M.~I.
\newblock Active learning with statistical models.
\newblock \emph{Journal of artificial intelligence research}, 4:\penalty0
  129--145, 1996.

\bibitem[Cutajar et~al.(2019)Cutajar, Pullin, Damianou, Lawrence, and
  Gonz{\'a}lez]{cutajar2019deep}
Cutajar, K., Pullin, M., Damianou, A., Lawrence, N., and Gonz{\'a}lez, J.
\newblock Deep gaussian processes for multi-fidelity modeling.
\newblock \emph{arXiv preprint arXiv:1903.07320}, 2019.

\bibitem[De et~al.(2020)De, Britton, Reynolds, Skinner, Jansen, and
  Doostan]{de2020transfer}
De, S., Britton, J., Reynolds, M., Skinner, R., Jansen, K., and Doostan, A.
\newblock On transfer learning of neural networks using bi-fidelity data for
  uncertainty propagation.
\newblock \emph{International Journal for Uncertainty Quantification},
  10\penalty0 (6), 2020.

\bibitem[Gal et~al.(2017)Gal, Islam, and Ghahramani]{gal2017deep}
Gal, Y., Islam, R., and Ghahramani, Z.
\newblock Deep bayesian active learning with image data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1183--1192. PMLR, 2017.

\bibitem[Garnelo et~al.(2018{\natexlab{a}})Garnelo, Rosenbaum, Maddison,
  Ramalho, Saxton, Shanahan, Teh, Rezende, and Eslami]{garnelo2018conditional}
Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan,
  M., Teh, Y.~W., Rezende, D., and Eslami, S.~A.
\newblock Conditional neural processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713. PMLR, 2018{\natexlab{a}}.

\bibitem[Garnelo et~al.(2018{\natexlab{b}})Garnelo, Schwarz, Rosenbaum, Viola,
  Rezende, Eslami, and Teh]{garnelo2018neural}
Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D.~J., Eslami, S.,
  and Teh, Y.~W.
\newblock Neural processes.
\newblock \emph{arXiv preprint arXiv:1807.01622}, 2018{\natexlab{b}}.

\bibitem[Guo et~al.(2022)Guo, Manzoni, Amendt, Conti, and
  Hesthaven]{guo2022multi}
Guo, M., Manzoni, A., Amendt, M., Conti, P., and Hesthaven, J.~S.
\newblock Multi-fidelity regression using artificial neural networks: efficient
  approximation of parameter-dependent output quantities.
\newblock \emph{Computer methods in applied mechanics and engineering},
  389:\penalty0 114378, 2022.

\bibitem[Hebbal et~al.(2021)Hebbal, Brevault, Balesdent, Talbi, and
  Melab]{hebbal2021multi}
Hebbal, A., Brevault, L., Balesdent, M., Talbi, E.-G., and Melab, N.
\newblock Multi-fidelity modeling with different input domain definitions using
  deep gaussian processes.
\newblock \emph{Structural and Multidisciplinary Optimization}, 63\penalty0
  (5):\penalty0 2267--2288, 2021.

\bibitem[Holl et~al.(2020)Holl, Koltun, and Thuerey]{holl2020learning}
Holl, P., Koltun, V., and Thuerey, N.
\newblock Learning to control pdes with differentiable physics.
\newblock \emph{arXiv preprint arXiv:2001.07457}, 2020.

\bibitem[Hosking(2020)]{Hosking2020}
Hosking, S.
\newblock Multifidelity climate modelling, github.
\newblock \url{https://github.com/scotthosking/mf_modelling}, 2020.

\bibitem[Houlsby et~al.(2011)Houlsby, Husz{\'a}r, Ghahramani, and
  Lengyel]{houlsby2011bayesian}
Houlsby, N., Husz{\'a}r, F., Ghahramani, Z., and Lengyel, M.
\newblock Bayesian active learning for classification and preference learning.
\newblock \emph{arXiv preprint arXiv:1112.5745}, 2011.

\bibitem[Jha et~al.(2022)Jha, Gong, Wang, Turner, and Yao]{jha2022neural}
Jha, S., Gong, D., Wang, X., Turner, R.~E., and Yao, L.
\newblock The neural process family: Survey, applications and perspectives.
\newblock \emph{arXiv preprint arXiv:2209.00517}, 2022.

\bibitem[Kandasamy et~al.(2017)Kandasamy, Dasarathy, Schneider, and
  P{\'o}czos]{kandasamy2017multi}
Kandasamy, K., Dasarathy, G., Schneider, J., and P{\'o}czos, B.
\newblock Multi-fidelity bayesian optimisation with continuous approximations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1799--1808. PMLR, 2017.

\bibitem[Kennedy \& O'Hagan(2000)Kennedy and O'Hagan]{kennedy2000predicting}
Kennedy, M.~C. and O'Hagan, A.
\newblock Predicting the output from a complex computer code when fast
  approximations are available.
\newblock \emph{Biometrika}, 87\penalty0 (1):\penalty0 1--13, 2000.

\bibitem[Kim et~al.(2018)Kim, Mnih, Schwarz, Garnelo, Eslami, Rosenbaum,
  Vinyals, and Teh]{kim2018attentive}
Kim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A., Rosenbaum, D.,
  Vinyals, O., and Teh, Y.~W.
\newblock Attentive neural processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kim et~al.(2019)Kim, Mnih, Schwarz, Garnelo, Eslami, Rosenbaum,
  Vinyals, and Teh]{kim2019attentive}
Kim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A., Rosenbaum, D.,
  Vinyals, O., and Teh, Y.~W.
\newblock Attentive neural processes.
\newblock \emph{arXiv preprint arXiv:1901.05761}, 2019.

\bibitem[Kirsch et~al.(2019)Kirsch, Van~Amersfoort, and
  Gal]{kirsch2019batchbald}
Kirsch, A., Van~Amersfoort, J., and Gal, Y.
\newblock Batchbald: Efficient and diverse batch acquisition for deep bayesian
  active learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Le~Gratiet \& Garnier(2014)Le~Gratiet and Garnier]{le2014recursive}
Le~Gratiet, L. and Garnier, J.
\newblock Recursive co-kriging model for design of computer experiments with
  multiple levels of fidelity.
\newblock \emph{International Journal for Uncertainty Quantification},
  4\penalty0 (5), 2014.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Kirby, and Zhe]{li2020deep}
Li, S., Kirby, R.~M., and Zhe, S.
\newblock Deep multi-fidelity active learning of high-dimensional outputs.
\newblock \emph{arXiv preprint arXiv:2012.00901}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Xing, Kirby, and Zhe]{li2020multi}
Li, S., Xing, W., Kirby, R., and Zhe, S.
\newblock Multi-fidelity bayesian optimization via deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 8521--8531, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Phillips, Yu, Kirby, and
  Zhe]{li2022batch}
Li, S., Phillips, J., Yu, X., Kirby, R., and Zhe, S.
\newblock Batch multi-fidelity active learning with budget constraints.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Wang, Kirby, and Zhe]{li2022deep}
Li, S., Wang, Z., Kirby, R., and Zhe, S.
\newblock Deep multi-fidelity active learning of high-dimensional outputs.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1694--1711. PMLR, 2022{\natexlab{b}}.

\bibitem[Louizos et~al.(2019)Louizos, Shi, Schutte, and
  Welling]{louizos2019functional}
Louizos, C., Shi, X., Schutte, K., and Welling, M.
\newblock The functional neural process.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Meng \& Karniadakis(2020)Meng and Karniadakis]{meng2020composite}
Meng, X. and Karniadakis, G.~E.
\newblock A composite neural network that learns from multi-fidelity data:
  Application to function approximation and inverse pde problems.
\newblock \emph{Journal of Computational Physics}, 401:\penalty0 109020, 2020.

\bibitem[Meng et~al.(2021)Meng, Babaee, and Karniadakis]{meng2021multi}
Meng, X., Babaee, H., and Karniadakis, G.~E.
\newblock Multi-fidelity bayesian neural networks: Algorithms and applications.
\newblock \emph{Journal of Computational Physics}, 438:\penalty0 110361, 2021.

\bibitem[{\O}ksendal(2003)]{oksendal2003stochastic}
{\O}ksendal, B.
\newblock Stochastic differential equations.
\newblock In \emph{Stochastic differential equations}, pp.\  65--84. Springer,
  2003.

\bibitem[Olsen-Kettle(2011)]{olsen2011numerical}
Olsen-Kettle, L.
\newblock Numerical solution of partial differential equations.
\newblock \emph{Lecture notes at University of Queensland, Australia}, 2011.

\bibitem[Peherstorfer et~al.(2018)Peherstorfer, Willcox, and
  Gunzburger]{peherstorfer2018survey}
Peherstorfer, B., Willcox, K., and Gunzburger, M.
\newblock Survey of multifidelity methods in uncertainty propagation,
  inference, and optimization.
\newblock \emph{Siam Review}, 60\penalty0 (3):\penalty0 550--591, 2018.

\bibitem[Perdikaris et~al.(2015)Perdikaris, Venturi, Royset, and
  Karniadakis]{perdikaris2015multi}
Perdikaris, P., Venturi, D., Royset, J.~O., and Karniadakis, G.~E.
\newblock Multi-fidelity modelling via recursive co-kriging and
  gaussian--markov random fields.
\newblock \emph{Proceedings of the Royal Society A: Mathematical, Physical and
  Engineering Sciences}, 471\penalty0 (2179):\penalty0 20150018, 2015.

\bibitem[Perdikaris et~al.(2016)Perdikaris, Venturi, and
  Karniadakis]{perdikaris2016multifidelity}
Perdikaris, P., Venturi, D., and Karniadakis, G.~E.
\newblock Multifidelity information fusion algorithms for high-dimensional
  systems and massive data sets.
\newblock \emph{SIAM Journal on Scientific Computing}, 38\penalty0
  (4):\penalty0 B521--B538, 2016.

\bibitem[Perdikaris et~al.(2017)Perdikaris, Raissi, Damianou, Lawrence, and
  Karniadakis]{perdikaris2017nonlinear}
Perdikaris, P., Raissi, M., Damianou, A., Lawrence, N.~D., and Karniadakis,
  G.~E.
\newblock Nonlinear information fusion algorithms for data-efficient
  multi-fidelity modelling.
\newblock \emph{Proceedings of the Royal Society A: Mathematical, Physical and
  Engineering Sciences}, 473\penalty0 (2198):\penalty0 20160751, 2017.

\bibitem[Perry et~al.(2019)Perry, Kirby, Narayan, and
  Whitaker]{perry2019allocation}
Perry, D.~J., Kirby, R.~M., Narayan, A., and Whitaker, R.~T.
\newblock Allocation strategies for high fidelity models in the multifidelity
  regime.
\newblock \emph{SIAM/ASA Journal on Uncertainty Quantification}, 7\penalty0
  (1):\penalty0 203--231, 2019.

\bibitem[Raissi \& Karniadakis(2016)Raissi and Karniadakis]{raissi2016deep}
Raissi, M. and Karniadakis, G.
\newblock Deep multi-fidelity gaussian processes.
\newblock \emph{arXiv preprint arXiv:1604.07484}, 2016.

\bibitem[Siddhant \& Lipton(2018)Siddhant and Lipton]{siddhant2018deep}
Siddhant, A. and Lipton, Z.~C.
\newblock Deep bayesian active learning for natural language processing:
  Results of a large-scale empirical study.
\newblock \emph{arXiv preprint arXiv:1808.05697}, 2018.

\bibitem[Singh et~al.(2019)Singh, Yoon, Son, and Ahn]{singh2019sequential}
Singh, G., Yoon, J., Son, Y., and Ahn, S.
\newblock Sequential neural processes.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 10254--10264, 2019.

\bibitem[Valero et~al.(2021)Valero, Jofre, and Torres]{valero2021multifidelity}
Valero, M.~M., Jofre, L., and Torres, R.
\newblock Multifidelity prediction in wildfire spread simulation: Modeling,
  uncertainty quantification and sensitivity analysis.
\newblock \emph{Environmental Modelling \& Software}, 141:\penalty0 105050,
  2021.

\bibitem[Volpp et~al.(2020)Volpp, Fl{\"u}renbrock, Grossberger, Daniel, and
  Neumann]{volpp2020bayesian}
Volpp, M., Fl{\"u}renbrock, F., Grossberger, L., Daniel, C., and Neumann, G.
\newblock Bayesian context aggregation for neural processes.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wang \& Van~Hoof(2020)Wang and Van~Hoof]{wang2020doubly}
Wang, Q. and Van~Hoof, H.
\newblock Doubly stochastic variational inference for neural processes with
  hierarchical latent variables.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10018--10028. PMLR, 2020.

\bibitem[Wang \& Lin(2020)Wang and Lin]{wang2020mfpc}
Wang, Y. and Lin, G.
\newblock Mfpc-net: Multi-fidelity physics-constrained neural process.
\newblock \emph{arXiv preprint arXiv:2010.01378}, 2020.

\bibitem[Wang et~al.(2021)Wang, Xing, Kirby, and Zhe]{wang2021multi}
Wang, Z., Xing, W., Kirby, R., and Zhe, S.
\newblock Multi-fidelity high-order gaussian processes for physical simulation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  847--855. PMLR, 2021.

\bibitem[Wu et~al.(2022)Wu, Chinazzi, Vespignani, Ma, and Yu]{wu2022multi}
Wu, D., Chinazzi, M., Vespignani, A., Ma, Y.-A., and Yu, R.
\newblock Multi-fidelity hierarchical neural processes.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pp.\  2029--2038, 2022.

\bibitem[Wu et~al.(2023)Wu, Niu, Chinazzi, Vespignani, Ma, and
  Yu]{wu2021accelerating}
Wu, D., Niu, R., Chinazzi, M., Vespignani, A., Ma, Y.-A., and Yu, R.
\newblock Deep bayesian active learning for accelerating stochastic simulation.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, 2023.

\bibitem[Zimmer et~al.(2018)Zimmer, Meister, and Nguyen-Tuong]{zimmer2018safe}
Zimmer, C., Meister, M., and Nguyen-Tuong, D.
\newblock Safe active learning for time-series modeling with gaussian
  processes.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  2735--2744, 2018.

\end{thebibliography}
