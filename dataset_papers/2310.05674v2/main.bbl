\begin{thebibliography}{10}

\bibitem{abbas2023semdedup}
Amro Kamal~Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and
  Ari~S Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic
  deduplication.
\newblock In {\em ICLR 2023 Workshop on Mathematical and Empirical
  Understanding of Foundation Models}, 2023.

\bibitem{antoniou2018how}
Antreas Antoniou, Harrison Edwards, and Amos Storkey.
\newblock How to train your {MAML}.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{arnold2020learn2learn}
S{\'e}bastien~MR Arnold, Praateek Mahajan, Debajyoti Datta, Ian Bunner, and
  Konstantinos~Saitas Zarkias.
\newblock learn2learn: A library for meta-learning research.
\newblock {\em arXiv preprint arXiv:2008.12284}, 2020.

\bibitem{blondel2022efficient}
Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer,
  Felipe Llinares-L{\'o}pez, Fabian Pedregosa, and Jean-Philippe Vert.
\newblock Efficient and modular implicit differentiation.
\newblock {\em Advances in neural information processing systems},
  35:5230--5242, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{choe2023betty}
Sang~Keun Choe, Willie Neiswanger, Pengtao Xie, and Eric Xing.
\newblock Betty: An automatic differentiation library for multilevel
  optimization.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{clarke2022scalable}
Ross~M Clarke, Elre~Talea Oldewage, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Scalable one-pass optimisation of high-dimensional weight-update
  hyperparameters by implicit differentiation.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{Coleman2020Selection}
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter
  Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{deleu2019torchmeta}
Tristan Deleu, Tobias W\"urfl, Mandana Samiei, Joseph~Paul Cohen, and Yoshua
  Bengio.
\newblock {Torchmeta: A Meta-Learning library for PyTorch}, 2019.
\newblock Available at: https://github.com/tristandeleu/pytorch-meta.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{dery2022should}
Lucio~M. Dery, Paul Michel, Ameet Talwalkar, and Graham Neubig.
\newblock Should we be pre-training? an argument for end-task aware training as
  an alternative.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{Dhillon2020A}
Guneet~Singh Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano
  Soatto.
\newblock A baseline for few-shot image classification.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem{franceschi2017forward}
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1165--1173. PMLR, 2017.

\bibitem{franceschi2018bilevel}
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
  Massimiliano Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1568--1577. PMLR, 2018.

\bibitem{fung2022jfb}
Samy~Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and
  Wotao Yin.
\newblock Jfb: Jacobian-free backpropagation for implicit networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 6648--6656, 2022.

\bibitem{gadre2023datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios
  Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu
  Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock {\em arXiv preprint arXiv:2304.14108}, 2023.

\bibitem{grazzi2020iteration}
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo.
\newblock On the iteration complexity of hypergradient computation.
\newblock In {\em International Conference on Machine Learning}, pages
  3748--3758. PMLR, 2020.

\bibitem{grefenstette2019generalized}
Edward Grefenstette, Brandon Amos, Denis Yarats, Phu~Mon Htut, Artem Molchanov,
  Franziska Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala.
\newblock Generalized inner loop meta-learning.
\newblock {\em arXiv preprint arXiv:1910.01727}, 2019.

\bibitem{gudovskiy2021autodo}
Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, and Sotaro
  Tsukizawa.
\newblock Autodo: Robust autoaugment for biased data with label noise via
  scalable probabilistic implicit differentiation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 16601--16610, 2021.

\bibitem{guo2022deepcore}
Chengcheng Guo, Bo~Zhao, and Yanbing Bai.
\newblock Deepcore: A comprehensive library for coreset selection in deep
  learning.
\newblock In {\em Database and Expert Systems Applications: 33rd International
  Conference, DEXA 2022, Vienna, Austria, August 22--24, 2022, Proceedings,
  Part I}, pages 181--195. Springer, 2022.

\bibitem{gupta2018meta}
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
\newblock Meta-reinforcement learning of structured exploration strategies.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{dontstoppretraining2020}
Suchin Gururangan, Ana MarasoviÄ‡, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A. Smith.
\newblock Don't stop pretraining: Adapt language models to domains and tasks.
\newblock In {\em Proceedings of ACL}, 2020.

\bibitem{hataya2023nystrom}
Ryuichiro Hataya and Makoto Yamada.
\newblock Nystr{\"o}m method for accurate and scalable implicit
  differentiation.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4643--4654. PMLR, 2023.

\bibitem{hataya2022meta}
Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama.
\newblock Meta approach to data augmentation optimization.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision}, pages 2574--2583, 2022.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
  HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{humplik2019meta}
Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro~A Ortega, Yee~Whye
  Teh, and Nicolas Heess.
\newblock Meta reinforcement learning as task inference.
\newblock {\em arXiv preprint arXiv:1905.06424}, 2019.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{devlin2018bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock {\em arXiv preprint arXiv:2304.02643}, 2023.

\bibitem{kwon2023fully}
Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert~D Nowak.
\newblock A fully first-order method for stochastic bilevel optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  18083--18113. PMLR, 2023.

\bibitem{li2020pytorch}
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,
  Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.
\newblock Pytorch distributed: experiences on accelerating data parallel
  training.
\newblock {\em Proceedings of the VLDB Endowment}, 13:3005--3018, 08 2020.

\bibitem{liao2018reviving}
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow,
  Raquel Urtasun, and Richard Zemel.
\newblock Reviving and improving recurrent back-propagation.
\newblock In {\em International Conference on Machine Learning}, pages
  3082--3091. PMLR, 2018.

\bibitem{liu2022bome}
Bo~Liu, Mao Ye, Stephen Wright, Peter Stone, and qiang liu.
\newblock {BOME}! bilevel optimization made easy: A simple first-order
  approach.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock {DARTS}: Differentiable architecture search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{lorraine2020optimizing}
Jonathan Lorraine, Paul Vicol, and David Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1540--1552. PMLR, 2020.

\bibitem{luketina2016scalable}
Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko.
\newblock Scalable gradient-based tuning of continuous regularization
  hyperparameters.
\newblock In {\em International conference on machine learning}, pages
  2952--2960. PMLR, 2016.

\bibitem{maclaurin2015gradient}
Dougal Maclaurin, David Duvenaud, and Ryan Adams.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In {\em International conference on machine learning}, pages
  2113--2122. PMLR, 2015.

\bibitem{metz2022velo}
Luke Metz, James Harrison, C~Daniel Freeman, Amil Merchant, Lucas Beyer, James
  Bradbury, Naman Agrawal, Ben Poole, Igor Mordatch, Adam Roberts, et~al.
\newblock Velo: Training versatile learned optimizers by scaling up.
\newblock {\em arXiv preprint arXiv:2211.09760}, 2022.

\bibitem{metz2019understanding}
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha
  Sohl-Dickstein.
\newblock Understanding and correcting pathologies in the training of learned
  optimizers.
\newblock In {\em International Conference on Machine Learning}, pages
  4556--4565. PMLR, 2019.

\bibitem{metz2016unrolled}
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein.
\newblock Unrolled generative adversarial networks.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{paul2021deep}
Mansheej Paul, Surya Ganguli, and Gintare~Karolina Dziugaite.
\newblock Deep learning on a data diet: Finding important examples early in
  training.
\newblock {\em Advances in Neural Information Processing Systems},
  34:20596--20607, 2021.

\bibitem{pearlmutter1994fast}
Barak~A Pearlmutter.
\newblock Fast exact multiplication by the hessian.
\newblock {\em Neural computation}, 6(1):147--160, 1994.

\bibitem{pedregosa2016hyperparameter}
Fabian Pedregosa.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In {\em International conference on machine learning}, pages
  737--746. PMLR, 2016.

\bibitem{radford2022robust}
Alec Radford, Jong~Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and
  Ilya Sutskever.
\newblock Robust speech recognition via large-scale weak supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  28492--28518. PMLR, 2023.

\bibitem{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{rakelly2019efficient}
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In {\em International conference on machine learning}, pages
  5331--5340. PMLR, 2019.

\bibitem{ratner2016data}
Alexander~J Ratner, Christopher~M De~Sa, Sen Wu, Daniel Selsam, and Christopher
  R{\'e}.
\newblock Data programming: Creating large training sets, quickly.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em International conference on machine learning}, pages
  4334--4343. PMLR, 2018.

\bibitem{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em nature}, 323(6088):533--536, 1986.

\bibitem{schuhmann2022laionb}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade~W Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, Patrick Schramowski, Srivatsa~R Kundurthy, Katherine Crowson,
  Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev.
\newblock {LAION}-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock In {\em Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock {\em arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{shu2019meta}
Jun Shu, Qi~Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.
\newblock Meta-weight-net: Learning an explicit mapping for sample weighting.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:19523--19536, 2022.

\bibitem{sutton2019bitter}
Richard Sutton.
\newblock The bitter lesson.
\newblock {\em Incomplete Ideas (blog)}, 13(1), 2019.

\bibitem{toneva2018empirical}
Mariya Toneva, Alessandro Sordoni, Remi~Tachet des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J. Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2023dynams}
Jiaxing Wang, Yong Li, Jingwei Zhuo, Xupeng Shi, WEIZHONG ZHANG, Lixing Gong,
  Tong Tao, Pengzhang Liu, Yongjun Bao, and Weipeng Yan.
\newblock Dyna{MS}: Dyanmic margin selection for efficient deep learning.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{wang2019characterizing}
Zirui Wang, Zihang Dai, Barnab{\'a}s P{\'o}czos, and Jaime Carbonell.
\newblock Characterizing and avoiding negative transfer.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 11293--11302, 2019.

\bibitem{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling.
\newblock {\em arXiv preprint arXiv:2302.03169}, 2023.

\bibitem{yu2021fine}
Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang.
\newblock Fine-tuning pre-trained language model with weak supervision: A
  contrastive-regularized self-training approach.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1063--1077, 2021.

\bibitem{zhang2021wrench}
Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and
  Alexander Ratner.
\newblock {WRENCH}: A comprehensive benchmark for weak supervision.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\bibitem{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
  Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?
\newblock {\em Advances in Neural Information Processing Systems},
  33:15383--15393, 2020.

\bibitem{zhang2021idarts}
Miao Zhang, Steven~W Su, Shirui Pan, Xiaojun Chang, Ehsan~M Abbasnejad, and
  Reza Haffari.
\newblock idarts: Differentiable architecture search with stochastic implicit
  gradients.
\newblock In {\em International Conference on Machine Learning}, pages
  12557--12566. PMLR, 2021.

\bibitem{zheng2021meta}
Guoqing Zheng, Ahmed~Hassan Awadallah, and Susan Dumais.
\newblock Meta label correction for noisy label learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 11053--11061, 2021.

\bibitem{zucchet2022contrastive}
Nicolas Zucchet, Simon Schug, Johannes Von~Oswald, Dominic Zhao, and Jo{\~a}o
  Sacramento.
\newblock A contrastive rule for meta-learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:25921--25936, 2022.

\end{thebibliography}
