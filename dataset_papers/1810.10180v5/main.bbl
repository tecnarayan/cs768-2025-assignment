\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, and de~Freitas]{andrychowicz2016learning}
Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.~W., Pfau, D., Schaul, T.,
  and de~Freitas, N.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3981--3989, 2016.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein gan.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Bello et~al.(2017)Bello, Zoph, Vasudevan, and Le]{Bello17}
Bello, I., Zoph, B., Vasudevan, V., and Le, Q.
\newblock Neural optimizer search with reinforcement learning.
\newblock 2017.
\newblock URL \url{https://arxiv.org/pdf/1709.07417.pdf}.

\bibitem[Bengio et~al.(1992)Bengio, Bengio, Cloutier, and
  Gecsei]{bengio1992optimization}
Bengio, S., Bengio, Y., Cloutier, J., and Gecsei, J.
\newblock On the optimization of a synaptic learning rule.
\newblock In \emph{Preprints Conf. Optimality in Artificial and Biological
  Neural Networks}, pp.\  6--8. Univ. of Texas, 1992.

\bibitem[Bengio(2000)]{bengio2000gradient}
Bengio, Y.
\newblock Gradient-based optimization of hyperparameters.
\newblock \emph{Neural computation}, 12\penalty0 (8):\penalty0 1889--1900,
  2000.

\bibitem[Bengio et~al.(1990)Bengio, Bengio, and Cloutier]{bengio1990learning}
Bengio, Y., Bengio, S., and Cloutier, J.
\newblock \emph{Learning a synaptic learning rule}.
\newblock Universit{\'e} de Montr{\'e}al, D{\'e}partement d'informatique et de
  recherche op{\'e}rationnelle, 1990.

\bibitem[Bergstra \& Bengio(2012)Bergstra and Bengio]{bergstra2012random}
Bergstra, J. and Bengio, Y.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Feb):\penalty0 281--305, 2012.

\bibitem[Buckman et~al.(2018)Buckman, Hafner, Tucker, Brevdo, and
  Lee]{buckman2018sample}
Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H.
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock \emph{arXiv preprint arXiv:1807.01675}, 2018.

\bibitem[Domke(2012)]{domke2012generic}
Domke, J.
\newblock Generic methods for optimization-based modeling.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  318--326,
  2012.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Fleiss(1993)]{fleiss1993review}
Fleiss, J.
\newblock Review papers: The statistical basis of meta-analysis.
\newblock \emph{Statistical methods in medical research}, 2\penalty0
  (2):\penalty0 121--145, 1993.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, and
  Pontil]{franceschi2018bilevel}
Franceschi, L., Frasconi, P., Salzo, S., and Pontil, M.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.04910}, 2018.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Goldberg \& Holland(1988)Goldberg and Holland]{goldberg1988genetic}
Goldberg, D.~E. and Holland, J.~H.
\newblock Genetic algorithms and machine learning.
\newblock \emph{Machine learning}, 3\penalty0 (2):\penalty0 95--99, 1988.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Graves, A., Wayne, G., and Danihelka, I.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pp.\  630--645.
  Springer, 2016.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter2001learning}
Hochreiter, S., Younger, A.~S., and Conwell, P.~R.
\newblock Learning to learn using gradient descent.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  87--94. Springer, 2001.

\bibitem[Houthooft et~al.(2018)Houthooft, Chen, Isola, Stadie, Wolski, Ho, and
  Abbeel]{houthooft2018evolved}
Houthooft, R., Chen, R.~Y., Isola, P., Stadie, B.~C., Wolski, F., Ho, J., and
  Abbeel, P.
\newblock Evolved policy gradients.
\newblock \emph{arXiv preprint arXiv:1802.04821}, 2018.

\bibitem[Kaiser \& Sutskever(2015)Kaiser and Sutskever]{kaiser2015neural}
Kaiser, {\L}. and Sutskever, I.
\newblock Neural gpus learn algorithms.
\newblock \emph{arXiv preprint arXiv:1511.08228}, 2015.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[LeCun(1998)]{lecun1998mnist}
LeCun, Y.
\newblock The mnist database of handwritten digits.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem[Li \& Malik(2017{\natexlab{a}})Li and Malik]{li2016learning}
Li, K. and Malik, J.
\newblock Learning to optimize.
\newblock \emph{International Conference on Learning Representations},
  2017{\natexlab{a}}.

\bibitem[Li \& Malik(2017{\natexlab{b}})Li and Malik]{li2017learning}
Li, K. and Malik, J.
\newblock Learning to optimize neural nets.
\newblock \emph{arXiv preprint arXiv:1703.00441}, 2017{\natexlab{b}}.

\bibitem[Lucas et~al.(2018)Lucas, Zemel, and Grosse]{lucas2018aggregated}
Lucas, J., Zemel, R., and Grosse, R.
\newblock Aggregated momentum: Stability through passive damping.
\newblock \emph{arXiv preprint arXiv:1804.00325}, 2018.

\bibitem[Lv et~al.(2017)Lv, Jiang, and Li]{lv2017learning}
Lv, K., Jiang, S., and Li, J.
\newblock Learning gradient descent: Better generalization and longer horizons.
\newblock \emph{arXiv preprint arXiv:1703.03633}, 2017.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2113--2122, 2015.

\bibitem[Metz et~al.(2018)Metz, Maheswaranathan, Cheung, and
  Sohl-Dickstein]{metz2018learning}
Metz, L., Maheswaranathan, N., Cheung, B., and Sohl-Dickstein, J.
\newblock Learning unsupervised learning rules.
\newblock \emph{arXiv preprint arXiv:1804.00222}, 2018.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Y.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence o (1/k\^{} 2).
\newblock In \emph{Doklady AN USSR}, volume 269, pp.\  543--547, 1983.

\bibitem[Nesterov \& Spokoiny(2011)Nesterov and Spokoiny]{nesterov2011random}
Nesterov, Y. and Spokoiny, V.
\newblock Random gradient-free minimization of convex functions.
\newblock Technical report, Universit{\'e} catholique de Louvain, Center for
  Operations Research and Econometrics (CORE), 2011.

\bibitem[Ollivier et~al.(2015)Ollivier, Tallec, and
  Charpiat]{ollivier2015training}
Ollivier, Y., Tallec, C., and Charpiat, G.
\newblock Training recurrent networks online without backtracking.
\newblock \emph{arXiv preprint arXiv:1507.07680}, 2015.

\bibitem[Parmas et~al.(2018)Parmas, Rasmussen, Peters, and
  Doya]{parmas2018pipps}
Parmas, P., Rasmussen, C.~E., Peters, J., and Doya, K.
\newblock Pipps: Flexible model-based policy search robust to the curse of
  chaos.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4062--4071, 2018.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1310--1318, 2013.

\bibitem[Rechenberg(1973)]{rechenberg1973evolutionsstrategie}
Rechenberg, I.
\newblock Evolutionsstrategie--optimierung technisher systeme nach prinzipien
  der biologischen evolution.
\newblock 1973.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Schmidhuber(1995)]{schmidhuber1995learning}
Schmidhuber, J.
\newblock On learning how to learn learning strategies.
\newblock 1995.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2951--2959, 2012.

\bibitem[Staines \& Barber(2012)Staines and Barber]{staines2012variational}
Staines, J. and Barber, D.
\newblock Variational optimization.
\newblock \emph{arXiv preprint arXiv:1212.4507}, 2012.

\bibitem[Tallec \& Ollivier(2017)Tallec and Ollivier]{tallec2017unbiasing}
Tallec, C. and Ollivier, Y.
\newblock Unbiasing truncated backpropagation through time.
\newblock \emph{arXiv preprint arXiv:1705.08209}, 2017.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Werbos(1990)]{werbos1990backpropagation}
Werbos, P.~J.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0
  1550--1560, 1990.

\bibitem[Wichrowska et~al.(2017)Wichrowska, Maheswaranathan, Hoffman,
  Colmenarejo, Denil, de~Freitas, and Sohl-Dickstein]{wichrowska2017learned}
Wichrowska, O., Maheswaranathan, N., Hoffman, M.~W., Colmenarejo, S.~G., Denil,
  M., de~Freitas, N., and Sohl-Dickstein, J.
\newblock Learned optimizers that scale and generalize.
\newblock \emph{International Conference on Machine Learning}, 2017.

\bibitem[Wierstra et~al.(2008)Wierstra, Schaul, Peters, and
  Schmidhuber]{wierstra2008natural}
Wierstra, D., Schaul, T., Peters, J., and Schmidhuber, J.
\newblock Natural evolution strategies.
\newblock In \emph{Evolutionary Computation, 2008. CEC 2008.(IEEE World
  Congress on Computational Intelligence). IEEE Congress on}, pp.\  3381--3387.
  IEEE, 2008.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Williams \& Zipser(1989)Williams and Zipser]{williams1989learning}
Williams, R.~J. and Zipser, D.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural computation}, 1\penalty0 (2):\penalty0 270--280, 1989.

\bibitem[Wolpert \& Macready(1997)Wolpert and Macready]{wolpert1997no}
Wolpert, D.~H. and Macready, W.~G.
\newblock No free lunch theorems for optimization.
\newblock \emph{IEEE transactions on evolutionary computation}, 1\penalty0
  (1):\penalty0 67--82, 1997.

\bibitem[Wu et~al.(2016)Wu, Ren, Liao, and Grosse]{wuunderstanding}
Wu, Y., Ren, M., Liao, R., and Grosse, R.~B.
\newblock Understanding short-horizon bias in stochastic meta-optimization.
\newblock pp.\  478--487, 2016.

\end{thebibliography}
