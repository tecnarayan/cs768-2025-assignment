\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and De~Freitas]{andrychowicz2016learning}
Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.~W., Pfau, D., Schaul, T.,
  Shillingford, B., and De~Freitas, N.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3981--3989, 2016.

\bibitem[Asuncion \& Newman(2007)Asuncion and Newman]{asuncion2007uci}
Asuncion, A. and Newman, D.
\newblock {UCI Machine Learning Repository}, 2007.

\bibitem[Baydin et~al.(2017)Baydin, Cornish, Rubio, Schmidt, and
  Wood]{baydin2017online}
Baydin, A.~G., Cornish, R., Rubio, D.~M., Schmidt, M., and Wood, F.
\newblock Online learning rate adaptation with hypergradient descent.
\newblock \emph{arXiv preprint arXiv:1703.04782}, 2017.

\bibitem[Bengio(2000)]{bengio2000gradient}
Bengio, Y.
\newblock Gradient-based optimization of hyperparameters.
\newblock \emph{Neural Computation}, 12\penalty0 (8):\penalty0 1889--1900,
  2000.

\bibitem[Benzing et~al.(2019)Benzing, Gauy, Mujika, Martinsson, and
  Steger]{benzing2019optimal}
Benzing, F., Gauy, M.~M., Mujika, A., Martinsson, A., and Steger, A.
\newblock {Optimal Kronecker-sum approximation of real time recurrent
  learning}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  604--613. PMLR, 2019.

\bibitem[Bergstra et~al.(2011)Bergstra, Bardenet, Bengio, and
  K{\'e}gl]{bergstra2011algorithms}
Bergstra, J.~S., Bardenet, R., Bengio, Y., and K{\'e}gl, B.
\newblock Algorithms for hyper-parameter optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  2546--2554, 2011.

\bibitem[Bertinetto et~al.(2018)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto2018meta}
Bertinetto, L., Henriques, J.~F., Torr, P.~H., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock \emph{arXiv preprint arXiv:1805.08136}, 2018.

\bibitem[Bischof et~al.(1996)Bischof, Pusch, and
  Knoesel]{bischof1996sensitivity}
Bischof, C.~H., Pusch, G.~D., and Knoesel, R.
\newblock {Sensitivity analysis of the MM5 weather model using automatic
  differentiation}.
\newblock \emph{Computers in Physics}, 10\penalty0 (6):\penalty0 605--612,
  1996.

\bibitem[Blondel et~al.(2021)Blondel, Berthet, Cuturi, Frostig, Hoyer,
  Llinares-L{\'o}pez, Pedregosa, and Vert]{blondel2021efficient}
Blondel, M., Berthet, Q., Cuturi, M., Frostig, R., Hoyer, S.,
  Llinares-L{\'o}pez, F., Pedregosa, F., and Vert, J.-P.
\newblock Efficient and modular implicit differentiation.
\newblock \emph{arXiv preprint arXiv:2105.15183}, 2021.

\bibitem[Chandra et~al.(2022)Chandra, Xie, Ragan-Kelley, and
  Meijer]{chandra2022gradient}
Chandra, K., Xie, A., Ragan-Kelley, J., and Meijer, E.
\newblock Gradient descent: The ultimate optimizer.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 8214--8225, 2022.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Domke(2012)]{domke2012generic}
Domke, J.
\newblock Generic methods for optimization-based modeling.
\newblock In \emph{Proceedings of Machine Learning Research}, pp.\  318--326,
  2012.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{finn2018probabilistic}
Finn, C., Xu, K., and Levine, S.
\newblock Probabilistic model-agnostic meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  31, 2018.

\bibitem[Finn(2018)]{finn2018learning}
Finn, C.~B.
\newblock \emph{Learning to learn with gradients}.
\newblock University of California, Berkeley, 2018.

\bibitem[Foo et~al.(2008)Foo, Do, and Ng]{foo2008efficient}
Foo, C.-S., Do, C.~B., and Ng, A.~Y.
\newblock Efficient multiple hyperparameter learning for log-linear models.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  377--384, 2008.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock \emph{arXiv preprint arXiv:1703.01785}, 2017.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,
  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan,
  et~al.]{jaderberg2017population}
Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W.~M., Donahue, J.,
  Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., et~al.
\newblock Population-based training of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.09846}, 2017.

\bibitem[Jamieson \& Talwalkar(2016)Jamieson and Talwalkar]{jamieson2016non}
Jamieson, K. and Talwalkar, A.
\newblock Non-stochastic best arm identification and hyperparameter
  optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2016.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock {Auto-encoding variational Bayes}.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kochkov et~al.(2021)Kochkov, Smith, Alieva, Wang, Brenner, and
  Hoyer]{kochkov2021machine}
Kochkov, D., Smith, J.~A., Alieva, A., Wang, Q., Brenner, M.~P., and Hoyer, S.
\newblock Machine learning--accelerated computational fluid dynamics.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (21):\penalty0 e2101784118, 2021.

\bibitem[K{\"o}hl \& Willebrand(2002)K{\"o}hl and Willebrand]{kohl2002adjoint}
K{\"o}hl, A. and Willebrand, J.
\newblock An adjoint method for the assimilation of statistical characteristics
  into eddy-resolving ocean models.
\newblock \emph{Tellus A: Dynamic Meteorology and Oceanography}, 54\penalty0
  (4):\penalty0 406--425, 2002.

\bibitem[Larsen et~al.(1996)Larsen, Hansen, Svarer, and
  Ohlsson]{larsen1996design}
Larsen, J., Hansen, L.~K., Svarer, C., and Ohlsson, M.
\newblock {Design and regularization of neural networks: The optimal use of a
  validation set}.
\newblock In \emph{IEEE Signal Processing Society Workshop}, pp.\  62--71,
  1996.

\bibitem[Lea et~al.(2000)Lea, Allen, and Haine]{lea2000sensitivity}
Lea, D.~J., Allen, M.~R., and Haine, T.~W.
\newblock Sensitivity analysis of the climate of a chaotic system.
\newblock \emph{Tellus A: Dynamic Meteorology and Oceanography}, 52\penalty0
  (5):\penalty0 523--532, 2000.

\bibitem[Li \& Malik(2016)Li and Malik]{li2016learning}
Li, K. and Malik, J.
\newblock Learning to optimize.
\newblock \emph{arXiv preprint arXiv:1606.01885}, 2016.

\bibitem[Li \& Malik(2017)Li and Malik]{li2017learning}
Li, K. and Malik, J.
\newblock Learning to optimize neural nets.
\newblock \emph{arXiv preprint arXiv:1703.00441}, 2017.

\bibitem[Li et~al.(2017)Li, Jamieson, DeSalvo, Rostamizadeh, and
  Talwalkar]{li2017hyperband}
Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A.
\newblock {Hyperband: A novel bandit-based approach to hyperparameter
  optimization}.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6765--6816, 2017.

\bibitem[Li et~al.(2023)Li, Harrison, Sohl-Dickstein, and Metz]{li2023noise}
Li, O., Harrison, J., Sohl-Dickstein, J., and Metz, L.
\newblock Noise reuse in online evolution strategies.
\newblock In \emph{International Conference on Machine Learning}, 2023.
\newblock Under review.

\bibitem[Lorraine \& Duvenaud(2018)Lorraine and
  Duvenaud]{lorraine2018stochastic}
Lorraine, J. and Duvenaud, D.
\newblock Stochastic hyperparameter optimization through hypernetworks.
\newblock \emph{arXiv preprint arXiv:1802.09419}, 2018.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and
  Duvenaud]{lorraine2020optimizing}
Lorraine, J., Vicol, P., and Duvenaud, D.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  1540--1552, 2020.

\bibitem[Luketina et~al.(2016)Luketina, Berglund, Greff, and
  Raiko]{luketina2016scalable}
Luketina, J., Berglund, M., Greff, K., and Raiko, T.
\newblock Scalable gradient-based tuning of continuous regularization
  hyperparameters.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2952--2960, 2016.

\bibitem[MacKay et~al.(2019)MacKay, Vicol, Lorraine, Duvenaud, and
  Grosse]{mackay2019self}
MacKay, M., Vicol, P., Lorraine, J., Duvenaud, D., and Grosse, R.
\newblock {Self-Tuning Networks: Bilevel optimization of hyperparameters using
  structured best-response functions}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2113--2122, 2015.

\bibitem[Maheswaranathan et~al.(2019)Maheswaranathan, Metz, Tucker, Choi, and
  Sohl-Dickstein]{maheswaranathan2019guided}
Maheswaranathan, N., Metz, L., Tucker, G., Choi, D., and Sohl-Dickstein, J.
\newblock Guided evolutionary strategies: Augmenting random search with
  surrogate gradients.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4264--4273. PMLR, 2019.

\bibitem[Mania et~al.(2018)Mania, Guy, and Recht]{mania2018simple}
Mania, H., Guy, A., and Recht, B.
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.07055}, 2018.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and
  Santorini]{marcus1993building}
Marcus, M.~P., Marcinkiewicz, M.~A., and Santorini, B.
\newblock {Building a large annotated corpus of English: The Penn Treebank}.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[McGreivy et~al.(2021)McGreivy, Hudson, and Zhu]{mcgreivy2021optimized}
McGreivy, N., Hudson, S.~R., and Zhu, C.
\newblock Optimized finite-build stellarator coils using automatic
  differentiation.
\newblock \emph{Nuclear Fusion}, 61\penalty0 (2):\penalty0 026020, 2021.

\bibitem[Menick et~al.(2021)Menick, Elsen, Evci, Osindero, Simonyan, and
  Graves]{menick2021practical}
Menick, J., Elsen, E., Evci, U., Osindero, S., Simonyan, K., and Graves, A.
\newblock Practical real time recurrent learning with a sparse approximation.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=q3KSThy2GwB}.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2017regularizing}
Merity, S., Keskar, N.~S., and Socher, R.
\newblock {Regularizing and optimizing LSTM language models}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Metz et~al.(2018)Metz, Maheswaranathan, Cheung, and
  Sohl-Dickstein]{metz2018meta}
Metz, L., Maheswaranathan, N., Cheung, B., and Sohl-Dickstein, J.
\newblock Meta-learning update rules for unsupervised representation learning.
\newblock \emph{arXiv preprint arXiv:1804.00222}, 2018.

\bibitem[Metz et~al.(2019)Metz, Maheswaranathan, Nixon, Freeman, and
  Sohl-Dickstein]{metz2019understanding}
Metz, L., Maheswaranathan, N., Nixon, J., Freeman, D., and Sohl-Dickstein, J.
\newblock Understanding and correcting pathologies in the training of learned
  optimizers.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4556--4565, 2019.

\bibitem[Metz et~al.(2020{\natexlab{a}})Metz, Maheswaranathan, Freeman, Poole,
  and Sohl-Dickstein]{metz2020tasks}
Metz, L., Maheswaranathan, N., Freeman, C.~D., Poole, B., and Sohl-Dickstein,
  J.
\newblock {Tasks, stability, architecture, and compute: Training more effective
  learned optimizers, and using them to train themselves}.
\newblock \emph{arXiv preprint arXiv:2009.11243}, 2020{\natexlab{a}}.

\bibitem[Metz et~al.(2020{\natexlab{b}})Metz, Maheswaranathan, Sun, Freeman,
  Poole, and Sohl-Dickstein]{metz2020using}
Metz, L., Maheswaranathan, N., Sun, R., Freeman, C.~D., Poole, B., and
  Sohl-Dickstein, J.
\newblock Using a thousand optimization tasks to learn hyperparameter search
  strategies.
\newblock \emph{arXiv preprint arXiv:2002.11887}, 2020{\natexlab{b}}.

\bibitem[Metz et~al.(2021)Metz, Freeman, Schoenholz, and
  Kachman]{metz2021gradients}
Metz, L., Freeman, C.~D., Schoenholz, S.~S., and Kachman, T.
\newblock Gradients are not all you need.
\newblock \emph{arXiv preprint arXiv:2111.05803}, 2021.

\bibitem[Micaelli \& Storkey(2020)Micaelli and Storkey]{micaelli2020non}
Micaelli, P. and Storkey, A.
\newblock Non-greedy gradient-based hyperparameter optimization over long
  horizons.
\newblock \emph{arXiv preprint arXiv:2007.07869}, 2020.

\bibitem[Mujika et~al.(2018)Mujika, Meier, and Steger]{mujika2018approximating}
Mujika, A., Meier, F., and Steger, A.
\newblock {Approximating real-time recurrent learning with random Kronecker
  factors}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  6594--6603, 2018.

\bibitem[Nesterov \& Spokoiny(2017)Nesterov and Spokoiny]{nesterov2017random}
Nesterov, Y. and Spokoiny, V.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17\penalty0
  (2):\penalty0 527--566, 2017.

\bibitem[Ni \& Wang(2017)Ni and Wang]{ni2017sensitivity}
Ni, A. and Wang, Q.
\newblock {Sensitivity analysis on chaotic dynamical systems by Non-Intrusive
  Least Squares Shadowing (NILSS)}.
\newblock \emph{Journal of Computational Physics}, 347:\penalty0 56--77, 2017.

\bibitem[Owen(2013)]{mcbook}
Owen, A.~B.
\newblock \emph{Monte Carlo Theory, Methods and Examples}.
\newblock 2013.

\bibitem[Parmas \& Sugiyama(2019)Parmas and Sugiyama]{parmas2019unified}
Parmas, P. and Sugiyama, M.
\newblock A unified view of likelihood ratio and reparameterization gradients
  and an optimal importance sampling scheme.
\newblock \emph{arXiv preprint arXiv:1910.06419}, 2019.

\bibitem[Parmas et~al.(2018)Parmas, Rasmussen, Peters, and
  Doya]{parmas2018pipps}
Parmas, P., Rasmussen, C.~E., Peters, J., and Doya, K.
\newblock {PIPPS: Flexible model-based policy search robust to the curse of
  chaos}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4062--4071, 2018.

\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}
Pedregosa, F.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  737--746, 2016.

\bibitem[Rechenberg(1973)]{rechenberg1973}
Rechenberg, I.
\newblock \emph{{Evolutionsstrategie: Optimierung technischer Systeme nach
  Prinzipien der biologischen Evolution}}.
\newblock Stuttgart: Frommann-Holzboog, 1973.

\bibitem[Ruiz et~al.(2016)Ruiz, AUEB, Blei, et~al.]{ruiz2016generalized}
Ruiz, F.~R., AUEB, T.~R., Blei, D., et~al.
\newblock The generalized reparameterization gradient.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Rumelhart et~al.(1985)Rumelhart, Hinton, and
  Williams]{rumelhart1985learning}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California University San Diego, La Jolla Institute
  for Cognitive Science, 1985.

\bibitem[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and
  Sutskever]{salimans2017evolution}
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1703.03864}, 2017.

\bibitem[Schulman et~al.(2015)Schulman, Heess, Weber, and
  Abbeel]{schulman2015gradient}
Schulman, J., Heess, N., Weber, T., and Abbeel, P.
\newblock Gradient estimation using stochastic computation graphs.
\newblock \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Schwefel \& Schwefel(1977)Schwefel and
  Schwefel]{schwefel1977evolutionsstrategien}
Schwefel, H.-P. and Schwefel, H.-P.
\newblock \emph{{Evolutionsstrategien f{\"u}r die numerische Optimierung}}.
\newblock Springer, 1977.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and
  Boots]{shaban2019truncated}
Shaban, A., Cheng, C.-A., Hatch, N., and Boots, B.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  1723--1732, 2019.

\bibitem[Silver et~al.(2021)Silver, Goyal, Danihelka, Hessel, and van
  Hasselt]{silver2021learning}
Silver, D., Goyal, A., Danihelka, I., Hessel, M., and van Hasselt, H.
\newblock Learning by directional gradient descent.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock {Practical Bayesian optimization of machine learning algorithms}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  2951--2959, 2012.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{snoek2015scalable}
Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N.,
  Patwary, M., Prabhat, M., and Adams, R.
\newblock {Scalable Bayesian optimization using deep neural networks}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2171--2180, 2015.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock {Dropout: A simple way to prevent neural networks from overfitting}.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Staines \& Barber(2012)Staines and Barber]{staines2012variational}
Staines, J. and Barber, D.
\newblock Variational optimization.
\newblock \emph{arXiv preprint arXiv:1212.4507}, 2012.

\bibitem[Swersky et~al.(2014)Swersky, Snoek, and Adams]{swersky2014freeze}
Swersky, K., Snoek, J., and Adams, R.~P.
\newblock {Freeze-thaw Bayesian optimization}.
\newblock \emph{arXiv preprint arXiv:1406.3896}, 2014.

\bibitem[Tallec \& Ollivier(2017{\natexlab{a}})Tallec and
  Ollivier]{tallec2017unbiased}
Tallec, C. and Ollivier, Y.
\newblock Unbiased online recurrent optimization.
\newblock \emph{arXiv preprint arXiv:1702.05043}, 2017{\natexlab{a}}.

\bibitem[Tallec \& Ollivier(2017{\natexlab{b}})Tallec and
  Ollivier]{tallec2017unbiasing}
Tallec, C. and Ollivier, Y.
\newblock Unbiasing truncated backpropagation through time.
\newblock \emph{arXiv preprint arXiv:1705.08209}, 2017{\natexlab{b}}.

\bibitem[Vicol et~al.(2021)Vicol, Metz, and Sohl-Dickstein]{vicol2021unbiased}
Vicol, P., Metz, L., and Sohl-Dickstein, J.
\newblock Unbiased gradient estimation in unrolled computation graphs with
  persistent evolution strategies.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  10553--10563, 2021.

\bibitem[Vicol et~al.(2022)Vicol, Lorraine, Pedregosa, Duvenaud, and
  Grosse]{vicol2022implicit}
Vicol, P., Lorraine, J., Pedregosa, F., Duvenaud, D., and Grosse, R.
\newblock {On Implicit Bias in Overparameterized Bilevel Optimization}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Vicol(2023)]{vicol2023bilevel}
Vicol, P.~A.
\newblock \emph{On Bilevel Optimization without Full Unrolls: Methods and
  Applications}.
\newblock PhD thesis, University of Toronto (Canada), 2023.

\bibitem[Werbos(1990)]{werbos1990backpropagation}
Werbos, P.~J.
\newblock {Backpropagation through time: What it does and how to do it}.
\newblock \emph{Proceedings of the IEEE}, 78\penalty0 (10):\penalty0
  1550--1560, 1990.

\bibitem[Wichrowska et~al.(2017)Wichrowska, Maheswaranathan, Hoffman,
  Colmenarejo, Denil, de~Freitas, and Sohl-Dickstein]{wichrowska2017learned}
Wichrowska, O., Maheswaranathan, N., Hoffman, M.~W., Colmenarejo, S.~G., Denil,
  M., de~Freitas, N., and Sohl-Dickstein, J.
\newblock Learned optimizers that scale and generalize.
\newblock \emph{arXiv preprint arXiv:1703.04813}, 2017.

\bibitem[Wierstra et~al.(2014)Wierstra, Schaul, Glasmachers, Sun, Peters, and
  Schmidhuber]{wierstra2014natural}
Wierstra, D., Schaul, T., Glasmachers, T., Sun, Y., Peters, J., and
  Schmidhuber, J.
\newblock Natural evolution strategies.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 949--980, 2014.

\bibitem[Williams \& Peng(1990)Williams and Peng]{williams1990efficient}
Williams, R.~J. and Peng, J.
\newblock An efficient gradient-based algorithm for on-line training of
  recurrent network trajectories.
\newblock \emph{Neural Computation}, 2\penalty0 (4):\penalty0 490--501, 1990.

\bibitem[Williams \& Zipser(1989)Williams and Zipser]{williams1989learning}
Williams, R.~J. and Zipser, D.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural Computation}, 1\penalty0 (2):\penalty0 270--280, 1989.

\bibitem[Wu et~al.(2018)Wu, Ren, Liao, and Grosse]{wu2018understanding}
Wu, Y., Ren, M., Liao, R., and Grosse, R.
\newblock Understanding short-horizon bias in stochastic meta-optimization.
\newblock \emph{arXiv preprint arXiv:1803.02021}, 2018.

\end{thebibliography}
