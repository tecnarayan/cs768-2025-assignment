@article{trigila2016data,
  title={Data-driven optimal transport},
  author={Trigila, Giulio and Tabak, Esteban G},
  journal={Communications on Pure and Applied Mathematics},
  volume={69},
  number={4},
  pages={613--648},
  year={2016},
  publisher={Wiley Online Library}
}

@book{sarkka2019applied,
  title={Applied stochastic differential equations},
  author={S{\"a}rkk{\"a}, Simo and Solin, Arno},
  volume={10},
  year={2019},
  publisher={Cambridge University Press}
}

@article{karras2022elucidating,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={arXiv preprint arXiv:2206.00364},
  year={2022}
}

@article{kovachki2020conditional,
  title={Conditional sampling with monotone GANs},
  author={Kovachki, Nikola and Baptista, Ricardo and Hosseini, Bamdad and Marzouk, Youssef},
  journal={arXiv preprint arXiv:2006.06755},
  year={2020}
}

@article{brenier1991polar,
  title={Polar factorization and monotone rearrangement of vector-valued functions},
  author={Brenier, Yann},
  journal={Communications on pure and applied mathematics},
  volume={44},
  number={4},
  pages={375--417},
  year={1991},
  publisher={Wiley Online Library}
}

@inproceedings{song2020score,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year={2020},
  booktitle={International Conference on Learning Representations}
}

@article{pooladian2021entropic,
  title={Entropic estimation of optimal transport maps},
  author={Pooladian, Aram-Alexandre and Niles-Weed, Jonathan},
  journal={arXiv preprint arXiv:2109.12004},
  year={2021}
}

@article{perrot2016mapping,
  title={Mapping estimation for discrete optimal transport},
  author={Perrot, Micha{\"e}l and Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{makkuva2020optimal,
  title={Optimal transport mapping via input convex neural networks},
  author={Makkuva, Ashok and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={6672--6681},
  year={2020},
  organization={PMLR}
}


@article{marzouk2016introduction,
  title={An introduction to sampling via measure transport},
  author={Marzouk, Youssef and Moselhy, Tarek and Parno, Matthew and Spantini, Alessio},
  journal={arXiv preprint arXiv:1602.05023},
  year={2016}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{gelu,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@book{Pinkus:n_width2012,
booktitle={n-Widths in Approximation Theory.},
title={Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge / A Series of Modern Surveys in Mathematics},
author={Pinkus, A.},
volume={1},
year={2012},
publisher={pringer Berlin Heidelberg,}
}

@misc{li2020fourier,
      title={Fourier Neural Operator for Parametric Partial Differential Equations}, 
      author={Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
      year={2020},
      eprint={2010.08895},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{Dresdner:2020ml_spectral,
      title={Learning to correct spectral methods for simulating turbulent flows}, 
      author={ Dresdner, Gideon and   Kochkov, Dmitrii and  Norgaard, Peter and  Zepeda-N{\'u}{\~{n}}ez, Leonardo and Smith, Jamie A. and   Brenner, Michael P. and  Hoyer, Stephan},
      year={2022},
      eprint={2207.00556},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Ayed_Gallinari:2019,
      title={Learning Dynamical Systems from Partial Observations}, 
      author={Ibrahim Ayed and Emmanuel de Bezenac and Arthur Pajot and Julien Brajard and Patrick Gallinari},
      year={2019},
      eprint={1902.11136 },
      archivePrefix={arXiv},
      primaryClass={cs.SY}
}


@article{Diez_Muixi:2021,
author = {Díez, Pedro and Muixí, Alba and Zlotnik, Sergio and García-González, Alberto},
title = {Nonlinear dimensionality reduction for parametric problems: A kernel proper orthogonal decomposition},
journal = {International Journal for Numerical Methods in Engineering},
volume = {122},
number = {24},
pages = {7306-7327},
keywords = {kPCA, nonlinear multidimensionality reduction, parametric problems, reduced-order models},
doi = {https://doi.org/10.1002/nme.6831},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.6831},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.6831},
abstract = {Abstract Reduced-order models are essential tools to deal with parametric problems in the context of optimization, uncertainty quantification, or control and inverse problems. The set of parametric solutions lies in a low-dimensional manifold (with dimension equal to the number of independent parameters) embedded in a large-dimensional space (dimension equal to the number of degrees of freedom of the full-order discrete model). A posteriori model reduction is based on constructing a basis from a family of snapshots (solutions of the full-order model computed offline), and then use this new basis to solve the subsequent instances online. Proper orthogonal decomposition (POD) reduces the problem into a linear subspace of lower dimension, eliminating redundancies in the family of snapshots. The strategy proposed here is to use a nonlinear dimensionality reduction technique, namely, the kernel principal component analysis (kPCA), in order to find a nonlinear manifold, with an expected much lower dimension, and to solve the problem in this low-dimensional manifold. Guided by this paradigm, the methodology devised here introduces different novel ideas, namely, 1) characterizing the nonlinear manifold using local tangent spaces, where the reduced-order problem is linear and based on the neighboring snapshots, 2) the approximation space is enriched with the cross-products of the snapshots, introducing a quadratic description, 3) the kernel for kPCA is defined ad hoc, based on physical considerations, and 4) the iterations in the reduced-dimensional space are performed using an algorithm based on a Delaunay tessellation of the cloud of snapshots in the reduced space. The resulting computational strategy is performing outstandingly in the numerical tests, alleviating many of the problems associated with POD and improving the numerical accuracy.},
year = {2021}
}



@article{Chen_Hesthaven:2016,
title = {Physics-informed machine learning for reduced-order modeling of nonlinear problems},
journal = {Journal of Computational Physics},
volume = {446},
pages = {110666},
year = {2021},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110666},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121005611},
author = {Wenqian Chen and Qian Wang and Jan S. Hesthaven and Chuhua Zhang},
keywords = {Physics-informed machine learning, Feedforward neural network, Reduced-order modeling, Nonlinear PDE},
abstract = {A reduced basis method based on a physics-informed machine learning framework is developed for efficient reduced-order modeling of parametrized partial differential equations (PDEs). A feedforward neural network is used to approximate the mapping from the time-parameter to the reduced coefficients. During the offline stage, the network is trained by minimizing the weighted sum of the residual loss of the reduced-order equations, and the data loss of the labeled reduced coefficients that are obtained via the projection of high-fidelity snapshots onto the reduced space. Such a network is referred to as physics-reinforced neural network (PRNN). As the number of residual points in time-parameter space can be very large, an accurate network – referred to as physics-informed neural network (PINN) – can be trained by minimizing only the residual loss. However, for complex nonlinear problems, the solution of the reduced-order equation is less accurate than the projection of high-fidelity solution onto the reduced space. Therefore, the PRNN trained with the snapshot data is expected to have higher accuracy than the PINN. Numerical results demonstrate that the PRNN is more accurate than the PINN and a purely data-driven neural network for complex problems. During the reduced basis refinement, the PRNN may obtain higher accuracy than the direct reduced-order model based on a Galerkin projection. The online evaluation of PINN/PRNN is orders of magnitude faster than that of the Galerkin reduced-order model.}
}


@article{Kim:2022,
title = {A fast and accurate physics-informed neural network reduced order model with shallow masked autoencoder},
journal = {Journal of Computational Physics},
volume = {451},
pages = {110841},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110841},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121007361},
author = {Youngkyu Kim and Youngsoo Choi and David Widemann and Tarek Zohdi},
keywords = {Nonlinear manifold solution representation, Physics-informed neural network, Reduced order model, Nonlinear dynamical system, Hyper-reduction},
abstract = {Traditional linear subspace reduced order models (LS-ROMs) are able to accelerate physical simulations in which the intrinsic solution space falls into a subspace with a small dimension, i.e., the solution space has a small Kolmogorov n-width. However, for physical phenomena not of this type, e.g., any advection-dominated flow phenomena such as in traffic flow, atmospheric flows, and air flow over vehicles, a low-dimensional linear subspace poorly approximates the solution. To address cases such as these, we have developed a fast and accurate physics-informed neural network ROM, namely nonlinear manifold ROM (NM-ROM), which can better approximate high-fidelity model solutions with a smaller latent space dimension than the LS-ROMs. Our method takes advantage of the existing numerical methods that are used to solve the corresponding full order models. The efficiency is achieved by developing a hyper-reduction technique in the context of the NM-ROM. Numerical results show that neural networks can learn a more efficient latent space representation on advection-dominated data from 1D and 2D Burgers' equations. A speedup of up to 2.6 for 1D Burgers' and a speedup of 11.7 for 2D Burgers' equations are achieved with an appropriate treatment of the nonlinear terms through a hyper-reduction technique. Finally, a posteriori error bounds for the NM-ROMs are derived that take account of the hyper-reduced operators.}
}

@article{Benner_Wilcox:2015,
author = {Benner, Peter and Gugercin, Serkan and Willcox, Karen},
title = {A Survey of Projection-Based Model Reduction Methods for Parametric Dynamical Systems},
journal = {SIAM Review},
volume = {57},
number = {4},
pages = {483-531},
year = {2015},
doi = {10.1137/130932715},
URL = {  https://doi.org/10.1137/130932715},
eprint = {https://doi.org/10.1137/130932715},
abstract = { Numerical simulation of large-scale dynamical systems plays a fundamental role in studying a wide range of complex physical phenomena; however, the inherent large-scale nature of the models often leads to unmanageable demands on computational resources. Model reduction aims to reduce this computational burden by generating reduced models that are faster and cheaper to simulate, yet accurately represent the original large-scale system behavior. Model reduction of linear, nonparametric dynamical systems has reached a considerable level of maturity, as reflected by several survey papers and books. However, parametric model reduction has emerged only more recently as an important and vibrant research area, with several recent advances making a survey paper timely. Thus, this paper aims to provide a resource that draws together recent contributions in different communities to survey the state of the art in parametric model reduction methods. Parametric model reduction targets the broad class of problems for which the equations governing the system behavior depend on a set of parameters. Examples include parameterized partial differential equations and large-scale systems of parameterized ordinary differential equations. The goal of parametric model reduction is to generate low-cost but accurate models that characterize system response for different values of the parameters. This paper surveys state-of-the-art methods in projection-based parametric model reduction, describing the different approaches within each class of methods for handling parametric variation and providing a comparative discussion that lends insights to potential advantages and disadvantages in applying each of the methods. We highlight the important role played by parametric model reduction in design, control, optimization, and uncertainty quantification---settings that require repeated model evaluations over different parameter values. }
}

@article{stachenfeld_learned_2022,
	title = {Learned {Coarse} {Models} for {Efficient} {Turbulence} {Simulation}},
	url = {http://arxiv.org/abs/2112.15275},
	abstract = {Turbulence simulation with classical numerical solvers requires very high-resolution grids to accurately resolve dynamics. Here we train learned simulators at low spatial and temporal resolutions to capture turbulent dynamics generated at high resolution. We show that our proposed model can simulate turbulent dynamics more accurately than classical numerical solvers at the same low resolutions across various scientifically relevant metrics. Our model is trained end-to-end from data and is capable of learning a range of challenging chaotic and turbulent dynamics at low resolution, including trajectories generated by the state-of-the-art Athena++ engine. We show that our simpler, general-purpose architecture outperforms various more specialized, turbulence-specific architectures from the learned turbulence simulation literature. In general, we see that learned simulators yield unstable trajectories; however, we show that tuning training noise and temporal downsampling solves this problem. We also find that while generalization beyond the training distribution is a challenge for learned models, training noise, convolutional architectures, and added loss constraints can help. Broadly, we conclude that our learned simulator outperforms traditional solvers run on coarser grids, and emphasize that simple design choices can offer stability and robust generalization.},
	urldate = {2022-03-30},
	journal = {arXiv:2112.15275 [physics]},
	author = {Stachenfeld, Kimberly and Fielding, Drummond B. and Kochkov, Dmitrii and Cranmer, Miles and Pfaff, Tobias and Godwin, Jonathan and Cui, Can and Ho, Shirley and Battaglia, Peter and Sanchez-Gonzalez, Alvaro},
	month = jan,
	year = {2022},
	note = {arXiv: 2112.15275},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Physics - Fluid Dynamics},
}

@article{bar-sinai_learning_2019,
	title = {Learning data-driven discretizations for partial differential equations},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1814058116},
	doi = {10.1073/pnas.1814058116},
	abstract = {Significance 
            In many physical systems, the governing equations are known with high confidence, but direct numerical solution is prohibitively expensive. Often this situation is alleviated by writing effective equations to approximate dynamics below the grid scale. This process is often impossible to perform analytically and is often ad hoc. Here we propose data-driven discretization, a method that uses machine learning to systematically derive discretizations for continuous physical systems. On a series of model problems, data-driven discretization gives accurate solutions with a dramatic drop in required resolution. 
          ,  
            The numerical solution of partial differential equations (PDEs) is challenging because of the need to resolve spatiotemporal features over wide length- and timescales. Often, it is computationally intractable to resolve the finest features in the solution. The only recourse is to use approximate coarse-grained representations, which aim to accurately represent long-wavelength dynamics while properly accounting for unresolved small-scale physics. Deriving such coarse-grained equations is notoriously difficult and often ad hoc. Here we introduce data-driven discretization, a method for learning optimized approximations to PDEs based on actual solutions to the known underlying equations. Our approach uses neural networks to estimate spatial derivatives, which are optimized end to end to best satisfy the equations on a low-resolution grid. The resulting numerical methods are remarkably accurate, allowing us to integrate in time a collection of nonlinear equations in 1 spatial dimension at resolutions 4× to 8× coarser than is possible with standard finite-difference methods.},
	language = {en},
	number = {31},
	urldate = {2022-03-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bar-Sinai, Yohai and Hoyer, Stephan and Hickey, Jason and Brenner, Michael P.},
	month = jul,
	year = {2019},
	pages = {15344--15349},
}

@ARTICLE{kochkov_machine_2021,
  title     = "Machine learning--accelerated computational fluid dynamics",
  author    = "Kochkov, Dmitrii and Smith, Jamie A and Alieva, Ayya and Wang,
               Qing and Brenner, Michael P and Hoyer, Stephan",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Academy of Sciences",
  volume    =  118,
  number    =  21,
  month     =  may,
  year      =  2021,
  url       = "https://www.pnas.org/content/118/21/e2101784118",
  language  = "en"
}

@inproceedings{meng2021sdedit,
  title={{SDE}dit: Guided image synthesis and editing with stochastic differential equations},
  author={Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@phdthesis{papadakis2015optimal,
  title={Optimal transport for image processing},
  author={Papadakis, Nicolas},
  year={2015},
  school={Universit{\'e} de Bordeaux}
}

@article{peyre2019computational,
  title={Computational optimal transport: With applications to data science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{zhao2022egsde,
  title={Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations},
  author={Zhao, Min and Bao, Fan and Li, Chongxuan and Zhu, Jun},
  journal={arXiv preprint arXiv:2207.06635},
  year={2022}
}

@article{courty2017joint,
  title={Joint distribution optimal transportation for domain adaptation},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury and Rakotomamonjy, Alain},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{flamary2016optimal,
  title={Optimal transport for domain adaptation},
  author={Flamary, R and Courty, N and Tuia, D and Rakotomamonjy, A},
  journal={IEEE Trans. Pattern Anal. Mach. Intell},
  volume={1},
  year={2016}
}

@inproceedings{park2020contrastive,
  title={Contrastive learning for unpaired image-to-image translation},
  author={Park, Taesung and Efros, Alexei A and Zhang, Richard and Zhu, Jun-Yan},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part IX 16},
  pages={319--345},
  year={2020},
  organization={Springer}
}

@article{wu2022unifying,
  title={Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance},
  author={Wu, Chen Henry and De la Torre, Fernando},
  journal={arXiv preprint arXiv:2210.05559},
  year={2022}
}

@article{choi2021ilvr,
  title={Ilvr: Conditioning method for denoising diffusion probabilistic models},
  author={Choi, Jooyoung and Kim, Sungwon and Jeong, Yonghyun and Gwon, Youngjune and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2108.02938},
  year={2021}
}

@article{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2022-03-30},
	journal = {arXiv:2010.08895 [cs, math]},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv: 2010.08895},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{list_learned_2022,
	title = {Learned {Turbulence} {Modelling} with {Differentiable} {Fluid} {Solvers}},
	url = {http://arxiv.org/abs/2202.06988},
	abstract = {In this paper, we train turbulence models based on convolutional neural networks. These learned turbulence models improve under-resolved low resolution solutions to the incompressible Navier-Stokes equations at simulation time. Our method involves the development of a differentiable numerical solver that supports the propagation of optimisation gradients through multiple solver steps. We showcase the significance of this property by demonstrating the superior stability and accuracy of those models that featured a higher number of unrolled steps during training. This approach is applied to three two-dimensional turbulence flow scenarios, a homogeneous decaying turbulence case, a temporally evolving mixing layer and a spatially evolving mixing layer. Our method achieves significant improvements of long-term {\textbackslash}textit\{a-posteriori\} statistics when compared to no-model simulations, without requiring these statistics to be directly included in the learning targets. At inference time, our proposed method also gains substantial performance improvements over similarly accurate, purely numerical methods.},
	urldate = {2022-03-30},
	journal = {arXiv:2202.06988 [physics]},
	author = {List, Björn and Chen, Li-Wei and Thuerey, Nils},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.06988},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Physics - Fluid Dynamics},
}


@InProceedings{ronneberger_u-net_2015,
author="Ronneberger, Olaf
and Fischer, Philipp
and Brox, Thomas",
editor="Navab, Nassir
and Hornegger, Joachim
and Wells, William M.
and Frangi, Alejandro F.",
title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="234--241",
abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
isbn="978-3-319-24574-4"
}

@INPROCEEDINGS{wang_towards_2020,
  title     = "Towards Physics-informed Deep Learning for Turbulent Flow
               Prediction",
  booktitle = "Proceedings of the 26th {ACM} {SIGKDD} International Conference
               on Knowledge Discovery \& Data Mining",
  author    = "Wang, Rui and Kashinath, Karthik and Mustafa, Mustafa and
               Albert, Adrian and Yu, Rose",
  year      =  2020,
  url       = "http://dx.doi.org/10.1145/3394486.3403198"
}


@article{sanchez-gonzalez_learning_2020,
	title = {Learning to {Simulate} {Complex} {Physics} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2002.09405},
	abstract = {Here we present a machine learning framework and model implementation that can learn to simulate a wide variety of challenging physical domains, involving ﬂuids, rigid solids, and deformable materials interacting with one another. Our framework—which we term “Graph Network-based Simulators” (GNS)—represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework advances the state-of-the-art in learned physical simulation, and holds promise for solving a wide range of complex forward and inverse problems.},
	language = {en},
	urldate = {2021-06-16},
	journal = {arXiv:2002.09405 [physics, stat]},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	month = sep,
	year = {2020},
	note = {arXiv: 2002.09405},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
}


@book{Hormander:FIO,
  author = {H\"ormander, Lars},
  title = {The Analysis of Linear Partial Differential Operators. {IV}: {F}ourier Integral Operators},
  series = {Classics in Mathematics},
  publisher = {Springer},
  address = {Berlin},
  year = {2009},
  volume = {63},
  annote = {book},
}


@book{Hormander:PseudoDiff,
  author = {H\"ormander, Lars},
  title = {The Analysis of Linear Partial Differential Operators. {III}: Pseudo-Differential Operators},
  series = {Classics in Mathematics},
  publisher = {Springer},
  address = {Berlin},
  year = {2007},
  volume = {63},
  annote = {book},
}

@article{tran_factorized_2021,
 abstract = {The Fourier Neural Operator (FNO) is a learning-based method for efficiently simulating partial differential equations. We propose the Factorized Fourier Neural Operator (F-FNO) that allows much better generalization with deeper networks. With a careful combination of the Fourier factorization, a shared kernel integral operator across all layers, the Markov property, and residual connections, F-FNOs achieve a six-fold reduction in error on the most turbulent setting of the Navier-Stokes benchmark dataset. We show that our model maintains an error rate of 2\% while still running an order of magnitude faster than a numerical solver, even when the problem setting is extended to include additional contexts such as viscosity and time-varying forces. This enables the same pretrained neural network to model vastly different conditions.},
 author = {Tran, Alasdair and Mathews, Alexander and Xie, Lexing and Ong, Cheng Soon},
 journal = {arXiv:2111.13802 [cs]},
 keywords = {Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning},
 month = {November},
 note = {arXiv: 2111.13802},
 title = {\href{http://arxiv.org/abs/2111.13802}{Factorized {Fourier} {Neural} {Operators}}},
 urldate = {2022-04-26},
 year = {2021}
}


@article{FanYing:mnnh2019,
 author = {Fan, Yuwei and Feliu-Fab{\`a}, Jordi and Lin, Lin and Ying, Lexing and Zepeda-N{\'u}{\~{n}}ez, Leonardo},
 day = {07},
 doi = {10.1007/s40687-019-0183-3},
 issn = {2197-9847},
 journal = {Research in the Mathematical Sciences},
 month = {Mar.},
 title = {A multiscale neural network based on hierarchical nested bases},
 volume = {6},
 year = {2019}
}


@article{Frezat2022-fs,
 archiveprefix = {arXiv},
 author = {Frezat, Hugo and Le Sommer, Julien and Fablet, Ronan and
Balarac, Guillaume and Lguensat, Redouane},
 eprint = {2204.03911},
 journal = {arXiv},
 month = {April},
 primaryclass = {physics.flu-dyn},
 title = {\href{http://arxiv.org/abs/2204.03911}{A posteriori learning for quasi-geostrophic turbulence
parametrization}},
 year = {2022}
}

@article{Gao:2022_Galerkin_PINN,
 abstract = {Despite the great promise of the physics-informed neural networks (PINNs) in solving forward and inverse problems, several technical challenges are present as roadblocks for more complex and realistic applications. First, most existing PINNs are based on point-wise formulation with fully-connected networks to learn continuous functions, which suffer from poor scalability and hard boundary enforcement. Second, the infinite search space over-complicates the non-convex optimization for network training. Third, although the convolutional neural network (CNN)-based discrete learning can significantly improve training efficiency, CNNs struggle to handle irregular geometries with unstructured meshes. To properly address these challenges, we present a novel discrete PINN framework based on graph convolutional network (GCN) and variational structure of PDE to solve forward and inverse partial differential equations (PDEs) in a unified manner. The use of a piecewise polynomial basis can reduce the dimension of search space and facilitate training and convergence. Without the need of tuning penalty parameters in classic PINNs, the proposed method can strictly impose boundary conditions and assimilate sparse data in both forward and inverse settings. The flexibility of GCNs is leveraged for irregular geometries with unstructured meshes. The effectiveness and merit of the proposed method are demonstrated over a variety of forward and inverse computational mechanics problems governed by both linear and nonlinear PDEs.},
 author = {Han Gao and Matthew J. Zahr and Jian-Xun Wang},
 doi = {https://doi.org/10.1016/j.cma.2021.114502},
 issn = {0045-7825},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {Partial differential equations, Inverse problem, Physics-informed machine learning, Graph convolutional neural networks, Mechanics},
 title = {\href{https://www.sciencedirect.com/science/article/pii/S0045782521007076}{Physics-informed graph neural Galerkin networks: A unified framework for solving PDE-governed forward and inverse problems}},
 volume = {390},
 year = {2022}
}


@inproceedings{hyperpinn,
 author = {Filipe de Avila Belbute-Peres and Yi-fan Chen and Fei Sha},
 booktitle = {The Symbiosis of Deep Learning and Differential Equations},
 title = {\href{https://openreview.net/forum?id=LxUuRDUhRjM}{Hyper{PINN}: Learning parameterized differential equations with physics-informed hypernetworks}},
 year = {2021}
}


@book{Implementing_SM_PDEs:2009,
 abstract = {This book offers a systematic and self-contained approach to solve partial differential equations numerically using single and multidomain spectralmethods. It contains detailed algorithms in pseudocode for the applicationof spectral approximations to both one and two dimensional PDEsof mathematical physics describing potentials,transport, and wave propagation. David Kopriva, a well-known researcher in the field with extensive practical experience, shows how only a few fundamental algorithms form the building blocks of any spectral code, even for problems with complex geometries. The book addresses computational and applications scientists, as it emphasizes the practical derivation and implementation of spectral methods over abstract mathematics. It is divided into two parts: First comes a primer on spectral approximation and the basic algorithms, including FFT algorithms, Gaussquadrature algorithms, and how to approximate derivatives. The secondpart shows how to use those algorithms to solve steady and time dependent PDEs in one and two space dimensions. Exercises and questions at theend of each chapter encourage the reader to experiment with thealgorithms.},
 author = {Kopriva, David A.},
 edition = {1st},
 isbn = {9048122600},
 publisher = {Springer Publishing Company, Incorporated},
 title = {Implementing Spectral Methods for Partial Differential Equations: Algorithms for Scientists and Engineers},
 year = {2009}
}


@article{bruno_fc-based_2021,
 abstract = {This paper presents a spectral scheme for the numerical solution of nonlinear conservation laws in non-periodic domains under arbitrary boundary conditions. The approach relies on the use of the Fourier Continuation (FC) method for spectral representation of non-periodic functions in conjunction with smooth localized artificial viscosity assignments produced by means of a Shock-Detecting Neural Network (SDNN). Like previous shock capturing schemes and artificial viscosity techniques, the combined FC-SDNN strategy effectively controls spurious oscillations in the proximity of discontinuities. Thanks to its use of a localized but smooth artificial viscosity term, whose support is restricted to a vicinity of flow-discontinuity points, the algorithm enjoys spectral accuracy and low dissipation away from flow discontinuities, and, in such regions, it produces smooth numerical solutions -- as evidenced by an essential absence of spurious oscillations in level set lines. The FC-SDNN viscosity assignment, which does not require use of problem-dependent algorithmic parameters, induces a significantly lower overall dissipation than other methods, including the Fourier-spectral versions of the previous entropy viscosity method. The character of the proposed algorithm is illustrated with a variety of numerical results for the linear advection, Burgers and Euler equations in one and two-dimensional non-periodic spatial domains.},
 author = {Bruno, Oscar P. and Hesthaven, Jan S. and Leibovici, Daniel V.},
 journal = {arXiv:2111.01315 [cs, math]},
 keywords = {65M70, G.1.1, G.1.2, G.1.8, Mathematics - Numerical Analysis},
 month = {November},
 note = {arXiv: 2111.01315},
 title = {\href{http://arxiv.org/abs/2111.01315}{{FC}-based shock-dynamics solver with neural-network localized artificial-viscosity assignment}},
 urldate = {2022-04-08},
 year = {2021}
}

@article{aubry_holmes_lumley_stone_1988,
title={The dynamics of coherent structures in the wall region of a turbulent boundary layer},
volume={192},
DOI={10.1017/S0022112088001818},
journal={Journal of Fluid Mechanics},
publisher={Cambridge University Press},
author={Aubry, Nadine and Holmes, Philip and Lumley, John L. and Stone, Emily},
year={1988}, 
pages={115–173}}


@article{Ahmed_Noack:2021,
author = {Ahmed,Shady E.  and Pawar,Suraj  and San,Omer  and Rasheed,Adil  and Iliescu,Traian  and Noack,Bernd R. },
title = {On closures for reduced order models—A spectrum of first-principle to machine-learned avenues},
journal = {Physics of Fluids},
volume = {33},
number = {9},
pages = {091301},
year = {2021},
doi = {10.1063/5.0061577},
URL = { https://doi.org/10.1063/5.0061577},
eprint = {https://doi.org/10.1063/5.0061577}}


@article{Ahmed_Iluescy:2021,
author = {Ahmed,Shady E.  and San,Omer  and Rasheed,Adil  and Iliescu,Traian },
title = {Nonlinear proper orthogonal decomposition for convection-dominated flows},
journal = {Physics of Fluids},
volume = {33},
number = {12},
pages = {121702},
year = {2021},
doi = {10.1063/5.0074310},

URL = { 
        https://doi.org/10.1063/5.0074310
    
},
eprint = { 
        https://doi.org/10.1063/5.0074310
    
}

}

@article{mishra_machine_2019,
	title = {A machine learning framework for data driven acceleration of computations of differential equations},
	volume = {1},
	copyright = {2019 The Author(s)},
	issn = {2640-3501},
	url = {https://www.aimspress.com/article/doi/10.3934/Mine.2018.1.118},
	doi = {10.3934/Mine.2018.1.118},
	abstract = {We propose a machine learning framework to accelerate numerical computations of time-dependent ODEs and PDEs. Our method is based on recasting (generalizations of) existing numerical methods as artificial neural networks, with a set of trainable parameters. These parameters are determined in an o ine training process by (approximately) minimizing suitable (possibly nonconvex) loss functions by (stochastic) gradient descent methods. The proposed algorithm is designed to be always consistent with the underlying di erential equation. Numerical experiments involving both linear and non-linear ODE and PDE model problems demonstrate a significant gain in computational e ciency over standard numerical methods.},
	language = {en},
	number = {1},
	urldate = {2022-04-26},
	journal = {Mathematics in Engineering},
	author = {Mishra, Siddhartha},
	year = {2018},
	pages = {118--146},
}

@article{Geelen_Wilcox:2022,
	title = {Operator inference for non-intrusive model reduction with nonlinear manifolds},
	url = {https://arxiv.org/abs/2205.02304},
	urldate = {2022-03-30},
	journal = {	arXiv:2205.02304 [math.NA]},
	author = { Geelen, Rudy and   Wright, Stephen and  Willcox, Karen},
	month = may,
	year = {2022},
	note = {arXiv: 2205.02304},
}

@article{Aubry_FirstPOD, 
title={The dynamics of coherent structures in the wall region of a turbulent boundary layer}, 
volume={192}, 
DOI={10.1017/S0022112088001818}, 
journal={Journal of Fluid Mechanics}, 
publisher={Cambridge University Press}, 
author={Aubry, Nadine and Holmes, Philip and Lumley, John L. and Stone, Emily}, 
year={1988}, 
pages={115–173}}

@article{PGD_shortreview,
  title={A short review on model order reduction based on proper generalized decomposition},
  author={Chinesta, Francisco and Ladeveze, Pierre and Cueto, Elias},
  journal={Archives of Computational Methods in Engineering},
  volume={18},
  number={4},
  pages={395--404},
  year={2011},
  publisher={Springer}
}

@article{Farhat_localbases,
  title={Nonlinear model order reduction based on local reduced-order bases},
  author={Amsallem, David and Zahr, Matthew J and Farhat, Charbel},
  journal={International Journal for Numerical Methods in Engineering},
  volume={92},
  number={10},
  pages={891--916},
  year={2012},
  publisher={Wiley Online Library}
}

@article{Galerkin,
title = {Series Occurring in Various Questions Concerning the Elastic Equilibrium of Rods and Plates},
journal = {Vestnik Inzhenernov i Tekhnikov},
volume = {19},
pages = {897-908},
year = {1915},
author = {B. G. Galerkin},
}


@article{EIM,
title = {An ‘empirical interpolation’ method: application to efficient reduced-basis discretization of partial differential equations},
journal = {Comptes Rendus Mathematique},
volume = {339},
number = {9},
pages = {667-672},
year = {2004},
issn = {1631-073X},
doi = {https://doi.org/10.1016/j.crma.2004.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1631073X04004248},
author = {Maxime Barrault and Yvon Maday and Ngoc Cuong Nguyen and Anthony T. Patera},
abstract = {We present an efficient reduced-basis discretization procedure for partial differential equations with nonaffine parameter dependence. The method replaces nonaffine coefficient functions with a collateral reduced-basis expansion which then permits an (effectively affine) offline–online computational decomposition. The essential components of the approach are (i) a good collateral reduced-basis approximation space, (ii) a stable and inexpensive interpolation procedure, and (iii) an effective a posteriori estimator to quantify the newly introduced errors. Theoretical and numerical results respectively anticipate and confirm the good behavior of the technique. To cite this article: M. Barrault et al., C. R. Acad. Sci. Paris, Ser. I 339 (2004).
Résumé
Nous présentons dans cette Note une méthode rapide de base réduite pour la résolution d'équations aux dérivées partielles ayant une dépendance non affine en ses paramètres. L'approche propose de remplacer le calcul des fonctionelles non affines par un développement en base réduite annexe qui conduit à une évaluation en ligne effectivement affine. Les points essentiels de cette approche sont (i) un bon système de base réduite annexe, (ii) une méthode stable et peu coûteuse d'interpolation dans cette base, et (iii) un estimateur a posteriori pertinent pour quantifier les nouvelles erreurs introduites. Des résultats théoriques et numériques viennent anticiper puis confirmer le bon comportement de cette technique. Pour citer cet article : M. Barrault et al., C. R. Acad. Sci. Paris, Ser. I 339 (2004).}
}

@article{DEIM,
author = {Chaturantabut, Saifon and Sorensen, Danny C.},
title = {Nonlinear Model Reduction via Discrete Empirical Interpolation},
journal = {SIAM Journal on Scientific Computing},
volume = {32},
number = {5},
pages = {2737-2764},
year = {2010},
doi = {10.1137/090766498},

URL = { 
        https://doi.org/10.1137/090766498
    
},
eprint = { 
        https://doi.org/10.1137/090766498
    
}
,
    abstract = { A dimension reduction method called discrete empirical interpolation is proposed and shown to dramatically reduce the computational complexity of the popular proper orthogonal decomposition (POD) method for constructing reduced-order models for time dependent and/or parametrized nonlinear partial differential equations (PDEs). In the presence of a general nonlinearity, the standard POD-Galerkin technique reduces dimension in the sense that far fewer variables are present, but the complexity of evaluating the nonlinear term remains that of the original problem. The original empirical interpolation method (EIM) is a modification of POD that reduces the complexity of evaluating the nonlinear term of the reduced model to a cost proportional to the number of reduced variables obtained by POD. We propose a discrete empirical interpolation method (DEIM), a variant that is suitable for reducing the dimension of systems of ordinary differential equations (ODEs) of a certain type. As presented here, it is applicable to ODEs arising from finite difference discretization of time dependent PDEs and/or parametrically dependent steady state problems. However, the approach extends to arbitrary systems of nonlinear ODEs with minor modification. Our contribution is a greatly simplified description of the EIM in a finite-dimensional setting that possesses an error bound on the quality of approximation. An application of DEIM to a finite difference discretization of the one-dimensional FitzHugh–Nagumo equations is shown to reduce the dimension from 1024 to order 5 variables with negligible error over a long-time integration that fully captures nonlinear limit cycle behavior. We also demonstrate applicability in higher spatial dimensions with similar state space dimension reduction and accuracy results. }
}

@ARTICLE{MissPtEst,
  author={Astrid, Patricia and Weiland, Siep and Willcox, Karen and Backx, Ton},
  journal={IEEE Transactions on Automatic Control}, 
  title={Missing Point Estimation in Models Described by Proper Orthogonal Decomposition}, 
  year={2008},
  volume={53},
  number={10},
  pages={2237-2251},
  doi={10.1109/TAC.2008.2006102}}


@article{GappyPOD,
title = {Unsteady flow sensing and estimation via the gappy proper orthogonal decomposition},
journal = {Computers \& Fluids},
volume = {35},
number = {2},
pages = {208-226},
year = {2006},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2004.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0045793005000113},
author = {K. Willcox},
abstract = {The proper orthogonal decomposition (POD) has been widely used in fluid dynamic applications for extracting dominant flow features. The “gappy” POD is an extension to this method that allows the consideration of incomplete data sets. In this paper, the gappy POD is extended to handle unsteady flow reconstruction problems, such as those encountered when limited flow measurement data is available. In addition, a systematic approach for effective sensor placement is formulated within the gappy framework using a condition number criterion. This criterion allows for accurate flow reconstruction results and yields sensor configurations that are robust to sensor noise. Two applications are considered. The first aims to reconstruct the unsteady flow field using a small number of surface pressure measurements for a subsonic airfoil undergoing plunging motion. The second considers estimation of POD modal content of a cylinder wake flow for active control purposes. In both cases, using the dominant POD basis vectors and a small number of sensor signals, the gappy approach is found to yield accurate flow reconstruction results.}
}


@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	language = {en},
	urldate = {2022-04-26},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}


@article{E:2018_deep_ritz,
abstract = {We propose a deep learning-based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz Method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
author = {E, Weinan and Yu, Bing},
date = {2018/03/01},
doi = {10.1007/s40304-018-0127-z},
id = {E2018},
isbn = {2194-671X},
journal = {Communications in Mathematics and Statistics},
number = {1},
pages = {1--12},
title = {The Deep Ritz Method: A Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
url = {https://doi.org/10.1007/s40304-018-0127-z},
volume = {6},
year = {2018},}


@article{eivazi_physics-informed_2021,
	title = {Physics-informed neural networks for solving {Reynolds}-averaged {Navier}-{Stokes} equations},
	url = {http://arxiv.org/abs/2107.10711},
	abstract = {Physics-informed neural networks (PINNs) are successful machine-learning methods for the solution and identification of partial differential equations (PDEs). We employ PINNs for solving the Reynolds-averaged Navier\${\textbackslash}unicode\{x2013\}\$Stokes (RANS) equations for incompressible turbulent flows without any specific model or assumption for turbulence, and by taking only the data on the domain boundaries. We first show the applicability of PINNs for solving the Navier\${\textbackslash}unicode\{x2013\}\$Stokes equations for laminar flows by solving the Falkner\${\textbackslash}unicode\{x2013\}\$Skan boundary layer. We then apply PINNs for the simulation of four turbulent-flow cases, i.e., zero-pressure-gradient boundary layer, adverse-pressure-gradient boundary layer, and turbulent flows over a NACA4412 airfoil and the periodic hill. Our results show the excellent applicability of PINNs for laminar flows with strong pressure gradients, where predictions with less than 1\% error can be obtained. For turbulent flows, we also obtain very good accuracy on simulation results even for the Reynolds-stress components.},
	urldate = {2022-04-26},
	journal = {arXiv:2107.10711 [physics]},
	author = {Eivazi, Hamidreza and Tahani, Mojtaba and Schlatter, Philipp and Vinuesa, Ricardo},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.10711},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Physics - Fluid Dynamics},
}


@inproceedings{graph_fmm:2020,
author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
title = {Multipole Graph Neural Operator for Parametric Partial Differential Equations},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the main challenges in using deep learning-based methods for simulating physical systems and solving partial differential equations (PDEs) is formulating physics-based data in the desired structure for neural networks. Graph neural networks (GNNs) have gained popularity in this area since graphs offer a natural way of modeling particle interactions and provide a clear way of discretizing the continuum models. However, the graphs constructed for approximating such tasks usually ignore long-range interactions due to unfavorable scaling of the compu tational complexity with respect to the number of nodes. The errors due to these approximations scale with the discretization of the system, thereby not allowing for generalization under mesh-refinement. Inspired by the classical multipole methods we propose a novel multi-level graph neural network framework that captures interaction at all ranges with only linear complexity. Our multi-level formulation is equivalent to recursively adding inducing points to the kernel matrix, unifying GNNs with multi-resolution matrix factorization of the kernel. Experiments con firm our multi-graph network learns discretization-invariant solution operators to PDEs and can be evaluated in linear time.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {567},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}


@article{ZANG:2021_WAN,
title = {Weak adversarial networks for high-dimensional partial differential equations},
journal = {Journal of Computational Physics},
volume = {411},
pages = {109409},
year = {2020},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2020.109409},
url = {https://www.sciencedirect.com/science/article/pii/S0021999120301832},
author = {Yaohua Zang and Gang Bao and Xiaojing Ye and Haomin Zhou},
keywords = {High dimensional PDE, Deep neural network, Adversarial network, Weak solution},
abstract = {Solving general high-dimensional partial differential equations (PDE) is a long-standing challenge in numerical mathematics. In this paper, we propose a novel approach to solve high-dimensional linear and nonlinear PDEs defined on arbitrary domains by leveraging their weak formulations. We convert the problem of finding the weak solution of PDEs into an operator norm minimization problem induced from the weak formulation. The weak solution and the test function in the weak formulation are then parameterized as the primal and adversarial networks respectively, which are alternately updated to approximate the optimal network parameter setting. Our approach, termed as the weak adversarial network (WAN), is fast, stable, and completely mesh-free, which is particularly suitable for high-dimensional PDEs defined on irregular domains where the classical numerical methods based on finite differences and finite elements suffer the issues of slow computation, instability and the curse of dimensionality. We apply our method to a variety of test problems with high-dimensional PDEs to demonstrate its promising performance.}
}

@inproceedings{Sanches_Kochkov:2020,
title	= {Learning latent field dynamics of PDEs},
author	= {Dmitrii Kochkov and Alvaro Sanchez-Gonzalez and Jamie Alexander Smith and Tobias Joachim Pfaff and Peter Battaglia and Michael Brenner},
booktitle = {Third Workshop on Machine Learning and the Physical Sciences (NeurIPS 2020)},
year	= {2020},
}

@article{hornik90,
      title   = {Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
      author  = {Hornik, K. and Stinchcombe, M.  and White, H. },
      journal = {Neural networks},
      volume  = {3},
      number    = {5},
      pages   = {551-560},
      year    = {1990}
}

@inproceedings{mildenhall:2020nerf,
 title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
 author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
 year={2020},
 booktitle={ECCV},
}

@article{Bruna_Peherstorfer:2022,
	title = {Neural Galerkin Scheme with Active Learning for High-Dimensional Evolution Equations},
	url = {https://arxiv.org/abs/2203.01360 },
	urldate = {2022-03-30},
	journal = {	arXiv:2203.01360  [math.NA]},
	author = { Bruna, Joan and  Peherstorfer, Benjamin and  Vanden-Eijnden, Eric},
	month = may,
	year = {2022},
	note = {arXiv: 2203.01360 },
}

@article{deeponet:2021,
  title   = {Learning nonlinear operators via {DeepONet} based on the universal approximation theorem of operators},
  author  = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  journal = {Nature Machine Intelligence},
  volume  = {3},
  number  = {3},
  pages   = {218--229},
  year    = {2021}
}


@book{Martinsson:fast_pde_solvers_2019,
author = {Martinsson, Per-Gunnar},
title = {Fast Direct Solvers for Elliptic PDEs},
publisher = {Society for Industrial and Applied Mathematics},
year = {2019},
doi = {10.1137/1.9781611976045},
address = {Philadelphia, PA},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976045},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611976045}
}


@book{Demmel:num_lin_alg_1997,
author = {Demmel, James W.},
title = {Applied Numerical Linear Algebra},
publisher = {Society for Industrial and Applied Mathematics},
year = {1997},
doi = {10.1137/1.9781611971446},
address = {},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611971446},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611971446}
}

@article{Dormand-Prince:1980,
title = {A family of embedded Runge-Kutta formulae},
journal = {Journal of Computational and Applied Mathematics},
volume = {6},
number = {1},
pages = {19-26},
year = {1980},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0771-050X(80)90013-3},
url = {https://www.sciencedirect.com/science/article/pii/0771050X80900133},
author = {J.R. Dormand and P.J. Prince},
abstract = {A family of embedded Runge-Kutta formulae RK5 (4) are derived. From these are presented formulae which have (a) ‘small’ principal truncation terms in the fifth order and (b) extended regions of absolute stability.}
}

@book{Boyd:01_Chebyshev_Fourier,
  added-at = {2015-10-15T15:26:57.000+0200},
  address = {Mineola, NY},
  author = {Boyd, John P.},
  biburl = {https://www.bibsonomy.org/bibtex/232e7a8973ba49d2b7669fceb96f52345/ytyoun},
  edition = {Second},
  interhash = {98c3a81f254f770f8e8982be5e0291c8},
  intrahash = {32e7a8973ba49d2b7669fceb96f52345},
  isbn = {0486411834 9780486411835},
  keywords = {chebyshev fourier polynomial textbook},
  publisher = {Dover Publications},
  refid = {43607416},
  series = {Dover Books on Mathematics},
  timestamp = {2015-12-19T13:10:16.000+0100},
  title = {{Chebyshev} and {Fourier} Spectral Methods},
  year = 2001
}

@inproceedings{Pu:2017_symm_vae,
author = {Pu, Yunchen and Wang, Weiyao and Henao, Ricardo and Chen, Liqun and Gan, Zhe and Li, Chunyuan and Carin, Lawrence},
title = {Adversarial Symmetric Variational Autoencoder},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmark datasets.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {4333–4342},
numpages = {10},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@article{Cooley_Tukey:1965,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2003354},
 author = {Cooley, J.~W. and  Tukey, J.~W.},
 journal = {Math. Comput.},
 number = {90},
 pages = {297--301},
 publisher = {American Mathematical Society},
 title = {An Algorithm for the Machine Calculation of Complex {F}ourier Series},
 volume = {19},
 year = {1965}
}

@book{trefethen_spectral_2000,
 author = {Trefethen, Lloyd N.},
 publisher = {Society for industrial and applied mathematics (SIAM)},
 title = {\href{https://doi.org/10.1137/1.9780898719598}{Spectral {Methods} in {MATLAB}}},
 urldate = {2022-03-30},
 year = {2000}
}

@book{Canuto:2007_spectral_methods,
author = {Canuto, Claudio G and Hussaini, M. Yousuff and Quarteroni, Alfio M. and Zang, Thomas A.},
title = {Spectral Methods: Evolution to Complex Geometries and Applications to Fluid Dynamics (Scientific Computation)},
year = {2007},
isbn = {3540307273},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@inproceedings{neuralODE,
 author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Ordinary Differential Equations},
 url = {https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
 volume = {31},
 year = {2018}
}

@article{pfaff2020learning,
  title={Learning mesh-based simulation with graph networks},
  author={Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter W},
  journal={arXiv preprint arXiv:2010.03409},
  year={2020}
}

@misc{message_passing_pde,
  doi = {10.48550/ARXIV.2202.03376},
  url = {https://arxiv.org/abs/2202.03376},
  author = {Brandstetter, Johannes and Worrall, Daniel and Welling, Max},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Numerical Analysis (math.NA), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title = {Message Passing Neural PDE Solvers},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{han_gnn_attention,
  author    = {Xu Han and
               Han Gao and
               Tobias Pfaff and
               Jian{-}Xun Wang and
               Li{-}Ping Liu},
  title     = {Predicting Physics in Mesh-reduced Space with Temporal Attention},
  journal   = {CoRR},
  volume    = {abs/2201.09113},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.09113},
  eprinttype = {arXiv},
  eprint    = {2201.09113},
  timestamp = {Thu, 18 Aug 2022 15:13:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2201-09113.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{keisler2022forecasting,
  title={Forecasting Global Weather with Graph Neural Networks},
  author={Keisler, Ryan},
  journal={arXiv preprint arXiv:2202.07575},
  year={2022}
}

@inproceedings{ramachandran18,
  author    = {Prajit Ramachandran and
               Barret Zoph and
               Quoc V. Le},
  title     = {Searching for Activation Functions},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=Hkuq2EkPf},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/RamachandranZL18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Wang_Phys_Inf:2021,
author = {Sifan Wang  and Hanwen Wang  and Paris Perdikaris },
title = {Learning the solution operator of parametric partial differential equations with physics-informed DeepONets},
journal = {Science Advances},
volume = {7},
number = {40},
pages = {eabi8605},
year = {2021},
doi = {10.1126/sciadv.abi8605},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.abi8605},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abi8605},
abstract = {Enabling the rapid emulation of parametric differential equations with physics-informed deep operator networks. Partial differential equations (PDEs) play a central role in the mathematical analysis and modeling of complex dynamic processes across all corners of science and engineering. Their solution often requires laborious analytical or computational tools, associated with a cost that is markedly amplified when different scenarios need to be investigated, for example, corresponding to different initial or boundary conditions, different inputs, etc. In this work, we introduce physics-informed DeepONets, a deep learning framework for learning the solution operator of arbitrary PDEs, even in the absence of any paired input-output training data. We illustrate the effectiveness of the proposed framework in rapidly predicting the solution of various types of parametric PDEs up to three orders of magnitude faster compared to conventional PDE solvers, setting a previously unexplored paradigm for modeling and simulation of nonlinear and nonequilibrium processes in science and engineering.}}


@article{Peherstorfer2020:rom_transport,
author = {Peherstorfer, Benjamin},
title = {Model Reduction for Transport-Dominated Problems via Online Adaptive Bases and Adaptive Sampling},
journal = {SIAM Journal on Scientific Computing},
volume = {42},
number = {5},
pages = {A2803-A2836},
year = {2020},
doi = {10.1137/19M1257275},
URL = {https://doi.org/10.1137/19M1257275},
eprint = {https://doi.org/10.1137/19M1257275},
abstract = { This work presents a model reduction approach for problems with coherent structures that propagate over time, such as convection-dominated flows and wave-type phenomena. Traditional model reduction methods have difficulties with these transport-dominated problems because propagating coherent structures typically introduce high-dimensional features that require high-dimensional approximation spaces. The approach proposed in this work exploits the locality in space and time of propagating coherent structures to derive efficient reduced models. Full-model solutions are approximated locally in time via local reduced spaces that are adapted with basis updates during time stepping. The basis updates are derived from querying the full model at a few selected spatial coordinates. A core contribution of this work is an adaptive sampling scheme for selecting at which components to query the full model to compute basis updates. The presented analysis shows that, in probability, the more local the coherent structure is in space, the fewer full-model samples are required to adapt the reduced basis with the proposed adaptive sampling scheme. Numerical results on benchmark examples with interacting wave-type structures and time-varying transport speeds and on a model combustor of a single-element rocket engine demonstrate the wide applicability of the proposed approach and runtime speedups of up to one order of magnitude compared to full models and traditional reduced models.}
}

@article{hammoud2022cdanet,
  title={CDAnet: A Physics-Informed Deep Neural Network for Downscaling Fluid Flows},
  author={Hammoud, Mohamad Abed El Rahman and Titi, Edriss S and Hoteit, Ibrahim and Knio, Omar},
  journal={Journal of Advances in Modeling Earth Systems},
  volume={14},
  number={12},
  pages={e2022MS003051},
  year={2022},
  publisher={Wiley Online Library}
}

@article{korotin2019wasserstein,
  title={Wasserstein-2 generative networks},
  author={Korotin, Alexander and Egiazarian, Vage and Asadulaev, Arip and Safin, Alexander and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:1909.13082},
  year={2019}
}

@article{cuturi2013sinkhorn,
  title={Sinkhorn distances: Lightspeed computation of optimal transport},
  author={Cuturi, Marco},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{altschuler2017near,
  title={Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration},
  author={Altschuler, Jason and Niles-Weed, Jonathan and Rigollet, Philippe},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{price2022increasing,
  title={Increasing the accuracy and resolution of precipitation forecasts using deep generative models},
  author={Price, Ilan and Rasp, Stephan},
  booktitle={International conference on artificial intelligence and statistics},
  pages={10555--10571},
  year={2022},
  organization={PMLR}
}

@article{harris2022generative,
  title={A generative deep learning approach to stochastic downscaling of precipitation forecasts},
  author={Harris, Lucy and McRae, Andrew TT and Chantry, Matthew and Dueben, Peter D and Palmer, Tim N},
  journal={Journal of Advances in Modeling Earth Systems},
  volume={14},
  number={10},
  pages={e2022MS003120},
  year={2022},
  publisher={Wiley Online Library}
}

@article{pan2021learning,
  title={Learning to correct climate projection biases},
  author={Pan, Baoxiang and Anderson, Gemma J and Goncalves, Andr{\'e} and Lucas, Donald D and Bonfils, C{\'e}line JW and Lee, Jiwoo and Tian, Yang and Ma, Hsi-Yen},
  journal={Journal of Advances in Modeling Earth Systems},
  volume={13},
  number={10},
  pages={e2021MS002509},
  year={2021},
  publisher={Wiley Online Library}
}

@inproceedings{park2019deepsdf,
  title={Deepsdf: Learning continuous signed distance functions for shape representation},
  author={Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={165--174},
  year={2019}
}

@article{lorenz2005designing,
  title={Designing chaotic models},
  author={Lorenz, Edward N},
  journal={Journal of the atmospheric sciences},
  volume={62},
  number={5},
  pages={1574--1587},
  year={2005},
  publisher={American Meteorological Society}
}

@article{schmid2010dynamic,
  title={Dynamic mode decomposition of numerical and experimental data},
  author={Schmid, Peter J},
  journal={Journal of fluid mechanics},
  volume={656},
  pages={5--28},
  year={2010},
  publisher={Cambridge University Press}
}

@book{kutz2016dmd,
author = {Kutz, J. Nathan and Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L.},
title = {Dynamic Mode Decomposition},
publisher = {Society for Industrial and Applied Mathematics},
year = {2016},
doi = {10.1137/1.9781611974508},
address = {Philadelphia, PA},
edition   = {},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611974508},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611974508}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{boffetta_two-dimensional_2012,
 abstract = {In physical systems, a reduction in dimensionality often leads to exciting new phenomena. Here we discuss the novel effects arising from the consideration of fluid turbulence confined to two spatial dimensions. The additional conservation constraint on squared vorticity relative to three-dimensional (3D) turbulence leads to the dual-cascade scenario of Kraichnan and Batchelor with an inverse energy cascade to larger scales and a direct enstrophy cascade to smaller scales. Specific theoretical predictions of spectra, structure functions, probability distributions, and mechanisms are presented, and major experimental and numerical comparisons are reviewed. The introduction of 3D perturbations does not destroy the main features of the cascade picture, implying that 2D turbulence phenomenology establishes the general picture of turbulent fluid flows when one spatial direction is heavily constrained by geometry or by applied body forces. Such flows are common in geophysical and planetary contexts, are beautiful to observe, and reflect the impact of dimensionality on fluid turbulence.},
 author = {Boffetta, Guido and Ecke, Robert E.},
 doi = {10.1146/annurev-fluid-120710-101240},
 issn = {0066-4189, 1545-4479},
 journal = {Annual Review of Fluid Mechanics},
 month = {January},
 title = {\href{https://www.annualreviews.org/doi/10.1146/annurev-fluid-120710-101240}{Two-{Dimensional} {Turbulence}}},
 urldate = {2022-03-31},
 volume = {44},
 year = {2012}
}

@inproceedings{climalign:2021,
author = {Groenke, Brian and Madaus, Luke and Monteleoni, Claire},
title = {ClimAlign: Unsupervised Statistical Downscaling of Climate Variables via Normalizing Flows},
year = {2021},
isbn = {9781450388481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429309.3429318},
doi = {10.1145/3429309.3429318},
abstract = {Downscaling is a common task in climate science and meteorology in which the goal is to use coarse scale, spatio-temporal data to infer values at finer scales. Statistical downscaling aims to approximate this task using statistical patterns gleaned from an existing dataset of downscaled values, often obtained from observations or physical models. In this work, we investigate the application of domain alignment to the task of statistical downscaling. We present ClimAlign, a novel method for unsupervised, generative downscaling using adaptations of recent work in normalizing flows for variational inference. We evaluate the viability of our method using several different metrics on two datasets consisting of daily temperature and precipitation values gridded at low (1° latitude/longitude) and high ( and ) resolutions. We show that our method achieves comparable predictive performance to existing supervised statistical downscaling methods while simultaneously allowing for both conditional and unconditional sampling from the joint distribution over high and low resolution spatial fields. To the best of our knowledge, this is the first proposed method for unsupervised statistical downscaling, and one of very few proposed methods that allows for efficient sampling of synthetic data.},
booktitle = {Proceedings of the 10th International Conference on Climate Informatics},
pages = {60–66},
numpages = {7},
keywords = {downscaling, normalizing flows, generative, unsupervised},
location = {virtual, United Kingdom},
series = {CI2020}
}

@article{Wilby_1998:downscaling,
author = {Wilby, R. L. and Wigley, T. M. L. and Conway, D. and Jones, P. D. and Hewitson, B. C. and Main, J. and Wilks, D. S.},
title = {Statistical downscaling of general circulation model output: A comparison of methods},
journal = {Water Resources Research},
volume = {34},
number = {11},
pages = {2995-3008},
doi = {https://doi.org/10.1029/98WR02577},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/98WR02577},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/98WR02577},
abstract = {A range of different statistical downscaling models was calibrated using both observed and general circulation model (GCM) generated daily precipitation time series and intercompared. The GCM used was the U.K. Meteorological Office, Hadley Centre's coupled ocean/atmosphere model (HadCM2) forced by combined CO2 and sulfate aerosol changes. Climate model results for 1980–1999 (present) and 2080–2099 (future) were used, for six regions across the United States. The downscaling methods compared were different weather generator techniques (the standard “WGEN” method, and a method based on spell-length durations), two different methods using grid point vorticity data as an atmospheric predictor variable (B-Circ and C-Circ), and two variations of an artificial neural network (ANN) transfer function technique using circulation data and circulation plus temperature data as predictor variables. Comparisons of results were facilitated by using standard sets of observed and GCM-derived predictor variables and by using a standard suite of diagnostic statistics. Significant differences in the level of skill were found among the downscaling methods. The weather generation techniques, which are able to fit a number of daily precipitation statistics exactly, yielded the smallest differences between observed and simulated daily precipitation. The ANN methods performed poorly because of a failure to simulate wet-day occurrence statistics adequately. Changes in precipitation between the present and future scenarios produced by the statistical downscaling methods were generally smaller than those produced directly by the GCM. Changes in daily precipitation produced by the GCM between 1980–1999 and 2080–2099 were therefore judged not to be due primarily to changes in atmospheric circulation. In the light of these results and detailed model comparisons, suggestions for future research and model refinements are presented.},
year = {1998}
}

@inproceedings{Vandal_2017:DeepDS,
author = {Vandal, Thomas and Kodra, Evan and Ganguly, Sangram and Michaelis, Andrew and Nemani, Ramakrishna and Ganguly, Auroop R.},
title = {DeepSD: Generating High Resolution Climate Change Projections through Single Image Super-Resolution},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098004},
doi = {10.1145/3097983.3098004},
abstract = {The impacts of climate change are felt by most critical systems, such as infrastructure, ecological systems, and power-plants. However, contemporary Earth System Models (ESM) are run at spatial resolutions too coarse for assessing effects this localized. Local scale projections can be obtained using statistical downscaling, a technique which uses historical climate observations to learn a low-resolution to high-resolution mapping. Depending on statistical modeling choices, downscaled projections have been shown to vary significantly terms of accuracy and reliability. The spatio-temporal nature of the climate system motivates the adaptation of super-resolution image processing techniques to statistical downscaling. In our work, we present DeepSD, a generalized stacked super resolution convolutional neural network (SRCNN) framework for statistical downscaling of climate variables. DeepSD augments SRCNN with multi-scale input channels to maximize predictability in statistical downscaling. We provide a comparison with Bias Correction Spatial Disaggregation as well as three Automated-Statistical Downscaling approaches in downscaling daily precipitation from 1 degree (~100km) to 1/8 degrees (~12.5km) over the Continental United States. Furthermore, a framework using the NASA Earth Exchange (NEX) platform is discussed for downscaling more than 20 ESM models with multiple emission scenarios.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1663–1672},
numpages = {10},
keywords = {daily precipitation, climate statistical downscaling, deep learning, super-resolution},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@article{Hessami_2008:linear_models_statistical_dowscaling,
title = {Automated regression-based statistical downscaling tool},
journal = {Environmental Modelling and Software},
volume = {23},
number = {6},
pages = {813-834},
year = {2008},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2007.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364815207001867},
author = {Masoud Hessami and Philippe Gachon and Taha B.M.J. Ouarda and André St-Hilaire},
keywords = {Climate change, Statistical downscaling, GCM, Multiple regression, Eastern Canada},
abstract = {Many impact studies require climate change information at a finer resolution than that provided by Global Climate Models (GCMs). In the last 10 years, downscaling techniques, both dynamical (i.e. Regional Climate Model) and statistical methods, have been developed to obtain fine resolution climate change scenarios. In this study, an automated statistical downscaling (ASD) regression-based approach inspired by the SDSM method (statistical downscaling model) developed by Wilby, R.L., Dawson, C.W., Barrow, E.M. [2002. SDSM – a decision support tool for the assessment of regional climate change impacts, Environmental Modelling and Software 17, 147–159] is presented and assessed to reconstruct the observed climate in eastern Canada based extremes as well as mean state. In the ASD model, automatic predictor selection methods are based on backward stepwise regression and partial correlation coefficients. The ASD model also gives the possibility to use ridge regression to alleviate the effect of the non-orthogonality of predictor vectors. Outputs from the first generation Canadian Coupled Global Climate Model (CGCM1) and the third version of the coupled global Hadley Centre Climate Model (HadCM3) are used to test this approach over the current period (i.e. 1961–1990), and compare results with observed temperature and precipitation from 10 meteorological stations of Environment Canada located in eastern Canada. All ASD and SDSM models, as these two models are evaluated and inter-compared, are calibrated using NCEP (National Center for Environmental Prediction) reanalysis data before the use of GCMs atmospheric fields as input variables. The results underline certain limitations to downscale the precipitation regime and its strength to downscale the temperature regime. When modeling precipitation, the most commonly combination of predictor variables were relative and specific humidity at 500hPa, surface airflow strength, 850hPa zonal velocity and 500hPa geopotential height. For modeling temperature, mean sea level pressure, surface vorticity and 850hPa geopotential height were the most dominant variables. To evaluate the performance of the statistical downscaling approach, several climatic and statistical indices were developed. Results indicate that the agreement of simulations with observations depends on the GCMs atmospheric variables used as “predictors” in the regression-based approach, and the performance of the statistical downscaling model varies for different stations and seasons. The comparison of SDSM and ASD models indicated that neither could perform well for all seasons and months. However, using different statistical downscaling models and multi-sources GCMs data can provide a better range of uncertainty for climatic and statistical indices.}
}


@article{charalampopoulos2023statistics,
title={Statistics of extreme events in coarse-scale climate simulations via machine learning correction operators trained on nudged datasets},
author={Charalampopoulos, Alexis-Tzianni and Zhang, Shuai and Harrop, Bryce and Leung,  Lai-yung Ruby and Sapsis,  Themistoklis},
journal={arXiv preprint arXiv:2304.02117},
year={2023}
}

@article {Candes_Fernandez_Granda_2014:Super_resolution,
author = {Cand\`es, E. J. and Fernandez-Granda, C.},
title = {Towards a Mathematical Theory of Super-resolution},
journal = {Comm. Pure and Appl. Math.},
volume = {67},
number = {6},
issn = {1097-0312},
url = {http://dx.doi.org/10.1002/cpa.21455},
doi = {10.1002/cpa.21455},
pages = {906--956},
year = {2014},
}

@Article{Candes:2013,
  doi = {10.1007/s00041-013-9292-3},
  url = {https://doi.org/10.1007/s00041-013-9292-3},
  year = {2013},
  month = aug,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {19},
  number = {6},
  pages = {1229--1254},
  author = {Emmanuel J. Cand{\`{e}}s and Carlos Fernandez-Granda},
  title = {Super-Resolution from Noisy Data},
  journal = {Journal of Fourier Analysis and Applications}
}

@article{Donoho:super_resolution1992,
author = {Donoho, David L.},
title = {Superresolution via Sparsity Constraints},
journal = {SIAM Journal on Mathematical Analysis},
volume = {23},
number = {5},
pages = {1309-1331},
year = {1992},
doi = {10.1137/0523074},
URL = { https://doi.org/10.1137/0523074},
eprint = {https://doi.org/10.1137/0523074}
}

@article{kolmogorov1962refinement,
title={A refinement of previous hypotheses concerning the local structure of turbulence in a viscous incompressible fluid at high Reynolds number},
author={Kolmogorov, A. N.},
journal={Journal of Fluid Mechanics},
volume={13},
number={1},
pages={82--85},
year={1962},
publisher={Cambridge University Press}
}

@ARTICLE{Hua_Sarkar:1991, 
author={Hua, Y. and Sarkar, T. K.}, 
journal={IEEE Trans. Sig. Proc.}, 
title={On {SVD} for estimating generalized eigenvalues of singular matrix pencil in noise}, 
year={1991}, 
volume={39}, 
number={4}, 
pages={892-900}, 
keywords={eigenvalues and eigenfunctions;matrix algebra;noise;spectral analysis;state-space methods;parameter estimation;SVD;generalized eigenvalues;singular matrix pencil;noise;singular value decomposition;direct matrix pencil algorithm;pro-ESPRIT;TLS-ESPRIT;TLS-Pro-ESPRIT;LS-ESPRIT;superimposed complex exponential signals;state-space algorithm;Eigenvalues and eigenfunctions;Matrix decomposition;Covariance matrix;Noise level;State-space methods;Approximation algorithms;Cleaning;Singular value decomposition;State estimation;Noise robustness}, 
doi={10.1109/78.80911}, 
ISSN={1941-0476}, 
month={April},}

@article{dixon2016evaluating,
  title={Evaluating the stationarity assumption in statistically downscaled climate projections: is past performance an indicator of future results?},
  author={Dixon, Keith W and Lanzante, John R and Nath, Mary J and Hayhoe, Katharine and Stoner, Anne and Radhakrishnan, Aparna and Balaji, Venkat and Gaitan, Carlos F},
  journal={Climatic Change},
  volume={135},
  number={3-4},
  pages={395--408},
  year={2016},
  publisher={Springer}
}

@article {BurgerDownscaling:2012,
      author = "G. Bürger and T. Q. Murdock and A. T. Werner and S. R. Sobie and A. J. Cannon",
      title = "Downscaling Extremes—An Intercomparison of Multiple Statistical Methods for Present Climate",
      journal = "Journal of Climate",
      year = "2012",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "25",
      number = "12",
      doi = "https://doi.org/10.1175/JCLI-D-11-00408.1",
      pages=      "4366 - 4388",
      url = "https://journals.ametsoc.org/view/journals/clim/25/12/jcli-d-11-00408.1.xml"
}

@article{Wilby:2006,
   author    =  "R. L. Wilby and P. Whitehead and A. Wade and D. Butterfield and R. Davis and G. Watts",
   title     =  "Integrated modelling of climate change impacts on water resources and quality in a lowland catchment: River Kennet, UK",
   year      =  "2006",
   journal   =  "J. Hydrol.",
   volume    =  "330",
   number    =  "1–2",
   pages     =  "204--220"}


@article{hersbach2020era5,
author = {Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Horányi, András and Muñoz-Sabater, Joaquín and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and Simmons, Adrian and Soci, Cornel and Abdalla, Saleh and Abellan, Xavier and Balsamo, Gianpaolo and Bechtold, Peter and Biavati, Gionata and Bidlot, Jean and Bonavita, Massimo and De Chiara, Giovanna and Dahlgren, Per and Dee, Dick and Diamantakis, Michail and Dragani, Rossana and Flemming, Johannes and Forbes, Richard and Fuentes, Manuel and Geer, Alan and Haimberger, Leo and Healy, Sean and Hogan, Robin J. and Hólm, Elías and Janisková, Marta and Keeley, Sarah and Laloyaux, Patrick and Lopez, Philippe and Lupu, Cristina and Radnoti, Gabor and de Rosnay, Patricia and Rozum, Iryna and Vamborg, Freja and Villaume, Sebastien and Thépaut, Jean-Noël},
title = {The ERA5 global reanalysis},
journal = {Quarterly Journal of the Royal Meteorological Society},
volume = {146},
number = {730},
pages = {1999-2049},
keywords = {climate reanalysis, Copernicus Climate Change Service, data assimilation, ERA5, historical observations},
doi = {https://doi.org/10.1002/qj.3803},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3803},
eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3803},
abstract = {Abstract Within the Copernicus Climate Change Service (C3S), ECMWF is producing the ERA5 reanalysis which, once completed, will embody a detailed record of the global atmosphere, land surface and ocean waves from 1950 onwards. This new reanalysis replaces the ERA-Interim reanalysis (spanning 1979 onwards) which was started in 2006. ERA5 is based on the Integrated Forecasting System (IFS) Cy41r2 which was operational in 2016. ERA5 thus benefits from a decade of developments in model physics, core dynamics and data assimilation. In addition to a significantly enhanced horizontal resolution of 31 km, compared to 80 km for ERA-Interim, ERA5 has hourly output throughout, and an uncertainty estimate from an ensemble (3-hourly at half the horizontal resolution). This paper describes the general set-up of ERA5, as well as a basic evaluation of characteristics and performance, with a focus on the dataset from 1979 onwards which is currently publicly available. Re-forecasts from ERA5 analyses show a gain of up to one day in skill with respect to ERA-Interim. Comparison with radiosonde and PILOT data prior to assimilation shows an improved fit for temperature, wind and humidity in the troposphere, but not the stratosphere. A comparison with independent buoy data shows a much improved fit for ocean wave height. The uncertainty estimate reflects the evolution of the observing systems used in ERA5. The enhanced temporal and spatial resolution allows for a detailed evolution of weather systems. For precipitation, global-mean correlation with monthly-mean GPCP data is increased from 67\% to 77\%. In general, low-frequency variability is found to be well represented and from 10 hPa downwards general patterns of anomalies in temperature match those from the ERA-Interim, MERRA-2 and JRA-55 reanalyses.},
year = {2020}
}

@article{Danabasoglu2020:cesm,
author = {Danabasoglu, G. and Lamarque, J.-F. and Bacmeister, J. and Bailey, D. A. and DuVivier, A. K.  and Edwards, J. and Emmons, L. K. and Fasullo, J. and Garcia, R. and Gettelman, A. and Hannay, C. and Holland, M. M. and Large, W. G. and Lauritzen, P. H. and Lawrence, D. M. and Lenaerts, J. T. M. and Lindsay, K. and Lipscomb, W. H. and Mills, M. J. and Neale, R. and Oleson, K. W. and Otto-Bliesner, B. and Phillips, A. S. and Sacks, W. and Tilmes, S. and van Kampenhout, L. and Vertenstein, M. and Bertini, A. and Dennis, J. and Deser, C. and Fischer, C. and Fox-Kemper, B. and Kay, J. E. and Kinnison, D. and Kushner, P. J. and Larson, V. E. and Long, M. C. and Mickelson, S. and Moore, J. K. and Nienhouse, E. and Polvani, L. and Rasch, P. J. and Strand, W. G.},
title = {The Community Earth System Model Version 2 (CESM2)},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {12},
number = {2},
pages = {e2019MS001916},
keywords = {Community Earth System Model (CESM), global coupled Earth system modeling, preindustrial and historical simulations, coupled model development and evaluation},
doi = {https://doi.org/10.1029/2019MS001916},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001916},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001916},
note = {e2019MS001916 2019MS001916},
abstract = {Abstract An overview of the Community Earth System Model Version 2 (CESM2) is provided, including a discussion of the challenges encountered during its development and how they were addressed. In addition, an evaluation of a pair of CESM2 long preindustrial control and historical ensemble simulations is presented. These simulations were performed using the nominal 1° horizontal resolution configuration of the coupled model with both the “low-top” (40 km, with limited chemistry) and “high-top” (130 km, with comprehensive chemistry) versions of the atmospheric component. CESM2 contains many substantial science and infrastructure improvements and new capabilities since its previous major release, CESM1, resulting in improved historical simulations in comparison to CESM1 and available observations. These include major reductions in low-latitude precipitation and shortwave cloud forcing biases; better representation of the Madden-Julian Oscillation; better El Niño-Southern Oscillation-related teleconnections; and a global land carbon accumulation trend that agrees well with observationally based estimates. Most tropospheric and surface features of the low- and high-top simulations are very similar to each other, so these improvements are present in both configurations. CESM2 has an equilibrium climate sensitivity of 5.1–5.3 °C, larger than in CESM1, primarily due to a combination of relatively small changes to cloud microphysics and boundary layer parameters. In contrast, CESM2's transient climate response of 1.9–2.0 °C is comparable to that of CESM1. The model outputs from these and many other simulations are available to the research community, and they represent CESM2's contributions to the Coupled Model Intercomparison Project Phase 6.},
year = {2020}
}

@article{vandal2017intercomparison,
  title={Intercomparison of machine learning methods for statistical downscaling: The case of daily and extreme precipitation},
  author={Vandal, Thomas and Kodra, Evan and Ganguly, Auroop R},
  journal={Theor. Appl. Climatol.},
  pages = {557–-570},
  volume={137},
  year={2019}
}

@misc{
grover2019alignflow,
title={AlignFlow: Cycle Consistent Learning from Multiple Domains via Normalizing Flows},
author={Aditya Grover and Christopher Chute and Rui Shu and Zhangjie Cao and Stefano Ermon},
year={2019},
url={https://openreview.net/forum?id=S1lNELLKuN}
} 


@article{sasaki2021:UNIT-DDPM,
  title={UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models},
  author={ Sasaki, Hiroshi and Willcocks,  Chris G. and Breckon, Toby P.},
  journal={arXiv preprint arXiv:2104.05358},
  year={2021}
}

@article{Huang2020:forced_paired_data, 
author = {Xingying Huang  and Daniel L. Swain  and Alex D. Hall },
title = {Future precipitation increase from very high resolution ensemble downscaling of extreme atmospheric river storms in California},
journal = {Science Advances},
volume = {6},
number = {29},
pages = {eaba1323},
year = {2020},
doi = {10.1126/sciadv.aba1323},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.aba1323},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aba1323},
abstract = {California’s most intense atmospheric river storms will bring increasingly extreme precipitation in a warming climate. Precipitation extremes will likely intensify under climate change. However, much uncertainty surrounds intensification of high-magnitude events that are often inadequately resolved by global climate models. In this analysis, we develop a framework involving targeted dynamical downscaling of historical and future extreme precipitation events produced by a large ensemble of a global climate model. This framework is applied to extreme “atmospheric river” storms in California. We find a substantial (10 to 40\%) increase in total accumulated precipitation, with the largest relative increases in valleys and mountain lee-side areas. We also report even higher and more spatially uniform increases in hourly maximum precipitation intensity, which exceed Clausius-Clapeyron expectations. Up to 85\% of this increase arises from thermodynamically driven increases in water vapor, with a smaller contribution by increased zonal wind strength. These findings imply substantial challenges for water and flood management in California, given future increases in intense atmospheric river-induced precipitation extremes.}}

@article{Fowler2007:donwscaling,
author = {Fowler, H. J. and Blenkinsop, S. and Tebaldi, C.},
title = {Linking climate change modelling to impacts studies: recent advances in downscaling techniques for hydrological modelling},
journal = {International Journal of Climatology},
volume = {27},
number = {12},
pages = {1547-1578},
keywords = {downscaling, climate change, hydrological impacts, comparative studies, extremes, uncertainty},
doi = {https://doi.org/10.1002/joc.1556},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/joc.1556},
eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.1556},
abstract = {Abstract There is now a large published literature on the strengths and weaknesses of downscaling methods for different climatic variables, in different regions and seasons. However, little attention is given to the choice of downscaling method when examining the impacts of climate change on hydrological systems. This review paper assesses the current downscaling literature, examining new developments in the downscaling field specifically for hydrological impacts. Sections focus on the downscaling concept; new methods; comparative methodological studies; the modelling of extremes; and the application to hydrological impacts. Consideration is then given to new developments in climate scenario construction which may offer the most potential for advancement within the ‘downscaling for hydrological impacts’ community, such as probabilistic modelling, pattern scaling and downscaling of multiple variables and suggests ways that they can be merged with downscaling techniques in a probabilistic climate change scenario framework to assess the uncertainties associated with future projections. Within hydrological impact studies there is still little consideration given to applied research; how the results can be best used to enable stakeholders and managers to make informed, robust decisions on adaptation and mitigation strategies in the face of many uncertainties about the future. It is suggested that there is a need for a move away from comparison studies into the provision of decision-making tools for planning and management that are robust to future uncertainties; with examination and understanding of uncertainties within the modelling system. Copyright © 2007 Royal Meteorological Society},
year = {2007}
}

@article{Abatzoglou2012:downscaling_wildfire,
author = {Abatzoglou, John T. and Brown, Timothy J.},
title = {A comparison of statistical downscaling methods suited for wildfire applications},
journal = {International Journal of Climatology},
volume = {32},
number = {5},
pages = {772-780},
keywords = {wildfire, downscaling, climate change},
doi = {https://doi.org/10.1002/joc.2312},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/joc.2312},
eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/joc.2312},
abstract = {Abstract Place-based data is required in wildfire analyses, particularly in regions of diverse terrain that foster not only strong gradients in meteorological variables, but also complex fire behaviour. However, a majority of downscaling methods are inappropriate for wildfire application due to the lack of daily timescales and variables such as humidity and winds that are important for fuel flammability and fire spread. Two statistical downscaling methods, the daily Bias corrected Spatial Downscaling (BCSD) and the Multivariate Adapted Constructed Analogs (MACA) that directly incorporate daily data from global climate models, were validated over the western US using global reanalysis data. While both methods outperformed results obtained from direct interpolation from reanalysis, MACA exhibited additional skill in temperature, humidity, wind, and precipitation due to its ability to jointly downscale temperature and dew point temperature, and its use of analog patterns rather than interpolation. Both downscaling methods exhibited value added information in tracking fire danger indices and periods of extreme fire danger; however, MACA outperformed the daily BCSD due to its ability to more accurately capture relative humidity and winds. Copyright © 2011 Royal Meteorological Society},
year = {2012}
}

@article {Grotch1991:GCMforclimate,
      author = "Stanley L.  Grotch and Michael C.  MacCracken",
      title = "The Use of General Circulation Models to Predict Regional Climatic Change",
      journal = "Journal of Climate",
      year = "1991",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "4",
      number = "3",
      doi = "https://doi.org/10.1175/1520-0442(1991)004<0286:TUOGCM>2.0.CO;2",
      pages=      "286 - 303",
      url = "https://journals.ametsoc.org/view/journals/clim/4/3/1520-0442_1991_004_0286_tuogcm_2_0_co_2.xml"
}

@article{Gutmann2014:statistical_downscaling_water_resources,
author = {Gutmann, Ethan and Pruitt, Tom and Clark, Martyn P. and Brekke, Levi and Arnold, Jeffrey R. and Raff, David A. and Rasmussen, Roy M.},
title = {An intercomparison of statistical downscaling methods used for water resource assessments in the United States},
journal = {Water Resources Research},
volume = {50},
number = {9},
pages = {7167-7186},
keywords = {statistical downscaling, Bias Corrected Spatial Disaggregation (BCSD), Bias Corrected Constructed Analog (BCCA), Asynchronous Regression},
doi = {https://doi.org/10.1002/2014WR015559},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014WR015559},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1002/2014WR015559},
abstract = {Abstract Information relevant for most hydrologic applications cannot be obtained directly from the native-scale outputs of climate models. As a result the climate model output must be downscaled, often using statistical methods. The plethora of statistical downscaling methods requires end-users to make a selection. This work is intended to provide end-users with aid in making an informed selection. We assess four commonly used statistical downscaling methods: daily and monthly disaggregated-to-daily Bias Corrected Spatial Disaggregation (BCSDd, BCSDm), Asynchronous Regression (AR), and Bias Corrected Constructed Analog (BCCA) as applied to a continental-scale domain and a regional domain (BCCAr). These methods are applied to the NCEP/NCAR Reanalysis, as a surrogate for a climate model, to downscale precipitation to a 12 km gridded observation data set. Skill is evaluated by comparing precipitation at daily, monthly, and annual temporal resolutions at individual grid cells and at aggregated scales. BCSDd and the BCCA methods overestimate wet day fraction, and underestimate extreme events. The AR method reproduces extreme events and wet day fraction well at the grid-cell scale, but over (under) estimates extreme events (wet day fraction) at aggregated scales. BCSDm reproduces extreme events and wet day fractions well at all space and time scales, but is limited to rescaling current weather patterns. In addition, we analyze the choice of calibration data set by looking at both a 12 km and a 6 km observational data set; the 6 km observed data set has more wet days and smaller extreme events than the 12 km product, the opposite of expected scaling.},
year = {2014}
}

@article{Hwang2014:Statistical_Downscaling_precipitations,
author = {Hwang, Syewoon and Graham, Wendy D.},
title = {Assessment of Alternative Methods for Statistically Downscaling Daily GCM Precipitation Outputs to Simulate Regional Streamflow},
journal = {JAWRA Journal of the American Water Resources Association},
volume = {50},
number = {4},
pages = {1010-1032},
keywords = {statistical downscaling, general circulation model (GCM), bias correction, spatiotemporal variability in daily precipitation, hydrologic implications, integrated hydrologic model (IHM)},
doi = {https://doi.org/10.1111/jawr.12154},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jawr.12154},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/jawr.12154},
abstract = {Abstract This study applied three statistical downscaling methods: (1) bias correction and spatial disaggregation at daily time scale (BCSD\_daily); (2) a modified version of BCSD which reverses the order of spatial disaggregation and bias correction (SDBC), and (3) the bias correction and stochastic analog method (BCSA) to downscale general circulation model daily precipitation outputs to the subbasin scale for west-central Florida. Each downscaled climate input dataset was then used in an integrated hydrologic model to examine differences in ability to simulate retrospective streamflow characteristics. Results showed the BCSD\_daily method consistently underestimated mean streamflow because the highly spatially correlated small precipitation events produced by this method resulted in overestimation of evapotranspiration. Highly spatially correlated large precipitation events produced by the SDBC method resulted in overestimation of the standard deviation of wet season daily streamflow and the magnitude/frequency of high streamflow events. BCSA showed better performance than the other methods in reproducing spatiotemporal statistics of daily precipitation and streamflow. This study demonstrated differences in statistical downscaling techniques propagate into significant differences in streamflow predictions, and underscores the need to carefully select a downscaling method that reproduces precipitation characteristics important for the hydrologic system under consideration.},
year = {2014}
}

@INPROCEEDINGS{Zhu2017:CycleGAN,
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={2242-2251},
  doi={10.1109/ICCV.2017.244}}


@inproceedings{
su2023:dual_diffusion,
title={Dual Diffusion Implicit Bridges for Image-to-Image Translation},
author={Xuan Su and Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=5HLoTvVGDe}
}

@article{bischoff2022:unpaired,
  title={Unpaired downscaling of fluid flows with diffusion bridges},
  author={Bischoff, Tobias and Deck, Katherine},
  journal={arXiv preprint arXiv:2305.01822},
  year={2023}
}

@inproceedings{
bortoli2021diffusion,
title={Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling},
author={Valentin De Bortoli and James Thornton and Jeremy Heng and Arnaud Doucet},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=9BnCwiXB0ty}
}


@article{cuturi2022ott,
title={Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein},
author={Cuturi, Marco and  Meng-Papaxanthos, Laetitia and Tian, Yingtao and Bunne, Charlotte  and Davis, Geoff and Teboul, Olivier},
journal={arXiv preprint arXiv:2201.12324},
year={2022}
}

@article{Candes2016:Superresolution,
author = {Candès, Emmanuel J. and Fernandez-Granda, Carlos},
title = {Towards a Mathematical Theory of Super-resolution},
journal = {Communications on Pure and Applied Mathematics},
volume = {67},
number = {6},
pages = {906-956},
doi = {https://doi.org/10.1002/cpa.21455},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21455},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21455},
abstract = {This paper develops a mathematical theory of super-resolution. Broadly speaking, super-resolution is the problem of recovering the fine details of an object—the high end of its spectrum—from coarse scale information only—from samples at the low end of the spectrum. Suppose we have many point sources at unknown locations in [0,1] and with unknown complex-valued amplitudes. We only observe Fourier samples of this object up to a frequency cutoff fc. We show that one can super-resolve these point sources with infinite precision—i.e., recover the exact locations and amplitudes—by solving a simple convex optimization problem, which can essentially be reformulated as a semidefinite program. This holds provided that the distance between sources is at least 2/fc. This result extends to higher dimensions and other models. In one dimension, for instance, it is possible to recover a piecewise smooth function by resolving the discontinuity points with infinite precision as well. We also show that the theory and methods are robust to noise. In particular, in the discrete setting we develop some theoretical results explaining how the accuracy of the super-resolved signal is expected to degrade when both the noise level and the super-resolution factor vary. © 2014 Wiley Periodicals, Inc.},
year = {2014}
}

@book{leveque_2002, 
place={Cambridge}, 
series={Cambridge Texts in Applied Mathematics}, 
title={Finite Volume Methods for Hyperbolic Problems}, 
DOI={10.1017/CBO9780511791253}, publisher={Cambridge University Press}, 
author={LeVeque, Randall J.}, 
year={2002}, 
collection={Cambridge Texts in Applied Mathematics}}

@article{chung2022diffusion,
  title={Diffusion posterior sampling for general noisy inverse problems},
  author={Chung, Hyungjin and Kim, Jeongsol and Mccann, Michael T and Klasky, Marc L and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2209.14687},
  year={2022}
}

@article{Zelinka2020,
author = {Zelinka, Mark D. and Myers, Timothy A. and McCoy, Daniel T. and Po-Chedley, Stephen and Caldwell, Peter M. and Ceppi, Paulo and Klein, Stephen A. and Taylor, Karl E.},
title = {Causes of Higher Climate Sensitivity in CMIP6 Models},
journal = {Geophysical Research Letters},
volume = {47},
number = {1},
pages = {e2019GL085782},
doi = {https://doi.org/10.1029/2019GL085782},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019GL085782},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019GL085782},
note = {e2019GL085782 10.1029/2019GL085782},
abstract = {Abstract Equilibrium climate sensitivity, the global surface temperature response to CO doubling, has been persistently uncertain. Recent consensus places it likely within 1.5–4.5 K. Global climate models (GCMs), which attempt to represent all relevant physical processes, provide the most direct means of estimating climate sensitivity via CO quadrupling experiments. Here we show that the closely related effective climate sensitivity has increased substantially in Coupled Model Intercomparison Project phase 6 (CMIP6), with values spanning 1.8–5.6 K across 27 GCMs and exceeding 4.5 K in 10 of them. This (statistically insignificant) increase is primarily due to stronger positive cloud feedbacks from decreasing extratropical low cloud coverage and albedo. Both of these are tied to the physical representation of clouds which in CMIP6 models lead to weaker responses of extratropical low cloud cover and water content to unforced variations in surface temperature. Establishing the plausibility of these higher sensitivity models is imperative given their implied societal ramifications.},
year = {2020}
}

@article{Christensen2008,
author = {Christensen, Jens H. and Boberg, Fredrik and Christensen, Ole B. and Lucas-Picher, Philippe},
title = {On the need for bias correction of regional climate change projections of temperature and precipitation},
journal = {Geophysical Research Letters},
volume = {35},
number = {20},
pages = {},
keywords = {regional climate change, temperature, precipitation},
doi = {https://doi.org/10.1029/2008GL035694},
url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2008GL035694},
eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2008GL035694},
abstract = {Within the framework of the European project ENSEMBLES (ensembles-based predictions of climate changes and their impacts) we explore the systematic bias in simulated monthly mean temperature and precipitation for an ensemble of thirteen regional climate models (RCMs). The models have been forced with the European Centre for Medium Range Weather Forecasting Reanalysis (ERA40) and are compared to a new high resolution gridded observational data set. We find that each model has a distinct systematic bias relating both temperature and precipitation bias to the observed mean. By excluding the twenty-five percent warmest and wettest months, respectively, we find that a derived second-order fit from the remaining months can be used to estimate the values of the excluded months. We demonstrate that the common assumption of bias cancellation (invariance) in climate change projections can have significant limitations when temperatures in the warmest months exceed 4–6 °C above present day conditions.},
year = {2008}
}

@article{Maraun2013:quantile_matching,
      author = "Douglas Maraun",
      title = "Bias Correction, Quantile Mapping, and Downscaling: Revisiting the Inflation Issue",
      journal = "Journal of Climate",
      year = "2013",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "26",
      number = "6",
      doi = "https://doi.org/10.1175/JCLI-D-12-00821.1",
      pages=      "2137 - 2143",
      url = "https://journals.ametsoc.org/view/journals/clim/26/6/jcli-d-12-00821.1.xml"
}

@article{Li2022:SRDiff,
title = {SRDiff: Single image super-resolution with diffusion probabilistic models},
journal = {Neurocomputing},
volume = {479},
pages = {47-59},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222000522},
author = {Haoying Li and Yifan Yang and Meng Chang and Shiqi Chen and Huajun Feng and Zhihai Xu and Qi Li and Yueting Chen},
keywords = {Single image super-resolution, Diffusion probabilistic model, Diverse results, Deep learning},
abstract = {Single image super-resolution (SISR) aims to reconstruct high-resolution (HR) images from given low-resolution (LR) images. It is an ill-posed problem because one LR image corresponds to multiple HR images. Recently, learning-based SISR methods have greatly outperformed traditional methods. However, PSNR-oriented, GAN-driven and flow-based methods suffer from over-smoothing, mode collapse and large model footprint issues, respectively. To solve these problems, we propose a novel SISR diffusion probabilistic model (SRDiff), which is the first diffusion-based model for SISR. SRDiff is optimized with a variant of the variational bound on the data likelihood. Through a Markov chain, it can provide diverse and realistic super-resolution (SR) predictions by gradually transforming Gaussian noise into a super-resolution image conditioned on an LR input. In addition, we introduce residual prediction to the whole framework to speed up model convergence. Our extensive experiments on facial and general benchmarks (CelebA and DIV2K datasets) show that (1) SRDiff can generate diverse SR results with rich details and achieve competitive performance against other state-of-the-art methods, when given only one LR input; (2) SRDiff is easy to train with a small footprint(The word “footprint” in this paper represents “model size” (number of model parameters).); (3) SRDiff can perform flexible image manipulation operations, including latent space interpolation and content fusion.}
}


@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  year={2009},
  publisher={Springer Berlin Heidelberg}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}
@article{efron2011tweedie,
  title={Tweedie’s formula and selection bias},
  author={Efron, Bradley},
  journal={Journal of the American Statistical Association},
  volume={106},
  number={496},
  pages={1602--1614},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{Agueh2011:Barycenter_map,
author = {Agueh, Martial and Carlier, Guillaume},
title = {Barycenters in the Wasserstein Space},
journal = {SIAM Journal on Mathematical Analysis},
volume = {43},
number = {2},
pages = {904-924},
year = {2011},
doi = {10.1137/100805741},
URL = {https://doi.org/10.1137/100805741},
eprint = {https://doi.org/10.1137/100805741},
bstract = { In this paper, we introduce a notion of barycenter in the Wasserstein space which generalizes McCann's interpolation to the case of more than two measures. We provide existence, uniqueness, characterizations, and regularity of the barycenter and relate it to the multimarginal optimal transport problem considered by Gangbo and Święch in [Comm. Pure Appl. Math., 51 (1998), pp. 23–45]. We also consider some examples and, in particular, rigorously solve the Gaussian case. We finally discuss convexity of functionals in the Wasserstein space. }
}


@article{Naveena2022:downscaling_temperature,
    author = {Naveena, N. and Satyanarayana, G. Ch. and Umakanth, N. and Rao, M. C. and Avinash, B. and Jaswanth, J. and Reddy, M. S. Sainath},
    title = "{Statistical downscaling in maximum temperature future climatology}",
    journal = {AIP Conference Proceedings},
    volume = {2357},
    number = {1},
    year = {2022},
    month = {05},
    abstract = "{Statistical downscaling of global models historical maximum temperature record were performed for 20 years during the base period 1986-2005 using Global Coupled Models (GCM), such as the Coupled Model Inter-comparison phase 5 (CMIP5).This period is termed as a ‘reference period’ for future projection of heat wave. The extreme weather events such as heat waves have a significant impact on the Indian climate. The frequency, intensity, duration and number of spells of these heat waves are increasing in the recent years. Local to regional scale climate cannot be predicted by GCM’s. Statistical downscaling method such as Bias-Correction Spatial Disaggregation (BCSD) method is used in generating the statistical downscaling algorithm to tackle the present limitations of GCM outputs. Four models such as CanESM2, MIROC-ESM-CHEM, IPSL-CM5A-LR and NorESM1-M from CMIP5 models were chosen and biases are corrected for Indian domain.Various geographical regions of India have been examined. Observational data is taken from India Meteorological Department (IMD) gridded daily maximum temperature data at 1 degree resolution for the same period. The CMIP5 models data has been obtained and it is downscaled using statistical methods. The performance of this downscale data is tested using parameters such as Mean Absolute Error (MAE), Root Mean square Error (RMSE), Correlation cofficienbetween each simulated model and observational data for the same time period over the study area, bias of each model from IMD observational data, Mean percentage error, Index of Agreement with given observation and model time series. By using this method biases were removed to greater extent and correlation coefficient also increased. This method can be applied to future data for bias correction and to improve the correlation.}",
    issn = {0094-243X},
    doi = {10.1063/5.0081087},
    url = {https://doi.org/10.1063/5.0081087},
    note = {030026},
    eprint = {https://pubs.aip.org/aip/acp/article-pdf/doi/10.1063/5.0081087/16194367/030026\_1\_online.pdf},
}

@article{schneider2017climate,
title={Climate goals and computing the future of clouds},
author={Schneider, Tapio and Teixeira, Jo{~a}o and Bretherton, Christopher S and Brient, Florent and Pressel, Kyle G and  Schär, Christoph and Siebesma,  A. Pier },
journal={Nature Climate Change},
volume={7},
number={1},
pages={3--5},
year={2017},
publisher={Springer Nature}
}









@article{Balaji2022:GCMs,
author = {V. Balaji  and Fleur Couvreux  and Julie Deshayes  and Jacques Gautrais  and Frédéric Hourdin  and Catherine Rio },
title = {Are general circulation models obsolete?},
journal = {Proceedings of the National Academy of Sciences},
volume = {119},
number = {47},
pages = {e2202075119},
year = {2022},
doi = {10.1073/pnas.2202075119},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2202075119},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2202075119},
abstract = {Traditional general circulation models, or GCMs—that is, three-dimensional dynamical models with unresolved terms represented in equations with tunable parameters—have been a mainstay of climate research for several decades, and some of the pioneering studies have recently been recognized by a Nobel prize in Physics. Yet, there is considerable debate around their continuing role in the future. Frequently mentioned as limitations of GCMs are the structural error and uncertainty across models with different representations of unresolved scales and the fact that the models are tuned to reproduce certain aspects of the observed Earth. We consider these shortcomings in the context of a future generation of models that may address these issues through substantially higher resolution and detail, or through the use of machine learning techniques to match them better to observations, theory, and process models. It is our contention that calibration, far from being a weakness of models, is an essential element in the simulation of complex systems, and contributes to our understanding of their inner workings. Models can be calibrated to reveal both fine-scale detail and the global response to external perturbations. New methods enable us to articulate and improve the connections between the different levels of abstract representation of climate processes, and our understanding resides in an entire hierarchy of models where GCMs will continue to play a central role for the foreseeable future.}}

@article{Hall2014:Projecting_regional_change,
author = {Alex Hall },
title = {Projecting regional change},
journal = {Science},
volume = {346},
number = {6216},
pages = {1461-1462},
year = {2014},
doi = {10.1126/science.aaa0629},
URL = {https://www.science.org/doi/abs/10.1126/science.aaa0629},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aaa0629},
abstract = {How accurate are regional projections of climate change derived from downscaling global climate model results? Techniques to downscale global climate model (GCM) output and produce high-resolution climate change projections have emerged over the past two decades. GCM projections of future climate change, with typical resolutions of about 100 km, are now routinely downscaled to resolutions as high as hundreds of meters. Pressure to use these techniques to produce policy-relevant information is enormous. To prevent bad decisions, the climate science community must identify downscaling's strengths and limitations and develop best practices. A starting point for this discussion is to acknowledge that downscaled climate signals arising from warming are more credible than those arising from circulation changes.}}

@InProceedings{dong2014learning,
author="Dong, Chao
and Loy, Chen Change
and He, Kaiming
and Tang, Xiaoou",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Learning a Deep Convolutional Network for Image Super-Resolution",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="184--199",
abstract="We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) [15] that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.",
isbn="978-3-319-10593-2"
}

@article{tian2022generative,
title={Generative Adversarial Networks for Image Super-Resolution: A Survey},
author={Tian, Chunwei and Zhang, Xuanyu and Lin, Jerry Chun-Wen and Zuo, Wangmeng and Zhang, Yanning},
journal={arXiv preprint arXiv:2204.13620},
year={2022}
}

@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36479--36494},
  year={2022}
}
@article{song2020improved,
  title={Improved techniques for training score-based generative models},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12438--12448},
  year={2020}
}
@inproceedings{choi2022perception,
  title={Perception prioritized training of diffusion models},
  author={Choi, Jooyoung and Lee, Jungbeom and Shin, Chaehun and Kim, Sungwon and Kim, Hyunwoo and Yoon, Sungroh},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11472--11481},
  year={2022}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{chung2022improving,
  title={Improving Diffusion Models for Inverse Problems using Manifold Constraints},
  author={Chung, Hyungjin and Sim, Byeongsu and Ryu, Dohoon and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2206.00941},
  year={2022}
}

@inproceedings{lugmayr2020srflow,
  title={{SRF}low: Learning the Super-Resolution Space with Normalizing Flow},
  author={Lugmayr, Andreas and Danelljan, Martin and Van Gool, Luc and Timofte, Radu},
  booktitle={ECCV},
  year={2020}
}

@article{shu2023physics,
  title={A physics-informed diffusion model for high-fidelity flow field reconstruction},
  author={Shu, Dule and Li, Zijie and Farimani, Amir Barati},
  journal={Journal of Computational Physics},
  volume={478},
  pages={111972},
  year={2023},
  publisher={Elsevier}
}

@book{scott2015multivariate,
  title={Multivariate density estimation: theory, practice, and visualization},
  author={Scott, David W},
  year={2015},
  publisher={John Wiley \& Sons}
}

@inproceedings{
kawar2021snips,
title={{SNIPS}: Solving Noisy Inverse Problems Stochastically},
author={Bahjat Kawar and Gregory Vaksman and Michael Elad},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=pBKOx_dxYAN}
}

@inproceedings{
kawar2022denoising,
title={Denoising Diffusion Restoration Models},
author={Bahjat Kawar and Michael Elad and Stefano Ermon and Jiaming Song},
booktitle={ICLR Workshop on Deep Generative Models for Highly Structured Data},
year={2022},
url={https://openreview.net/forum?id=BExXihVOvWq}
}


@InProceedings{pmlrfinzi23a,
  title = {User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems},
  author = {Finzi, Marc Anton and Boral, Anudhyan and Wilson, Andrew Gordon and Sha, Fei and Zepeda-Nunez, Leonardo},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages = {10136--10152},
  year = {2023},
  editor = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = {202},
  series = {Proceedings of Machine Learning Research},
  month = {23--29 Jul},
  publisher = {PMLR},
  pdf = {https://proceedings.mlr.press/v202/finzi23a/finzi23a.pdf},
  url = {https://proceedings.mlr.press/v202/finzi23a.html},
  abstract = {Diffusion models are a class of probabilistic generative models that have been widely used as a prior for image processing tasks like text conditional generation and inpainting. We demonstrate that these models can be adapted to make predictions and provide uncertainty quantification for chaotic dynamical systems. In these applications, diffusion models can implicitly represent knowledge about outliers and extreme events; however, querying that knowledge through conditional sampling or measuring probabilities is surprisingly difficult. Existing methods for conditional sampling at inference time seek mainly to enforce the constraints, which is insufficient to match the statistics of the distribution or compute the probability of the chosen events. To achieve these ends, optimally one would use the conditional score function, but its computation is typically intractable. In this work, we develop a probabilistic approximation scheme for the conditional score function which provably converges to the true distribution as the noise level decreases. With this scheme we are able to sample conditionally on nonlinear user-defined events at inference time, and matches data statistics even when sampling from the tails of the distribution.}
}


@ARTICLE{ResLap,
  author={Cheng, Jianxin and Kuang, Qiuming and Shen, Chenkai and Liu, Jin and Tan, Xicheng and Liu, Wang},
  journal={IEEE Access}, 
  title={ResLap: Generating High-Resolution Climate Prediction Through Image Super-Resolution}, 
  year={2020},
  volume={8},
  number={},
  pages={39623-39634},
  doi={10.1109/ACCESS.2020.2974785}}


@inbook{maraun_widmann_2018,
place={Cambridge},
title={Perfect Prognosis},
DOI={10.1017/9781107588783.012},
booktitle={Statistical Downscaling and Bias Correction for Climate Research},
publisher={Cambridge University Press},
author={Maraun, Douglas and Widmann, Martin},
year={2018},
pages={141–169}}

@article{harder2022generating,
  title={Generating physically-consistent high-resolution climate data with hard-constrained neural networks},
  author={Harder, Paula and Yang, Qidong and Ramesh, Venkatesh and Sattigeri, Prasanna and Hernandez-Garcia, Alex and Watson, Campbell and Szwarcman, Daniela and Rolnick, David},
  journal={arXiv preprint arXiv:2208.05424},
  year={2022}
}