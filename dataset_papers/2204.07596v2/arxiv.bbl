\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abavisani et~al.(2020)Abavisani, Naghizadeh, Metaxas, and
  Patel]{abavisani2020deep}
Mahdi Abavisani, Alireza Naghizadeh, Dimitris~N Metaxas, and Vishal~M Patel.
\newblock Deep subspace clustering with data augmentation.
\newblock In \emph{Thirty-fourth Conference on Neural Information Processing
  Systems}, 2020.

\bibitem[Arora et~al.(2019)Arora, Khandeparkar, Khodak, Plevrakis, and
  Saunshi]{arora2019theoretical}
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and
  Nikunj Saunshi.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock \emph{arXiv preprint arXiv:1902.09229}, 2019.

\bibitem[Borodachov et~al.(2019)Borodachov, Hardin, and
  Saff]{borodachov2019discrete}
Sergiy~V Borodachov, Douglas~P Hardin, and Edward~B Saff.
\newblock \emph{Discrete energy on rectifiable sets}.
\newblock Springer, 2019.

\bibitem[Bostock(2018)]{bostock2018imagenet}
Mike Bostock.
\newblock Imagenet hierarchy, 2018.
\newblock URL \url{https://observablehq.com/@mbostock/imagenet-hierarchy}.

\bibitem[Bukchin et~al.(2021)Bukchin, Schwartz, Saenko, Shahar, Feris, Giryes,
  and Karlinsky]{bukchin2021fine}
Guy Bukchin, Eli Schwartz, Kate Saenko, Ori Shahar, Rogerio Feris, Raja Giryes,
  and Leonid Karlinsky.
\newblock Fine-grained angular contrastive learning with coarse labels.
\newblock In \emph{2021 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}. IEEE, Jun 2021.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020swav}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}. PMLR,
  2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and
  He]{chen2020mocov2}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{b}}.

\bibitem[Codella et~al.(2019)Codella, Rotemberg, Tschandl, Celebi, Dusza,
  Gutman, Helba, Kalloo, Liopyris, Marchetti, et~al.]{codella2019skin}
Noel Codella, Veronica Rotemberg, Philipp Tschandl, M~Emre Celebi, Stephen
  Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael
  Marchetti, et~al.
\newblock Skin lesion analysis toward melanoma detection 2018: A challenge
  hosted by the international skin imaging collaboration (isic).
\newblock \emph{arXiv preprint arXiv:1902.03368}, 2019.

\bibitem[Dao et~al.(2019)Dao, Gu, Ratner, Smith, De~Sa, and
  R\'e]{dao2019kernel}
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De~Sa, and
  Christopher R\'e.
\newblock A kernel theory of modern data augmentation.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  1528--1537. PMLR, 09--15 Jun 2019.

\bibitem[d'Eon et~al.(2021)d'Eon, d'Eon, Wright, and
  Leyton-Brown]{d2021spotlight}
Greg d'Eon, Jason d'Eon, James~R Wright, and Kevin Leyton-Brown.
\newblock The spotlight: A general method for discovering systematic errors in
  deep learning models.
\newblock \emph{arXiv preprint arXiv:2107.00758}, 2021.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Duchi et~al.(2020)Duchi, Hashimoto, and
  Namkoong]{duchi2020distributionally}
John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong.
\newblock Distributionally robust losses for latent covariate mixtures.
\newblock \emph{arXiv preprint arXiv:2007.13982}, 2020.

\bibitem[Epstein and Meir(2019)]{epstein2019generalization}
Baruch Epstein and Ron Meir.
\newblock Generalization bounds for unsupervised and semi-supervised learning
  with autoencoders.
\newblock \emph{arXiv preprint arXiv:1902.01449}, 2019.

\bibitem[Falcon and Cho(2020)]{falcon2020framework}
William Falcon and Kyunghyun Cho.
\newblock A framework for contrastive self-supervised learning and designing a
  new approach.
\newblock \emph{arXiv preprint arXiv:2009.00104}, 2020.

\bibitem[Galanti et~al.(2021)Galanti, Gy{\"o}rgy, and Hutter]{galanti2021role}
Tomer Galanti, Andr{\'a}s Gy{\"o}rgy, and Marcus Hutter.
\newblock On the role of neural collapse in transfer learning.
\newblock \emph{arXiv preprint arXiv:2112.15121}, 2021.

\bibitem[Goel et~al.(2020)Goel, Gu, Li, and Re]{goel2020model}
Karan Goel, Albert Gu, Yixuan Li, and Christopher Re.
\newblock Model patching: Closing the subgroup performance gap with data
  augmentation.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Goyal et~al.(2021)Goyal, Caron, Lefaudeux, Xu, Wang, Pai, Singh,
  Liptchinsky, Misra, Joulin, et~al.]{goyal2021self}
Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek
  Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et~al.
\newblock Self-supervised pretraining of visual features in the wild.
\newblock \emph{arXiv preprint arXiv:2103.01988}, 2021.

\bibitem[Graf et~al.(2021)Graf, Hofer, Niethammer, and
  Kwitt]{graf2021dissecting}
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt.
\newblock Dissecting supervised constrastive learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3821--3830. PMLR, 2021.

\bibitem[Guo et~al.(2018)Guo, Zhu, Liu, and Yin]{guo2018deep}
Xifeng Guo, En~Zhu, Xinwang Liu, and Jianping Yin.
\newblock Deep embedded clustering with data augmentation.
\newblock In \emph{Asian conference on machine learning}, pages 550--565. PMLR,
  2018.

\bibitem[Han et~al.(2021)Han, Papyan, and Donoho]{han2021neural}
X.~Y. Han, Vardan Papyan, and David~L. Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the
  central path, 2021.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
Jeff~Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock \emph{arXiv preprint arXiv:2106.04156}, 2021.

\bibitem[He et~al.(2019)He, Fan, Wu, Xie, and Girshick]{he2019moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock \emph{arXiv preprint arXiv:1911.05722}, 2019.

\bibitem[Hoffmann et~al.(2001)Hoffmann, Kwok, and Compton]{hoffmann2001using}
Achim Hoffmann, Rex Kwok, and Paul Compton.
\newblock Using subclasses to improve classification learning.
\newblock In \emph{European Conference on Machine Learning}, pages 203--213.
  Springer, 2001.

\bibitem[Huang et~al.(2021)Huang, Yi, and Zhao]{huang2021generalization}
Weiran Huang, Mingyang Yi, and Xuyang Zhao.
\newblock Towards the generalization of contrastive self-supervised learning,
  2021.

\bibitem[Hui et~al.(2022)Hui, Belkin, and Nakkiran]{hui2022limitations}
Like Hui, Mikhail Belkin, and Preetum Nakkiran.
\newblock Limitations of neural collapse for understanding generalization in
  deep learning, 2022.

\bibitem[Islam et~al.(2021)Islam, Chen, Panda, Karlinsky, Radke, and
  Feris]{islam2021broad}
Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky, Richard Radke,
  and Rogerio Feris.
\newblock A broad study on the transferability of visual representations with
  contrastive learning.
\newblock \emph{arXiv preprint arXiv:2103.13517}, 2021.

\bibitem[Jing et~al.(2021)Jing, Vincent, LeCun, and
  Tian]{jing2021understanding}
Li~Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2110.09348}, 2021.

\bibitem[Khosla et~al.(2011)Khosla, Jayadevaprakash, Yao, and
  Li]{khosla2011novel}
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li.
\newblock Novel dataset for fine-grained image categorization: Stanford dogs.
\newblock In \emph{Proc. CVPR workshop on fine-grained visual categorization
  (FGVC)}, volume~2. Citeseer, 2011.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Mschinot, Liu, and Krishnan]{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Mschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock In \emph{Thirty-Fourth Conference on Neural Information Processing
  Systems}, 2020.

\bibitem[Kothapalli et~al.(2022)Kothapalli, Rasromani, and
  Awatramani]{kothapalli2022neural}
Vignesh Kothapalli, Ebrahim Rasromani, and Vasudev Awatramani.
\newblock Neural collapse: A review on modelling principles and generalization,
  2022.

\bibitem[Le et~al.(2018)Le, Patterson, and White]{le2018supervised}
Lei Le, Andrew Patterson, and Martha White.
\newblock Supervised autoencoders: Improving generalization performance with
  unsupervised regularizers.
\newblock In \emph{Thirty-second Conference on Neural Information Processing
  Systems}, 2018.

\bibitem[Le and Yang(2015)]{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Liu et~al.(2021)Liu, Haghgoo, Chen, Raghunathan, Koh, Sagawa, Liang,
  and Finn]{liu2021just}
Evan~Z Liu, Behzad Haghgoo, Annie~S Chen, Aditi Raghunathan, Pang~Wei Koh,
  Shiori Sagawa, Percy Liang, and Chelsea Finn.
\newblock Just train twice: Improving group robustness without training group
  information.
\newblock In \emph{International Conference on Machine Learning}, pages
  6781--6792. PMLR, 2021.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem[Lu and Steinerberger(2020)]{lu2020neural}
Jianfeng Lu and Stefan Steinerberger.
\newblock Neural collapse with cross-entropy loss, 2020.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Oakden-Rayner et~al.(2020)Oakden-Rayner, Dunnmon, Carneiro, and
  R{\'e}]{oakden2020hidden}
Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher R{\'e}.
\newblock Hidden stratification causes clinically meaningful failures in
  machine learning for medical imaging.
\newblock In \emph{Proceedings of the ACM conference on health, inference, and
  learning}, pages 151--159, 2020.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Vardan Papyan, XY~Han, and David~L Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Robinson et~al.(2020)Robinson, Chuang, Sra, and
  Jegelka]{robinson2020contrastive}
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka.
\newblock Contrastive learning with hard negative samples.
\newblock \emph{arXiv preprint arXiv:2010.04592}, 2020.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019groupdro}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Sohoni et~al.(2020)Sohoni, Dunnmon, Angus, Gu, and
  R{\'e}]{sohoni2020george}
Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R{\'e}.
\newblock No subclass left behind: Fine-grained robustness in coarse-grained
  classification problems.
\newblock \emph{Thirty-fourth Conference on Neural Information Processing
  Systems}, 2020.

\bibitem[Thomson(1897)]{thomson1897cathode}
J.~J. Thomson.
\newblock Xl. cathode rays.
\newblock \emph{The London, Edinburgh, and Dublin Philosophical Magazine and
  Journal of Science}, 44\penalty0 (269):\penalty0 293--316, 1897.

\bibitem[Tian et~al.(2020)Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian2020makes}
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and
  Phillip Isola.
\newblock What makes for good views for contrastive learning?
\newblock \emph{arXiv preprint arXiv:2005.10243}, 2020.

\bibitem[Tsai et~al.(2020)Tsai, Wu, Salakhutdinov, and Morency]{tsai2020self}
Yao-Hung~Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency.
\newblock Self-supervised learning from a multi-view perspective.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Tschannen et~al.(2020)Tschannen, Djolonga, Rubenstein, Gelly, and
  Lucic]{tschannen2020on}
Michael Tschannen, Josip Djolonga, Paul~K. Rubenstein, Sylvain Gelly, and Mario
  Lucic.
\newblock On mutual information maximization for representation learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wang and Isola(2020)]{wang2020understanding}
Tongzhou Wang and Phillip Isola.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{International Conference on Machine Learning}, pages
  9929--9939. PMLR, 2020.

\bibitem[Welinder et~al.(2010{\natexlab{a}})Welinder, Branson, Mita, Wah,
  Schroff, Belongie, and Perona]{WelinderEtal2010}
P.~Welinder, S.~Branson, T.~Mita, C.~Wah, F.~Schroff, S.~Belongie, and
  P.~Perona.
\newblock {Caltech-UCSD Birds 200}.
\newblock Technical Report CNS-TR-2010-001, California Institute of Technology,
  2010{\natexlab{a}}.

\bibitem[Welinder et~al.(2010{\natexlab{b}})Welinder, Branson, Mita, Wah,
  Schroff, Belongie, and Perona]{welinder2010caltech}
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff,
  Serge Belongie, and Pietro Perona.
\newblock Caltech-ucsd birds 200.
\newblock 2010{\natexlab{b}}.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Zhou et~al.(2014)Zhou, Lapedriza, Xiao, Torralba, and
  Oliva]{zhou2014learning}
Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva.
\newblock Learning deep features for scene recognition using places database.
\newblock \emph{Twenty-eighth Conference on Neural Information Processing
  Systems}, 2014.

\bibitem[Zimmermann et~al.(2021)Zimmermann, Sharma, Schneider, Bethge, and
  Brendel]{zimmermann2021contrastive}
Roland~S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and
  Wieland Brendel.
\newblock Contrastive learning inverts the data generating process.
\newblock \emph{arXiv preprint arXiv:2012.08850}, 2021.

\end{thebibliography}
