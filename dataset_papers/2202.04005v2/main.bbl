\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.

\bibitem[Alaoui \& Mahoney(2014)Alaoui and Mahoney]{alaoui2014fast}
Alaoui, A.~E. and Mahoney, M.~W.
\newblock Fast randomized kernel methods with statistical guarantees.
\newblock \emph{arXiv preprint arXiv:1411.0306}, 2014.

\bibitem[Antonini et~al.(2008)Antonini, Kozachenko, and
  Volodin]{antonini2008convergence}
Antonini, R.~G., Kozachenko, Y., and Volodin, A.
\newblock Convergence of series of dependent $\varphi$-subgaussian random
  variables.
\newblock \emph{Journal of Mathematical Analysis and Applications},
  338\penalty0 (2):\penalty0 1188--1203, 2008.

\bibitem[Burt et~al.(2019)Burt, Rasmussen, and Van Der~Wilk]{Burt2019Rates}
Burt, D., Rasmussen, C.~E., and Van Der~Wilk, M.
\newblock Rates of convergence for sparse variational {G}aussian process
  regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  862--871, 2019.

\bibitem[Burt et~al.(2020)Burt, Rasmussen, and van~der Wilk]{JMLR:v21:19-1015}
Burt, D.~R., Rasmussen, C.~E., and van~der Wilk, M.
\newblock Convergence of sparse variational inference in {G}aussian processes
  regression.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (131):\penalty0 1--63, 2020.

\bibitem[Calandriello et~al.(2019)Calandriello, Carratino, Lazaric, Valko, and
  Rosasco]{Calandriello2019Adaptive}
Calandriello, D., Carratino, L., Lazaric, A., Valko, M., and Rosasco, L.
\newblock Gaussian process optimization with adaptive sketching: Scalable and
  no regret.
\newblock In \emph{Conference on Learning Theory}, 2019.

\bibitem[Calandriello et~al.(2022)Calandriello, Carratino, Lazaric, Valko, and
  Rosasco]{calandriello2022scaling}
Calandriello, D., Carratino, L., Lazaric, A., Valko, M., and Rosasco, L.
\newblock Scaling {G}aussian process optimization by evaluating a few unique
  candidates multiple times.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Camilleri et~al.(2021)Camilleri, Jamieson, and
  Katz-Samuels]{camilleri2021high}
Camilleri, R., Jamieson, K., and Katz-Samuels, J.
\newblock High-dimensional experimental design and kernel bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1227--1237. PMLR, 2021.

\bibitem[Chareka et~al.(2006)Chareka, Chareka, and
  Kennendy]{chareka2006locally}
Chareka, P., Chareka, O., and Kennendy, S.
\newblock Locally sub-{G}aussian random variable and the strong law of large
  numbers.
\newblock \emph{Atlantic Electronic Journal of Mathematics}, 1\penalty0
  (1):\penalty0 75--81, 2006.

\bibitem[Chen \& Yang(2021)Chen and Yang]{chen2021fast}
Chen, Y. and Yang, Y.
\newblock Fast statistical leverage score approximation in kernel ridge
  regression.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2935--2943. PMLR, 2021.

\bibitem[Chowdhury \& Gopalan(2017)Chowdhury and Gopalan]{Chowdhury2017bandit}
Chowdhury, S.~R. and Gopalan, A.
\newblock On kernelized multi-armed bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  844--853, 2017.

\bibitem[Cover(1999)]{cover1999elementsold}
Cover, T.~M.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 1999.

\bibitem[Hensman et~al.(2013)Hensman, Fusi, and Lawrence]{Hensman2013}
Hensman, J., Fusi, N., and Lawrence, N.~D.
\newblock Gaussian processes for big data.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence
  ({UAI})}, 2013.

\bibitem[Janz et~al.(2020)Janz, Burt, and Gonzalez]{Janz2020SlightImprov}
Janz, D., Burt, D., and Gonzalez, J.
\newblock Bandit optimisation of functions in the matern kernel {RKHS}.
\newblock In \emph{Proceedings of Machine Learning Research}, volume 108, pp.\
  2486--2495. PMLR, 26--28 Aug 2020.

\bibitem[Kanagawa et~al.(2018)Kanagawa, Hennig, Sejdinovic, and
  Sriperumbudur]{Kanagawa2018}
Kanagawa, M., Hennig, P., Sejdinovic, D., and Sriperumbudur, B.~K.
\newblock Gaussian processes and kernel methods: A review on connections and
  equivalences.
\newblock \emph{arXiv preprint arXiv:1807.02582}, 2018.

\bibitem[Li \& Scarlett(2022)Li and Scarlett]{li2021gaussian}
Li, Z. and Scarlett, J.
\newblock Gaussian process bandit optimization with few batches.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2022.

\bibitem[Lin \& Cevher(2018)Lin and Cevher]{lin2018optimal}
Lin, J. and Cevher, V.
\newblock Optimal rates of sketched-regularized algorithms for least-squares
  regression over {H}ilbert spaces.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3102--3111. PMLR, 2018.

\bibitem[Liu et~al.(2020)Liu, Ong, Shen, and Cai]{liu2020gaussian}
Liu, H., Ong, Y.-S., Shen, X., and Cai, J.
\newblock When {G}aussian process meets big data: A review of scalable {GP}s.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  31\penalty0 (11):\penalty0 4405--4423, 2020.

\bibitem[Matthews et~al.(2016)Matthews, Hensman, Turner, and
  Ghahramani]{matthews2016sparse}
Matthews, A. G. d.~G., Hensman, J., Turner, R., and Ghahramani, Z.
\newblock On sparse variational methods and the {K}ullback-{L}eibler divergence
  between stochastic processes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  231--239, 2016.

\bibitem[Matthews et~al.(2017)Matthews, {van der Wilk}, Nickson, Fujii,
  {Boukouvalas}, {Le{\'o}n-Villagr{\'a}}, Ghahramani, and Hensman]{GPflow2017}
Matthews, A. G. d.~G., {van der Wilk}, M., Nickson, T., Fujii, K.,
  {Boukouvalas}, A., {Le{\'o}n-Villagr{\'a}}, P., Ghahramani, Z., and Hensman,
  J.
\newblock {{GP}flow: A {G}aussian process library using {T}ensor{F}low}.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (40):\penalty0 1--6, apr 2017.

\bibitem[Mockus et~al.(1978)Mockus, Tiesis, and
  Zilinskas]{mockus1978application}
Mockus, J., Tiesis, V., and Zilinskas, A.
\newblock The application of {B}ayesian methods for seeking the extremum.
\newblock \emph{Towards Global Optimization}, 2\penalty0 (117-129):\penalty0 2,
  1978.

\bibitem[Musco \& Musco(2016)Musco and Musco]{musco2016recursive}
Musco, C. and Musco, C.
\newblock Recursive sampling for the {N}ystr\"om method.
\newblock \emph{arXiv preprint arXiv:1605.07583}, 2016.

\bibitem[Nieman et~al.(2021)Nieman, Szabo, and van
  Zanten]{nieman2021contraction}
Nieman, D., Szabo, B., and van Zanten, H.
\newblock Contraction rates for sparse variational approximations in {G}aussian
  process regression.
\newblock \emph{arXiv preprint arXiv:2109.10755}, 2021.

\bibitem[Rasmussen \& Williams(2006)Rasmussen and Williams]{Rasmussen2006}
Rasmussen, C.~E. and Williams, C.~K.
\newblock \emph{{Gaussian Processes for Machine Learning}}.
\newblock MIT Press, 2006.

\bibitem[Rossi et~al.(2021)Rossi, Heinonen, Bonilla, Shen, and
  Filippone]{rossi2021sparse}
Rossi, S., Heinonen, M., Bonilla, E., Shen, Z., and Filippone, M.
\newblock Sparse {G}aussian processes revisited: Bayesian approaches to
  inducing-variable approximations.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1837--1845. PMLR, 2021.

\bibitem[Rudi et~al.(2015)Rudi, Camoriano, and Rosasco]{rudi2015less}
Rudi, A., Camoriano, R., and Rosasco, L.
\newblock Less is more: Nystr{\"o}m computational regularization.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Salgia et~al.(2021)Salgia, Vakili, and Zhao]{salgia2021domain}
Salgia, S., Vakili, S., and Zhao, Q.
\newblock A domain-shrinking based {B}ayesian optimization algorithm with
  order-optimal regret performance.
\newblock \emph{Conference on Neural Information Processing Systems}, 34, 2021.

\bibitem[Scarlett et~al.(2017)Scarlett, Bogunovic, and
  Cevher]{Scarlett2017Lower}
Scarlett, J., Bogunovic, I., and Cevher, V.
\newblock Lower bounds on regret for noisy {G}aussian process bandit
  optimization.
\newblock In \emph{Conference on Learning Theory}, pp.\  1723--1742, 2017.

\bibitem[Sch{\"o}lkopf et~al.(2002)Sch{\"o}lkopf, Smola, Bach,
  et~al.]{scholkopf2002learning}
Sch{\"o}lkopf, B., Smola, A.~J., Bach, F., et~al.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Seeger et~al.(2003)Seeger, Williams, and Lawrence]{seeger2003fast}
Seeger, M.~W., Williams, C.~K., and Lawrence, N.~D.
\newblock Fast forward selection to speed up sparse {G}aussian process
  regression.
\newblock In \emph{International Workshop on Artificial Intelligence and
  Statistics}, pp.\  254--261. PMLR, 2003.

\bibitem[{Shahriari} et~al.(2016){Shahriari}, {Swersky}, {Wang}, {Adams}, and
  {de Freitas}]{Shahriari2016outofloop}
{Shahriari}, B., {Swersky}, K., {Wang}, Z., {Adams}, R.~P., and {de Freitas},
  N.
\newblock Taking the human out of the loop: A review of {B}ayesian
  optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2016.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2010gaussian}
Srinivas, N., Krause, A., Kakade, S., and Seeger, M.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1015--1022, 2010.

\bibitem[Titsias(2009)]{Titsias2009Variational}
Titsias, M.~K.
\newblock Variational learning of inducing variables in sparse {G}aussian
  processes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  567--574, 2009.

\bibitem[Vakili et~al.(2021{\natexlab{a}})Vakili, Bouziani, Jalali, Bernacchia,
  and Shiu]{vakili2021optimal}
Vakili, S., Bouziani, N., Jalali, S., Bernacchia, A., and Shiu, D.-s.
\newblock Optimal order simple regret for {G}aussian process bandits.
\newblock In \emph{Conference on Neural Information Processing Systems},
  2021{\natexlab{a}}.

\bibitem[Vakili et~al.(2021{\natexlab{b}})Vakili, Bromberg, Garcia, Shiu, and
  Bernacchia]{vakili2021uniform}
Vakili, S., Bromberg, M., Garcia, J., Shiu, D.-s., and Bernacchia, A.
\newblock Uniform generalization bounds for overparameterized neural networks.
\newblock \emph{arXiv preprint arXiv:2109.06099}, 2021{\natexlab{b}}.

\bibitem[Vakili et~al.(2021{\natexlab{c}})Vakili, Khezeli, and
  Picheny]{vakili2020information}
Vakili, S., Khezeli, K., and Picheny, V.
\newblock On information gain and regret bounds in {Gaussian} process bandits.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  82--90, 2021{\natexlab{c}}.

\bibitem[Vakili et~al.(2021{\natexlab{d}})Vakili, Picheny, and
  Artemev]{Vakili2020Scalable}
Vakili, S., Picheny, V., and Artemev, A.
\newblock Scalable {T}hompson sampling using sparse {G}aussian process models.
\newblock In \emph{Conference on Neural Information Processing Systems},
  2021{\natexlab{d}}.

\bibitem[Vakili et~al.(2021{\natexlab{e}})Vakili, Scarlett, and
  Javidi]{vakili2021open}
Vakili, S., Scarlett, J., and Javidi, T.
\newblock Open problem: Tight online confidence intervals for {RKHS} elements.
\newblock In \emph{Conference on Learning Theory}, pp.\  4647--4652. PMLR,
  2021{\natexlab{e}}.

\bibitem[Valko et~al.(2013)Valko, Korda, Munos, Flaounas, and
  Cristianini]{Valko2013kernelbandit}
Valko, M., Korda, N., Munos, R., Flaounas, I., and Cristianini, N.
\newblock Finite-time analysis of kernelised contextual bandits.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pp.\
  654–663, 2013.

\bibitem[Wild et~al.(2021)Wild, Kanagawa, and Sejdinovic]{wild2021connections}
Wild, V., Kanagawa, M., and Sejdinovic, D.
\newblock Connections and equivalences between the {N}ystr\"om method and
  sparse variational {G}aussian processes.
\newblock \emph{arXiv preprint arXiv:2106.01121}, 2021.

\bibitem[Williams \& Seeger(2001)Williams and Seeger]{williams2001using}
Williams, C. and Seeger, M.
\newblock Using the {N}ystr{\"o}m method to speed up kernel machines.
\newblock In \emph{Conference on Neural Information Processing Systems}, pp.\
  682--688, 2001.

\bibitem[Zenati et~al.(2022)Zenati, Bietti, Diemert, Mairal, Martin, and
  Gaillard]{pmlr-v151-zenati22a}
Zenati, H., Bietti, A., Diemert, E., Mairal, J., Martin, M., and Gaillard, P.
\newblock Efficient kernelized {UCB} for contextual bandits.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  5689--5720. PMLR, 2022.

\bibitem[Zhang(2005)]{zhang2005learning}
Zhang, T.
\newblock Learning bounds for kernel regression using effective data
  dimensionality.
\newblock \emph{Neural Computation}, 17\penalty0 (9):\penalty0 2077--2098,
  2005.

\end{thebibliography}
