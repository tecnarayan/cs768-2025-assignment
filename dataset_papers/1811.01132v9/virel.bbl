\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{Abdolmaleki18}
Abdolmaleki, A., Springenberg, J.~T., Tassa, Y., Munos, R., Heess, N., and
  Riedmiller, M.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=S1ANxQW0b}.

\bibitem[Baird(1995)]{Baird95}
Baird, L.
\newblock {Residual algorithms: Reinforcement learning with function
  approximation}.
\newblock \emph{Machine Learning-International Workshop Then Conference-},
  \penalty0 (July):\penalty0 30--37, 1995.
\newblock ISSN 00043702.
\newblock \doi{10.1.1.48.3256}.

\bibitem[Bass(2013)]{Bass13}
Bass, R.
\newblock \emph{Real Analysis for Graduate Students}.
\newblock Createspace Ind Pub, 2013.
\newblock ISBN 9781481869140.
\newblock URL \url{https://books.google.co.uk/books?id=s6mVlgEACAAJ}.

\bibitem[Beal(2003)]{Beal03}
Beal, M.~J.
\newblock \emph{Variational algorithms for approximate Bayesian inference}.
\newblock PhD thesis, 2003.

\bibitem[Bertsekas(1996)]{Bertsekas96}
Bertsekas, D.
\newblock \emph{Constrained Optimization and Lagrange Multiplier Methods}.
\newblock Athena scientific series in optimization and neural computation.
  Athena Scientific, 1996.
\newblock ISBN 9781886529045.
\newblock URL \url{http://web.mit.edu/dimitrib/www/Constrained-Opt.pdf}.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Precup, Silver, Sutton, Maei, and
  Szepesv\'{a}ri]{Maei09}
Bhatnagar, S., Precup, D., Silver, D., Sutton, R.~S., Maei, H.~R., and
  Szepesv\'{a}ri, C.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In Bengio, Y., Schuurmans, D., Lafferty, J.~D., Williams, C. K.~I.,
  and Culotta, A. (eds.), \emph{Advances in Neural Information Processing
  Systems 22}, pp.\  1204--1212. Curran Associates, Inc., 2009.

\bibitem[Bishop(2006)]{Bishop2006}
Bishop, C.~M.
\newblock \emph{Pattern Recognition and Machine Learning (Information Science
  and Statistics)}.
\newblock Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006.
\newblock ISBN 0387310738.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{vi_review}
Blei, D.~M., Kucukelbir, A., and McAuliffe, J.~D.
\newblock {Variational Inference: A Review for Statisticians}, 2017.
\newblock ISSN 1537274X.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{CoRR}, abs/1606.01540, 2016.
\newblock URL \url{http://arxiv.org/abs/1606.01540}.

\bibitem[Ciosek \& Whiteson(2018{\natexlab{a}})Ciosek and Whiteson]{epg}
Ciosek, K. and Whiteson, S.
\newblock {E}xpected {P}olicy {G}radients.
\newblock \emph{The Thirty-Second AAAI Conference on Artificial Intelligence
  (AAAI-18)}, 2018{\natexlab{a}}.

\bibitem[Ciosek \& Whiteson(2018{\natexlab{b}})Ciosek and
  Whiteson]{epg-journal}
Ciosek, K. and Whiteson, S.
\newblock Expected policy gradients for reinforcement learning.
\newblock \emph{journal submission, arXiv preprint arXiv:1801.03326},
  2018{\natexlab{b}}.

\bibitem[Dayan \& Hinton(1997)Dayan and Hinton]{Dayan97}
Dayan, P. and Hinton, G.~E.
\newblock Using expectation-maximization for reinforcement learning.
\newblock \emph{Neural Computation}, 9\penalty0 (2):\penalty0 271--278, 1997.
\newblock \doi{10.1162/neco.1997.9.2.271}.
\newblock URL \url{https://doi.org/10.1162/neco.1997.9.2.271}.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and
  Rubin]{Dempster77maximumlikelihood}
Dempster, A.~P., Laird, N.~M., and Rubin, D.~B.
\newblock Maximum likelihood from incomplete data via the em algorithm.
\newblock \emph{Journal of the Royal Statistical Society, Series B},
  39\penalty0 (1):\penalty0 1--38, 1977.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and Abbeel]{rllab}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1329--1338, 2016.

\bibitem[Fellows et~al.(2018)Fellows, Ciosek, and Whiteson]{fpg}
Fellows, M., Ciosek, K., and Whiteson, S.
\newblock {F}ourier policy gradients.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1486--1495, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/fellows18a.html}.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Al-Shedivat, Rockt{\"a}schel,
  Xing, and Whiteson]{Foerster18}
Foerster, J., Farquhar, G., Al-Shedivat, M., Rockt{\"a}schel, T., Xing, E., and
  Whiteson, S.
\newblock {D}i{CE}: The infinitely differentiable {M}onte {C}arlo estimator.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1529--1538, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/foerster18a.html}.

\bibitem[Fox \& Roberts(2010)Fox and Roberts]{vi_tutorial}
Fox, C.~W. and Roberts, S.~J.
\newblock {A tutorial on variational Bayesian inference}.
\newblock \emph{Artificial Intelligence Review}, pp.\  1--11, 2010.
\newblock ISSN 0269-2821.
\newblock \doi{10.1007/s10462-011-9236-8}.
\newblock URL
  \url{papers2://publication/uuid/1B6D2DDA-67F6-4EEE-9720-2907FFB14789}.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1587--1596, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/fujimoto18a.html}.

\bibitem[Furmston \& Barber(2010)Furmston and Barber]{Furmston2010}
Furmston, T. and Barber, D.
\newblock {Variational Methods For Reinforcement Learning}.
\newblock \emph{In AISTATS}, pp.\  241--248, 2010.
\newblock ISSN 15324435.

\bibitem[Goyal et~al.(2018)Goyal, Brakel, Fedus, Lillicrap, Levine, Larochelle,
  and Bengio]{Goyal18}
Goyal, A., Brakel, P., Fedus, W., Lillicrap, T.~P., Levine, S., Larochelle, H.,
  and Bengio, Y.
\newblock Recall traces: Backtracking models for efficient reinforcement
  learning.
\newblock \emph{CoRR}, abs/1804.00379, 2018.
\newblock URL \url{http://arxiv.org/abs/1804.00379}.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Ghahramani, Turner, and Levine]{qprop}
Gu, S., Lillicrap, T., Ghahramani, Z., Turner, R.~E., and Levine, S.
\newblock {Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}.
\newblock pp.\  1--13, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.02247}.

\bibitem[Gunawardana \& Byrne(2005)Gunawardana and Byrne]{Gunawardana05}
Gunawardana, A. and Byrne, W.
\newblock Convergence theorems for generalized alternating minimization
  procedures.
\newblock \emph{J. Mach. Learn. Res.}, 6:\penalty0 2049--2073, December 2005.
\newblock ISSN 1532-4435.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=1046920.1194913}.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{Haarnoja17}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1352--1361, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v70/haarnoja17a.html}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{Haarnoja18}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1861--1870, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/haarnoja18b.html}.

\bibitem[Hachiya et~al.(2009)Hachiya, Peters, and Sugiyama]{Hachiya09}
Hachiya, H., Peters, J., and Sugiyama, M.
\newblock Efficient sample reuse in em-based policy search.
\newblock In Buntine, W., Grobelnik, M., Mladeni{\'{c}}, D., and Shawe-Taylor,
  J. (eds.), \emph{Machine Learning and Knowledge Discovery in Databases}, pp.\
   469--484, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg.
\newblock ISBN 978-3-642-04180-8.

\bibitem[Heess et~al.(2013)Heess, Silver, and Teh]{Heess12}
Heess, N., Silver, D., and Teh, Y.~W.
\newblock Actor-critic reinforcement learning with energy-based policies.
\newblock In Deisenroth, M.~P., Szepesvári, C., and Peters, J. (eds.),
  \emph{Proceedings of the Tenth European Workshop on Reinforcement Learning},
  volume~24 of \emph{Proceedings of Machine Learning Research}, pp.\  45--58,
  Edinburgh, Scotland, 30 Jun--01 Jul 2013. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v24/heess12a.html}.

\bibitem[Heess et~al.(2015{\natexlab{a}})Heess, Wayne, Silver, Lillicrap, Erez,
  and Tassa]{Heess15}
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., and Tassa, Y.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In Cortes, C., Lawrence, N.~D., Lee, D.~D., Sugiyama, M., and
  Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems
  28}, pp.\  2944--2952. Curran Associates, Inc., 2015{\natexlab{a}}.

\bibitem[Heess et~al.(2015{\natexlab{b}})Heess, Wayne, Silver, Lillicrap,
  Tassa, and Erez]{svg}
Heess, N., Wayne, G., Silver, D., Lillicrap, T., Tassa, Y., and Erez, T.
\newblock {Learning Continuous Control Policies by Stochastic Value Gradients}.
\newblock pp.\  1--13, 2015{\natexlab{b}}.
\newblock ISSN 10495258.
\newblock URL \url{http://arxiv.org/abs/1510.09142}.

\bibitem[Jordan(1999)]{Jordan1999}
Jordan, M.~I. (ed.).
\newblock \emph{Learning in Graphical Models}.
\newblock MIT Press, Cambridge, MA, USA, 1999.
\newblock ISBN 0-262-60032-3.

\bibitem[Kelly(2008)]{Kelly08}
Kelly, J.
\newblock \emph{Generalized Functions}, chapter~4, pp.\  111--124.
\newblock John Wiley \& Sons, Ltd, 2008.
\newblock ISBN 9783527618897.
\newblock \doi{10.1002/9783527618897.ch4}.
\newblock URL
  \url{https://onlinelibrary.wiley.com/doi/abs/10.1002/9783527618897.ch4}.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{auto_bayes}
Kingma, D.~P. and Welling, M.
\newblock {Auto-Encoding Variational Bayes PPT}.
\newblock \emph{Proceedings of the 2nd International Conference on Learning
  Representations (ICLR)}, 2014.
\newblock ISSN 1312.6114v10.
\newblock URL \url{http://arxiv.org/abs/1312.6114}.

\bibitem[Koller \& Parr(2000)Koller and Parr]{Koller00}
Koller, D. and Parr, R.
\newblock Policy iteration for factored mdps.
\newblock In \emph{Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence}, UAI'00, pp.\  326--334, San Francisco, CA, USA,
  2000. Morgan Kaufmann Publishers Inc.
\newblock ISBN 1-55860-709-9.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=2073946.2073985}.

\bibitem[Levine(2014)]{Levine14}
Levine, S.
\newblock \emph{Motor Skill Learning with Trajectory Methods}.
\newblock PhD thesis, 2014.
\newblock URL
  \url{https://people.eecs.berkeley.edu/{~}svlevine/papers/thesis.pdf}.

\bibitem[Levine(2018)]{virl_review}
Levine, S.
\newblock {Reinforcement Learning and Control as Probabilistic Inference:
  Tutorial and Review}.
\newblock 2018.
\newblock URL \url{https://arxiv.org/pdf/1805.00909.pdf}.

\bibitem[Levine \& Koltun(2013)Levine and Koltun]{Levine13}
Levine, S. and Koltun, V.
\newblock Variational policy search via trajectory optimization.
\newblock In Burges, C. J.~C., Bottou, L., Welling, M., Ghahramani, Z., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 26}, pp.\  207--215. Curran Associates, Inc., 2013.

\bibitem[Liberzon(2011)]{Liberzon11}
Liberzon, D.
\newblock \emph{Calculus of Variations and Optimal Control Theory: A Concise
  Introduction}.
\newblock Princeton University Press, Princeton, NJ, USA, 2011.
\newblock ISBN 0691151873, 9780691151878.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Mahajan \& Tulabandhula(2017{\natexlab{a}})Mahajan and
  Tulabandhula]{mahajan2017asymmetry}
Mahajan, A. and Tulabandhula, T.
\newblock Symmetry learning for function approximation in reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1706.02999}, 2017{\natexlab{a}}.

\bibitem[Mahajan \& Tulabandhula(2017{\natexlab{b}})Mahajan and
  Tulabandhula]{mahajan2017symmetry}
Mahajan, A. and Tulabandhula, T.
\newblock Symmetry detection and exploitation for function approximation in
  deep rl.
\newblock In \emph{Proceedings of the 16th Conference on Autonomous Agents and
  MultiAgent Systems}, pp.\  1619--1621. International Foundation for
  Autonomous Agents and Multiagent Systems, 2017{\natexlab{b}}.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and
  Whiteson]{mahajan2019maven}
Mahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S.
\newblock Maven: Multi-agent variational exploration, 2019.
\newblock URL \url{https://arxiv.org/abs/1910.07483}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock {Human-level control through deep reinforcement learning}.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.
\newblock ISSN 14764687.
\newblock \doi{10.1038/nature14236}.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{Mnih16}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In Balcan, M.~F. and Weinberger, K.~Q. (eds.), \emph{Proceedings of
  The 33rd International Conference on Machine Learning}, volume~48 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1928--1937, New York,
  New York, USA, 20--22 Jun 2016. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v48/mniha16.html}.

\bibitem[Neumann(2011)]{Neumann11}
Neumann, G.
\newblock Variational inference for policy search in changing situations.
\newblock In \emph{Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, ICML'11, pp.\  817--824, USA,
  2011. Omnipress.
\newblock ISBN 978-1-4503-0619-5.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3104482.3104585}.

\bibitem[Pearlmutter(1994)]{Pearlmutter94}
Pearlmutter, B.~A.
\newblock Fast exact multiplication by the hessian.
\newblock \emph{Neural Computation}, 6:\penalty0 147--160, 1994.

\bibitem[Peters \& Schaal(2007)Peters and Schaal]{Peters07}
Peters, J. and Schaal, S.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In \emph{Proceedings of the 24th International Conference on Machine
  Learning}, ICML '07, pp.\  745--750, New York, NY, USA, 2007. ACM.
\newblock ISBN 978-1-59593-793-3.
\newblock \doi{10.1145/1273496.1273590}.
\newblock URL \url{http://doi.acm.org/10.1145/1273496.1273590}.

\bibitem[Rawlik et~al.(2010)Rawlik, Toussaint, and Vijayakumar]{Rawlik10}
Rawlik, K., Toussaint, M., and Vijayakumar, S.
\newblock Approximate inference and stochastic optimal control.
\newblock \emph{CoRR}, abs/1009.3958, 2010.
\newblock URL \url{http://arxiv.org/abs/1009.3958}.

\bibitem[Rawlik et~al.(2012)Rawlik, Toussaint, and Vijayakumar]{Rawlik12}
Rawlik, K., Toussaint, M., and Vijayakumar, S.
\newblock On stochastic optimal control and reinforcement learning by
  approximate inference.
\newblock In \emph{Robotics: Science and Systems}, 2012.

\bibitem[Sallans \& Hinton(2004)Sallans and Hinton]{Sallans04}
Sallans, B. and Hinton, G.~E.
\newblock Reinforcement learning with factored states and actions.
\newblock \emph{J. Mach. Learn. Res.}, 5:\penalty0 1063--1088, dec 2004.
\newblock ISSN 1532-4435.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=1005332.1016794}.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Heess, Weber, and
  Abbeel]{Schulman15a}
Schulman, J., Heess, N., Weber, T., and Abbeel, P.
\newblock Gradient estimation using stochastic computation graphs.
\newblock In Cortes, C., Lawrence, N.~D., Lee, D.~D., Sugiyama, M., and
  Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems
  28}, pp.\  3528--3536. Curran Associates, Inc., 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{Schulman15b}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock 37:\penalty0 1889--1897, 07--09 Jul 2015{\natexlab{b}}.
\newblock URL \url{http://proceedings.mlr.press/v37/schulman15.html}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{Schulman2017ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{dpg}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock {Deterministic Policy Gradient Algorithms}.
\newblock \emph{Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pp.\  387--395, 2014.
\newblock ISSN 1938-7228.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton}
Sutton, R.~S. and Barto, A.~G.
\newblock {Sutton {\&} Barto Book: Reinforcement Learning: An Introduction}.
\newblock \emph{MIT Press, Cambridge, MA, A Bradford Book}, 1998.
\newblock ISSN 10459227.
\newblock \doi{10.1109/TNN.1998.712192}.

\bibitem[Sutton \& Barto(2017)Sutton and Barto]{Sutton17}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Introduction to Reinforcement Learning}.
\newblock MIT Press, Cambridge, MA, USA, 2nd edition, 2017.
\newblock ISBN 0262193981.

\bibitem[Sutton et~al.(1999)Sutton, Mcallester, Singh, and Mansour]{Sutton00}
Sutton, R.~S., Mcallester, D., Singh, S., and Mansour, Y.
\newblock {Policy Gradient Methods for Reinforcement Learning with Function
  Approximation}.
\newblock \emph{Advances in Neural Information Processing Systems 12}, pp.\
  1057--1063, 1999.
\newblock ISSN 0047-2875.
\newblock \doi{10.1.1.37.9714}.

\bibitem[Sutton et~al.(2009{\natexlab{a}})Sutton, Maei, Precup, Bhatnagar,
  Silver, Szepesv\'{a}ri, and Wiewiora]{Sutton09a}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv\'{a}ri, C., and Wiewiora, E.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, ICML '09, pp.\  993--1000, New York, NY, USA,
  2009{\natexlab{a}}. ACM.
\newblock ISBN 978-1-60558-516-1.
\newblock \doi{10.1145/1553374.1553501}.
\newblock URL \url{http://doi.acm.org/10.1145/1553374.1553501}.

\bibitem[Sutton et~al.(2009{\natexlab{b}})Sutton, Maei, and
  Szepesv\'{a}ri]{Sutton09}
Sutton, R.~S., Maei, H.~R., and Szepesv\'{a}ri, C.
\newblock A convergent o(n) temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L. (eds.),
  \emph{Advances in Neural Information Processing Systems 21}, pp.\
  1609--1616. Curran Associates, Inc., 2009{\natexlab{b}}.

\bibitem[Szepesv{\'{a}}ri(2010)]{rl_algorithms}
Szepesv{\'{a}}ri, C.
\newblock {Algorithms for Reinforcement Learning}.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 4\penalty0 (1):\penalty0 1--103, 2010.
\newblock ISSN 1939-4608.
\newblock \doi{10.2200/S00268ED1V01Y201005AIM009}.
\newblock URL
  \url{http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009}.

\bibitem[Thomas(2014)]{Thomas14}
Thomas, P.
\newblock Bias in natural actor-critic algorithms.
\newblock In Xing, E.~P. and Jebara, T. (eds.), \emph{Proceedings of the 31st
  International Conference on Machine Learning}, volume~32 of \emph{Proceedings
  of Machine Learning Research}, pp.\  441--448, Bejing, China, 22--24 Jun
  2014. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v32/thomas14.html}.

\bibitem[Todorov(2007)]{Todorov06}
Todorov, E.
\newblock Linearly-solvable markov decision problems.
\newblock In Sch\"{o}lkopf, B., Platt, J.~C., and Hoffman, T. (eds.),
  \emph{Advances in Neural Information Processing Systems 19}, pp.\
  1369--1376. MIT Press, 2007.

\bibitem[Toussaint(2009{\natexlab{a}})]{Toussaint09a}
Toussaint, M.
\newblock {Robot trajectory optimization using approximate inference}.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning - ICML '09}, pp.\  1--8, 2009{\natexlab{a}}.
\newblock ISBN 9781605585161.
\newblock \doi{10.1145/1553374.1553508}.
\newblock URL
  \url{https://homes.cs.washington.edu/{~}todorov/courses/amath579/reading/Toussaint.pdf
  http://portal.acm.org/citation.cfm?doid=1553374.1553508}.

\bibitem[Toussaint(2009{\natexlab{b}})]{Toussaint09b}
Toussaint, M.
\newblock Probabilistic inference as a model of planned behavior.
\newblock \emph{Kunstliche Intelligenz}, 3, 01 2009{\natexlab{b}}.

\bibitem[Toussaint \& Storkey(2006)Toussaint and Storkey]{Toussaint06b}
Toussaint, M. and Storkey, A.
\newblock {Probabilistic inference for solving discrete and continuous state
  Markov Decision Processes}.
\newblock \emph{Proceedings of the 23rd international conference on Machine
  learning - ICML '06}, pp.\  945--952, 2006.
\newblock \doi{10.1145/1143844.1143963}.
\newblock URL \url{http://portal.acm.org/citation.cfm?doid=1143844.1143963}.

\bibitem[Tsitsiklis \& {Van Roy}(1997)Tsitsiklis and {Van Roy}]{Tsitsiklis1997}
Tsitsiklis, J.~N. and {Van Roy}, B.
\newblock {An analysis of temporal-difference learning with function
  approximation}.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.
\newblock ISSN 00189286.
\newblock \doi{10.1109/9.580874}.

\bibitem[Turner \& Sahani(2011)Turner and Sahani]{Turner11}
Turner, R.~E. and Sahani, M.
\newblock \emph{Two problems with variational expectation maximisation for time
  series models}, pp.\  104–124.
\newblock Cambridge University Press, 2011.
\newblock \doi{10.1017/CBO9780511984679.006}.

\bibitem[van Hasselt et~al.(2015)van Hasselt, Guez, and Silver]{doubleq}
van Hasselt, H., Guez, A., and Silver, D.
\newblock {Deep Reinforcement Learning with Double Q-learning}.
\newblock 2015.
\newblock ISSN 00043702.
\newblock \doi{10.1016/j.artint.2015.09.002}.
\newblock URL \url{http://arxiv.org/abs/1509.06461}.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{qlearning}
Watkins, C. J. C.~H. and Dayan, P.
\newblock {Q-learning}.
\newblock \emph{Machine Learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.
\newblock ISSN 0885-6125.
\newblock \doi{10.1007/BF00992698}.
\newblock URL \url{http://link.springer.com/10.1007/BF00992698}.

\bibitem[Williams \& Peng(1991)Williams and Peng]{Williams91}
Williams, R.~J. and Peng, J.
\newblock Function optimization using connectionist reinforcement learning
  algorithms.
\newblock \emph{Connection Science}, 3\penalty0 (3):\penalty0 241--268, 1991.
\newblock \doi{10.1080/09540099108946587}.
\newblock URL \url{https://doi.org/10.1080/09540099108946587}.

\bibitem[Williams et~al.(1993)Williams, Baird, and III]{Williams93}
Williams, R.~J., Baird, L.~C., and III.
\newblock Analysis of some incremental variants of policy iteration: First
  steps toward understanding actor-critic learning systems, 1993.

\bibitem[Wu(1983)]{Wu83}
Wu, C. F.~J.
\newblock {On the Convergence Properties of the EM Algorithm'}.
\newblock \emph{Source: The Annals of Statistics The Annals of Statistics},
  11\penalty0 (1):\penalty0 95--103, 1983.

\bibitem[Ziebart(2010)]{Ziebart10b}
Ziebart, B.~D.
\newblock \emph{Modeling Purposeful Adaptive Behavior with the Principle of
  Maximum Causal Entropy}.
\newblock PhD thesis, 2010.
\newblock URL
  \url{http://www.cs.cmu.edu/{~}bziebart/publications/thesis-bziebart.pdf}.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and Dey]{Ziebart08}
Ziebart, B.~D., Maas, A., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the 23rd National Conference on Artificial
  Intelligence - Volume 3}, AAAI'08, pp.\  1433--1438. AAAI Press, 2008.
\newblock ISBN 978-1-57735-368-3.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=1620270.1620297}.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{Ziebart10a}
Ziebart, B.~D., Bagnell, J.~A., and Dey, A.~K.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{Proceedings of the 27th International Conference on
  International Conference on Machine Learning}, ICML'10, pp.\  1255--1262,
  USA, 2010. Omnipress.
\newblock ISBN 978-1-60558-907-7.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3104322.3104481}.

\end{thebibliography}
