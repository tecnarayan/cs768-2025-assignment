\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bau et~al.(2020{\natexlab{a}})Bau, Liu, Wang, Zhu, and Torralba]{bau2020rewriting}
David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba.
\newblock Rewriting a deep generative model.
\newblock In \emph{European conference on computer vision}, pages 351--369. Springer, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/pdf/2007.15646.pdf}.

\bibitem[Bau et~al.(2020{\natexlab{b}})Bau, Zhu, Strobelt, Lapedriza, Zhou, and Torralba]{bau2020understanding}
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba.
\newblock Understanding the role of individual units in a deep neural network.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0 (48):\penalty0 30071--30078, 2020{\natexlab{b}}.
\newblock URL \url{https://www.pnas.org/doi/pdf/10.1073/pnas.1907375117}.

\bibitem[Bolukbasi et~al.(2021)Bolukbasi, Pearce, Yuan, Coenen, Reif, Vi{\'e}gas, and Wattenberg]{bolukbasi2021interpretability}
Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Vi{\'e}gas, and Martin Wattenberg.
\newblock An interpretability illusion for bert.
\newblock \emph{arXiv preprint arXiv:2104.07143}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2104.07143.pdf}.

\bibitem[Casper et~al.(2022)Casper, Hod, Filan, Wild, Critch, and Russell]{casper2022graphical}
Stephen Casper, Shlomi Hod, Daniel Filan, Cody Wild, Andrew Critch, and Stuart Russell.
\newblock Graphical clusterability and local specialization in deep neural networks.
\newblock In \emph{ICLR 2022 Workshop on PAIR}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2110.08058v2.pdf}.

\bibitem[Chormai et~al.(2022)Chormai, Herrmann, M{\"u}ller, and Montavon]{chormai2022disentangled}
Pattarawat Chormai, Jan Herrmann, Klaus-Robert M{\"u}ller, and Gr{\'e}goire Montavon.
\newblock Disentangled explanations of neural network predictions by finding relevant subspaces.
\newblock \emph{arXiv preprint arXiv:2212.14855}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2212.14855.pdf}.

\bibitem[Csord{\'a}s et~al.(2020)Csord{\'a}s, van Steenkiste, and Schmidhuber]{csordas2020neural}
R{\'o}bert Csord{\'a}s, Sjoerd van Steenkiste, and J{\"u}rgen Schmidhuber.
\newblock Are neural nets modular? inspecting functional modularity through differentiable weight masks.
\newblock \emph{arXiv preprint arXiv:2010.02066}, 2020.
\newblock URL \url{https://arxiv.org/pdf/2010.02066.pdf}.

\bibitem[Cui et~al.(2022)Cui, Jahanian, Lapedriza, Torralba, Mahdizadehaghdam, Kumar, and Bau]{cui2022local}
Audrey Cui, Ali Jahanian, Agata Lapedriza, Antonio Torralba, Shahin Mahdizadehaghdam, Rohit Kumar, and David Bau.
\newblock Local relighting of real scenes.
\newblock \emph{arXiv preprint arXiv:2207.02774}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2207.02774.pdf}.

\bibitem[Dai et~al.(2022)Dai, Dong, Hao, Sui, and Wei]{dai2021knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, and Furu Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock In \emph{ACL}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2104.08696.pdf}.

\bibitem[De~Cao et~al.(2021{\natexlab{a}})De~Cao, Aziz, and Titov]{de2021editing}
Nicola De~Cao, Wilker Aziz, and Ivan Titov.
\newblock Editing factual knowledge in language models.
\newblock In \emph{EMNLP}, pages 6491--6506. Association for Computational Linguistics, November 2021{\natexlab{a}}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.522}.

\bibitem[De~Cao et~al.(2021{\natexlab{b}})De~Cao, Schmid, Hupkes, and Titov]{de2021sparse}
Nicola De~Cao, Leon Schmid, Dieuwke Hupkes, and Ivan Titov.
\newblock Sparse interventions in language models with differentiable masking.
\newblock In \emph{EMNLP BlackboxNLP Workshop}, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/pdf/2112.06837.pdf}.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds, Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Geva et~al.(2021)Geva, Schuster, Berant, and Levy]{geva2020transformer}
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
\newblock Transformer feed-forward layers are key-value memories.
\newblock In \emph{EMNLP}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2012.14913.pdf}.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and Goldberg]{geva2022transformer}
Mor Geva, Avi Caciularu, Kevin~Ro Wang, and Yoav Goldberg.
\newblock Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space.
\newblock \emph{arXiv preprint arXiv:2203.14680}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2203.14680.pdf}.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Wexler, Zou, and Kim]{ghorbani2019towards}
Amirata Ghorbani, James Wexler, James~Y Zou, and Been Kim.
\newblock Towards automatic concept-based explanations.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.
\newblock URL \url{https://arxiv.org/pdf/1902.03129.pdf}.

\bibitem[Hase et~al.(2021)Hase, Diab, Celikyilmaz, Li, Kozareva, Stoyanov, Bansal, and Iyer]{hase2021language}
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer.
\newblock Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs.
\newblock \emph{arXiv preprint arXiv:2111.13654}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2111.13654.pdf}.

\bibitem[Hernandez et~al.(2022)Hernandez, Schwettmann, Bau, Bagashvili, Torralba, and Andreas]{hernandez2022natural}
Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas.
\newblock Natural language descriptions of deep visual features.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/pdf?id=NudBMY-tzDr}.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Viegas, and sayres]{kim2018tcav}
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, and Rory sayres.
\newblock Interpretability beyond feature attribution: Quantitative testing with concept activation vectors ({TCAV}).
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the 35th International Conference on Machine Learning}, volume~80 of \emph{Proceedings of Machine Learning Research}, pages 2668--2677. PMLR, 10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/kim18d.html}.

\bibitem[Lakretz et~al.(2019)Lakretz, Kruszewski, Desbordes, Hupkes, Dehaene, and Baroni]{lakretz2019emergence}
Yair Lakretz, German Kruszewski, Theo Desbordes, Dieuwke Hupkes, Stanislas Dehaene, and Marco Baroni.
\newblock The emergence of number and syntax units in lstm language models.
\newblock In \emph{NAACL-HLT}, 2019.
\newblock URL \url{https://arxiv.org/pdf/1903.07435.pdf}.

\bibitem[Lakretz et~al.(2021)Lakretz, Hupkes, Vergallito, Marelli, Baroni, and Dehaene]{lakretz2021mechanisms}
Yair Lakretz, Dieuwke Hupkes, Alessandra Vergallito, Marco Marelli, Marco Baroni, and Stanislas Dehaene.
\newblock Mechanisms for handling nested dependencies in neural-network language models and humans.
\newblock \emph{Cognition}, 213:\penalty0 104699, 04 2021.
\newblock \doi{10.1016/j.cognition.2021.104699}.
\newblock URL \url{https://arxiv.org/ftp/arxiv/papers/2006/2006.11098.pdf}.

\bibitem[Levy et~al.(2017)Levy, Seo, Choi, and Zettlemoyer]{levy-etal-2017-zero}
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.
\newblock Zero-shot relation extraction via reading comprehension.
\newblock In \emph{Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)}, pages 333--342, Vancouver, Canada, August 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/K17-1034}.
\newblock URL \url{https://aclanthology.org/K17-1034}.

\bibitem[Meng et~al.(2022{\natexlab{a}})Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual knowledge in gpt.
\newblock In \emph{NeurIPS 2022}, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/pdf/2202.05262.pdf}.

\bibitem[Meng et~al.(2022{\natexlab{b}})Meng, Sharma, Andonian, Belinkov, and Bau]{meng2022mass}
Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.
\newblock Mass-editing memory in a transformer.
\newblock \emph{arXiv preprint arXiv:2210.07229}, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/pdf/2210.07229.pdf}.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and Dean]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock In \emph{arXiv preprint arXiv:1301.3781}, 2013.
\newblock URL \url{https://arxiv.org/pdf/1301.3781.pdf}.

\bibitem[Mitchell et~al.(2021)Mitchell, Lin, Bosselut, Finn, and Manning]{mitchell2021fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D Manning.
\newblock Fast model editing at scale.
\newblock \emph{arXiv preprint arXiv:2110.11309}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2110.11309.pdf}.

\bibitem[Mitchell et~al.(2022)Mitchell, Lin, Bosselut, Manning, and Finn]{mitchell2022memory}
Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher~D Manning, and Chelsea Finn.
\newblock Memory-based model editing at scale.
\newblock In \emph{International Conference on Machine Learning}, pages 15817--15831. PMLR, 2022.
\newblock URL \url{https://arxiv.org/pdf/2206.06520.pdf}.

\bibitem[Mu and Andreas(2020)]{mu2020compositional}
Jesse Mu and Jacob Andreas.
\newblock Compositional explanations of neurons.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 17153--17163, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/file/c74956ffb38ba48ed6ce977af6727275-Paper.pdf}.

\bibitem[Olah et~al.(2018)Olah, Satyanarayan, Johnson, Carter, Schubert, Ye, and Mordvintsev]{olah2018the}
Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev.
\newblock The building blocks of interpretability.
\newblock \emph{Distill}, 2018.
\newblock \doi{10.23915/distill.00010}.
\newblock https://distill.pub/2018/building-blocks.

\bibitem[Petroni et~al.(2019)Petroni, Rockt{\"a}schel, Riedel, Lewis, Bakhtin, Wu, and Miller]{petroni-etal-2019-language}
Fabio Petroni, Tim Rockt{\"a}schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.
\newblock Language models as knowledge bases?
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2463--2473, Hong Kong, China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1250}.
\newblock URL \url{https://aclanthology.org/D19-1250}.

\bibitem[Radford et~al.(2017)Radford, Jozefowicz, and Sutskever]{radford2017learning}
Alec Radford, Rafal Jozefowicz, and Ilya Sutskever.
\newblock Learning to generate reviews and discovering sentiment.
\newblock \emph{arXiv preprint arXiv:1704.01444}, 2017.
\newblock URL \url{https://arxiv.org/pdf/1704.01444.pdf}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.
\newblock URL \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[Rogers et~al.(2020)Rogers, Kovaleva, and Rumshisky]{rogers-etal-2020-primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer in {BERT}ology: What we know about how {BERT} works.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 8:\penalty0 842--866, 2020.
\newblock \doi{10.1162/tacl_a_00349}.
\newblock URL \url{https://aclanthology.org/2020.tacl-1.54}.

\bibitem[Santurkar et~al.(2021)Santurkar, Tsipras, Elango, Bau, Torralba, and Madry]{santurkar2021editing}
Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry.
\newblock Editing a classifier by rewriting its prediction rules.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, volume~34, pages 23359--23373. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/file/c46489a2d5a9a9ecfc53b17610926ddd-Paper.pdf}.

\bibitem[Schneider and Vlachos(2021)]{schneider2021explaining}
Johannes Schneider and Michalis Vlachos.
\newblock Explaining neural networks by decoding layer activations.
\newblock In \emph{International Symposium on Intelligent Data Analysis}, pages 63--75. Springer, 2021.
\newblock URL \url{https://arxiv.org/pdf/2005.13630.pdf}.

\bibitem[Vig et~al.(2020)Vig, Gehrmann, Belinkov, Qian, Nevo, Sakenis, Huang, Singer, and Shieber]{vig2020causal}
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer, and Stuart Shieber.
\newblock Causal mediation analysis for interpreting neural nlp: The case of gender bias.
\newblock \emph{arXiv preprint arXiv:2004.12265}, 2020.
\newblock URL \url{https://arxiv.org/pdf/2004.12265.pdf}.

\bibitem[Wang and Komatsuzaki(2021)]{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wang et~al.(2022)Wang, Wen, Zhang, Hou, Liu, and Li]{wang2022finding}
Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li.
\newblock Finding skill neurons in pre-trained transformer-based language models.
\newblock \emph{arXiv preprint arXiv:2211.07349}, 2022.
\newblock URL \url{https://arxiv.org/pdf/2211.07349.pdf}.

\bibitem[Zeiler and Fergus(2014)]{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In \emph{European conference on computer vision}, pages 818--833. Springer, 2014.
\newblock URL \url{https://arxiv.org/pdf/1311.2901.pdf}.

\bibitem[Zhang et~al.(2021)Zhang, Madumal, Miller, Ehinger, and Rubinstein]{zhang2021invertible}
Ruihan Zhang, Prashan Madumal, Tim Miller, Krista~A Ehinger, and Benjamin~IP Rubinstein.
\newblock Invertible concept-based explanations for cnn models with non-negative concept activation vectors.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2021.
\newblock URL \url{https://arxiv.org/pdf/2006.15417.pdf}.

\bibitem[Zhao et~al.(2021)Zhao, Pascual, Brunner, and Wattenhofer]{zhao2021non}
Sumu Zhao, Dami{\'a}n Pascual, Gino Brunner, and Roger Wattenhofer.
\newblock Of non-linearity and commutativity in bert.
\newblock In \emph{2021 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8. IEEE, 2021.
\newblock URL \url{https://arxiv.org/pdf/2101.04547.pdf}.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Bau, Oliva, and Torralba]{zhou2018interpreting}
Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba.
\newblock Interpreting deep visual representations via network dissection.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 41\penalty0 (9):\penalty0 2131--2145, 2018{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/pdf/1711.05611.pdf}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Sun, Bau, and Torralba]{zhou2018interpretable}
Bolei Zhou, Yiyou Sun, David Bau, and Antonio Torralba.
\newblock Interpretable basis decomposition for visual explanation.
\newblock In \emph{Proceedings of the European Conference on Computer Vision (ECCV)}, pages 119--134, 2018{\natexlab{b}}.
\newblock URL \url{https://people.csail.mit.edu/bzhou/publication/eccv18-IBD}.

\bibitem[Zhu et~al.(2020)Zhu, Rawat, Zaheer, Bhojanapalli, Li, Yu, and Kumar]{zhu2020modifying}
Chen Zhu, Ankit~Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.
\newblock Modifying memories in transformer models.
\newblock \emph{arXiv preprint arXiv:2012.00363}, 2020.
\newblock URL \url{https://arxiv.org/pdf/2012.00363.pdf}.

\end{thebibliography}
