\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko \& Flammarion(2022)Andriushchenko and Flammarion]{andriushchenko2022towards}
Andriushchenko, M. and Flammarion, N.
\newblock Towards understanding sharpness-aware minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  639--668. PMLR, 2022.

\bibitem[Andriushchenko et~al.(2023{\natexlab{a}})Andriushchenko, Bahri, Mobahi, and Flammarion]{andriushchenko2023sharpness}
Andriushchenko, M., Bahri, D., Mobahi, H., and Flammarion, N.
\newblock Sharpness-aware minimization leads to low-rank features.
\newblock \emph{arXiv preprint arXiv:2305.16292}, 2023{\natexlab{a}}.

\bibitem[Andriushchenko et~al.(2023{\natexlab{b}})Andriushchenko, Croce, M{\"u}ller, Hein, and Flammarion]{andriushchenko2023modern}
Andriushchenko, M., Croce, F., M{\"u}ller, M., Hein, M., and Flammarion, N.
\newblock A modern look at the relationship between sharpness and generalization.
\newblock \emph{arXiv preprint arXiv:2302.07011}, 2023{\natexlab{b}}.

\bibitem[Ash \& Adams(2020)Ash and Adams]{ash2020warm}
Ash, J. and Adams, R.~P.
\newblock On warm-starting neural network training.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 3884--3894, 2020.

\bibitem[Baek et~al.(2024)Baek, Kolter, and Raghunathan]{baek2024why}
Baek, C., Kolter, J.~Z., and Raghunathan, A.
\newblock Why is {SAM} robust to label noise?
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Bahri et~al.(2021)Bahri, Mobahi, and Tay]{bahri2021sharpness}
Bahri, D., Mobahi, H., and Tay, Y.
\newblock Sharpness-aware minimization improves language model generalization.
\newblock \emph{arXiv preprint arXiv:2110.08529}, 2021.

\bibitem[Bartlett et~al.(2023)Bartlett, Long, and Bousquet]{bartlett2023dynamics}
Bartlett, P.~L., Long, P.~M., and Bousquet, O.
\newblock The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (316):\penalty0 1--36, 2023.

\bibitem[Beyer et~al.(2020)Beyer, H{\'e}naff, Kolesnikov, Zhai, and Oord]{beyer2020we}
Beyer, L., H{\'e}naff, O.~J., Kolesnikov, A., Zhai, X., and Oord, A. v.~d.
\newblock Are we done with imagenet?
\newblock \emph{arXiv preprint arXiv:2006.07159}, 2020.

\bibitem[Chen et~al.(2021)Chen, Hsieh, and Gong]{chen2021vision}
Chen, X., Hsieh, C.-J., and Gong, B.
\newblock When vision transformers outperform resnets without pre-training or strong data augmentations.
\newblock \emph{arXiv preprint arXiv:2106.01548}, 2021.

\bibitem[Chen et~al.(2023)Chen, Zhang, Kou, Chen, Hsieh, and Gu]{chen2023does}
Chen, Z., Zhang, J., Kou, Y., Chen, X., Hsieh, C.-J., and Gu, Q.
\newblock Why does sharpness-aware minimization generalize better than sgd?
\newblock \emph{arXiv preprint arXiv:2310.07269}, 2023.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pp.\  248--255. {IEEE}, 2009.

\bibitem[Dohare et~al.(2023)Dohare, Hernandez-Garcia, Rahman, Sutton, and Mahmood]{dohare2023maintaining}
Dohare, S., Hernandez-Garcia, J.~F., Rahman, P., Sutton, R.~S., and Mahmood, A.~R.
\newblock Maintaining plasticity in deep continual learning.
\newblock \emph{arXiv preprint arXiv:2306.13812}, 2023.

\bibitem[D'Oro et~al.(2023)D'Oro, Schwarzer, Nikishin, Bacon, Bellemare, and Courville]{doro2023sampleefficient}
D'Oro, P., Schwarzer, M., Nikishin, E., Bacon, P.-L., Bellemare, M.~G., and Courville, A.
\newblock Sample-efficient reinforcement learning by breaking the replay ratio barrier.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Elsayed \& Mahmood(2023)Elsayed and Mahmood]{elsayed2023utility}
Elsayed, M. and Mahmood, A.~R.
\newblock Utility-based perturbed gradient descent: An optimizer for continual learning.
\newblock In \emph{OPT 2023: Optimization for Machine Learning}, 2023.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and Neyshabur]{foret2020sharpness}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  8340--8349, 2021.

\bibitem[Jiang et~al.(2019)Jiang, Neyshabur, Mobahi, Krishnan, and Bengio]{jiang2019fantastic}
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock \emph{arXiv preprint arXiv:1912.02178}, 2019.

\bibitem[Juditsky et~al.(2011)Juditsky, Nemirovski, and Tauvel]{juditsky2011solving}
Juditsky, A., Nemirovski, A., and Tauvel, C.
\newblock Solving variational inequalities with stochastic mirror-prox algorithm.
\newblock \emph{Stochastic Systems}, 1\penalty0 (1):\penalty0 17--58, 2011.

\bibitem[Kaur et~al.(2023)Kaur, Cohen, and Lipton]{kaur2023maximum}
Kaur, S., Cohen, J., and Lipton, Z.~C.
\newblock On the maximum hessian eigenvalue and generalization.
\newblock In \emph{Proceedings on}, pp.\  51--65. PMLR, 2023.

\bibitem[Kawaguchi et~al.(2023)Kawaguchi, Deng, Ji, and Huang]{kawaguchi2023does}
Kawaguchi, K., Deng, Z., Ji, X., and Huang, J.
\newblock How does information bottleneck help deep learning?
\newblock \emph{arXiv preprint arXiv:2305.18887}, 2023.

\bibitem[Kim et~al.(2023)Kim, Park, Choi, and Lee]{kim2023fantastic}
Kim, H., Park, J., Choi, Y., and Lee, J.
\newblock Fantastic robustness measures: The secrets of robust generalization.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Kim et~al.(2022)Kim, Li, Hu, and Hospedales]{kim2022fisher}
Kim, M., Li, D., Hu, S.~X., and Hospedales, T.
\newblock Fisher sam: Information geometry and sharpness aware minimisation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  11148--11161. PMLR, 2022.

\bibitem[Korpelevich(1976)]{korpelevich1976extragradient}
Korpelevich, G.~M.
\newblock The extragradient method for finding saddle points and other problems.
\newblock \emph{Matecon}, 12:\penalty0 747--756, 1976.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical report}, 2009.

\bibitem[Kumar et~al.(2023)Kumar, Marklund, and Van~Roy]{kumar2023maintaining}
Kumar, S., Marklund, H., and Van~Roy, B.
\newblock Maintaining plasticity via regenerative regularization.
\newblock \emph{arXiv preprint arXiv:2308.11958}, 2023.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Kwon, J., Kim, J., Park, H., and Choi, I.~K.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5905--5914. PMLR, 2021.

\bibitem[Liu et~al.(2022)Liu, Mai, Cheng, Chen, Hsieh, and You]{liu2022random}
Liu, Y., Mai, S., Cheng, M., Chen, X., Hsieh, C.-J., and You, Y.
\newblock Random sharpness-aware minimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24543--24556, 2022.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Kovalev, Shulgin, Richt{\'a}rik, and Malitsky]{mishchenko2020revisiting}
Mishchenko, K., Kovalev, D., Shulgin, E., Richt{\'a}rik, P., and Malitsky, Y.
\newblock Revisiting stochastic extragradient.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  4573--4582. PMLR, 2020.

\bibitem[M{\"o}llenhoff \& Khan(2023)M{\"o}llenhoff and Khan]{mollenhoff2023sam}
M{\"o}llenhoff, T. and Khan, M.~E.
\newblock {SAM} as an optimal relaxation of bayes.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Mueller et~al.(2023)Mueller, Vlaar, Rolnick, and Hein]{mueller2023normalization}
Mueller, M., Vlaar, T., Rolnick, D., and Hein, M.
\newblock Normalization layers are all that sharpness-aware minimization needs.
\newblock \emph{arXiv preprint arXiv:2306.04226}, 2023.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{recht2019imagenet}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International conference on machine learning}, pp.\  5389--5400. PMLR, 2019.

\bibitem[Ren et~al.(2020)Ren, Guo, Labeau, Cohen, and Kirby]{ren2020compositional}
Ren, Y., Guo, S., Labeau, M., Cohen, S.~B., and Kirby, S.
\newblock Compositional languages emerge in a neural iterated learning model.
\newblock \emph{arXiv preprint arXiv:2002.01365}, 2020.

\bibitem[Saxe et~al.(2019)Saxe, Bansal, Dapello, Advani, Kolchinsky, Tracey, and Cox]{saxe2019information}
Saxe, A.~M., Bansal, Y., Dapello, J., Advani, M., Kolchinsky, A., Tracey, B.~D., and Cox, D.~D.
\newblock On the information bottleneck theory of deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019\penalty0 (12):\penalty0 124020, 2019.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  1--9, 2015.

\bibitem[Taha et~al.(2021)Taha, Shrivastava, and Davis]{taha2021knowledge}
Taha, A., Shrivastava, A., and Davis, L.~S.
\newblock Knowledge evolution in neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  12843--12852, 2021.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
Tishby, N., Pereira, F.~C., and Bialek, W.
\newblock The information bottleneck method.
\newblock \emph{arXiv preprint physics/0004057}, 2000.

\bibitem[Tiwari \& Shenoy(2023)Tiwari and Shenoy]{tiwari2023overcoming}
Tiwari, R. and Shenoy, P.
\newblock Overcoming simplicity bias in deep networks using a feature sieve.
\newblock In \emph{International Conference on Machine Learning}, pp.\  34330--34343. PMLR, 2023.

\bibitem[Ujv{\'a}ry et~al.(2022)Ujv{\'a}ry, Telek, Kerekes, M{\'e}sz{\'a}ros, and Husz{\'a}r]{ujvary2022rethinking}
Ujv{\'a}ry, S., Telek, Z., Kerekes, A., M{\'e}sz{\'a}ros, A., and Husz{\'a}r, F.
\newblock Rethinking sharpness-aware minimization as variational inference.
\newblock \emph{arXiv preprint arXiv:2210.10452}, 2022.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{wang2019learning}
Wang, H., Ge, S., Lipton, Z., and Xing, E.~P.
\newblock Learning robust global representations by penalizing local predictive power.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  10506--10518, 2019.

\bibitem[Wen et~al.(2022)Wen, Ma, and Li]{wen2022sharpness}
Wen, K., Ma, T., and Li, Z.
\newblock How sharpness-aware minimization minimizes sharpness?
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Wen et~al.(2023)Wen, Ma, and Li]{wen2023sharpness}
Wen, K., Ma, T., and Li, Z.
\newblock Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization.
\newblock \emph{arXiv preprint arXiv:2307.11007}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Vani, Larochelle, and Courville]{zhou2022fortuitous}
Zhou, H., Vani, A., Larochelle, H., and Courville, A.
\newblock Fortuitous forgetting in connectionist networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhuang et~al.(2022)Zhuang, Gong, Yuan, Cui, Adam, Dvornek, Tatikonda, Duncan, and Liu]{zhuang2022surrogate}
Zhuang, J., Gong, B., Yuan, L., Cui, Y., Adam, H., Dvornek, N., Tatikonda, S., Duncan, J., and Liu, T.
\newblock Surrogate gap minimization improves sharpness-aware training.
\newblock \emph{arXiv preprint arXiv:2203.08065}, 2022.

\end{thebibliography}
