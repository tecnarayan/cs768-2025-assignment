\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akopyan et~al.(2015)Akopyan, Sawada, Cassidy, Alvarez-Icaza, Arthur, Merolla, Imam, Nakamura, Datta, Nam, et~al.]{2015TrueNorth}
Akopyan, F., Sawada, J., Cassidy, A., Alvarez-Icaza, R., Arthur, J., Merolla, P., Imam, N., Nakamura, Y., Datta, P., Nam, G.-J., et~al.
\newblock Truenorth: Design and tool flow of a 65 mw 1 million neuron programmable neurosynaptic chip.
\newblock \emph{IEEE transactions on computer-aided design of integrated circuits and systems}, 34\penalty0 (10):\penalty0 1537--1557, 2015.

\bibitem[Bellec et~al.(2018)Bellec, Salaj, Subramoney, Legenstein, and Maass]{bellec2018long}
Bellec, G., Salaj, D., Subramoney, A., Legenstein, R., and Maass, W.
\newblock Long short-term memory and learning-to-learn in networks of spiking neurons.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Bohte(2011)]{2011Error}
Bohte, S.~M.
\newblock Error-backpropagation in networks of fractionally predictive spiking neurons.
\newblock In \emph{International Conference on Artificial Neural Networks}, pp.\  60--68. Springer, 2011.

\bibitem[Bu et~al.(2021)Bu, Fang, Ding, Dai, Yu, and Huang]{bu2022optimal}
Bu, T., Fang, W., Ding, J., Dai, P., Yu, Z., and Huang, T.
\newblock Optimal ann-snn conversion for high-accuracy and ultra-low-latency spiking neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Bu et~al.(2022)Bu, Ding, Yu, and Huang]{2022Optimized}
Bu, T., Ding, J., Yu, Z., and Huang, T.
\newblock Optimized potential initialization for low-latency spiking neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  11--20, 2022.

\bibitem[Cheng et~al.(2020)Cheng, Hao, Xu, and Xu]{2020LISNN}
Cheng, X., Hao, Y., Xu, J., and Xu, B.
\newblock Lisnn: Improving spiking neural networks with lateral interactions for robust object recognition.
\newblock In \emph{IJCAI}, pp.\  1519--1525, 2020.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and Bengio]{courbariaux2016binarized}
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and Le]{cubuk2019autoaugment}
Cubuk, E.~D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q.~V.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Davies et~al.(2018)Davies, Srinivasa, Lin, Chinya, Cao, Choday, Dimou, Joshi, Imam, Jain, et~al.]{2018Loihi}
Davies, M., Srinivasa, N., Lin, T.-H., Chinya, G., Cao, Y., Choday, S.~H., Dimou, G., Joshi, P., Imam, N., Jain, S., et~al.
\newblock Loihi: A neuromorphic manycore processor with on-chip learning.
\newblock \emph{Ieee Micro}, 38\penalty0 (1):\penalty0 82--99, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{2009ImageNet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Deng et~al.(2022)Deng, Li, Zhang, and Gu]{deng2022temporal}
Deng, S., Li, Y., Zhang, S., and Gu, S.
\newblock Temporal efficient training of spiking neural network via gradient re-weighting.
\newblock \emph{arXiv preprint arXiv:2202.11946}, 2022.

\bibitem[Diehl \& Cook(2015)Diehl and Cook]{Peter2015Unsupervised}
Diehl, P.~U. and Cook, M.
\newblock Unsupervised learning of digit recognition using spike-timing-dependent plasticity.
\newblock \emph{Frontiers in computational neuroscience}, 9:\penalty0 99, 2015.

\bibitem[Duan et~al.(2022)Duan, Ding, Chen, Yu, and Huang]{duan2022temporal}
Duan, C., Ding, J., Chen, S., Yu, Z., and Huang, T.
\newblock Temporal effective batch normalization in spiking neural networks.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=fLIgyyQiJqz}.

\bibitem[Esser et~al.(2016)Esser, Merolla, Arthur, Cassidy, Appuswamy, Andreopoulos, Berg, McKinstry, Melano, Barch, et~al.]{esser2016cover}
Esser, S.~K., Merolla, P.~A., Arthur, J.~V., Cassidy, A.~S., Appuswamy, R., Andreopoulos, A., Berg, D.~J., McKinstry, J.~L., Melano, T., Barch, D.~R., et~al.
\newblock From the cover: Convolutional networks for fast, energy-efficient neuromorphic computing.
\newblock \emph{Proceedings of the National Academy of Sciences of the United States of America}, 113\penalty0 (41):\penalty0 11441, 2016.

\bibitem[Fang et~al.(2021{\natexlab{a}})Fang, Yu, Chen, Huang, Masquelier, and Tian]{2021Deep}
Fang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., and Tian, Y.
\newblock Deep residual learning in spiking neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 21056--21069, 2021{\natexlab{a}}.

\bibitem[Fang et~al.(2021{\natexlab{b}})Fang, Yu, Chen, Masquelier, Huang, and Tian]{2020Incorporating}
Fang, W., Yu, Z., Chen, Y., Masquelier, T., Huang, T., and Tian, Y.
\newblock Incorporating learnable membrane time constant to enhance learning of spiking neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  2661--2671, 2021{\natexlab{b}}.

\bibitem[Guo et~al.(2022{\natexlab{a}})Guo, Chen, Zhang, Liu, Wang, Huang, and Ma]{guo2022imloss}
Guo, Y., Chen, Y., Zhang, L., Liu, X., Wang, Y., Huang, X., and Ma, Z.
\newblock {IM}-loss: Information maximization loss for spiking neural networks.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=Jw34v_84m2b}.

\bibitem[Guo et~al.(2022{\natexlab{b}})Guo, Chen, Zhang, Wang, Liu, Tong, Ou, Huang, and Ma]{Guo2022eccv}
Guo, Y., Chen, Y., Zhang, L., Wang, Y., Liu, X., Tong, X., Ou, Y., Huang, X., and Ma, Z.
\newblock Reducing information loss for spiking neural networks.
\newblock In Avidan, S., Brostow, G., Ciss{\'e}, M., Farinella, G.~M., and Hassner, T. (eds.), \emph{Computer Vision -- ECCV 2022}, pp.\  36--52, Cham, 2022{\natexlab{b}}. Springer Nature Switzerland.
\newblock ISBN 978-3-031-20083-0.

\bibitem[Guo et~al.(2022{\natexlab{c}})Guo, Tong, Chen, Zhang, Liu, Ma, and Huang]{Guo_2022_CVPR}
Guo, Y., Tong, X., Chen, Y., Zhang, L., Liu, X., Ma, Z., and Huang, X.
\newblock Recdis-snn: Rectifying membrane potential distribution for directly training spiking neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  326--335, June 2022{\natexlab{c}}.

\bibitem[Guo et~al.(2022{\natexlab{d}})Guo, Zhang, Chen, Tong, Liu, Wang, Huang, and Ma]{guo2022real}
Guo, Y., Zhang, L., Chen, Y., Tong, X., Liu, X., Wang, Y., Huang, X., and Ma, Z.
\newblock Real spike: Learning real-valued spikes for spiking neural networks.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XII}, pp.\  52--68. Springer, 2022{\natexlab{d}}.

\bibitem[Guo et~al.(2023{\natexlab{a}})Guo, Huang, and Ma]{guo2023direct}
Guo, Y., Huang, X., and Ma, Z.
\newblock Direct learning-based deep spiking neural networks: a review.
\newblock \emph{Frontiers in Neuroscience}, 17:\penalty0 1209795, 2023{\natexlab{a}}.

\bibitem[Guo et~al.(2023{\natexlab{b}})Guo, Zhang, Chen, Peng, Liu, Zhang, Huang, and Ma]{Guo_2023_ICCV}
Guo, Y., Zhang, Y., Chen, Y., Peng, W., Liu, X., Zhang, L., Huang, X., and Ma, Z.
\newblock Membrane potential batch normalization for spiking neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pp.\  19420--19430, October 2023{\natexlab{b}}.

\bibitem[Guo et~al.(2024)Guo, Chen, Liu, Peng, Zhang, Huang, and Ma]{guo2024ternary}
Guo, Y., Chen, Y., Liu, X., Peng, W., Zhang, Y., Huang, X., and Ma, Z.
\newblock Ternary spike: Learning ternary spikes for spiking neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pp.\  12244--12252, 2024.

\bibitem[Han \& Roy(2020)Han and Roy]{2020Deep}
Han, B. and Roy, K.
\newblock Deep spiking neural network: Energy efficiency through time based coding.
\newblock In \emph{European Conference on Computer Vision}, pp.\  388--404. Springer, 2020.

\bibitem[Hao et~al.(2020)Hao, Huang, Dong, and Xu]{2018ABiologically}
Hao, Y., Huang, X., Dong, M., and Xu, B.
\newblock A biologically plausible supervised learning method for spiking neural networks using the symmetric stdp rule.
\newblock \emph{Neural Networks}, 121:\penalty0 387--395, 2020.

\bibitem[Hao et~al.(2023{\natexlab{a}})Hao, Bu, Ding, Huang, and Yu]{hao2023reducing}
Hao, Z., Bu, T., Ding, J., Huang, T., and Yu, Z.
\newblock Reducing ann-snn conversion error through residual membrane potential.
\newblock \emph{arXiv preprint arXiv:2302.02091}, 2023{\natexlab{a}}.

\bibitem[Hao et~al.(2023{\natexlab{b}})Hao, Ding, Bu, Huang, and Yu]{hao2023bridging}
Hao, Z., Ding, J., Bu, T., Huang, T., and Yu, Z.
\newblock Bridging the gap between anns and snns by calibrating offset spikes.
\newblock \emph{arXiv preprint arXiv:2302.10685}, 2023{\natexlab{b}}.

\bibitem[Ho \& Chang(2021)Ho and Chang]{2020TCL}
Ho, N.-D. and Chang, I.-J.
\newblock Tcl: an ann-to-snn conversion with trainable clipping layers.
\newblock In \emph{2021 58th ACM/IEEE Design Automation Conference (DAC)}, pp.\  793--798. IEEE, 2021.

\bibitem[Hu et~al.(2021)Hu, Wu, Deng, and Li]{2021Advancing}
Hu, Y., Wu, Y., Deng, L., and Li, G.
\newblock Advancing residual learning towards powerful deep spiking neural networks.
\newblock \emph{arXiv preprint arXiv:2112.08954}, 2021.

\bibitem[Hu et~al.(2023)Hu, Deng, Wu, Yao, and Li]{hu2023advancing}
Hu, Y., Deng, L., Wu, Y., Yao, M., and Li, G.
\newblock Advancing spiking neural networks towards deep residual learning, 2023.

\bibitem[Ikegawa et~al.(2022)Ikegawa, Saiin, Sawada, and Natori]{Rethinking2022}
Ikegawa, S.-i., Saiin, R., Sawada, Y., and Natori, N.
\newblock Rethinking the role of normalization and residual blocks for spiking neural networks.
\newblock \emph{Sensors}, 22\penalty0 (8), 2022.
\newblock ISSN 1424-8220.
\newblock \doi{10.3390/s22082876}.
\newblock URL \url{https://www.mdpi.com/1424-8220/22/8/2876}.

\bibitem[Kim et~al.(2019)Kim, Park, Na, and Yoon]{kim2019spikingyolo}
Kim, S., Park, S., Na, B., and Yoon, S.
\newblock Spiking-yolo: Spiking neural network for energy-efficient object detection, 2019.

\bibitem[Kim \& Panda(2021)Kim and Panda]{2020Revisiting}
Kim, Y. and Panda, P.
\newblock Revisiting batch normalization for training low-latency deep spiking neural networks from scratch.
\newblock \emph{Frontiers in neuroscience}, pp.\  1638, 2021.

\bibitem[Krizhevsky et~al.(2010)Krizhevsky, Nair, and Hinton]{CIFAR-10}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock \emph{URL http://www. cs. toronto. edu/kriz/cifar. html}, 5\penalty0 (4):\penalty0 1, 2010.

\bibitem[Lan et~al.(2023)Lan, Zhang, Ma, Qu, and Fu]{lan2023efficient}
Lan, Y., Zhang, Y., Ma, X., Qu, Y., and Fu, Y.
\newblock Efficient converted spiking neural network for 3d and 2d classification.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pp.\  9211--9220, 2023.

\bibitem[Li et~al.(2017)Li, Liu, Ji, Li, and Shi]{2017CIFAR10}
Li, H., Liu, H., Ji, X., Li, G., and Shi, L.
\newblock Cifar10-dvs: an event-stream dataset for object classification.
\newblock \emph{Frontiers in neuroscience}, 11:\penalty0 309, 2017.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Deng, Dong, Gong, and Gu]{li2021free}
Li, Y., Deng, S., Dong, X., Gong, R., and Gu, S.
\newblock A free lunch from ann: Towards efficient, accurate spiking neural networks calibration.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6316--6325. PMLR, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Guo, Zhang, Deng, Hai, and Gu]{li2021differentiable}
Li, Y., Guo, Y., Zhang, S., Deng, S., Hai, Y., and Gu, S.
\newblock Differentiable spike: Rethinking gradient-descent for training spiking neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 23426--23439, 2021{\natexlab{b}}.

\bibitem[Lobov et~al.(2020)Lobov, Mikhaylov, and Kazantsev]{2020Spatial}
Lobov, S.~A., Mikhaylov, A.~N., and Kazantsev, V.~B.
\newblock Spatial properties of stdp in a self-learning spiking neural network enable controlling a mobile robot.
\newblock \emph{Frontiers in Neuroscience}, 14:\penalty0 --, 2020.

\bibitem[Ma et~al.(2017)Ma, Shen, Gu, Zhang, Zhu, Xu, Xu, Shen, and Pan]{2015Darwin}
Ma, D., Shen, J., Gu, Z., Zhang, M., Zhu, X., Xu, X., Xu, Q., Shen, Y., and Pan, G.
\newblock Darwin: A neuromorphic hardware co-processor based on spiking neural networks.
\newblock \emph{Journal of Systems Architecture}, 77:\penalty0 43--51, 2017.

\bibitem[Meng et~al.(2022)Meng, Xiao, Yan, Wang, Lin, and Luo]{meng2022training}
Meng, Q., Xiao, M., Yan, S., Wang, Y., Lin, Z., and Luo, Z.-Q.
\newblock Training high-performance low-latency spiking neural networks by differentiation on spike representation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  12444--12453, 2022.

\bibitem[Neftci et~al.(2019)Neftci, Mostafa, and Zenke]{2019Surrogate}
Neftci, E.~O., Mostafa, H., and Zenke, F.
\newblock Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks.
\newblock \emph{IEEE Signal Processing Magazine}, 36\penalty0 (6):\penalty0 51--63, 2019.

\bibitem[Park et~al.(2020)Park, Kim, Na, and Yoon]{2020T2FSNN}
Park, S., Kim, S., Na, B., and Yoon, S.
\newblock T2fsnn: Deep spiking neural networks with time-to-first-spike coding.
\newblock In \emph{2020 57th ACM/IEEE Design Automation Conference (DAC)}, pp.\  1--6. IEEE, 2020.

\bibitem[Pei et~al.(2019)Pei, Deng, Song, Zhao, Zhang, Wu, Wang, Zou, Wu, He, et~al.]{2019Towards}
Pei, J., Deng, L., Song, S., Zhao, M., Zhang, Y., Wu, S., Wang, G., Zou, Z., Wu, Z., He, W., et~al.
\newblock Towards artificial general intelligence with hybrid tianjic chip architecture.
\newblock \emph{Nature}, 572\penalty0 (7767):\penalty0 106--111, 2019.

\bibitem[Qu et~al.(2023)Qu, Gao, Zhang, Lu, Tang, and Qiao]{qu2023spiking}
Qu, J., Gao, Z., Zhang, T., Lu, Y., Tang, H., and Qiao, H.
\newblock Spiking neural network for ultra-low-latency and high-accurate object detection, 2023.

\bibitem[Rathi \& Roy(2020)Rathi and Roy]{2020DIET}
Rathi, N. and Roy, K.
\newblock Diet-snn: Direct input encoding with leakage and threshold optimization in deep spiking neural networks.
\newblock \emph{arXiv preprint arXiv:2008.03658}, 2020.

\bibitem[Rathi et~al.(2020)Rathi, Srinivasan, Panda, and Roy]{2020Enabling}
Rathi, N., Srinivasan, G., Panda, P., and Roy, K.
\newblock Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation.
\newblock \emph{arXiv preprint arXiv:2005.01807}, 2020.

\bibitem[Ren et~al.(2023)Ren, Ma, Chen, Peng, Liu, Zhang, and Guo]{ren2023spiking}
Ren, D., Ma, Z., Chen, Y., Peng, W., Liu, X., Zhang, Y., and Guo, Y.
\newblock Spiking pointnet: Spiking neural networks for point clouds.
\newblock \emph{arXiv preprint arXiv:2310.06232}, 2023.

\bibitem[Sengupta et~al.(2019)Sengupta, Ye, Wang, Liu, and Roy]{2019Going}
Sengupta, A., Ye, Y., Wang, R., Liu, C., and Roy, K.
\newblock Going deeper in spiking neural networks: Vgg and residual architectures.
\newblock \emph{Frontiers in neuroscience}, 13:\penalty0 95, 2019.

\bibitem[Wu et~al.(2021{\natexlab{a}})Wu, Chua, Zhang, Li, Li, and Tan]{wu2021tandem}
Wu, J., Chua, Y., Zhang, M., Li, G., Li, H., and Tan, K.~C.
\newblock A tandem learning rule for effective training and rapid inference of deep spiking neural networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2021{\natexlab{a}}.

\bibitem[Wu et~al.(2021{\natexlab{b}})Wu, Xu, Han, Zhou, Zhang, Li, and Tan]{wu2021progressive}
Wu, J., Xu, C., Han, X., Zhou, D., Zhang, M., Li, H., and Tan, K.~C.
\newblock Progressive tandem learning for pattern recognition with deep spiking neural networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44\penalty0 (11):\penalty0 7824--7840, 2021{\natexlab{b}}.

\bibitem[Wu et~al.(2018)Wu, Deng, Li, Zhu, and Shi]{2018Spatio}
Wu, Y., Deng, L., Li, G., Zhu, J., and Shi, L.
\newblock Spatio-temporal backpropagation for training high-performance spiking neural networks.
\newblock \emph{Frontiers in neuroscience}, 12:\penalty0 331, 2018.

\bibitem[Wu et~al.(2019)Wu, Deng, Li, Zhu, Xie, and Shi]{2018Direct}
Wu, Y., Deng, L., Li, G., Zhu, J., Xie, Y., and Shi, L.
\newblock Direct training for spiking neural networks: Faster, larger, better.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pp.\  1311--1318, 2019.

\bibitem[Xiao et~al.(2021)Xiao, Meng, Zhang, Wang, and Lin]{2021TrainingXiao}
Xiao, M., Meng, Q., Zhang, Z., Wang, Y., and Lin, Z.
\newblock Training feedback spiking neural networks by implicit differentiation on the equilibrium state.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 14516--14528, 2021.

\bibitem[Xu et~al.(2023)Xu, Li, Shen, Liu, Tang, and Pan]{xu2023constructing}
Xu, Q., Li, Y., Shen, J., Liu, J.~K., Tang, H., and Pan, G.
\newblock Constructing deep spiking neural networks from artificial neural networks with knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2304.05627}, 2023.

\bibitem[Yang et~al.(2022)Yang, Wu, Zhang, Chua, Wang, and Li]{yang2022training}
Yang, Q., Wu, J., Zhang, M., Chua, Y., Wang, X., and Li, H.
\newblock Training spiking neural networks with local tandem learning.
\newblock \emph{arXiv preprint arXiv:2210.04532}, 2022.

\bibitem[Yao et~al.(2022)Yao, Li, Mo, and Cheng]{yao2023glif}
Yao, X., Li, F., Mo, Z., and Cheng, J.
\newblock Glif: A unified gated leaky integrate-and-fire neuron for spiking neural networks.
\newblock \emph{arXiv preprint arXiv:2210.13768}, 2022.

\bibitem[Zenke \& Ganguli(2018)Zenke and Ganguli]{2017SuperSpike}
Zenke, F. and Ganguli, S.
\newblock Superspike: Supervised learning in multilayer spiking neural networks.
\newblock \emph{Neural computation}, 30\penalty0 (6):\penalty0 1514--1541, 2018.

\bibitem[Zhang \& Li(2020)Zhang and Li]{2020Temporal}
Zhang, W. and Li, P.
\newblock Temporal spike sequence learning via backpropagation for deep spiking neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 12022--12033, 2020.

\bibitem[Zhang et~al.(2024)Zhang, Liu, Chen, Peng, Guo, Huang, and Ma]{zhang2024enhancing}
Zhang, Y., Liu, X., Chen, Y., Peng, W., Guo, Y., Huang, X., and Ma, Z.
\newblock Enhancing representation of spiking neural networks via similarity-sensitive contrastive learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pp.\  16926--16934, 2024.

\bibitem[Zheng et~al.(2021)Zheng, Wu, Deng, Hu, and Li]{2020Going}
Zheng, H., Wu, Y., Deng, L., Hu, Y., and Li, G.
\newblock Going deeper with directly-trained larger spiking neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  11062--11070, 2021.

\bibitem[Zou et~al.(2023)Zou, Mu, Zuo, Wang, and Cheng]{zou2023eventbased}
Zou, S., Mu, Y., Zuo, X., Wang, S., and Cheng, L.
\newblock Event-based human pose tracking by spiking spatiotemporal transformer, 2023.

\end{thebibliography}
