\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Afrasiyabi et~al.(2021)Afrasiyabi, Lalonde, and
  Gagn{\'e}]{afrasiyabi2021mixture}
Afrasiyabi, A., Lalonde, J.-F., and Gagn{\'e}, C.
\newblock Mixture-based feature space learning for few-shot image
  classification.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9041--9051, 2021.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and De~Freitas]{andrychowicz2016learning}
Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.~W., Pfau, D., Schaul, T.,
  Shillingford, B., and De~Freitas, N.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3981--3989, 2016.

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O'Connor, and
  McGuinness]{arazo2019unsupervised}
Arazo, E., Ortego, D., Albert, P., O'Connor, N.~E., and McGuinness, K.
\newblock Unsupervised label noise modeling and loss correction.
\newblock \emph{arXiv preprint arXiv:1904.11238}, 2019.

\bibitem[Baik et~al.(2021)Baik, Choi, Kim, Cho, Min, and Lee]{baik2021meta}
Baik, S., Choi, J., Kim, H., Cho, D., Min, J., and Lee, K.~M.
\newblock Meta-learning with task-adaptive loss function for few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9465--9474, 2021.

\bibitem[Basu \& Christensen(2013)Basu and Christensen]{basu2013teaching}
Basu, S. and Christensen, J.
\newblock Teaching classification boundaries to humans.
\newblock In \emph{Twenty-Seventh AAAI Conference on Artificial Intelligence},
  2013.

\bibitem[Bertinetto et~al.(2018)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto2018meta}
Bertinetto, L., Henriques, J.~F., Torr, P.~H., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock \emph{arXiv preprint arXiv:1805.08136}, 2018.

\bibitem[Cao et~al.(2019)Cao, Law, and Fidler]{cao2019theoretical}
Cao, T., Law, M., and Fidler, S.
\newblock A theoretical analysis of the number of shots in few-shot learning.
\newblock \emph{arXiv preprint arXiv:1909.11722}, 2019.

\bibitem[Chi et~al.(2021)Chi, Liu, Yang, Lan, Liu, Niu, and Han]{chi2021meta}
Chi, H., Liu, F., Yang, W., Lan, L., Liu, T., Niu, G., and Han, B.
\newblock Meta discovery: Learning to discover novel classes given very limited
  data.
\newblock \emph{arXiv preprint arXiv:2102.04002}, 2021.

\bibitem[Du et~al.(2021)Du, Zhang, Han, Liu, Rong, Niu, Huang, and
  Sugiyama]{du2021learning}
Du, X., Zhang, J., Han, B., Liu, T., Rong, Y., Niu, G., Huang, J., and
  Sugiyama, M.
\newblock Learning diverse-structured networks for adversarial robustness.
\newblock \emph{arXiv preprint arXiv:2102.01886}, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1126--1135. JMLR. org, 2017.

\bibitem[Flennerhag et~al.(2020)Flennerhag, Rusu, Pascanu, Visin, Yin, and
  Hadsell]{flennerhag2020meta}
Flennerhag, S., Rusu, A., Pascanu, R., Visin, F., Yin, H., and Hadsell, R.
\newblock Meta-learning with warped gradient descent.
\newblock In \emph{International Conference on Learning Representations 2020},
  2020.

\bibitem[Fr{\'e}nay \& Verleysen(2013)Fr{\'e}nay and
  Verleysen]{frenay2013classification}
Fr{\'e}nay, B. and Verleysen, M.
\newblock Classification in the presence of label noise: a survey.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  25\penalty0 (5):\penalty0 845--869, 2013.

\bibitem[Gao et~al.(2020)Gao, Liu, Zhang, Han, Liu, Niu, and
  Sugiyama]{gao2020maximum}
Gao, R., Liu, F., Zhang, J., Han, B., Liu, T., Niu, G., and Sugiyama, M.
\newblock Maximum mean discrepancy is aware of adversarial attacks.
\newblock \emph{arXiv preprint arXiv:2010.11415}, 2020.

\bibitem[Gidaris \& Komodakis(2018)Gidaris and Komodakis]{gidaris2018dynamic}
Gidaris, S. and Komodakis, N.
\newblock Dynamic few-shot visual learning without forgetting.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4367--4375, 2018.

\bibitem[Han et~al.(2018{\natexlab{a}})Han, Yao, Niu, Zhou, Tsang, Zhang, and
  Sugiyama]{han2018masking}
Han, B., Yao, J., Niu, G., Zhou, M., Tsang, I., Zhang, Y., and Sugiyama, M.
\newblock Masking: A new perspective of noisy supervision.
\newblock \emph{arXiv preprint arXiv:1805.08193}, 2018{\natexlab{a}}.

\bibitem[Han et~al.(2018{\natexlab{b}})Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock \emph{arXiv preprint arXiv:1804.06872}, 2018{\natexlab{b}}.

\bibitem[Han et~al.(2020)Han, Niu, Yu, Yao, Xu, Tsang, and
  Sugiyama]{han2020sigua}
Han, B., Niu, G., Yu, X., Yao, Q., Xu, M., Tsang, I., and Sugiyama, M.
\newblock Sigua: Forgetting may make learning with noisy labels more robust.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4006--4016. PMLR, 2020.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and
  Gimpel]{hendrycks2018using}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  10456--10465, 2018.

\bibitem[Hu et~al.(2020)Hu, Moreno, Xiao, Shen, Obozinski, Lawrence, and
  Damianou]{hu2020empirical}
Hu, S.~X., Moreno, P.~G., Xiao, Y., Shen, X., Obozinski, G., Lawrence, N.~D.,
  and Damianou, A.
\newblock Empirical bayes transductive meta-learning with synthetic gradients.
\newblock \emph{arXiv preprint arXiv:2004.12696}, 2020.

\bibitem[Hu et~al.(2017)Hu, Li, Li, and Liu]{hu2017diffusion}
Hu, W., Li, C.~J., Li, L., and Liu, J.-G.
\newblock On the diffusion approximation of nonconvex stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1705.07562}, 2017.

\bibitem[Jamal \& Qi(2019)Jamal and Qi]{jamal2019task}
Jamal, M.~A. and Qi, G.-J.
\newblock Task agnostic meta-learning for few-shot learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  11719--11727, 2019.

\bibitem[Jastrzbski et~al.(2017)Jastrzbski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{jastrzkebski2017three}
Jastrzbski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and
  Storkey, A.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Khan et~al.(2011)Khan, Mutlu, and Zhu]{khan2011humans}
Khan, F., Mutlu, B., and Zhu, J.
\newblock How do humans teach: On curriculum learning and teaching dimension.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1449--1457, 2011.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Koch et~al.(2015)Koch, Zemel, and Salakhutdinov]{koch2015siamese}
Koch, G., Zemel, R., and Salakhutdinov, R.
\newblock Siamese neural networks for one-shot image recognition.
\newblock In \emph{ICML deep learning workshop}, volume~2. Lille, 2015.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010self}
Kumar, M.~P., Packer, B., and Koller, D.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1189--1197, 2010.

\bibitem[Lee et~al.(2020)Lee, Nam, Yang, and Hwang]{lee2020meta}
Lee, H., Nam, T., Yang, E., and Hwang, S.
\newblock Meta dropout: Learning to perturb latent features for generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lee et~al.(2019)Lee, Nam, Yang, and Hwang]{lee2019meta}
Lee, H.~B., Nam, T., Yang, E., and Hwang, S.~J.
\newblock Meta dropout: Learning to perturb latent features for generalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Wang, Tang, Shi, Wu, Zhuang, and
  Wang]{li2020unsupervised}
Li, J., Wang, X., Tang, S., Shi, H., Wu, F., Zhuang, Y., and Wang, W.~Y.
\newblock Unsupervised reinforcement learning of transferable meta-skills for
  embodied navigation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12123--12132, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Soltanolkotabi, and
  Oymak]{li2020gradient}
Li, M., Soltanolkotabi, M., and Oymak, S.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pp.\  4313--4324. PMLR, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{li2017meta}
Li, Z., Zhou, F., Chen, F., and Li, H.
\newblock Meta-sgd: Learning to learn quickly for few-shot learning.
\newblock \emph{arXiv preprint arXiv:1707.09835}, 2017.

\bibitem[Lin et~al.(2014)Lin, Weld, et~al.]{lin2014re}
Lin, C.~H., Weld, D.~S., et~al.
\newblock To re (label), or not to re (label).
\newblock In \emph{Second AAAI conference on human computation and
  crowdsourcing}, 2014.

\bibitem[Liu(2020)]{liu2020towards}
Liu, F.
\newblock \emph{Towards Realistic Transfer Learning Methods: Theory and
  Algorithms}.
\newblock PhD thesis, 2020.

\bibitem[Mandt et~al.(2016)Mandt, Hoffman, and Blei]{mandt2016variational}
Mandt, S., Hoffman, M., and Blei, D.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In \emph{International conference on machine learning}, pp.\
  354--363, 2016.

\bibitem[Mishra et~al.(2017)Mishra, Rohaninejad, Chen, and
  Abbeel]{mishra2017simple}
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.
\newblock A simple neural attentive meta-learner.
\newblock \emph{arXiv preprint arXiv:1707.03141}, 2017.

\bibitem[Mordatch(2018)]{mordatch2018concept}
Mordatch, I.
\newblock Concept learning with energy-based models.
\newblock \emph{arXiv preprint arXiv:1811.02486}, 2018.

\bibitem[Neelakantan et~al.(2015)Neelakantan, Vilnis, Le, Sutskever, Kaiser,
  Kurach, and Martens]{neelakantan2015adding}
Neelakantan, A., Vilnis, L., Le, Q.~V., Sutskever, I., Kaiser, L., Kurach, K.,
  and Martens, J.
\newblock Adding gradient noise improves learning for very deep networks.
\newblock \emph{arXiv preprint arXiv:1511.06807}, 2015.

\bibitem[Ni et~al.(2021)Ni, Goldblum, Sharaf, Kong, and Goldstein]{ni2021data}
Ni, R., Goldblum, M., Sharaf, A., Kong, K., and Goldstein, T.
\newblock Data augmentation for meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8152--8161. PMLR, 2021.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Nichol, A., Achiam, J., and Schulman, J.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Oreshkin et~al.(2018)Oreshkin, L{\'o}pez, and
  Lacoste]{oreshkin2018tadam}
Oreshkin, B., L{\'o}pez, P.~R., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  721--731, 2018.

\bibitem[Park \& Oliva(2020)Park and Oliva]{park2020meta}
Park, E. and Oliva, J.~B.
\newblock Meta-curvature.
\newblock 2020.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1944--1952, 2017.

\bibitem[Rajendran et~al.(2020)Rajendran, Irpan, and Jang]{rajendran2020meta}
Rajendran, J., Irpan, A., and Jang, E.
\newblock Meta-learning requires meta-augmentation.
\newblock \emph{arXiv preprint arXiv:2007.05549}, 2020.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  113--124, 2019.

\bibitem[Ravi \& Larochelle(2016)Ravi and Larochelle]{ravi2016optimization}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock 2016.

\bibitem[Ravichandran et~al.(2019)Ravichandran, Bhotika, and
  Soatto]{ravichandran2019few}
Ravichandran, A., Bhotika, R., and Soatto, S.
\newblock Few-shot learning with embedded class models and shot-free meta
  training.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  331--339, 2019.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro2016meta}
Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1842--1850, 2016.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Snell, J., Swersky, K., and Zemel, R.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4077--4087, 2017.

\bibitem[Tang et~al.(2012)Tang, Yang, and Gao]{tang2012self}
Tang, Y., Yang, Y.-B., and Gao, Y.
\newblock Self-paced dictionary learning for image classification.
\newblock In \emph{Proceedings of the 20th ACM international conference on
  Multimedia}, pp.\  833--836, 2012.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3630--3638, 2016.

\bibitem[Von~Oswald et~al.(2021)Von~Oswald, Zhao, Kobayashi, Schug, Caccia,
  Zucchet, and Sacramento]{von2021learning}
Von~Oswald, J., Zhao, D., Kobayashi, S., Schug, S., Caccia, M., Zucchet, N.,
  and Sacramento, J.
\newblock Learning where to learn: Gradient sparsity in meta and continual
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wu et~al.(2019)Wu, Hu, Xiong, Huan, Braverman, and Zhu]{Wu2019On}
Wu, J., Hu, W., Xiong, H., Huan, J., Braverman, V., and Zhu, Z.
\newblock On the noisy gradient descent that generalizes as sgd.
\newblock 2019.

\bibitem[Yang et~al.(2020)Yang, Li, Zhang, Zhou, Zhou, and Liu]{yang2020dpgn}
Yang, L., Li, L., Zhang, Z., Zhou, X., Zhou, E., and Liu, Y.
\newblock Dpgn: Distribution propagation graph network for few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13390--13399, 2020.

\bibitem[Yang et~al.(2021)Yang, Liu, and Xu]{yang2021free}
Yang, S., Liu, L., and Xu, M.
\newblock Free lunch for few-shot learning: Distribution calibration.
\newblock \emph{arXiv preprint arXiv:2101.06395}, 2021.

\bibitem[Yao et~al.(2018)Yao, Wang, Tsang, Zhang, Sun, Zhang, and
  Zhang]{yao2018deep}
Yao, J., Wang, J., Tsang, I.~W., Zhang, Y., Sun, J., Zhang, C., and Zhang, R.
\newblock Deep learning from noisy image labels with quality embedding.
\newblock \emph{IEEE Transactions on Image Processing}, 28\penalty0
  (4):\penalty0 1909--1922, 2018.

\bibitem[Yin et~al.(2019)Yin, Tucker, Zhou, Levine, and Finn]{yin2019meta}
Yin, M., Tucker, G., Zhou, M., Levine, S., and Finn, C.
\newblock Meta-learning without memorization.
\newblock \emph{arXiv preprint arXiv:1912.03820}, 2019.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama, M.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7164--7173. PMLR, 2019.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Han, Niu, Cui, Sugiyama, and
  Kankanhalli]{zhang2020attacks}
Zhang, J., Xu, X., Han, B., Niu, G., Cui, L., Sugiyama, M., and Kankanhalli, M.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11278--11287. PMLR, 2020.

\bibitem[Zhu et~al.(2019)Zhu, Ma, and Hu]{zhu2019self}
Zhu, P., Ma, W., and Hu, Q.
\newblock Self-paced robust deep face recognition with label noise.
\newblock In \emph{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining}, pp.\  425--435. Springer, 2019.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarli, Kurin, Hofmann, and
  Whiteson]{zintgraf2019fast}
Zintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Fast context adaptation via meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7693--7702. PMLR, 2019.

\bibitem[Zintgraf et~al.(2018)Zintgraf, Shiarlis, Kurin, Hofmann, and
  Whiteson]{zintgraf2018caml}
Zintgraf, L.~M., Shiarlis, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Caml: Fast context adaptation via meta-learning.
\newblock 2018.

\bibitem[Zuo et~al.(2018)Zuo, Lu, Zhang, and Liu]{zuo2018fuzzy}
Zuo, H., Lu, J., Zhang, G., and Liu, F.
\newblock Fuzzy transfer learning using an infinite gaussian mixture model and
  active learning.
\newblock \emph{IEEE Transactions on Fuzzy Systems}, 27\penalty0 (2):\penalty0
  291--303, 2018.

\end{thebibliography}
