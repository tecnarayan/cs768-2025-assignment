We propose a novel formalism for describing Structural Causal Models (SCMs)
as fixed-point problems on causally ordered variables, eliminating the need for
Directed Acyclic Graphs (DAGs), and establish the weakest known conditions for
their unique recovery given the topological ordering (TO). Based on this, we
design a two-stage causal generative model that first infers in a zero-shot
manner a valid TO from observations, and then learns the generative SCM on the
ordered variables. To infer TOs, we propose to amortize the learning of TOs on
synthetically generated datasets by sequentially predicting the leaves of
graphs seen during training. To learn SCMs, we design a transformer-based
architecture that exploits a new attention mechanism enabling the modeling of
causal structures, and show that this parameterization is consistent with our
formalism. Finally, we conduct an extensive evaluation of each method
individually, and show that when combined, our model outperforms various
baselines on generated out-of-distribution problems. The code is available on
\href{https://github.com/microsoft/causica/tree/main/research_experiments/fip}{Github}.