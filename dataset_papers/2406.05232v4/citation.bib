@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@InProceedings{bojar-EtAl:2016:WMT1,
  author    = {Bojar, Ond
{r}ej  and  Chatterjee, Rajen  and  Federmann, Christian  and  Graham, Yvette  and  Haddow, Barry  and  Huck, Matthias  and  Jimeno Yepes, Antonio  and  Koehn, Philipp  and  Logacheva, Varvara  and  Monz, Christof  and  Negri, Matteo  and  Neveol, Aurelie  and  Neves, Mariana  and  Popel, Martin  and  Post, Matt  and  Rubino, Raphael  and  Scarton, Carolina  and  Specia, Lucia  and  Turchi, Marco  and  Verspoor, Karin  and  Zampieri, Marcos},
  title     = {Findings of the 2016 Conference on Machine Translation},
  booktitle = {Proceedings of the First Conference on Machine Translation},
  month     = {August},
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  pages     = {131--198},
  url       = {http://www.aclweb.org/anthology/W/W16/W16-2301}
}
@article{lin2024detecting,
  title={Detecting multimedia generated by large ai models: A survey},
  author={Lin, Li and Gupta, Neeraj and Zhang, Yue and Ren, Hainan and Liu, Chun-Hao and Ding, Feng and Wang, Xin and Li, Xin and Verdoliva, Luisa and Hu, Shu},
  journal={arXiv preprint arXiv:2402.00045},
  year={2024}
}
@misc{dugan2024raid,
      title={RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors}, 
      author={Liam Dugan and Alyssa Hwang and Filip Trhlik and Josh Magnus Ludan and Andrew Zhu and Hainiu Xu and Daphne Ippolito and Chris Callison-Burch},
      year={2024},
      eprint={2405.07940},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{
nicks2024language,
title={Language Model Detectors Are Easily Optimized Against},
author={Charlotte Nicks and Eric Mitchell and Rafael Rafailov and Archit Sharma and Christopher D Manning and Chelsea Finn and Stefano Ermon},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=4eJDMjYZZG}
}



@misc{deng2023efficient,
      title={Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model}, 
      author={Zhijie Deng and Hongcheng Gao and Yibo Miao and Hao Zhang},
      year={2023},
      eprint={2305.16617},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{venkatraman2024gptwho,
      title={GPT-who: An Information Density-based Machine-Generated Text Detector}, 
      author={Saranya Venkatraman and Adaku Uchendu and Dongwon Lee},
      year={2024},
      eprint={2310.06202},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{su2023detectllm,
      title={DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text}, 
      author={Jinyan Su and Terry Yue Zhuo and Di Wang and Preslav Nakov},
      year={2023},
      eprint={2306.05540},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@INPROCEEDINGS{sason2013entropy,
  author={Sason, Igal},
  booktitle={2013 IEEE International Symposium on Information Theory}, 
  title={Entropy bounds for discrete random variables via coupling}, 
  year={2013},
  volume={},
  number={},
  pages={414-418},
  keywords={Entropy;Random variables;Couplings;Digital TV;Information theory;Upper bound;Approximation methods;Entropy;local distance;maximal coupling;Poisson approximation;Stein's method;total variation distance},
  doi={10.1109/ISIT.2013.6620259}}

@article{yang2023code,
  title={Zero-shot detection of machine-generated codes},
  author={Yang, Xianjun and Zhang, Kexun and Chen, Haifeng and Petzold, Linda and Wang, William Yang and Cheng, Wei},
  journal={arXiv preprint arXiv:2310.05103},
  year={2023}
}

@article{hendrycks2021measuring,
  title={Measuring coding challenge competence with apps},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and others},
  journal={arXiv preprint arXiv:2105.09938},
  year={2021}
}
@misc{mireshghallah2024smaller,
      title={Smaller Language Models are Better Black-box Machine-Generated Text Detectors}, 
      author={Niloofar Mireshghallah and Justus Mattern and Sicun Gao and Reza Shokri and Taylor Berg-Kirkpatrick},
      year={2024},
      eprint={2305.09859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@InProceedings{ju22robust,
  title = 	 {Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees},
  author =       {Ju, Haotian and Li, Dongyue and Zhang, Hongyang R},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {10431--10461},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
}
@misc{krishna2023paraphrasing,
      title={Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense}, 
      author={Kalpesh Krishna and Yixiao Song and Marzena Karpinska and John Wieting and Mohit Iyyer},
      year={2023},
      eprint={2303.13408},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{hans2024spotting,
      title={Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text}, 
      author={Abhimanyu Hans and Avi Schwarzschild and Valeriia Cherepanova and Hamid Kazemi and Aniruddha Saha and Micah Goldblum and Jonas Geiping and Tom Goldstein},
      year={2024},
      eprint={2401.12070},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{mao2024raidar,
      title={Raidar: geneRative AI Detection viA Rewriting}, 
      author={Chengzhi Mao and Carl Vondrick and Hao Wang and Junfeng Yang},
      year={2024},
      eprint={2401.12970},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shi2024words,
      title={Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling}, 
      author={Yuhui Shi and Qiang Sheng and Juan Cao and Hao Mi and Beizhe Hu and Danding Wang},
      year={2024},
      eprint={2402.09199},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{DBLP:conf/icml/NaeemOUCY20,
  author       = {Muhammad Ferjad Naeem and
                  Seong Joon Oh and
                  Youngjung Uh and
                  Yunjey Choi and
                  Jaejun Yoo},
  title        = {Reliable Fidelity and Diversity Metrics for Generative Models},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {7176--7185},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/naeem20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/NaeemOUCY20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Llama3, 
    author = {Meta},
    title = {{M}eta {M}odels - {LlaMa}3},
    publisher = {Meta},
    url       = {https://ai.meta.com/blog/meta-llama-3/}, 
    year      = 2024,
} 

@misc{llama3.1,
  title         = {Llama 3.1: Open Foundation and Fine-Tuned Chat Models},
  author        = {Meta},
  year          = {2024},
  url  = {https://ai.meta.com/blog/meta-llama-3-1/},
  note          = {Accessed: 2024-10-27}
}
@misc{claude3_Data, 
    author = {Sao10K},
    title = {{C}laude-3-{O}pus-{I}nstruct-15{K}},
    publisher = {Sao10K},
    url       = {https://huggingface.co/datasets/Sao10K/Claude-3-Opus-Instruct-15K}, 
    year      = 2024,
} 

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{m2022exploring,
  title={Exploring the role of artificial intelligence in enhancing academic performance: A case study of ChatGPT},
  author={M Alshater, Muneer},
  journal={Available at SSRN 4312358},
  year={2022}
}
@article{ahmed2021detectingfakenews,
  title={Detecting fake news using machine learning: A systematic literature review},
  author={Ahmed, Alim Al Ayub and Aljabouh, Ayman and Donepudi, Praveen Kumar and Choi, Myung Suh},
  journal={arXiv preprint arXiv:2102.04458},
  year={2021}
}
@inproceedings{adelani2020generatingreviews,
  title={Generating sentiment-preserving fake online reviews using neural language models and their human-and machine-based detection},
  author={Adelani, David Ifeoluwa and Mai, Haotian and Fang, Fuming and Nguyen, Huy H and Yamagishi, Junichi and Echizen, Isao},
  booktitle={Advanced information networking and applications: Proceedings of the 34th international conference on advanced information networking and applications (AINA-2020)},
  pages={1341--1354},
  year={2020},
  organization={Springer}
}

@inproceedings{lee2023languageplagiarize,
  title={Do language models plagiarize?},
  author={Lee, Jooyoung and Le, Thai and Chen, Jinghui and Lee, Dongwon},
  booktitle={Proceedings of the ACM Web Conference 2023},
  pages={3637--3647},
  year={2023}
}

@article{gehrmann2019gltr,
  title={GLTR: Statistical Detection and Visualization of Generated Text},
  author={Gehrmann, Sebastian and Harvard, SEAS and Strobelt, Hendrik and Rush, Alexander M},
  journal={ACL 2019},
  pages={111},
  year={2019}
}

@article{li2023deepfake,
  title={Deepfake Text Detection in the Wild},
  author={Li, Yafu and Li, Qintong and Cui, Leyang and Bi, Wei and Wang, Longyue and Yang, Linyi and Shi, Shuming and Zhang, Yue},
  journal={arXiv preprint arXiv:2305.13242},
  year={2023}
}

@misc{wang2023m4,
      title={M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection}, 
      author={Yuxia Wang and Jonibek Mansurov and Petar Ivanov and Jinyan Su and Artem Shelmanov and Akim Tsvigun and Chenxi Whitehouse and Osama Mohammed Afzal and Tarek Mahmoud and Alham Fikri Aji and Preslav Nakov},
      year={2023},
      eprint={2305.14902},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2023llmdet,
      title={LLMDet: A Large Language Models Detection Tool}, 
      author={Kangxi Wu and Liang Pang and Huawei Shen and Xueqi Cheng and Tat-Seng Chua},
      year={2023},
      eprint={2305.15004},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{verma2023ghostbuster,
      title={Ghostbuster: Detecting Text Ghostwritten by Large Language Models}, 
      author={Vivek Verma and Eve Fleisig and Nicholas Tomlin and Dan Klein},
      year={2023},
      eprint={2305.15047},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yu2023gpt,
  title={GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance},
  author={Yu, Xiao and Qi, Yuang and Chen, Kejiang and Chen, Guoqiang and Yang, Xi and Zhu, Pengyuan and Zhang, Weiming and Yu, Nenghai},
  journal={arXiv preprint arXiv:2305.12519},
  year={2023}
}


@article{zhan2023g3detector,
  title={G3Detector: General GPT-Generated Text Detector},
  author={Zhan, Haolan and He, Xuanli and Xu, Qiongkai and Wu, Yuxiang and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2305.12680},
  year={2023}
}

@inproceedings{badaskar2008identifying,
  title={Identifying real or fake articles: Towards better language modeling},
  author={Badaskar, Sameer and Agarwal, Sachin and Arora, Shilpa},
  booktitle={Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-II},
  year={2008}
}

@article{wang2023bot,
  title={Bot or Human? Detecting ChatGPT Imposters with A Single Question},
  author={Wang, Hong and Luo, Xuan and Wang, Weizhi and Yan, Xifeng},
  journal={arXiv preprint arXiv:2305.06424},
  year={2023}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}
@inproceedings{grivas2022low,
  title={Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice},
  author={Grivas, Andreas and Bogoychev, Nikolay and Lopez, Adam},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6738--6758},
  year={2022}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{stokel2022ai,
  title={AI bot ChatGPT writes smart essays-should academics worry?},
  author={Stokel-Walker, Chris},
  journal={Nature},
  year={2022}
}

@article{mireshghallah2023smaller,
  title={Smaller Language Models are Better Black-box Machine-Generated Text Detectors},
  author={Mireshghallah, Fatemehsadat and Mattern, Justus and Gao, Sicun and Shokri, Reza and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2305.09859},
  year={2023}
}

@inproceedings{fan-etal-2019-eli5,
    title = "{ELI}5: Long Form Question Answering",
    author = "Fan, Angela  and
      Jernite, Yacine  and
      Perez, Ethan  and
      Grangier, David  and
      Weston, Jason  and
      Auli, Michael",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1346",
    doi = "10.18653/v1/P19-1346",
    pages = "3558--3567",
    abstract = "We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum {``}Explain Like I{'}m Five{''} (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline.However, our best model is still far from human performance since raters prefer gold responses in over 86{\%} of cases, leaving ample opportunity for future improvement.",
}


@article{solaiman2019release,
  title={Release strategies and the social impacts of language models},
  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},
  journal={arXiv preprint arXiv:1908.09203},
  year={2019}
}

@article{wang2022squality,
  title={Squality: Building a long-document summarization dataset the hard way},
  author={Wang, Alex and Pang, Richard Yuanzhe and Chen, Angelica and Phang, Jason and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2205.11465},
  year={2022}
}

 

@article{liang2023gpt,
  title={GPT detectors are biased against non-native English writers},
  author={Liang, Weixin and Yuksekgonul, Mert and Mao, Yining and Wu, Eric and Zou, James},
  journal={arXiv preprint arXiv:2304.02819},
  year={2023}
}

 

@article{grechnikov2009detection,
  title={Detection of artificial texts},
  author={Grechnikov, EA and Gusev, GG and Kustarev, AA and Raigorodsky, AM},
  journal={RCDL2009 Proceedings. Petrozavodsk},
  pages={306--308},
  year={2009}
}

@inproceedings{jin-etal-2019-pubmedqa,
    title = "{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering",
    author = "Jin, Qiao  and
      Dhingra, Bhuwan  and
      Liu, Zhengping  and
      Cohen, William  and
      Lu, Xinghua",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    year = "2019",
    pages = "2567--2577",
}


@book{le2012asymptotic,
  title={Asymptotic methods in statistical decision theory},
  author={Le Cam, Lucien},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2019",
    pages = "4171--4186",
}


@inproceedings{gehrmann-etal-2019-gltr,
    title = "{GLTR}: Statistical Detection and Visualization of Generated Text",
    author = "Gehrmann, Sebastian  and
      Strobelt, Hendrik  and
      Rush, Alexander",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    year = "2019",
    pages = "111--116",
}

@inproceedings{ippolito-etal-2020-automatic,
    title = "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    author = "Ippolito, Daphne  and
      Duckworth, Daniel  and
      Callison-Burch, Chris  and
      Eck, Douglas",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    pages = "1808--1822",
}


@inproceedings{holtzmancurious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year = "2020",
}

@misc{wasserman2013,
  author = {Wasserman, Larry},
  title = {Lecture Notes for STAT 705: Advanced Data Analysis},
  year = {2013},
  howpublished = {\url{https://www.stat.cmu.edu/~larry/=stat705/Lecture27.pdf}},
  note = {Accessed on April 9, 2023}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}


@article{black2022gpt,
  title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={Challenges \& Perspectives in Creating Large Language Models},
  pages={95},
  year={2022}
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Leo, Gao and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  note         = {{If you use this software, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}
@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{li2023origin,
  title={Origin Tracing and Detecting of LLMs},
  author={Li, Linyang and Wang, Pengyu and Ren, Ke and Sun, Tianxiang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2304.14072},
  year={2023}
}

@article{yu2023bag,
  title={Bag of tricks for training data extraction from language models},
  author={Yu, Weichen and Pang, Tianyu and Liu, Qian and Du, Chao and Kang, Bingyi and Huang, Yan and Lin, Min and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2302.04460},
  year={2023}
}

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}


@inproceedings{bojar2016findings,
  title={Findings of the 2016 conference on machine translation},
  author={Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and others},
  booktitle={Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
  pages={131--198},
  year={2016}
}

@article{Else2023AbstractsWB,
  title={Abstracts written by ChatGPT fool scientists},
  author={Holly Else},
  journal={Nature},
  year={2023},
  volume={613},
  pages={423 - 423}
}

@article{gao2022comparing,
  title={Comparing scientific abstracts generated by ChatGPT to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers},
  author={Gao, Catherine A and Howard, Frederick M and Markov, Nikolay S and Dyer, Emma C and Ramesh, Siddhi and Luo, Yuan and Pearson, Alexander T},
  journal={bioRxiv},
  pages={2022--12},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@misc{schulman2022chatgpt,
  title={ChatGPT: Optimizing language models for dialogue},
  author={Schulman, J and Zoph, B and Kim, C and Hilton, J and Menick, J and Weng, J and Uribe, JFC and Fedus, L and Metz, L and Pokorny, M and others},
  year={2022},
  publisher={OpenAI. com. https://openai. com/blog/chatgpt}
}

@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models.},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security Symposium},
  volume={6},
  year={2021}
}

@inproceedings{narayan-etal-2018-dont,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    year = "2018",
    pages = "1797--1807",
}


@inproceedings{meritypointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{sadasivan2023can,
  title={Can AI-Generated Text be Reliably Detected?},
  author={Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
  journal={arXiv preprint arXiv:2303.11156},
  year={2023}
}

@article{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}

@inproceedings{jawahar2020automatic,
  title={Automatic Detection of Machine Generated Text: A Critical Survey},
  author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Laks Lakshmanan, VS},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={2296--2309},
  year={2020}
}

@inproceedings{meister2022high,
  title={High probability or low information? The probability--quality paradox in language generation},
  author={Meister, Clara and Wiher, Gian and Pimentel, Tiago and Cotterell, Ryan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={36--45},
  year={2022}
}

@misc{GPTZero, 
    author = {Tian, Edward},
    url       = {https://gptzero.me/}, 
    year      = 2023,
    title     = {GPTZero: An AI Text Detector}
} 


@article{dugan2022real,
  title={Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text},
  author={Dugan, Liam and Ippolito, Daphne and Kirubarajan, Arun and Shi, Sherry and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:2212.12672},
  year={2022}
}

@article{mitchell2023detectgpt,
  title={Detectgpt: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2301.11305},
  year={2023}
}

@article{zhang2021trading,
  title={Trading Off Diversity and Quality in Natural Language Generation},
  author={Zhang, Hugh and Duckworth, Daniel and Ippolito, Daphne and Neelakantan, Arvind},
  journal={EACL 2021},
  pages={25},
  year={2021}
}

@article{chakraborty2023possibilities,
  title={On the Possibilities of AI-Generated Text Detection},
  author={Chakraborty, Souradip and Bedi, Amrit Singh and Zhu, Sicheng and An, Bang and Manocha, Dinesh and Huang, Furong},
  journal={arXiv preprint arXiv:2304.04736},
  year={2023}
}


@misc{polyanskiy2022information,
  title={Information Theory: From Coding to Learning},
  author={Polyanskiy, Yury and Wu, Yihong},
  year={2022},
  publisher={Cambridge University Press}
}

 @article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4816--4828},
  year={2021}
}


@article{chen2023gpt,
  title={GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content},
  author={Chen, Yutian and Kang, Hao and Zhai, Vivian and Li, Liangze and Singh, Rita and Ramakrishnan, Bhiksha},
  journal={arXiv preprint arXiv:2305.07969},
  year={2023}
}

@misc{GPT35, 
    author = {OpenAI},
    title = {{O}pen{AI} {M}odels - {GPT}3.5},
    publisher = {OpenAI},
    url       = {https://platform.openai.com/docs/models/gpt-3-5}, 
    year      = 2022,
} 

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@misc{AITextClassifier, 
    author = {OpenAI},
    title = {{AI} Text Classifier},
    month     = {Jan},
    day       = 31,
    journal   = {AI Text Classifier},
    publisher = {OpenAI},
    url       = {https://beta.openai.com/ai-text-classifier}, 
    year      = 2023,
} 


@misc{Bard, 
    author = {Google},
    title = {Bard, Google's experimental conversational AI service powered by LaMDA},
    publisher = {Google},
    url       = {https://blog.google/technology/ai/bard-google-ai-search-updates/}, 
    year      = 2023,
} 
@misc{mitrović2023chatgpt,
      title={ChatGPT or Human? Detect and Explain. Explaining Decisions of Machine Learning Model for Detecting Short ChatGPT-generated Text}, 
      author={Sandra Mitrović and Davide Andreoletti and Omran Ayoub},
      year={2023},
      eprint={2301.13852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{yang2023dna,
  title={Dna-gpt: Divergent n-gram analysis for training-free detection of gpt-generated text},
  author={Yang, Xianjun and Cheng, Wei and Petzold, Linda and Wang, William Yang and Chen, Haifeng},
  journal={arXiv preprint arXiv:2305.17359},
  year={2023}
}
@article{bao2023fast,
  title={Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature},
  author={Bao, Guangsheng and Zhao, Yanbin and Teng, Zhiyang and Yang, Linyi and Zhang, Yue},
  journal={arXiv preprint arXiv:2310.05130},
  year={2023}
}

@article{yang2023zero,
  title={Zero-shot detection of machine-generated codes},
  author={Yang, Xianjun and Zhang, Kexun and Chen, Haifeng and Petzold, Linda and Wang, William Yang and Cheng, Wei},
  journal={arXiv preprint arXiv:2310.05103},
  year={2023}
}

@article{yang2023survey,
  title={A Survey on Detection of LLMs-Generated Content},
  author={Yang, Xianjun and Pan, Liangming and Zhao, Xuandong and Chen, Haifeng and Petzold, Linda and Wang, William Yang and Cheng, Wei},
  journal={arXiv preprint arXiv:2310.15654},
  year={2023}
}


@article{zhao2024wildchat,
  title={WildChat: 1M ChatGPT Interaction Logs in the Wild},
  author={Zhao, Wenting and Ren, Xiang and Hessel, Jack and Cardie, Claire and Choi, Yejin and Deng, Yuntian},
  journal={arXiv preprint arXiv:2405.01470},
  year={2024}
}


@article{chen2023gptsentinel,
  title={Gpt-sentinel: Distinguishing human and chatgpt generated content},
  author={Chen, Yutian and Kang, Hao and Zhai, Vivian and Li, Liangze and Singh, Rita and Raj, Bhiksha},
  journal={arXiv preprint arXiv:2305.07969},
  year={2023}
}


@misc{claude, 
    author = {Anthropic},
    title = {{A}nthropic {M}odels - {C}laude3},
    publisher = {Anthropic},
    url       = {https://https://www.anthropic.com/claude}, 
    year      = 2024,
} 



@misc{guo2023authentigpt,
      title={AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising}, 
      author={Zhen Guo and Shangdi Yu},
      year={2023},
      eprint={2311.07700},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{chen2023token,
  title={Token Prediction as Implicit Classification to Identify LLM-Generated Text},
  author={Chen, Yutian and Kang, Hao and Zhai, Vivian and Li, Liangze and Singh, Rita and Raj, Bhiksha},
  journal={arXiv preprint arXiv:2311.08723},
  year={2023}
}

@misc{soto2024fewshot,
      title={Few-Shot Detection of Machine-Generated Text using Style Representations}, 
      author={Rafael Rivera Soto and Kailin Koch and Aleem Khan and Barry Chen and Marcus Bishop and Nicholas Andrews},
      year={2024},
      eprint={2401.06712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhang2024detecting,
  title={Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy},
  author={Zhang, Shuhai and Liu, Feng and Yang, Jiahao and Yang, Yifan and Li, Changsheng and Han, Bo and Tan, Mingkui},
  journal={arXiv preprint arXiv:2402.16041},
  year={2024}
}
@article{bakhtin2019real,
  title={Real or fake? learning to discriminate machine from human generated text},
  author={Bakhtin, Anton and Gross, Sam and Ott, Myle and Deng, Yuntian and Ranzato, Marc'Aurelio and Szlam, Arthur},
  journal={arXiv preprint arXiv:1906.03351},
  year={2019}
}



@article{wang2023seqxgpt,
  title={SeqXGPT: Sentence-Level AI-Generated Text Detection},
  author={Wang, Pengyu and Li, Linyang and Ren, Ke and Jiang, Botian and Zhang, Dong and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2310.08903},
  year={2023}
}





% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").



@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}



@article{christ2023undetectable,
  title={Undetectable Watermarks for Language Models},
  author={Christ, Miranda and Gunn, Sam and Zamir, Or},
  journal={Cryptology ePrint Archive},
  year={2023}
}

@article{lee2023wrote,
  title={Who Wrote this Code? Watermarking for Code Generation},
  author={Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee},
  journal={arXiv preprint arXiv:2305.15060},
  year={2023}
}



@article{lavergne2008detecting,
  title={Detecting Fake Content with Relative Entropy Scoring.},
  author={Lavergne, Thomas and Urvoy, Tanguy and Yvon, Fran{\c{c}}ois},
  journal={PAN},
  volume={8},
  pages={27--31},
  year={2008}
}



@article{bhagat2013paraphrase,
  title={What is a paraphrase?},
  author={Bhagat, Rahul and Hovy, Eduard},
  journal={Computational Linguistics},
  volume={39},
  number={3},
  pages={463--472},
  year={2013},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}



@inproceedings{beresneva2016computer,
  title={Computer-generated text detection using machine learning: A systematic review},
  author={Beresneva, Daria},
  booktitle={Natural Language Processing and Information Systems: 21st International Conference on Applications of Natural Language to Information Systems, NLDB 2016, Salford, UK, June 22-24, 2016, Proceedings 21},
  pages={421--426},
  year={2016},
  organization={Springer}
}

@article{grinbaum2022ethical,
  title={The Ethical Need for Watermarks in Machine-Generated Language},
  author={Grinbaum, Alexei and Adomaitis, Laurynas},
  journal={arXiv preprint arXiv:2209.03118},
  year={2022}
}

@article{meral2009natural,
  title={Natural language watermarking via morphosyntactic alterations},
  author={Meral, Hasan Mesut and Sankur, B{\"u}lent and {\"O}zsoy, A Sumru and G{\"u}ng{\"o}r, Tunga and Sevin{\c{c}}, Emre},
  journal={Computer Speech \& Language},
  volume={23},
  number={1},
  pages={107--125},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{topkara2005natural,
  title={Natural language watermarking},
  author={Topkara, Mercan and Taskiran, Cuneyt M and Delp III, Edward J},
  booktitle={Security, Steganography, and Watermarking of Multimedia Contents VII},
  volume={5681},
  pages={441--452},
  year={2005},
  organization={SPIE}
}

@article{rizzo2019fine,
  title={Fine-grain watermarking for intellectual property protection},
  author={Rizzo, Stefano Giovanni and Bertini, Flavio and Montesi, Danilo},
  journal={EURASIP Journal on Information Security},
  volume={2019},
  pages={1--20},
  year={2019},
  publisher={Springer}
}

@inproceedings{topkara2006hiding,
  title={The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions},
  author={Topkara, Umut and Topkara, Mercan and Atallah, Mikhail J},
  booktitle={Proceedings of the 8th workshop on Multimedia and security},
  pages={164--174},
  year={2006}
}

@inproceedings{atallah2003natural,
  title={Natural language watermarking and tamperproofing},
  author={Atallah, Mikhail J and Raskin, Victor and Hempelmann, Christian F and Karahan, Mercan and Sion, Radu and Topkara, Umut and Triezenberg, Katrina E},
  booktitle={Information Hiding: 5th International Workshop, IH 2002 Noordwijkerhout, The Netherlands, October 7-9, 2002 Revised Papers 5},
  pages={196--212},
  year={2003},
  organization={Springer}
}

@article{yang2023watermarking,
  title={Watermarking Text Generated by Black-Box Language Models},
  author={Yang, Xi and Chen, Kejiang and Zhang, Weiming and Liu, Chang and Qi, Yuang and Zhang, Jie and Fang, Han and Yu, Nenghai},
  journal={arXiv preprint arXiv:2305.08883},
  year={2023}
}

@inproceedings{yang2022tracing,
  title={Tracing text provenance via context-aware lexical substitution},
  author={Yang, Xi and Zhang, Jie and Chen, Kejiang and Zhang, Weiming and Ma, Zehua and Wang, Feng and Yu, Nenghai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={11613--11621},
  year={2022}
}

@article{zhao2023recipe,
  title={A recipe for watermarking diffusion models},
  author={Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Cheung, Ngai-Man and Lin, Min},
  journal={arXiv preprint arXiv:2303.10137},
  year={2023}
}

@article{wen2023tree,
  title={Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust},
  author={Wen, Yuxin and Kirchenbauer, John and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2305.20030},
  year={2023}
}

@article{yoo2023robust,
  title={Robust natural language watermarking through invariant features},
  author={Yoo, KiYoon and Ahn, Wonhyuk and Jang, Jiho and Kwak, Nojun},
  journal={arXiv preprint arXiv:2305.01904},
  year={2023}
}



@inproceedings{DBLP:conf/icml/NaeemOUCY20,
  author       = {Muhammad Ferjad Naeem and
                  Seong Joon Oh and
                  Youngjung Uh and
                  Yunjey Choi and
                  Jaejun Yoo},
  title        = {Reliable Fidelity and Diversity Metrics for Generative Models},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {7176--7185},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/naeem20a.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/NaeemOUCY20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{li2023deepfake,
  title={Deepfake Text Detection in the Wild},
  author={Li, Yafu and Li, Qintong and Cui, Leyang and Bi, Wei and Wang, Longyue and Yang, Linyi and Shi, Shuming and Zhang, Yue},
  journal={arXiv preprint arXiv:2305.13242},
  year={2023}
}

@article{desaire2023chatgpt,
  title={ChatGPT or academic scientist? Distinguishing authorship with over 99\% accuracy using off-the-shelf machine learning tools},
  author={Desaire, Heather and Chua, Aleesa E and Isom, Madeline and Jarosova, Romana and Hua, David},
  journal={arXiv preprint arXiv:2303.16352},
  year={2023}
}

@article{tang2023baselines,
  title={Baselines for Identifying Watermarked Large Language Models},
  author={Tang, Leonard and Uberti, Gavin and Shlomi, Tom},
  journal={arXiv preprint arXiv:2305.18456},
  year={2023}
}



@article{he2023mgtbench,
  title={Mgtbench: Benchmarking machine-generated text detection},
  author={He, Xinlei and Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2303.14822},
  year={2023}
}

@article{sison2023chatgpt,
  title={ChatGPT: More than a Weapon of Mass Deception, Ethical challenges and responses from the Human-Centered Artificial Intelligence (HCAI) perspective},
  author={Sison, Alejo Jose G and Daza, Marco Tulio and Gozalo-Brizuela, Roberto and Garrido-Merch{\'a}n, Eduardo C},
  journal={arXiv preprint arXiv:2304.11215},
  year={2023}
}

@article{hacker2023regulating,
  title={Regulating ChatGPT and other large generative AI models},
  author={Hacker, Philipp and Engel, Andreas and Mauer, Marco},
  journal={arXiv preprint arXiv:2302.02337},
  year={2023}
}

@article{vasilatos2023howkgpt,
  title={HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis},
  author={Vasilatos, Christoforos and Alam, Manaar and Rahwan, Talal and Zaki, Yasir and Maniatakos, Michail},
  journal={arXiv preprint arXiv:2305.18226},
  year={2023}
}

@article{perkins2023game,
  title={Game of Tones: Faculty detection of GPT-4 generated content in university assessments},
  author={Perkins, Mike and Roe, Jasper and Postma, Darius and McGaughran, James and Hickerson, Don},
  journal={arXiv preprint arXiv:2305.18081},
  year={2023}
}

@article{jiang2023evading,
  title={Evading Watermark based Detection of AI-Generated Content},
  author={Jiang, Zhengyuan and Zhang, Jinghuai and Gong, Neil Zhenqiang},
  journal={arXiv preprint arXiv:2305.03807},
  year={2023}
}

@article{pan2023risk,
  title={On the Risk of Misinformation Pollution with Large Language Models},
  author={Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.13661},
  year={2023}
}

@article{wang2023evaluating,
  title={Evaluating AIGC Detectors on Code Content},
  author={Wang, Jian and Liu, Shangqing and Xie, Xiaofei and Li, Yi},
  journal={arXiv preprint arXiv:2304.05193},
  year={2023}
}

@article{hanley2023machine,
  title={Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites},
  author={Hanley, Hans WA and Durumeric, Zakir},
  journal={arXiv preprint arXiv:2305.09820},
  year={2023}
}

@inproceedings{maia201818,
  title={Www'18 open challenge: financial opinion mining and question answering},
  author={Maia, Macedo and Handschuh, Siegfried and Freitas, Andr{\'e} and Davis, Brian and McDermott, Ross and Zarrouk, Manel and Balahur, Alexandra},
  booktitle={Companion proceedings of the the web conference 2018},
  pages={1941--1942},
  year={2018}
}

@article{schweinhart2021persistent,
  title={Persistent homology and the upper box dimension},
  author={Schweinhart, Benjamin},
  journal={Discrete \& Computational Geometry},
  volume={65},
  number={2},
  pages={331--364},
  year={2021},
  publisher={Springer}
}

@misc{tulchinskii2023intrinsic,
      title={Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts}, 
      author={Eduard Tulchinskii and Kristian Kuznetsov and Laida Kushnareva and Daniil Cherniavskii and Serguei Barannikov and Irina Piontkovskaya and Sergey Nikolenko and Evgeny Burnaev},
      year={2023},
      eprint={2306.04723},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kirchenbauer2023reliability,
      title={On the Reliability of Watermarks for Large Language Models}, 
      author={John Kirchenbauer and Jonas Geiping and Yuxin Wen and Manli Shu and Khalid Saifullah and Kezhi Kong and Kasun Fernando and Aniruddha Saha and Micah Goldblum and Tom Goldstein},
      year={2023},
      eprint={2306.04634},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{chen2020meddialog,
  title={Meddialog: a large-scale medical dialogue dataset},
  author={Chen, Shu and Ju, Zeqian and Dong, Xiangyu and Fang, Hongchao and Wang, Sicheng and Yang, Yue and Zeng, Jiaqi and Zhang, Ruisi and Zhang, Ruoyu and Zhou, Meng and others},
  journal={arXiv preprint arXiv:2004.03329},
  year={2020}
}

@article{guo2023close,
  title={How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection},
  author={Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},
  journal={arXiv preprint arXiv:2301.07597},
  year={2023}
}

@article{fagni2021tweepfake,
  title={TweepFake: About detecting deepfake tweets},
  author={Fagni, Tiziano and Falchi, Fabrizio and Gambini, Margherita and Martella, Antonio and Tesconi, Maurizio},
  journal={Plos one},
  volume={16},
  number={5},
  pages={e0251415},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{tian2023multiscale,
  title={Multiscale Positive-Unlabeled Detection of AI-Generated Texts},
  author={Tian, Yuchuan and Chen, Hanting and Wang, Xutao and Bai, Zheyuan and Zhang, Qinghua and Li, Ruifeng and Xu, Chao and Wang, Yunhe},
  journal={arXiv preprint arXiv:2305.18149},
  year={2023}
}

@article{liu2023argugpt,
  title={ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models},
  author={Liu, Yikang and Zhang, Ziyin and Zhang, Wanyang and Yue, Shisen and Zhao, Xiaojing and Cheng, Xinyuan and Zhang, Yiwen and Hu, Hai},
  journal={arXiv preprint arXiv:2304.07666},
  year={2023}
}

@article{weng2023towards,
  title={Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection},
  author={Weng, Luoxuan and Zhu, Minfeng and Wong, Kam Kwai and Liu, Shi and Sun, Jiashun and Zhu, Hang and Han, Dongming and Chen, Wei},
  journal={arXiv preprint arXiv:2304.05011},
  year={2023}
}



@article{zan2022neural,
  title={When Neural Model Meets NL2Code: A Survey},
  author={Zan, Daoguang and Chen, Bei and Zhang, Fengji and Lu, Dianjie and Wu, Bingchao and Guan, Bei and Wang, Yongji and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2212.09420},
  year={2022}
}

@misc{wang2023m4,
      title={M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection}, 
      author={Yuxia Wang and Jonibek Mansurov and Petar Ivanov and Jinyan Su and Artem Shelmanov and Akim Tsvigun and Chenxi Whitehouse and Osama Mohammed Afzal and Tarek Mahmoud and Alham Fikri Aji and Preslav Nakov},
      year={2023},
      eprint={2305.14902},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}








@article{wang2023bot,
  title={Bot or Human? Detecting ChatGPT Imposters with A Single Question},
  author={Wang, Hong and Luo, Xuan and Wang, Weizhi and Yan, Xifeng},
  journal={arXiv preprint arXiv:2305.06424},
  year={2023}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@inproceedings{grivas2022low,
  title={Low-Rank Softmax Can Have Unargmaxable Classes in Theory but Rarely in Practice},
  author={Grivas, Andreas and Bogoychev, Nikolay and Lopez, Adam},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6738--6758},
  year={2022}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{deng2020residual,
  title={Residual energy-based models for text generation},
  author={Deng, Yuntian and Bakhtin, Anton and Ott, Myle and Szlam, Arthur and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:2004.11714},
  year={2020}
}

@article{stokel2022ai,
  title={AI bot ChatGPT writes smart essays-should academics worry?},
  author={Stokel-Walker, Chris},
  journal={Nature},
  year={2022}
}



@article{wang2022squality,
  title={Squality: Building a long-document summarization dataset the hard way},
  author={Wang, Alex and Pang, Richard Yuanzhe and Chen, Angelica and Phang, Jason and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2205.11465},
  year={2022}
}

@article{crothers2022machine,
  title={Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods},
  author={Crothers, Evan and Japkowicz, Nathalie and Viktor, Herna},
  journal={arXiv preprint arXiv:2210.07321},
  year={2022}
}


@article{tang2023science,
  title={The science of detecting llm-generated texts},
  author={Tang, Ruixiang and Chuang, Yu-Neng and Hu, Xia},
  journal={arXiv preprint arXiv:2303.07205},
  year={2023}
}

@inproceedings{chew2003baffletext,
  title={Baffletext: A human interactive proof},
  author={Chew, Monica and Baird, Henry S},
  booktitle={Document Recognition and Retrieval X},
  volume={5010},
  pages={305--316},
  year={2003},
  organization={SPIE}
}
 

@inproceedings{atallah2001natural,
  title={Natural language watermarking: Design, analysis, and a proof-of-concept implementation},
  author={Atallah, Mikhail J and Raskin, Victor and Crogan, Michael and Hempelmann, Christian and Kerschbaum, Florian and Mohamed, Dina and Naik, Sanket},
  booktitle={Information Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, April 25--27, 2001 Proceedings 4},
  pages={185--200},
  year={2001},
  organization={Springer}
}

@article{bakhtin2019real,
  title={Real or fake? learning to discriminate machine from human generated text},
  author={Bakhtin, Anton and Gross, Sam and Ott, Myle and Deng, Yuntian and Ranzato, Marc'Aurelio and Szlam, Arthur},
  journal={arXiv preprint arXiv:1906.03351},
  year={2019}
}

@inproceedings{abdelnabi21oakland,
    title = {Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding},
    author = {Sahar Abdelnabi and Mario Fritz},
    booktitle = {42nd IEEE Symposium on Security and Privacy},
    year = {2021}
}

@inproceedings{holtzmancurious,
  title={The Curious Case of Neural Text Degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={International Conference on Learning Representations},
  year = "2020",
}

@misc{wasserman2013,
  author = {Wasserman, Larry},
  title = {Lecture Notes for STAT 705: Advanced Data Analysis},
  year = {2013},
  howpublished = {\url{https://www.stat.cmu.edu/~larry/=stat705/Lecture27.pdf}},
  note = {Accessed on April 9, 2023}
}


@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{wahle2023ai,
  title={Ai usage cards: Responsibly reporting ai-generated content},
  author={Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif M and Meuschke, Norman and Gipp, Bela},
  journal={arXiv preprint arXiv:2303.03886},
  year={2023}
}


@article{black2022gpt,
  title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={Challenges \& Perspectives in Creating Large Language Models},
  pages={95},
  year={2022}
}

@article{lu2023large,
  title={Large Language Models can be Guided to Evade AI-Generated Text Detection},
  author={Lu, Ning and Liu, Shengcai and He, Rui and Tang, Ke},
  journal={arXiv preprint arXiv:2305.10847},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@misc{polyanskiy2022information,
  title={Information Theory: From Coding to Learning},
  author={Polyanskiy, Yury and Wu, Yihong},
  year={2022},
  publisher={Cambridge University Press}
}



@article{zhu2023promptbench,
  title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Gong, Neil Zhenqiang and Zhang, Yue and others},
  journal={arXiv preprint arXiv:2306.04528},
  year={2023}
}

@article{becker2023paraphrase,
  title={Paraphrase Detection: Human vs. Machine Content},
  author={Becker, Jonas and Wahle, Jan Philip and Ruas, Terry and Gipp, Bela},
  journal={arXiv preprint arXiv:2303.13989},
  year={2023}
}

@article{yu2023bag,
  title={Bag of tricks for training data extraction from language models},
  author={Yu, Weichen and Pang, Tianyu and Liu, Qian and Du, Chao and Kang, Bingyi and Huang, Yan and Lin, Min and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2302.04460},
  year={2023}
}


@inproceedings{bojar2016findings,
  title={Findings of the 2016 conference on machine translation},
  author={Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and others},
  booktitle={Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
  pages={131--198},
  year={2016}
}



@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{alzantot2018generating,
  title={Generating natural language adversarial examples},
  author={Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1804.07998},
  year={2018}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@misc{schulman2022chatgpt,
  title={ChatGPT: Optimizing language models for dialogue},
  author={Schulman, J and Zoph, B and Kim, C and Hilton, J and Menick, J and Weng, J and Uribe, JFC and Fedus, L and Metz, L and Pokorny, M and others},
  year={2022},
  publisher={OpenAI. com. https://openai. com/blog/chatgpt}
}

@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models.},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security Symposium},
  volume={6},
  year={2021}
}




@inproceedings{meritypointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017}
}




@inproceedings{jawahar2020automatic,
  title={Automatic Detection of Machine Generated Text: A Critical Survey},
  author={Jawahar, Ganesh and Abdul-Mageed, Muhammad and Laks Lakshmanan, VS},
  booktitle={Proceedings of the 28th International Conference on Computational Linguistics},
  pages={2296--2309},
  year={2020}
}

@inproceedings{meister2022high,
  title={High probability or low information? The probability--quality paradox in language generation},
  author={Meister, Clara and Wiher, Gian and Pimentel, Tiago and Cotterell, Ryan},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={36--45},
  year={2022}
}


@article{dugan2022real,
  title={Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text},
  author={Dugan, Liam and Ippolito, Daphne and Kirubarajan, Arun and Shi, Sherry and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:2212.12672},
  year={2022}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{mireshghallah2023smaller,
  title={Smaller Language Models are Better Black-box Machine-Generated Text Detectors},
  author={Mireshghallah, Fatemehsadat and Mattern, Justus and Gao, Sicun and Shokri, Reza and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2305.09859},
  year={2023}
}

@article{shi2023red,
  title={Red Teaming Language Model Detectors with Language Models},
  author={Shi, Zhouxing and Wang, Yihan and Yin, Fan and Chen, Xiangning and Chang, Kai-Wei and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2305.19713},
  year={2023}
}


@article{zhang2021trading,
  title={Trading Off Diversity and Quality in Natural Language Generation},
  author={Zhang, Hugh and Duckworth, Daniel and Ippolito, Daphne and Neelakantan, Arvind},
  journal={EACL 2021},
  pages={25},
  year={2021}
}


 
 @article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4816--4828},
  year={2021}
}

@article{tu2023chatlog,
  title={ChatLog: Recording and Analyzing ChatGPT Across Time},
  author={Tu, Shangqing and Li, Chunyang and Yu, Jifan and Wang, Xiaozhi and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2304.14106},
  year={2023}
}



@article{chen2023gpt,
  title={GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content},
  author={Chen, Yutian and Kang, Hao and Zhai, Vivian and Li, Liangze and Singh, Rita and Ramakrishnan, Bhiksha},
  journal={arXiv preprint arXiv:2305.07969},
  year={2023}
}

@misc{Gokaslan2019OpenWeb,
  title        = {OpenWebText Corpus},
  author       = {Aaron Gokaslan and Vanya Cohen},
  howpublished = {\url{http://Skylion007.github.io/OpenWebTextCorpus}},
  year         = {2019}
}


@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{singer2022make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{saharia2022photorealistic,
  title={Photorealistic text-to-image diffusion models with deep language understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily L and Ghasemipour, Kamyar and Gontijo Lopes, Raphael and Karagol Ayan, Burcu and Salimans, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36479--36494},
  year={2022}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}



@misc{Bard, 
    author = {Google},
    title = {Bard, Google's experimental conversational AI service powered by LaMDA},
    publisher = {Google},
    url       = {https://blog.google/technology/ai/bard-google-ai-search-updates/}, 
    year      = 2023,
} 

@article{bian2023drop,
  title={A Drop of Ink may Make a Million Think: The Spread of False Information in Large Language Models},
  author={Bian, Ning and Liu, Peilin and Han, Xianpei and Lin, Hongyu and Lu, Yaojie and He, Ben and Sun, Le},
  journal={arXiv preprint arXiv:2305.04812},
  year={2023}
}

@article{oshikawa2018survey,
  title={A survey on natural language processing for fake news detection},
  author={Oshikawa, Ray and Qian, Jing and Wang, William Yang},
  journal={arXiv preprint arXiv:1811.00770},
  year={2018}
}
 

@article{herbold2023ai,
  title={AI, write an essay for me: A large-scale comparison of human-written versus ChatGPT-generated essays},
  author={Herbold, Steffen and Hautli-Janisz, Annette and Heuer, Ute and Kikteva, Zlata and Trautsch, Alexander},
  journal={arXiv preprint arXiv:2304.14276},
  year={2023}
}

@article{ippolito2019automatic,
  title={Automatic detection of generated text is easiest when humans are fooled},
  author={Ippolito, Daphne and Duckworth, Daniel and Callison-Burch, Chris and Eck, Douglas},
  journal={arXiv preprint arXiv:1911.00650},
  year={2019}
}

@article{yu2023cheat,
  title={CHEAT: A Large-scale Dataset for Detecting ChatGPT-writtEn AbsTracts},
  author={Yu, Peipeng and Chen, Jiahan and Feng, Xuan and Xia, Zhihua},
  journal={arXiv preprint arXiv:2304.12008},
  year={2023}
}
 