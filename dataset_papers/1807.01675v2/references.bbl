\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pages 265--283, 2016.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  TB, Muldal, Heess, and Lillicrap]{barth-maron2018distributional}
G.~Barth-Maron, M.~W. Hoffman, D.~Budden, W.~Dabney, D.~Horgan, D.~TB,
  A.~Muldal, N.~Heess, and T.~Lillicrap.
\newblock Distributional policy gradients.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Deisenroth and Rasmussen(2011)]{deisenroth2011pilco}
M.~Deisenroth and C.~E. Rasmussen.
\newblock {PILCO}: A model-based and data-efficient approach to policy search.
\newblock In \emph{Proceedings of the 28th International Conference on machine
  learning (ICML-11)}, pages 465--472, 2011.

\bibitem[Depeweg et~al.(2016)Depeweg, Hern{\'a}ndez-Lobato, Doshi-Velez, and
  Udluft]{depeweg2016learning}
S.~Depeweg, J.~M. Hern{\'a}ndez-Lobato, F.~Doshi-Velez, and S.~Udluft.
\newblock Learning and policy search in stochastic dynamical systems with
  {Bayesian} neural networks.
\newblock 2016.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Feinberg et~al.(2018)Feinberg, Wan, Stoica, Jordan, Gonzalez, and
  Levine]{feinberg2018model}
V.~Feinberg, A.~Wan, I.~Stoica, M.~I. Jordan, J.~E. Gonzalez, and S.~Levine.
\newblock Model-based value estimation for efficient model-free reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.00101}, 2018.

\bibitem[Fleiss(1993)]{fleiss1993review}
J.~Fleiss.
\newblock Review papers: The statistical basis of meta-analysis.
\newblock \emph{Statistical methods in medical research}, 2\penalty0
  (2):\penalty0 121--145, 1993.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{pmlr-v80-fujimoto18a}
S.~Fujimoto, H.~van Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In J.~Dy and A.~Krause, editors, \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pages 1587--1596, Stockholmsm√§ssan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/fujimoto18a.html}.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a {Bayesian} approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pages
  1050--1059, 2016.

\bibitem[Gal et~al.()Gal, McAllister, and Rasmussen]{gal2016improving}
Y.~Gal, R.~McAllister, and C.~E. Rasmussen.
\newblock Improving {PILCO} with {Bayesian} neural network dynamics models.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Sutskever, and Levine]{gu2016continuous}
S.~Gu, T.~Lillicrap, I.~Sutskever, and S.~Levine.
\newblock Continuous deep {Q}-learning with model-based acceleration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2829--2838, 2016.

\bibitem[Gu et~al.(2017)Gu, Lillicrap, Ghahramani, Turner, and Levine]{gu2016q}
S.~Gu, T.~Lillicrap, Z.~Ghahramani, R.~E. Turner, and S.~Levine.
\newblock Q-prop: Sample-efficient policy gradient with an off-policy critic.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor, 2018.

\bibitem[Heess et~al.(2015)Heess, Wayne, Silver, Lillicrap, Erez, and
  Tassa]{heess2015learning}
N.~Heess, G.~Wayne, D.~Silver, T.~Lillicrap, T.~Erez, and Y.~Tassa.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2944--2952, 2015.

\bibitem[Horgan et~al.(2018)Horgan, Quan, Budden, Barth-Maron, Hessel, van
  Hasselt, and Silver]{horgan2018distributed}
D.~Horgan, J.~Quan, D.~Budden, G.~Barth-Maron, M.~Hessel, H.~van Hasselt, and
  D.~Silver.
\newblock Distributed prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kalweit and Boedecker(2017)]{kalweit2017uncertainty}
G.~Kalweit and J.~Boedecker.
\newblock Uncertainty-driven imagination for continuous deep reinforcement
  learning.
\newblock In \emph{Conference on Robot Learning}, pages 195--206, 2017.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Klimov and Schulman()]{roboschool}
O.~Klimov and J.~Schulman.
\newblock Roboschool.
\newblock \url{https://github.com/openai/roboschool}.

\bibitem[Kurutach et~al.(2018)Kurutach, Clavera, Duan, Tamar, and
  Abbeel]{kurutach2018modelensemble}
T.~Kurutach, I.~Clavera, Y.~Duan, A.~Tamar, and P.~Abbeel.
\newblock Model-ensemble trust-region policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[MacKay(1992)]{mackay1992practical}
D.~J. MacKay.
\newblock A practical {Bayesian} framework for backpropagation networks.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock In \emph{NIPS Deep Learning Workshop}. 2013.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
R.~Munos, T.~Stepleton, A.~Harutyunyan, and M.~Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1054--1062, 2016.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
I.~Osband, C.~Blundell, A.~Pritzel, and B.~Van~Roy.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock In \emph{Advances in neural information processing systems}, pages
  4026--4034, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sutton(1990)]{sutton1990integrated}
R.~S. Sutton.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In \emph{Machine Learning Proceedings 1990}, pages 216--224.
  Elsevier, 1990.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}, volume~1.
\newblock MIT Press Cambridge, 1998.

\bibitem[Thomas et~al.(2015)Thomas, Niekum, Theocharous, and
  Konidaris]{thomas2015policy}
P.~S. Thomas, S.~Niekum, G.~Theocharous, and G.~Konidaris.
\newblock Policy evaluation using the {$\Omega$}-return.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  334--342, 2015.

\bibitem[Weber et~al.(2017)Weber, Racani{\`e}re, Reichert, Buesing, Guez,
  Rezende, Badia, Vinyals, Heess, Li, et~al.]{weber2017imagination}
T.~Weber, S.~Racani{\`e}re, D.~P. Reichert, L.~Buesing, A.~Guez, D.~J. Rezende,
  A.~P. Badia, O.~Vinyals, N.~Heess, Y.~Li, et~al.
\newblock Imagination-augmented agents for deep reinforcement learning.
\newblock \emph{31st Conference on Neural Information Processing Systems},
  2017.

\end{thebibliography}
