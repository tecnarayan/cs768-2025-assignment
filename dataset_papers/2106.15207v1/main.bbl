\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2020)Ahn, Yun, and Sra]{ahn2020sgd}
K.~Ahn, C.~Yun, and S.~Sra.
\newblock Sgd with shuffling: optimal rates without component convexity and
  large epoch requirements.
\newblock \emph{arXiv preprint arXiv:2006.06946}, 2020.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n, and
  Talwar]{bassily2020stability}
R.~Bassily, V.~Feldman, C.~Guzm{\'a}n, and K.~Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S.~Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Devroye and Wagner(1979{\natexlab{a}})]{devroye1979distributiona}
L.~Devroye and T.~Wagner.
\newblock Distribution-free inequalities for the deleted and holdout error
  estimates.
\newblock \emph{IEEE Transactions on Information Theory}, 25\penalty0
  (2):\penalty0 202--207, 1979{\natexlab{a}}.

\bibitem[Devroye and Wagner(1979{\natexlab{b}})]{devroye1979distributionb}
L.~Devroye and T.~Wagner.
\newblock Distribution-free performance bounds with the resubstitution error
  estimate (corresp.).
\newblock \emph{IEEE Transactions on Information Theory}, 25\penalty0
  (2):\penalty0 208--210, 1979{\natexlab{b}}.

\bibitem[Feldman and Vondrak(2019)]{feldman2019high}
V.~Feldman and J.~Vondrak.
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In \emph{Conference on Learning Theory}, pages 1270--1279. PMLR,
  2019.

\bibitem[Garber et~al.(2020)Garber, Korcia, and Levy]{garber2020online}
D.~Garber, G.~Korcia, and K.~Levy.
\newblock Online convex optimization in the random order model.
\newblock In \emph{International Conference on Machine Learning}, pages
  3387--3396. PMLR, 2020.

\bibitem[G{\"u}rb{\"u}zbalaban et~al.(2019)G{\"u}rb{\"u}zbalaban, Ozdaglar, and
  Parrilo]{gurbuzbalaban2019random}
M.~G{\"u}rb{\"u}zbalaban, A.~Ozdaglar, and P.~A. Parrilo.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, pages 1--36, 2019.

\bibitem[Haochen and Sra(2019)]{haochen2019random}
J.~Haochen and S.~Sra.
\newblock Random shuffling beats sgd after finite epochs.
\newblock In \emph{International Conference on Machine Learning}, pages
  2624--2633. PMLR, 2019.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  1225--1234. PMLR, 2016.

\bibitem[Hazan(2019)]{hazan2019introduction}
E.~Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{arXiv preprint arXiv:1909.05207}, 2019.

\bibitem[Hazan and Kale(2011)]{hazan2011beyond}
E.~Hazan and S.~Kale.
\newblock Beyond the regret minimization barrier: an optimal algorithm for
  stochastic strongly-convex optimization.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 421--436. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
E.~Hazan, A.~Agarwal, and S.~Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[London(2017)]{london2017pac}
B.~London.
\newblock A pac-bayesian analysis of randomized learning with application to
  stochastic gradient descent.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 2935--2944, 2017.

\bibitem[Nagaraj et~al.(2019)Nagaraj, Jain, and Netrapalli]{nagaraj2019sgd}
D.~Nagaraj, P.~Jain, and P.~Netrapalli.
\newblock Sgd without replacement: Sharper rates for general smooth convex
  functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  4703--4711. PMLR, 2019.

\bibitem[Nguyen et~al.(2020)Nguyen, Tran-Dinh, Phan, Nguyen, and van
  Dijk]{nguyen2020unified}
L.~M. Nguyen, Q.~Tran-Dinh, D.~T. Phan, P.~H. Nguyen, and M.~van Dijk.
\newblock A unified convergence analysis for shuffling-type gradient methods.
\newblock \emph{arXiv preprint arXiv:2002.08246}, 2020.

\bibitem[Rajput et~al.(2020)Rajput, Gupta, and
  Papailiopoulos]{rajput2020closing}
S.~Rajput, A.~Gupta, and D.~Papailiopoulos.
\newblock Closing the convergence gap of sgd without replacement.
\newblock In \emph{International Conference on Machine Learning}, pages
  7964--7973. PMLR, 2020.

\bibitem[Rogers and Wagner(1978)]{rogers1978finite}
W.~H. Rogers and T.~J. Wagner.
\newblock A finite sample distribution-free performance bound for local
  discrimination rules.
\newblock \emph{The Annals of Statistics}, pages 506--514, 1978.

\bibitem[Safran and Shamir(2020)]{safran2020good}
I.~Safran and O.~Shamir.
\newblock How good is sgd with random shuffling?
\newblock In \emph{Conference on Learning Theory}, pages 3250--3284. PMLR,
  2020.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2010learnability}
S.~Shalev-Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2635--2670, 2010.

\bibitem[Shamir(2016)]{shamir2016without}
O.~Shamir.
\newblock Without-replacement sampling for stochastic gradient methods.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pages 46--54, 2016.

\bibitem[Vitter(1985)]{vitter1985random}
J.~S. Vitter.
\newblock Random sampling with a reservoir.
\newblock \emph{ACM Transactions on Mathematical Software (TOMS)}, 11\penalty0
  (1):\penalty0 37--57, 1985.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
M.~Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th international conference on machine
  learning (icml-03)}, pages 928--936, 2003.

\end{thebibliography}
