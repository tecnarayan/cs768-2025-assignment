\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alacaoglu and Malitsky(2022)]{alacaoglu2021stochastic}
Ahmet Alacaoglu and Yura Malitsky.
\newblock Stochastic variance reduction for variational inequality methods.
\newblock In \emph{Proc.~COLT'22}, 2022.

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: {The} first direct acceleration of stochastic gradient
  methods.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8194--8244, 2017.

\bibitem[Arjevani et~al.(2020)Arjevani, Carmon, Duchi, Foster, Sekhari, and
  Sridharan]{arjevani2020second}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Ayush Sekhari, and
  Karthik Sridharan.
\newblock Second-order information in non-convex stochastic optimization: Power
  and limitations.
\newblock In \emph{Proc.~COLT'20}, 2020.

\bibitem[Beck(2017)]{beck2017first}
Amir Beck.
\newblock \emph{First-order methods in optimization}, volume~25.
\newblock SIAM, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Jin, Sidford, and Tian]{carmon2019variance}
Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian.
\newblock Variance reduction for matrix games.
\newblock In \emph{Proc.~NeurIPS'19}, 2019.

\bibitem[Chavdarova et~al.(2019)Chavdarova, Gidel, Fleuret, and
  Lacoste-Julien]{chavdarova2019reducing}
Tatjana Chavdarova, Gauthier Gidel, Fran{\c{c}}ois Fleuret, and Simon
  Lacoste-Julien.
\newblock Reducing noise in {GAN} training with variance reduced extragradient.
\newblock In \emph{Proc.~NeurIPS'19}, 2019.

\bibitem[Chen and Luo(2022)]{chen2022near}
Lesi Chen and Luo Luo.
\newblock Near-optimal algorithms for making the gradient small in stochastic
  minimax optimization.
\newblock \emph{arXiv preprint arXiv:2208.05925}, 2022.

\bibitem[Cheval et~al.(2022)Cheval, Kohlenbach, and
  Leustean]{cheval2022modified}
Horatiu Cheval, Ulrich Kohlenbach, and Laurentiu Leustean.
\newblock On modified {Halpern} and {Tikhonov-Mann} iterations.
\newblock \emph{arXiv preprint arXiv:2203.11003}, 2022.

\bibitem[Contreras and Cominetti(2021)]{Contreras:2021}
Juan~Pablo Contreras and Roberto Cominetti.
\newblock Optimal error bounds for nonexpansive fixed-point iterations in
  normed spaces.
\newblock \emph{arXiv preprint, arXiv:2108.10969}, 2021.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{Proc.~NeurIPS'19}, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: {A} fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Proc.~NeurIPS'14}, 2014.

\bibitem[Diakonikolas(2020)]{diakonikolas2020halpern}
Jelena Diakonikolas.
\newblock Halpern iteration for near-optimal and parameter-free monotone
  inclusion and strong solutions to variational inequalities.
\newblock In \emph{Proc.~COLT'20}, 2020.

\bibitem[Diakonikolas and Wang(2022)]{diakonikolas2021potential}
Jelena Diakonikolas and Puqian Wang.
\newblock Potential function-based framework for minimizing gradients in convex
  and min-max optimization.
\newblock \emph{SIAM Journal on Optimization}, 32\penalty0 (3):\penalty0
  1668--1697, 2022.

\bibitem[Diakonikolas et~al.(2021)Diakonikolas, Daskalakis, and
  Jordan]{diakonikolas2021efficient}
Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan.
\newblock Efficient methods for structured nonconvex-nonconcave min-max
  optimization.
\newblock In \emph{Proc.~AISTATS'21}, 2021.

\bibitem[Dua and Graff(2017)]{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} {Machine} {Learning} {Repository}, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[El~Ghaoui and Lebret(1997)]{el1997robust}
Laurent El~Ghaoui and Herv{\'e} Lebret.
\newblock Robust solutions to least-squares problems with uncertain data.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 18\penalty0
  (4):\penalty0 1035--1064, 1997.

\bibitem[Facchinei and Pang(2003)]{facchinei2007finite}
Francisco Facchinei and Jong-Shi Pang.
\newblock \emph{Finite-dimensional variational inequalities and complementarity
  problems}.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: {Near}-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Proc.~NeurIPS'18}, 2018.

\bibitem[Ghadimi and Lan(2016)]{ghadimi2016accelerated}
Saeed Ghadimi and Guanghui Lan.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1-2):\penalty0 59--99,
  2016.

\bibitem[Golowich et~al.(2020)Golowich, Pattathil, Daskalakis, and
  Ozdaglar]{golowich2020last}
Noah Golowich, Sarath Pattathil, Constantinos Daskalakis, and Asuman Ozdaglar.
\newblock Last iterate is slower than averaged iterate in smooth convex-concave
  saddle point problems.
\newblock In \emph{Proc.~COLT'20}, 2020.

\bibitem[Halpern(1967)]{halpern1967fixed}
Benjamin Halpern.
\newblock Fixed points of nonexpanding maps.
\newblock \emph{Bulletin of the American Mathematical Society}, 73\penalty0
  (6):\penalty0 957--961, 1967.

\bibitem[Hamidieh(2018)]{HAMIDIEH2018346}
Kam Hamidieh.
\newblock A data-driven statistical model for predicting the critical
  temperature of a superconductor.
\newblock \emph{Computational Materials Science}, 154:\penalty0 346--354, 2018.

\bibitem[Iusem et~al.(2017)Iusem, Jofr{\'e}, Oliveira, and
  Thompson]{iusem2017extragradient}
Alfredo~N Iusem, Alejandro Jofr{\'e}, Roberto~Imbuzeiro Oliveira, and Philip
  Thompson.
\newblock Extragradient method with variance reduction for stochastic
  variational inequalities.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (2):\penalty0
  686--724, 2017.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Proc.~NeurIPS'13}, 2013.

\bibitem[Kim(2019)]{kim2019accelerated}
Donghwan Kim.
\newblock Accelerated proximal point method and forward method for monotone
  inclusions.
\newblock \emph{arXiv preprint arXiv:1905.05149}, 2019.

\bibitem[Kohlenbach(2011)]{kohlenbach2011quantitative}
Ulrich Kohlenbach.
\newblock On quantitative versions of theorems due to {F.E.} {Browder} and {R}.
  {Wittmann}.
\newblock \emph{Advances in Mathematics}, 226\penalty0 (3):\penalty0
  2764--2795, 2011.

\bibitem[Kohlenbach and Leu{\c{s}}tean(2012)]{kohlenbach2012effective}
Ulrich Kohlenbach and Lauren{\c{t}}iu Leu{\c{s}}tean.
\newblock Effective metastability of {Halpern} iterates in {CAT}(0) spaces.
\newblock \emph{Advances in Mathematics}, 231\penalty0 (5):\penalty0
  2526--2556, 2012.

\bibitem[Korpelevich(1977)]{korpelevich1977extragradient}
GM~Korpelevich.
\newblock Extragradient method for finding saddle points and other problems.
\newblock \emph{Matekon}, 13\penalty0 (4):\penalty0 35--49, 1977.

\bibitem[Lee and Kim(2021)]{lee2021fast}
Sucheol Lee and Donghwan Kim.
\newblock Fast extra gradient methods for smooth structured
  nonconvex-nonconcave minimax problems.
\newblock In \emph{Proc.~NeurIPS'21}, 2021.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via {SCSG} methods.
\newblock In \emph{Proc.~NeurIPS'17}, 2017.

\bibitem[Leustean(2007)]{leustean2007rates}
Laurentiu Leustean.
\newblock Rates of asymptotic regularity for {Halpern} iterations of
  nonexpansive mappings.
\newblock \emph{Journal of Universal Computer Science}, 13\penalty0
  (11):\penalty0 1680--1691, 2007.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt{\'a}rik]{li2020page}
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt{\'a}rik.
\newblock {PAGE}: {A} simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{Proc.~ICML'21}, 2021.

\bibitem[Lieder(2021)]{lieder2019convergence}
Felix Lieder.
\newblock On the convergence rate of the {Halpern}-iteration.
\newblock \emph{Optimization Letters}, 15\penalty0 (2):\penalty0 405--418,
  2021.

\bibitem[Loizou et~al.(2021)Loizou, Berard, Gidel, Mitliagkas, and
  Lacoste-Julien]{loizou2021stochastic}
Nicolas Loizou, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, and Simon
  Lacoste-Julien.
\newblock Stochastic gradient descent-ascent and consensus optimization for
  smooth games: {Convergence} analysis under expected co-coercivity.
\newblock In \emph{Proc.~NeurIPS'21}, 2021.

\bibitem[Minty(1962)]{minty1962monotone}
George~J Minty.
\newblock Monotone (nonlinear) operators in {Hilbert} space.
\newblock \emph{Duke Mathematical Journal}, 29\penalty0 (3):\penalty0 341--346,
  1962.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence {$O(1/t)$} for variational
  inequalities with {L}ipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Nesterov(2007)]{nesterov2007dual}
Yurii Nesterov.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock \emph{Mathematical Programming}, 109\penalty0 (2-3):\penalty0
  319--344, 2007.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{Proc.~ICML'17}, 2017.

\bibitem[Ouyang and Xu(2019)]{Ouyang2019}
Yuyuan Ouyang and Yangyang Xu.
\newblock Lower complexity bounds of first-order methods for convex-concave
  bilinear saddle-point problems.
\newblock \emph{Mathematical Programming}, Aug 2019.

\bibitem[Palaniappan and Bach(2016)]{palaniappan2016stochastic}
Balamurugan Palaniappan and Francis Bach.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock In \emph{Proc.~NeurIPS'16}, 2016.

\bibitem[Pang(1997)]{pang1997error}
Jong-Shi Pang.
\newblock Error bounds in mathematical programming.
\newblock \emph{Mathematical Programming}, 79\penalty0 (1):\penalty0 299--332,
  1997.

\bibitem[Popov(1980)]{Popov1980}
L.~D. Popov.
\newblock A modification of the {Arrow-Hurwicz} method for search of saddle
  points.
\newblock \emph{Mathematical notes of the Academy of Sciences of the USSR},
  28\penalty0 (5):\penalty0 845--848, Nov 1980.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proc.~ICML'16}, 2016.

\bibitem[Rockafellar(1970)]{rockafellar1970monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators associated with saddle-functions and minimax
  problems.
\newblock \emph{Nonlinear Functional Analysis}, 18\penalty0 (part 1):\penalty0
  397--407, 1970.

\bibitem[Rockafellar(1976)]{rockafellar1976monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock \emph{SIAM Journal on Control and Optimization}, 14\penalty0
  (5):\penalty0 877--898, 1976.

\bibitem[Roulet and d'Aspremont(2020)]{roulet2020sharpness}
Vincent Roulet and Alexandre d'Aspremont.
\newblock Sharpness, restart, and acceleration.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0
  262--289, 2020.

\bibitem[Sabach and Shtern(2017)]{Sabach:2017}
Shoham Sabach and Shimrit Shtern.
\newblock A first order method for solving convex bilevel optimization
  problems.
\newblock \emph{{SIAM} Journal on Optimization}, 27\penalty0 (2):\penalty0
  640--660, 2017.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1):\penalty0 83--112,
  2017.

\bibitem[Song et~al.(2020)Song, Jiang, and Ma]{song2020variance}
Chaobing Song, Yong Jiang, and Yi~Ma.
\newblock Variance reduction via accelerated dual averaging for finite-sum
  optimization.
\newblock In \emph{Proc.~NeurIPS'20}, 2020.

\bibitem[Stampacchia(1964)]{stampacchia1964formes}
Guido Stampacchia.
\newblock Formes bilineaires coercitives sur les ensembles convexes.
\newblock \emph{Acad\'emie des Sciences de Paris}, 258:\penalty0 4413--4416,
  1964.

\bibitem[Tran-Dinh and Luo(2021)]{tran2021halpern}
Quoc Tran-Dinh and Yang Luo.
\newblock Halpern-type accelerated and splitting algorithms for monotone
  inclusions.
\newblock \emph{arXiv preprint arXiv:2110.08150}, 2021.

\bibitem[Wittmann(1992)]{wittmann1992approximation}
Rainer Wittmann.
\newblock Approximation of fixed points of nonexpansive mappings.
\newblock \emph{Archiv der Mathematik}, 58\penalty0 (5):\penalty0 486--491,
  1992.

\bibitem[Yoon and Ryu(2021)]{Yoon2021OptimalGradientNorm}
Taeho Yoon and Ernest~K Ryu.
\newblock Accelerated algorithms for smooth convex-concave minimax problems
  with {$O(1/k^2)$} rate on squared gradient norm.
\newblock In \emph{Proc.~ICML'21}, 2021.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Xu, and Gu]{zhou2018finding}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Finding local minima via stochastic nested variance reduction.
\newblock \emph{arXiv preprint arXiv:1806.08782}, 2018{\natexlab{a}}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Xu, and Gu]{zhou2018stochastic}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock In \emph{Proc.~NeurIPS'18}, 2018{\natexlab{b}}.

\end{thebibliography}
