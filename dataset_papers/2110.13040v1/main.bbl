\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai2019deep}
S.~Bai, J.~Z. Kolter, and V.~Koltun.
\newblock Deep equilibrium models.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{behrmann2019invertible}
J.~Behrmann, W.~Grathwohl, R.~T. Chen, D.~Duvenaud, and J.-H. Jacobsen.
\newblock Invertible residual networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Bengio et~al.(1994)Bengio, Simard, and Frasconi]{bengio1994learning}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock \emph{IEEE transactions on neural networks}, 5\penalty0 (2):\penalty0
  157--166, 1994.

\bibitem[Berg et~al.(2018)Berg, Hasenclever, Tomczak, and
  Welling]{berg2018sylvester}
R.~v.~d. Berg, L.~Hasenclever, J.~M. Tomczak, and M.~Welling.
\newblock Sylvester normalizing flows for variational inference.
\newblock In \emph{UAI 2018}, 2018.

\bibitem[Bilo{\v{s}} and G{\"u}nnemann(2021)]{bilos2021scalable}
M.~Bilo{\v{s}} and S.~G{\"u}nnemann.
\newblock Scalable normalizing flows for permutation invariant densities.
\newblock In \emph{ICML}, 2021.

\bibitem[Bilo{\v{s}} et~al.(2019)Bilo{\v{s}}, Charpentier, and
  G{\"u}nnemann]{bilos2019uncertainty}
M.~Bilo{\v{s}}, B.~Charpentier, and S.~G{\"u}nnemann.
\newblock Uncertainty on asynchronous time event prediction.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Chang et~al.(2018)Chang, Meng, Haber, Ruthotto, Begert, and
  Holtham]{chang2018reversible}
B.~Chang, L.~Meng, E.~Haber, L.~Ruthotto, D.~Begert, and E.~Holtham.
\newblock Reversible architectures for arbitrarily deep residual neural
  networks.
\newblock In \emph{AAAI}, 2018.

\bibitem[Che et~al.(2018)Che, Purushotham, Cho, Sontag, and
  Liu]{che2018recurrent}
Z.~Che, S.~Purushotham, K.~Cho, D.~Sontag, and Y.~Liu.
\newblock Recurrent neural networks for multivariate time series with missing
  values.
\newblock \emph{Scientific reports}, 8\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Chen et~al.(2021)Chen, Amos, and Nickel]{chen2021spatio}
R.~T.~Q. Chen, B.~Amos, and M.~Nickel.
\newblock Neural spatio-temporal point processes.
\newblock In \emph{ICLR}, 2021.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
T.~Chen, B.~Xu, C.~Zhang, and C.~Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
T.~Q. Chen, Y.~Rubanova, J.~Bettencourt, and D.~K. Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Bahdanau, and
  Bengio]{cho2014gru}
K.~Cho, B.~Van~Merri{\"e}nboer, D.~Bahdanau, and Y.~Bengio.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock In \emph{Eighth Workshop on Syntax, Semantics and Structure in
  Statistical Translation (SSST-8)}, 2014.

\bibitem[Ciccone et~al.(2018)Ciccone, Gallieri, Masci, Osendorfer, and
  Gomez]{ciccone2018nais}
M.~Ciccone, M.~Gallieri, J.~Masci, C.~Osendorfer, and F.~Gomez.
\newblock {NAIS}-{N}et: Stable deep networks from non-autonomous differential
  equations.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Coddington and Levinson(1955)]{coddington1955theory}
E.~A. Coddington and N.~Levinson.
\newblock \emph{Theory of ordinary differential equations}.
\newblock Tata McGraw-Hill Education, 1955.

\bibitem[Daley and Vere-Jones(2007)]{daleyintroduction}
D.~Daley and D.~Vere-Jones.
\newblock \emph{An Introduction to the Theory of Point Processes: Volume {I}:
  Elementary Theory and Methods}.
\newblock Springer Science \& Business Media, 2007.

\bibitem[De~Brouwer et~al.(2019)De~Brouwer, Simm, Arany, and Moreau]{de2019gru}
E.~De~Brouwer, J.~Simm, A.~Arany, and Y.~Moreau.
\newblock {GRU-ODE-Bayes}: Continuous modeling of sporadically-observed time
  series.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Dinh et~al.(2017)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
L.~Dinh, J.~Sohl-Dickstein, and S.~Bengio.
\newblock Density estimation using {R}eal {NVP}.
\newblock In \emph{ICLR}, 2017.

\bibitem[Dormand and Prince(1980)]{dormand1980family}
J.~R. Dormand and P.~J. Prince.
\newblock A family of embedded {R}unge-{K}utta formulae.
\newblock \emph{Journal of computational and applied mathematics}, 6\penalty0
  (1):\penalty0 19--26, 1980.

\bibitem[Du et~al.(2016)Du, Dai, Trivedi, Upadhyay, Gomez-Rodriguez, and
  Song]{du2016recurrent}
N.~Du, H.~Dai, R.~Trivedi, U.~Upadhyay, M.~Gomez-Rodriguez, and L.~Song.
\newblock {RMTPP}: Embedding event history to vector.
\newblock In \emph{KDD}, 2016.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
E.~Dupont, A.~Doucet, and Y.~W. Teh.
\newblock Augmented neural {ODE}s.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Farrell et~al.(2013)Farrell, Ham, Funke, and
  Rognes]{farrell2013automated}
P.~E. Farrell, D.~A. Ham, S.~W. Funke, and M.~E. Rognes.
\newblock Automated derivation of the adjoint of high-level transient finite
  element programs.
\newblock \emph{SIAM Journal on Scientific Computing}, 35\penalty0
  (4):\penalty0 C369--C393, 2013.

\bibitem[Finlay et~al.(2020)Finlay, Jacobsen, Nurbekyan, and
  Oberman]{finlay2020train}
C.~Finlay, J.-H. Jacobsen, L.~Nurbekyan, and A.~M. Oberman.
\newblock How to train your neural {ODE}.
\newblock In \emph{ICML}, 2020.

\bibitem[Gholami et~al.(2019)Gholami, Keutzer, and Biros]{gholami2019anode}
A.~Gholami, K.~Keutzer, and G.~Biros.
\newblock {ANODE}: Unconditionally accurate memory-efficient gradients for
  neural {ODE}s.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Ghosh et~al.(2020)Ghosh, Behl, Dupont, Torr, and
  Namboodiri]{ghosh2020steer}
A.~Ghosh, H.~S. Behl, E.~Dupont, P.~H. Torr, and V.~Namboodiri.
\newblock {STEER}: Simple temporal regularization for neural odes.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Goldberger et~al.(2000)Goldberger, Amaral, Glass, Hausdorff, Ivanov,
  Mark, Mietus, Moody, Peng, and Stanley]{physionet}
A.~L. Goldberger, L.~A.~N. Amaral, L.~Glass, J.~M. Hausdorff, P.~C. Ivanov,
  R.~G. Mark, J.~E. Mietus, G.~B. Moody, C.-K. Peng, and H.~E. Stanley.
\newblock Physiobank, {P}hysiotoolkit, and {P}hysionet: Components of a new
  research resource for complex physiologic signals.
\newblock \emph{Circulation}, 101\penalty0 (23):\penalty0 e215--e220, 2000.

\bibitem[Gouk et~al.(2021)Gouk, Frank, Pfahringer, and
  Cree]{gouk2021regularisation}
H.~Gouk, E.~Frank, B.~Pfahringer, and M.~J. Cree.
\newblock Regularisation of neural networks by enforcing {L}ipschitz
  continuity.
\newblock \emph{Machine Learning}, 110\penalty0 (2):\penalty0 393--416, 2021.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Chen, Bettencourt, Sutskever, and
  Duvenaud]{grathwohl2018ffjord}
W.~Grathwohl, R.~T. Chen, J.~Bettencourt, I.~Sutskever, and D.~Duvenaud.
\newblock {FFJORD}: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock In \emph{ICLR}, 2019.

\bibitem[Haber and Ruthotto(2017)]{haber2017stable}
E.~Haber and L.~Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock \emph{Inverse Problems}, 34\penalty0 (1):\penalty0 014004, 2017.

\bibitem[Hawkes(1971)]{hawkes1971spectra}
A.~G. Hawkes.
\newblock Spectra of some self-exciting and mutually exciting point processes.
\newblock \emph{Biometrika}, 58\penalty0 (1):\penalty0 83--90, 1971.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE CCVPR}, 2016.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hyndman et~al.(2008)Hyndman, Koehler, Ord, and Snyder]{Hyndman2008}
R.~Hyndman, A.~Koehler, K.~Ord, and R.~Snyder.
\newblock \emph{Forecasting with exponential smoothing. The state space
  approach}.
\newblock Springer Science \& Business Media, 2008.
\newblock \doi{10.1007/978-3-540-71918-2}.

\bibitem[Jacobsen et~al.(2018)Jacobsen, Smeulders, and
  Oyallon]{jacobsen2018revnet}
J.~Jacobsen, A.~W.~M. Smeulders, and E.~Oyallon.
\newblock i-revnet: Deep invertible networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Jia and Benson(2019)]{jia2019jump}
J.~Jia and A.~R. Benson.
\newblock Neural jump stochastic differential equations.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Johnson et~al.(2016)Johnson, Pollard, Shen, Li-Wei, Feng, Ghassemi,
  Moody, Szolovits, Celi, and Mark]{johnson2016mimic}
A.~Johnson, T.~Pollard, L.~Shen, H.~L. Li-Wei, M.~Feng, M.~Ghassemi, B.~Moody,
  P.~Szolovits, L.~A. Celi, and R.~G. Mark.
\newblock {MIMIC}-{III}, a freely accessible critical care database.
\newblock \emph{Scientific data}, 3\penalty0 (1):\penalty0 1--9, 2016.

\bibitem[Johnson et~al.(2021)Johnson, Bulgarelli, Pollard, Horng, Celi, and
  Mark]{mimic4}
A.~Johnson, L.~Bulgarelli, T.~Pollard, S.~Horng, L.~A. Celi, and R.~Mark.
\newblock {MIMIC}-{IV} (version 1.0).
\newblock \emph{PhysioNet}, 2021.
\newblock \doi{10.13026/s6n6-xd98}.

\bibitem[Kelly et~al.(2020)Kelly, Bettencourt, Johnson, and
  Duvenaud]{kelly2020learning}
J.~Kelly, J.~Bettencourt, M.~J. Johnson, and D.~Duvenaud.
\newblock Learning differential equations that are easy to solve.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Kidger et~al.(2021)Kidger, Chen, and Lyons]{kidger2020hey}
P.~Kidger, R.~T. Chen, and T.~Lyons.
\newblock "{Hey}, that's not an {ODE}": {F}aster {ODE} adjoints via seminorms.
\newblock In \emph{ICML}, 2021.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
D.~P. Kingma and P.~Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
D.~P. Kingma, T.~Salimans, R.~Jozefowicz, X.~Chen, I.~Sutskever, and
  M.~Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[{Kobyzev} et~al.(2020){Kobyzev}, {Prince}, and
  {Brubaker}]{kobyzev2020normalizing}
I.~{Kobyzev}, S.~{Prince}, and M.~{Brubaker}.
\newblock Normalizing flows: An introduction and review of current methods.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2020.

\bibitem[K{\"o}hler et~al.(2020)K{\"o}hler, Klein, and
  No{\'e}]{kohler2020equivariant}
J.~K{\"o}hler, L.~Klein, and F.~No{\'e}.
\newblock Equivariant flows: exact likelihood generative learning for symmetric
  densities.
\newblock In \emph{ICML}, 2020.

\bibitem[Kumar et~al.(2019)Kumar, Zhang, and Leskovec]{kumar2019predicting}
S.~Kumar, X.~Zhang, and J.~Leskovec.
\newblock Predicting dynamic embedding trajectory in temporal interaction
  networks.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1269--1278, 2019.

\bibitem[Lagaris et~al.(1998)Lagaris, Likas, and
  Fotiadis]{lagaris1998artificial}
I.~E. Lagaris, A.~Likas, and D.~I. Fotiadis.
\newblock Artificial neural networks for solving ordinary and partial
  differential equations.
\newblock \emph{IEEE transactions on neural networks}, 9\penalty0 (5):\penalty0
  987--1000, 1998.

\bibitem[Lechner and Hasani(2020)]{lechner2020learning}
M.~Lechner and R.~Hasani.
\newblock Learning long-term dependencies in irregularly-sampled time series.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Lee(2012)]{lee2013smooth}
J.~M. Lee.
\newblock \emph{Introduction to Smooth Manifolds}.
\newblock Springer, 2012.

\bibitem[Li et~al.(2019)Li, Lin, and Shen]{li2019deep}
Q.~Li, T.~Lin, and Z.~Shen.
\newblock Deep learning via dynamical systems: An approximation perspective.
\newblock \emph{arXiv preprint arXiv:1912.10382}, 2019.

\bibitem[Li et~al.(2021)Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya,
  Stuart, and Anandkumar]{li2020fourier}
Z.~Li, N.~Kovachki, K.~Azizzadenesheli, B.~Liu, K.~Bhattacharya, A.~Stuart, and
  A.~Anandkumar.
\newblock Fourier neural operator for parametric partial differential
  equations.
\newblock In \emph{ICLR}, 2021.

\bibitem[{Li Jianyu} et~al.(2002){Li Jianyu}, {Luo Siwei}, {Qi Yingjian}, and
  {Huang Yaping}]{jianyu2002numerical}
{Li Jianyu}, {Luo Siwei}, {Qi Yingjian}, and {Huang Yaping}.
\newblock Numerical solution of differential equations by radial basis function
  neural networks.
\newblock In \emph{IJCNN}, 2002.

\bibitem[Liao and Poggio(2016)]{liao2016bridging}
Q.~Liao and T.~Poggio.
\newblock Bridging the gaps between residual learning, recurrent neural
  networks and visual cortex.
\newblock \emph{arXiv preprint arXiv:1604.03640}, 2016.

\bibitem[Liberty et~al.(2020)Liberty, Karnin, Xiang, Rouesnel, Coskun,
  Nallapati, Delgado, Sadoughi, Astashonok, Das, et~al.]{liberty2020elastic}
E.~Liberty, Z.~Karnin, B.~Xiang, L.~Rouesnel, B.~Coskun, R.~Nallapati,
  J.~Delgado, A.~Sadoughi, Y.~Astashonok, P.~Das, et~al.
\newblock Elastic machine learning algorithms in amazon sagemaker.
\newblock In \emph{Proceedings of the ACM SIGMOD International Conference on
  Management of Data}, 2020.

\bibitem[Lin and Jegelka(2018)]{lin2018resnet}
H.~Lin and S.~Jegelka.
\newblock {ResNet} with one-neuron hidden layers is a universal approximator.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{lu2018beyond}
Y.~Lu, A.~Zhong, Q.~Li, and B.~Dong.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock In \emph{ICML}, 2018.

\bibitem[Meade~Jr and Fernandez(1994)]{meade1994numerical}
A.~J. Meade~Jr and A.~A. Fernandez.
\newblock The numerical solution of linear ordinary differential equations by
  feedforward neural networks.
\newblock \emph{Mathematical and Computer Modelling}, 19\penalty0
  (12):\penalty0 1--25, 1994.

\bibitem[Mei and Eisner(2017)]{mei2017neural}
H.~Mei and J.~M. Eisner.
\newblock The neural hawkes process: A neurally self-modulating multivariate
  point process.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Meyer et~al.(2012)Meyer, Elias, and H{\"o}hle]{meyer2012space}
S.~Meyer, J.~Elias, and M.~H{\"o}hle.
\newblock A space--time conditional intensity model for invasive meningococcal
  disease occurrence.
\newblock \emph{Biometrics}, 68\penalty0 (2):\penalty0 607--616, 2012.

\bibitem[Neil et~al.(2016)Neil, Pfeiffer, and Liu]{neil2016phased}
D.~Neil, M.~Pfeiffer, and S.-C. Liu.
\newblock Phased {LSTM}: Accelerating recurrent network training for long or
  event-based sequences.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Norcliffe et~al.(2020)Norcliffe, Bodnar, Day, Simidjievski, and
  Li{\`o}]{norcliffe2020second}
A.~Norcliffe, C.~Bodnar, B.~Day, N.~Simidjievski, and P.~Li{\`o}.
\newblock On second order behaviour in augmented neural {ODE}s.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Ogata and Vere-Jones(1984)]{ogata1984inference}
Y.~Ogata and D.~Vere-Jones.
\newblock Inference for earthquake models: {A} self-correcting model.
\newblock \emph{Stochastic processes and their applications}, 17\penalty0
  (2):\penalty0 337--347, 1984.

\bibitem[Omi et~al.(2019)Omi, Ueda, and Aihara]{omi2019fully}
T.~Omi, N.~Ueda, and K.~Aihara.
\newblock Fully neural network based model for general temporal point
  processes.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Oord et~al.(2016)Oord, Dieleman, Zen, Simonyan, Vinyals, Graves,
  Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
A.~v.~d. Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves,
  N.~Kalchbrenner, A.~Senior, and K.~Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock In \emph{SSW}, 2016.

\bibitem[Ott et~al.(2021)Ott, Katiyar, Hennig, and Tiemann]{ott2020neural}
K.~Ott, P.~Katiyar, P.~Hennig, and M.~Tiemann.
\newblock {ResNet} after all? {N}eural {ODEs} and their numerical solution.
\newblock \emph{ICLR}, 2021.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
G.~Papamakarios, T.~Pavlakou, and I.~Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Papamakarios et~al.(2019)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2019normalizing}
G.~Papamakarios, E.~Nalisnick, D.~J. Rezende, S.~Mohamed, and
  B.~Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{arXiv:1912.02762}, 2019.

\bibitem[Piscopo et~al.(2019)Piscopo, Spannowsky, and
  Waite]{piscopo2019solving}
M.~L. Piscopo, M.~Spannowsky, and P.~Waite.
\newblock Solving differential equations with neural networks: Applications to
  the calculation of cosmological phase transitions.
\newblock \emph{Phys. Rev. D}, 2019.

\bibitem[Poli et~al.(2020)Poli, Massaroli, Yamashita, Asama, and
  Park]{poli2020hyper}
M.~Poli, S.~Massaroli, A.~Yamashita, H.~Asama, and J.~Park.
\newblock Hypersolvers: Toward fast continuous-depth models.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Rangapuram et~al.(2018)Rangapuram, Seeger, Gasthaus, Stella, Wang, and
  Januschowski]{rangapuram2018deep}
S.~S. Rangapuram, M.~W. Seeger, J.~Gasthaus, L.~Stella, Y.~Wang, and
  T.~Januschowski.
\newblock Deep state space models for time series forecasting.
\newblock In \emph{{NeurIPS}}, 2018.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Y.~Rubanova, R.~T. Chen, and D.~Duvenaud.
\newblock Latent {ODEs} for irregularly-sampled time series.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Salinas et~al.(2020)Salinas, Flunkert, Gasthaus, and
  Januschowski]{salinas2020deepar}
D.~Salinas, V.~Flunkert, J.~Gasthaus, and T.~Januschowski.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent
  networks.
\newblock \emph{International Journal of Forecasting}, 36\penalty0
  (3):\penalty0 1181--1191, 2020.

\bibitem[Shchur et~al.(2020)Shchur, Bilo{\v{s}}, and
  G{\"u}nnemann]{shchur2019intensity}
O.~Shchur, M.~Bilo{\v{s}}, and S.~G{\"u}nnemann.
\newblock Intensity-free learning of temporal point processes.
\newblock In \emph{ICLR}, 2020.

\bibitem[Shchur et~al.(2021)Shchur, T{\"u}rkmen, Januschowski, and
  G{\"u}nnemann]{shchur2021neural}
O.~Shchur, A.~C. T{\"u}rkmen, T.~Januschowski, and S.~G{\"u}nnemann.
\newblock Neural temporal point processes: A review.
\newblock In \emph{IJCAI}, 2021.

\bibitem[Silva et~al.(2012)Silva, Moody, Scott, Celi, and
  Mark]{silva2012predicting}
I.~Silva, G.~Moody, D.~J. Scott, L.~A. Celi, and R.~G. Mark.
\newblock Predicting in-hospital mortality of icu patients: The
  physionet/computing in cardiology challenge 2012.
\newblock In \emph{2012 Computing in Cardiology}, pages 245--248. IEEE, 2012.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, Casas, Budden,
  Abdolmaleki, Merel, Lefrancq, et~al.]{tassa2018deepmind}
Y.~Tassa, Y.~Doron, A.~Muldal, T.~Erez, Y.~Li, D.~d.~L. Casas, D.~Budden,
  A.~Abdolmaleki, J.~Merel, A.~Lefrancq, et~al.
\newblock Deepmind control suite.
\newblock \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Teshima et~al.(2020{\natexlab{a}})Teshima, Ishikawa, Tojo, Oono,
  Ikeda, and Sugiyama]{teshima2020coupling}
T.~Teshima, I.~Ishikawa, K.~Tojo, K.~Oono, M.~Ikeda, and M.~Sugiyama.
\newblock Coupling-based invertible neural networks are universal
  diffeomorphism approximators.
\newblock In \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Teshima et~al.(2020{\natexlab{b}})Teshima, Tojo, Ikeda, Ishikawa, and
  Oono]{teshima2020universal}
T.~Teshima, K.~Tojo, M.~Ikeda, I.~Ishikawa, and K.~Oono.
\newblock Universal approximation property of neural ordinary differential
  equations.
\newblock In \emph{NeurIPS 2020 Workshop on Differential Geometry meets Deep
  Learning}, 2020{\natexlab{b}}.

\bibitem[{The New York Times}(2020)]{nycovid}
{The New York Times}.
\newblock Coronavirus ({C}ovid-19) data in the {U}nited {S}tates, 2020.
\newblock URL \url{https://github.com/nytimes/covid-19-data}.

\bibitem[{U.S. Geological Survey}(2020)]{geosurvey}
{U.S. Geological Survey}.
\newblock Earthquake catalogue (accessed {M}ay 15, 2021), 2020.
\newblock URL \url{https://earthquake.usgs.gov/earthquakes/search/}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Weinan(2017)]{weinan2017proposal}
E.~Weinan.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 5\penalty0
  (1):\penalty0 1--11, 2017.

\bibitem[Zhang et~al.(2020)Zhang, Gao, Unterman, and
  Arodz]{zhang2020approximation}
H.~Zhang, X.~Gao, J.~Unterman, and T.~Arodz.
\newblock Approximation capabilities of neural {ODE}s and invertible residual
  networks.
\newblock In \emph{ICML}, pages 11086--11095, 2020.

\bibitem[Zhuang et~al.(2020)Zhuang, Dvornek, Li, Tatikonda, Papademetris, and
  Duncan]{zhuang2020adaptive}
J.~Zhuang, N.~Dvornek, X.~Li, S.~Tatikonda, X.~Papademetris, and J.~Duncan.
\newblock Adaptive checkpoint adjoint method for gradient estimation in neural
  {ODE}.
\newblock In \emph{ICML}, 2020.

\end{thebibliography}
