\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Arpit et~al.(2021)Arpit, Wang, Zhou, and Xiong]{arpit2021ensemble}
Arpit, D., Wang, H., Zhou, Y., and Xiong, C.
\newblock Ensemble of averages: Improving model selection and boosting
  performance in domain generalization.
\newblock \emph{arXiv preprint arXiv:2110.10832}, 2021.

\bibitem[Asano et~al.(2020)Asano, Rupprecht, and
  Vedaldi]{Asano2020SelflabellingVS}
Asano, Y.~M., Rupprecht, C., and Vedaldi, A.
\newblock Self-labelling via simultaneous clustering and representation
  learning.
\newblock \emph{ArXiv}, abs/1911.05371, 2020.

\bibitem[Bahng et~al.(2020)Bahng, Chun, Yun, Choo, and Oh]{bahng2020learning}
Bahng, H., Chun, S., Yun, S., Choo, J., and Oh, S.~J.
\newblock Learning de-biased representations with biased representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  528--539. PMLR, 2020.

\bibitem[Bai et~al.(2021{\natexlab{a}})Bai, Sun, Hong, Zhou, Ye, Ye, Chan, and
  Li]{bai2020decaug}
Bai, H., Sun, R., Hong, L., Zhou, F., Ye, N., Ye, H.-J., Chan, S.-H.~G., and
  Li, Z.
\newblock Decaug: Out-of-distribution generalization via decomposed feature
  representation and semantic augmentation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  6705--6713, 2021{\natexlab{a}}.

\bibitem[Bai et~al.(2021{\natexlab{b}})Bai, Zhou, Hong, Ye, Chan, and
  Li]{bai2021ood}
Bai, H., Zhou, F., Hong, L., Ye, N., Chan, S.-H.~G., and Li, Z.
\newblock Nas-ood: Neural architecture search for out-of-distribution
  generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  8320--8329, 2021{\natexlab{b}}.

\bibitem[Bao et~al.(2021)Bao, Dong, and Wei]{bao2021beit}
Bao, H., Dong, L., and Wei, F.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{arXiv preprint arXiv:2106.08254}, 2021.

\bibitem[Beery et~al.(2018{\natexlab{a}})Beery, Horn, and
  Perona]{Beery2018RecognitionIT}
Beery, S., Horn, G.~V., and Perona, P.
\newblock Recognition in terra incognita.
\newblock In \emph{ECCV}, 2018{\natexlab{a}}.

\bibitem[Beery et~al.(2018{\natexlab{b}})Beery, Van~Horn, and
  Perona]{beery2018recognition}
Beery, S., Van~Horn, G., and Perona, P.
\newblock Recognition in terra incognita.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  456--473, 2018{\natexlab{b}}.

\bibitem[Caron et~al.(2018)Caron, Bojanowski, Joulin, and
  Douze]{Caron2018DeepCF}
Caron, M., Bojanowski, P., Joulin, A., and Douze, M.
\newblock Deep clustering for unsupervised learning of visual features.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  132--149, 2018.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{Caron2020UnsupervisedLO}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{Thirty-fourth Conference on Neural Information Processing
  Systems (NeurIPS)}, 2020.

\bibitem[Cha et~al.(2021)Cha, Chun, Lee, Cho, Park, Lee, and
  Park]{Cha2021SWADDG}
Cha, J., Chun, S., Lee, K., Cho, H.-C., Park, S., Lee, Y., and Park, S.
\newblock Swad: Domain generalization by seeking flat minima.
\newblock \emph{arXiv preprint arXiv:2102.08604}, 2021.

\bibitem[Chen et~al.(2020)Chen, Fan, Girshick, and He]{Chen2020ImprovedBW}
Chen, X., Fan, H., Girshick, R.~B., and He, K.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{ArXiv}, abs/2003.04297, 2020.

\bibitem[Chen et~al.(2022)Chen, Xiong, Ma, and Lan]{chen2022does}
Chen, Y., Xiong, R., Ma, Z.-M., and Lan, Y.
\newblock When does group invariant learning survive spurious correlations?
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 7038--7051, 2022.

\bibitem[Dai \& Van~Gool(2018)Dai and Van~Gool]{dai2018dark}
Dai, D. and Van~Gool, L.
\newblock Dark model adaptation: Semantic image segmentation from daytime to
  nighttime.
\newblock In \emph{2018 21st International Conference on Intelligent
  Transportation Systems (ITSC)}, pp.\  3819--3824. IEEE, 2018.

\bibitem[Dong et~al.(2022)Dong, Muhammad, Zhou, Xie, Hu, Yang, Bae, and
  Li]{dong2022zood}
Dong, Q., Muhammad, A., Zhou, F., Xie, C., Hu, T., Yang, Y., Bae, S.-H., and
  Li, Z.
\newblock Zood: Exploiting model zoo for out-of-distribution generalization.
\newblock \emph{arXiv preprint arXiv:2210.09236}, 2022.

\bibitem[Ericsson et~al.(2021)Ericsson, Gouk, and
  Hospedales]{Ericsson2021HowTransfer}
Ericsson, L., Gouk, H., and Hospedales, T.~M.
\newblock How well do self-supervised models transfer?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  5414--5423, 2021.

\bibitem[Fang et~al.(2013)Fang, Xu, and Rockmore]{Fang2013UnbiasedML}
Fang, C., Xu, Y., and Rockmore, D.~N.
\newblock Unbiased metric learning: On the utilization of multiple datasets and
  web images for softening bias.
\newblock \emph{2013 IEEE International Conference on Computer Vision}, pp.\
  1657--1664, 2013.

\bibitem[Gontijo-Lopes et~al.(2022)Gontijo-Lopes, Dauphin, and
  Cubuk]{gontijo-lopes2022no}
Gontijo-Lopes, R., Dauphin, Y., and Cubuk, E.~D.
\newblock No one representation to rule them all: Overlapping features of
  training methods.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=BK-4qbGgIE3}.

\bibitem[Gretton et~al.(2007)Gretton, Fukumizu, Teo, Song, Sch{\"o}lkopf, and
  Smola]{gretton2007kernel}
Gretton, A., Fukumizu, K., Teo, C., Song, L., Sch{\"o}lkopf, B., and Smola, A.
\newblock A kernel statistical test of independence.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch'e, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{Grill2020BootstrapYO}
Grill, J.-B., Strub, F., Altch'e, F., Tallec, C., Richemond, P.~H.,
  Buchatskaya, E., Doersch, C., Pires, B.~{\'A}., Guo, Z.~D., Azar, M.~G.,
  Piot, B., Kavukcuoglu, K., Munos, R., and Valko, M.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock \emph{ArXiv}, abs/2006.07733, 2020.

\bibitem[Gulrajani \& Lopez-Paz(2021)Gulrajani and
  Lopez-Paz]{gulrajani2020search}
Gulrajani, I. and Lopez-Paz, D.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016DeepRL}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2016.

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2021masked}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock \emph{arXiv preprint arXiv:2111.06377}, 2021.

\bibitem[Hu et~al.(2022{\natexlab{a}})Hu, Liu, Zhou, Wang, and
  Huang]{hu2022your}
Hu, T., Liu, Z., Zhou, F., Wang, W., and Huang, W.
\newblock Your contrastive learning is secretly doing stochastic neighbor
  embedding.
\newblock \emph{arXiv preprint arXiv:2205.14814}, 2022{\natexlab{a}}.

\bibitem[Hu et~al.(2022{\natexlab{b}})Hu, Wang, Wang, and
  Li]{hu2022understanding}
Hu, T., Wang, J., Wang, W., and Li, Z.
\newblock Understanding square loss in training overparametrized neural network
  classifiers.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16495--16508, 2022{\natexlab{b}}.

\bibitem[Huang et~al.(2017)Huang, Liu, and Weinberger]{Huang2017DenselyCC}
Huang, G., Liu, Z., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  2261--2269, 2017.

\bibitem[Idrissi et~al.(2022)Idrissi, Bouchacourt, Balestriero, Evtimov,
  Hazirbas, Ballas, Vincent, Drozdzal, Lopez-Paz, and
  Ibrahim]{idrissi2022imagenet}
Idrissi, B.~Y., Bouchacourt, D., Balestriero, R., Evtimov, I., Hazirbas, C.,
  Ballas, N., Vincent, P., Drozdzal, M., Lopez-Paz, D., and Ibrahim, M.
\newblock Imagenet-x: Understanding model mistakes with factor of variation
  annotations.
\newblock \emph{arXiv preprint arXiv:2211.01866}, 2022.

\bibitem[Inc.(2023)]{hughub}
Inc., H.~F.
\newblock The model hub of hugging face.
\newblock \url{https://huggingface.co/models}, 2023.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang,
  Le~Priol, and Courville]{krueger2021out}
Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Zhang, D.,
  Le~Priol, R., and Courville, A.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5815--5826. PMLR, 2021.

\bibitem[Kuang et~al.(2018)Kuang, Cui, Athey, Xiong, and Li]{Kuang2018}
Kuang, K., Cui, P., Athey, S., Xiong, R., and Li, B.
\newblock Stable prediction across unknown environments.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  1617--1626, 2018.

\bibitem[Lee et~al.(2018)Lee, Lee, Lee, and Shin]{lee2018simple}
Lee, K., Lee, K., Lee, H., and Shin, J.
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li et~al.(2017)Li, Yang, Song, and Hospedales]{Li2017DeeperBA}
Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T.~M.
\newblock Deeper, broader and artier domain generalization.
\newblock \emph{2017 IEEE International Conference on Computer Vision (ICCV)},
  pp.\  5543--5551, 2017.

\bibitem[Li et~al.(2018)Li, Yang, Song, and Hospedales]{Li2018LearningTG}
Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T.~M.
\newblock Learning to generalize: Meta-learning for domain generalization.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Li et~al.(2021)Li, Zhou, Xiong, Socher, and Hoi]{Li2021PrototypicalCL}
Li, J., Zhou, P., Xiong, C., Socher, R., and Hoi, S. C.~H.
\newblock Prototypical contrastive learning of unsupervised representations.
\newblock \emph{ArXiv}, abs/2005.04966, 2021.

\bibitem[Li et~al.(2022)Li, Ren, Jiang, Li, Zhang, and Li]{li2022domain}
Li, Z., Ren, K., Jiang, X., Li, B., Zhang, H., and Li, D.
\newblock Domain generalization using pretrained models without fine-tuning.
\newblock \emph{arXiv preprint arXiv:2203.04600}, 2022.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{Liu2021SwinTH}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock \emph{ArXiv}, abs/2103.14030, 2021.

\bibitem[Liu et~al.(2022)Liu, Han, Chen, Hong, Xu, Xu, and Li]{liu2022task}
Liu, Z., Han, J., Chen, K., Hong, L., Xu, H., Xu, C., and Li, Z.
\newblock Task-customized self-supervised pre-training with scalable dynamic
  routing.
\newblock \emph{Transfer}, 55:\penalty0 65, 2022.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{Madry2018TowardsDL}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{ArXiv}, abs/1706.06083, 2018.

\bibitem[Misra \& van~der Maaten(2020)Misra and van~der
  Maaten]{Misra2020SelfSupervisedLO}
Misra, I. and van~der Maaten, L.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  6706--6716, 2020.

\bibitem[Nagarajan et~al.(2021)Nagarajan, Andreassen, and
  Neyshabur]{nagarajan2021understanding}
Nagarajan, V., Andreassen, A., and Neyshabur, B.
\newblock Understanding the failure modes of out-of-distribution
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d`Alch\'{e} Buc, F.,
  Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems 32}, pp.\  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and
  Wang]{Peng2019MomentMF}
Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B.
\newblock Moment matching for multi-source domain adaptation.
\newblock \emph{2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pp.\  1406--1415, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and
  Sutskever]{Radford2021LearningTV}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Ram{\'e} et~al.(2022)Ram{\'e}, Ahuja, Zhang, Cord, Bottou, and
  Lopez-Paz]{rame2022recycling}
Ram{\'e}, A., Ahuja, K., Zhang, J., Cord, M., Bottou, L., and Lopez-Paz, D.
\newblock Recycling diverse models for out-of-distribution generalization.
\newblock \emph{arXiv preprint arXiv:2212.10445}, 2022.

\bibitem[Rame et~al.(2022)Rame, Kirchmeyer, Rahier, Rakotomamonjy, patrick
  gallinari, and Cord]{rame2022diverse}
Rame, A., Kirchmeyer, M., Rahier, T., Rakotomamonjy, A., patrick gallinari, and
  Cord, M.
\newblock Diverse weight averaging for out-of-distribution generalization.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=tq_J_MqB3UB}.

\bibitem[Russakovsky et~al.(2015{\natexlab{a}})Russakovsky, Deng, Su, Krause,
  Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein,
  et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015{\natexlab{a}}.

\bibitem[Russakovsky et~al.(2015{\natexlab{b}})Russakovsky, Deng, Su, Krause,
  Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, and
  Fei-Fei]{Russakovsky2015ImageNetLS}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M.~S., Berg, A.~C., and Fei-Fei, L.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115:\penalty0
  211--252, 2015{\natexlab{b}}.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{Salman2020DoAR}
Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., and Madry, A.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{ArXiv}, abs/2007.08489, 2020.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{Sandler2018MobileNetV2IR}
Sandler, M., Howard, A.~G., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  4510--4520, 2018.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{Szegedy2015GoingDW}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.~E., Anguelov, D., Erhan,
  D., Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock \emph{2015 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  1--9, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{Szegedy2016RethinkingTI}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  2818--2826, 2016.

\bibitem[Tan \& Le(2019)Tan and Le]{Tan2019EfficientNetRM}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6105--6114. PMLR, 2019.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and
  Panchanathan]{Venkateswara2017DeepHN}
Venkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  5385--5394, 2017.

\bibitem[Volk et~al.(2019)Volk, M{\"u}ller, Von~Bernuth, Hospach, and
  Bringmann]{volk2019towards}
Volk, G., M{\"u}ller, S., Von~Bernuth, A., Hospach, D., and Bringmann, O.
\newblock Towards robust cnn-based object detection through augmentation with
  synthetic rain variations.
\newblock In \emph{2019 IEEE Intelligent Transportation Systems Conference
  (ITSC)}, pp.\  285--292. IEEE, 2019.

\bibitem[Wang et~al.(2022)Wang, Yi, Chen, and Zhu]{wang2022out}
Wang, R., Yi, M., Chen, Z., and Zhu, S.
\newblock Out-of-distribution generalization with causal invariant
  transformations.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  375--385, 2022.

\bibitem[Wiles et~al.(2022)Wiles, Gowal, Stimberg, Rebuffi, Ktena, Dvijotham,
  and Cemgil]{wiles2022a}
Wiles, O., Gowal, S., Stimberg, F., Rebuffi, S.-A., Ktena, I., Dvijotham,
  K.~D., and Cemgil, A.~T.
\newblock A fine-grained analysis on distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Dl4LetuLdyK}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.~L., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A.~M.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Wortsman, M., Ilharco, G., Gadre, S.~Y., Roelofs, R., Gontijo-Lopes, R.,
  Morcos, A.~S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  23965--23998. PMLR, 2022.

\bibitem[Wu et~al.(2020)Wu, Xu, Dai, Wan, Zhang, Yan, Tomizuka, Gonzalez,
  Keutzer, and Vajda]{wu2020visual}
Wu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Yan, Z., Tomizuka, M., Gonzalez,
  J., Keutzer, K., and Vajda, P.
\newblock Visual transformers: Token-based image representation and processing
  for computer vision, 2020.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{Wu2018UnsupervisedFL}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3733--3742, 2018.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{Xie2017AggregatedRT}
Xie, S., Girshick, R.~B., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  5987--5995, 2017.

\bibitem[Xiong et~al.(2021)Xiong, Chen, Pang, Cheng, Ma, and
  Lan]{xiong2021uncertainty}
Xiong, R., Chen, Y., Pang, L., Cheng, X., Ma, Z.-M., and Lan, Y.
\newblock Uncertainty calibration for ensemble-based debiasing methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 13657--13669, 2021.

\bibitem[Yalniz et~al.(2019)Yalniz, J{\'e}gou, Chen, Paluri, and
  Mahajan]{Yalniz2019BillionscaleSL}
Yalniz, I.~Z., J{\'e}gou, H., Chen, K., Paluri, M., and Mahajan, D.~K.
\newblock Billion-scale semi-supervised learning for image classification.
\newblock \emph{ArXiv}, abs/1905.00546, 2019.

\bibitem[Ye et~al.(2022)Ye, Li, Bai, Yu, Hong, Zhou, Li, and Zhu]{ye2022ood}
Ye, N., Li, K., Bai, H., Yu, R., Hong, L., Zhou, F., Li, Z., and Zhu, J.
\newblock Ood-bench: Quantifying and understanding two dimensions of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  7947--7958, 2022.

\bibitem[Yi et~al.(2021)Yi, Hou, Sun, Shang, Jiang, Liu, and
  Ma]{yi2021improved}
Yi, M., Hou, L., Sun, J., Shang, L., Jiang, X., Liu, Q., and Ma, Z.
\newblock Improved ood generalization via adversarial training and pretraing.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11987--11997. PMLR, 2021.

\bibitem[Yi et~al.(2023)Yi, Wang, Sun, Li, and Ma]{yi2023breaking}
Yi, M., Wang, R., Sun, J., Li, Z., and Ma, Z.-M.
\newblock Breaking correlation shift via conditional invariant regularizer.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[You et~al.(2021)You, Liu, Wang, and Long]{you2021logme}
You, K., Liu, Y., Wang, J., and Long, M.
\newblock Logme: Practical assessment of pre-trained models for transfer
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12133--12143. PMLR, 2021.

\end{thebibliography}
