\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arbuthnot(1712)]{Arbuthnot1712}
Arbuthnot, J.
\newblock {II.} {An} argument for divine providence, taken from the constant
  regularity observ'd in the births of both sexes. {By Dr. John Arbuthnott},
  {Physitian in Ordinary to Her Majesty, and Fellow of the College of
  Physitians and the Royal Society}.
\newblock \emph{Philosophical Transactions of the Royal Society of London},
  27\penalty0 (328):\penalty0 186--190, 1712.

\bibitem[Baird(1995)]{baird1995}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock \emph{Proceedings of the Twelfth International Conference on Machine
  Learning}, pp.\  30--37, 1995.

\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and
  {Bowling}]{bellemare2013}
{Bellemare}, M.~G., {Naddaf}, Y., {Veness}, J., and {Bowling}, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253–279, 2013.

\bibitem[Bellman(1957)]{bellman1957}
Bellman, R.
\newblock A markovian decision process.
\newblock \emph{Journal of Mathematics and Mechanics}, 1957.

\bibitem[Budden et~al.(2020)Budden, Hessel, Quan, Kapturowski, Baumli,
  Bhupatiraju, Guy, and King]{rlax2020}
Budden, D., Hessel, M., Quan, J., Kapturowski, S., Baumli, K., Bhupatiraju, S.,
  Guy, A., and King, M.
\newblock {RL}ax: {R}einforcement {L}earning in {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/rlax}.

\bibitem[Degris \& Modayil(2012)Degris and Modayil]{degris2012knowledge}
Degris, T. and Modayil, J.
\newblock Scaling-up knowledge for a cognizant robot.
\newblock \emph{AAAI Spring Symposium: Designing Intelligent Robots}, 2012.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{espeholt2018}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.
\newblock {IMPALA:} scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock \emph{CoRR}, 2018.

\bibitem[Ghiassian et~al.(2018)Ghiassian, Patterson, White, Sutton, and
  White]{ghiassian2018online}
Ghiassian, S., Patterson, A., White, M., Sutton, R.~S., and White, A.
\newblock Online off-policy prediction.
\newblock \emph{arXiv preprint arXiv:1811.02597}, 2018.

\bibitem[Hallak et~al.(2016)Hallak, Tamar, Munos, and Mannor]{hallak2016}
Hallak, A., Tamar, A., Munos, R., and Mannor, S.
\newblock Generalized emphatic temporal difference learning: Bias-variance
  analysis.
\newblock \emph{In Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence (AAAI-16)}, 2016.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and Babuschkin]{haiku2020}
Hennigan, T., Cai, T., Norman, T., and Babuschkin, I.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/dm-haiku}.

\bibitem[Hessel et~al.(2019)Hessel, Soyer, Espeholt, Czarnecki, Schmitt, and
  van Hasselt]{hessel2019multi}
Hessel, M., Soyer, H., Espeholt, L., Czarnecki, W., Schmitt, S., and van
  Hasselt, H.
\newblock Multi-task deep reinforcement learning with popart.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  33\penalty0 (01):\penalty0 3796--3803, 2019.

\bibitem[Hessel et~al.(2020)Hessel, Budden, Viola, Rosca, Sezener, and
  Hennigan]{optax2020}
Hessel, M., Budden, D., Viola, F., Rosca, M., Sezener, E., and Hennigan, T.
\newblock Optax: composable gradient transformation and optimisation, in
  {JAX}!, 2020.
\newblock URL \url{http://github.com/deepmind/optax}.

\bibitem[Hessel et~al.(2021)Hessel, Kroiss, Clark, Kemaev, Quan, Keck, Viola,
  and van Hasselt]{sebulba2021}
Hessel, M., Kroiss, M., Clark, A., Kemaev, I., Quan, J., Keck, T., Viola, F.,
  and van Hasselt, H.
\newblock Podracer architectures for scalable reinforcement learning.
\newblock 2021.
\newblock URL \url{https://arxiv.org/pdf/2104.06272.pdf}.

\bibitem[Imani et~al.(2018)Imani, Graves, and White]{imani2018}
Imani, E., Graves, E., and White, M.
\newblock An off-policy policy gradient theorem using emphatic weightings.
\newblock \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems (NeurIPS 2018)}, 2018.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,
  Silver, and Kavukcuoglu]{jaderberg2016}
Jaderberg, M., Mnih, V., Czarnecki, W., Schaul, T., Leibo, J., Silver, D., and
  Kavukcuoglu, K.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock \emph{ICLR}, 2017.

\bibitem[Lin(1992)]{Lin:1992}
Lin, L.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 293--321, 1992.

\bibitem[Mahmood et~al.(2017)Mahmood, Yu, and Sutton]{Mahmood:2017AB}
Mahmood, A.~R., Yu, H., and Sutton, R.~S.
\newblock Multi-step off-policy learning without importance sampling ratios.
\newblock \emph{arXiv preprint arXiv:1702.03006}, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 2015.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001off}
Precup, D., Sutton, R.~S., and Dasgupta, S.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock \emph{ICML}, pp.\  417--424, 2001.

\bibitem[Sutton(1987)]{sutton1989}
Sutton, R.~S.
\newblock Implementation details of the td($\lambda$) procedure for the case of
  vector predictions and backpropagation.
\newblock \emph{GTE Laboratories Technical Note TN87-509.1}, 1987.

\bibitem[Sutton(1988)]{Sutton:1988}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock The MIT Press, Cambridge, MA, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{Sutton:2000}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in Neural Information Processing Systems 13},
  12:\penalty0 1057--1063, 2000.

\bibitem[Sutton et~al.(2009)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesv{\'a}ri, and Wiewiora]{Sutton:2009}
Sutton, R.~S., Maei, H.~R., Precup, D., Bhatnagar, S., Silver, D.,
  Szepesv{\'a}ri, C., and Wiewiora, E.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock pp.\  993--1000. ACM, 2009.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
Sutton, R.~S., Modayil, J., Delp, M., Degris, T., Pilarski, P.~M., White, A.,
  and Precup, D.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pp.\  761--768, 2011.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{sutton2016emphatic}
Sutton, R.~S., Mahmood, A.~R., and White, M.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2603--2631, 2016.

\bibitem[Tsitsiklis \& {Van Roy}(1997)Tsitsiklis and {Van
  Roy}]{Tsitsiklis_VanRoy:97}
Tsitsiklis, J.~N. and {Van Roy}, B.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{{IEEE} Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.

\bibitem[van Hasselt et~al.(2018)van Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{hasselt2018}
van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil,
  J.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{CoRR}, abs/1812.02648, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.02648}.

\bibitem[{van Hasselt} et~al.(2021){van Hasselt}, Madjiheurem, Hessel, Silver,
  Barreto, and Borsa]{vanHasselt:2021}
{van Hasselt}, H., Madjiheurem, S., Hessel, M., Silver, D., Barreto, A., and
  Borsa, D.
\newblock Expected eligibility traces.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  35\penalty0 (11):\penalty0 9997--10005, May 2021.

\bibitem[Watkins(1989)]{watkins1989}
Watkins, C. J. C.~H.
\newblock Learning from delayed rewards.
\newblock 1989.

\bibitem[Yu(2015)]{yu2015}
Yu, H.
\newblock On convergence of emphatic temporal-difference learning.
\newblock \emph{JMLR: Workshop and Conference Proceedings}, 40:\penalty0
  1–28, 2015.

\bibitem[{Zahavy} et~al.(2020){Zahavy}, {Xu}, {Veeriah}, {Hessel}, {Oh}, {van
  Hasselt}, {Silver}, and {Singh}]{zahavy2020}
{Zahavy}, T., {Xu}, Z., {Veeriah}, V., {Hessel}, M., {Oh}, J., {van Hasselt},
  H., {Silver}, D., and {Singh}, S.
\newblock A self-tuning actor-critic algorithm.
\newblock \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS 2020)}, 2020.

\end{thebibliography}
