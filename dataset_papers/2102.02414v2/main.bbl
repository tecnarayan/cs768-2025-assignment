\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Angluin \& Laird(1988)Angluin and Laird]{angluin1988learning}
Angluin, D. and Laird, P.
\newblock Learning from noisy examples.
\newblock \emph{Machine Learning}, 2\penalty0 (4):\penalty0 343--370, 1988.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{\k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, 2017.

\bibitem[Bartlett et~al.(2006)Bartlett, Jordan, and
  McAuliffe]{bartlett2006convexity}
Bartlett, P.~L., Jordan, M.~I., and McAuliffe, J.~D.
\newblock Convexity, classification, and risk bounds.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (473):\penalty0 138--156, 2006.

\bibitem[Berthon et~al.(2021)Berthon, Han, Niu, Liu, and
  Sugiyama]{berthon2021confidence}
Berthon, A., Han, B., Niu, G., Liu, T., and Sugiyama, M.
\newblock Confidence scores make instance-dependent label-noise learning
  possible.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, 2021.

\bibitem[Blanchard \& Scott(2014)Blanchard and
  Scott]{blanchard2014decontamination}
Blanchard, G. and Scott, C.
\newblock Decontamination of mutually contaminated models.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1--9, 2014.

\bibitem[Charoenphakdee et~al.(2019)Charoenphakdee, Lee, and
  Sugiyama]{charoenphakdee2019symmetric}
Charoenphakdee, N., Lee, J., and Sugiyama, M.
\newblock On symmetric losses for learning from corrupted labels.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  961--970, 2019.

\bibitem[Charoenphakdee et~al.(2021)Charoenphakdee, Cui, Zhang, and
  Sugiyama]{charoenphakdee2020classification}
Charoenphakdee, N., Cui, Z., Zhang, Y., and Sugiyama, M.
\newblock Classification with rejection based on cost-sensitive classification.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, 2021.

\bibitem[Chen et~al.(2021)Chen, Ye, Chen, Zhao, and Heng]{chen2021robustness}
Chen, P., Ye, J., Chen, G., Zhao, J., and Heng, P.-A.
\newblock Robustness of accuracy metric and its inspirations in learning with
  noisy labels.
\newblock In \emph{Proceedings of the Thirty-Fifth AAAI Conference on
  Artificial Intelligence}, 2021.

\bibitem[Cheng et~al.(2020)Cheng, Liu, Ramamohanarao, and
  Tao]{cheng2020learning}
Cheng, J., Liu, T., Ramamohanarao, K., and Tao, D.
\newblock Learning with bounded instance-and label-dependent label noise.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  1789--1799, 2020.

\bibitem[Cortes \& Vapnik(1995)Cortes and Vapnik]{cortes1995support}
Cortes, C. and Vapnik, V.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20\penalty0 (3):\penalty0 273--297, 1995.

\bibitem[Del~Moral et~al.(2003)Del~Moral, Ledoux, and
  Miclo]{del2003contraction}
Del~Moral, P., Ledoux, M., and Miclo, L.
\newblock On contraction properties of markov kernels.
\newblock \emph{Probability theory and related fields}, 126\penalty0
  (3):\penalty0 395--420, 2003.

\bibitem[Diaconis \& Ylvisaker(1979)Diaconis and
  Ylvisaker]{diaconis1979conjugate}
Diaconis, P. and Ylvisaker, D.
\newblock Conjugate priors for exponential families.
\newblock \emph{The Annals of statistics}, pp.\  269--281, 1979.

\bibitem[du~Plessis et~al.(2014)du~Plessis, Niu, and Sugiyama]{du2014analysis}
du~Plessis, M.~C., Niu, G., and Sugiyama, M.
\newblock Analysis of learning from positive and unlabeled data.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  703--711, 2014.

\bibitem[El-Yaniv \& Wiener(2010)El-Yaniv and Wiener]{el2010foundations}
El-Yaniv, R. and Wiener, Y.
\newblock On the foundations of noise-free selective classification.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (5), 2010.

\bibitem[Feng et~al.(2020)Feng, Shu, Lin, Lv, Li, and An]{feng2020can}
Feng, L., Shu, S., Lin, Z., Lv, F., Li, L., and An, B.
\newblock Can cross entropy loss be robust to label noise?
\newblock In \emph{Proceedings of the Twenty-Ninth International Joint
  Conference on Artificial Intelligence}, pp.\  2206--2212, 2020.

\bibitem[Fergus et~al.(2005)Fergus, Fei-Fei, Perona, and
  Zisserman]{fergus2005learning}
Fergus, R., Fei-Fei, L., Perona, P., and Zisserman, A.
\newblock Learning object categories from {Google}'s image search.
\newblock In \emph{Tenth IEEE International Conference on Computer Vision},
  volume~2, pp.\  1816--1823, 2005.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Ghosh, A., Kumar, H., and Sastry, P.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Proceedings of the Thirty-First AAAI Conference on
  Artificial Intelligence}, pp.\  1919--1925, 2017.

\bibitem[Goldberger \& Ben{-}Reuven(2017)Goldberger and
  Ben{-}Reuven]{goldberger2017training}
Goldberger, J. and Ben{-}Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
\newblock \emph{Deep learning}.
\newblock MIT press Cambridge, 2016.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  1321--1330, 2017.

\bibitem[Han et~al.(2018{\natexlab{a}})Han, Yao, Niu, Zhou, Tsang, Zhang, and
  Sugiyama]{han2018masking}
Han, B., Yao, J., Niu, G., Zhou, M., Tsang, I., Zhang, Y., and Sugiyama, M.
\newblock Masking: A new perspective of noisy supervision.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5836--5846, 2018{\natexlab{a}}.

\bibitem[Han et~al.(2018{\natexlab{b}})Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8527--8537, 2018{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hein et~al.(2019)Hein, Andriushchenko, and Bitterwolf]{hein2019relu}
Hein, M., Andriushchenko, M., and Bitterwolf, J.
\newblock Why {ReLU} networks yield high-confidence predictions far away from
  the training data and how to mitigate the problem.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  41--50, 2019.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2017beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Ishida et~al.(2017)Ishida, Niu, Hu, and Sugiyama]{ishida2017learning}
Ishida, T., Niu, G., Hu, W., and Sugiyama, M.
\newblock Learning from complementary labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5639--5649, 2017.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock {MentorNet}: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  2304--2313, 2018.

\bibitem[Jiang et~al.(2020)Jiang, Huang, Liu, and Yang]{jiang2020beyond}
Jiang, L., Huang, D., Liu, M., and Yang, W.
\newblock Beyond synthetic noise: Deep learning on controlled noisy labels.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  4804--4815, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations},
  2015.

\bibitem[Kloft et~al.(2009)Kloft, Brefeld, Sonnenburg, Laskov, M{\"u}ller, and
  Zien]{kloft2009efficient}
Kloft, M., Brefeld, U., Sonnenburg, S., Laskov, P., M{\"u}ller, K., and Zien,
  A.
\newblock Efficient and accurate $\ell_p$-norm multiple kernel learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  997--1005, 2009.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Kuhn et~al.(1951)Kuhn, Tucker, et~al.]{kuhn1951nonlinear}
Kuhn, H., Tucker, A., et~al.
\newblock Nonlinear programming.
\newblock In \emph{Proceedings of the Second Berkeley Symposium on Mathematical
  Statistics and Probability}. The Regents of the University of California,
  1951.

\bibitem[Kull et~al.(2019)Kull, Nieto, K{\"a}ngsepp, Silva~Filho, Song, and
  Flach]{kull2019beyond}
Kull, M., Nieto, M.~P., K{\"a}ngsepp, M., Silva~Filho, T., Song, H., and Flach,
  P.
\newblock Beyond temperature scaling: Obtaining well-calibrated multi-class
  probabilities with dirichlet calibration.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  12316--12326, 2019.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition, 1998.

\bibitem[Li et~al.(2019)Li, Wong, Zhao, and Kankanhalli]{li2019learning}
Li, J., Wong, Y., Zhao, Q., and Kankanhalli, M.~S.
\newblock Learning to learn from noisy labeled data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  5051--5059, 2019.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{li2020dividemix}
Li, J., Socher, R., and Hoi, S.~C.
\newblock {DivideMix}: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Li et~al.(2021)Li, Liu, Han, Niu, and Sugiyama]{li2021provably}
Li, X., Liu, T., Han, B., Niu, G., and Sugiyama, M.
\newblock Provably end-to-end label-noise learning without anchor point.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, 2021.

\bibitem[Liu \& Tao(2015)Liu and Tao]{liu2015classification}
Liu, T. and Tao, D.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2015.

\bibitem[Liu \& Guo(2020)Liu and Guo]{liu2020peer}
Liu, Y. and Guo, H.
\newblock Peer loss functions: Learning from noisy labels without knowing noise
  rates.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  6226--6236, 2020.

\bibitem[Long \& Servedio(2010)Long and Servedio]{long2010random}
Long, P.~M. and Servedio, R.~A.
\newblock Random classification noise defeats all convex potential boosters.
\newblock \emph{Machine learning}, 78\penalty0 (3):\penalty0 287--304, 2010.

\bibitem[Lu et~al.(2019)Lu, Niu, Menon, and Sugiyama]{lu2019on}
Lu, N., Niu, G., Menon, A.~K., and Sugiyama, M.
\newblock On the minimal supervision for training any binary classifier from
  only unlabeled data.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lyu \& Tsang(2020)Lyu and Tsang]{lyu2020curriculum}
Lyu, Y. and Tsang, I.~W.
\newblock Curriculum loss: Robust learning and generalization against label
  corruption.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Ma et~al.(2020)Ma, Huang, Wang, Romano, Erfani, and
  Bailey]{ma2020normalized}
Ma, X., Huang, H., Wang, Y., Romano, S., Erfani, S., and Bailey, J.
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  6543--6553, 2020.

\bibitem[Malach \& Shalev-Shwartz(2017)Malach and
  Shalev-Shwartz]{malach2017decoupling}
Malach, E. and Shalev-Shwartz, S.
\newblock Decoupling ``when to update'' from ``how to update''.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  960--970, 2017.

\bibitem[Menon et~al.(2015)Menon, Van~Rooyen, Ong, and
  Williamson]{menon2015learning}
Menon, A., Van~Rooyen, B., Ong, C.~S., and Williamson, B.
\newblock Learning from corrupted binary labels via class-probability
  estimation.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pp.\  125--134, 2015.

\bibitem[Menon et~al.(2018)Menon, Van~Rooyen, and Natarajan]{menon2018learning}
Menon, A.~K., Van~Rooyen, B., and Natarajan, N.
\newblock Learning from binary labels with instance-dependent noise.
\newblock \emph{Machine Learning}, 107\penalty0 (8-10):\penalty0 1561--1595,
  2018.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Cao, and
  Leskovec]{mirzasoleiman2020coresets}
Mirzasoleiman, B., Cao, K., and Leskovec, J.
\newblock Coresets for robust training of deep neural networks against noisy
  labels.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Mozannar \& Sontag(2020)Mozannar and Sontag]{mozannar2020consistent}
Mozannar, H. and Sontag, D.
\newblock Consistent estimators for learning to defer to an expert.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7076--7087, 2020.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A.
\newblock Learning with noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1196--1204, 2013.

\bibitem[Nguyen et~al.(2020)Nguyen, Mummadi, Ngo, Nguyen, Beggel, and
  Brox]{nguyen2019self}
Nguyen, D.~T., Mummadi, C.~K., Ngo, T. P.~N., Nguyen, T. H.~P., Beggel, L., and
  Brox, T.
\newblock {SELF}: Learning to filter noisy labels with self-ensembling.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8026--8037, 2019.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1944--1952, 2017.

\bibitem[Rahimi et~al.(2020)Rahimi, Shaban, Cheng, Hartley, and
  Boots]{rahimi2020intra}
Rahimi, A., Shaban, A., Cheng, C.-A., Hartley, R., and Boots, B.
\newblock Intra order-preserving functions for calibration of multi-class
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Ramaswamy et~al.(2016)Ramaswamy, Scott, and
  Tewari]{ramaswamy2016mixture}
Ramaswamy, H., Scott, C., and Tewari, A.
\newblock Mixture proportion estimation via kernel embeddings of distributions.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pp.\  2052--2060, 2016.

\bibitem[Scott(2015)]{scott2015rate}
Scott, C.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{Proceedings of the Eighteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  838--846, 2015.

\bibitem[Scott et~al.(2013)Scott, Blanchard, and
  Handy]{scott2013classification}
Scott, C., Blanchard, G., and Handy, G.
\newblock Classification with asymmetric label noise: Consistency and maximal
  denoising.
\newblock In \emph{Proceedings of the 26th Annual Conference on Learning
  Theory}, pp.\  489--511, 2013.

\bibitem[Shu et~al.(2019)Shu, Xie, Yi, Zhao, Zhou, Xu, and Meng]{shu2019meta}
Shu, J., Xie, Q., Yi, L., Zhao, Q., Zhou, S., Xu, Z., and Meng, D.
\newblock {Meta-Weight-Net}: Learning an explicit mapping for sample weighting.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1139--1147, 2013.

\bibitem[Tewari \& Bartlett(2007)Tewari and Bartlett]{tewari2007consistency}
Tewari, A. and Bartlett, P.~L.
\newblock On the consistency of multiclass classification methods.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0
  (May):\penalty0 1007--1025, 2007.

\bibitem[Thulasidasan et~al.(2019)Thulasidasan, Bhattacharya, Bilmes,
  Chennupati, and Mohd-Yusof]{thulasidasan2019combating}
Thulasidasan, S., Bhattacharya, T., Bilmes, J., Chennupati, G., and Mohd-Yusof,
  J.
\newblock Combating label noise in deep learning using abstention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6234--6243, 2019.

\bibitem[Van~Rooyen et~al.(2015)Van~Rooyen, Menon, and
  Williamson]{van2015learning}
Van~Rooyen, B., Menon, A., and Williamson, R.~C.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10--18, 2015.

\bibitem[Wang et~al.(2018)Wang, Liu, Ma, Bailey, Zha, Song, and
  Xia]{wang2018iterative}
Wang, Y., Liu, W., Ma, X., Bailey, J., Zha, H., Song, L., and Xia, S.-T.
\newblock Iterative learning with open-set noisy labels.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  8688--8696, 2018.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and
  Bailey]{wang2019symmetric}
Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., and Bailey, J.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  322--330, 2019.

\bibitem[Wei et~al.(2020)Wei, Feng, Chen, and An]{wei2020combating}
Wei, H., Feng, L., Chen, X., and An, B.
\newblock Combating noisy labels by agreement: A joint training method with
  co-regularization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13726--13735, 2020.

\bibitem[Wu et~al.(2020)Wu, Zheng, Goswami, Metaxas, and
  Chen]{wu2020topological}
Wu, P., Zheng, S., Goswami, M., Metaxas, D., and Chen, C.
\newblock A topological filter for learning with label noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Xia et~al.(2019)Xia, Liu, Wang, Han, Gong, Niu, and
  Sugiyama]{xia2019anchor}
Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6838--6849, 2019.

\bibitem[Xia et~al.(2020)Xia, Liu, Han, Wang, Gong, Liu, Niu, Tao, and
  Sugiyama]{xia2020part}
Xia, X., Liu, T., Han, B., Wang, N., Gong, M., Liu, H., Niu, G., Tao, D., and
  Sugiyama, M.
\newblock Part-dependent label noise: Towards instance-dependent label noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2691--2699, 2015.

\bibitem[Yao et~al.(2020)Yao, Liu, Han, Gong, Deng, Niu, and
  Sugiyama]{yao2020dual}
Yao, Y., Liu, T., Han, B., Gong, M., Deng, J., Niu, G., and Sugiyama, M.
\newblock Dual {T}: Reducing estimation error for transition matrix in
  label-noise learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Yu et~al.(2018)Yu, Liu, Gong, and Tao]{yu2018learning}
Yu, X., Liu, T., Gong, M., and Tao, D.
\newblock Learning with biased complementary labels.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  68--83, 2018.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama, M.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  7164--7173, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8778--8788, 2018.

\end{thebibliography}
