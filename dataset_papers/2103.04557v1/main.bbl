\begin{thebibliography}{}

\bibitem[Alemohammad et~al., 2020]{alemohammad2020recurrent}
Alemohammad, S., Wang, Z., Balestriero, R., and Baraniuk, R. (2020).
\newblock The recurrent neural tangent kernel.
\newblock {\em arXiv preprint arXiv:2006.10246}.

\bibitem[Allen-Zhu et~al., 2018]{allen2018convergence}
Allen-Zhu, Z., Li, Y., and Song, Z. (2018).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em arXiv preprint arXiv:1811.03962}.

\bibitem[Anthony and Bartlett, 2009]{anthony2009neural}
Anthony, M. and Bartlett, P.~L. (2009).
\newblock {\em Neural network learning: Theoretical foundations}.
\newblock cambridge university press.

\bibitem[Bayati and Montanari, 2011]{bayati2011dynamics}
Bayati, M. and Montanari, A. (2011).
\newblock The dynamics of message passing on dense graphs, with applications to
  compressed sensing.
\newblock {\em IEEE Transactions on Information Theory}, 57(2):764--785.

\bibitem[Belkin et~al., 2019a]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S. (2019a).
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854.

\bibitem[Belkin et~al., 2019b]{belkin2019two}
Belkin, M., Hsu, D., and Xu, J. (2019b).
\newblock Two models of double descent for weak features.
\newblock {\em arXiv preprint arXiv:1903.07571}.

\bibitem[Deng et~al., 2019]{deng2019model}
Deng, Z., Kammoun, A., and Thrampoulidis, C. (2019).
\newblock A model of double descent for high-dimensional binary linear
  classification.
\newblock {\em arXiv preprint arXiv:1911.05822}.

\bibitem[Dicker et~al., 2016]{dicker2016ridge}
Dicker, L.~H. et~al. (2016).
\newblock Ridge regression and asymptotic minimax estimation over spheres of
  growing dimension.
\newblock {\em Bernoulli}, 22(1):1--37.

\bibitem[Dobriban et~al., 2018]{dobriban2018high}
Dobriban, E., Wager, S., et~al. (2018).
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock {\em The Annals of Statistics}, 46(1):247--279.

\bibitem[Donoho et~al., 2010]{donoho2010message}
Donoho, D.~L., Maleki, A., and Montanari, A. (2010).
\newblock Message passing algorithms for compressed sensing: I. motivation and
  construction.
\newblock In {\em 2010 IEEE information theory workshop on information theory
  (ITW 2010, Cairo)}, pages 1--5. IEEE.

\bibitem[Du et~al., 2018a]{du2018gradient}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X. (2018a).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}.

\bibitem[Du et~al., 2018b]{du2018gradient2}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A. (2018b).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}.

\bibitem[Emami et~al., 2020]{emami2020generalization}
Emami, M., Sahraee-Ardakan, M., Pandit, P., Rangan, S., and Fletcher, A.~K.
  (2020).
\newblock Generalization error of generalized linear models in high dimensions.
\newblock {\em arXiv preprint arXiv:2005.00180}.

\bibitem[Friedrich et~al., 2017]{friedrich2017fast}
Friedrich, J., Zhou, P., and Paninski, L. (2017).
\newblock Fast online deconvolution of calcium imaging data.
\newblock {\em PLoS computational biology}, 13(3):e1005423.

\bibitem[Hastie et~al., 2019]{hastie2019surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J. (2019).
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}.

\bibitem[Huang et~al., 2018]{DBLP:journals/corr/abs-1811-06965}
Huang, Y., Cheng, Y., Chen, D., Lee, H., Ngiam, J., Le, Q.~V., and Chen, Z.
  (2018).
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock {\em CoRR}, abs/1811.06965.

\bibitem[Jacot et~al., 2018]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580.

\bibitem[Kalimeris et~al., 2019]{kalimeris2019sgd}
Kalimeris, D., Kaplun, G., Nakkiran, P., Edelman, B., Yang, T., Barak, B., and
  Zhang, H. (2019).
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3496--3506.

\bibitem[Li et~al., 2020]{li2020gradient}
Li, M., Soltanolkotabi, M., and Oymak, S. (2020).
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4313--4324. PMLR.

\bibitem[Li and Liang, 2018]{li2018learning}
Li, Y. and Liang, Y. (2018).
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8157--8166.

\bibitem[Li et~al., 2019]{li2019enhanced}
Li, Z., Wang, R., Yu, D., Du, S.~S., Hu, W., Salakhutdinov, R., and Arora, S.
  (2019).
\newblock Enhanced convolutional neural tangent kernels.
\newblock {\em arXiv preprint arXiv:1911.00809}.

\bibitem[Liang et~al., 2020]{liang2020just}
Liang, T., Rakhlin, A., et~al. (2020).
\newblock Just interpolate: Kernel “ridgeless” regression can generalize.
\newblock {\em Annals of Statistics}, 48(3):1329--1347.

\bibitem[Liao et~al., 2020]{liao2020random}
Liao, Z., Couillet, R., and Mahoney, M.~W. (2020).
\newblock A random matrix analysis of random fourier features: beyond the
  gaussian kernel, a precise phase transition, and the corresponding double
  descent.
\newblock {\em arXiv preprint arXiv:2006.05013}.

\bibitem[Maleki et~al., 2013]{maleki2013asymptotic}
Maleki, A., Anitori, L., Yang, Z., and Baraniuk, R.~G. (2013).
\newblock Asymptotic analysis of complex lasso via complex approximate message
  passing (camp).
\newblock {\em IEEE Transactions on Information Theory}, 59(7):4290--4308.

\bibitem[McNally et~al., 1999]{mcnally1999three}
McNally, J.~G., Karpova, T., Cooper, J., and Conchello, J.~A. (1999).
\newblock Three-dimensional imaging by deconvolution microscopy.
\newblock {\em Methods}, 19(3):373--385.

\bibitem[Mei and Montanari, 2019]{mei2019generalization}
Mei, S. and Montanari, A. (2019).
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}.

\bibitem[Montanari et~al., 2019]{montanari2019generalization}
Montanari, A., Ruan, F., Sohn, Y., and Yan, J. (2019).
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock {\em arXiv preprint arXiv:1911.01544}.

\bibitem[Mueller, 1985]{mueller1985source}
Mueller, C.~S. (1985).
\newblock Source pulse enhancement by deconvolution of an empirical green's
  function.
\newblock {\em Geophysical Research Letters}, 12(1):33--36.

\bibitem[Nakkiran et~al., 2019]{nakkiran2019deep}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
  (2019).
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock {\em arXiv preprint arXiv:1912.02292}.

\bibitem[Oymak and Soltanolkotabi, 2019]{oymak2019overparameterized}
Oymak, S. and Soltanolkotabi, M. (2019).
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In {\em International Conference on Machine Learning}, pages
  4951--4960.

\bibitem[Pandit et~al., 2020]{pandit2020inference}
Pandit, P., Sahraee-Ardakan, M., Rangan, S., Schniter, P., and Fletcher, A.~K.
  (2020).
\newblock Inference with deep generative priors in high dimensions.
\newblock {\em IEEE Journal on Selected Areas in Information Theory}.

\bibitem[Peligrad et~al., 2010]{peligrad2010central}
Peligrad, M., Wu, W.~B., et~al. (2010).
\newblock Central limit theorem for fourier transforms of stationary processes.
\newblock {\em The Annals of Probability}, 38(5):2009--2022.

\bibitem[Rangan et~al., 2019]{rangan2019vector}
Rangan, S., Schniter, P., and Fletcher, A.~K. (2019).
\newblock Vector approximate message passing.
\newblock {\em IEEE Transactions on Information Theory}, 65(10):6664--6684.

\bibitem[Soltanolkotabi et~al., 2018]{soltanolkotabi2018theoretical}
Soltanolkotabi, M., Javanmard, A., and Lee, J.~D. (2018).
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 65(2):742--769.

\bibitem[Starck et~al., 2002]{starck2002deconvolution}
Starck, J.-L., Pantin, E., and Murtagh, F. (2002).
\newblock Deconvolution in astronomy: A review.
\newblock {\em Publications of the Astronomical Society of the Pacific},
  114(800):1051.

\bibitem[Treitel and Lines, 1982]{treitel1982linear}
Treitel, S. and Lines, L. (1982).
\newblock Linear inverse theory and deconvolution.
\newblock {\em Geophysics}, 47(8):1153--1159.

\bibitem[Ulyanov et~al., 2018]{ulyanov2018deep}
Ulyanov, D., Vedaldi, A., and Lempitsky, V. (2018).
\newblock Deep image prior.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9446--9454.

\bibitem[Villani, 2008]{villani2008optimal}
Villani, C. (2008).
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media.

\bibitem[Yang, 2020]{yang2020tensor}
Yang, G. (2020).
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock {\em arXiv preprint arXiv:2006.14548}.

\bibitem[Zhang et~al., 2016]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2016).
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}.

\bibitem[Zou et~al., 2020]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q. (2020).
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock {\em Machine Learning}, 109(3):467--492.

\end{thebibliography}
