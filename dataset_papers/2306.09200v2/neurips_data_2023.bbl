\begin{thebibliography}{10}

\bibitem{playwright}
Playwright.
\newblock \url{https://github.com/microsoft/playwright}.

\bibitem{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
  David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
  et~al.
\newblock {Do as I can, not as I say: Grounding language in robotic
  affordances}.
\newblock {\em arXiv preprint arXiv:2204.01691}, 2022.

\bibitem{gpt4all}
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy
  Mulyar.
\newblock {GPT4All: Training an Assistant-style Chatbot with Large Scale Data
  Distillation from GPT-3.5-Turbo}.
\newblock \url{https://github.com/nomic-ai/gpt4all}, 2023.

\bibitem{baker2022video}
Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien
  Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online
  videos.
\newblock {\em Advances in Neural Information Processing Systems},
  35:24639--24654, 2022.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
  O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai
  Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der
  Wal.
\newblock {Pythia: A Suite for Analyzing Large Language Models Across Training
  and Scaling}, 2023.

\bibitem{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock {GPT-NeoX-20B: An Open-Source Autoregressive Language Model}.
\newblock {\em arXiv preprint arXiv:2204.06745}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock {Sparks of artificial general intelligence: Early experiments with
  GPT-4}.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{campbell2002DeepBlue}
Murray Campbell, A.~Joseph Hoane, and Feng-hsiung Hsu.
\newblock Deep {{Blue}}.
\newblock {\em Artificial Intelligence}, 134(1):57--83, January 2002.

\bibitem{ccrl}
{CCRL}.
\newblock \url{https://www.computerchess.org.uk/ccrl/}.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems},
  34:15084--15097, 2021.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT
  Quality}, March 2023.

\bibitem{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{dolly2}
Dahoas.
\newblock {D}olly2.
\newblock
  \url{https://huggingface.co/datasets/databricks/databricks-dolly-15k}.

\bibitem{du2023guiding}
Yuqing Du, Olivia Watkins, Zihan Wang, C{\'e}dric Colas, Trevor Darrell, Pieter
  Abbeel, Abhishek Gupta, and Jacob Andreas.
\newblock Guiding pretraining in reinforcement learning with large language
  models.
\newblock {\em arXiv preprint arXiv:2302.06692}, 2023.

\bibitem{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, 2022.

\bibitem{fan2022minedojo}
Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,
  Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.
\newblock {Minedojo: Building open-ended embodied agents with internet-scale
  knowledge}.
\newblock {\em arXiv preprint arXiv:2206.08853}, 2022.

\bibitem{wikidump}
Wikimedia Foundation.
\newblock {Wikimedia Downloads}.
\newblock \url{https://dumps.wikimedia.org}.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep {{Residual Learning}} for {{Image Recognition}}.
\newblock In {\em 2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern
  Recognition}} ({{CVPR}})}, pages 770--778, {Las Vegas, NV, USA}, June 2016.
  {IEEE}.

\bibitem{hong2023metagpt}
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang,
  Steven Ka~Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et~al.
\newblock Metagpt: Meta programming for multi-agent collaborative framework.
\newblock {\em arXiv preprint arXiv:2308.00352}, 2023.

\bibitem{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock {Offline reinforcement learning as one big sequence modeling
  problem}.
\newblock {\em Advances in neural information processing systems},
  34:1273--1286, 2021.

\bibitem{safe_rlhf}
Dai Juntao, Ji~Jiaming, Pan Xuehai, Sun Ruiyang, Wang Yizhou, and Yang Yaodong.
\newblock {Constrained Value-Aligned LLM via Safe RLHF}, May 2023.

\bibitem{kopf2023openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis,
  Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver
  Stanley, Rich{\'a}rd Nagyfi, et~al.
\newblock {OpenAssistant Conversations--Democratizing Large Language Model
  Alignment}.
\newblock {\em arXiv preprint arXiv:2304.07327}, 2023.

\bibitem{bigscience2022roots}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki,
  Albert~Villanova del Moral, Teven~Le Scao, Leandro~Von Werra, Chenghao Mou,
  Eduardo~Gonz{\'a}lez Ponferrada, Huu Nguyen, J{\"o}rg Frohberg, Mario
  {\v{S}}a{\v{s}}ko, Quentin Lhoest, Angelina McMillan-Major, G{\'e}rard
  Dupont, Stella Biderman, Anna Rogers, Loubna~Ben allal, Francesco~De Toni,
  Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre
  Colombo, Javier de~la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre,
  Sebastian Nagel, Leon Weber, Manuel~Romero Mu{\~n}oz, Jian Zhu, Daniel~Van
  Strien, Zaid Alyafeai, Khalid Almubarak, Vu~Minh Chien, Itziar Gonzalez-Dios,
  Aitor Soroa, Kyle Lo, Manan Dey, Pedro~Ortiz Suarez, Aaron Gokaslan, Shamik
  Bose, David~Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny
  Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and
  Yacine Jernite.
\newblock The bigscience {ROOTS} corpus: A 1.6{TB} composite multilingual
  dataset.
\newblock In {\em Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem{LeelaChessZero}
Leela {{Chess Zero}}.
\newblock \url{https://lczero.org/}.

\bibitem{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em The Journal of Machine Learning Research}, 17(1):1334--1373,
  2016.

\bibitem{li2021grounded}
Liunian~Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan
  Li, Yiwu Zhong, Lijuan Wang, Lu~Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei
  Chang, and Jianfeng Gao.
\newblock Grounded language-image pre-training.
\newblock In {\em CVPR}, 2022.

\bibitem{opening}
Lichess chess opening names.
\newblock \url{https://github.com/lichess-org/chess-openings}.

\bibitem{lichess}
{Lichess Developers}.
\newblock Lichess.
\newblock \url{https://lichess.org/}.

\bibitem{liu2023agentbench}
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu~Gu,
  Hangliang Ding, Kaiwen Men, Kejuan Yang, et~al.
\newblock Agentbench: Evaluating llms as agents.
\newblock {\em arXiv preprint arXiv:2308.03688}, 2023.

\bibitem{mcgrathAcquisitionChessKnowledge2022}
Thomas McGrath, Andrei Kapishnikov, Nenad Toma{\v s}ev, Adam Pearce, Martin
  Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik.
\newblock Acquisition of chess knowledge in {{AlphaZero}}.
\newblock {\em Proceedings of the National Academy of Sciences},
  119(47):e2206625119, November 2022.

\bibitem{young2020maia}
Reid {McIlroy-Young}, Siddhartha Sen, Jon Kleinberg, and Ashton Anderson.
\newblock Aligning {{Superhuman AI}} with {{Human Behavior}}: {{Chess}} as a
  {{Model System}}.
\newblock In {\em Proceedings of the 26th {{ACM SIGKDD International
  Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}}, {{KDD}} '20,
  pages 1677--1687, {New York, NY, USA}, August 2020. {Association for
  Computing Machinery}.

\bibitem{mishra2021cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock {\em arXiv preprint arXiv:2104.08773}, 2021.

\bibitem{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock {Playing Atari with deep reinforcement learning}.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem{noever2020chess}
David Noever, Matt Ciolino, and Josh Kalin.
\newblock The chess transformer: Mastering play using generative language
  models.
\newblock {\em arXiv preprint arXiv:2008.04057}, 2020.

\bibitem{OrtizSuarezSagotRomary2019}
Pedro~Javier {Ortiz Su{'a}rez}, Benoit Sagot, and Laurent Romary.
\newblock Asynchronous pipelines for processing huge corpora on medium to low
  resource infrastructures.
\newblock In Piotr Bański, Adrien Barbaresi, Hanno Biber, Evelyn Breiteneder,
  Simon Clematide, Marc Kupietz, Harald L{"u}ngen, and Caroline Iliadi,
  editors, {\em Proceedings of the Workshop on Challenges in the Management of
  Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019}, pages 9 -- 16,
  Mannheim, 2019. Leibniz-Institut f{"u}r Deutsche Sprache.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744, 2022.

\bibitem{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock {Instruction Tuning with GPT-4}.
\newblock {\em arXiv preprint arXiv:2304.03277}, 2023.

\bibitem{perez2018film}
Ethan Perez, Florian Strub, Harm {de Vries}, Vincent Dumoulin, and Aaron
  Courville.
\newblock {{FiLM}}: Visual reasoning with a general conditioning layer.
\newblock In {\em Proceedings of the {{Thirty-Second AAAI Conference}} on
  {{Artificial Intelligence}} and {{Thirtieth Innovative Applications}} of
  {{Artificial Intelligence Conference}} and {{Eighth AAAI Symposium}} on
  {{Educational Advances}} in {{Artificial Intelligence}}},
  {{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18, pages 3942--3951, {New Orleans,
  Louisiana, USA}, February 2018. {AAAI Press}.

\bibitem{python_chess}
python-chess: a chess library for python.
\newblock \url{https://github.com/niklasf/python-chess}.

\bibitem{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning {{Transferable Visual Models From Natural Language
  Supervision}}, February 2021.

\bibitem{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv e-prints}, 2019.

\bibitem{reed2022generalist}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander
  Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
  Jost~Tobias Springenberg, et~al.
\newblock A generalist agent.
\newblock {\em arXiv preprint arXiv:2205.06175}, 2022.

\bibitem{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al.
\newblock {M}astering {A}tari, {G}o, chess and shogi by planning with a learned
  model.
\newblock {\em Nature}, 588(7839):604--609, 2020.

\bibitem{sharegpt}
{ShareGPT}.
\newblock
  \url{https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}.

\bibitem{silver2018az}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, Timothy Lillicrap, Karen Simonyan, and {Demis Hassabis}.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and {{Go}} through self-play.
\newblock {\em Science (New York, N.Y.)}, 362(6419):1140--1144, 2018.

\bibitem{selenium}
Alexei~Barantsev Simon~Stewart.
\newblock Selenium, March 2023.

\bibitem{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{stockl2021watching}
Andreas St{\"o}ckl.
\newblock Watching a language model learning chess.
\newblock In {\em Proceedings of the International Conference on Recent
  Advances in Natural Language Processing (RANLP 2021)}, pages 1369--1379,
  2021.

\bibitem{moss}
Tianxiang Sun and Xipeng Qiu.
\newblock Moss, March 2023.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock {Stanford Alpaca: An Instruction-following LLaMA model}.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{together2023redpajama}
{Together Computer}.
\newblock {RedPajama: An Open Source Recipe to Reproduce LLaMA training
  dataset}.
\newblock \url{https://github.com/togethercomputer/RedPajama-Data}, april 2023.

\bibitem{toshniwal2022chess}
Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel.
\newblock {Chess as a Testbed for Language Model State Tracking}.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock {Llama: Open and efficient foundation language models}.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{turing1953digital}
Alan Turing.
\newblock Digital computers applied to games.
\newblock {\em Faster than thought}, 1953.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,
  Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock {\em arXiv preprint arXiv: Arxiv-2305.16291}, 2023.

\bibitem{selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock {Self-Instruct: Aligning Language Model with Self Generated
  Instructions}, 2022.

\bibitem{wang2023describe}
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang.
\newblock Describe, explain, plan and select: Interactive planning with large
  language models enables open-world multi-task agents.
\newblock {\em arXiv preprint arXiv:2302.01560}, 2023.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{yang2023auto}
Hui Yang, Sifu Yue, and Yunzhong He.
\newblock Auto-gpt for online decision making: Benchmarks and additional
  opinions.
\newblock {\em arXiv preprint arXiv:2306.02224}, 2023.

\bibitem{yujian2007normalized}
Li~Yujian and Liu Bo.
\newblock A normalized levenshtein distance metric.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  29(6):1091--1095, 2007.

\bibitem{chatglm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock {ChatGLM-6B}, March 2023.

\end{thebibliography}
