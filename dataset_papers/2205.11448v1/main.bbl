\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas
  Heess, and Martin Riedmiller.
\newblock Maximum a posteriori policy optimisation, 2018.

\bibitem[Galashov et~al.(2019)Galashov, Jayakumar, Hasenclever, Tirumala,
  Schwarz, Desjardins, Czarnecki, Teh, Pascanu, and
  Heess]{galashov2019information}
Alexandre Galashov, Siddhant~M. Jayakumar, Leonard Hasenclever, Dhruva
  Tirumala, Jonathan Schwarz, Guillaume Desjardins, Wojciech~M. Czarnecki,
  Yee~Whye Teh, Razvan Pascanu, and Nicolas Heess.
\newblock Information asymmetry in kl-regularized rl, 2019.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, G{\'o}mez, Zolna,
  Agarwal, Merel, Mankowitz, Paduraru, et~al.]{gulcehre2020rl}
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio G{\'o}mez,
  Konrad Zolna, Rishabh Agarwal, Josh~S Merel, Daniel~J Mankowitz, Cosmin
  Paduraru, et~al.
\newblock Rl unplugged: A collection of benchmarks for offline reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth-Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli, Henderson, Novikov,
  Colmenarejo, Cabi, Gulcehre, Paine, Cowie, Wang, Piot, and
  de~Freitas]{hoffman2020acme}
Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal
  Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate
  Baumli, Sarah Henderson, Alex Novikov, Sergio~Gómez Colmenarejo, Serkan
  Cabi, Caglar Gulcehre, Tom~Le Paine, Andrew Cowie, Ziyu Wang, Bilal Piot, and
  Nando de~Freitas.
\newblock Acme: A research framework for distributed reinforcement learning,
  2020.

\bibitem[Laskey et~al.(2017)Laskey, Lee, Fox, Dragan, and
  Goldberg]{laskey2017dart}
Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan, and Ken Goldberg.
\newblock Dart: Noise injection for robust imitation learning.
\newblock In \emph{Conference on robot learning}, pages 143--156. PMLR, 2017.

\bibitem[Laskin et~al.(2020)Laskin, Lee, Stooke, Pinto, Abbeel, and
  Srinivas]{laskin2020reinforcement}
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and
  Aravind Srinivas.
\newblock Reinforcement learning with augmented data, 2020.

\bibitem[Merel et~al.(2019)Merel, Hasenclever, Galashov, Ahuja, Pham, Wayne,
  Teh, and Heess]{merel2019neural}
Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu~Pham, Greg
  Wayne, Yee~Whye Teh, and Nicolas Heess.
\newblock Neural probabilistic motor primitives for humanoid control, 2019.

\bibitem[Michie and Sammut(1996)]{donald1996bc}
Donald Michie and Claude Sammut.
\newblock \emph{Behavioural Clones and Cognitive Skill Models}, page 387–395.
\newblock Oxford University Press, Inc., USA, 1996.
\newblock ISBN 019853860X.

\bibitem[Mordatch and Todorov(2014)]{mordatch2014combining}
Igor Mordatch and Emo Todorov.
\newblock Combining the benefits of function approximation and trajectory
  optimization.
\newblock In \emph{Robotics: Science and Systems}, volume~4, 2014.

\bibitem[Mordatch et~al.(2015)Mordatch, Lowrey, Andrew, Popovic, and
  Todorov]{mordatch2015interactive}
Igor Mordatch, Kendall Lowrey, Galen Andrew, Zoran Popovic, and Emanuel~V
  Todorov.
\newblock Interactive control of diverse complex characters with neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 3132--3140, 2015.

\bibitem[Parisotto and Salakhutdinov(2021)]{parisotto2021efficient}
Emilio Parisotto and Ruslan Salakhutdinov.
\newblock Efficient transformers in reinforcement learning using actor-learner
  distillation, 2021.

\bibitem[Pomerleau(1989)]{pomerleau1989alvinn}
Dean~A Pomerleau.
\newblock Alvinn: An autonomous land vehicle in a neural network.
\newblock Technical report, Carnegie-Mellon, 1989.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
Stephane Ross, Geoffrey~J. Gordon, and J.~Andrew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning, 2011.

\bibitem[Schmitt et~al.(2018)Schmitt, Hudson, Zidek, Osindero, Doersch,
  Czarnecki, Leibo, Kuttler, Zisserman, Simonyan, and
  Eslami]{schmitt2018kickstarting}
Simon Schmitt, Jonathan~J. Hudson, Augustin Zidek, Simon Osindero, Carl
  Doersch, Wojciech~M. Czarnecki, Joel~Z. Leibo, Heinrich Kuttler, Andrew
  Zisserman, Karen Simonyan, and S.~M.~Ali Eslami.
\newblock Kickstarting deep reinforcement learning, 2018.

\bibitem[Shorten and Khoshgoftaar(2019)]{connor2019}
Connor Shorten and Taghi Khoshgoftaar.
\newblock A survey on image data augmentation for deep learning.
\newblock \emph{Journal of Big Data}, 6, 07 2019.
\newblock \doi{10.1186/s40537-019-0197-0}.

\bibitem[Song et~al.(2019)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae,
  Noury, Ahuja, Liu, Tirumala, Heess, Belov, Riedmiller, and
  Botvinick]{song2019vmpo}
H.~Francis Song, Abbas Abdolmaleki, Jost~Tobias Springenberg, Aidan Clark,
  Hubert Soyer, Jack~W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala,
  Nicolas Heess, Dan Belov, Martin Riedmiller, and Matthew~M. Botvinick.
\newblock V-mpo: On-policy maximum a posteriori policy optimization for
  discrete and continuous control, 2019.

\bibitem[Teh et~al.(2017)Teh, Bapst, Czarnecki, Quan, Kirkpatrick, Hadsell,
  Heess, and Pascanu]{teh2017distral}
Yee~Whye Teh, Victor Bapst, Wojciech~Marian Czarnecki, John Quan, James
  Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.04175}, 2017.

\bibitem[Tirumala et~al.(2020)Tirumala, Galashov, Noh, Hasenclever, Pascanu,
  Schwarz, Desjardins, Czarnecki, Ahuja, Teh, et~al.]{tirumala2020behavior}
Dhruva Tirumala, Alexandre Galashov, Hyeonwoo Noh, Leonard Hasenclever, Razvan
  Pascanu, Jonathan Schwarz, Guillaume Desjardins, Wojciech~Marian Czarnecki,
  Arun Ahuja, Yee~Whye Teh, et~al.
\newblock Behavior priors for efficient reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.14274}, 2020.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem[Tunyasuvunakool et~al.(2020)Tunyasuvunakool, Muldal, Doron, Liu,
  Bohez, Merel, Erez, Lillicrap, Heess, and
  Tassa]{tunyasuvunakool2020dm_control}
Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez,
  Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa.
\newblock dm\_control: Software and tasks for continuous control.
\newblock \emph{Software Impacts}, 6:\penalty0 100022, 2020.

\bibitem[Yarats et~al.(2021)Yarats, Kostrikov, and Fergus]{yarats2021image}
Denis Yarats, Ilya Kostrikov, and Rob Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In \emph{9th International Conference on Learning Representations,
  ICLR}, volume 2021, 2021.

\end{thebibliography}
