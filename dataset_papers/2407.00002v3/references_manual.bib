@misc{michael2024continuous,
      title={{A Continuous Relaxation for Discrete Bayesian Optimization}}, 
      author={Richard Michael and Simon Bartels and Miguel González-Duque and Yevgen Zainchkovskyy and Jes Frellsen and Søren Hauberg and Wouter Boomsma},
      year={2024},
      eprint={2404.17452},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{hellinger_ref,
url = {http://dx.doi.org/10.1561/0100000004},
year = {2004},
volume = {1},
journal = {Foundations and Trends® in Communications and Information Theory},
title = {Information Theory and Statistics: A Tutorial},
doi = {10.1561/0100000004},
issn = {1567-2190},
number = {4},
pages = {417-528},
author = {Imre Csiszár and Paul C. Shields}
}

@misc{gao2023knowledgedesign,
      title={Knowledge-Design: Pushing the Limit of Protein Design via Knowledge Refinement}, 
      author={Zhangyang Gao and Cheng Tan and Stan Z. Li},
      year={2023},
      eprint={2305.15151},
      archivePrefix={arXiv},
      primaryClass={q-bio.BM}
}


@article{10.5555/3454287.3455704,
  title={Generative models for graph-based protein design},
  author={Ingraham, John and Garg, Vikas and Barzilay, Regina and Jaakkola, Tommi},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{
kucera2023proteinshake,
title={ProteinShake: Building datasets and benchmarks for deep learning on protein structures},
author={Tim Kucera and Carlos Oliver and Dexiong Chen and Karsten Borgwardt},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2023},
url={https://openreview.net/forum?id=27vPcG4vKV}
}


@article{abdar_review_2021,
  title = {A Review of Uncertainty Quantification in Deep Learning: {{Techniques}}, Applications and Challenges},
  shorttitle = {A Review of Uncertainty Quantification in Deep Learning},
  author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
  year = {2021},
  month = dec,
  journal = {Information Fusion},
  volume = {76},
  pages = {243--297},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.05.008},
  urldate = {2023-03-01},
  abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
  langid = {english},
  keywords = {Artificial intelligence,Bayesian statistics,Deep learning,Ensemble learning,Machine learning,Uncertainty quantification},
  file = {/Users/pmg/Zotero/storage/CD8R2LDU/Abdar et al. - 2021 - A review of uncertainty quantification in deep lea.pdf;/Users/pmg/Zotero/storage/7CVHJ7XY/S1566253521001081.html}
}

@article{akdel_caretta_2020,
  title = {Caretta {\textendash} {{A}} Multiple Protein Structure Alignment and Feature Extraction Suite},
  author = {Akdel, Mehmet and Durairaj, Janani and {de Ridder}, Dick and {van Dijk}, Aalt D. J.},
  year = {2020},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {18},
  pages = {981--992},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2020.03.011},
  urldate = {2022-09-28},
  abstract = {The vast number of protein structures currently available opens exciting opportunities for machine learning on proteins, aimed at predicting and understanding functional properties. In particular, in combination with homology modelling, it is now possible to not only use sequence features as input for machine learning, but also structure features. However, in order to do so, robust multiple structure alignments are imperative. Here we present Caretta, a multiple structure alignment suite meant for homologous but sequentially divergent protein families which consistently returns accurate alignments with a higher coverage than current state-of-the-art tools. Caretta is available as a GUI and command-line application and additionally outputs an aligned structure feature matrix for a given set of input structures, which can readily be used in downstream steps for supervised or unsupervised machine learning. We show Caretta's performance on two benchmark datasets, and present an example application of Caretta in predicting the conformational state of cyclin-dependent kinases.},
  langid = {english},
  keywords = {Dynamic programming,Machine learning,Protein structure,Structure alignment},
  file = {/Users/pmg/Zotero/storage/KP23UZT7/Akdel et al. - 2020 - Caretta – A multiple protein structure alignment a.pdf;/Users/pmg/Zotero/storage/2GKCXW4C/S2001037019304477.html}
}

@misc{alquraishi_protein_2018,
  title = {Protein {{Linguistics}}},
  author = {AlQuraishi, Mohammed},
  year = {2018},
  month = feb,
  journal = {Some Thoughts on a Mysterious Universe},
  urldate = {2023-09-01},
  abstract = {For over a decade now I have been working, essentially off the grid, on protein folding. I started thinking about the problem during my undergraduate years and actively working on it from the very {\dots}},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/WQU6WD8N/protein-linguistics.html}
}

@article{alquraishi_proteinnet_2019,
  title = {{{ProteinNet}}: A Standardized Data Set for Machine Learning of Protein Structure},
  shorttitle = {{{ProteinNet}}},
  author = {AlQuraishi, Mohammed},
  year = {2019},
  month = jun,
  journal = {BMC Bioinformatics},
  volume = {20},
  number = {1},
  pages = {311},
  issn = {1471-2105},
  doi = {10.1186/s12859-019-2932-0},
  urldate = {2022-07-11},
  abstract = {Rapid progress in deep learning has spurred its application to bioinformatics problems including protein structure prediction and design. In classic machine learning problems like computer vision, progress has been driven by standardized data sets that facilitate fair assessment of new methods and lower the barrier to entry for non-domain experts. While data sets of protein sequence and structure exist, they lack certain components critical for machine learning, including high-quality multiple sequence alignments and insulated training/validation splits that account for deep but only weakly detectable homology across protein space.},
  keywords = {CASP,Co-evolution,Database,Deep learning,Machine learning,Protein sequence,Protein structure,Protein structure prediction,Proteins,PSSM},
  file = {/Users/pmg/Zotero/storage/KUIHPM7E/AlQuraishi - 2019 - ProteinNet a standardized data set for machine le.pdf;/Users/pmg/Zotero/storage/5DQWC9EV/s12859-019-2932-0.html}
}

@misc{alvarez_kernels_2012,
  title = {Kernels for {{Vector-Valued Functions}}: A {{Review}}},
  shorttitle = {Kernels for {{Vector-Valued Functions}}},
  author = {Alvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  year = {2012},
  month = apr,
  number = {arXiv:1106.6251},
  eprint = {1106.6251},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2022-10-04},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a regularization perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a probabilistic perspective they are the key in the context of Gaussian processes, where the kernel function is known as the covariance function. Traditionally, kernel methods have been used in supervised learning problem with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partly by frameworks like multitask learning. In this paper, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/FW3UC647/Alvarez et al. - 2012 - Kernels for Vector-Valued Functions a Review.pdf}
}

@misc{ament_unexpected_2023,
  title = {Unexpected {{Improvements}} to {{Expected Improvement}} for {{Bayesian Optimization}}},
  author = {Ament, Sebastian and Daulton, Samuel and Eriksson, David and Balandat, Maximilian and Bakshy, Eytan},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20708},
  eprint = {2310.20708},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.20708},
  urldate = {2023-12-08},
  abstract = {Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their constrained, noisy, and parallel variants, and propose corresponding reformulations that remedy these pathologies. Our empirical results show that members of the LogEI family of acquisition functions substantially improve on the optimization performance of their canonical counterparts and surprisingly, are on par with or exceed the performance of recent state-of-the-art acquisition functions, highlighting the understated role of numerical optimization in the literature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/6ZPT6GZY/Ament et al. - 2023 - Unexpected Improvements to Expected Improvement fo.pdf;/Users/pmg/Zotero/storage/B2UBKPCI/2310.html}
}


@inproceedings{anand_generative_2018,
  title = {Generative Modeling for Protein Structures},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Anand, Namrata and Huang, Possu},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-04},
  abstract = {Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing structurally plausible solutions.},
  file = {/Users/pmg/Zotero/storage/RV27ULIK/Anand and Huang - 2018 - Generative modeling for protein structures.pdf}
}

@article{anand_protein_2022,
  title = {Protein Sequence Design with a Learned Potential},
  author = {Anand, Namrata and Eguchi, Raphael and Mathews, Irimpan I. and Perez, Carla P. and Derry, Alexander and Altman, Russ B. and Huang, Po-Ssu},
  year = {2022},
  month = feb,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {746},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-28313-9},
  urldate = {2022-10-14},
  abstract = {The task of protein sequence design is central to nearly all rational protein engineering problems, and enormous effort has gone into the development of energy functions to guide design. Here, we investigate the capability of a deep neural network model to automate design of sequences onto protein backbones, having learned directly from crystal structure data and without any human-specified priors. The model generalizes to native topologies not seen during training, producing experimentally stable designs. We evaluate the generalizability of our method to a de novo TIM-barrel scaffold. The model produces novel sequences, and high-resolution crystal structures of two designs show excellent agreement with in silico models. Our findings demonstrate the tractability of an entirely learned method for protein sequence design.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Protein design},
  file = {/Users/pmg/Zotero/storage/EJEGMHFA/Anand et al. - 2022 - Protein sequence design with a learned potential.pdf;/Users/pmg/Zotero/storage/589S3MPW/s41467-022-28313-9.html}
}

@misc{anand_protein_2022-1,
  title = {Protein {{Structure}} and {{Sequence Generation}} with {{Equivariant Denoising Diffusion Probabilistic Models}}},
  author = {Anand, Namrata and Achim, Tudor},
  year = {2022},
  month = may,
  number = {arXiv:2205.15019},
  eprint = {2205.15019},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.15019},
  urldate = {2022-12-06},
  abstract = {Proteins are macromolecules that mediate a significant fraction of the cellular processes that underlie life. An important task in bioengineering is designing proteins with specific 3D structures and chemical properties which enable targeted functions. To this end, we introduce a generative model of both protein structure and sequence that can operate at significantly larger scales than previous molecular generative modeling approaches. The model is learned entirely from experimental data and conditions its generation on a compact specification of protein topology to produce a full-atom backbone configuration as well as sequence and side-chain predictions. We demonstrate the quality of the model via qualitative and quantitative analysis of its samples. Videos of sampling trajectories are available at https://nanand2.github.io/proteins .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/PYTA5UQU/Anand and Achim - 2022 - Protein Structure and Sequence Generation with Equ.pdf;/Users/pmg/Zotero/storage/6FN6GA9K/2205.html}
}

@misc{anand_protein_2022-2,
  title = {Protein {{Structure}} and {{Sequence Generation}} with {{Equivariant Denoising Diffusion Probabilistic Models}}},
  author = {Anand, Namrata and Achim, Tudor},
  year = {2022},
  month = may,
  number = {arXiv:2205.15019},
  eprint = {2205.15019},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.15019},
  urldate = {2023-02-03},
  abstract = {Proteins are macromolecules that mediate a significant fraction of the cellular processes that underlie life. An important task in bioengineering is designing proteins with specific 3D structures and chemical properties which enable targeted functions. To this end, we introduce a generative model of both protein structure and sequence that can operate at significantly larger scales than previous molecular generative modeling approaches. The model is learned entirely from experimental data and conditions its generation on a compact specification of protein topology to produce a full-atom backbone configuration as well as sequence and side-chain predictions. We demonstrate the quality of the model via qualitative and quantitative analysis of its samples. Videos of sampling trajectories are available at https://nanand2.github.io/proteins .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/D525KNJZ/Anand and Achim - 2022 - Protein Structure and Sequence Generation with Equ.pdf;/Users/pmg/Zotero/storage/IKELM9T2/2205.html}
}

@misc{angelopoulos_gentle_2022,
  title = {A {{Gentle Introduction}} to {{Conformal Prediction}} and {{Distribution-Free Uncertainty Quantification}}},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen},
  year = {2022},
  month = dec,
  number = {arXiv:2107.07511},
  eprint = {2107.07511},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07511},
  urldate = {2023-03-01},
  abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/pmg/Zotero/storage/MKP7TSIT/Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf;/Users/pmg/Zotero/storage/PCED4SQM/2107.html}
}

@inproceedings{anonymous_progen2_2022,
  title = {{{ProGen2}}: {{Exploring}} the {{Boundaries}} of {{Protein Language Models}}},
  shorttitle = {{{ProGen2}}},
  booktitle = {Submitted to {{The Eleventh International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2022},
  month = sep,
  urldate = {2022-10-06},
  abstract = {Exploration of billion-scale model and dataset sizes to examine the boundaries of protein language models},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/RBRHA7FZ/Anonymous - 2022 - ProGen2 Exploring the Boundaries of Protein Langu.pdf;/Users/pmg/Zotero/storage/8RLUHE6G/forum.html}
}

@inproceedings{anonymous_protein_2022,
  title = {Protein {{Representation Learning}} by {{Geometric Structure Pretraining}}},
  booktitle = {Submitted to {{The Eleventh International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2022},
  month = sep,
  urldate = {2022-10-06},
  abstract = {In this work, we propose a versatile protein structure encoder GearNet, a superior protein structure pre-trainining algorithm Multiview Contrast and a suite of protein structure pre-training...},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/YSNKDGYC/Anonymous - 2022 - Protein Representation Learning by Geometric Struc.pdf;/Users/pmg/Zotero/storage/H6U374QA/forum.html}
}

@inproceedings{anonymous_protein_2022-1,
  title = {Protein {{Sequence Design}} in a {{Latent Space}} via {{Model-based Reinforcement Learning}}},
  booktitle = {Submitted to {{The Eleventh International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2022},
  month = oct,
  urldate = {2022-11-10},
  abstract = {Proteins are complex molecules responsible for different functions in the human body. Enhancing the functionality of a protein and/or cellular fitness can significantly impact various industries. However, their optimization remains challenging, and sequences generated by data-driven methods often fail in wet lab experiments. This study investigates the limitations of existing model-based sequence design methods and presents a novel optimization framework that can efficiently traverse the latent representation space instead of the protein sequence space. Our framework generates proteins with higher functionality and cellular fitness by modeling the sequence design task as a Markov decision process and applying model-based reinforcement learning. We discuss the results in a comprehensive evaluation of two distinct proteins, GPF and His3, along with the predicted structure of optimized sequences using deep learning-based structure prediction.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/U27XEFBR/forum.html}
}

@inproceedings{anonymous_protein_2022-2,
  title = {Protein {{Sequence Design}} in a {{Latent Space}} via {{Model-based Reinforcement Learning}}},
  booktitle = {Submitted to {{The Eleventh International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2022},
  month = oct,
  urldate = {2022-11-10},
  abstract = {Proteins are complex molecules responsible for different functions in the human body. Enhancing the functionality of a protein and/or cellular fitness can significantly impact various industries. However, their optimization remains challenging, and sequences generated by data-driven methods often fail in wet lab experiments. This study investigates the limitations of existing model-based sequence design methods and presents a novel optimization framework that can efficiently traverse the latent representation space instead of the protein sequence space. Our framework generates proteins with higher functionality and cellular fitness by modeling the sequence design task as a Markov decision process and applying model-based reinforcement learning. We discuss the results in a comprehensive evaluation of two distinct proteins, GPF and His3, along with the predicted structure of optimized sequences using deep learning-based structure prediction.},
  langid = {english}
}

@inproceedings{anonymous_protein_2022-3,
  title = {Protein {{Sequence Design}} in a {{Latent Space}} via {{Model-based Reinforcement Learning}}},
  booktitle = {Submitted to {{The Eleventh International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2022},
  month = oct,
  urldate = {2022-11-10},
  abstract = {Proteins are complex molecules responsible for different functions in the human body. Enhancing the functionality of a protein and/or cellular fitness can significantly impact various industries. However, their optimization remains challenging, and sequences generated by data-driven methods often fail in wet lab experiments. This study investigates the limitations of existing model-based sequence design methods and presents a novel optimization framework that can efficiently traverse the latent representation space instead of the protein sequence space. Our framework generates proteins with higher functionality and cellular fitness by modeling the sequence design task as a Markov decision process and applying model-based reinforcement learning. We discuss the results in a comprehensive evaluation of two distinct proteins, GPF and His3, along with the predicted structure of optimized sequences using deep learning-based structure prediction.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/JD2JCDDN/Anonymous - 2022 - Protein Sequence Design in a Latent Space via Mode.pdf;/Users/pmg/Zotero/storage/8YXJPH7N/forum.html}
}

@misc{arpit_closer_2017,
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrz{\k e}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and {Lacoste-Julien}, Simon},
  year = {2017},
  month = jul,
  number = {arXiv:1706.05394},
  eprint = {1706.05394},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.05394},
  urldate = {2023-01-24},
  abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/IWS397MY/Arpit et al. - 2017 - A Closer Look at Memorization in Deep Networks.pdf;/Users/pmg/Zotero/storage/9ZJGI5SF/1706.html}
}

@misc{austin_structured_2021,
  title = {Structured {{Denoising Diffusion Models}} in {{Discrete State-Spaces}}},
  author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03006},
  eprint = {2107.03006},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.03006},
  urldate = {2022-12-14},
  abstract = {Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/WLNRCPJV/Austin et al. - 2021 - Structured Denoising Diffusion Models in Discrete .pdf;/Users/pmg/Zotero/storage/JA2V3K44/2107.html}
}

@article{authors_protein_nodate,
  title = {Protein {{Sequence Design}} in a {{Latent Space}} via {{Model-based Reinforcement Learning}}},
  author = {Authors, Anonymous},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/7NKDPGVS/Authors - Protein Sequence Design in a Latent Space via Mode.pdf}
}

@article{authors_protein_nodate-1,
  title = {Protein {{Sequence Design}} in a {{Latent Space}} via {{Model-based Reinforcement Learning}}},
  author = {Authors, Anonymous},
  langid = {english},
  file = {/Users/pmg/Downloads/6246.pdf}
}

@article{baek_accurate_2021,
  title = {Accurate Prediction of Protein Structures and Interactions Using a Three-Track Neural Network},
  author = {Baek, Minkyung and DiMaio, Frank and Anishchenko, Ivan and Dauparas, Justas and Ovchinnikov, Sergey and Lee, Gyu Rie and Wang, Jue and Cong, Qian and Kinch, Lisa N. and Schaeffer, R. Dustin and Mill{\'a}n, Claudia and Park, Hahnbeom and Adams, Carson and Glassman, Caleb R. and DeGiovanni, Andy and Pereira, Jose H. and Rodrigues, Andria V. and {van Dijk}, Alberdina A. and Ebrecht, Ana C. and Opperman, Diederik J. and Sagmeister, Theo and Buhlheller, Christoph and {Pavkov-Keller}, Tea and Rathinaswamy, Manoj K. and Dalwadi, Udit and Yip, Calvin K. and Burke, John E. and Garcia, K. Christopher and Grishin, Nick V. and Adams, Paul D. and Read, Randy J. and Baker, David},
  year = {2021},
  month = aug,
  journal = {Science},
  volume = {373},
  number = {6557},
  pages = {871--876},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.abj8754},
  urldate = {2024-02-01},
  abstract = {DeepMind presented notably accurate predictions at the recent 14th Critical Assessment of Structure Prediction (CASP14) conference. We explored network architectures that incorporate related ideas and obtained the best performance with a three-track network in which information at the one-dimensional (1D) sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging x-ray crystallography and cryo{\textendash}electron microscopy structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short-circuiting traditional approaches that require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.},
  file = {/Users/pmg/Zotero/storage/AD5NJYH5/Baek et al. - 2021 - Accurate prediction of protein structures and inte.pdf}
}

@article{baldassarre_graphqa_2021,
  title = {{{GraphQA}}: Protein Model Quality Assessment Using Graph Convolutional Networks},
  shorttitle = {{{GraphQA}}},
  author = {Baldassarre, Federico and Men{\'e}ndez Hurtado, David and Elofsson, Arne and Azizpour, Hossein},
  year = {2021},
  month = feb,
  journal = {Bioinformatics},
  volume = {37},
  number = {3},
  pages = {360--366},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btaa714},
  urldate = {2022-04-07},
  abstract = {Proteins are ubiquitous molecules whose function in biological processes is determined by their 3D structure. Experimental identification of a protein's structure can be time-consuming, prohibitively expensive and not always possible. Alternatively, protein folding can be modeled using computational methods, which however are not guaranteed to always produce optimal results. GraphQA is a graph-based method to estimate the quality of protein models, that possesses favorable properties such as representation learning, explicit modeling of both sequential and 3D structure, geometric invariance and computational efficiency.GraphQA performs similarly to state-of-the-art methods despite using a relatively low number of input features. In addition, the graph network structure provides an improvement over the architecture used in ProQ4 operating on the same input features. Finally, the individual contributions of GraphQA components are carefully evaluated.PyTorch implementation, datasets, experiments and link to an evaluation server are available through this GitHub repository: github.com/baldassarreFe/graphqa.Supplementary data are available at Bioinformatics online.},
  file = {/Users/pmg/Zotero/storage/285SGBPF/Baldassarre et al. - 2021 - GraphQA protein model quality assessment using gr.pdf}
}

@misc{balestriero_cookbook_2023,
  title = {A {{Cookbook}} of {{Self-Supervised Learning}}},
  author = {Balestriero, Randall and Ibrahim, Mark and Sobal, Vlad and Morcos, Ari and Shekhar, Shashank and Goldstein, Tom and Bordes, Florian and Bardes, Adrien and Mialon, Gregoire and Tian, Yuandong and Schwarzschild, Avi and Wilson, Andrew Gordon and Geiping, Jonas and Garrido, Quentin and Fernandez, Pierre and Bar, Amir and Pirsiavash, Hamed and LeCun, Yann and Goldblum, Micah},
  year = {2023},
  month = jun,
  number = {arXiv:2304.12210},
  eprint = {2304.12210},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12210},
  urldate = {2023-08-23},
  abstract = {Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/LMEX3R74/Balestriero et al. - 2023 - A Cookbook of Self-Supervised Learning.pdf;/Users/pmg/Zotero/storage/ZY7RP969/2304.html}
}

@misc{banavar_geometrical_2023,
  title = {A Geometrical Framework for Thinking about Proteins},
  author = {Banavar, Jayanth R. and Giacometti, Achille and Hoang, Trinh X. and Maritan, Amos and {\v S}krbi{\'c}, Tatjana},
  year = {2023},
  month = jun,
  number = {arXiv:2306.10853},
  eprint = {2306.10853},
  primaryclass = {cond-mat, physics:physics, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.10853},
  urldate = {2023-07-03},
  abstract = {We present a model, based on symmetry and geometry, for proteins. Using elementary ideas from mathematics and physics, we derive the geometries of discrete helices and sheets. We postulate a compatible solvent-mediated emergent pairwise attraction that assembles these building blocks, while respecting their individual symmetries. Instead of seeking to mimic the complexity of proteins, we look for a simple abstraction of reality that yet captures the essence of proteins. We employ analytic calculations and detailed Monte Carlo simulations to explore some consequences of our theory. The predictions of our approach are in accord with experimental data. Our framework provides a rationalization for understanding the common characteristics of proteins. Our results show that the free energy landscape of a globular protein is pre-sculpted at the backbone level, sequences and functionalities evolve in the fixed backdrop of the folds determined by geometry and symmetry, and that protein structures are unique in being simultaneously characterized by stability, diversity, and sensitivity.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Soft Condensed Matter,Physics - Biological Physics,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/QH294AR8/Banavar et al. - 2023 - A geometrical framework for thinking about protein.pdf;/Users/pmg/Zotero/storage/4VSUQENW/2306.html}
}

@misc{battaglia_relational_2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  number = {arXiv:1806.01261},
  eprint = {1806.01261},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.01261},
  urldate = {2022-08-24},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/UMD843NY/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/Users/pmg/Zotero/storage/VJM6GJUR/1806.html}
}

@misc{beaudry_intuitive_2012,
  title = {An Intuitive Proof of the Data Processing Inequality},
  author = {Beaudry, Normand J. and Renner, Renato},
  year = {2012},
  month = sep,
  number = {arXiv:1107.0740},
  eprint = {1107.0740},
  primaryclass = {quant-ph},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1107.0740},
  urldate = {2023-01-17},
  abstract = {The data processing inequality (DPI) is a fundamental feature of information theory. Informally it states that you cannot increase the information content of a quantum system by acting on it with a local physical operation. When the smooth min-entropy is used as the relevant information measure, then the DPI follows immediately from the definition of the entropy. The DPI for the von Neumann entropy is then obtained by specializing the DPI for the smooth min-entropy by using the quantum asymptotic equipartition property (QAEP). We provide a new, simplified proof of the QAEP and therefore obtain a self-contained proof of the DPI for the von Neumann entropy.},
  archiveprefix = {arxiv},
  keywords = {Quantum Physics},
  file = {/Users/pmg/Zotero/storage/62BK3JJF/Beaudry and Renner - 2012 - An intuitive proof of the data processing inequali.pdf;/Users/pmg/Zotero/storage/F8GQHI9E/1107.html}
}

@article{bedbrook_machine_2019,
  title = {Machine Learning-Guided Channelrhodopsin Engineering Enables Minimally Invasive Optogenetics},
  author = {Bedbrook, Claire N. and Yang, Kevin K. and Robinson, J. Elliott and Mackey, Elisha D. and Gradinaru, Viviana and Arnold, Frances H.},
  year = {2019},
  month = nov,
  journal = {Nature Methods},
  volume = {16},
  number = {11},
  pages = {1176--1184},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0583-8},
  urldate = {2023-10-09},
  abstract = {We engineered light-gated channelrhodopsins (ChRs) whose current strength and light sensitivity enable minimally invasive neuronal circuit interrogation. Current ChR tools applied to the mammalian brain require intracranial surgery for transgene delivery and implantation of fiber-optic cables to produce light-dependent activation of a small volume of tissue. To facilitate expansive optogenetics without the need for invasive implants, our engineering approach leverages the substantial literature of ChR variants to train statistical models for the design of high-performance ChRs. With Gaussian process models trained on a limited experimental set of 102 functionally characterized ChRs, we designed high-photocurrent ChRs with high light sensitivity. Three of these, ChRger1{\textendash}3, enable optogenetic activation of the nervous system via systemic transgene delivery. ChRger2 enables light-induced neuronal excitation without fiber-optic implantation; that is, this opsin enables transcranial optogenetics.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Machine learning,Mouse,Neuroscience,Optogenetics,Protein function predictions},
  file = {/Users/pmg/Zotero/storage/DVWFAPYF/Bedbrook et al. - 2019 - Machine learning-guided channelrhodopsin engineeri.pdf}
}

@article{belkin_reconciling_2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias{\textendash}Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  pages = {15849--15854},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1903070116},
  urldate = {2023-08-23},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias{\textendash}variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias{\textendash}variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ``double-descent'' curve subsumes the textbook U-shaped bias{\textendash}variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  file = {/Users/pmg/Zotero/storage/IIX6XUL7/Belkin et al. - 2019 - Reconciling modern machine-learning practice and t.pdf}
}

@article{benevenuta_antisymmetric_2021,
  title = {An Antisymmetric Neural Network to Predict Free Energy Changes in Protein Variants},
  author = {Benevenuta, S. and Pancotti, C. and Fariselli, P. and Birolo, G. and Sanavia, T.},
  year = {2021},
  month = mar,
  journal = {Journal of Physics D: Applied Physics},
  volume = {54},
  number = {24},
  pages = {245403},
  publisher = {{IOP Publishing}},
  issn = {0022-3727},
  doi = {10.1088/1361-6463/abedfb},
  urldate = {2022-11-10},
  abstract = {The prediction of free energy changes upon protein residue variations is an important application in biophysics and biomedicine. Several methods have been developed to address this problem so far, including physical-based and machine learning models. However, most of the current computational tools, especially data-driven approaches, fail to incorporate the antisymmetric basic thermodynamic principle: a variation from wild-type to a mutated form of the protein structure () and its reverse process () must have opposite values of the free energy difference: . Here, we build a deep neural network system that, by construction, satisfies the antisymmetric properties. We show that the new method (ACDC-NN) achieved comparable or better performance with respect to other state-of-the-art approaches on both direct and reverse variations, making this method suitable for scoring new protein variants preserving the antisymmetry. The code is available at: https://github.com/compbiomed-unito/acdc-nn.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/FXDWDMK3/Benevenuta et al. - 2021 - An antisymmetric neural network to predict free en.pdf}
}

@misc{bengio_representation_2014,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2014},
  month = apr,
  number = {arXiv:1206.5538},
  eprint = {1206.5538},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1206.5538},
  urldate = {2022-08-26},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/7WVNBPSX/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf;/Users/pmg/Zotero/storage/3MRLIFUI/1206.html}
}

@misc{benton_loss_2021,
  title = {Loss {{Surface Simplexes}} for {{Mode Connecting Volumes}} and {{Fast Ensembling}}},
  author = {Benton, Gregory W. and Maddox, Wesley J. and Lotfi, Sanae and Wilson, Andrew Gordon},
  year = {2021},
  month = nov,
  number = {arXiv:2102.13042},
  eprint = {2102.13042},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.13042},
  urldate = {2023-08-23},
  abstract = {With a better understanding of the loss surfaces for multilayer networks, we can build more robust and accurate training procedures. Recently it was discovered that independently trained SGD solutions can be connected along one-dimensional paths of near-constant training loss. In this paper, we show that there are mode-connecting simplicial complexes that form multi-dimensional manifolds of low loss, connecting many independently trained models. Inspired by this discovery, we show how to efficiently build simplicial complexes for fast ensembling, outperforming independently trained deep ensembles in accuracy, calibration, and robustness to dataset shift. Notably, our approach only requires a few training epochs to discover a low-loss simplex, starting from a pre-trained solution. Code is available at https://github.com/g-benton/loss-surface-simplexes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/C5BFTVMZ/Benton et al. - 2021 - Loss Surface Simplexes for Mode Connecting Volumes.pdf;/Users/pmg/Zotero/storage/ILAH8WTH/2102.html}
}

@misc{binois_survey_2022,
  title = {A Survey on High-Dimensional {{Gaussian}} Process Modeling with Application to {{Bayesian}} Optimization},
  author = {Binois, Mickael and Wycoff, Nathan},
  year = {2022},
  month = may,
  number = {arXiv:2111.05040},
  eprint = {2111.05040},
  primaryclass = {math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.05040},
  urldate = {2023-06-22},
  abstract = {Bayesian Optimization, the application of Bayesian function approximation to finding optima of expensive functions, has exploded in popularity in recent years. In particular, much attention has been paid to improving its efficiency on problems with many parameters to optimize. This attention has trickled down to the workhorse of high dimensional BO, high dimensional Gaussian process regression, which is also of independent interest. The great flexibility that the Gaussian process prior implies is a boon when modeling complicated, low dimensional surfaces but simply says too little when dimension grows too large. A variety of structural model assumptions have been tested to tame high dimensions, from variable selection and additive decomposition to low dimensional embeddings and beyond. Most of these approaches in turn require modifications of the acquisition function optimization strategy as well. Here we review the defining structural model assumptions and discuss the benefits and drawbacks of these approaches in practice.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Optimization and Control},
  file = {/Users/pmg/Zotero/storage/Y3D42YZP/Binois and Wycoff - 2022 - A survey on high-dimensional Gaussian process mode.pdf;/Users/pmg/Zotero/storage/8VQRPLFZ/2111.html}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information Science and Statistics},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-31073-2},
  langid = {english},
  lccn = {Q327 .B52 2006},
  keywords = {Machine learning,Pattern perception},
  file = {/Users/pmg/Zotero/storage/Q2S3WEXD/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}

@article{biswas_low-n_2021,
  title = {Low-{{N}} Protein Engineering with Data-Efficient Deep Learning},
  author = {Biswas, Surojit and Khimulya, Grigory and Alley, Ethan C. and Esvelt, Kevin M. and Church, George M.},
  year = {2021},
  month = apr,
  journal = {Nature Methods},
  volume = {18},
  number = {4},
  pages = {389--396},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-021-01100-y},
  urldate = {2023-03-13},
  abstract = {Protein engineering has enormous academic and industrial potential. However, it is limited by the lack of experimental assays that are consistent with the design goal and sufficiently high throughput to find rare, enhanced variants. Here we introduce a machine learning-guided paradigm that can use as few as 24 functionally assayed mutant sequences to build an accurate virtual fitness landscape and screen ten million sequences via in silico directed evolution. As demonstrated in two dissimilar proteins, GFP from Aequorea victoria (avGFP) and E. coli strain TEM-1 {$\beta$}-lactamase, top candidates from a single round are diverse and as active as engineered mutants obtained from previous high-throughput efforts. By distilling information from natural protein sequence landscapes, our model learns a latent representation of `unnaturalness', which helps to guide search away from nonfunctional sequence neighborhoods. Subsequent low-N supervision then identifies improvements to the activity of interest. In sum, our approach enables efficient use of resource-intensive high-fidelity assays without sacrificing throughput, and helps to accelerate engineered proteins into the fermenter, field and clinic.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Machine learning,Protein design},
  file = {/Users/pmg/Zotero/storage/HF486X63/Biswas et al. - 2021 - Low-N protein engineering with data-efficient deep.pdf}
}

@misc{blaabjerg_rapid_2022,
  title = {Rapid Protein Stability Prediction Using Deep Learning Representations},
  author = {Blaabjerg, Lasse M. and Kassem, Maher M. and Good, Lydia L. and Jonsson, Nicolas and Cagiada, Matteo and Johansson, Kristoffer E. and Boomsma, Wouter and Stein, Amelie and {Lindorff-Larsen}, Kresten},
  year = {2022},
  month = aug,
  primaryclass = {New Results},
  pages = {2022.07.14.500157},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.07.14.500157},
  urldate = {2022-11-10},
  abstract = {Predicting the thermodynamic stability of proteins is a common and widely used step in protein engineering, and when elucidating the molecular mechanisms behind evolution and disease. Here, we present RaSP, a method for making rapid and accurate predictions of changes in protein stability by leveraging deep learning representations. RaSP performs on-par with biophysics-based methods and enables saturation mutagenesis stability predictions in less than a second per residue. We use RaSP to calculate {\textasciitilde} 8.8 million stability changes for nearly all single amino acid changes in 1,381 human proteins, and examine variants observed in the human population. We find that variants that are common in the population are substantially depleted for severe destabilization, and that there are substantial differences between benign and pathogenic variants, highlighting the role of protein stability in genetic diseases. RaSP is freely available{\textemdash}including via a Web interface{\textemdash}and enables large-scale analyses of stability in experimental and predicted protein structures.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/D6EM84T3/Blaabjerg et al. - 2022 - Rapid protein stability prediction using deep lear.pdf;/Users/pmg/Zotero/storage/LRXMFSLW/2022.07.14.html}
}

@article{blei_variational_2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  primaryclass = {cs, stat},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  urldate = {2022-12-21},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/B6NA3NUL/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf;/Users/pmg/Zotero/storage/BJ6RWVLT/1601.html}
}

@misc{boca_predicting_2023,
  title = {Predicting Protein Variants with Equivariant Graph Neural Networks},
  author = {Boca, Antonia and Mathis, Simon},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12231},
  eprint = {2306.12231},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.12231},
  urldate = {2023-07-03},
  abstract = {Pre-trained models have been successful in many protein engineering tasks. Most notably, sequence-based models have achieved state-of-the-art performance on protein fitness prediction while structure-based models have been used experimentally to develop proteins with enhanced functions. However, there is a research gap in comparing structure- and sequence-based methods for predicting protein variants that are better than the wildtype protein. This paper aims to address this gap by conducting a comparative study between the abilities of equivariant graph neural networks (EGNNs) and sequence-based approaches to identify promising amino-acid mutations. The results show that our proposed structural approach achieves a competitive performance to sequence-based methods while being trained on significantly fewer molecules. Additionally, we find that combining assay labelled data with structure pre-trained models yields similar trends as with sequence pre-trained models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/MUUHAQXV/Boca and Mathis - 2023 - Predicting protein variants with equivariant graph.pdf;/Users/pmg/Zotero/storage/D2WJA58W/2306.html}
}

@article{boomsma_generative_2008,
  title = {A Generative, Probabilistic Model of Local Protein Structure},
  author = {Boomsma, Wouter and Mardia, Kanti V. and Taylor, Charles C. and {Ferkinghoff-Borg}, Jesper and Krogh, Anders and Hamelryck, Thomas},
  year = {2008},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {105},
  number = {26},
  pages = {8932--8937},
  issn = {0027-8424},
  doi = {10.1073/pnas.0801715105},
  urldate = {2022-11-21},
  abstract = {Despite significant progress in recent years, protein structure prediction maintains its status as one of the prime unsolved problems in computational biology. One of the key remaining challenges is an efficient probabilistic exploration of the structural space that correctly reflects the relative conformational stabilities. Here, we present a fully probabilistic, continuous model of local protein structure in atomic detail. The generative model makes efficient conformational sampling possible and provides a framework for the rigorous analysis of local sequence{\textendash}structure correlations in the native state. Our method represents a significant theoretical and practical improvement over the widely used fragment assembly technique by avoiding the drawbacks associated with a discrete and nonprobabilistic approach.},
  pmcid = {PMC2440424},
  pmid = {18579771},
  file = {/Users/pmg/Zotero/storage/UKMVU6XQ/Boomsma et al. - 2008 - A generative, probabilistic model of local protein.pdf}
}

@article{borgwardt_graph_2020,
  title = {Graph {{Kernels}}: {{State-of-the-Art}} and {{Future Challenges}}},
  shorttitle = {Graph {{Kernels}}},
  author = {Borgwardt, Karsten and Ghisu, Elisabetta and {Llinares-L{\'o}pez}, Felipe and O'Bray, Leslie and Rieck, Bastian},
  year = {2020},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {13},
  number = {5-6},
  eprint = {2011.03854},
  primaryclass = {cs, stat},
  pages = {531--712},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000076},
  urldate = {2023-09-15},
  abstract = {Graph-structured data are an integral part of many application domains, including chemoinformatics, computational biology, neuroimaging, and social network analysis. Over the last two decades, numerous graph kernels, i.e. kernel functions between graphs, have been proposed to solve the problem of assessing the similarity between graphs, thereby making it possible to perform predictions in both classification and regression settings. This manuscript provides a review of existing graph kernels, their applications, software plus data resources, and an empirical comparison of state-of-the-art graph kernels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/K3ULJ7HW/Borgwardt et al. - 2020 - Graph Kernels State-of-the-Art and Future Challen.pdf;/Users/pmg/Zotero/storage/L9TP3CVV/2011.html}
}

@article{bork_predicting_1998,
  title = {Predicting Functions from Protein Sequences--Where Are the Bottlenecks?},
  author = {Bork, P. and Koonin, E. V.},
  year = {1998},
  month = apr,
  journal = {Nature Genetics},
  volume = {18},
  number = {4},
  pages = {313--318},
  issn = {1061-4036},
  doi = {10.1038/ng0498-313},
  abstract = {The exponential growth of sequence data does not necessarily lead to an increase in knowledge about the functions of genes and their products. Prediction of function using comparative sequence analysis is extremely powerful but, if not performed appropriately, may also lead to the creation and propagation of assignment errors. While current homology detection methods can cope with the data flow, the identification, verification and annotation of functional features need to be drastically improved.},
  langid = {english},
  pmid = {9537411},
  keywords = {Amino Acid Sequence,Animals,Computational Biology,{Databases, Factual},Humans,Molecular Sequence Data,Proteins,Sequence Alignment,{Sequence Homology, Amino Acid}},
  file = {/Users/pmg/Zotero/storage/DN8Q9L7M/Bork and Koonin - 1998 - Predicting functions from protein sequences—where .pdf}
}

@misc{borovitskiy_isotropic_2023,
  title = {Isotropic {{Gaussian Processes}} on {{Finite Spaces}} of {{Graphs}}},
  author = {Borovitskiy, Viacheslav and Karimi, Mohammad Reza and Somnath, Vignesh Ram and Krause, Andreas},
  year = {2023},
  month = feb,
  number = {arXiv:2211.01689},
  eprint = {2211.01689},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01689},
  urldate = {2023-10-26},
  abstract = {We propose a principled way to define Gaussian process priors on various sets of unweighted graphs: directed or undirected, with or without loops. We endow each of these sets with a geometric structure, inducing the notions of closeness and symmetries, by turning them into a vertex set of an appropriate metagraph. Building on this, we describe the class of priors that respect this structure and are analogous to the Euclidean isotropic processes, like squared exponential or Mat{\textbackslash}'ern. We propose an efficient computational technique for the ostensibly intractable problem of evaluating these priors' kernels, making such Gaussian processes usable within the usual toolboxes and downstream applications. We go further to consider sets of equivalence classes of unweighted graphs and define the appropriate versions of priors thereon. We prove a hardness result, showing that in this case, exact kernel computation cannot be performed efficiently. However, we propose a simple Monte Carlo approximation for handling moderately sized cases. Inspired by applications in chemistry, we illustrate the proposed techniques on a real molecular property prediction task in the small data regime.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/4NIK7QE4/Borovitskiy et al. - 2023 - Isotropic Gaussian Processes on Finite Spaces of G.pdf;/Users/pmg/Zotero/storage/CQE9AGZW/2211.html}
}

@article{brandes_proteinbert_2022,
  title = {{{ProteinBERT}}: A Universal Deep-Learning Model of Protein Sequence and Function},
  shorttitle = {{{ProteinBERT}}},
  author = {Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal},
  year = {2022},
  month = apr,
  journal = {Bioinformatics (Oxford, England)},
  volume = {38},
  number = {8},
  pages = {2102--2110},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btac020},
  abstract = {SUMMARY: Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data. AVAILABILITY AND IMPLEMENTATION: Code and pretrained model weights are available at https://github.com/nadavbra/protein\_bert. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  langid = {english},
  pmcid = {PMC9386727},
  pmid = {35020807},
  keywords = {Amino Acid Sequence,Deep Learning,Language,Natural Language Processing,Proteins},
  file = {/Users/pmg/Zotero/storage/NJ5R8937/Brandes et al. - 2022 - ProteinBERT a universal deep-learning model of pr.pdf}
}

@inproceedings{brandstetter_geometric_2021,
  title = {Geometric and {{Physical Quantities}} Improve {{E}}(3) {{Equivariant Message Passing}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Brandstetter, Johannes and Hesselink, Rob and van der Pol, Elise and Bekkers, Erik J. and Welling, Max},
  year = {2021},
  month = sep,
  urldate = {2022-04-28},
  abstract = {Including covariant information, such as position, force, velocity or spin is important in many tasks in computational physics and chemistry. We introduce Steerable E(\$3\$) Equivariant Graph Neural...},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/BTVFFH53/Brandstetter et al. - 2021 - Geometric and Physical Quantities improve E(3) Equ.pdf;/Users/pmg/Zotero/storage/XBPEXLSD/forum.html}
}

@article{breiman_statistical_2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}}},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  eprint = {2676681},
  eprinttype = {jstor},
  pages = {199--215},
  abstract = {Thereare twoculturesin the use ofstatisticalmodelingto reachconclusionsfromdata. One assumes thatthe data are generated bya givenstochasticdata model.The otheruses algorithmimc odelsand treatsthe data mechanismas unknownT. he statisticalcommunityhas beencommittetdothealmostexclusiveuse ofdata models.Thiscommitmenthas ledtoirrelevantheoryq,uestionableconclusionsa,nd has kept statisticiansfromworkingon a largerangeofinterestingcurrentproblems.Algorithmimc odelingb, othin theoryand practice,has developed rapidlyin fieldsoutsidestatisticsI.t can be used bothon largecomplex data sets and as a moreaccurateand informativaelternativeto data modelingon smallerdata sets. If our goal as a fieldis to use data to solveproblemst,henwe need to moveawayfromexclusivedependence on data modelsand adopta morediverseset oftools.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/IJ4E6C7T/Breiman - 2001 - Statistical Modeling The Two Cultures.pdf}
}

@misc{bronstein_geometric_2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = may,
  number = {arXiv:2104.13478},
  eprint = {2104.13478},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.13478},
  urldate = {2022-08-24},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/7V7XUS5L/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf;/Users/pmg/Zotero/storage/LZS4DRWS/2104.html}
}

@article{bryant_deep_2021,
  title = {Deep Diversification of an {{AAV}} Capsid Protein by Machine Learning},
  author = {Bryant, Drew H. and Bashir, Ali and Sinai, Sam and Jain, Nina K. and Ogden, Pierce J. and Riley, Patrick F. and Church, George M. and Colwell, Lucy J. and Kelsic, Eric D.},
  year = {2021},
  month = jun,
  journal = {Nature Biotechnology},
  volume = {39},
  number = {6},
  pages = {691--696},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-020-00793-4},
  urldate = {2022-05-03},
  abstract = {Modern experimental technologies can assay large numbers of biological sequences, but engineered protein libraries rarely exceed the sequence diversity of natural protein families. Machine learning (ML) models trained directly on experimental data without biophysical modeling provide one route to accessing the full potential diversity of engineered proteins. Here we apply deep learning to design highly diverse adeno-associated virus\,2 (AAV2) capsid protein variants that remain viable for packaging of a DNA payload. Focusing on a 28-amino acid segment, we generated 201,426\,variants of the AAV2 wild-type (WT) sequence yielding 110,689\,viable engineered capsids, 57,348 of which surpass the average diversity of natural AAV serotype sequences, with 12{\textendash}29\,mutations across this region. Even when trained on limited data, deep neural network models accurately predict capsid viability across diverse variants. This approach unlocks vast areas of functional but previously unreachable sequence space, with many potential applications for the generation of improved viral vectors and protein therapeutics.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Computational models,Machine learning,Protein design},
  file = {/Users/pmg/Zotero/storage/VSP8YER7/Bryant et al. - 2021 - Deep diversification of an AAV capsid protein by m.pdf;/Users/pmg/Zotero/storage/TDSBPKPG/s41587-020-00793-4.html}
}

@misc{burda_importance_2016,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2016},
  month = nov,
  number = {arXiv:1509.00519},
  eprint = {1509.00519},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1509.00519},
  urldate = {2022-06-01},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/2V934KFV/Burda et al. - 2016 - Importance Weighted Autoencoders.pdf;/Users/pmg/Zotero/storage/QT2NXL6X/1509.html}
}

@misc{cai_note_2020,
  title = {A {{Note}} on {{Over-Smoothing}} for {{Graph Neural Networks}}},
  author = {Cai, Chen and Wang, Yusu},
  year = {2020},
  month = jun,
  number = {arXiv:2006.13318},
  eprint = {2006.13318},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.13318},
  urldate = {2022-09-01},
  abstract = {Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results {\textbackslash}cite\{oono2019graph\} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure "expressiveness" of embedding is conceptually clean; it leads to simpler proofs than {\textbackslash}cite\{oono2019graph\} and can handle more non-linearities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/NFVQDPRV/Cai and Wang - 2020 - A Note on Over-Smoothing for Graph Neural Networks.pdf;/Users/pmg/Zotero/storage/ZA7MG4X9/2006.html}
}

@misc{cai_pretrainable_2023,
  title = {Pretrainable {{Geometric Graph Neural Network}} for {{Antibody Affinity Maturation}}},
  author = {Cai, Huiyu and Zhang, Zuobai and Wang, Mingkai and Zhong, Bozitao and Wu, Yanling and Ying, Tianlei and Tang, Jian},
  year = {2023},
  month = dec,
  primaryclass = {New Results},
  pages = {2023.08.10.552845},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.08.10.552845},
  urldate = {2023-12-08},
  abstract = {In the realm of antibody therapeutics development, increasing the binding affinity of an antibody to its target antigen is a crucial task. This paper presents GearBind, a pretrainable deep neural network designed to be effective for in silico affinity maturation. Leveraging multi-level geometric message passing alongside contrastive pretraining on protein structural data, GearBind capably models the complex interplay of atom-level interactions within protein complexes, surpassing previous state-of-the-art approaches on SKEMPI v2 in terms of Pearson correlation, mean absolute error (MAE) and root mean square error (RMSE). In silico experiments elucidate that pretraining helps GearBind become sensitive to mutation-induced binding affinity changes and reflective of amino acid substitution tendency. Using an ensemble model based on pretrained GearBind, we successfully optimize the affinity of CR3022 to the spike (S) protein of the SARS-CoV-2 Omicron strain. Our strategy yields a high success rate with up to 17-fold affinity increase. GearBind proves to be an effective tool in narrowing the search space for in vitro antibody affinity maturation, underscoring the utility of geometric deep learning and adept pre-training in macromolecule interaction modeling.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/W4KMRZCT/Cai et al. - 2023 - Pretrainable Geometric Graph Neural Network for An.pdf}
}

@article{caldararu_systematic_2020,
  title = {Systematic {{Investigation}} of the {{Data Set Dependency}} of {{Protein Stability Predictors}}},
  author = {Caldararu, Octav and Mehra, Rukmankesh and Blundell, Tom L. and Kepp, Kasper P.},
  year = {2020},
  month = oct,
  journal = {Journal of Chemical Information and Modeling},
  volume = {60},
  number = {10},
  pages = {4772--4784},
  publisher = {{American Chemical Society}},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.0c00591},
  urldate = {2022-11-10},
  abstract = {Prediction of protein stability changes caused by mutation is of major importance to protein engineering and for understanding protein misfolding diseases and protein evolution. The major limitation to these applications is the fact that different prediction methods vary substantially in terms of performance for specific proteins; i.e., performance is not transferable from one type of mutation or protein to another. In this study, we investigated the performance and transferability of eight widely used methods. We first constructed a new data set composed of 2647 mutations using strict selection criteria for the experimental data and then defined a variety of subdata sets that are unbiased with respect to various aspects such as mutation type, stabilization extent, structure type, and solvent exposure. Benchmarking the methods against these subdata sets enabled us to systematically investigate how data set biases affect predictor performance. In particular, we use a reduced amino acid alphabet to quantify the bias toward mutation type, which we identify as the major bias in current approaches. Our results show that all prediction methods exhibit large biases, stemming not from failures of the models applied but mostly from the selection biases of experimental data used for training or parametrization. Our identification of these biases and the construction of new mutation-type-balanced data should lead to the development of more balanced and transferable prediction methods in the future.},
  file = {/Users/pmg/Zotero/storage/NVCWJJ9L/Caldararu et al. - 2020 - Systematic Investigation of the Data Set Dependenc.pdf;/Users/pmg/Zotero/storage/CQM7SKAN/acs.jcim.html}
}

@article{callaway_it_2020,
  title = {`{{It}} Will Change Everything': {{DeepMind}}'s {{AI}} Makes Gigantic Leap in Solving Protein Structures},
  shorttitle = {`{{It}} Will Change Everything'},
  author = {Callaway, Ewen},
  year = {2020},
  month = nov,
  journal = {Nature},
  volume = {588},
  number = {7837},
  pages = {203--204},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-020-03348-4},
  urldate = {2022-11-21},
  abstract = {Google's deep-learning program for determining the 3D shapes of proteins stands to transform biology, say scientists.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Drug discovery,Structural biology},
  annotation = {Bandiera\_abtest: a Cg\_type: News Subject\_term: Computational biology and bioinformatics, Structural biology, Drug discovery},
  file = {/Users/pmg/Zotero/storage/YCHNHN8L/Callaway - 2020 - ‘It will change everything’ DeepMind’s AI makes g.pdf;/Users/pmg/Zotero/storage/BLQ8RR4T/d41586-020-03348-4.html}
}

@techreport{carbone_adversarial_2022,
  type = {Preprint},
  title = {Adversarial {{Attacks}} on {{Protein Language Models}}},
  author = {Carbone, Ginevra and Cuturello, Francesca and Bortolussi, Luca and Cazzaniga, Alberto},
  year = {2022},
  month = oct,
  institution = {{Bioinformatics}},
  doi = {10.1101/2022.10.24.513465},
  urldate = {2022-12-06},
  abstract = {Deep Learning models for protein structure prediction, such as AlphaFold2, leverage Transformer architectures and their attention mechanism to capture structural and functional properties of amino acid sequences. Despite the high accuracy of predictions, biologically insignificant perturbations of the input sequences, or even single point mutations, can lead to substantially different 3d structures. On the other hand, protein language models are often insensitive to biologically relevant mutations that induce misfolding or dysfunction (e.g. missense mutations). Precisely, predictions of the 3d coordinates do not reveal the structure-disruptive effect of these mutations. Therefore, there is an evident inconsistency between the biological importance of mutations and the resulting change in structural prediction. Inspired by this problem, we introduce the concept of adversarial perturbation of protein sequences in continuous embedding spaces of protein language models. Our method relies on attention scores to detect the most vulnerable amino acid positions in the input sequences. Adversarial mutations are biologically diverse from their references and are able to significantly alter the resulting 3d structures.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/PBEFLG8E/Carbone et al. - 2022 - Adversarial Attacks on Protein Language Models.pdf}
}

@misc{casp_casp15_2023,
  title = {{{CASP15}}  {{John Jumper}}},
  author = {{CASP}},
  year = {2023},
  month = jan,
  urldate = {2023-01-31}
}

@article{castro_guided_2022,
  title = {Guided {{Generative Protein Design}} Using {{Regularized Transformers}}},
  author = {Castro, Egbert and Godavarthi, Abhinav and Rubinfien, Julian and Givechian, Kevin B. and Bhaskar, Dhananjay and Krishnaswamy, Smita},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.09948 [cs]},
  eprint = {2201.09948},
  primaryclass = {cs},
  urldate = {2022-04-12},
  abstract = {The development of powerful natural language models have increased the ability to learn meaningful representations of protein sequences. In addition, advances in high-throughput mutagenesis, directed evolution, and next-generation sequencing have allowed for the accumulation of large amounts of labeled fitness data. Leveraging these two trends, we introduce Regularized Latent Space Optimization (ReLSO), a deep transformer-based autoencoder which is trained to jointly generate sequences as well as predict fitness. Using ReLSO, we explicitly model the underlying sequence-function landscape of large labeled datasets and optimize within latent space using gradient-based methods. Through regularized prediction heads, ReLSO introduces a powerful protein sequence encoder and novel approach for efficient fitness landscape traversal.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/UZBQV3RH/Castro et al. - 2022 - Guided Generative Protein Design using Regularized.pdf;/Users/pmg/Zotero/storage/EXK9YBF8/2201.html}
}

@article{chatzou_multiple_2016,
  title = {Multiple Sequence Alignment Modeling: Methods and Applications},
  shorttitle = {Multiple Sequence Alignment Modeling},
  author = {Chatzou, Maria and Magis, Cedrik and Chang, Jia-Ming and Kemena, Carsten and Bussotti, Giovanni and Erb, Ionas and Notredame, Cedric},
  year = {2016},
  month = nov,
  journal = {Briefings in Bioinformatics},
  volume = {17},
  number = {6},
  pages = {1009--1023},
  issn = {1467-5463},
  doi = {10.1093/bib/bbv099},
  urldate = {2022-09-22},
  abstract = {This review provides an overview on the development of Multiple sequence alignment (MSA) methods and their main applications. It is focused on progress made over the past decade. The three first sections review recent algorithmic developments for protein, RNA/DNA and genomic alignments. The fourth section deals with benchmarks and explores the relationship between empirical and simulated data, along with the impact on method developments. The last part of the review gives an overview on available MSA local reliability estimators and their dependence on various algorithmic properties of available methods.},
  file = {/Users/pmg/Zotero/storage/KHV3H57N/Chatzou et al. - 2016 - Multiple sequence alignment modeling methods and .pdf;/Users/pmg/Zotero/storage/CBF7MXYI/2606431.html}
}

@article{chen_premps_2020,
  title = {{{PremPS}}: {{Predicting}} the Impact of Missense Mutations on Protein Stability},
  shorttitle = {{{PremPS}}},
  author = {Chen, Yuting and Lu, Haoyu and Zhang, Ning and Zhu, Zefeng and Wang, Shuqin and Li, Minghui},
  year = {2020},
  month = dec,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {12},
  pages = {e1008543},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008543},
  urldate = {2022-11-10},
  abstract = {Computational methods that predict protein stability changes induced by missense mutations have made a lot of progress over the past decades. Most of the available methods however have very limited accuracy in predicting stabilizing mutations because existing experimental sets are dominated by mutations reducing protein stability. Moreover, few approaches could consistently perform well across different test cases. To address these issues, we developed a new computational method PremPS to more accurately evaluate the effects of missense mutations on protein stability. The PremPS method is composed of only ten evolutionary- and structure-based features and parameterized on a balanced dataset with an equal number of stabilizing and destabilizing mutations. A comprehensive comparison of the predictive performance of PremPS with other available methods on nine benchmark datasets confirms that our approach consistently outperforms other methods and shows considerable improvement in estimating the impacts of stabilizing mutations. A protein could have multiple structures available, and if another structure of the same protein is used, the predicted change in stability for structure-based methods might be different. Thus, we further estimated the impact of using different structures on prediction accuracy, and demonstrate that our method performs well across different types of structures except for low-resolution structures and models built based on templates with low sequence identity. PremPS can be used for finding functionally important variants, revealing the molecular mechanisms of functional influences and protein design. PremPS is freely available at https://lilab.jysw.suda.edu.cn/research/PremPS/, which allows to do large-scale mutational scanning and takes about four minutes to perform calculations for a single mutation per protein with {\textasciitilde} 300 residues and requires {\textasciitilde} 0.4 seconds for each additional mutation.},
  langid = {english},
  keywords = {Electron cryo-microscopy,Molecular structure,Mutation detection,Point mutation,Protein structure,Protein structure prediction,Reverse mutation,Structural proteins},
  file = {/Users/pmg/Zotero/storage/R49Q9223/Chen et al. - 2020 - PremPS Predicting the impact of missense mutation.pdf;/Users/pmg/Zotero/storage/FQCFUKE2/article.html}
}

@misc{chen_simple_2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  number = {arXiv:2002.05709},
  eprint = {2002.05709},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.05709},
  urldate = {2022-11-18},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/Q4KNUQQP/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;/Users/pmg/Zotero/storage/Z5TLXT85/2002.html}
}

@article{chen_structure-aware_2021,
  title = {Structure-Aware Protein Solubility Prediction from Sequence through Graph Convolutional Network and Predicted Contact Map},
  author = {Chen, Jianwen and Zheng, Shuangjia and Zhao, Huiying and Yang, Yuedong},
  year = {2021},
  month = feb,
  journal = {Journal of Cheminformatics},
  volume = {13},
  number = {1},
  pages = {7},
  issn = {1758-2946},
  doi = {10.1186/s13321-021-00488-1},
  urldate = {2022-10-31},
  abstract = {Protein solubility is significant in producing new soluble proteins that can reduce the cost of biocatalysts or therapeutic agents. Therefore, a computational model is highly desired to accurately predict protein solubility from the amino acid sequence. Many methods have been developed, but they are mostly based on the one-dimensional embedding of amino acids that is limited to catch spatially structural information. In this study, we have developed a new structure-aware method GraphSol to predict protein solubility by attentive graph convolutional network (GCN), where the protein topology attribute graph was constructed through predicted contact maps only from the sequence. GraphSol was shown to substantially outperform other sequence-based methods. The model was proven to be stable by consistent \$\$\{{\textbackslash}text\{R\}\}\^\{2\}\$\$of 0.48 in both the cross-validation and independent test of the eSOL dataset. To our best knowledge, this is the first study to utilize the GCN for sequence-based protein solubility predictions. More importantly, this architecture could be easily extended to other protein prediction tasks requiring a raw protein sequence.},
  keywords = {Deep learning,Graph neural network,Predicted contact map,Protein solubility prediction},
  file = {/Users/pmg/Zotero/storage/B9BM7TPJ/Chen et al. - 2021 - Structure-aware protein solubility prediction from.pdf;/Users/pmg/Zotero/storage/HPIIYN8I/s13321-021-00488-1.html}
}

@article{cheng_accurate_2023,
  title = {Accurate Proteome-Wide Missense Variant Effect Prediction with {{AlphaMissense}}},
  author = {Cheng, Jun and Novati, Guido and Pan, Joshua and Bycroft, Clare and {\v Z}emgulyt{\.e}, Akvil{\.e} and Applebaum, Taylor and Pritzel, Alexander and Wong, Lai Hong and Zielinski, Michal and Sargeant, Tobias and Schneider, Rosalia G. and Senior, Andrew W. and Jumper, John and Hassabis, Demis and Kohli, Pushmeet and Avsec, {\v Z}iga},
  year = {2023},
  month = sep,
  journal = {Science},
  volume = {381},
  number = {6664},
  pages = {eadg7492},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.adg7492},
  urldate = {2023-10-11},
  abstract = {The vast majority of missense variants observed in the human genome are of unknown clinical significance. We present AlphaMissense, an adaptation of AlphaFold fine-tuned on human and primate variant population frequency databases to predict missense variant pathogenicity. By combining structural context and evolutionary conservation, our model achieves state-of-the-art results across a wide range of genetic and experimental benchmarks, all without explicitly training on such data. The average pathogenicity score of genes is also predictive for their cell essentiality, capable of identifying short essential genes that existing statistical approaches are underpowered to detect. As a resource to the community, we provide a database of predictions for all possible human single amino acid substitutions and classify 89\% of missense variants as either likely benign or likely pathogenic.},
  file = {/Users/pmg/Zotero/storage/FSH92R6P/Cheng et al. - 2023 - Accurate proteome-wide missense variant effect pre.pdf}
}

@article{chicco_advantages_2020,
  title = {The Advantages of the {{Matthews}} Correlation Coefficient ({{MCC}}) over {{F1}} Score and Accuracy in Binary Classification Evaluation},
  author = {Chicco, Davide and Jurman, Giuseppe},
  year = {2020},
  month = jan,
  journal = {BMC Genomics},
  volume = {21},
  number = {1},
  pages = {6},
  issn = {1471-2164},
  doi = {10.1186/s12864-019-6413-7},
  urldate = {2022-08-12},
  abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
  keywords = {Accuracy,Binary classification,Biostatistics,Confusion matrices,Dataset imbalance,F1 score,Genomics,Machine learning,Matthews correlation coefficient},
  file = {/Users/pmg/Zotero/storage/E9QAAHET/Chicco and Jurman - 2020 - The advantages of the Matthews correlation coeffic.pdf;/Users/pmg/Zotero/storage/328PYCQL/s12864-019-6413-7.html}
}

@misc{chu_protein_2023,
  title = {Protein {{Engineering}} for {{Thermostability}} through {{Deep Evolution}}},
  author = {Chu, Huanyu and Tian, Zhenyang and Hu, Lingling and Zhang, Hejian and Chang, Hong and Bai, Jie and Liu, Dingyu and Cheng, Jian and Jiang, Huifeng},
  year = {2023},
  month = may,
  primaryclass = {New Results},
  pages = {2023.05.04.539497},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.05.04.539497},
  urldate = {2023-05-26},
  abstract = {Protein engineering for increased thermostability through iterative mutagenesis and high throughput screening is labor-intensive, expensive and inefficient. Here, we developed a deep evolution (DeepEvo) strategy to engineer protein thermostability through global sequence generation and selection using deep learning models. We firstly constructed a thermostability selector based on a protein language model to extract thermostability-related features in high-dimensional latent spaces of protein sequences with high temperature tolerance. Subsequently, we constructed a variant generator based on a generative adversarial network to create protein sequences containing the desirable function with more than 50\% accuracy. Finally, the generator and selector were utilized to iteratively improve the performance of DeepEvo on the model protein glyceraldehyde-3-phosphate dehydrogenase (G3PDH), whereby 8 highly thermostable variants were obtained from only 30 generated sequences, demonstrating the high efficiency of DeepEvo for the engineering of protein thermostability.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/YUW5XIFR/Chu et al. - 2023 - Protein Engineering for Thermostability through De.pdf}
}

@misc{cia_pyscomotif_2023,
  title = {{{pyScoMotif}}: {{Discovery}} of Similar {{3D}} Structural Motifs across Proteins},
  shorttitle = {{{pyScoMotif}}},
  author = {Cia, Gabriel and Kwasigroch, Jean Marc and Stamatopoulos, Basile and Rooman, Marianne and Pucci, Fabrizio},
  year = {2023},
  month = aug,
  primaryclass = {New Results},
  pages = {2023.08.27.554982},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.08.27.554982},
  urldate = {2023-09-04},
  abstract = {Motivation The fast and accurate detection of similar geometrical arrangements of protein residues, known as 3-dimensional (3D) structural motifs, is highly relevant for many applications such as binding region and catalytic site detection, drug discovery and structure conservation analyses. With the recent publication of new protein structure prediction methods, the number of available protein structures is exploding, which makes efficient and easy-to-use tools for identifying 3D structural motifs essential. Results We present an open-source Python package that enables the search for both exact and mutated motifs with position-specific residue substitutions. The tool is fast, accurate and suitable to run both on computer clusters and personal laptops. Successful application of pyScoMotif to catalytic site identification is showcased. Availability The pyScoMotif package can be installed from the PyPI repository and is also available at https://github.com/3BioCompBio/pyScoMotif. It is free to use for non-commercial purposes; for commercial purposes, please contact us.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/32BYAK5K/Cia et al. - 2023 - pyScoMotif Discovery of similar 3D structural mot.pdf}
}

@article{colvin_pel_2011,
  title = {The {{Pel Polysaccharide Can Serve}} a {{Structural}} and {{Protective Role}} in the {{Biofilm Matrix}} of {{Pseudomonas}} Aeruginosa},
  author = {Colvin, Kelly M. and Gordon, Vernita D. and Murakami, Keiji and Borlee, Bradley R. and Wozniak, Daniel J. and Wong, Gerard C. L. and Parsek, Matthew R.},
  year = {2011},
  month = jan,
  journal = {PLOS Pathogens},
  volume = {7},
  number = {1},
  pages = {e1001264},
  publisher = {{Public Library of Science}},
  issn = {1553-7374},
  doi = {10.1371/journal.ppat.1001264},
  urldate = {2023-01-25},
  abstract = {Bacterial extracellular polysaccharides are a key constituent of the extracellular matrix material of biofilms. Pseudomonas aeruginosa is a model organism for biofilm studies and produces three extracellular polysaccharides that have been implicated in biofilm development, alginate, Psl and Pel. Significant work has been conducted on the roles of alginate and Psl in biofilm development, however we know little regarding Pel. In this study, we demonstrate that Pel can serve two functions in biofilms. Using a novel assay involving optical tweezers, we demonstrate that Pel is crucial for maintaining cell-to-cell interactions in a PA14 biofilm, serving as a primary structural scaffold for the community. Deletion of pelB resulted in a severe biofilm deficiency. Interestingly, this effect is strain-specific. Loss of Pel production in the laboratory strain PAO1 resulted in no difference in attachment or biofilm development; instead Psl proved to be the primary structural polysaccharide for biofilm maturity. Furthermore, we demonstrate that Pel plays a second role by enhancing resistance to aminoglycoside antibiotics. This protection occurs only in biofilm populations. We show that expression of the pel gene cluster and PelF protein levels are enhanced during biofilm growth compared to liquid cultures. Thus, we propose that Pel is capable of playing both a structural and a protective role in P. aeruginosa biofilms.},
  langid = {english},
  keywords = {Antibiotics,Arabinose,Bacterial biofilms,Biofilm culture,Biofilms,Extracellular matrix,Polysaccharides,Pseudomonas aeruginosa},
  file = {/Users/pmg/Zotero/storage/HWIFU6MI/Colvin et al. - 2011 - The Pel Polysaccharide Can Serve a Structural and .pdf}
}

@article{csiszar_information_2004,
  title = {Information {{Theory}} and {{Statistics}}: {{A Tutorial}}},
  shorttitle = {Information {{Theory}} and {{Statistics}}},
  author = {Csisz{\'a}r, I. and Shields, P. C.},
  year = {2004},
  month = dec,
  journal = {Foundations and Trends{\textregistered} in Communications and Information Theory},
  volume = {1},
  number = {4},
  pages = {417--528},
  publisher = {{Now Publishers, Inc.}},
  issn = {1567-2190, 1567-2328},
  doi = {10.1561/0100000004},
  urldate = {2024-02-01},
  abstract = {Information Theory and Statistics: A Tutorial},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/ABURS8AR/Csiszár and Shields - 2004 - Information Theory and Statistics A Tutorial.pdf}
}

  @inproceedings{
dallago_flip_2022,
title={{FLIP}: Benchmark tasks in fitness landscape inference for proteins},
author={Christian Dallago and Jody Mou and Kadina E Johnston and Bruce Wittmann and Nick Bhattacharya and Samuel Goldman and Ali Madani and Kevin K. Yang},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=p2dMLEwL8tF}
}

@article{dallago_learned_2021,
  title = {Learned {{Embeddings}} from {{Deep Learning}} to {{Visualize}} and {{Predict Protein Sets}}},
  author = {Dallago, Christian and Sch{\"u}tze, Konstantin and Heinzinger, Michael and Olenyi, Tobias and Littmann, Maria and Lu, Amy X. and Yang, Kevin K. and Min, Seonwoo and Yoon, Sungroh and Morton, James T. and Rost, Burkhard},
  year = {2021},
  journal = {Current Protocols},
  volume = {1},
  number = {5},
  pages = {e113},
  issn = {2691-1299},
  doi = {10.1002/cpz1.113},
  urldate = {2022-10-31},
  abstract = {Models from machine learning (ML) or artificial intelligence (AI) increasingly assist in guiding experimental design and decision making in molecular biology and medicine. Recently, Language Models (LMs) have been adapted from Natural Language Processing (NLP) to encode the implicit language written in protein sequences. Protein LMs show enormous potential in generating descriptive representations (embeddings) for proteins from just their sequences, in a fraction of the time with respect to previous approaches, yet with comparable or improved predictive ability. Researchers have trained a variety of protein LMs that are likely to illuminate different angles of the protein language. By leveraging the bio\_embeddings pipeline and modules, simple and reproducible workflows can be laid out to generate protein embeddings and rich visualizations. Embeddings can then be leveraged as input features through machine learning libraries to develop methods predicting particular aspects of protein function and structure. Beyond the workflows included here, embeddings have been leveraged as proxies to traditional homology-based inference and even to align similar protein sequences. A wealth of possibilities remain for researchers to harness through the tools provided in the following protocols. {\textcopyright} 2021 The Authors. Current Protocols published by Wiley Periodicals LLC. The following protocols are included in this manuscript: Basic Protocol 1: Generic use of the bio\_embeddings pipeline to plot protein sequences and annotations Basic Protocol 2: Generate embeddings from protein sequences using the bio\_embeddings pipeline Basic Protocol 3: Overlay sequence annotations onto a protein space visualization Basic Protocol 4: Train a machine learning classifier on protein embeddings Alternate Protocol 1: Generate 3D instead of 2D visualizations Alternate Protocol 2: Visualize protein solubility instead of protein subcellular localization Support Protocol: Join embedding generation and sequence space visualization in a pipeline},
  langid = {english},
  keywords = {deep learning embeddings,machine learning,protein annotation pipeline,protein representations,protein visualization},
  file = {/Users/pmg/Zotero/storage/35NGQH7T/Dallago et al. - 2021 - Learned Embeddings from Deep Learning to Visualize.pdf}
}

@misc{daulton_parallel_2021,
  title = {Parallel {{Bayesian Optimization}} of {{Multiple Noisy Objectives}} with {{Expected Hypervolume Improvement}}},
  author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
  year = {2021},
  month = oct,
  number = {arXiv:2105.08195},
  eprint = {2105.08195},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-10-03},
  abstract = {Optimizing multiple competing black-box objectives is a challenging problem in many fields, including science, engineering, and machine learning. Multi-objective Bayesian optimization (MOBO) is a sample-efficient approach for identifying the optimal trade-offs between the objectives. However, many existing methods perform poorly when the observations are corrupted by noise. We propose a novel acquisition function, NEHVI, that overcomes this important practical limitation by applying a Bayesian treatment to the popular expected hypervolume improvement (EHVI) criterion and integrating over this uncertainty in the Pareto frontier. We argue that, even in the noiseless setting, generating multiple candidates in parallel is an incarnation of EHVI with uncertainty in the Pareto frontier and therefore can be addressed using the same underlying technique. Through this lens, we derive a natural parallel variant, qNEHVI, that reduces computational complexity of parallel EHVI from exponential to polynomial with respect to the batch size. qNEHVI is one-step Bayes-optimal for hypervolume maximization in both noisy and noiseless environments, and we show that it can be optimized effectively with gradient-based methods via sample average approximation. Empirically, we demonstrate not only that qNEHVI is substantially more robust to observation noise than existing MOBO approaches, but also that it achieves state-of-the-art optimization performance and competitive wall-times in large-batch environments.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/VGASMXL8/Daulton et al. - 2021 - Parallel Bayesian Optimization of Multiple Noisy O.pdf}
}

@article{dauparas_robust_2022,
  title={{Robust deep learning--based protein sequence design using ProteinMPNN}},
  author={Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Courbet, Alexis and de Haas, Rob J. and Bethel, Neville and others},
  journal={Science},
  volume={378},
  number={6615},
  pages={49--56},
  year={2022},
  publisher={American Association for the Advancement of Science}
}


@article{dauparas_robust_2022-1,
  title = {Robust Deep Learning{\textendash}Based Protein Sequence Design Using {{ProteinMPNN}}},
  author = {Dauparas, J. and Anishchenko, I. and Bennett, N. and Bai, H. and Ragotte, R. J. and Milles, L. F. and Wicky, B. I. M. and Courbet, A. and {de Haas}, R. J. and Bethel, N. and Leung, P. J. Y. and Huddy, T. F. and Pellock, S. and Tischer, D. and Chan, F. and Koepnick, B. and Nguyen, H. and Kang, A. and Sankaran, B. and Bera, A. K. and King, N. P. and Baker, D.},
  year = {2022},
  month = oct,
  journal = {Science},
  volume = {378},
  number = {6615},
  pages = {49--56},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.add2187},
  urldate = {2024-02-01},
  abstract = {Although deep learning has revolutionized protein structure prediction, almost all experimentally characterized de novo protein designs have been generated using physically based approaches such as Rosetta. Here, we describe a deep learning{\textendash}based protein sequence design method, ProteinMPNN, that has outstanding performance in both in silico and experimental tests. On native protein backbones, ProteinMPNN has a sequence recovery of 52.4\% compared with 32.9\% for Rosetta. The amino acid sequence at different positions can be coupled between single or multiple chains, enabling application to a wide range of current protein design challenges. We demonstrate the broad utility and high accuracy of ProteinMPNN using x-ray crystallography, cryo{\textendash}electron microscopy, and functional studies by rescuing previously failed designs, which were made using Rosetta or AlphaFold, of protein monomers, cyclic homo-oligomers, tetrahedral nanoparticles, and target-binding proteins.},
  file = {/Users/pmg/Zotero/storage/3LRNI8PJ/Dauparas et al. - 2022 - Robust deep learning–based protein sequence design.pdf}
}

@article{delgado_foldx_2019,
  title = {{{FoldX}} 5.0: Working with {{RNA}}, Small Molecules and a New Graphical Interface},
  shorttitle = {{{FoldX}} 5.0},
  author = {Delgado, Javier and Radusky, Leandro G and Cianferoni, Damiano and Serrano, Luis},
  year = {2019},
  month = oct,
  journal = {Bioinformatics},
  volume = {35},
  number = {20},
  pages = {4168--4169},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btz184},
  urldate = {2022-10-20},
  abstract = {A new version of FoldX, whose main new features allows running classic FoldX commands on structures containing RNA molecules and includes a module that allows parametrization of ligands or small molecules (ParamX) that were not previously recognized in old versions, has been released. An extended FoldX graphical user interface has also being developed (available as a python plugin for the YASARA molecular viewer) allowing user-friendly parametrization of new custom user molecules encoded using JSON format.http://foldxsuite.crg.eu/},
  file = {/Users/pmg/Zotero/storage/7NDTWAWE/Delgado et al. - 2019 - FoldX 5.0 working with RNA, small molecules and a.pdf;/Users/pmg/Zotero/storage/KTMCBAXA/5381539.html}
}

@article{deorowicz_famsa_2016,
  title = {{{FAMSA}}: {{Fast}} and Accurate Multiple Sequence Alignment of Huge Protein Families},
  shorttitle = {{{FAMSA}}},
  author = {Deorowicz, Sebastian and {Debudaj-Grabysz}, Agnieszka and Gudy{\'s}, Adam},
  year = {2016},
  month = sep,
  journal = {Scientific Reports},
  volume = {6},
  number = {1},
  pages = {33964},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep33964},
  urldate = {2022-09-27},
  abstract = {Rapid development of modern sequencing platforms has contributed to the unprecedented growth of protein families databases. The abundance of sets containing hundreds of thousands of sequences is a formidable challenge for multiple sequence alignment algorithms. The article introduces FAMSA, a new progressive algorithm designed for fast and accurate alignment of thousands of protein sequences. Its features include the utilization of the longest common subsequence measure for determining pairwise similarities, a novel method of evaluating gap costs, and a new iterative refinement scheme. What matters is that its implementation is highly optimized and parallelized to make the most of modern computer platforms. Thanks to the above, quality indicators, i.e. sum-of-pairs and total-column scores, show FAMSA to be superior to competing algorithms, such as Clustal Omega or MAFFT for datasets exceeding a few thousand sequences. Quality does not compromise on time or memory requirements, which are an order of magnitude lower than those in the existing solutions. For example, a family of 415519 sequences was analyzed in less than two hours and required no more than 8\,GB of RAM. FAMSA is available for free at http://sun.aei.polsl.pl/REFRESH/famsa.},
  copyright = {2016 The Author(s)},
  langid = {english},
  keywords = {Protein sequence analyses,Software},
  file = {/Users/pmg/Zotero/storage/THAT3BZV/Deorowicz et al. - 2016 - FAMSA Fast and accurate multiple sequence alignme.pdf;/Users/pmg/Zotero/storage/NR4YFJEK/srep33964.html}
}

@article{detlefsen_learning_2022,
  title = {Learning Meaningful Representations of Protein Sequences},
  author = {Detlefsen, Nicki Skafte and Hauberg, S{\o}ren and Boomsma, Wouter},
  year = {2022},
  month = apr,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {1914},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-29443-w},
  urldate = {2022-04-12},
  abstract = {How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational models,Data mining},
  file = {/Users/pmg/Zotero/storage/KDH7BVK4/Detlefsen et al. - 2022 - Learning meaningful representations of protein seq.pdf;/Users/pmg/Zotero/storage/PJT4ECXP/s41467-022-29443-w.html}
}

@misc{dettmers_qlora_2023,
  title = {{{QLoRA}}: {{Efficient Finetuning}} of {{Quantized LLMs}}},
  shorttitle = {{{QLoRA}}},
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  year = {2023},
  month = may,
  number = {arXiv:2305.14314},
  eprint = {2305.14314},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14314},
  urldate = {2023-12-07},
  abstract = {We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters{\textasciitilde}(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3\% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/8E4GXK5N/Dettmers et al. - 2023 - QLoRA Efficient Finetuning of Quantized LLMs.pdf;/Users/pmg/Zotero/storage/XU49N9SM/2305.html}
}

@misc{devlin_bert_2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2023-06-30},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/pmg/Zotero/storage/Z2985FRH/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/pmg/Zotero/storage/MEPFSWJF/1810.html}
}

@misc{dieckhaus_transfer_2023,
  title = {Transfer Learning to Leverage Larger Datasets for Improved Prediction of Protein Stability Changes},
  author = {Dieckhaus, Henry and Brocidiacono, Michael and Randolph, Nicholas and Kuhlman, Brian},
  year = {2023},
  month = jul,
  primaryclass = {New Results},
  pages = {2023.07.27.550881},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.07.27.550881},
  urldate = {2023-07-31},
  abstract = {Amino acid mutations that lower a protein's thermodynamic stability are implicated in numerous diseases, and engineered proteins with enhanced stability are important in research and medicine. Computational methods for predicting how mutations perturb protein stability are therefore of great interest. Despite recent advancements in protein design using deep learning, in silico prediction of stability changes has remained challenging, in part due to a lack of large, high-quality training datasets for model development. Here we introduce ThermoMPNN, a deep neural network trained to predict stability changes for protein point mutations given an initial structure. In doing so, we demonstrate the utility of a newly released mega-scale stability dataset for training a robust stability model. We also employ transfer learning to leverage a second, larger dataset by using learned features extracted from a deep neural network trained to predict a protein's amino acid sequence given its three-dimensional structure. We show that our method achieves competitive performance on established benchmark datasets using a lightweight model architecture that allows for rapid, scalable predictions. Finally, we make ThermoMPNN readily available as a tool for stability prediction and design.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/IKTUX8FP/Dieckhaus et al. - 2023 - Transfer learning to leverage larger datasets for .pdf}
}

@article{ding_co-evolution_2022,
  title = {Co-Evolution of Interacting Proteins through Non-Contacting and Non-Specific Mutations},
  author = {Ding, David and Green, Anna G. and Wang, Boyuan and Lite, Thuy-Lan Vo and Weinstein, Eli N. and Marks, Debora S. and Laub, Michael T.},
  year = {2022},
  month = may,
  journal = {Nature Ecology \& Evolution},
  volume = {6},
  number = {5},
  pages = {590--603},
  issn = {2397-334X},
  doi = {10.1038/s41559-022-01688-0},
  abstract = {Proteins often accumulate neutral mutations that do not affect current functions but can profoundly influence future mutational possibilities and functions. Understanding such hidden potential has major implications for protein design and evolutionary forecasting but has been limited by a lack of systematic efforts to identify potentiating mutations. Here, through the comprehensive analysis of a bacterial toxin-antitoxin system, we identified all possible single substitutions in the toxin that enable it to tolerate otherwise interface-disrupting mutations in its antitoxin. Strikingly, the majority of enabling mutations in the toxin do not contact and promote tolerance non-specifically to many different antitoxin mutations, despite covariation in homologues occurring primarily between specific pairs of contacting residues across the interface. In addition, the enabling mutations we identified expand future mutational paths that both maintain old toxin-antitoxin interactions and form new ones. These non-specific mutations are missed by widely used covariation and machine learning methods. Identifying such enabling mutations will be critical for ensuring continued binding of therapeutically relevant proteins, such as antibodies, aimed at evolving targets.},
  langid = {english},
  pmcid = {PMC9090974},
  pmid = {35361892},
  keywords = {Amino Acid Sequence,Antitoxins,Bacterial Proteins,Bacterial Toxins,Mutation},
  file = {/Users/pmg/Zotero/storage/G2QAN2V9/Ding et al. - 2022 - Co-evolution of interacting proteins through non-c.pdf}
}

@article{ding_co-evolution_2022-1,
  title = {Co-Evolution of Interacting Proteins through Non-Contacting and Non-Specific Mutations},
  author = {Ding, David and Green, Anna G. and Wang, Boyuan and Lite, Thuy-Lan Vo and Weinstein, Eli N. and Marks, Debora S. and Laub, Michael T.},
  year = {2022},
  month = may,
  journal = {Nature Ecology \& Evolution},
  volume = {6},
  number = {5},
  pages = {590--603},
  issn = {2397-334X},
  doi = {10.1038/s41559-022-01688-0},
  abstract = {Proteins often accumulate neutral mutations that do not affect current functions but can profoundly influence future mutational possibilities and functions. Understanding such hidden potential has major implications for protein design and evolutionary forecasting but has been limited by a lack of systematic efforts to identify potentiating mutations. Here, through the comprehensive analysis of a bacterial toxin-antitoxin system, we identified all possible single substitutions in the toxin that enable it to tolerate otherwise interface-disrupting mutations in its antitoxin. Strikingly, the majority of enabling mutations in the toxin do not contact and promote tolerance non-specifically to many different antitoxin mutations, despite covariation in homologues occurring primarily between specific pairs of contacting residues across the interface. In addition, the enabling mutations we identified expand future mutational paths that both maintain old toxin-antitoxin interactions and form new ones. These non-specific mutations are missed by widely used covariation and machine learning methods. Identifying such enabling mutations will be critical for ensuring continued binding of therapeutically relevant proteins, such as antibodies, aimed at evolving targets.},
  langid = {english},
  pmcid = {PMC9090974},
  pmid = {35361892},
  keywords = {Amino Acid Sequence,Antitoxins,Bacterial Proteins,Bacterial Toxins,Mutation},
  file = {/Users/pmg/Zotero/storage/TL2YFGA5/Ding et al. - 2022 - Co-evolution of interacting proteins through non-c.pdf}
}

@article{ding_co-evolution_2022-2,
  title = {Co-Evolution of Interacting Proteins through Non-Contacting and Non-Specific Mutations},
  author = {Ding, David and Green, Anna G. and Wang, Boyuan and Lite, Thuy-Lan Vo and Weinstein, Eli N. and Marks, Debora S. and Laub, Michael T.},
  year = {2022},
  month = may,
  journal = {Nature Ecology \& Evolution},
  volume = {6},
  number = {5},
  pages = {590--603},
  publisher = {{Nature Publishing Group}},
  issn = {2397-334X},
  doi = {10.1038/s41559-022-01688-0},
  urldate = {2023-10-24},
  abstract = {Proteins often accumulate neutral mutations that do not affect current functions but can profoundly influence future mutational possibilities and functions. Understanding such hidden potential has major implications for protein design and evolutionary forecasting but has been limited by a lack of systematic efforts to identify potentiating mutations. Here, through the comprehensive analysis of a bacterial toxin{\textendash}antitoxin system, we identified all possible single substitutions in the toxin that enable it to tolerate otherwise interface-disrupting mutations in its antitoxin. Strikingly, the majority of enabling mutations in the toxin do not contact and promote tolerance non-specifically to many different antitoxin mutations, despite covariation in homologues occurring primarily between specific pairs of contacting residues across the interface. In addition, the enabling mutations we identified expand future mutational paths that both maintain old toxin{\textendash}antitoxin interactions and form new ones. These non-specific mutations are missed by widely used covariation and machine learning methods. Identifying such enabling mutations will be critical for ensuring continued binding of therapeutically relevant proteins, such as antibodies, aimed at evolving targets.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Evolution,Genetics},
  file = {/Users/pmg/Zotero/storage/GIG86GUC/Ding et al. - 2022 - Co-evolution of interacting proteins through non-c.pdf}
}

@article{domingos_few_2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  year = {2012},
  month = oct,
  journal = {Communications of the ACM},
  volume = {55},
  number = {10},
  pages = {78--87},
  issn = {0001-0782},
  doi = {10.1145/2347736.2347755},
  urldate = {2022-06-09},
  abstract = {Tapping into the "folk knowledge" needed to advance machine learning applications.},
  file = {/Users/pmg/Zotero/storage/GD4L3WWW/Domingos - 2012 - A few useful things to know about machine learning.pdf}
}

@article{dong_pybiomed_2018,
  title = {{{PyBioMed}}: A Python Library for Various Molecular Representations of Chemicals, Proteins and {{DNAs}} and Their Interactions},
  shorttitle = {{{PyBioMed}}},
  author = {Dong, Jie and Yao, Zhi-Jiang and Zhang, Lin and Luo, Feijun and Lin, Qinlu and Lu, Ai-Ping and Chen, Alex F. and Cao, Dong-Sheng},
  year = {2018},
  month = mar,
  journal = {Journal of Cheminformatics},
  volume = {10},
  number = {1},
  pages = {16},
  issn = {1758-2946},
  doi = {10.1186/s13321-018-0270-2},
  abstract = {BACKGROUND: With the increasing development of biotechnology and informatics technology, publicly available data in chemistry and biology are undergoing explosive growth. Such wealthy information in these data needs to be extracted and transformed to useful knowledge by various data mining methods. Considering the amazing rate at which data are accumulated in chemistry and biology fields, new tools that process and interpret large and complex interaction data are increasingly important. So far, there are no suitable toolkits that can effectively link the chemical and biological space in view of molecular representation. To further explore these complex data, an integrated toolkit for various molecular representation is urgently needed which could be easily integrated with data mining algorithms to start a full data analysis pipeline. RESULTS: Herein, the python library PyBioMed is presented, which comprises functionalities for online download for various molecular objects by providing different IDs, the pretreatment of molecular structures, the computation of various molecular descriptors for chemicals, proteins, DNAs and their interactions. PyBioMed is a feature-rich and highly customized python library used for the characterization of various complex chemical and biological molecules and interaction samples. The current version of PyBioMed could calculate 775 chemical descriptors and 19 kinds of chemical fingerprints, 9920 protein descriptors based on protein sequences, more than 6000 DNA descriptors from nucleotide sequences, and interaction descriptors from pairwise samples using three different combining strategies. Several examples and five real-life applications were provided to clearly guide the users how to use PyBioMed as an integral part of data analysis projects. By using PyBioMed, users are able to start a full pipelining from getting molecular data, pretreating molecules, molecular representation to constructing machine learning models conveniently. CONCLUSION: PyBioMed provides various user-friendly and highly customized APIs to calculate various features of biological molecules and complex interaction samples conveniently, which aims at building integrated analysis pipelines from data acquisition, data checking, and descriptor calculation to modeling. PyBioMed is freely available at http://projects.scbdd.com/pybiomed.html .},
  langid = {english},
  pmcid = {PMC5861255},
  pmid = {29556758},
  keywords = {Bioinformatics,Chemoinformatics,Data integration,Molecular descriptors,Molecular representation,Python library},
  file = {/Users/pmg/Zotero/storage/JRTBVKB4/Dong et al. - 2018 - PyBioMed a python library for various molecular r.pdf}
}

@article{dubchak_prediction_1995,
  title = {Prediction of Protein Folding Class Using Global Description of Amino Acid Sequence.},
  author = {Dubchak, I and Muchnik, I and Holbrook, S R and Kim, S H},
  year = {1995},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {92},
  number = {19},
  pages = {8700--8704},
  issn = {0027-8424},
  urldate = {2023-01-03},
  abstract = {We present a method for predicting protein folding class based on global protein chain description and a voting process. Selection of the best descriptors was achieved by a computer-simulated neural network trained on a data base consisting of 83 folding classes. Protein-chain descriptors include overall composition, transition, and distribution of amino acid attributes, such as relative hydrophobicity, predicted secondary structure, and predicted solvent exposure. Cross-validation testing was performed on 15 of the largest classes. The test shows that proteins were assigned to the correct class (correct positive prediction) with an average accuracy of 71.7\%, whereas the inverse prediction of proteins as not belonging to a particular class (correct negative prediction) was 90-95\% accurate. When tested on 254 structures used in this study, the top two predictions contained the correct class in 91\% of the cases.},
  pmcid = {PMC41034},
  pmid = {7568000},
  file = {/Users/pmg/Zotero/storage/X67L497E/Dubchak et al. - 1995 - Prediction of protein folding class using global d.pdf}
}

@article{eddy_where_2004,
  title = {Where Did the {{BLOSUM62}} Alignment Score Matrix Come From?},
  author = {Eddy, Sean R.},
  year = {2004},
  month = aug,
  journal = {Nature Biotechnology},
  volume = {22},
  number = {8},
  pages = {1035--1036},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/nbt0804-1035},
  urldate = {2023-09-18},
  abstract = {Many sequence alignment programs use the BLOSUM62 score matrix to score pairs of aligned residues. Where did BLOSUM62 come from?},
  copyright = {2004 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Agriculture,Bioinformatics,Biomedical Engineering/Biotechnology,Biomedicine,Biotechnology,general,Life Sciences},
  file = {/Users/pmg/Zotero/storage/M65LDGQX/Eddy - 2004 - Where did the BLOSUM62 alignment score matrix come.pdf}
}

@article{eismann_protein_nodate,
  title = {Protein Model Quality Assessment Using Rotation-Equivariant Transformations on Point Clouds},
  author = {Eismann, Stephan and Suriana, Patricia and Jing, Bowen and Townshend, Raphael J. L. and Dror, Ron O.},
  journal = {Proteins: Structure, Function, and Bioinformatics},
  volume = {n/a},
  number = {n/a},
  issn = {1097-0134},
  doi = {10.1002/prot.26494},
  urldate = {2023-06-05},
  abstract = {Machine learning research concerning protein structure has seen a surge in popularity over the last years with promising advances for basic science and drug discovery. Working with macromolecular structure in a machine learning context requires an adequate numerical representation, and researchers have extensively studied representations such as graphs, discretized 3D grids, and distance maps. As part of CASP14, we explored a new and conceptually simple representation in a blind experiment: atoms as points in 3D, each with associated features. These features{\textemdash}initially just the basic element type of each atom{\textemdash}are updated through a series of neural network layers featuring rotation-equivariant convolutions. Starting from all atoms, we further aggregate information at the level of alpha carbons before making a prediction at the level of the entire protein structure. We find that this approach yields competitive results in protein model quality assessment despite its simplicity and despite the fact that it incorporates minimal prior information and is trained on relatively little data. Its performance and generality are particularly noteworthy in an era where highly complex, customized machine learning methods such as AlphaFold 2 have come to dominate protein structure prediction.},
  copyright = {{\textcopyright} 2023 Wiley Periodicals LLC.},
  langid = {english},
  keywords = {geometric deep learning,model quality assessment,physics-aware machine learning,protein structure prediction},
  file = {/Users/pmg/Zotero/storage/49VFCEND/prot.html}
}

@misc{elnaggar_ankh_2023,
  title = {Ankh: {{Optimized Protein Language Model Unlocks General-Purpose Modelling}}},
  shorttitle = {Ankh},
  author = {Elnaggar, Ahmed and Essam, Hazem and {Salah-Eldin}, Wafaa and Moustafa, Walid and Elkerdawy, Mohamed and Rochereau, Charlotte and Rost, Burkhard},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06568},
  eprint = {2301.06568},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.06568},
  urldate = {2023-01-18},
  abstract = {As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google's TPU-v4 surpassing the state-of-the-art performance with fewer parameters ({$<$}10\% for pre-training, {$<$}7\% for inference, and {$<$}30\% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/KHXI4UP2/Elnaggar et al. - 2023 - Ankh Optimized Protein Language Model Unlocks Gen.pdf}
}

@misc{elnaggar_ankh_2023-1,
  title = {Ankh: {{Optimized Protein Language Model Unlocks General-Purpose Modelling}}},
  shorttitle = {Ankh},
  author = {Elnaggar, Ahmed and Essam, Hazem and {Salah-Eldin}, Wafaa and Moustafa, Walid and Elkerdawy, Mohamed and Rochereau, Charlotte and Rost, Burkhard},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06568},
  eprint = {2301.06568},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.06568},
  urldate = {2023-09-18},
  abstract = {As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google's TPU-v4 surpassing the state-of-the-art performance with fewer parameters ({$<$}10\% for pre-training, {$<$}7\% for inference, and {$<$}30\% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/C6DV6CQ8/Elnaggar et al. - 2023 - Ankh Optimized Protein Language Model Unlocks Gen.pdf;/Users/pmg/Zotero/storage/TVYZ8EGB/2301.html}
}

  @article{elnaggar_prottrans_2021,
  title={{ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning}},
  author={Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={44},
  number={10},
  pages={7112--7127},
  year={2021},
  publisher={IEEE}
}


@inproceedings{emmerich_hypervolume-based_2011,
  title = {Hypervolume-Based Expected Improvement: {{Monotonicity}} Properties and Exact Computation},
  shorttitle = {Hypervolume-Based Expected Improvement},
  booktitle = {2011 {{IEEE Congress}} of {{Evolutionary Computation}} ({{CEC}})},
  author = {Emmerich, Michael T. M. and Deutz, Andr{\'e} H. and Klinkenberg, Jan Willem},
  year = {2011},
  month = jun,
  pages = {2147--2154},
  issn = {1941-0026},
  doi = {10.1109/CEC.2011.5949880},
  abstract = {The expected improvement (EI) is a well established criterion in Bayesian global optimization (BGO) and metamodel assisted evolutionary computation, both applied in optimization with costly function evaluations. Recently, it has been adopted in different ways to multiobjective optimization. A promising approach to formulate the expected improvement in this context, is to base it on the hypervolume indicator. Given the Bayesian model of the optimization landscape, the EI in hypervolume computes the expected gain in attained hypervolume for a given input point. Although a formulation of this expected improvement is relatively straightforward, its computation and mathematical properties are still to be investigated. This paper will outline and derive an algorithm for the exact computation of the proposed hypervolume-based EI. Moreover, this paper establishes monotonicity properties of the expected improvement. In particular the effect of the predictive distribution's variance on the hypervolume-based EI and elementary properties of the EI landscape are studied. The monotonicity properties will reveal regions where Pareto front approximations can be improved as well as underexplored regions that are favored by the hypervolume based expected improvement. A first numerical example is included that illustrates the behavior of the hypervolume-based EI in the multiobjective BGO framework.},
  keywords = {Approximation methods,Argon,Computational modeling,Gaussian distribution,Optimization,Silicon,Strips},
  file = {/Users/pmg/Zotero/storage/BBLW5GHQ/5949880.html}
}

@article{fang_critical_2020,
  title = {A Critical Review of Five Machine Learning-Based Algorithms for Predicting Protein Stability Changes upon Mutation},
  author = {Fang, Jianwen},
  year = {2020},
  month = jul,
  journal = {Briefings in Bioinformatics},
  volume = {21},
  number = {4},
  pages = {1285--1292},
  issn = {1477-4054},
  doi = {10.1093/bib/bbz071},
  urldate = {2022-11-10},
  abstract = {A number of machine learning (ML)-based algorithms have been proposed for predicting mutation-induced stability changes in proteins. In this critical review, we used hypothetical reverse mutations to evaluate the performance of five representative algorithms and found all of them suffer from the problem of overfitting. This approach is based on the fact that if a wild-type protein is more stable than a mutant protein, then the same mutant is less stable than the wild-type protein. We analyzed the underlying issues and suggest that the main causes of the overfitting problem include that the numbers of training cases were too small, and the features used in the models were not sufficiently informative for the task. We make recommendations on how to avoid overfitting in this important research area and improve the reliability and robustness of ML-based algorithms in general.},
  file = {/Users/pmg/Zotero/storage/FJNHL7TB/Fang - 2020 - A critical review of five machine learning-based a.pdf;/Users/pmg/Zotero/storage/TJU6G8MB/5527140.html}
}

@inproceedings{fannjiang_designing_2022,
  title = {Designing Active and Thermostable Enzymes with Sequence-Only Predictive Models},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Learning Meaningful Representations}} of {{Life}}},
  author = {Fannjiang, Clara and Olivas, Micah and Greene, Eric R. and Markin, Craig J. and Wallace, Bram and Krause, Ben and Pinney, Margaux M. and Fraser, James and Fordyce, Polly M. and Madani, Ali and Naik, Nikhil},
  year = {2022},
  month = nov,
  urldate = {2022-12-06},
  abstract = {Data-driven models of fitness can be useful in designing novel proteins with desired properties, but many questions remain regarding how and in what settings they should be used. Here, we ask: How can we use predictive models of protein fitness, whose predictions we might not always trust, to design protein sequences enhanced for multiple fitness functions? We propose a general approach for doing so, and apply it to design novel variants of eight different acylphosphatase and lysozyme wild types, intended to be more thermostable and at least as catalytically active as the wild types. Our method does not require a structure, experimental measurements of activity, curation of homologous sequences, or family-specific thermostability data. Experimental characterizations of our designed sequences, as well as sequences designed by PROSS, a competitive baseline method for improving protein thermostability, are currently underway and forthcoming.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/LIU9JZQ2/Fannjiang et al. - 2022 - Designing active and thermostable enzymes with seq.pdf;/Users/pmg/Zotero/storage/SC78XJPP/forum.html}
}

@misc{farid_task-relevant_2022,
  title = {Task-{{Relevant Failure Detection}} for {{Trajectory Predictors}} in {{Autonomous Vehicles}}},
  author = {Farid, Alec and Veer, Sushant and Ivanovic, Boris and Leung, Karen and Pavone, Marco},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12380},
  eprint = {2207.12380},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.12380},
  urldate = {2022-08-23},
  abstract = {In modern autonomy stacks, prediction modules are paramount to planning motions in the presence of other mobile agents. However, failures in prediction modules can mislead the downstream planner into making unsafe decisions. Indeed, the high uncertainty inherent to the task of trajectory forecasting ensures that such mispredictions occur frequently. Motivated by the need to improve safety of autonomous vehicles without compromising on their performance, we develop a probabilistic run-time monitor that detects when a "harmful" prediction failure occurs, i.e., a task-relevant failure detector. We achieve this by propagating trajectory prediction errors to the planning cost to reason about their impact on the AV. Furthermore, our detector comes equipped with performance measures on the false-positive and the false-negative rate and allows for data-free calibration. In our experiments we compared our detector with various others and found that our detector has the highest area under the receiver operator characteristic curve.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/pmg/Zotero/storage/C57KB93Q/Farid et al. - 2022 - Task-Relevant Failure Detection for Trajectory Pre.pdf;/Users/pmg/Zotero/storage/4CA8LLSF/2207.html}
}

@misc{faure_genetic_2023,
  title = {The Genetic Architecture of Protein Stability},
  author = {Faure, Andre J. and {Marti-Aranda}, Aina and Hidalgo, Cristina and Schmiedel, Joern M. and Lehner, Ben},
  year = {2023},
  month = oct,
  primaryclass = {New Results},
  pages = {2023.10.27.564339},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.10.27.564339},
  urldate = {2023-10-31},
  abstract = {There are more ways to synthesize a 100 amino acid protein (20100) than atoms in the universe. Only a miniscule fraction of such a vast sequence space can ever be experimentally or computationally surveyed. Deep neural networks are increasingly being used to navigate high-dimensional sequence spaces. However, these models are extremely complicated and provide little insight into the fundamental genetic architecture of proteins. Here, by experimentally exploring sequence spaces {$>$}1010, we show that the genetic architecture of at least some proteins is remarkably simple, allowing accurate genetic prediction in high-dimensional sequence spaces with fully interpretable biophysical models. These models capture the non-linear relationships between free energies and phenotypes but otherwise consist of additive free energy changes with a small contribution from pairwise energetic couplings. These energetic couplings are sparse and caused by structural contacts and backbone propagations. Our results suggest that artificial intelligence models may be vastly more complicated than the proteins that they are modeling and that protein genetics is actually both simple and intelligible.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/CLTFY37I/Faure et al. - 2023 - The genetic architecture of protein stability.pdf}
}

@misc{ferruz_sequence_2022,
  title = {From Sequence to Function through Structure: Deep Learning for Protein Design},
  shorttitle = {From Sequence to Function through Structure},
  author = {Ferruz, Noelia and Heinzinger, Michael and Akdel, Mehmet and Goncearenco, Alexander and Naef, Luca and Dallago, Christian},
  year = {2022},
  month = sep,
  primaryclass = {Confirmatory Results},
  pages = {2022.08.31.505981},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.08.31.505981},
  urldate = {2022-10-31},
  abstract = {The process of designing biomolecules, in particular proteins, is witnessing a rapid change in available tooling and approaches, moving from design through physicochemical force fields, to producing plausible, complex sequences fast via end-to-end differentiable statistical models. To achieve conditional and controllable protein design, researchers at the interface of artificial intelligence and biology leverage advances in natural language processing (NLP) and computer vision techniques, coupled with advances in computing hardware to learn patterns from growing biological databases, curated annotations thereof, or both. Once learned, these patterns can be leveraged to provide novel insights into mechanistic biology and the design of biomolecules. However, navigating and understanding the practical applications for the many recent protein design tools is complex. To facilitate this, we 1) document recent advances in deep learning (DL) assisted protein design from the last three years, 2) present a practical pipeline that allows to go from de novo-generated sequences to their predicted properties and web-powered visualization within minutes, and 3) leverage it to suggest a generated protein sequence which might be used to engineer a biosynthetic gene cluster to produce a molecular glue-like compound. Lastly, we discuss challenges and highlight opportunities for the protein design field. Availability pLM generated and UniRef50 sampled sequence sets and predictions are available at http://data.bioembeddings.com/public/design. Code-base and Notebooks for analysis are available at https://github.com/hefeda/PGP. An online version of Table 1 can be found at https://github.com/hefeda/design\_tools.},
  archiveprefix = {bioRxiv},
  chapter = {Confirmatory Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/SKY36EIH/Ferruz et al. - 2022 - From sequence to function through structure deep .pdf;/Users/pmg/Zotero/storage/SXSMN7DH/2022.08.31.html}
}

@article{ferruz_sequence_2022-1,
  title = {From Sequence to Function through Structure: Deep Learning for Protein Design},
  shorttitle = {From Sequence to Function through Structure},
  author = {Ferruz, Noelia and Heinzinger, Michael and Akdel, Mehmet and Goncearenco, Alexander and Naef, Luca and Dallago, Christian},
  year = {2022},
  month = nov,
  journal = {Computational and Structural Biotechnology Journal},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2022.11.014},
  urldate = {2022-11-29},
  abstract = {The process of designing biomolecules, in particular proteins, is witnessing a rapid change in available tooling and approaches, moving from design through physicochemical force fields, to producing plausible, complex sequences fast via end-to-end differentiable statistical models. To achieve conditional and controllable protein design, researchers at the interface of artificial intelligence and biology leverage advances in natural language processing (NLP) and computer vision techniques, coupled with advances in computing hardware to learn patterns from growing biological databases, curated annotations thereof, or both. Once learned, these patterns can be leveraged to provide novel insights into mechanistic biology and the design of biomolecules. However, navigating and understanding the practical applications for the many recent protein design tools is complex. To facilitate this, we 1) document recent advances in deep learning (DL) assisted protein design from the last three years, 2) present a practical pipeline that allows to go from de novo-generated sequences to their predicted properties and web-powered visualization within minutes, and 3) leverage it to suggest a generated protein sequence which might be used to engineer a biosynthetic gene cluster to produce a molecular glue-like compound. Lastly, we discuss challenges and highlight opportunities for the protein design field.},
  langid = {english},
  keywords = {deep learning,drug discovery,protein design,protein language models,protein prediction},
  file = {/Users/pmg/Zotero/storage/8BKRIAPJ/Ferruz et al. - 2022 - From sequence to function through structure deep .pdf;/Users/pmg/Zotero/storage/QKIRSATF/S2001037022005086.html}
}

@misc{ferruz_sequence_2022-2,
  title = {From Sequence to Function through Structure: Deep Learning for Protein Design},
  shorttitle = {From Sequence to Function through Structure},
  author = {Ferruz, Noelia and Heinzinger, Michael and Akdel, Mehmet and Goncearenco, Alexander and Naef, Luca and Dallago, Christian},
  year = {2022},
  month = sep,
  primaryclass = {Confirmatory Results},
  pages = {2022.08.31.505981},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.08.31.505981},
  urldate = {2023-04-20},
  abstract = {The process of designing biomolecules, in particular proteins, is witnessing a rapid change in available tooling and approaches, moving from design through physicochemical force fields, to producing plausible, complex sequences fast via end-to-end differentiable statistical models. To achieve conditional and controllable protein design, researchers at the interface of artificial intelligence and biology leverage advances in natural language processing (NLP) and computer vision techniques, coupled with advances in computing hardware to learn patterns from growing biological databases, curated annotations thereof, or both. Once learned, these patterns can be leveraged to provide novel insights into mechanistic biology and the design of biomolecules. However, navigating and understanding the practical applications for the many recent protein design tools is complex. To facilitate this, we 1) document recent advances in deep learning (DL) assisted protein design from the last three years, 2) present a practical pipeline that allows to go from de novo-generated sequences to their predicted properties and web-powered visualization within minutes, and 3) leverage it to suggest a generated protein sequence which might be used to engineer a biosynthetic gene cluster to produce a molecular glue-like compound. Lastly, we discuss challenges and highlight opportunities for the protein design field. Availability pLM generated and UniRef50 sampled sequence sets and predictions are available at http://data.bioembeddings.com/public/design. Code-base and Notebooks for analysis are available at https://github.com/hefeda/PGP. An online version of Table 1 can be found at https://github.com/hefeda/design\_tools.},
  archiveprefix = {bioRxiv},
  chapter = {Confirmatory Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/SGAVT5Y8/Ferruz et al. - 2022 - From sequence to function through structure deep .pdf}
}

@article{figliuzzi_how_2018,
  title = {How Pairwise Coevolutionary Models Capture the Collective Residue Variability in Proteins},
  author = {Figliuzzi, Matteo and {Barrat-Charlaix}, Pierre and Weigt, Martin},
  year = {2018},
  month = apr,
  journal = {Molecular Biology and Evolution},
  volume = {35},
  number = {4},
  eprint = {1801.04184},
  primaryclass = {cond-mat, q-bio},
  pages = {1018--1027},
  issn = {0737-4038, 1537-1719},
  doi = {10.1093/molbev/msy007},
  urldate = {2023-08-08},
  abstract = {Global coevolutionary models of homologous protein families, as constructed by direct coupling analysis (DCA), have recently gained popularity in particular due to their capacity to accurately predict residue-residue contacts from sequence information alone, and thereby to facilitate tertiary and quaternary protein structure prediction. More recently, they have also been used to predict fitness effects of amino-acid substitutions in proteins, and to predict evolutionary conserved protein-protein interactions. These models are based on two currently unjustified hypotheses: (a) correlations in the amino-acid usage of different positions are resulting collectively from networks of direct couplings; and (b) pairwise couplings are sufficient to capture the amino-acid variability. Here we propose a highly precise inference scheme based on Boltzmann-machine learning, which allows us to systematically address these hypotheses. We show how correlations are built up in a highly collective way by a large number of coupling paths, which are based on the protein's three-dimensional structure. We further find that pairwise coevolutionary models capture the collective residue variability across homologous proteins even for quantities which are not imposed by the inference procedure, like three-residue correlations, the clustered structure of protein families in sequence space or the sequence distances between homologs. These findings strongly suggest that pairwise coevolutionary models are actually sufficient to accurately capture the residue variability in homologous protein families.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Statistical Mechanics,Quantitative Biology - Biomolecules,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/ZBFT3C4W/Figliuzzi et al. - 2018 - How pairwise coevolutionary models capture the col.pdf;/Users/pmg/Zotero/storage/2DRPD985/1801.html}
}

@article{firnberg_comprehensive_2014,
  title = {A Comprehensive, High-Resolution Map of a Gene's Fitness Landscape},
  author = {Firnberg, Elad and Labonte, Jason W. and Gray, Jeffrey J. and Ostermeier, Marc},
  year = {2014},
  month = jun,
  journal = {Molecular Biology and Evolution},
  volume = {31},
  number = {6},
  pages = {1581--1592},
  issn = {1537-1719},
  doi = {10.1093/molbev/msu081},
  abstract = {Mutations are central to evolution, providing the genetic variation upon which selection acts. A mutation's effect on the suitability of a gene to perform a particular function (gene fitness) can be positive, negative, or neutral. Knowledge of the distribution of fitness effects (DFE) of mutations is fundamental for understanding evolutionary dynamics, molecular-level genetic variation, complex genetic disease, the accumulation of deleterious mutations, and the molecular clock. We present comprehensive DFEs for point and codon mutants of the Escherichia coli TEM-1 {$\beta$}-lactamase gene and missense mutations in the TEM-1 protein. These DFEs provide insight into the inherent benefits of the genetic code's architecture, support for the hypothesis that mRNA stability dictates codon usage at the beginning of genes, an extensive framework for understanding protein mutational tolerance, and evidence that mutational effects on protein thermodynamic stability shape the DFE. Contrary to prevailing expectations, we find that deleterious effects of mutation primarily arise from a decrease in specific protein activity and not cellular protein levels.},
  langid = {english},
  pmcid = {PMC4032126},
  pmid = {24567513},
  keywords = {beta-lactamase,beta-Lactamases,{Codon, Nonsense},Enzyme Stability,Escherichia coli,Escherichia coli Proteins,{Evolution, Molecular},fitness landscape,Genetic Code,Genetic Fitness,Genetic Variation,{Models, Molecular},Mutation Rate,{Mutation, Missense},Point Mutation,protein evolution,{Protein Structure, Tertiary},Synthetic Biology},
  file = {/Users/pmg/Zotero/storage/BZ5VTP3X/Firnberg et al. - 2014 - A comprehensive, high-resolution map of a gene's f.pdf}
}

@misc{flier_what_2023,
  title = {What Makes the Effect of Protein Mutations Difficult to Predict?},
  author = {van der Flier, Floris Julian and Estell, Dave and Pricelius, Sina and Dankmeyer, Lydia and Thans, Sander van Stigt and Mulder, Harm and Otsuka, Rei and Goedegebuur, Frits and Lammerts, Laurens and Staphorst, Diego and van Dijk, Aalt D. J. and de Ridder, Dick and Redestig, Henning},
  year = {2023},
  month = sep,
  primaryclass = {New Results},
  pages = {2023.09.25.559319},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.09.25.559319},
  urldate = {2023-09-26},
  abstract = {Protein engineering increasingly relies on machine learning models to computationally pre-screen variants to identify those that meet the target requirements. Although machine learning approaches have proven effective, their performance on prospective screening data has room for improvement. Prediction accuracy can vary greatly from one variant to the next. So far, it is unclear what characterizes variants that are associated with large model error. We designed and generated a dataset that can be stratified based on four structural characteristics (buriedness, number of contact residues, proximity to the active site and presence of secondary structure), to answer this question. We found that variants with multiple mutations that are buried, closely connected with other residues or close to the active site, which we call challenging mutations, are harder to model than their counterparts (i.e. exposed, loosely connected, far from the active site). This effect emerges only for variants with multiple challenging mutations, since single mutations at these sites were not harder to model. Our findings indicate that variants with challenging mutations are appropriate benchmarking targets for assessing model quality and that stratified dataset design can be leveraged to highlight areas of improvement for machine learning guided protein engineering.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/8LJU4Q8H/Flier et al. - 2023 - What makes the effect of protein mutations difficu.pdf}
}

@article{fowler_deep_2014,
  title = {Deep Mutational Scanning: A New Style of Protein Science},
  shorttitle = {Deep Mutational Scanning},
  author = {Fowler, Douglas M. and Fields, Stanley},
  year = {2014},
  month = aug,
  journal = {Nature Methods},
  volume = {11},
  number = {8},
  pages = {801--807},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/nmeth.3027},
  urldate = {2022-09-26},
  abstract = {This Perspective discusses the power of large mutational scans for the study of protein properties, the analytical challenges posed by the resulting data sets and the potential of this approach to further our understanding of human genetic variation.},
  copyright = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {High-throughput screening,Mutation,Protein analysis,Proteins},
  file = {/Users/pmg/Zotero/storage/8ZZK7TX5/Fowler and Fields - 2014 - Deep mutational scanning a new style of protein s.pdf;/Users/pmg/Zotero/storage/IG37DGRJ/nmeth.html}
}

@misc{fram_simultaneous_2023,
  title = {Simultaneous Enhancement of Multiple Functional Properties Using Evolution-Informed Protein Design},
  author = {Fram, Benjamin and Truebridge, Ian and Su, Yang and Riesselman, Adam J. and Ingraham, John B. and Passera, Alessandro and Napier, Eve and Thadani, Nicole N. and Lim, Samuel and Roberts, Kristen and Kaur, Gurleen and Stiffler, Michael and Marks, Debora S. and Bahl, Christopher D. and Khan, Amir R. and Sander, Chris and Gauthier, Nicholas P.},
  year = {2023},
  month = may,
  primaryclass = {New Results},
  pages = {2023.05.09.539914},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.05.09.539914},
  urldate = {2023-08-28},
  abstract = {Designing optimized proteins is important for a range of practical applications. Protein design is a rapidly developing field that would benefit from approaches that enable many changes in the amino acid primary sequence, rather than a small number of mutations, while maintaining structure and enhancing function. Homologous protein sequences contain extensive information about various protein properties and activities that have emerged over billions of years of evolution. Evolutionary models of sequence co-variation, derived from a set of homologous sequences, have proven effective in a range of applications including structure determination and mutation effect prediction. In this work we apply one of these models (EVcouplings) to computationally design highly divergent variants of the model protein TEM-1 {$\beta$}-lactamase, and characterize these designs experimentally using multiple biochemical and biophysical assays. Nearly all designed variants were functional, including one with 84 mutations from the nearest natural homolog. Surprisingly, all functional designs had large increases in thermostability and most had a broadening of available substrates. These property enhancements occurred while maintaining a nearly identical structure to the wild type enzyme. Collectively, this work demonstrates that evolutionary models of sequence co-variation (1) are able to capture complex epistatic interactions that successfully guide large sequence departures from natural contexts, and (2) can be applied to generate functional diversity useful for many applications in protein design.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/WJQHKYXF/Fram et al. - 2023 - Simultaneous enhancement of multiple functional pr.pdf}
}

@misc{frankle_linear_2020,
  title = {Linear {{Mode Connectivity}} and the {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2020},
  month = jul,
  number = {arXiv:1912.05671},
  eprint = {1912.05671},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.05671},
  urldate = {2023-08-23},
  abstract = {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/N3SVPBPG/Frankle et al. - 2020 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf;/Users/pmg/Zotero/storage/4UNPBAAB/1912.html}
}

@article{frazer_disease_2021,
  title = {Disease Variant Prediction with Deep Generative Models of Evolutionary Data},
  author = {Frazer, Jonathan and Notin, Pascal and Dias, Mafalda and Gomez, Aidan and Min, Joseph K. and Brock, Kelly and Gal, Yarin and Marks, Debora S.},
  year = {2021},
  month = nov,
  journal = {Nature},
  volume = {599},
  number = {7883},
  pages = {91--95},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04043-8},
  urldate = {2022-04-08},
  abstract = {Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98\%) of these variants still have unknown consequences1{\textendash}3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4{\textendash}10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification12{\textendash}16. We predict the pathogenicity of more than 36~million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown~significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational models,Disease genetics,Genetic variation,Genetics research,Machine learning},
  file = {/Users/pmg/Zotero/storage/QV4AFMZA/Frazer et al. - 2021 - Disease variant prediction with deep generative mo.pdf}
}

@misc{frazier_tutorial_2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I.},
  year = {2018},
  month = jul,
  number = {arXiv:1807.02811},
  eprint = {1807.02811},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.02811},
  urldate = {2023-04-03},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/MCGVTL5N/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf;/Users/pmg/Zotero/storage/K8IT464X/1807.html}
}

@misc{frey_protein_2023,
  title = {Protein {{Discovery}} with {{Discrete Walk-Jump Sampling}}},
  author = {Frey, Nathan C. and Berenberg, Daniel and Zadorozhny, Karina and Kleinhenz, Joseph and {Lafrance-Vanasse}, Julien and Hotzel, Isidro and Wu, Yan and Ra, Stephen and Bonneau, Richard and Cho, Kyunghyun and Loukas, Andreas and Gligorijevic, Vladimir and Saremi, Saeed},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12360},
  eprint = {2306.12360},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.12360},
  urldate = {2023-09-13},
  abstract = {We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling formalism combines the maximum likelihood training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the distributional conformity score to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100\% of generated samples are successfully expressed and purified and 35\% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a single round of laboratory experiments. We also report the first demonstration of long-run fast-mixing MCMC chains where diverse antibody protein classes are visited in a single MCMC chain.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/VSRZM5U4/Frey et al. - 2023 - Protein Discovery with Discrete Walk-Jump Sampling.pdf;/Users/pmg/Zotero/storage/FNPC8LTB/2306.html}
}

@misc{frisby_identifying_2023,
  title = {Identifying {{Promising Sequences For Protein Engineering Using A Deep Transformer Protein Language Model}}},
  author = {Frisby, Trevor S. and Langmead, Christopher James},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.15.528697},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.15.528697},
  urldate = {2023-02-20},
  abstract = {Protein engineers aim to discover and design novel sequences with targeted, desirable properties. Given the near limitless size of the protein sequence landscape, it is no surprise that these desirable sequences are often a relative rarity. This makes identifying such sequences a costly and time-consuming endeavor. In this work, we show how to use a deep Transformer Protein Language Model to identify sequences that have the most promise. Specifically, we use the model's self-attention map to calculate a PROMISE SCORE that weights the relative importance of a given sequence according to predicted interactions with a specified binding partner. This PROMISE SCORE can then be used to identify strong binders worthy of further study and experimentation. We use the PROMISE SCORE within two protein engineering contexts{\textemdash} Nanobody (Nb) discovery and protein optimization. With Nb discovery, we show how the PROMISE SCORE provides an effective way to select lead sequences from Nb repertoires. With protein optimization, we show how to use the PROMISE SCORE to select site-specific mutagenesis experiments that identify a high percentage of improved sequences. In both cases, we also show how the self-attention map used to calculate the PROMISE SCORE can indicate which regions of a protein are involved in intermolecular interactions that drive the targeted property. Finally, we describe how to fine-tune the Transformer Protein Language Model to learn a predictive model for the targeted property, and discuss the capabilities and limitations of fine-tuning with and without knowledge transfer within the context of protein engineering.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/9TINSV4G/Frisby and Langmead - 2023 - Identifying Promising Sequences For Protein Engine.pdf}
}

@article{gainza_deciphering_2020,
  title={Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning},
  author={Gainza, Pablo and Sverrisson, Freyr and Monti, Frederico and Rodola, Emanuele and Boscaini, Davide and Bronstein, Michael M and Correia, Bruno E},
  journal={Nature Methods},
  volume={17},
  number={2},
  pages={184--192},
  year={2020},
  publisher={Nature Publishing Group US New York}
}


@article{gainza_novo_2023,
  title = {De Novo Design of Protein Interactions with Learned Surface Fingerprints},
  author = {Gainza, Pablo and Wehrle, Sarah and {Van Hall-Beauvais}, Alexandra and Marchand, Anthony and Scheck, Andreas and Harteveld, Zander and Buckley, Stephen and Ni, Dongchun and Tan, Shuguang and Sverrisson, Freyr and Goverde, Casper and Turelli, Priscilla and Raclot, Charl{\`e}ne and Teslenko, Alexandra and Pacesa, Martin and Rosset, St{\'e}phane and Georgeon, Sandrine and Marsden, Jane and Petruzzella, Aaron and Liu, Kefang and Xu, Zepeng and Chai, Yan and Han, Pu and Gao, George F. and Oricchio, Elisa and Fierz, Beat and Trono, Didier and Stahlberg, Henning and Bronstein, Michael and Correia, Bruno E.},
  year = {2023},
  month = may,
  journal = {Nature},
  volume = {617},
  number = {7959},
  pages = {176--184},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-05993-x},
  urldate = {2023-05-10},
  abstract = {Physical interactions between proteins are essential for most biological processes governing life1. However, the molecular determinants of such interactions have been challenging to understand, even as genomic, proteomic and structural data increase. This knowledge gap has been a major obstacle for the comprehensive understanding of cellular protein{\textendash}protein interaction networks and for the de novo design of protein binders that are crucial for synthetic biology and translational applications2{\textendash}9. Here we use a geometric deep-learning framework operating on protein surfaces that generates fingerprints to describe geometric and chemical features that are critical to drive protein{\textendash}protein interactions10. We hypothesized that these fingerprints capture the key aspects of molecular recognition that represent a new paradigm in the computational design of novel protein interactions. As a proof of principle, we computationally designed several de novo protein binders to engage four protein targets: SARS-CoV-2 spike, PD-1, PD-L1 and CTLA-4. Several designs were experimentally optimized, whereas others were generated purely in silico, reaching nanomolar affinity with structural and mutational characterization showing highly accurate predictions. Overall, our surface-centric approach captures the physical and chemical determinants of molecular recognition, enabling an approach for the de novo design of protein interactions and, more broadly, of artificial proteins with function.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Protein design},
  file = {/Users/pmg/Zotero/storage/CEZQYJDJ/Gainza et al. - 2023 - De novo design of protein interactions with learne.pdf}
}

@misc{gao_deep_2022,
  title = {Deep Transfer Learning Provides a {{Pareto}} Improvement for Multi-Ancestral Clinico-Genomic Prediction of Diseases},
  author = {Gao, Yan and Cui, Yan},
  year = {2022},
  month = nov,
  primaryclass = {New Results},
  pages = {2022.09.22.509055},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.09.22.509055},
  urldate = {2022-11-21},
  abstract = {Accurate genomic predisposition assessment is essential for the prevention and early detection of diseases. Polygenic scores and machine learning models have been developed for disease prediction based on genetic variants and other risk factors. However, over 80\% of existing genomic data were acquired from individuals of European descent. As a result, clinico-genomic risk prediction is less accurate for non-European populations. Here we employ a transfer learning strategy to improve the clinico-genomic prediction of disease occurrence for the data-disadvantaged populations. Our multi-ancestral machine learning experiments on clinico-genomic datasets of cancers and Alzheimer's disease and synthetic datasets with built-in data inequality and subpopulation shift show that transfer learning can significantly improve disease prediction accuracy for data-disadvantaged populations. Under the transfer learning scheme, the prediction accuracy for the data-disadvantaged populations can be improved without compromising the prediction accuracy for other populations. Therefore, transfer learning provides a Pareto improvement toward equitable machine learning for genomic medicine.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/SKTRY8D3/Gao and Cui - 2022 - Deep transfer learning provides a Pareto improveme.pdf;/Users/pmg/Zotero/storage/6G6682V7/2022.09.22.html}
}

@misc{gao_diffsds_2023,
  title = {{{DiffSDS}}: {{A}} Language Diffusion Model for Protein Backbone Inpainting under Geometric Conditions and Constraints},
  shorttitle = {{{DiffSDS}}},
  author = {Gao, Zhangyang and Tan, Cheng and Li, Stan Z.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.09642},
  eprint = {2301.09642},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.09642},
  urldate = {2023-01-31},
  abstract = {Have you ever been troubled by the complexity and computational cost of SE(3) protein structure modeling and been amazed by the simplicity and power of language modeling? Recent work has shown promise in simplifying protein structures as sequences of protein angles; therefore, language models could be used for unconstrained protein backbone generation. Unfortunately, such simplification is unsuitable for the constrained protein inpainting problem, where the model needs to recover masked structures conditioned on unmasked ones, as it dramatically increases the computing cost of geometric constraints. To overcome this dilemma, we suggest inserting a hidden {\textbackslash}textbf\{a\}tomic {\textbackslash}textbf\{d\}irection {\textbackslash}textbf\{s\}pace ({\textbackslash}textbf\{ADS\}) upon the language model, converting invariant backbone angles into equivalent direction vectors and preserving the simplicity, called Seq2Direct encoder (\${\textbackslash}text\{Enc\}\_\{s2d\}\$). Geometric constraints could be efficiently imposed on the newly introduced direction space. A Direct2Seq decoder (\${\textbackslash}text\{Dec\}\_\{d2s\}\$) with mathematical guarantees is also introduced to develop a {\textbackslash}textbf\{SDS\} (\${\textbackslash}text\{Enc\}\_\{s2d\}\$+\${\textbackslash}text\{Dec\}\_\{d2s\}\$) model. We apply the SDS model as the denoising neural network during the conditional diffusion process, resulting in a constrained generative model--{\textbackslash}textbf\{DiffSDS\}. Extensive experiments show that the plug-and-play ADS could transform the language model into a strong structural model without loss of simplicity. More importantly, the proposed DiffSDS outperforms previous strong baselines by a large margin on the task of protein inpainting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/5LYV2U98/Gao et al. - 2023 - DiffSDS A language diffusion model for protein ba.pdf;/Users/pmg/Zotero/storage/XNHEBK6T/2301.html}
}

@misc{gao_graph_2019,
  title = {Graph {{Representation Learning}} via {{Hard}} and {{Channel-Wise Attention Networks}}},
  author = {Gao, Hongyang and Ji, Shuiwang},
  year = {2019},
  month = jul,
  number = {arXiv:1907.04652},
  eprint = {1907.04652},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.04652},
  urldate = {2023-04-24},
  abstract = {Attention operators have been widely applied in various fields, including computer vision, natural language processing, and network embedding learning. Attention operators on graph data enables learnable weights when aggregating information from neighboring nodes. However, graph attention operators (GAOs) consume excessive computational resources, preventing their applications on large graphs. In addition, GAOs belong to the family of soft attention, instead of hard attention, which has been shown to yield better performance. In this work, we propose novel hard graph attention operator (hGAO) and channel-wise graph attention operator (cGAO). hGAO uses the hard attention mechanism by attending to only important nodes. Compared to GAO, hGAO improves performance and saves computational cost by only attending to important nodes. To further reduce the requirements on computational resources, we propose the cGAO that performs attention operations along channels. cGAO avoids the dependency on the adjacency matrix, leading to dramatic reductions in computational resource requirements. Experimental results demonstrate that our proposed deep models with the new operators achieve consistently better performance. Comparison results also indicates that hGAO achieves significantly better performance than GAO on both node and graph embedding tasks. Efficiency comparison shows that our cGAO leads to dramatic savings in computational resources, making them applicable to large graphs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/PU9WGQJ2/Gao and Ji - 2019 - Graph Representation Learning via Hard and Channel.pdf;/Users/pmg/Zotero/storage/7RA9IDLW/1907.html}
}

@inproceedings{
gao_pifold_2023,
title={{PiFold: Toward effective and efficient protein inverse folding}},
author={Zhangyang Gao and Cheng Tan and Stan Z. Li},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=oMsN9TYwJ0j}
}

@inproceedings{gao_proteininvbench_2023,
  title = {{{ProteinInvBench}}: {{Benchmarking Protein Inverse Folding}} on {{Diverse Tasks}}, {{Models}}, and {{Metrics}}},
  shorttitle = {{{ProteinInvBench}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Gao, Zhangyang and Tan, Cheng and Zhang, Yijie and Chen, Xingran and Wu, Lirong and Li, Stan Z.},
  year = {2023},
  month = nov,
  urldate = {2024-01-08},
  abstract = {Protein inverse folding has attracted increasing attention in recent years. However, we observe that current methods are usually limited to the CATH dataset and the recovery metric. The lack of a unified framework for ensembling and comparing different methods hinders the comprehensive investigation. In this paper, we propose ProteinBench, a new benchmark for protein design, which comprises extended protein design tasks, integrated models, and diverse evaluation metrics. We broaden the application of methods originally designed for single-chain protein design to new scenarios of multi-chain and {\textbackslash}textit\{de novo\} protein design. Recent impressive methods, including GraphTrans, StructGNN, GVP, GCA, AlphaDesign, ProteinMPNN, PiFold and KWDesign are integrated into our framework. In addition to the recovery, we also evaluate the confidence, diversity, sc-TM, efficiency, and robustness to thoroughly revisit current protein design approaches and inspire future work. As a result, we establish the first comprehensive benchmark for protein design, which is publicly available at {\textbackslash}url\{https://github.com/A4Bio/OpenCPD\}.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/NES6SP4A/Gao et al. - 2023 - ProteinInvBench Benchmarking Protein Inverse Fold.pdf}
}

@article{gao_sample_2022,
  title = {Sample {{Efficiency Matters}}: {{A Benchmark}} for {{Practical Molecular Optimization}}},
  shorttitle = {Sample {{Efficiency Matters}}},
  author = {Gao, Wenhao and Fu, Tianfan and Sun, Jimeng and Coley, Connor},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {21342--21357},
  urldate = {2023-08-30},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/BT76QPQI/Gao et al. - 2022 - Sample Efficiency Matters A Benchmark for Practic.pdf}
}

@book{gardenfors_conceptual_2000,
  title = {Conceptual {{Spaces}}: {{The Geometry}} of {{Thought}}},
  shorttitle = {Conceptual {{Spaces}}},
  author = {G{\"a}rdenfors, Peter},
  year = {2000},
  month = mar,
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/2076.001.0001},
  urldate = {2023-08-28},
  abstract = {Within cognitive science, two approaches currently dominate the problem of modeling representations. The symbolic approach views cognition as computation involv},
  isbn = {978-0-262-27355-8},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/HUPHR2P7/Conceptual-SpacesThe-Geometry-of-Thought.html}
}

@misc{gardner_competency_2021,
  title = {Competency {{Problems}}: {{On Finding}} and {{Removing Artifacts}} in {{Language Data}}},
  shorttitle = {Competency {{Problems}}},
  author = {Gardner, Matt and Merrill, William and Dodge, Jesse and Peters, Matthew E. and Ross, Alexis and Singh, Sameer and Smith, Noah A.},
  year = {2021},
  month = dec,
  number = {arXiv:2104.08646},
  eprint = {2104.08646},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08646},
  urldate = {2023-08-24},
  abstract = {Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have "spurious" instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word "amazing" on its own should not give information about a sentiment label independent of the context in which it appears, which could include negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of creating data for competency problems when human bias is taken into account, showing that realistic datasets will increasingly deviate from competency problems as dataset size increases. This analysis gives us a simple statistical test for dataset artifacts, which we use to show more subtle biases than were described in prior work, including demonstrating that models are inappropriately affected by these less extreme biases. Our theoretical treatment of this problem also allows us to analyze proposed solutions, such as making local edits to dataset instances, and to give recommendations for future data collection and model design efforts that target competency problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/pmg/Zotero/storage/533Y6NVN/Gardner et al. - 2021 - Competency Problems On Finding and Removing Artif.pdf;/Users/pmg/Zotero/storage/GUPRP87R/2104.html}
}

@article{gardner_gpytorch_2021,
  title = {{{GPyTorch}}: {{Blackbox Matrix-Matrix Gaussian Process Inference}} with {{GPU Acceleration}}},
  author={Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q. and Bindel, David and Wilson, Andrew G.},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}


@article{garrido-merchan_dealing_2020,
  title = {Dealing with {{Categorical}} and {{Integer-valued Variables}} in {{Bayesian Optimization}} with {{Gaussian Processes}}},
  author = {{Garrido-Merch{\'a}n}, Eduardo C. and {Hern{\'a}ndez-Lobato}, Daniel},
  year = {2020},
  month = mar,
  journal = {Neurocomputing},
  volume = {380},
  eprint = {1805.03463},
  primaryclass = {cs, stat},
  pages = {20--35},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.11.004},
  urldate = {2022-10-31},
  abstract = {Bayesian Optimization (BO) methods are useful for optimizing functions that are expen- sive to evaluate, lack an analytical expression and whose evaluations can be contaminated by noise. These methods rely on a probabilistic model of the objective function, typically a Gaussian process (GP), upon which an acquisition function is built. The acquisition function guides the optimization process and measures the expected utility of performing an evaluation of the objective at a new point. GPs assume continous input variables. When this is not the case, for example when some of the input variables take categorical or integer values, one has to introduce extra approximations. Consider a suggested input location taking values in the real line. Before doing the evaluation of the objective, a common approach is to use a one hot encoding approximation for categorical variables, or to round to the closest integer, in the case of integer-valued variables. We show that this can lead to problems in the optimization process and describe a more principled approach to account for input variables that are categorical or integer-valued. We illustrate in both synthetic and a real experiments the utility of our approach, which significantly improves the results of standard BO methods using Gaussian processes on problems with categorical or integer-valued variables.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/EFLK3ISS/Garrido-Merchán and Hernández-Lobato - 2020 - Dealing with Categorical and Integer-valued Variab.pdf;/Users/pmg/Zotero/storage/94UFZRU3/1805.html}
}

@article{gelman_prior_nodate,
  title = {Prior Distributions for Variance Parameters in Hierarchical Models},
  author = {Gelman, Andrew},
  abstract = {Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of ``noninformative'' prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired. We also illustrate the use of the half-t family for hierarchical modeling of multiple variance parameters such as arise in the analysis of variance.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/VCPTQ5F6/Gelman - Prior distributions for variance parameters in hie.pdf}
}

@misc{ghaffari_property-guided_2023,
  title = {Property-{{Guided Generative Modelling}} for {{Robust Model-Based Design}} with {{Imbalanced Data}}},
  author = {Ghaffari, Saba and Saleh, Ehsan and Schwing, Alexander G. and Wang, Yu-Xiong and Burke, Martin D. and Sinha, Saurabh},
  year = {2023},
  month = may,
  number = {arXiv:2305.13650},
  eprint = {2305.13650},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.13650},
  urldate = {2023-05-26},
  abstract = {The problem of designing protein sequences with desired properties is challenging, as it requires to explore a high-dimensional protein sequence space with extremely sparse meaningful regions. This has led to the development of model-based optimization (MBO) techniques that aid in the design, by using effective search models guided by the properties over the sequence space. However, the intrinsic imbalanced nature of experimentally derived datasets causes existing MBO approaches to struggle or outright fail. We propose a property-guided variational auto-encoder (PGVAE) whose latent space is explicitly structured by the property values such that samples are prioritized according to these properties. Through extensive benchmarking on real and semi-synthetic protein datasets, we demonstrate that MBO with PGVAE robustly finds sequences with improved properties despite significant dataset imbalances. We further showcase the generality of our approach to continuous design spaces, and its robustness to dataset imbalance in an application to physics-informed neural networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/ZK4ZX2KR/Ghaffari et al. - 2023 - Property-Guided Generative Modelling for Robust Mo.pdf;/Users/pmg/Zotero/storage/KQAMQ4L8/2305.html}
}

@article{gibney_could_2022,
  title = {Could Machine Learning Fuel a Reproducibility Crisis in Science?},
  author = {Gibney, Elizabeth},
  year = {2022},
  month = jul,
  journal = {Nature},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-022-02035-w},
  urldate = {2022-08-05},
  abstract = {`Data leakage' threatens the reliability of machine-learning use across disciplines, researchers warn.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Machine learning,Mathematics and computing,Publishing},
  annotation = {Bandiera\_abtest: a Cg\_type: News Subject\_term: Machine learning, Publishing, Mathematics and computing},
  file = {/Users/pmg/Zotero/storage/3CKT2R28/d41586-022-02035-w.html}
}

@article{gislason_prediction_2021,
  title = {Prediction of {{GPI-anchored}} Proteins with Pointer Neural Networks},
  author = {G{\'i}slason, Magn{\'u}s Halld{\'o}r and Nielsen, Henrik and Almagro Armenteros, Jos{\'e} Juan and Johansen, Alexander Rosenberg},
  year = {2021},
  month = jan,
  journal = {Current Research in Biotechnology},
  volume = {3},
  pages = {6--13},
  issn = {2590-2628},
  doi = {10.1016/j.crbiot.2021.01.001},
  urldate = {2023-02-22},
  abstract = {GPI-anchors constitute a very important post-translational modification, linking many proteins to the outer face of the plasma membrane in eukaryotic cells. Since experimental validation of GPI-anchoring signals is slow and costly, computational approaches for predicting them from amino acid sequences are needed. However, the most recent GPI predictor is more than a decade old and considerable progress has been made in machine learning since then. We present a new dataset and a novel method, NetGPI, for GPI signal prediction. NetGPI is based on recurrent neural networks, incorporating an attention mechanism that simultaneously detects GPI-anchoring signals and points out the location of their {$\omega$}-sites. The performance of NetGPI is superior to existing methods with regards to discrimination between GPI-anchored proteins and other secretory proteins and approximate ({$\pm$}1 position) placement of the {$\omega$}-site. NetGPI is available at: https://services.healthtech.dtu.dk/service.php?NetGPI. The code repository is available at: https://github.com/mhgislason/netgpi-1.1.},
  langid = {english},
  keywords = {Glycosylphosphatidylinositol,Lipid anchored proteins,Neural networks,Post-translational modification,Prediction,Protein sorting},
  file = {/Users/pmg/Zotero/storage/LWL3TENK/Gíslason et al. - 2021 - Prediction of GPI-anchored proteins with pointer n.pdf;/Users/pmg/Zotero/storage/9GLYZLPF/S2590262821000010.html}
}

@article{gligorijevic_structure-based_2021,
  title = {Structure-Based Protein Function Prediction Using Graph Convolutional Networks},
  author = {Gligorijevi{\'c}, Vladimir and Renfrew, P. Douglas and Kosciolek, Tomasz and Leman, Julia Koehler and Berenberg, Daniel and Vatanen, Tommi and Chandler, Chris and Taylor, Bryn C. and Fisk, Ian M. and Vlamakis, Hera and Xavier, Ramnik J. and Knight, Rob and Cho, Kyunghyun and Bonneau, Richard},
  year = {2021},
  month = may,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {3168},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23303-9},
  urldate = {2022-09-01},
  abstract = {The rapid increase in the number of proteins in sequence databases and the diversity of their functions challenge computational approaches for automated function prediction. Here, we introduce DeepFRI, a Graph Convolutional Network for predicting protein functions by leveraging sequence features extracted from a protein language model and protein structures. It outperforms current leading methods and sequence-based Convolutional Neural Networks and scales to the size of current sequence repositories. Augmenting the training set of experimental structures with homology models allows us to significantly expand the number of predictable functions. DeepFRI has significant de-noising capability, with only a minor drop in performance when experimental structures are replaced by protein models. Class activation mapping allows function predictions at an unprecedented resolution, allowing site-specific annotations at the residue-level in an automated manner. We show the utility and high performance of our method by annotating structures from the PDB and SWISS-MODEL, making several new confident function predictions. DeepFRI is available as a webserver at https://beta.deepfri.flatironinstitute.org/.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Protein function predictions,Protein structure predictions},
  file = {/Users/pmg/Zotero/storage/XATGBRLL/Gligorijević et al. - 2021 - Structure-based protein function prediction using .pdf;/Users/pmg/Zotero/storage/4SBBTR4X/s41467-021-23303-9.html}
}

@article{gobel_correlated_1994,
  title = {Correlated Mutations and Residue Contacts in Proteins},
  author = {G{\"o}bel, U. and Sander, C. and Schneider, R. and Valencia, A.},
  year = {1994},
  month = apr,
  journal = {Proteins},
  volume = {18},
  number = {4},
  pages = {309--317},
  issn = {0887-3585},
  doi = {10.1002/prot.340180402},
  abstract = {The maintenance of protein function and structure constrains the evolution of amino acid sequences. This fact can be exploited to interpret correlated mutations observed in a sequence family as an indication of probable physical contact in three dimensions. Here we present a simple and general method to analyze correlations in mutational behavior between different positions in a multiple sequence alignment. We then use these correlations to predict contact maps for each of 11 protein families and compare the result with the contacts determined by crystallography. For the most strongly correlated residue pairs predicted to be in contact, the prediction accuracy ranges from 37 to 68\% and the improvement ratio relative to a random prediction from 1.4 to 5.1. Predicted contact maps can be used as input for the calculation of protein tertiary structure, either from sequence information alone or in combination with experimental information.},
  langid = {english},
  pmid = {8208723},
  keywords = {Amino Acid Sequence,Biological Evolution,{Crystallography, X-Ray},Mathematical Computing,{Models, Genetic},{Models, Molecular},{Models, Theoretical},Molecular Sequence Data,Mutation,{Protein Structure, Secondary},Proteins,{Ribonuclease, Pancreatic},Sequence Alignment,Structure-Activity Relationship,Trypsin Inhibitors}
}

@article{goldman_machine_2022,
  title = {Machine Learning Modeling of Family Wide Enzyme-Substrate Specificity Screens},
  author = {Goldman, Samuel and Das, Ria and Yang, Kevin K. and Coley, Connor W.},
  year = {2022},
  month = feb,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {2},
  eprint = {2109.03900},
  primaryclass = {cs, q-bio},
  pages = {e1009853},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1009853},
  urldate = {2022-05-24},
  abstract = {Biocatalysis is a promising approach to sustainably synthesize pharmaceuticals, complex natural products, and commodity chemicals at scale. However, the adoption of biocatalysis is limited by our ability to select enzymes that will catalyze their natural chemical transformation on non-natural substrates. While machine learning and in silico directed evolution are well-posed for this predictive modeling challenge, efforts to date have primarily aimed to increase activity against a single known substrate, rather than to identify enzymes capable of acting on new substrates of interest. To address this need, we curate 6 different high-quality enzyme family screens from the literature that each measure multiple enzymes against multiple substrates. We compare machine learning-based compound-protein interaction (CPI) modeling approaches from the literature used for predicting drug-target interactions. Surprisingly, comparing these interaction-based models against collections of independent (single task) enzyme-only or substrate-only models reveals that current CPI approaches are incapable of learning interactions between compounds and proteins in the current family level data regime. We further validate this observation by demonstrating that our no-interaction baseline can outperform CPI-based models from the literature used to guide the discovery of kinase inhibitors. Given the high performance of non-interaction based models, we introduce a new structure-based strategy for pooling residue representations across a protein sequence. Altogether, this work motivates a principled path forward in order to build and evaluate meaningful predictive models for biocatalysis and other drug discovery applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/LN56RF6B/Goldman et al. - 2022 - Machine learning modeling of family wide enzyme-su.pdf;/Users/pmg/Zotero/storage/9FMQTZB7/2109.html}
}

@article{gomez-bombarelli_automatic_2018,
  title = {Automatic {{Chemical Design Using}} a {{Data-Driven Continuous Representation}} of {{Molecules}}},
  author = {{G{\'o}mez-Bombarelli}, Rafael and Wei, Jennifer N. and Duvenaud, David and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and {S{\'a}nchez-Lengeling}, Benjam{\'i}n and Sheberla, Dennis and {Aguilera-Iparraguirre}, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and {Aspuru-Guzik}, Al{\'a}n},
  year = {2018},
  month = feb,
  journal = {ACS Central Science},
  volume = {4},
  number = {2},
  pages = {268--276},
  publisher = {{American Chemical Society}},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.7b00572},
  urldate = {2022-06-09},
  abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.},
  file = {/Users/pmg/Zotero/storage/SCD4V8TY/Gómez-Bombarelli et al. - 2018 - Automatic Chemical Design Using a Data-Driven Cont.pdf;/Users/pmg/Zotero/storage/A2USGJKI/acscentsci.html}
}

@inproceedings{gonzalez_glasses_2016,
  title = {{{GLASSES}}: {{Relieving The Myopia Of Bayesian Optimisation}}},
  shorttitle = {{{GLASSES}}},
  booktitle = {Proceedings of the 19th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gonzalez, Javier and Osborne, Michael and Lawrence, Neil},
  year = {2016},
  month = may,
  pages = {790--799},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2022-09-18},
  abstract = {We present GLASSES: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss. We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/B3T9GUSK/Gonzalez et al. - 2016 - GLASSES Relieving The Myopia Of Bayesian Optimisa.pdf}
}

@article{goyal_exposing_2022,
  title = {Exposing the {{Implicit Energy Networks}} behind {{Masked Language Models}} via {{Metropolis--Hastings}}},
  author = {Goyal, Kartik and Dyer, Chris and {Berg-Kirkpatrick}, Taylor},
  year = {2022},
  month = mar,
  journal = {arXiv:2106.02736 [cs]},
  eprint = {2106.02736},
  primaryclass = {cs},
  urldate = {2022-04-28},
  abstract = {While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an open question if these MLMs specify a principled probability distribution over the space of possible sequences. In this paper, we interpret MLMs as energy-based sequence models and propose two energy parametrizations derivable from the trained MLMs. In order to draw samples correctly from these models, we develop a tractable sampling scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our approach, samples are proposed from the same masked conditionals used for training the masked language models, and they are accepted or rejected based on their energy values according to the target distribution. We validate the effectiveness of the proposed parametrizations by exploring the quality of samples drawn from these energy-based models for both open-ended unconditional generation and a conditional generation task of machine translation. We theoretically and empirically justify our sampling algorithm by showing that the masked conditionals on their own do not yield a Markov chain whose stationary distribution is that of our target distribution, and our approach generates higher quality samples than other recently proposed undirected generation approaches (Wang et al., 2019, Ghazvininejad et al., 2019).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/62W6KK5P/Goyal et al. - 2022 - Exposing the Implicit Energy Networks behind Maske.pdf;/Users/pmg/Zotero/storage/I4RRBW4J/2106.html}
}

@article{gray_quantitative_2018,
  title = {Quantitative {{Missense Variant Effect Prediction Using Large-Scale Mutagenesis Data}}},
  author = {Gray, Vanessa E. and Hause, Ronald J. and Luebeck, Jens and Shendure, Jay and Fowler, Douglas M.},
  year = {2018},
  month = jan,
  journal = {Cell Systems},
  volume = {6},
  number = {1},
  pages = {116-124.e3},
  issn = {2405-4712},
  doi = {10.1016/j.cels.2017.11.003},
  abstract = {Large datasets describing the quantitative effects of mutations on protein function are becoming increasingly available. Here, we leverage these datasets to develop Envision, which predicts the magnitude of a missense variant's molecular effect. Envision combines 21,026 variant effect measurements from nine large-scale experimental mutagenesis datasets, a hitherto untapped training resource, with a supervised, stochastic gradient boosting learning algorithm. Envision outperforms other missense variant effect predictors both on large-scale mutagenesis data and on an independent test dataset comprising 2,312 TP53 variants whose effects were measured using a low-throughput approach. This dataset was never used for hyperparameter tuning or model training and thus serves as an independent validation set. Envision prediction accuracy is also more consistent across amino acids than other predictors. Finally, we demonstrate that Envision's performance improves as more large-scale mutagenesis data are incorporated. We precompute Envision predictions for every possible single amino acid variant in human, mouse, frog, zebrafish, fruit fly, worm, and yeast proteomes (https://envision.gs.washington.edu/).},
  langid = {english},
  pmcid = {PMC5799033},
  pmid = {29226803},
  keywords = {Algorithms,Animals,Computational Biology,{Databases, Genetic},Forecasting,{Genes, p53},Humans,large-scale mutagenesis,machine learning,Machine Learning,Mutagenesis,{Mutation, Missense},variant effect prediction},
  file = {/Users/pmg/Zotero/storage/95AGS76G/Gray et al. - 2018 - Quantitative Missense Variant Effect Prediction Us.pdf}
}

@misc{greener_fast_2022,
  title = {Fast Protein Structure Searching Using Structure Graph Embeddings},
  author = {Greener, Joe G. and Jamali, Kiarash},
  year = {2022},
  month = nov,
  primaryclass = {New Results},
  pages = {2022.11.28.518224},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.11.28.518224},
  urldate = {2022-12-06},
  abstract = {Comparing and searching protein structures independent of primary sequence has proved useful for remote homology detection, function annotation and protein classification. With the recent leap in accuracy of protein structure prediction methods and increased availability of protein models, attention is turning to how to best make use of this data. Fast and accurate methods to search databases of millions of structures will be essential to this endeavour, in the same way that fast protein sequence searching underpins much of bioinformatics. We train a simple graph neural network using supervised contrastive learning to learn a low-dimensional embedding of protein structure. The embedding can be used to search structures against large structural databases with accuracy comparable to current methods. The speed of the method and ability to scale to millions of structures makes it suitable for this structure-rich era. The method, called Progres, is available at https://github.com/jgreener64/progres.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/WK8CRL7J/Greener and Jamali - 2022 - Fast protein structure searching using structure g.pdf;/Users/pmg/Zotero/storage/XAT9Q9PR/2022.11.28.html}
}

@inproceedings{
greenman_benchmarking_2023,
  title = {Benchmarking {{Uncertainty Quantification}} for {{Protein Engineering}}},
author={Kevin P. Greenman and Ava Soleimany and Kevin K. Yang},
booktitle={ICLR2022 Machine Learning for Drug Discovery},
year={2022},
url={https://openreview.net/forum?id=G0vuqNwxaeA}
}

@misc{grosnit_high-dimensional_2021,
  title = {High-{{Dimensional Bayesian Optimisation}} with {{Variational Autoencoders}} and {{Deep Metric Learning}}},
  author = {Grosnit, Antoine and Tutunov, Rasul and Maraval, Alexandre Max and Griffiths, Ryan-Rhys and {Cowen-Rivers}, Alexander I. and Yang, Lin and Zhu, Lin and Lyu, Wenlong and Chen, Zhitang and Wang, Jun and Peters, Jan and {Bou-Ammar}, Haitham},
  year = {2021},
  month = nov,
  number = {arXiv:2106.03609},
  eprint = {2106.03609},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.03609},
  urldate = {2022-08-12},
  abstract = {We introduce a method combining variational autoencoders (VAEs) and deep metric learning to perform Bayesian optimisation (BO) over high-dimensional and structured input spaces. By adapting ideas from deep metric learning, we use label guidance from the blackbox function to structure the VAE latent space, facilitating the Gaussian process fit and yielding improved BO performance. Importantly for BO problem settings, our method operates in semi-supervised regimes where only few labelled data points are available. We run experiments on three real-world tasks, achieving state-of-the-art results on the penalised logP molecule generation benchmark using just 3\% of the labelled data required by previous approaches. As a theoretical contribution, we present a proof of vanishing regret for VAE BO.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Reading group},
  file = {/Users/pmg/Zotero/storage/3UE4H7PD/Grosnit et al. - 2021 - High-Dimensional Bayesian Optimisation with Variat.pdf;/Users/pmg/Zotero/storage/M92NTPJP/2106.html}
}

@misc{groth_flop_2023,
  title = {{{FLOP}}: {{Tasks}} for {{Fitness Landscapes Of Protein}} Wildtypes},
  shorttitle = {{{FLOP}}},
  author = {Groth, Peter M{\o}rch and Michael, Richard and Salomon, Jesper and Tian, Pengfei and Boomsma, Wouter},
  year = {2023},
  month = jun,
  primaryclass = {New Results},
  pages = {2023.06.21.545880},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.06.21.545880},
  urldate = {2023-06-26},
  abstract = {Protein engineering has the potential to create optimized protein variants with improved properties and function. An initial step in the protein optimization process typically consists of a search among natural (wildtype) sequences to find the naturally occurring proteins with the most desirable properties. Promising candidates from this initial discovery phase then form the basis of the second step: a more local optimization procedure, exploring the space of variants separated from this candidate by a number of mutations. While considerable progress has been made on evaluating machine learning methods on single protein datasets, benchmarks of data-driven approaches for global fitness landscape exploration are still lacking. In this paper, we have carefully curated a representative benchmark dataset, which reflects industrially relevant scenarios for the initial wildtype discovery phase of protein engineering. We focus on exploration within a protein family, and investigate the downstream predictive power of various protein representation paradigms, i.e., protein language model-based representations, structure-based representations, and evolution-based representations. Our benchmark highlights the importance of coherent split strategies, and how we can be misled into overly optimistic estimates of the state of the field. The codebase and data can be accessed via https://github.com/petergroth/FLOP.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/M8VW4SHE/Groth et al. - 2023 - FLOP Tasks for Fitness Landscapes Of Protein wild.pdf}
}

@misc{grover_graphite_2019,
  title = {Graphite: {{Iterative Generative Modeling}} of {{Graphs}}},
  shorttitle = {Graphite},
  author = {Grover, Aditya and Zweig, Aaron and Ermon, Stefano},
  year = {2019},
  month = may,
  number = {arXiv:1803.10459},
  eprint = {1803.10459},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.10459},
  urldate = {2022-08-17},
  abstract = {Graphs are a fundamental abstraction for modeling relational data. However, graphs are discrete and combinatorial in nature, and learning representations suitable for machine learning tasks poses statistical and computational challenges. In this work, we propose Graphite, an algorithmic framework for unsupervised learning of representations over nodes in large graphs using deep latent variable generative models. Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding. On a wide variety of synthetic and benchmark datasets, Graphite outperforms competing approaches for the tasks of density estimation, link prediction, and node classification. Finally, we derive a theoretical connection between message passing in graph neural networks and mean-field variational inference.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/8QFV6D2T/Grover et al. - 2019 - Graphite Iterative Generative Modeling of Graphs.pdf;/Users/pmg/Zotero/storage/J6PGYTEW/1803.html}
}

@misc{gruver_protein_2023,
  title = {Protein {{Design}} with {{Guided Discrete Diffusion}}},
  author = {Gruver, Nate and Stanton, Samuel and Frey, Nathan C. and Rudner, Tim G. J. and Hotzel, Isidro and {Lafrance-Vanasse}, Julien and Rajpal, Arvind and Cho, Kyunghyun and Wilson, Andrew Gordon},
  year = {2023},
  month = may,
  number = {arXiv:2305.20009},
  eprint = {2305.20009},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.20009},
  urldate = {2023-09-04},
  abstract = {A popular approach to protein design is to combine a generative model with a discriminative model for conditional sampling. The generative model samples plausible sequences while the discriminative model guides a search for sequences with high fitness. Given its broad success in conditional sampling, classifier-guided diffusion modeling is a promising foundation for protein design, leading many to develop guided diffusion models for structure with inverse folding to recover sequences. In this work, we propose diffusioN Optimized Sampling (NOS), a guidance method for discrete diffusion models that follows gradients in the hidden states of the denoising network. NOS makes it possible to perform design directly in sequence space, circumventing significant limitations of structure-based methods, including scarce data and challenging inverse design. Moreover, we use NOS to generalize LaMBO, a Bayesian optimization procedure for sequence design that facilitates multiple objectives and edit-based constraints. The resulting method, LaMBO-2, enables discrete diffusions and stronger performance with limited edits through a novel application of saliency maps. We apply LaMBO-2 to a real-world protein design task, optimizing antibodies for higher expression yield and binding affinity to a therapeutic target under locality and liability constraints, with 97\% expression rate and 25\% binding rate in exploratory in vitro experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/3CTPFW6S/Gruver et al. - 2023 - Protein Design with Guided Discrete Diffusion.pdf;/Users/pmg/Zotero/storage/S3PPNU5F/2305.html}
}

@misc{gu_efficiently_2022,
  title = {Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}},
  author = {Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  year = {2022},
  month = aug,
  number = {arXiv:2111.00396},
  eprint = {2111.00396},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.00396},
  urldate = {2023-05-30},
  abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) {\textbackslash}( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) {\textbackslash}), and showed that for appropriate choices of the state matrix {\textbackslash}( A {\textbackslash}), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning {\textbackslash}( A {\textbackslash}) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91{\textbackslash}\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60{\textbackslash}times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/N8YRB932/Gu et al. - 2022 - Efficiently Modeling Long Sequences with Structure.pdf;/Users/pmg/Zotero/storage/NHP6TQDQ/2111.html}
}

@misc{guo_calibration_2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = aug,
  number = {arXiv:1706.04599},
  eprint = {1706.04599},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-21},
  abstract = {Confidence calibration {\textendash} the problem of predicting probability estimates representative of the true correctness likelihood {\textendash} is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling {\textendash} a singleparameter variant of Platt Scaling {\textendash} is surprisingly effective at calibrating predictions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/PMG2E9FL/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf}
}

@misc{guo_calibration_2017-1,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = aug,
  number = {arXiv:1706.04599},
  eprint = {1706.04599},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.04599},
  urldate = {2024-01-17},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/2HWKEYK2/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf;/Users/pmg/Zotero/storage/8R4XURNW/1706.html}
}


@inproceedings{gustafsson_evaluating_2020,
  title = {{Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision}},
  author={Gustafsson, Fredrik K. and Danelljan, Martin and Schon, Thomas B.},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={318--319},
  year={2020}
}


@misc{hajij_topological_2022,
  title = {Topological {{Deep Learning}}: {{Going Beyond Graph Data}}},
  shorttitle = {Topological {{Deep Learning}}},
  author = {Hajij, Mustafa and Zamzmi, Ghada and Papamarkou, Theodore and Miolane, Nina and {Guzm{\'a}n-S{\'a}enz}, Aldo and Ramamurthy, Karthikeyan Natesan and Birdal, Tolga and Dey, Tamal K. and Mukherjee, Soham and Samaga, Shreyas N. and Livesay, Neal and Walters, Robin and Rosen, Paul and Schaub, Michael T.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.00606},
  eprint = {2206.00606},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.00606},
  urldate = {2023-07-04},
  abstract = {Topological deep learning is a rapidly growing field that pertains to the development of deep learning models for data supported on topological domains such as simplicial complexes, cell complexes, and hypergraphs, which generalize many domains encountered in scientific computations. In this paper, we present a unifying deep learning framework built upon a richer data structure that includes widely adopted topological domains. Specifically, we first introduce combinatorial complexes, a novel type of topological domain. Combinatorial complexes can be seen as generalizations of graphs that maintain certain desirable properties. Similar to hypergraphs, combinatorial complexes impose no constraints on the set of relations. In addition, combinatorial complexes permit the construction of hierarchical higher-order relations, analogous to those found in simplicial and cell complexes. Thus, combinatorial complexes generalize and combine useful traits of both hypergraphs and cell complexes, which have emerged as two promising abstractions that facilitate the generalization of graph neural networks to topological spaces. Second, building upon combinatorial complexes and their rich combinatorial and algebraic structure, we develop a general class of message-passing combinatorial complex neural networks (CCNNs), focusing primarily on attention-based CCNNs. We characterize permutation and orientation equivariances of CCNNs, and discuss pooling and unpooling operations within CCNNs in detail. Third, we evaluate the performance of CCNNs on tasks related to mesh shape analysis and graph learning. Our experiments demonstrate that CCNNs have competitive performance as compared to state-of-the-art deep learning models specifically tailored to the same tasks. Our findings demonstrate the advantages of incorporating higher-order relations into deep learning models in different applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Mathematics - Algebraic Topology,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/LGIPWEEU/Hajij et al. - 2022 - Topological Deep Learning Going Beyond Graph Data.pdf;/Users/pmg/Zotero/storage/ZJSSDUBM/2206.html}
}

@article{hamilton_graph_nodate,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, William L},
  pages = {141},
  abstract = {Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial if we want systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D-vision, recommender systems, question answering, and social network analysis.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/ZKZ668IE/Hamilton - Graph Representation Learning.pdf}
}

@inproceedings{harteveld_deep_2022,
  title = {Deep Sharpening of Topological Features for de Novo Protein Design},
  booktitle = {{{ICLR2022 Machine Learning}} for {{Drug Discovery}}},
  author = {Harteveld, Zander and Southern, Joshua and Defferrard, Micha{\"e}l and Loukas, Andreas and Vandergheynst, Pierre and Bronstein, Micheal and Correia, Bruno},
  year = {2022},
  month = mar,
  urldate = {2022-04-27},
  abstract = {Tailored de novo protein design using a neural networks.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/5UNVBT9P/Harteveld et al. - 2022 - Deep sharpening of topological features for de nov.pdf;/Users/pmg/Zotero/storage/XALBX94U/forum.html}
}

@article{hauberg_differential_nodate,
  title = {Differential {{Geometry}} for {{Generative Modelling}}},
  author = {Hauberg, S{\o}ren},
  pages = {88},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/9ASZELWH/Hauberg - This version is printed on October 27, 2021. For t.pdf}
}

@article{hauberg_this_nodate,
  title = {This Version Is Printed on {{October}} 27, 2021. {{For}} the Latest Version, Please See {{http://compute.dtu.dk/{\texttildelow}sohau/weekendwithbernie/}}},
  author = {Hauberg, S{\o}ren},
  pages = {88},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/HV9QYJL7/Hauberg - This version is printed on October 27, 2021. For t.pdf}
}

@article{heins_phylogenomically_2014,
  title = {Phylogenomically {{Guided Identification}} of {{Industrially Relevant GH1}} {$\beta$}-{{Glucosidases}} through {{DNA Synthesis}} and {{Nanostructure-Initiator Mass Spectrometry}}},
  author = {Heins, Richard A. and Cheng, Xiaoliang and Nath, Sangeeta and Deng, Kai and Bowen, BenjaminP. and Chivian, Dylan C. and Datta, Supratim and Friedland, Gregory D. and D'Haeseleer, Patrik and Wu, Dongying and {Tran-Gyamfi}, Mary and Scullin, Chessa S. and Singh, Seema and Shi, Weibing and Hamilton, MatthewG. and Bendall, Matthew L. and Sczyrba, Alexander and Thompson, John and Feldman, Taya and Guenther, Joel M. and Gladden, John M. and Cheng, Jan-Fang and Adams, Paul D. and Rubin, Edward M. and Simmons, BlakeA. and Sale, Kenneth L. and Northen, TrentR. and Deutsch, Samuel},
  year = {2014},
  month = sep,
  journal = {ACS Chemical Biology},
  volume = {9},
  number = {9},
  pages = {2082--2091},
  issn = {1554-8929},
  doi = {10.1021/cb500244v},
  urldate = {2022-05-02},
  abstract = {, Harnessing the biotechnological potential of the large number of proteins available in sequence databases requires scalable methods for functional characterization. Here we propose a workflow to address this challenge by combining phylogenomic guided DNA synthesis with high-throughput mass spectrometry and apply it to the systematic characterization of GH1 {$\beta$}-glucosidases, a family of enzymes necessary for biomass hydrolysis, an important step in the conversion of lignocellulosic feedstocks to fuels and chemicals. We synthesized and expressed 175 GH1s, selected from over 2000 candidate sequences to cover maximum sequence diversity. These enzymes were functionally characterized over a range of temperatures and pHs using nanostructure-initiator mass spectrometry (NIMS), generating over 10,000 data points. When combined with HPLC-based sugar profiling, we observed GH1 enzymes active over a broad temperature range and toward many different {$\beta$}-linked disaccharides. For some GH1s we also observed activity toward laminarin, a more complex oligosaccharide present as a major component of macroalgae. An area of particular interest was the identification of GH1 enzymes compatible with the ionic liquid 1-ethyl-3-methylimidazolium acetate ([C2mim][OAc]), a next-generation biomass pretreatment technology. We thus searched for GH1 enzymes active at 70 {\textdegree}C and 20\% (v/v) [C2mim][OAc] over the course of a 24-h saccharification reaction. Using our unbiased approach, we identified multiple enzymes of different phylogentic origin with such activities. Our approach of characterizing sequence diversity through targeted gene synthesis coupled to high-throughput screening technologies is a broadly applicable paradigm for a wide range of biological problems.},
  pmcid = {PMC4168791},
  pmid = {24984213},
  file = {/Users/pmg/Zotero/storage/3EQ5KXIP/Heins et al. - 2014 - Phylogenomically Guided Identification of Industri.pdf}
}

@article{heinzinger_contrastive_2022,
  title = {Contrastive Learning on Protein Embeddings Enlightens Midnight Zone},
  author = {Heinzinger, Michael and Littmann, Maria and Sillitoe, Ian and Bordin, Nicola and Orengo, Christine and Rost, Burkhard},
  year = {2022},
  month = jun,
  journal = {NAR Genomics and Bioinformatics},
  volume = {4},
  number = {2},
  pages = {lqac043},
  issn = {2631-9268},
  doi = {10.1093/nargab/lqac043},
  urldate = {2022-11-18},
  abstract = {Experimental structures are leveraged through multiple sequence alignments, or more generally through homology-based inference (HBI), facilitating the transfer of information from a protein with known annotation to a query without any annotation. A recent alternative expands the concept of HBI from sequence-distance lookup to embedding-based annotation transfer (EAT). These embeddings are derived from protein Language Models (pLMs). Here, we introduce using single protein representations from pLMs for contrastive learning. This learning procedure creates a new set of embeddings that optimizes constraints captured by hierarchical classifications of protein 3D structures defined by the CATH resource. The approach, dubbed ProtTucker, has an improved ability to recognize distant homologous relationships than more traditional techniques such as threading or fold recognition. Thus, these embeddings have allowed sequence comparison to step into the `midnight zone' of protein similarity, i.e. the region in which distantly related sequences have a seemingly random pairwise sequence similarity. The novelty of this work is in the particular combination of tools and sampling techniques that ascertained good performance comparable or better to existing state-of-the-art sequence comparison methods. Additionally, since this method does not need to generate alignments it is also orders of magnitudes faster. The code is available at https://github.com/Rostlab/EAT.},
  file = {/Users/pmg/Zotero/storage/PKD74AZT/Heinzinger et al. - 2022 - Contrastive learning on protein embeddings enlight.pdf;/Users/pmg/Zotero/storage/HHU3DVDF/6605840.html}
}

@misc{heinzinger_contrastive_2022-1,
  title = {Contrastive Learning on Protein Embeddings Enlightens Midnight Zone},
  author = {Heinzinger, Michael and Littmann, Maria and Sillitoe, Ian and Bordin, Nicola and Orengo, Christine and Rost, Burkhard},
  year = {2022},
  month = mar,
  primaryclass = {New Results},
  pages = {2021.11.14.468528},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.11.14.468528},
  urldate = {2023-06-27},
  abstract = {Experimental structures are leveraged through multiple sequence alignments, or more generally through homology-based inference (HBI), facilitating the transfer of information from a protein with known annotation to a query without any annotation. A recent alternative expands the concept of HBI from sequence-distance lookup to embedding-based annotation transfer (EAT). These embeddings are derived from protein Language Models (pLMs). Here, we introduce using single protein representations from pLMs for contrastive learning. This learning procedure creates a new set of embeddings that optimizes constraints captured by hierarchical classifications of protein 3D structures defined by the CATH resource. The approach, dubbed ProtTucker, has an improved ability to recognize distant homologous relationships than more traditional techniques such as threading or fold recognition. Thus, these embeddings have allowed sequence comparison to step into the ``midnight zone'' of protein similarity, i.e., the region in which distantly related sequences have a seemingly random pairwise sequence similarity. The novelty of this work is in the particular combination of tools and sampling techniques that ascertained good performance comparable or better to existing state-of-the-art sequence comparison methods. Additionally, since this method does not need to generate alignments it is also orders of magnitudes faster. The code is available at https://github.com/Rostlab/EAT.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/22KC5DG5/Heinzinger et al. - 2022 - Contrastive learning on protein embeddings enlight.pdf}
}

@misc{heinzinger_prostt5_2023,
  title = {{{ProstT5}}: {{Bilingual Language Model}} for {{Protein Sequence}} and {{Structure}}},
  shorttitle = {{{ProstT5}}},
  author = {Heinzinger, Michael and Weissenow, Konstantin and Sanchez, Joaquin Gomez and Henkel, Adrian and Steinegger, Martin and Rost, Burkhard},
  year = {2023},
  month = jul,
  primaryclass = {New Results},
  pages = {2023.07.23.550085},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.07.23.550085},
  urldate = {2023-07-26},
  abstract = {Advanced Artificial Intelligence (AI) enabled large language models (LLMs) to revolutionize Natural Language Processing (NLP). Their adaptation to protein sequences spawned the development of powerful protein language models (pLMs). Concurrently, AlphaFold2 broke through in protein structure prediction. For the first time, we can now systematically and comprehensively explore the dual nature of proteins that act and exist as three-dimensional (3D) machines and evolve in linear strings of one-dimensional (1D) sequences. Here, we leverage pLMs to simultaneously model both modalities by combining 1D sequences with 3D structure in one generic model. For this, we encode protein structures as token sequences using the 3Di-alphabet introduced by Foldseek. The resulting 'structure-sequence' representation is processed by a pLM to extract features and patterns. Toward this end, we constructed a non-redundant dataset from AlphaFoldDB and fine-tuned an existing pLM (ProtT5) to translate between 3Di and amino acid sequences. As a proof-of-concept for our novel approach, dubbed Protein structure-sequence T5 (ProstT5), we showed improved performance for subsequent prediction tasks, and for 'inverse folding', namely the generation of novel protein sequences adopting a given structural scaffold ('fold'). Our work showcased the potential of pLMs to tap into the information-rich protein structure revolution fueled by AlphaFold2. It paves the way for the development of tools optimizing the integration of this vast 3D structure data resource, opening new research avenues in the post AlphaFold2 era. We released our model at https://github.com/mheinzinger/ProstT5 .},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/IR6NC2HD/Heinzinger et al. - 2023 - ProstT5 Bilingual Language Model for Protein Sequ.pdf}
}

@misc{hermosilla_contrastive_2022,
  title = {Contrastive {{Representation Learning}} for {{3D Protein Structures}}},
  author = {Hermosilla, Pedro and Ropinski, Timo},
  year = {2022},
  month = may,
  number = {arXiv:2205.15675},
  eprint = {2205.15675},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.15675},
  urldate = {2023-06-27},
  abstract = {Learning from 3D protein structures has gained wide interest in protein modeling and structural bioinformatics. Unfortunately, the number of available structures is orders of magnitude lower than the training data sizes commonly used in computer vision and machine learning. Moreover, this number is reduced even further, when only annotated protein structures can be considered, making the training of existing models difficult and prone to over-fitting. To address this challenge, we introduce a new representation learning framework for 3D protein structures. Our framework uses unsupervised contrastive learning to learn meaningful representations of protein structures, making use of proteins from the Protein Data Bank. We show, how these representations can be used to solve a large variety of tasks, such as protein function prediction, protein fold classification, structural similarity prediction, and protein-ligand binding affinity prediction. Moreover, we show how fine-tuned networks, pre-trained with our algorithm, lead to significantly improved task performance, achieving new state-of-the-art results in many tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/Y2CA29CB/Hermosilla and Ropinski - 2022 - Contrastive Representation Learning for 3D Protein.pdf;/Users/pmg/Zotero/storage/QG384W99/2205.html}
}

@inproceedings{hermosilla_intrinsic-extrinsic_2022,
  title = {Intrinsic-{{Extrinsic Convolution}} and {{Pooling}} for {{Learning}} on {{3D Protein Structures}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hermosilla, Pedro and Sch{\"a}fer, Marco and Lang, Matej and Fackelmann, Gloria and V{\'a}zquez, Pere-Pau and Kozlikova, Barbora and Krone, Michael and Ritschel, Tobias and Ropinski, Timo},
  year = {2022},
  month = feb,
  urldate = {2022-10-06},
  abstract = {Proteins perform a large variety of functions in living organisms and thus play a key role in biology. However, commonly used algorithms in protein representation learning were not specifically...},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/U494WQLS/Hermosilla et al. - 2022 - Intrinsic-Extrinsic Convolution and Pooling for Le.pdf;/Users/pmg/Zotero/storage/I7MAAS4N/forum.html}
}

@misc{hesslow_rita_2022,
  title = {{{RITA}}: A {{Study}} on {{Scaling Up Generative Protein Sequence Models}}},
  shorttitle = {{{RITA}}},
  author = {Hesslow, Daniel and Zanichelli, Niccol{\'o} and Notin, Pascal and Poli, Iacopo and Marks, Debora},
  year = {2022},
  month = may,
  number = {arXiv:2205.05789},
  eprint = {2205.05789},
  primaryclass = {cs, q-bio},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.05789},
  urldate = {2022-05-24},
  abstract = {In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid prediction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/P6MGTS4Q/Hesslow et al. - 2022 - RITA a Study on Scaling Up Generative Protein Seq.pdf;/Users/pmg/Zotero/storage/MEG5MUS2/2205.html}
}

@article{hie_adaptive_2021,
  title = {Adaptive Machine Learning for Protein Engineering},
  author = {Hie, Brian L. and Yang, Kevin K.},
  year = {2021},
  month = jul,
  journal = {arXiv:2106.05466 [cs, q-bio]},
  eprint = {2106.05466},
  primaryclass = {cs, q-bio},
  urldate = {2022-04-27},
  abstract = {Machine-learning models that learn from data to predict how protein sequence encodes function are emerging as a useful protein engineering tool. However, when using these models to suggest new protein designs, one must deal with the vast combinatorial complexity of protein sequences. Here, we review how to use a sequence-to-function machine-learning surrogate model to select sequences for experimental measurement. First, we discuss how to select sequences through a single round of machine-learning optimization. Then, we discuss sequential optimization, where the goal is to discover optimized sequences and improve the model across multiple rounds of training, optimization, and experimental measurement.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/6RJWAIXT/Hie and Yang - 2021 - Adaptive machine learning for protein engineering.pdf;/Users/pmg/Zotero/storage/S2YDA7JC/2106.html}
}

@article{hie_leveraging_2020,
  title = {Leveraging {{Uncertainty}} in {{Machine Learning Accelerates Biological Discovery}} and {{Design}}},
  author = {Hie, Brian and Bryson, Bryan D. and Berger, Bonnie},
  year = {2020},
  month = nov,
  journal = {Cell Systems},
  volume = {11},
  number = {5},
  pages = {461-477.e9},
  publisher = {{Elsevier}},
  issn = {2405-4712},
  doi = {10.1016/j.cels.2020.09.007},
  urldate = {2022-10-17},
  langid = {english},
  pmid = {33065027},
  keywords = {active learning,compound-kinase affinity,Gaussian process,generative design,machine learning,Mycobacterium tuberculosis,PknB,protein fitness landscape,transcriptomic imputation,uncertainty prediction},
  file = {/Users/pmg/Zotero/storage/GYEPNBSV/Hie et al. - 2020 - Leveraging Uncertainty in Machine Learning Acceler.pdf;/Users/pmg/Zotero/storage/4UVHQXXQ/S2405-4712(20)30364-1.html}
}

@misc{ho_classifier-free_2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.12598},
  urldate = {2023-01-31},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/GNSDE4NQ/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf;/Users/pmg/Zotero/storage/CISEK9HC/2207.html}
}

@misc{ho_classifier-free_2022-1,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.12598},
  urldate = {2023-02-03},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/F3UP2TZZ/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf;/Users/pmg/Zotero/storage/KLER5CA7/2207.html}
}

@misc{ho_denoising_2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.11239},
  urldate = {2022-12-13},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/H9S3YWZL/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf;/Users/pmg/Zotero/storage/9H4D7FLS/2006.html}
}

@misc{hoffbauer_transmep_2024,
  title = {{{TransMEP}}: {{Transfer}} Learning on Large Protein Language Models to Predict Mutation Effects of Proteins from a Small Known Dataset},
  shorttitle = {{{TransMEP}}},
  author = {Hoffbauer, Tilman and Strodel, Birgit},
  year = {2024},
  month = jan,
  primaryclass = {New Results},
  pages = {2024.01.12.575432},
  publisher = {{bioRxiv}},
  doi = {10.1101/2024.01.12.575432},
  urldate = {2024-01-16},
  abstract = {Machine learning-guided optimization has become a driving force for recent improvements in protein engineering. In addition, new protein language models are learning the grammar of evolutionarily occurring sequences at large scales. This work combines both approaches to make predictions about mutational effects that support protein engineering. To this end, an easy-to-use software tool called TransMEP is developed using transfer learning by feature extraction with Gaussian process regression. A large collection of datasets is used to evaluate its quality, which scales with the size of the training set, and to show its improvements over previous fine-tuning approaches. Wet-lab studies are simulated to evaluate the use of mutation effect prediction models for protein engineering. This showed that TransMEP finds the best performing mutants with a limited study budget by considering the trade-off between exploration and exploitation.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/5AUJP9G3/Hoffbauer and Strodel - 2024 - TransMEP Transfer learning on large protein langu.pdf}
}

@article{hoie_predicting_2022,
  title = {Predicting and Interpreting Large-Scale Mutagenesis Data Using Analyses of Protein Stability and Conservation},
  author = {H{\o}ie, Magnus Haraldson and Cagiada, Matteo and Beck Frederiksen, Anders Haagen and Stein, Amelie and {Lindorff-Larsen}, Kresten},
  year = {2022},
  month = jan,
  journal = {Cell Reports},
  volume = {38},
  number = {2},
  pages = {110207},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2021.110207},
  urldate = {2023-03-16},
  abstract = {Understanding and predicting the functional consequences of single amino acid changes is central in many areas of protein science. Here, we collect and analyze experimental measurements of effects of {$>$}150,000 variants in 29 proteins. We use biophysical calculations to predict changes in stability for each variant and assess them in light of sequence conservation. We find that the sequence analyses give more accurate prediction of variant effects than predictions of stability and that about half of the variants that show loss of function do so due to stability effects. We construct a machine learning model to predict variant effects from protein structure and sequence alignments and show how the two sources of information support one another and enable mechanistic interpretations. Together, our results show how one can leverage large-scale experimental assessments of variant effects to gain deeper and general insights into the mechanisms that cause loss of function.},
  langid = {english},
  keywords = {machine learning,protein evolution,protein stability,variant effects},
  file = {/Users/pmg/Zotero/storage/HYWSLN9K/Høie et al. - 2022 - Predicting and interpreting large-scale mutagenesi.pdf;/Users/pmg/Zotero/storage/39AWTNYD/S2211124721017113.html}
}

@misc{hoogeboom_argmax_2021,
  title = {Argmax {{Flows}} and {{Multinomial Diffusion}}: {{Learning Categorical Distributions}}},
  shorttitle = {Argmax {{Flows}} and {{Multinomial Diffusion}}},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  year = {2021},
  month = oct,
  number = {arXiv:2102.05379},
  eprint = {2102.05379},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.05379},
  urldate = {2022-12-14},
  abstract = {Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/7FZ9ETMZ/Hoogeboom et al. - 2021 - Argmax Flows and Multinomial Diffusion Learning C.pdf;/Users/pmg/Zotero/storage/HRPZYVRD/2102.html}
}

@misc{hoogeboom_autoregressive_2022,
  title = {Autoregressive {{Diffusion Models}}},
  author = {Hoogeboom, Emiel and Gritsenko, Alexey A. and Bastings, Jasmijn and Poole, Ben and van den Berg, Rianne and Salimans, Tim},
  year = {2022},
  month = feb,
  number = {arXiv:2110.02037},
  eprint = {2110.02037},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.02037},
  urldate = {2023-02-03},
  abstract = {We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/X5N6UREA/Hoogeboom et al. - 2022 - Autoregressive Diffusion Models.pdf;/Users/pmg/Zotero/storage/R3PIZGPI/2110.html}
}

@misc{hoogeboom_equivariant_2022,
  title = {Equivariant {{Diffusion}} for {{Molecule Generation}} in {{3D}}},
  author = {Hoogeboom, Emiel and Satorras, Victor Garcia and Vignac, Cl{\'e}ment and Welling, Max},
  year = {2022},
  month = jun,
  number = {arXiv:2203.17003},
  eprint = {2203.17003},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.17003},
  urldate = {2022-12-14},
  abstract = {This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and efficiency at training time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/EKAK8944/Hoogeboom et al. - 2022 - Equivariant Diffusion for Molecule Generation in 3.pdf;/Users/pmg/Zotero/storage/7FT7ATPS/2203.html}
}

@article{hopf_evcouplings_2019,
  title = {The {{EVcouplings Python}} Framework for Coevolutionary Sequence Analysis},
  author = {Hopf, Thomas A. and Green, Anna G. and Schubert, Benjamin and Mersmann, Sophia and Sch{\"a}rfe, Charlotta P. I. and Ingraham, John B. and {Toth-Petroczy}, Agnes and Brock, Kelly and Riesselman, Adam J. and Palmedo, Perry and Kang, Chan and Sheridan, Robert and Draizen, Eli J. and Dallago, Christian and Sander, Chris and Marks, Debora S.},
  year = {2019},
  month = may,
  journal = {Bioinformatics (Oxford, England)},
  volume = {35},
  number = {9},
  pages = {1582--1584},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/bty862},
  abstract = {SUMMARY: Coevolutionary sequence analysis has become a commonly used technique for de novo prediction of the structure and function of proteins, RNA, and protein complexes. We present the EVcouplings framework, a fully integrated open-source application and Python package for coevolutionary analysis. The framework enables generation of sequence alignments, calculation and evaluation of evolutionary couplings (ECs), and de novo prediction of structure and mutation effects. The combination of an easy to use, flexible command line interface and an underlying modular Python package makes the full power of coevolutionary analyses available to entry-level and advanced users. AVAILABILITY AND IMPLEMENTATION: https://github.com/debbiemarkslab/evcouplings.},
  langid = {english},
  pmcid = {PMC6499242},
  pmid = {30304492},
  keywords = {Proteins,RNA,Sequence Alignment,Sequence Analysis,Software},
  file = {/Users/pmg/Zotero/storage/CR9FXNJK/Hopf et al. - 2019 - The EVcouplings Python framework for coevolutionar.pdf}
}

@article{hopf_mutation_2017,
  title = {Mutation Effects Predicted from Sequence Co-Variation},
  author = {Hopf, Thomas A. and Ingraham, John B. and Poelwijk, Frank J. and Sch{\"a}rfe, Charlotta P. I. and Springer, Michael and Sander, Chris and Marks, Debora S.},
  year = {2017},
  month = feb,
  journal = {Nature Biotechnology},
  volume = {35},
  number = {2},
  pages = {128--135},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/nbt.3769},
  urldate = {2023-06-28},
  abstract = {The global effects of epistasis on protein and RNA function are revealed by an unsupervised model of amino acid co-conservation in evolutionary sequence variation.},
  copyright = {2017 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational models,Molecular evolution,Mutation,Protein function predictions},
  file = {/Users/pmg/Zotero/storage/6HNM6TNM/Hopf et al. - 2017 - Mutation effects predicted from sequence co-variat.pdf}
}

@misc{horlacher_systematic_2023,
  title = {A {{Systematic Benchmark}} of {{Machine Learning Methods}} for {{Protein-RNA Interaction Prediction}}},
  author = {Horlacher, Marc and Cantini, Giulia and Hesse, Julian and Schinke, Patrick and Goedert, Nicolas and Londhe, Shubhankar and Moyon, Lambert and Marsico, Annalisa},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.14.528560},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.14.528560},
  urldate = {2023-03-09},
  abstract = {RNA-binding proteins (RBPs) are central actors of RNA post-transcriptional regulation. Experiments to profile binding sites of RBPs in vivo are limited to transcripts expressed in the experimental cell type, creating the need for computational methods to infer missing binding information. While numerous machine-learning based methods have been developed for this task, their use of heterogeneous training and evaluation datasets across different sets of RBPs and CLIP-seq protocols makes a direct comparison of their performance difficult. Here, we compile a set of 37 machine learning (primarily deep learning) methods for in vivo RBP-RNA interaction prediction and systematically benchmark a subset of 11 representative methods across hundreds of CLIP-seq datasets and RBPs. Using homogenized sample pre-processing and two negative-class sample generation strategies, we evaluate methods in terms of predictive performance and assess the impact of neural network architectures and input modalities on model performance. We believe that this study will not only enable researchers to choose the optimal prediction method for their tasks at hand, but also aid method developers in developing novel, high-performing methods by introducing a standardized framework for their evaluation.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/CLS9BFTU/Horlacher et al. - 2023 - A Systematic Benchmark of Machine Learning Methods.pdf}
}

@inproceedings{hsu_learning_2022,
  title = {{Learning Inverse Folding from Millions of Predicted Structures}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Hsu, Chloe and Verkuil, Robert and Liu, Jason and Lin, Zeming and Hie, Brian and Sercu, Tom and Lerer, Adam and Rives, Alexander},
  year = {2022},
  month = jun,
  pages = {8946--8970},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-09-27},
  abstract = {We consider the problem of predicting a protein sequence from its backbone atom coordinates. Machine learning approaches to this problem to date have been limited by the number of available experimentally determined protein structures. We augment training data by nearly three orders of magnitude by predicting structures for 12M protein sequences using AlphaFold2. Trained with this additional data, a sequence-to-sequence transformer with invariant geometric input processing layers achieves 51\% native sequence recovery on structurally held-out backbones with 72\% recovery for buried residues, an overall improvement of almost 10 percentage points over existing methods. The model generalizes to a variety of more complex tasks including design of protein complexes, partially masked structures, binding interfaces, and multiple states.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/CK9NCSTI/Hsu et al. - 2022 - Learning inverse folding from millions of predicte.pdf}
}

@article{hsu_learning_2022-1,
  title = {Learning Protein Fitness Models from Evolutionary and Assay-Labeled Data},
  author = {Hsu, Chloe and Nisonoff, Hunter and Fannjiang, Clara and Listgarten, Jennifer},
  year = {2022},
  month = jan,
  journal = {Nature Biotechnology},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-021-01146-5},
  urldate = {2022-04-05},
  abstract = {Machine learning-based models of protein fitness typically learn from either unlabeled, evolutionarily related sequences or variant sequences with experimentally measured labels. For regimes where only limited experimental data are available, recent work has suggested methods for combining both sources of information. Toward that goal, we propose a simple combination approach that is competitive with, and on average outperforms more sophisticated methods. Our approach uses ridge regression on site-specific amino acid features combined with one probability density feature from modeling the evolutionary data. Within this approach, we find that a variational autoencoder-based probability density model showed the best overall performance, although any evolutionary density model can be used. Moreover, our analysis highlights the importance of systematic evaluations and sufficient baselines.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Machine learning,Protein design},
  file = {/Users/pmg/Zotero/storage/RWLS26BI/Hsu et al. - 2022 - Learning protein fitness models from evolutionary .pdf;/Users/pmg/Zotero/storage/TN6CSYEI/s41587-021-01146-5.html}
}

@misc{hu_exploring_2022,
  title = {Exploring Evolution-Based \& -Free Protein Language Models as Protein Function Predictors},
  author = {Hu, Mingyang and Yuan, Fajie and Yang, Kevin K. and Ju, Fusong and Su, Jin and Wang, Hui and Yang, Fei and Ding, Qiuyang},
  year = {2022},
  month = jun,
  number = {arXiv:2206.06583},
  eprint = {2206.06583},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.06583},
  urldate = {2022-06-22},
  abstract = {Large-scale Protein Language Models (PLMs) have improved performance in protein prediction tasks, ranging from 3D structure prediction to various function predictions. In particular, AlphaFold, a ground-breaking AI system, could potentially reshape structural biology. However, the utility of the PLM module in AlphaFold, Evoformer, has not been explored beyond structure prediction. In this paper, we investigate the representation ability of three popular PLMs: ESM-1b (single sequence), MSA-Transformer (multiple sequence alignment) and Evoformer (structural), with a special focus on Evoformer. Specifically, we aim to answer the following key questions: (i) Does the Evoformer trained as part of AlphaFold produce representations amenable to predicting protein function? (ii) If yes, can Evoformer replace ESM-1b and MSA-Transformer? (iii) How much do these PLMs rely on evolution-related protein data? In this regard, are they complementary to each other? We compare these models by empirical study along with new insights and conclusions. Finally, we release code and datasets for reproducibility.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/K8XUYZQB/Hu et al. - 2022 - Exploring evolution-based & -free protein language.pdf;/Users/pmg/Zotero/storage/CXLA46KS/2206.html}
}

@misc{hu_learning_2023,
  title = {Learning {{Complete Protein Representation}} by {{Deep Coupling}} of {{Sequence}} and {{Structure}}},
  author = {Hu, Bozhen and Tan, Cheng and Xia, Jun and Zheng, Jiangbin and Huang, Yufei and Wu, Lirong and Liu, Yue and Xu, Yongjie and Li, Stan Z.},
  year = {2023},
  month = jul,
  primaryclass = {New Results},
  pages = {2023.07.05.547769},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.07.05.547769},
  urldate = {2023-07-26},
  abstract = {Learning effective representations is crucial for understanding proteins and their biological functions. Recent advancements in language models and graph neural networks have enabled protein models to leverage primary or tertiary structure information to learn representations. However, the lack of practical methods to deeply co-model the relationships between protein sequences and structures has led to suboptimal embeddings. In this work, we propose CoupleNet, a network that couples protein sequence and structure to obtain informative protein representations. CoupleNet incorporates multiple levels of features in proteins, including the residue identities and positions for sequences, as well as geometric representations for tertiary structures. We construct two types of graphs to model the extracted sequential features and structural geometries, achieving completeness on these graphs, respectively, and perform convolution on nodes and edges simultaneously to obtain superior embeddings. Experimental results on a range of tasks, such as protein fold classification and function prediction, demonstrate that our proposed model outperforms the state-of-the-art methods by large margins.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/WU7PKGD3/Hu et al. - 2023 - Learning Complete Protein Representation by Deep C.pdf}
}

@misc{hu_lora_2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2023-09-01},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/H2KGALRG/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/Users/pmg/Zotero/storage/XJZMWFJW/2106.html}
}

@misc{hu_strategies_2020,
  title = {Strategies for {{Pre-training Graph Neural Networks}}},
  author = {Hu, Weihua and Liu, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
  year = {2020},
  month = feb,
  number = {arXiv:1905.12265},
  eprint = {1905.12265},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.12265},
  urldate = {2023-04-24},
  abstract = {Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4\% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/Y283IF66/Hu et al. - 2020 - Strategies for Pre-training Graph Neural Networks.pdf;/Users/pmg/Zotero/storage/HHWG4CS2/1905.html}
}

@misc{hua_high-order_2022,
  title = {High-{{Order Pooling}} for {{Graph Neural Networks}} with {{Tensor Decomposition}}},
  author = {Hua, Chenqing and Rabusseau, Guillaume and Tang, Jian},
  year = {2022},
  month = oct,
  number = {arXiv:2205.11691},
  eprint = {2205.11691},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11691},
  urldate = {2023-04-26},
  abstract = {Graph Neural Networks (GNNs) are attracting growing attention due to their effectiveness and flexibility in modeling a variety of graph-structured data. Exiting GNN architectures usually adopt simple pooling operations (eg. sum, average, max) when aggregating messages from a local neighborhood for updating node representation or pooling node representations from the entire graph to compute the graph representation. Though simple and effective, these linear operations do not model high-order non-linear interactions among nodes. We propose the Tensorized Graph Neural Network (tGNN), a highly expressive GNN architecture relying on tensor decomposition to model high-order non-linear node interactions. tGNN leverages the symmetric CP decomposition to efficiently parameterize permutation-invariant multilinear maps for modeling node interactions. Theoretical and empirical analysis on both node and graph classification tasks show the superiority of tGNN over competitive baselines. In particular, tGNN achieves the most solid results on two OGB node classification datasets and one OGB graph classification dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/3L97DEAC/Hua et al. - 2022 - High-Order Pooling for Graph Neural Networks with .pdf;/Users/pmg/Zotero/storage/3XAJUA9Y/2205.html}
}

@article{huang_accurate_2023,
  title = {Accurate and Efficient Protein Sequence Design through Learning Concise Local Environment of Residues},
  author = {Huang, Bin and Fan, Tingwen and Wang, Kaiyue and Zhang, Haicang and Yu, Chungong and Nie, Shuyu and Qi, Yangshuo and Zheng, Wei-Mou and Han, Jian and Fan, Zheng and Sun, Shiwei and Ye, Sheng and Yang, Huaiyi and Bu, Dongbo},
  year = {2023},
  month = mar,
  journal = {Bioinformatics},
  volume = {39},
  number = {3},
  pages = {btad122},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btad122},
  urldate = {2024-01-31},
  abstract = {Computational protein sequence design has been widely applied in rational protein engineering and increasing the design accuracy and efficiency is highly desired.Here, we present ProDESIGN-LE, an accurate and efficient approach to protein sequence design. ProDESIGN-LE adopts a concise but informative representation of the residue's local environment and trains a transformer to learn the correlation between local environment of residues and their amino acid types. For a target backbone structure, ProDESIGN-LE uses the transformer to assign an appropriate residue type for each position based on its local environment within this structure, eventually acquiring a designed sequence with all residues fitting well with their local environments. We applied ProDESIGN-LE to design sequences for 68 naturally occurring and 129 hallucinated proteins within 20\,s per protein on average. The designed proteins have their predicted structures perfectly resembling the target structures with a state-of-the-art average TM-score exceeding 0.80. We further experimentally validated ProDESIGN-LE by designing five sequences for an enzyme, chloramphenicol O-acetyltransferase type III (CAT III), and recombinantly expressing the proteins in Escherichia coli. Of these proteins, three exhibited excellent solubility, and one yielded monomeric species with circular dichroism spectra consistent with the natural CAT III protein.The source code of ProDESIGN-LE is available at https://github.com/bigict/ProDESIGN-LE.},
  file = {/Users/pmg/Zotero/storage/9MV8YCJT/Huang et al. - 2023 - Accurate and efficient protein sequence design thr.pdf;/Users/pmg/Zotero/storage/6A9B2PGK/7077134.html}
}

@article{hutcheson_test_1970,
  title = {A Test for Comparing Diversities Based on the Shannon Formula},
  author = {Hutcheson, Kermit},
  year = {1970},
  month = oct,
  journal = {Journal of Theoretical Biology},
  volume = {29},
  number = {1},
  pages = {151--154},
  issn = {0022-5193},
  doi = {10.1016/0022-5193(70)90124-4},
  urldate = {2022-06-14},
  abstract = {Avifaunal diversity can act as a proxy to prioritize areas for conservation, such as Important Bird and Biodiversity Areas (IBAs). Within such priority areas, different forest structures can have contrasting effects on avifauna, and could potentially lead to an overlapping but somewhat different species assemblage. Therefore, we aimed to quantify the relative importance of horizontal forest structures and fragmented habitats i.e., contiguous forest and isolated forest, in harboring avifauna and also understanding the patterns of diversity in different seasons in the Mai Valley IBA of Eastern Nepal. We used a double-observer method to collect bird species lists during repeated visits in winter and spring to an isolated and a contiguous forest area. We used Hill numbers and Hutcheson's t-test to account for differences in the number of survey points sampled in each forest. The study recorded higher diversity in the contiguous forest compared to the isolated forest. However, the bird assemblage based on foraging and habitat guild were found to be similar between these two forests. Despite the importance of both forests for avifauna, restricted-range species were recorded only in contiguous forest. The ecological integrity of contiguous forests provides a natural habitat with a variety of niches influencing the presence of forest specialists, which might lead to the higher species diversity observed in contiguous forest. Also, the difference in area, and forest management regime could have influenced the diversity difference between the two forest types. We recommend more research on avifaunal assemblage and their habitat to prioritize these critical areas for conservation. Furthermore, considering the role played by contiguous forest in maintaining bird communities, we emphasize the maintenance of forest birds corridors by preserving natural forests and planting native trees in private orchards in the Mai Valley Area. Commonly detected in surface waters worldwide, odorous compounds such as geosmin (GSM) and 2-methylisoborneol (MIB) pose important issues in drinking water and aquaculture industries. Impaired waters are frequently plagued by severe Cyanobacterial blooms and seasonal occurrences of Actinobacteria due to increased anthropogenic stressors as well as climate change that negatively impact the quality of water resources globally. Historically, studies have focused on the identification of key environmental factors and organisms involved in the production of GSM and MIB. This past decade, fast-evolving molecular techniques have greatly contributed to document the structure of bacterioplankton communities. Eagle Creek Reservoir, a eutrophic temperate reservoir, is frequently impacted by seasonal odorous episodes. During the spring 2013 outbreak of MIB and GSM (120.9 and 51.4 ng L-1, respectively), a shotgun metagenomics approach illustrated the dominance of Actinobacteria, Proteobacteria, Firmicutes, Bacteroidetes and Cyanobacteria in the reservoir waters. A network analysis highlighted interrelationships between bacterial taxa and environmental factors, and revealed two major clusters of bacteria: Cluster I driven by temperature and nitrate nitrogen and, Cluster II reflective of elevated concentrations of MIB and GSM. Analysis of 16S-based sequences recovered prevalently MIB- and GSM-encoding gene sequences belonging to several Actinobacteria (genus Streptomyces) rather than Cyanobacteria, formerly assumed to be the main producers. The use of genetics shifted our perception and stressed the role of Actinobacteria in the biosynthesis of odorous metabolites through the non-mevalonate pathway; as well as the detection of non-producing Arthrobacter potentially involved in the degradation of these compounds. Declines in native warm season grasslands have been linked to grassland bird population decline due to habitat loss including conversion to non-native grasses. Rotational grazing (ROT) and patch-burn grazing (PBG) are two possible tools to restore native warm-season grasses (NWSG) on working-lands in the Mid-South USA and thus aid in the recovery of grassland bird populations. This project compares ROT, PBG, and before treatment implementation to assess their effects on grassland-associated bird species. At three research sites between KY and TN, 14{\textendash}10 ha NWSG pastures were established and randomly assigned 7 pastures each to ROT and PBG treatment and monitored avian relative abundance during the breeding season from 2014 to 2017. Avian call count data and vegetation characteristics were collected in 2014 and treated as a before treatment year. Following 2014, ROT and PBG treatments were implemented across each respective research site. We used the open N-mixture model framework to estimate avian relative abundance related to year, treatments, research site, and landscape and within-field variables. Avian species richness and diversity were calculated for each treatment, research site, and year. Landscape variables, within-field variables, and research sites exerted more influence on relative abundance than ROT or PBG. Grassland-associated bird species relative abundance and species richness/diversity were affected by habitat disturbances (both ROT and PBG) but varied by species and site. Field sparrows [Spizella pusilla] had the highest increase in relative abundance (9.68 {$\pm$} 1.24 birds/point count location or 1.77 ha) while northern cardinals [Cardinalis cardinalis] exhibited a significant decrease in relative abundance (3.44 {$\pm$} 1.54 birds/point count location) following treatment implementation on specific research sites. Species diversity and richness did not differ between ROT and PBG treatments. However, a site and year difference were observed for both estimates. Using ROT and/or PBG to create habitat disturbances can alter within-field variables (i.e., vegetation height) which, taken into context with landscape variables, could impact grassland bird populations and diversity depending on grassland bird species habitat requirements. Our research provided the baseline information for ROT and PBG impacts on grassland birds in the east/southeastern USA. However, we believe future research should focus on breeding and annual fecundity to better understand how populations will change over time and how working lands conservation might aid this conservation effort without a reduction in livestock productivity. We explored the influence of dietary experiences on ensuing ingestive responses to novel feeds and flavors by lambs. Twenty lambs housed in individual pens were assigned to two groups (10 lambs/group). In four different periods, all animals were offered four different nutritive novel feeds (oats, wheat bran, corn, beep pulp), followed by intra-ruminal infusions of lithium chloride-LiCl (150 mg/kg BW), a toxicant that causes food aversions (Group NE; negative experiences with novel foods), or just or water (Group PE; Control). After exposure, all lambs were tested for their acceptance of single novel feeds (sorghum grain, rice bran, Calf manna{\textregistered} pellets and soybean meal) presented separately and simultaneously in 5-way preference tests with alfalfa (a familiar feed) at familiar and unfamiliar locations. Lambs were also tested for their acceptance and preference for novel flavors (onion, oregano, cinnamon, garlic) added to alfalfa at the unfamiliar location. During exposure, the PE group ate more of the novel feeds than the NE group (P {$<$} 0.05). During testing at both familiar and unfamiliar locations, PE lambs ate more (P {$<$} 0.05) of the novel feeds than NE lambs, displaying greater preferences for novel feeds and a greater Shannon's diversity index (P {$<$} 0.05). No differences between groups regarding ingestive behavior were observed when lambs were offered novel flavors at the unfamiliar location (P {$>$} 0.05), likely due to generalization of familiar cues in flavored alfalfa. Thus, prior negative experiences with novel foods increased neophobia and constrained dietary diversity during exposure to novel feeds, an outcome that could impact the composition and structure of plant communities, as well as the nutrition and welfare of grazing animals. Finally, nutritious and safe cues from familiar feeds may be important at attenuating the influence of such negative experiences in novel feeding environments. The broad perspective regarding marine-organisms biodiversity indicates a latitudinal gradient. Understanding gradients in diversity at large scales has important applications for understanding future impacts on local and regional communities. We use and propose the edible mollusk Chiton articulatus as the basibiont proxy to analyze the spatio-temporal changes in biodiversity patterns of the epibiotic community occupying its scleritome surface. The analyses covered approximately 1817 km of the Mexican Pacific, using seven sites that included samples from Sinaloa ``SIN'' (23{\textdegree}38{$\prime$}N, 106{\textdegree}48{$\prime$}W); Nayarit ``NAY'' (20{\textdegree}45{$\prime$}N, 105{\textdegree}22{$\prime$}W); Jalisco ``JAL'' (19{\textdegree}13{$\prime$}N, 104{\textdegree}41{$\prime$}W); Colima ``COL'' (19{\textdegree}06{$\prime$}N, 104{\textdegree}20{$\prime$}W); Michoacan ``MIC'' (17{\textdegree}54{$\prime$}N, 102{\textdegree}11'"W); Guerrero ``GRO'' (16{\textdegree}48{$\prime$}N, 99{\textdegree}52{$\prime$}W) and Oaxaca ``OAX'' (15{\textdegree}39{$\prime$}N, 96{\textdegree}30{$\prime$}W). A temporal gradient (monthly sampling from October 2015 to October 2016) was also analyzed. Evaluation of the sampling effort demonstrated that 76{\textendash}86\% of the epibiotic fauna was recovered. The most representative epibionts were barnacles, a limpet and an acrothoracican. Abundances of the barnacles and the limpet tended to decrease southwards, while the acrothoracican tended to increase northwards. The epibiotic community did not exhibit the typical latitudinal gradient of diversity expected, but a fragmented pattern within three main areas, which coincide with three of the marine biogeographic ecoregions proposed for the shallow waters of the Warm Temperate Northern Pacific and Tropical Eastern Pacific provinces. These were a northern group within SIN, which belong to the southern border of the Magdalena Transition ecoregion (subtropical); a middle group within NAY, JAL, COL, MIC, and GRO, which belongs to the Mexican Tropical Pacific ecoregion (tropical); and a southern group within OAX exclusively, belonging to the northern border of the Chiapas-Nicaragua ecoregion (tropical). The southern localities showed larger dominances and the central localities exhibited more stable communities. Our evidence suggests that as we move across the different shore types of the Mexican Pacific, the coastline supports distinct biotic assemblages, and the community on the rocky shores is biologically heterogeneous and mainly influenced by conditions at the local level. In consequence, conservation and protection efforts should consider preserving the local heterogeneity of habitats in the study area. Furthermore, since C. articulatus is edible, its epibiota represents an incidental fishing bycatch, with the result that the extraction of the chiton and its epibiota can jeopardy the diversity in the intertidal rocky shore of the Mexican Pacific.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/99GBHBW3/0022519370901244.html}
}

@misc{hwang_genomic_2023,
  title = {Genomic Language Model Predicts Protein Co-Regulation and Function},
  author = {Hwang, Yunha and Cornman, Andre L. and Kellogg, Elizabeth H. and Ovchinnikov, Sergey and Girguis, Peter R.},
  year = {2023},
  month = may,
  primaryclass = {New Results},
  pages = {2023.04.07.536042},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.04.07.536042},
  urldate = {2023-06-28},
  abstract = {Deciphering the relationship between a gene and its genomic context is fundamental to understanding and engineering biological systems. Machine learning has shown promise in learning latent relationships underlying the sequence-structure-function paradigm from massive protein sequence datasets. However, to date, limited attempts have been made in extending this continuum to include higher order genomic context information. Evolutionary processes dictate the specificity of genomic contexts in which a gene is found across phylogenetic distances, and these emergent genomic patterns can be leveraged to uncover functional relationships between gene products. Here, we trained a genomic language model (gLM) on millions of metagenomic scaffolds to learn the latent functional and regulatory relationships between genes. gLM learns contextualized protein embeddings that capture the genomic context as well as the protein sequence itself and encode biologically meaningful and functionally relevant information (e.g., enzymatic function, taxonomy). Our analysis of the attention patterns demonstrates that gLM is learning co-regulated functional modules (i.e., operons). Our findings illustrate that gLM's unsupervised deep learning of the metagenomic corpus is an effective and promising approach to encode functional semantics and regulatory syntax of genes in their genomic contexts and uncover complex relationships between genes in a genomic region.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/HSBIDMGG/Hwang et al. - 2023 - Genomic language model predicts protein co-regulat.pdf}
}

@misc{igashov_equivariant_2022,
  title = {Equivariant {{3D-Conditional Diffusion Models}} for {{Molecular Linker Design}}},
  author = {Igashov, Ilia and St{\"a}rk, Hannes and Vignac, Cl{\'e}ment and Satorras, Victor Garcia and Frossard, Pascal and Welling, Max and Bronstein, Michael and Correia, Bruno},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05274},
  eprint = {2210.05274},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.05274},
  urldate = {2022-12-14},
  abstract = {Fragment-based drug discovery has been an effective paradigm in early-stage drug development. An open challenge in this area is designing linkers between disconnected molecular fragments of interest to obtain chemically-relevant candidate drug molecules. In this work, we propose DiffLinker, an E(3)-equivariant 3D-conditional diffusion model for molecular linker design. Given a set of disconnected fragments, our model places missing atoms in between and designs a molecule incorporating all the initial fragments. Unlike previous approaches that are only able to connect pairs of molecular fragments, our method can link an arbitrary number of fragments. Additionally, the model automatically determines the number of atoms in the linker and its attachment points to the input fragments. We demonstrate that DiffLinker outperforms other methods on the standard datasets generating more diverse and synthetically-accessible molecules. Besides, we experimentally test our method in real-world applications, showing that it can successfully generate valid linkers conditioned on target protein pockets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/UNNZ7NZ5/Igashov et al. - 2022 - Equivariant 3D-Conditional Diffusion Models for Mo.pdf;/Users/pmg/Zotero/storage/SP6623AP/2210.html}
}

@misc{iliadis_comparison_2023,
  title = {A {{Comparison}} of {{Embedding Aggregation Strategies}} in {{Drug-Target Interaction Prediction}}},
  author = {Iliadis, Dimitrios and Baets, Bernard De and Pahikkala, Tapio and Waegeman, Willem},
  year = {2023},
  month = oct,
  primaryclass = {New Results},
  pages = {2023.09.25.559265},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.09.25.559265},
  urldate = {2023-10-06},
  abstract = {The prediction of interactions between novel drugs and biological targets is a vital step in the early stage of the drug discovery pipeline. Many deep learning approaches have been proposed over the last decade, with a substantial fraction of them sharing the same underlying two-branch architecture. Their distinction is limited to the use of different types of feature representations and branches (multi-layer perceptrons, convolutional neural networks, graph neural networks and transformers). In contrast, the strategy used to combine the outputs (embeddings) of the branches has remained mostly the same. The same general architecture has also been used extensively in the area of recommender systems, where the choice of an aggregation strategy is still an open question. In this work, we investigate the effectiveness of three different embedding aggregation strategies in the area of drug-target interaction (DTI) prediction. We formally define these strategies and prove their universal approximator capabilities. We then present experiments that compare the different strategies on benchmark datasets from the area of DTI prediction, showcasing conditions under which specific strategies could be the obvious choice.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/NB23DSBN/Iliadis et al. - 2023 - A Comparison of Embedding Aggregation Strategies i.pdf}
}

@inproceedings{ingraham_generative_2019,
  title = {Generative {{Models}} for {{Graph-Based Protein Design}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ingraham, John and Garg, Vikas and Barzilay, Regina and Jaakkola, Tommi},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-04-06},
  abstract = {Engineered proteins offer the potential to solve many problems in biomedicine, energy, and materials science, but creating designs that succeed is difficult in practice. A significant aspect of this challenge is the complex coupling between protein sequence and 3D structure, with the task of finding a viable design often referred to as the inverse protein folding problem. We develop relational language models for protein sequences that directly condition on a graph specification of the  target structure. Our approach efficiently captures the complex dependencies in proteins by focusing on those that are long-range in sequence but local in 3D space. Our framework significantly improves in both speed and robustness over conventional and deep-learning-based methods for structure-based protein sequence design, and takes a step toward rapid and targeted biomolecular design with the aid of deep generative models.},
  file = {/Users/pmg/Zotero/storage/A6K4QQYD/Ingraham et al. - 2019 - Generative Models for Graph-Based Protein Design.pdf}
}

@misc{ingraham_illuminating_2022,
  title = {Illuminating Protein Space with a Programmable Generative Model},
  author = {Ingraham, John and Baranov, Max and Costello, Zak and Frappier, Vincent and Ismail, Ahmed and Tie, Shan and Wang, Wujie and Xue, Vincent and Obermeyer, Fritz and Beam, Andrew and Grigoryan, Gevorg},
  year = {2022},
  month = dec,
  primaryclass = {New Results},
  pages = {2022.12.01.518682},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.12.01.518682},
  urldate = {2022-12-15},
  abstract = {Three billion years of evolution have produced a tremendous diversity of protein molecules, and yet the full potential of this molecular class is likely far greater. Accessing this potential has been challenging for computation and experiments because the space of possible protein molecules is much larger than the space of those likely to host function. Here we introduce Chroma, a generative model for proteins and protein complexes that can directly sample novel protein structures and sequences and that can be conditioned to steer the generative process towards desired properties and functions. To enable this, we introduce a diffusion process that respects the conformational statistics of polymer ensembles, an efficient neural architecture for molecular systems based on random graph neural networks that enables long-range reasoning with sub-quadratic scaling, equivariant layers for efficiently synthesizing 3D structures of proteins from predicted inter-residue geometries, and a general low-temperature sampling algorithm for diffusion models. We suggest that Chroma can effectively realize protein design as Bayesian inference under external constraints, which can involve symmetries, substructure, shape, semantics, and even natural language prompts. With this unified approach, we hope to accelerate the prospect of programming protein matter for human health, materials science, and synthetic biology.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/SJEKPHZN/Ingraham et al. - 2022 - Illuminating protein space with a programmable gen.pdf}
}

@inproceedings{ivanovic_propagating_2022,
  title = {Propagating {{State Uncertainty Through Trajectory Forecasting}}},
  booktitle = {2022 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Ivanovic, Boris and Lin, Yifeng and Shrivastava, Shubham and Chakravarty, Punarjay and Pavone, Marco},
  year = {2022},
  month = may,
  pages = {2351--2358},
  doi = {10.1109/ICRA46639.2022.9811776},
  abstract = {Uncertainty pervades through the modern robotic autonomy stack, with nearly every component (e.g., sensors, detection, classification, tracking, behavior prediction) producing continuous or discrete probabilistic distributions. Trajectory forecasting, in particular, is surrounded by uncertainty as its inputs are produced by (noisy) upstream perception and its outputs are predictions that are often probabilistic for use in downstream planning. However, most trajectory forecasting methods do not account for upstream uncertainty, instead taking only the most-likely values. As a result, perceptual uncer-tainties are not propagated through forecasting and predictions are frequently overconfident. To address this, we present a novel method for incorporating perceptual state uncertainty in trajectory forecasting, a key component of which is a new statistical distance-based loss function which encourages predicting uncertainties that better match upstream perception. We evaluate our approach both in illustrative simulations and on large-scale, real-world data, demonstrating its efficacy in propagating perceptual state uncertainty through prediction and producing more calibrated predictions.},
  keywords = {Data models,Predictive models,Probabilistic logic,Robot sensing systems,Sensors,Trajectory,Uncertainty},
  file = {/Users/pmg/Zotero/storage/5ZUFTJ6B/Ivanovic et al. - 2022 - Propagating State Uncertainty Through Trajectory F.pdf;/Users/pmg/Zotero/storage/DZKN45JB/9811776.html}
}

@article{jain_biological_2022,
  title = {Biological {{Sequence Design}} with {{GFlowNets}}},
  author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and {Rector-Brooks}, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.04115 [cs, q-bio]},
  eprint = {2203.04115},
  primaryclass = {cs, q-bio},
  urldate = {2022-04-12},
  abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/W8QX9MEX/Jain et al. - 2022 - Biological Sequence Design with GFlowNets.pdf;/Users/pmg/Zotero/storage/J9VJTQG6/2203.html}
}

@misc{jain_biological_2022-1,
  title = {Biological {{Sequence Design}} with {{GFlowNets}}},
  author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and {Rector-Brooks}, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
  year = {2022},
  month = oct,
  number = {arXiv:2203.04115},
  eprint = {2203.04115},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.04115},
  urldate = {2022-10-26},
  abstract = {Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/IQPUZBAR/Jain et al. - 2022 - Biological Sequence Design with GFlowNets.pdf;/Users/pmg/Zotero/storage/GEI7UTB4/2203.html}
}

@misc{jain_multi-objective_2022,
  title = {Multi-{{Objective GFlowNets}}},
  author = {Jain, Moksh and Raparthy, Sharath Chandra and {Hernandez-Garcia}, Alex and {Rector-Brooks}, Jarrid and Bengio, Yoshua and Miret, Santiago and Bengio, Emmanuel},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12765},
  eprint = {2210.12765},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12765},
  urldate = {2022-10-25},
  abstract = {In many applications of machine learning, like drug discovery and material design, the goal is to generate candidates that simultaneously maximize a set of objectives. As these objectives are often conflicting, there is no single candidate that simultaneously maximizes all objectives, but rather a set of Pareto-optimal candidates where one objective cannot be improved without worsening another. Moreover, in practice, these objectives are often under-specified, making the diversity of candidates a key consideration. The existing multi-objective optimization methods focus predominantly on covering the Pareto front, failing to capture diversity in the space of candidates. Motivated by the success of GFlowNets for generation of diverse candidates in a single objective setting, in this paper we consider Multi-Objective GFlowNets (MOGFNs). MOGFNs consist of a novel Conditional GFlowNet which models a family of single-objective sub-problems derived by decomposing the multi-objective optimization problem. Our work is the first to empirically demonstrate conditional GFlowNets. Through a series of experiments on synthetic and benchmark tasks, we empirically demonstrate that MOGFNs outperform existing methods in terms of Hypervolume, R2-distance and candidate diversity. We also demonstrate the effectiveness of MOGFNs over existing methods in active learning settings. Finally, we supplement our empirical results with a careful analysis of each component of MOGFNs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/IXNFWQ7M/Jain et al. - 2022 - Multi-Objective GFlowNets.pdf;/Users/pmg/Zotero/storage/2TZTPHLN/2210.html}
}

@article{jankauskaite_skempi_2019,
  title = {{{SKEMPI}} 2.0: An Updated Benchmark of Changes in Protein-Protein Binding Energy, Kinetics and Thermodynamics upon Mutation},
  shorttitle = {{{SKEMPI}} 2.0},
  author = {Jankauskaite, Justina and {Jim{\'e}nez-Garc{\'i}a}, Brian and Dapkunas, Justas and {Fern{\'a}ndez-Recio}, Juan and Moal, Iain H.},
  year = {2019},
  month = feb,
  journal = {Bioinformatics (Oxford, England)},
  volume = {35},
  number = {3},
  pages = {462--469},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/bty635},
  abstract = {Motivation: Understanding the relationship between the sequence, structure, binding energy, binding kinetics and binding thermodynamics of protein-protein interactions is crucial to understanding cellular signaling, the assembly and regulation of molecular complexes, the mechanisms through which mutations lead to disease, and protein engineering. Results: We present SKEMPI 2.0, a major update to our database of binding free energy changes upon mutation for structurally resolved protein-protein interactions. This version now contains manually curated binding data for 7085 mutations, an increase of 133\%, including changes in kinetics for 1844 mutations, enthalpy and entropy changes for 443 mutations, and 440 mutations, which abolish detectable binding. Availability and implementation: The database is available as supplementary data and at https://life.bsc.es/pid/skempi2/. Supplementary information: Supplementary data are available at Bioinformatics online.},
  langid = {english},
  pmcid = {PMC6361233},
  pmid = {30020414},
  keywords = {{Databases, Protein},Kinetics,Mutation,Protein Binding,Thermodynamics},
  file = {/Users/pmg/Zotero/storage/CVQ65N6G/Jankauskaite et al. - 2019 - SKEMPI 2.0 an updated benchmark of changes in pro.pdf}
}

@article{jarzab_meltome_2020,
  title = {Meltome Atlas{\textemdash}Thermal Proteome Stability across the Tree of Life},
  author = {Jarzab, Anna and Kurzawa, Nils and Hopf, Thomas and Moerch, Matthias and Zecha, Jana and Leijten, Niels and Bian, Yangyang and Musiol, Eva and Maschberger, Melanie and Stoehr, Gabriele and Becher, Isabelle and Daly, Charlotte and Samaras, Patroklos and Mergner, Julia and Spanier, Britta and Angelov, Angel and Werner, Thilo and Bantscheff, Marcus and Wilhelm, Mathias and Klingenspor, Martin and Lemeer, Simone and Liebl, Wolfgang and Hahne, Hannes and Savitski, Mikhail M. and Kuster, Bernhard},
  year = {2020},
  month = may,
  journal = {Nature Methods},
  volume = {17},
  number = {5},
  pages = {495--503},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-020-0801-4},
  urldate = {2022-04-06},
  abstract = {We have used a mass spectrometry-based proteomic approach to compile an atlas of the thermal stability of 48,000 proteins across 13 species ranging from archaea to humans and covering melting temperatures of 30{\textendash}90\,{\textdegree}C. Protein sequence, composition and size affect thermal stability in prokaryotes and eukaryotic proteins show a nonlinear relationship between the degree of disordered protein structure and thermal stability. The data indicate that evolutionary conservation of protein complexes is reflected by similar thermal stability of their proteins, and we show examples in which genomic alterations can affect thermal stability. Proteins of the respiratory chain were found to be very stable in many organisms, and human mitochondria showed close to normal respiration at 46\,{\textdegree}C. We also noted cell-type-specific effects that can affect protein stability or the efficacy of drugs. This meltome atlas broadly defines the proteome amenable to thermal profiling in biology and drug discovery and can be explored online at http://meltomeatlas.proteomics.wzw.tum.de:5003/ and http://www.proteomicsdb.org.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Drug discovery,Proteome informatics,Proteomics},
  file = {/Users/pmg/Zotero/storage/QD8Q9QLC/Jarzab et al. - 2020 - Meltome atlas—thermal proteome stability across th.pdf;/Users/pmg/Zotero/storage/L28VB9KZ/s41592-020-0801-4.html}
}

@misc{jawaid_improving_2023,
  title = {Improving Few-Shot Learning-Based Protein Engineering with Evolutionary Sampling},
  author = {Jawaid, M. Zaki and Yeo, Robin W. and Gautam, Aayushma and Gainous, T. Blair and Hart, Daniel O. and Daley, Timothy P.},
  year = {2023},
  month = may,
  number = {arXiv:2305.15441},
  eprint = {2305.15441},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.15441},
  urldate = {2023-05-26},
  abstract = {Designing novel functional proteins remains a slow and expensive process due to a variety of protein engineering challenges; in particular, the number of protein variants that can be experimentally tested in a given assay pales in comparison to the vastness of the overall sequence space, resulting in low hit rates and expensive wet lab testing cycles. In this paper, we propose a few-shot learning approach to novel protein design that aims to accelerate the expensive wet lab testing cycle and is capable of leveraging a training dataset that is both small and skewed (\${\textbackslash}approx 10\^5\$ datapoints, \${$<$} 1{\textbackslash}\%\$ positive hits). Our approach is composed of two parts: a semi-supervised transfer learning approach to generate a discrete fitness landscape for a desired protein function and a novel evolutionary Monte Carlo Markov Chain sampling algorithm to more efficiently explore the fitness landscape. We demonstrate the performance of our approach by experimentally screening predicted high fitness gene activators, resulting in a dramatically improved hit rate compared to existing methods. Our method can be easily adapted to other protein engineering and design problems, particularly where the cost associated with obtaining labeled data is significantly high. We have provided open source code for our method at https:// github.com/SuperSecretBioTech/evolutionary\_monte\_carlo\_search.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/LT5XKEWN/Jawaid et al. - 2023 - Improving few-shot learning-based protein engineer.pdf;/Users/pmg/Zotero/storage/NYWTJY82/2305.html}
}

@article{jennings_pel_2015,
  title = {Pel Is a Cationic Exopolysaccharide That Cross-Links Extracellular {{DNA}} in the {{Pseudomonas}} Aeruginosa Biofilm Matrix},
  author = {Jennings, Laura K. and Storek, Kelly M. and Ledvina, Hannah E. and Coulon, Charl{\`e}ne and Marmont, Lindsey S. and Sadovskaya, Irina and Secor, Patrick R. and Tseng, Boo Shan and Scian, Michele and Filloux, Alain and Wozniak, Daniel J. and Howell, P. Lynne and Parsek, Matthew R.},
  year = {2015},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {36},
  pages = {11353--11358},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1503058112},
  urldate = {2023-01-15},
  abstract = {Biofilm formation is a complex, ordered process. In the opportunistic pathogen Pseudomonas aeruginosa, Psl and Pel exopolysaccharides and extracellular DNA (eDNA) serve as structural components of the biofilm matrix. Despite intensive study, Pel's chemical structure and spatial localization within mature biofilms remain unknown. Using specialized carbohydrate chemical analyses, we unexpectedly found that Pel is a positively charged exopolysaccharide composed of partially acetylated 1{\textrightarrow}4 glycosidic linkages of N-acetylgalactosamine and N-acetylglucosamine. Guided by the knowledge of Pel's sugar composition, we developed a tool for the direct visualization of Pel in biofilms by combining Pel-specific Wisteria floribunda lectin staining with confocal microscopy. The results indicate that Pel cross-links eDNA in the biofilm stalk via ionic interactions. Our data demonstrate that the cationic charge of Pel is distinct from that of other known P. aeruginosa exopolysaccharides and is instrumental in its ability to interact with other key biofilm matrix components.}
}

@article{jennings_pel_2015-1,
  title = {Pel Is a Cationic Exopolysaccharide That Cross-Links Extracellular {{DNA}} in the {{Pseudomonas}} Aeruginosa Biofilm Matrix},
  author = {Jennings, Laura K. and Storek, Kelly M. and Ledvina, Hannah E. and Coulon, Charl{\`e}ne and Marmont, Lindsey S. and Sadovskaya, Irina and Secor, Patrick R. and Tseng, Boo Shan and Scian, Michele and Filloux, Alain and Wozniak, Daniel J. and Howell, P. Lynne and Parsek, Matthew R.},
  year = {2015},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {36},
  pages = {11353--11358},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1503058112},
  urldate = {2023-01-15},
  abstract = {Biofilm formation is a complex, ordered process. In the opportunistic pathogen Pseudomonas aeruginosa, Psl and Pel exopolysaccharides and extracellular DNA (eDNA) serve as structural components of the biofilm matrix. Despite intensive study, Pel's chemical structure and spatial localization within mature biofilms remain unknown. Using specialized carbohydrate chemical analyses, we unexpectedly found that Pel is a positively charged exopolysaccharide composed of partially acetylated 1{\textrightarrow}4 glycosidic linkages of N-acetylgalactosamine and N-acetylglucosamine. Guided by the knowledge of Pel's sugar composition, we developed a tool for the direct visualization of Pel in biofilms by combining Pel-specific Wisteria floribunda lectin staining with confocal microscopy. The results indicate that Pel cross-links eDNA in the biofilm stalk via ionic interactions. Our data demonstrate that the cationic charge of Pel is distinct from that of other known P. aeruginosa exopolysaccharides and is instrumental in its ability to interact with other key biofilm matrix components.}
}

@misc{jin_junction_2019,
  title = {Junction {{Tree Variational Autoencoder}} for {{Molecular Graph Generation}}},
  author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  year = {2019},
  month = mar,
  number = {arXiv:1802.04364},
  eprint = {1802.04364},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.04364},
  urldate = {2022-08-17},
  abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/3T3PPCRI/Jin et al. - 2019 - Junction Tree Variational Autoencoder for Molecula.pdf;/Users/pmg/Zotero/storage/8UGWSEL3/1802.html}
}

@article{jing_equivariant_2021,
  title = {Equivariant {{Graph Neural Networks}} for {{3D Macromolecular Structure}}},
  author = {Jing, Bowen and Eismann, Stephan and Soni, Pratham N. and Dror, Ron O.},
  year = {2021},
  month = jul,
  journal = {arXiv:2106.03843 [cs, q-bio]},
  eprint = {2106.03843},
  primaryclass = {cs, q-bio},
  urldate = {2022-04-07},
  abstract = {Representing and reasoning about 3D structures of macromolecules is emerging as a distinct challenge in machine learning. Here, we extend recent work on geometric vector perceptrons and apply equivariant graph neural networks to a wide range of tasks from structural biology. Our method outperforms all reference architectures on three out of eight tasks in the ATOM3D benchmark, is tied for first on two others, and is competitive with equivariant networks using higher-order representations and spherical harmonic convolutions. In addition, we demonstrate that transfer learning can further improve performance on certain downstream tasks. Code is available at https://github.com/drorlab/gvp-pytorch.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/Y758PPR6/Jing et al. - 2021 - Equivariant Graph Neural Networks for 3D Macromole.pdf;/Users/pmg/Zotero/storage/WQXBFDVE/2106.html}
}

@article{jing_learning_2021,
  title = {Learning from {{Protein Structure}} with {{Geometric Vector Perceptrons}}},
  author = {Jing, Bowen and Eismann, Stephan and Suriana, Patricia and Townshend, Raphael J. L. and Dror, Ron},
  year = {2021},
  month = may,
  journal = {arXiv:2009.01411 [cs, q-bio, stat]},
  eprint = {2009.01411},
  primaryclass = {cs, q-bio, stat},
  urldate = {2022-04-06},
  abstract = {Learning on 3D structures of large biomolecules is emerging as a distinct area in machine learning, but there has yet to emerge a unifying network architecture that simultaneously leverages the graph-structured and geometric aspects of the problem domain. To address this gap, we introduce geometric vector perceptrons, which extend standard dense layers to operate on collections of Euclidean vectors. Graph neural networks equipped with such layers are able to perform both geometric and relational reasoning on efficient and natural representations of macromolecular structure. We demonstrate our approach on two important problems in learning from protein structure: model quality assessment and computational protein design. Our approach improves over existing classes of architectures, including state-of-the-art graph-based and voxel-based methods. We release our code at https://github.com/drorlab/gvp.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Geometry,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/DKVCNE8T/Jing et al. - 2021 - Learning from Protein Structure with Geometric Vec.pdf;/Users/pmg/Zotero/storage/DH33C34A/2009.html}
}

@misc{jo_score-based_2022,
  title = {Score-Based {{Generative Modeling}} of {{Graphs}} via the {{System}} of {{Stochastic Differential Equations}}},
  author = {Jo, Jaehyeong and Lee, Seul and Hwang, Sung Ju},
  year = {2022},
  month = jun,
  number = {arXiv:2202.02514},
  eprint = {2202.02514},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.02514},
  urldate = {2023-04-17},
  abstract = {Generating graph-structured data requires learning the underlying distribution of graphs. Yet, this is a challenging problem, and the previous graph generative methods either fail to capture the permutation-invariance property of graphs or cannot sufficiently model the complex dependency between nodes and edges, which is crucial for generating real-world graphs such as molecules. To overcome such limitations, we propose a novel score-based generative model for graphs with a continuous-time framework. Specifically, we propose a new graph diffusion process that models the joint distribution of the nodes and edges through a system of stochastic differential equations (SDEs). Then, we derive novel score matching objectives tailored for the proposed diffusion process to estimate the gradient of the joint log-density with respect to each component, and introduce a new solver for the system of SDEs to efficiently sample from the reverse diffusion process. We validate our graph generation method on diverse datasets, on which it either achieves significantly superior or competitive performance to the baselines. Further analysis shows that our method is able to generate molecules that lie close to the training distribution yet do not violate the chemical valency rule, demonstrating the effectiveness of the system of SDEs in modeling the node-edge relationships. Our code is available at https://github.com/harryjo97/GDSS.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/WC9WB8P6/Jo et al. - 2022 - Score-based Generative Modeling of Graphs via the .pdf;/Users/pmg/Zotero/storage/UUI2FZ2C/2202.html}
}

@misc{johnson_computational_2023,
  title = {Computational {{Scoring}} and {{Experimental Evaluation}} of {{Enzymes Generated}} by {{Neural Networks}}},
  author = {Johnson, Sean R. and Fu, Xiaozhi and Viknander, Sandra and Goldin, Clara and Monaco, Sarah and Zelezniak, Aleksej and Yang, Kevin K.},
  year = {2023},
  month = mar,
  primaryclass = {New Results},
  pages = {2023.03.04.531015},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.03.04.531015},
  urldate = {2023-03-20},
  abstract = {In recent years, generative protein sequence models have been developed to sample novel sequences. However, predicting whether generated proteins will fold and function remains challenging. We evaluate computational metrics to assess the quality of enzyme sequences produced by three contrasting generative models: ancestral sequence reconstruction, a generative adversarial network, and a protein language model. Focusing on two enzyme families, we expressed and purified over 440 natural and generated sequences with 70-90\% identity to the most similar natural sequences to benchmark computational metrics for predicting in vitro enzyme activity. Over three rounds of experiments, we developed a computational filter that improved experimental success rates by 44-100\%. Surprisingly, neither sequence identity to natural sequences nor AlphaFold2 residue-confidence scores were predictive of enzyme activity. The proposed metrics and models will drive protein engineering research by serving as a benchmark for generative protein sequence models and helping to select active variants to test experimentally.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/IUMGZ7N5/Johnson et al. - 2023 - Computational Scoring and Experimental Evaluation .pdf}
}

@article{johnson_hidden_2010,
  title = {Hidden {{Markov}} Model Speed Heuristic and Iterative {{HMM}} Search Procedure},
  author = {Johnson, L. Steven and Eddy, Sean R. and Portugaly, Elon},
  year = {2010},
  month = aug,
  journal = {BMC Bioinformatics},
  volume = {11},
  number = {1},
  pages = {431},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-431},
  urldate = {2023-01-23},
  abstract = {Profile hidden Markov models (profile-HMMs) are sensitive tools for remote protein homology detection, but the main scoring algorithms, Viterbi or Forward, require considerable time to search large sequence databases.},
  keywords = {Entropy Weighting,Iterative Search,Profile Hide Markov Model,Search Time,Test Database},
  file = {/Users/pmg/Zotero/storage/AJMPQJSJ/Johnson et al. - 2010 - Hidden Markov model speed heuristic and iterative .pdf;/Users/pmg/Zotero/storage/6LHKNJ5Y/1471-2105-11-431.html}
}

@article{jokinen_mgpfusion_2018,
  title = {{{mGPfusion}}: Predicting Protein Stability Changes with {{Gaussian}} Process Kernel Learning and Data Fusion},
  shorttitle = {{{mGPfusion}}},
  author = {Jokinen, Emmi and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri},
  year = {2018},
  month = jul,
  journal = {Bioinformatics},
  volume = {34},
  number = {13},
  pages = {i274-i283},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bty238},
  urldate = {2023-09-22},
  abstract = {Proteins are commonly used by biochemical industry for numerous processes. Refining these proteins' properties via mutations causes stability effects as well. Accurate computational method to predict how mutations affect protein stability is necessary to facilitate efficient protein design. However, accuracy of predictive models is ultimately constrained by the limited availability of experimental data.We have developed mGPfusion, a novel Gaussian process (GP) method for predicting protein's stability changes upon single and multiple mutations. This method complements the limited experimental data with large amounts of molecular simulation data. We introduce a Bayesian data fusion model that re-calibrates the experimental and in silico data sources and then learns a predictive GP model from the combined data. Our protein-specific model requires experimental data only regarding the protein of interest and performs well even with few experimental measurements. The mGPfusion models proteins by contact maps and infers the stability effects caused by mutations with a mixture of graph kernels. Our results show that mGPfusion outperforms state-of-the-art methods in predicting protein stability on a dataset of 15 different proteins and that incorporating molecular simulation data improves the model learning and prediction accuracy.Software implementation and datasets are available at github.com/emmijokinen/mgpfusion.Supplementary data are available at Bioinformatics online.},
  file = {/Users/pmg/Zotero/storage/K99368RD/Jokinen et al. - 2018 - mGPfusion predicting protein stability changes wi.pdf}
}

@inproceedings{joshi_expressive_2022,
  title = {On the {{Expressive Power}} of {{Geometric Graph Neural Networks}}},
  booktitle = {The {{First Learning}} on {{Graphs Conference}}},
  author = {Joshi, Chaitanya K. and Bodnar, Cristian and Mathis, Simon V. and Cohen, Taco and Lio, Pietro},
  year = {2022},
  month = dec,
  urldate = {2023-04-17},
  abstract = {The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the lens of the Weisfeiler-Leman (WL) graph isomorphism test. Yet, many graphs in scientific and engineering applications come embedded in Euclidean space with an additional notion of geometric isomorphism, which is not covered by the WL framework. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of GNNs that are invariant or equivariant to physical symmetries in terms of the classes of geometric graphs they can distinguish. This allows us to formalise the advantages of equivariant GNN layers over invariant ones: equivariant GNNs have greater expressive power as they enable propagating geometric information beyond local neighbourhoods, while invariant GNNs cannot distinguish graphs that are locally similar, highlighting their inability to compute global geometric quantities.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/XDD3IEE7/Joshi et al. - 2022 - On the Expressive Power of Geometric Graph Neural .pdf}
}

@article{jumper_highly_2021,
  title = {Highly Accurate Protein Structure Prediction with {{AlphaFold}}},
  author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {583--589},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03819-2},
  urldate = {2022-06-01},
  abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1{\textendash}4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence{\textemdash}the structure prediction component of the `protein folding problem'8{\textemdash}has been an important open research problem for more than 50~years9. Despite recent progress10{\textendash}14, existing methods fall far~short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational biophysics,Machine learning,Protein structure predictions,Structural biology},
  file = {/Users/pmg/Zotero/storage/22ZNC5WI/Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf;/Users/pmg/Zotero/storage/BQIYZ5CW/s41586-021-03819-2.html}
}

@misc{kaminski_plm-blast_2023,
  title = {{{pLM-BLAST}} - Distant Homology Detection Based on Direct Comparison of Sequence Representations from Protein Language Models},
  author = {Kaminski, Kamil and Ludwiczak, Jan and Pawlicki, Kamil and Alva, Vikram and {Dunin-Horkawicz}, Stanislaw},
  year = {2023},
  month = jul,
  primaryclass = {New Results},
  pages = {2022.11.24.517862},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.11.24.517862},
  urldate = {2023-07-31},
  abstract = {Motivation: The detection of homology through sequence comparison is a typical first step in the study of protein function and evolution. In this work, we explore the applicability of protein language models to this task. Results: We introduce pLM-BLAST, a tool inspired by BLAST, that detects distant homology by comparing single-sequence representations (embeddings) derived from a protein language model, ProtT5. Our benchmarks reveal that pLM-BLAST maintains a level of accuracy on par with HHsearch for both highly similar sequences (with over 50\% identity) and markedly divergent sequences (with less than 30\% identity), while being significantly faster. Additionally, pLM-BLAST stands out among other embedding-based tools due to its ability to compute local alignments. We show that these local alignments, produced by pLM-BLAST, often connect highly divergent proteins, thereby highlighting its potential to uncover previously undiscovered homologous relationships and improve protein annotation. Availability and Implementation: pLM-BLAST is accessible via the MPI Bioinformatics Toolkit as a web server for searching precomputed databases (https://toolkit.tuebingen.mpg.de/tools/plmblast). It is also available as a standalone tool for building custom databases and performing batch searches (https://github.com/labstructbioinf/pLM-BLAST).},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/F463PGD7/Kaminski et al. - 2023 - pLM-BLAST - distant homology detection based on di.pdf}
}

@article{kang_conditional_2019,
  title = {Conditional {{Molecular Design}} with {{Deep Generative Models}}},
  author = {Kang, Seokho and Cho, Kyunghyun},
  year = {2019},
  month = jan,
  journal = {Journal of Chemical Information and Modeling},
  volume = {59},
  number = {1},
  pages = {43--52},
  publisher = {{American Chemical Society}},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00263},
  urldate = {2022-12-08},
  abstract = {Although machine learning has been successfully used to propose novel molecules that satisfy desired properties, it is still challenging to explore a large chemical space efficiently. In this paper, we present a conditional molecular design method that facilitates generating new molecules with desired properties. The proposed model, which simultaneously performs both property prediction and molecule generation, is built as a semisupervised variational autoencoder trained on a set of existing molecules with only a partial annotation. We generate new molecules with desired properties by sampling from the generative distribution estimated by the model. We demonstrate the effectiveness of the proposed model by evaluating it on drug-like molecules. The model improves the performance of property prediction by exploiting unlabeled molecules and efficiently generates novel molecules fulfilling various target conditions.},
  file = {/Users/pmg/Zotero/storage/G6F8CBGJ/Kang and Cho - 2019 - Conditional Molecular Design with Deep Generative .pdf}
}

@article{katoh_mafft_2002,
  title = {{{MAFFT}}: A Novel Method for Rapid Multiple Sequence Alignment Based on Fast {{Fourier}} Transform},
  shorttitle = {{{MAFFT}}},
  author = {Katoh, Kazutaka and Misawa, Kazuharu and Kuma, Kei-ichi and Miyata, Takashi},
  year = {2002},
  month = jul,
  journal = {Nucleic Acids Research},
  volume = {30},
  number = {14},
  pages = {3059--3066},
  issn = {0305-1048},
  doi = {10.1093/nar/gkf436},
  urldate = {2022-09-22},
  abstract = {A multiple sequence alignment program, MAFFT, has been developed. The CPU time is drastically reduced as compared with existing methods. MAFFT includes two novel techniques. (i) Homo logous regions are rapidly identified by the fast Fourier transform (FFT), in which an amino acid sequence is converted to a sequence composed of volume and polarity values of each amino acid residue. (ii) We propose a simplified scoring system that performs well for reducing CPU time and increasing the accuracy of alignments even for sequences having large insertions or extensions as well as distantly related sequences of similar length. Two different heuristics, the progressive method (FFT-NS-2) and the iterative refinement method (FFT-NS-i), are implemented in MAFFT. The performances of FFT-NS-2 and FFT-NS-i were compared with other methods by computer simulations and benchmark tests; the CPU time of FFT-NS-2 is drastically reduced as compared with CLUSTALW with comparable accuracy. FFT-NS-i is over 100 times faster than T-COFFEE, when the number of input sequences exceeds 60, without sacrificing the accuracy.},
  file = {/Users/pmg/Zotero/storage/WINE8QNZ/Katoh et al. - 2002 - MAFFT a novel method for rapid multiple sequence .pdf;/Users/pmg/Zotero/storage/MNGELR8F/2904316.html}
}

@misc{kempen_foldseek_2022,
  title = {Foldseek: Fast and Accurate Protein Structure Search},
  shorttitle = {Foldseek},
  author = {van Kempen, Michel and Kim, Stephanie and Tumescheit, Charlotte and Mirdita, Milot and Gilchrist, Cameron L. M. and Soeding, Johannes and Steinegger, Martin},
  year = {2022},
  month = sep,
  primaryclass = {New Results},
  pages = {2022.02.07.479398},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.02.07.479398},
  urldate = {2022-09-21},
  abstract = {Highly accurate structure prediction methods are generating an avalanche of publicly available protein structures. Searching through these structures is becoming the main bottleneck in their analysis. Foldseek enables fast and sensitive comparisons of large structure sets. It reaches sensitivities similar to state-of-the-art structural aligners while being four to five orders of magnitude faster. Foldseek is free open-source software available at foldseek.com and as a webserver at search.foldseek.com.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/V55X9TYC/Kempen et al. - 2022 - Foldseek fast and accurate protein structure sear.pdf;/Users/pmg/Zotero/storage/XJI6YN9J/2022.02.07.html}
}

@article{khurana_deepsol_2018,
  title = {{{DeepSol}}: A Deep Learning Framework for Sequence-Based Protein Solubility Prediction},
  shorttitle = {{{DeepSol}}},
  author = {Khurana, Sameer and Rawi, Reda and Kunji, Khalid and Chuang, Gwo-Yu and Bensmail, Halima and Mall, Raghvendra},
  year = {2018},
  month = aug,
  journal = {Bioinformatics (Oxford, England)},
  volume = {34},
  number = {15},
  pages = {2605--2613},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/bty166},
  abstract = {Motivation: Protein solubility plays a vital role in pharmaceutical research and production yield. For a given protein, the extent of its solubility can represent the quality of its function, and is ultimately defined by its sequence. Thus, it is imperative to develop novel, highly accurate in silico sequence-based protein solubility predictors. In this work we propose, DeepSol, a novel Deep Learning-based protein solubility predictor. The backbone of our framework is a convolutional neural network that exploits k-mer structure and additional sequence and structural features extracted from the protein sequence. Results: DeepSol outperformed all known sequence-based state-of-the-art solubility prediction methods and attained an accuracy of 0.77 and Matthew's correlation coefficient of 0.55. The superior prediction accuracy of DeepSol allows to screen for sequences with enhanced production capacity and can more reliably predict solubility of novel proteins. Availability and implementation: DeepSol's best performing models and results are publicly deposited at https://doi.org/10.5281/zenodo.1162886 (Khurana and Mall, 2018). Supplementary information: Supplementary data are available at Bioinformatics online.},
  langid = {english},
  pmcid = {PMC6355112},
  pmid = {29554211},
  keywords = {Amino Acid Sequence,Computational Biology,Computer Simulation,Deep Learning,Proteins,Solubility},
  file = {/Users/pmg/Zotero/storage/UVKSFHJ6/Khurana et al. - 2018 - DeepSol a deep learning framework for sequence-ba.pdf}
}

@article{kingma_introduction_2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  primaryclass = {cs, stat},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  urldate = {2022-06-01},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/VAZER794/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/Users/pmg/Zotero/storage/E5P7JAVV/1906.html}
}

@misc{kingma_variational_2022,
  title = {Variational {{Diffusion Models}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  year = {2022},
  month = dec,
  number = {arXiv:2107.00630},
  eprint = {2107.00630},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.00630},
  urldate = {2022-12-14},
  abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/V9NYSWFT/Kingma et al. - 2022 - Variational Diffusion Models.pdf;/Users/pmg/Zotero/storage/2HBTEHYX/2107.html}
}

@misc{kipf_semi-supervised_2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.02907},
  urldate = {2022-09-01},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/5AJRKKRS/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/Users/pmg/Zotero/storage/YUZH5DNS/1609.html}
}

@misc{kipf_variational_2016,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2016},
  month = nov,
  number = {arXiv:1611.07308},
  eprint = {1611.07308},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.07308},
  urldate = {2022-08-17},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/5QDXYG3C/Kipf and Welling - 2016 - Variational Graph Auto-Encoders.pdf;/Users/pmg/Zotero/storage/AZ5XV7Q7/1611.html}
}

@misc{kirjner_optimizing_2023,
  title = {{Optimizing Protein Fitness Using Gibbs Sampling with Graph-based Smoothing}},
  author = {Kirjner, Andrew and Yim, Jason and Samusevich, Raman and Jaakkola, Tommi and Barzilay, Regina and Fiete, Ila},
  year = {2023},
  month = jul,
  number = {arXiv:2307.00494},
  eprint = {2307.00494},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.00494},
  urldate = {2023-11-02},
  abstract = {The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/V8YF3D4T/Kirjner et al. - 2023 - Optimizing protein fitness using Gibbs sampling wi.pdf;/Users/pmg/Zotero/storage/EAVT9MR4/2307.html}
}

@inproceedings{kirschner_adaptive_2019,
  title = {Adaptive and {{Safe Bayesian Optimization}} in {{High Dimensions}} via {{One-Dimensional Subspaces}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kirschner, Johannes and Mutny, Mojmir and Hiller, Nicole and Ischebeck, Rasmus and Krause, Andreas},
  year = {2019},
  month = may,
  pages = {3429--3438},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-23},
  abstract = {Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/9LLGHU8M/Kirschner et al. - 2019 - Adaptive and Safe Bayesian Optimization in High Di.pdf;/Users/pmg/Zotero/storage/CVNB8QMF/Kirschner et al. - 2019 - Adaptive and Safe Bayesian Optimization in High Di.pdf}
}

@inproceedings{kohavi_study_1995,
  title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
  booktitle = {Proceedings of the 14th International Joint Conference on {{Artificial}} Intelligence - {{Volume}} 2},
  author = {Kohavi, Ron},
  year = {1995},
  month = aug,
  series = {{{IJCAI}}'95},
  pages = {1137--1143},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  urldate = {2022-09-26},
  abstract = {We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds.},
  isbn = {978-1-55860-363-9}
}

@inproceedings{kolli_data-driven_2022,
  title = {Data-{{Driven Optimization}} for {{Protein Design}}: {{Workflows}}, {{Algorithms}} and {{Metrics}}},
  shorttitle = {Data-{{Driven Optimization}} for {{Protein Design}}},
  booktitle = {{{ICLR2022 Machine Learning}} for {{Drug Discovery}}},
  author = {Kolli, Sathvik and Lu, Amy X. and Geng, Xinyang and Kumar, Aviral and Levine, Sergey},
  year = {2022},
  month = may,
  urldate = {2023-03-13},
  abstract = {Recent works have successfully demonstrated the ability of deep neural networks in predicting important properties such as fitness and stability from protein sequences via supervised learning. However, the use of learned deep neural network models for the task of designing de novo proteins that maximize a certain fitness value with backbones from scratch remains under-explored. In this paper, we study the problem of designing proteins where the optimization be carried out in a purely data-driven, ``offline'' manner, by utilizing databases of experimental data collected from wet lab evaluations. Synthesis of proteins proposed by the algorithm in an experimental setup in a wet lab, which incurs a big manual overhead for designers, is not allowed. Such an offline optimization problem require that a practitioner make several several design choices: a designer must decide what data distribution to train on, how their method would be evaluated, and must additionally devise workflows for tuning the optimization method they wish to use. In this paper, we perform a systematic study of various design choices that arise in in protein design, grounded in the problem of optimizing for protein stability, and use these insights to propose workflows, protocols and metrics to assist practitioners in effectively applying such data-driven approaches to protein design problems.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/UEN6FYL3/Kolli et al. - 2022 - Data-Driven Optimization for Protein Design Workf.pdf}
}

@techreport{krapp_pesto_2022,
  type = {Preprint},
  title = {{PeSTo: Parameter-Free Geometric Deep Learning for Accurate Prediction of Protein Interacting Interfaces}},
  shorttitle = {{{PeSTo}}},
  author = {Krapp, Lucien F. and Abriata, Luciano A. and Rodriguez, Fabio Cort{\'e}s and Peraro, Matteo Dal},
  year = {2022},
  month = may,
  institution = {{Bioinformatics}},
  doi = {10.1101/2022.05.09.491165},
  urldate = {2022-08-05},
  abstract = {Predicting the interactions that a protein can establish with other molecules from its structure remains a major challenge. As shown by recent applications to tertiary structure prediction and opposite to current mainstream methods for interaction interface prediction, low-level, geometry-based, physicochemical-agnostic representations of structures have several advantages over methods that require precalculation of surfaces, charges, hydrophobicity, and other kinds of parameterizations. Here we introduce a new geometric transformer that acts directly on protein atoms labelled with nothing more than element names. The resulting model outperforms the state of the art for the prediction of protein-protein interaction interfaces and distinguishes interfaces with nucleic acids, lipids, small molecules and ions with high confidence. The low computational cost of this method (available online at https://pesto.epfl.ch/) enables processing high volumes of structural data, such as molecular dynamics trajectories allowing the discovery of interfaces that remain inconspicuous in static experimentally solved structures.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/IGD62IQB/Krapp et al. - 2022 - PeSTo parameter-free geometric deep learning for .pdf}
}

@inproceedings{kristiadi_being_2020,
  title = {Being {{Bayesian}}, {{Even Just}} a {{Bit}}, {{Fixes Overconfidence}} in {{ReLU Networks}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Kristiadi, Agustinus and Hein, Matthias and Hennig, Philipp},
  year = {2020},
  month = nov,
  pages = {5436--5446},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-04-12},
  abstract = {The point estimates of ReLU classification networks{\textemdash}arguably the most widely used neural network architecture{\textemdash}have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is ``to be a bit Bayesian''. These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/2PD4ASUZ/Kristiadi et al. - 2020 - Being Bayesian, Even Just a Bit, Fixes Overconfide.pdf;/Users/pmg/Zotero/storage/3378WM3W/Kristiadi et al. - 2020 - Being Bayesian, Even Just a Bit, Fixes Overconfide.pdf}
}

@article{kryshtafovych_critical_2019,
  title = {{Critical Assessment of Methods of Protein Structure Prediction (CASP){\textemdash}{{Round XIII}}}},
  author = {Kryshtafovych, Andriy and Schwede, Torsten and Topf, Maya and Fidelis, Krzysztof and Moult, John},
  year = {2019},
  journal = {Proteins: Structure, Function, and Bioinformatics},
  volume = {87},
  number = {12},
  pages = {1011--1020},
  issn = {1097-0134},
  doi = {10.1002/prot.25823},
  urldate = {2022-09-22},
  abstract = {CASP (critical assessment of structure prediction) assesses the state of the art in modeling protein structure from amino acid sequence. The most recent experiment (CASP13 held in 2018) saw dramatic progress in structure modeling without use of structural templates (historically ``ab initio'' modeling). Progress was driven by the successful application of deep learning techniques to predict inter-residue distances. In turn, these results drove dramatic improvements in three-dimensional structure accuracy: With the proviso that there are an adequate number of sequences known for the protein family, the new methods essentially solve the long-standing problem of predicting the fold topology of monomeric proteins. Further, the number of sequences required in the alignment has fallen substantially. There is also substantial improvement in the accuracy of template-based models. Other areas{\textemdash}model refinement, accuracy estimation, and the structure of protein assemblies{\textemdash}have again yielded interesting results. CASP13 placed increased emphasis on the use of sparse data together with modeling and chemical crosslinking, SAXS, and NMR all yielded more mature results. This paper summarizes the key outcomes of CASP13. The special issue of PROTEINS contains papers describing the CASP13 assessments in each modeling category and contributions from the participants.},
  langid = {english},
  keywords = {CASP,community wide experiment,protein structure prediction},
  file = {/Users/pmg/Zotero/storage/7WMK2J9N/Kryshtafovych et al. - 2019 - Critical assessment of methods of protein structur.pdf;/Users/pmg/Zotero/storage/BNAHQ378/prot.html}
}

@inproceedings{kucera_proteinshake_2023,
  title = {{ProteinShake: Building Datasets and Benchmarks for Deep Learning on Protein Structures}},
  shorttitle = {{{ProteinShake}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Kucera, Tim and Oliver, Carlos and Chen, Dexiong and Borgwardt, Karsten},
  year = {2023},
  month = nov,
  urldate = {2024-02-01},
  abstract = {We present ProteinShake, a Python software package that simplifies dataset creation and model evaluation for deep learning on protein structures. Users can create custom datasets or load an extensive set of pre-processed datasets from biological data repositories such as the Protein Data Bank (PDB) and AlphaFoldDB. Each dataset is associated with prediction tasks and evaluation functions covering a broad array of biological challenges. A benchmark on these tasks shows that pre- training almost always improves performance, the optimal data modality (graphs, voxel grids, or point clouds) is task-dependent, and models struggle to generalize to new structures. ProteinShake makes protein structure data easily accessible and comparison among models straightforward, providing challenging benchmark settings with real-world implications. ProteinShake is available at: https://proteinshake.ai},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/CUBJ9A5R/Kucera et al. - 2023 - ProteinShake Building datasets and benchmarks for.pdf}
}

@misc{kuleshov_accurate_2018,
  title = {Accurate {{Uncertainties}} for {{Deep Learning Using Calibrated Regression}}},
  author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  year = {2018},
  month = jun,
  number = {arXiv:1807.00263},
  eprint = {1807.00263},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.00263},
  urldate = {2024-01-17},
  abstract = {Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/T7X3SVUR/Kuleshov et al. - 2018 - Accurate Uncertainties for Deep Learning Using Cal.pdf;/Users/pmg/Zotero/storage/AS52XLU7/1807.html}
}

@article{kulikova_learning_2021,
  title = {{Learning the Local Landscape of Protein Structures with Convolutional Neural Networks}},
  author = {Kulikova, Anastasiya V. and Diaz, Daniel J. and Loy, James M. and Ellington, Andrew D. and Wilke, Claus O.},
  year = {2021},
  month = dec,
  journal = {Journal of Biological Physics},
  volume = {47},
  number = {4},
  pages = {435--454},
  issn = {1573-0689},
  doi = {10.1007/s10867-021-09593-6},
  urldate = {2022-05-02},
  abstract = {One fundamental problem of protein biochemistry is to predict protein structure from amino acid sequence. The inverse problem, predicting either entire sequences or individual mutations that are consistent with a given protein structure, has received much less attention even though it has important applications in both protein engineering and evolutionary biology. Here, we ask whether 3D convolutional neural networks (3D CNNs) can learn the local fitness landscape of protein structure to reliably predict either the wild-type amino acid or the consensus in a multiple sequence alignment from the local structural context surrounding site of interest. We find that the network can predict wild type with good accuracy, and that network confidence is a reliable measure of whether a given prediction is likely going to be correct or not. Predictions of consensus are less accurate and are primarily driven by whether or not the consensus matches the wild type. Our work suggests that high-confidence mis-predictions of the wild type may identify sites that are primed for mutation and likely targets for protein engineering.},
  langid = {english},
  keywords = {Convolutional neural network,Microenvironment,Mutation,Protein structure},
  file = {/Users/pmg/Zotero/storage/KEY2YGUZ/Kulikova et al. - 2021 - Learning the local landscape of protein structures.pdf}
}

@article{le_representation_nodate,
  title = {Representation {{Learning}} on {{Biomolecular Structures}} Using {{Equivariant Graph Attention}}},
  author = {Le, Tuan and No{\'e}, Frank},
  pages = {12},
  abstract = {Learning and reasoning about 3D molecular structures with varying size is an emerging and important challenge in machine learning and especially in the development of biotherapeutics. Equivariant Graph Neural Networks (GNNs) can simultaneously leverage the geometric and relational detail of the problem domain and are known to learn expressive representations through the propagation of information between nodes leveraging geometrical details, such as directionality in their intermediate layers. In this work, we propose an equivariant GNN that operates with Cartesian coordinates to incorporate directionality and implements a novel attention mechanism, acting as a content and spatial dependent filter. Our proposed message function processes vector features in a geometrically meaningful way by mixing existing vectors and creating new ones based on cross products.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/GKTWHRRT/Le and Noé - Representation Learning on Biomolecular Structures.pdf}
}

@techreport{lee_equifold_2022,
  type = {Preprint},
  title = {{{EquiFold}}: {{Protein Structure Prediction}} with a {{Novel Coarse-Grained Structure Representation}}},
  shorttitle = {{{EquiFold}}},
  author = {Lee, Jae Hyeon and Yadollahpour, Payman and Watkins, Andrew and Frey, Nathan C. and {Leaver-Fay}, Andrew and Ra, Stephen and Cho, Kyunghyun and Gligorijevic, Vladimir and Regev, Aviv and Bonneau, Richard},
  year = {2022},
  month = oct,
  institution = {{Bioinformatics}},
  doi = {10.1101/2022.10.07.511322},
  urldate = {2022-12-06},
  abstract = {Designing proteins to achieve specific functions often requires in silico modeling of their properties at high throughput scale and can significantly benefit from fast and accurate protein structure prediction. We introduce EquiFold, a new end-to-end differentiable, SE(3)-equivariant, all-atom protein structure prediction model. EquiFold uses a novel coarse-grained representation of protein structures that does not require multiple sequence alignments or protein language model embeddings, inputs that are commonly used in other state-of-the-art structure prediction models. Our method relies on geometrical structure representation and is substantially smaller than prior state-of-the-art models. In preliminary studies, EquiFold achieved comparable accuracy to AlphaFold but was orders of magnitude faster. The combination of high speed and accuracy make EquiFold suitable for a number of downstream tasks, including protein property prediction and design.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/4T7L2V99/Lee et al. - 2022 - EquiFold Protein Structure Prediction with a Nove.pdf}
}

@article{lee_protein_2022,
  title = {Protein {{Sequence Design}} in a {{Latent Space}} via {{Model-based Reinforcement Learning}}},
  author = {Lee, Minji and Vecchietti, Luiz Felipe and Jung, Hyunkyu and Ro, Hyunjoo and Cha, Meeyoung and Kim, Ho Min},
  year = {2022},
  month = sep,
  urldate = {2023-09-05},
  abstract = {Proteins are complex molecules responsible for different functions in the human body. Enhancing the functionality of a protein and/or cellular fitness can significantly impact various industries. However, their optimization remains challenging, and sequences generated by data-driven methods often fail in wet lab experiments. This study investigates the limitations of existing model-based sequence design methods and presents a novel optimization framework that can efficiently traverse the latent representation space instead of the protein sequence space. Our framework generates proteins with higher functionality and cellular fitness by modeling the sequence design task as a Markov decision process and applying model-based reinforcement learning. We discuss the results in a comprehensive evaluation of two distinct proteins, GPF and His3, along with the predicted structure of optimized sequences using deep learning-based structure prediction.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/8F43WJJ8/Lee et al. - 2022 - Protein Sequence Design in a Latent Space via Mode.pdf}
}

@misc{lee_surgical_2022,
  title = {Surgical {{Fine-Tuning Improves Adaptation}} to {{Distribution Shifts}}},
  author = {Lee, Yoonho and Chen, Annie S. and Tajwar, Fahim and Kumar, Ananya and Yao, Huaxiu and Liang, Percy and Finn, Chelsea},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11466},
  eprint = {2210.11466},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11466},
  urldate = {2022-10-28},
  abstract = {A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shift.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/VPIZ4FZQ/Lee et al. - 2022 - Surgical Fine-Tuning Improves Adaptation to Distri.pdf;/Users/pmg/Zotero/storage/N54EWI23/2210.html}
}

@article{leslie_mismatch_2004,
  title = {{Mismatch String Kernels for Discriminative Protein Classification}},
  author = {Leslie, Christina S. and Eskin, Eleazar and Cohen, Adiel and Weston, Jason and Noble, William Stafford},
  year = {2004},
  month = mar,
  journal = {Bioinformatics},
  volume = {20},
  number = {4},
  pages = {467--476},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btg431},
  urldate = {2024-01-31},
  abstract = {Motivation: Classification of proteins sequences into functional and structural families based on sequence homology is a central problem in computational biology. Discriminative supervised machine learning approaches provide good performance, but simplicity and computational efficiency of training and prediction are also important concerns.Results: We introduce a class of string kernels, called mismatch kernels, for use with support vector machines (SVMs) in a discriminative approach to the problem of protein classification and remote homology detection. These kernels measure sequence similarity based on shared occurrences of fixed-length patterns in the data, allowing for mutations between patterns. Thus, the kernels provide a biologically well-motivated way to compare protein sequences without relying on family-based generative models such as hidden Markov models. We compute the kernels efficiently using a mismatch tree data structure, allowing us to calculate the contributions of all patterns occurring in the data in one pass while traversing the tree. When used with an SVM, the kernels enable fast prediction on test sequences. We report experiments on two benchmark SCOP datasets, where we show that the mismatch kernel used with an SVM classifier performs competitively with state-of-the-art methods for homology detection, particularly when very few training examples are available. Examination of the highest-weighted patterns learned by the SVM classifier recovers biologically important motifs in protein families and superfamilies.Availability: SVM software is publicly available at http://microarray.cpmc.columbia.edu/gist. Mismatch kernel software is available upon request.},
  file = {/Users/pmg/Zotero/storage/UMNKY8JZ/Leslie et al. - 2004 - Mismatch string kernels for discriminative protein.pdf;/Users/pmg/Zotero/storage/GCEV2SM2/192308.html}
}

@incollection{leslie_spectrum_2001,
  title = {{The spectrum kernel: A string kernel for SVM protein classification}},
  shorttitle = {The Spectrum Kernel},
  booktitle = {Biocomputing 2002},
  author = {Leslie, Christina and Eskin, Eleazar and Noble, William Stafford},
  year = {2001},
  month = dec,
  pages = {564--575},
  publisher = {{WORLD SCIENTIFIC}},
  doi = {10.1142/9789812799623_0053},
  urldate = {2024-01-31},
  isbn = {978-981-02-4777-5}
}

@misc{letham_constrained_2018,
  title = {Constrained {{Bayesian Optimization}} with {{Noisy Experiments}}},
  author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
  year = {2018},
  month = jun,
  number = {arXiv:1706.07094},
  eprint = {1706.07094},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.07094},
  urldate = {2022-10-04},
  abstract = {Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasi-Monte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/4XR35GV4/Letham et al. - 2018 - Constrained Bayesian Optimization with Noisy Exper.pdf;/Users/pmg/Zotero/storage/NKTZ9RG9/1706.html}
}


@article{levi_evaluating_2020,
  title = {Evaluating and {{Calibrating Uncertainty Prediction}} in {{Regression Tasks}}},
  author={Levi, Dan and Gispan, Liran and Giladi, Niv and Fetaya, Ethan},
  journal={Sensors},
  volume={22},
  number={15},
  pages={5540},
  year={2022},
  publisher={MDPI}
}


@article{li_deep_2022,
  title = {Deep Learning-Based Kcat Prediction Enables Improved Enzyme-Constrained Model Reconstruction},
  author = {Li, Feiran and Yuan, Le and Lu, Hongzhong and Li, Gang and Chen, Yu and Engqvist, Martin K. M. and Kerkhoven, Eduard J. and Nielsen, Jens},
  year = {2022},
  month = aug,
  journal = {Nature Catalysis},
  volume = {5},
  number = {8},
  pages = {662--672},
  publisher = {{Nature Publishing Group}},
  issn = {2520-1158},
  doi = {10.1038/s41929-022-00798-z},
  urldate = {2022-08-29},
  abstract = {Enzyme turnover numbers (kcat) are key to understanding cellular metabolism, proteome allocation and physiological diversity, but experimentally measured kcat data are sparse and noisy. Here we provide a deep learning approach (DLKcat) for high-throughput kcat prediction for metabolic enzymes from any organism merely from substrate structures and protein sequences. DLKcat can capture kcat changes for mutated enzymes and identify amino acid residues with a strong impact on kcat values. We applied this approach to predict genome-scale kcat values for more than 300 yeast species. Additionally, we designed a Bayesian pipeline to parameterize enzyme-constrained genome-scale metabolic models from predicted kcat values. The resulting models outperformed the corresponding original enzyme-constrained genome-scale metabolic models from previous pipelines in predicting phenotypes and proteomes, and enabled us to explain phenotypic differences. DLKcat and the enzyme-constrained genome-scale metabolic model construction pipeline are valuable tools to uncover global trends of enzyme kinetics and physiological diversity, and to further elucidate cellular metabolism on a large scale.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Enzymes,Systems biology},
  file = {/Users/pmg/Zotero/storage/7YMFPMSM/Li et al. - 2022 - Deep learning-based kcat prediction enables improv.pdf;/Users/pmg/Zotero/storage/QRRMWP6W/s41929-022-00798-z.html}
}

@inproceedings{li_multi-fidelity_2020,
  title = {Multi-{{Fidelity Bayesian Optimization}} via {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Shibo and Xing, Wei and Kirby, Robert and Zhe, Shandian},
  year = {2020},
  volume = {33},
  pages = {8521--8531},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-06-07},
  abstract = {Bayesian optimization (BO) is a popular framework for optimizing black-box functions. In many applications,  the objective function can be evaluated at multiple fidelities to enable a trade-off between the cost and accuracy. To reduce the optimization cost, many multi-fidelity BO methods have been proposed. Despite their success, these methods either ignore or over-simplify the strong, complex correlations across the fidelities. While the acquisition function is therefore easy and convenient to calculate,  these methods can be inefficient in estimating the objective function. To address this issue, we propose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can flexibly capture all kinds of complicated relationships between the fidelities to improve the objective function estimation and hence the optimization performance. We use sequential,  fidelity-wise Gauss-Hermite quadrature and moment-matching to compute a mutual information-based acquisition function in a tractable and highly efficient way. We show the advantages of our method in both synthetic benchmark datasets and real-world applications in engineering design.},
  file = {/Users/pmg/Zotero/storage/KPH88SAN/Li et al. - 2020 - Multi-Fidelity Bayesian Optimization via Deep Neur.pdf}
}

@patent{li_polypeptides_2019,
  title = {Polypeptides},
  shorttitle = {Polypeptides},
  author = {Li, Ming and Salomon, Jesper and Segura, Dorotea Raventos and Stringer, Mary Ann and Vejborg, Rebecca Munk and Klitgaard, Dorte Marie Koefoed and Nissen, Dorota and Peng, Wei and Sun, Tianqi},
  year = {2019},
  month = dec,
  number = {WO/2019/228448},
  urldate = {2023-01-16},
  abstract = {The invention relates to polypeptides having hydrolytic activity and polynucleotides encoding the polypeptides. The invention also relates to nucleic acid constructs, vectors, and host cells comprising the polynucleotides as well as methods of producing and using the polypeptides.},
  assignee = {Novozymes A/S, LI, Ming},
  langid = {english},
  keywords = {Polypeptides},
  file = {/Users/pmg/Zotero/storage/ECS35A3J/detail.html}
}

@patent{li_polypeptides_2019-1,
  title = {Polypeptides},
  author = {Li, Ming and Salomon, Jesper and Segura, Dorotea Raventos and Stringer, Mary Ann and Vejborg, Rebecca Munk and Klitgaard, Dorte Marie Koefoed and Nissen, Dorota and Peng, Wei and Sun, Tianqi},
  year = {2019},
  month = dec,
  urldate = {2023-05-25},
  abstract = {This patent search tool allows you not only to search the PCT database of about 2 million International Applications but also the worldwide patent collections. This search facility features: flexible search syntax; automatic word stemming and relevance ranking; as well as graphical results.},
  assignee = {NOVOZYMES A/S, LI, Ming},
  langid = {english},
  keywords = {Polypeptides},
  file = {/Users/pmg/Zotero/storage/R29I44HA/detail.html}
}

@article{li_predicting_2020,
  title = {Predicting Changes in Protein Thermodynamic Stability upon Point Mutation with Deep {{3D}} Convolutional Neural Networks},
  author = {Li, Bian and Yang, Yucheng T. and Capra, John A. and Gerstein, Mark B.},
  year = {2020},
  month = nov,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {11},
  pages = {e1008291},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008291},
  urldate = {2022-11-10},
  abstract = {Predicting mutation-induced changes in protein thermodynamic stability ({$\Delta\Delta$}G) is of great interest in protein engineering, variant interpretation, and protein biophysics. We introduce ThermoNet, a deep, 3D-convolutional neural network (3D-CNN) designed for structure-based prediction of {$\Delta\Delta$}Gs upon point mutation. To leverage the image-processing power inherent in CNNs, we treat protein structures as if they were multi-channel 3D images. In particular, the inputs to ThermoNet are uniformly constructed as multi-channel voxel grids based on biophysical properties derived from raw atom coordinates. We train and evaluate ThermoNet with a curated data set that accounts for protein homology and is balanced with direct and reverse mutations; this provides a framework for addressing biases that have likely influenced many previous {$\Delta\Delta$}G prediction methods. ThermoNet demonstrates performance comparable to the best available methods on the widely used Ssym test set. In addition, ThermoNet accurately predicts the effects of both stabilizing and destabilizing mutations, while most other methods exhibit a strong bias towards predicting destabilization. We further show that homology between Ssym and widely used training sets like S2648 and VariBench has likely led to overestimated performance in previous studies. Finally, we demonstrate the practical utility of ThermoNet in predicting the {$\Delta\Delta$}Gs for two clinically relevant proteins, p53 and myoglobin, and for pathogenic and benign missense variants from ClinVar. Overall, our results suggest that 3D-CNNs can model the complex, non-linear interactions perturbed by mutations, directly from biophysical properties of atoms.},
  langid = {english},
  keywords = {Biophysics,Point mutation,Protein folding,Protein structure,Protein structure determination,Protein structure prediction,Reverse mutation,Thermodynamics},
  file = {/Users/pmg/Zotero/storage/53Q6RHDW/Li et al. - 2020 - Predicting changes in protein thermodynamic stabil.pdf;/Users/pmg/Zotero/storage/TBACU2GW/article.html}
}

@article{li_pretrained_nodate,
  title = {Pretrained Protein Language Model Transfer Learning: Is the Final Layer Representation What We Want?},
  author = {Li, Francesca-Zhoufan and Yang, Kevin K. and Amini, Ava P and Lu, Alex X},
  pages = {18},
  abstract = {Large pretrained protein language models have improved protein sequence-tofunction prediction. This often takes the form of transfer learning, where final-layer representations from large pretrained models are extracted for downstream tasks. Although pretrained models have been empirically successful, there is little current understanding of how the features learned by pretraining relate to and are useful for downstream tasks. In this work, we investigate whether transferring a partial model, by using the output from a middle layer, is as effective as full model transfer, and if so, whether successful transfer depends on the downstream task and model properties. Across datasets and tasks, we evaluate partial model transfer of pretrained transformer and convolutional neural networks of varying sizes. We observe that pretrained representations outperform the one-hot baseline for most tasks. More importantly, we find that representations from middle layers can be as effective as those from later layers. To our knowledge, our work is the first to report the effectiveness of partial model transfer for protein property prediction. Our results point to a mismatch between the pretraining and downstream tasks, indicating a need for more relevant pretraining tasks so that representations from later layers can be better utilized for downstream tasks.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/RXMYQUY6/Li et al. - Pretrained protein language model transfer learnin.pdf}
}

@inproceedings{li_protein_2017,
  title = {Protein {{Loop Modeling Using Deep Generative Adversarial Network}}},
  booktitle = {2017 {{IEEE}} 29th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Li, Zhaoyu and Nguyen, Son P. and Xu, Dong and Shang, Yi},
  year = {2017},
  month = nov,
  pages = {1085--1091},
  issn = {2375-0197},
  doi = {10.1109/ICTAI.2017.00166},
  abstract = {Biology and medicine have a long-standing interest in computational structure prediction and modeling of proteins. There are often missing regions or regions that need to be remodeled in protein structures. The process of predicting particular missing regions in a protein structure is called loop modeling. In this paper, we propose a generative adversarial network (GAN) in deep learning for loop modeling using the idea of image inpainting. The generative network is to capture the context of the loop region and predict the missing area. The adversarial network is to make the prediction look real and provide gradients to the generative network. The proposed network was evaluated on a common benchmark for loop modeling. Experiments show that our method can successfully predict the loop region and has achieved better performance than the state-of-the-art tools. To our knowledge, this work represents the first attempt of using GAN for any bioinformatics studies.},
  keywords = {Computational modeling,deep learning,Gallium nitride,generative adversarial network,loop modeling,Machine learning,Neural networks,Predictive models,protein structure prediction,Proteins,Tools},
  file = {/Users/pmg/Zotero/storage/L948FMAN/Li et al. - 2017 - Protein Loop Modeling Using Deep Generative Advers.pdf;/Users/pmg/Zotero/storage/EIWN58HZ/8372069.html}
}

@misc{liao_efficient_2020,
  title = {Efficient {{Graph Generation}} with {{Graph Recurrent Attention Networks}}},
  author = {Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L. and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.00760},
  eprint = {1910.00760},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.00760},
  urldate = {2022-08-17},
  abstract = {We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: https://github.com/lrjconan/GRAN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/3REUVQ8L/Liao et al. - 2020 - Efficient Graph Generation with Graph Recurrent At.pdf;/Users/pmg/Zotero/storage/YATJVEU2/1910.html}
}

@misc{lin_generating_2023,
  title = {Generating {{Novel}}, {{Designable}}, and {{Diverse Protein Structures}} by {{Equivariantly Diffusing Oriented Residue Clouds}}},
  author = {Lin, Yeqing and AlQuraishi, Mohammed},
  year = {2023},
  month = feb,
  number = {arXiv:2301.12485},
  eprint = {2301.12485},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.12485},
  urldate = {2023-02-06},
  abstract = {Proteins power a vast array of functional processes in living cells. The capability to create new proteins with designed structures and functions would thus enable the engineering of cellular behavior and development of protein-based therapeutics and materials. Structure-based protein design aims to find structures that are designable (can be realized by a protein sequence), novel (have dissimilar geometry from natural proteins), and diverse (span a wide range of geometries). While advances in protein structure prediction have made it possible to predict structures of novel protein sequences, the combinatorially large space of sequences and structures limits the practicality of search-based methods. Generative models provide a compelling alternative, by implicitly learning the low-dimensional structure of complex data distributions. Here, we leverage recent advances in denoising diffusion probabilistic models and equivariant neural networks to develop Genie, a generative model of protein structures that performs discrete-time diffusion using a cloud of oriented reference frames in 3D space. Through in silico evaluations, we demonstrate that Genie generates protein backbones that are more designable, novel, and diverse than existing models. This indicates that Genie is capturing key aspects of the distribution of protein structure space and facilitates protein design with high success rates. Code for generating new proteins and training new versions of Genie is available at https://github.com/aqlaboratory/genie.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/UB9Y84VY/Lin and AlQuraishi - 2023 - Generating Novel, Designable, and Diverse Protein .pdf;/Users/pmg/Zotero/storage/FPXKSJRH/2301.html}
}

@article{lin_language_2022,
  title={Evolutionary-scale prediction of atomic-level protein structure with a language model},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and others},
  journal={Science},
  volume={379},
  number={6637},
  pages={1123--1130},
  year={2023},
  publisher={American Association for the Advancement of Science}
}


@article{lin_preference_2022,
  title = {Preference {{Exploration}} for {{Efficient Bayesian Optimization}} with {{Multiple Outcomes}}},
  author = {Lin, Zhiyuan Jerry and Astudillo, Raul and Frazier, Peter I. and Bakshy, Eytan},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.11382 [cs, math, stat]},
  eprint = {2203.11382},
  primaryclass = {cs, math, stat},
  urldate = {2022-04-12},
  abstract = {We consider Bayesian optimization of expensive-to-evaluate experiments that generate vector-valued outcomes over which a decision-maker (DM) has preferences. These preferences are encoded by a utility function that is not known in closed form but can be estimated by asking the DM to express preferences over pairs of outcome vectors. To address this problem, we develop Bayesian optimization with preference exploration, a novel framework that alternates between interactive real-time preference learning with the DM via pairwise comparisons between outcomes, and Bayesian optimization with a learned compositional model of DM utility and outcomes. Within this framework, we propose preference exploration strategies specifically designed for this task, and demonstrate their performance via extensive simulation studies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/IUASUJ4D/Lin et al. - 2022 - Preference Exploration for Efficient Bayesian Opti.pdf;/Users/pmg/Zotero/storage/7CBYAT3U/2203.html}
}

@misc{liu_generative_2023,
  title = {Generative {{Diffusion Models}} on {{Graphs}}: {{Methods}} and {{Applications}}},
  shorttitle = {Generative {{Diffusion Models}} on {{Graphs}}},
  author = {Liu, Chengyi and Fan, Wenqi and Liu, Yunqing and Li, Jiatong and Li, Hang and Liu, Hui and Tang, Jiliang and Li, Qing},
  year = {2023},
  month = aug,
  number = {arXiv:2302.02591},
  eprint = {2302.02591},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.02591},
  urldate = {2023-09-04},
  abstract = {Diffusion models, as a novel generative paradigm, have achieved remarkable success in various image generation tasks such as image inpainting, image-to-text translation, and video generation. Graph generation is a crucial computational task on graphs with numerous real-world applications. It aims to learn the distribution of given graphs and then generate new graphs. Given the great success of diffusion models in image generation, increasing efforts have been made to leverage these techniques to advance graph generation in recent years. In this paper, we first provide a comprehensive overview of generative diffusion models on graphs, In particular, we review representative algorithms for three variants of graph diffusion models, i.e., Score Matching with Langevin Dynamics (SMLD), Denoising Diffusion Probabilistic Model (DDPM), and Score-based Generative Model (SGM). Then, we summarize the major applications of generative diffusion models on graphs with a specific focus on molecule and protein modeling. Finally, we discuss promising directions in generative diffusion models on graph-structured data. For this survey, we also created a GitHub project website by collecting the supporting resources for generative diffusion models on graphs, at the link: https://github.com/ChengyiLIU-cs/Generative-Diffusion-Models-on-Graphs},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/pmg/Zotero/storage/9Z3ZZ44F/Liu et al. - 2023 - Generative Diffusion Models on Graphs Methods and.pdf;/Users/pmg/Zotero/storage/6PIKDUS4/2302.html}
}

@misc{liu_gflowout_2022,
  title = {{{GFlowOut}}: {{Dropout}} with {{Generative Flow Networks}}},
  shorttitle = {{{GFlowOut}}},
  author = {Liu, Dianbo and Jain, Moksh and Dossou, Bonaventure and Shen, Qianli and Lahlou, Salem and Goyal, Anirudh and Malkin, Nikolay and Emezue, Chris and Zhang, Dinghuai and Hassen, Nadhir and Ji, Xu and Kawaguchi, Kenji and Bengio, Yoshua},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12928},
  eprint = {2210.12928},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12928},
  urldate = {2022-10-25},
  abstract = {Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GFlowOut to address these issues. GFlowOut leverages the recently proposed probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks. We empirically demonstrate that GFlowOut results in predictive distributions that generalize better to out-of-distribution data, and provide uncertainty estimates which lead to better performance in downstream tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/HHA25MMV/Liu et al. - 2022 - GFlowOut Dropout with Generative Flow Networks.pdf;/Users/pmg/Zotero/storage/68954DZX/2210.html}
}

@misc{liu_protein_2022,
  title = {Protein {{Language Model Predicts Mutation Pathogenicity}} and {{Clinical Prognosis}}},
  author = {Liu, Xiangling and Yang, Xinyu and Ouyang, Linkun and Guo, Guibing and Su, Jin and Xi, Ruibin and Yuan, Ke and Yuan, Fajie},
  year = {2022},
  month = nov,
  primaryclass = {New Results},
  pages = {2022.09.30.510294},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.09.30.510294},
  urldate = {2022-11-21},
  abstract = {Accurately predicting the effects of mutations in cancer has the potential to improve existing treatments and identify novel therapeutic targets. In this paper, we evidence for the first time that the large-scale pre-trained protein language models (PPLMs) are zero-shot predictors for two clinically relevant tasks: identifying disease-causing mutations and predicting patient survival rate. Then we benchmark a series of state-of-the-art (SOTA) PPLMs on 2279 protein variants across 20 cancer-related genes. Our empirical results show that the PPLMs outperform the SOTA baseline, EVE , trained on multiple sequence alignment (MSA) data. We also demonstrate that the evolutionary index score, generated from the PPLMs softmax layer, is good indicator for both mutation pathogenicity and patient survival rate. Our paper has taken a key step toward the clinical utility of large-scale PPLMs.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/UJMI8AGP/Liu et al. - 2022 - Protein Language Model Predicts Mutation Pathogeni.pdf;/Users/pmg/Zotero/storage/D9Z47GP7/2022.09.30.html}
}

@misc{liu_roberta_2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.11692},
  urldate = {2023-04-27},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/pmg/Zotero/storage/BG3WCREQ/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/Users/pmg/Zotero/storage/62RRGQC8/1907.html}
}

@misc{liu_text-guided_2023,
  title = {A {{Text-guided Protein Design Framework}}},
  author = {Liu, Shengchao and Zhu, Yutao and Lu, Jiarui and Xu, Zhao and Nie, Weili and Gitter, Anthony and Xiao, Chaowei and Tang, Jian and Guo, Hongyu and Anandkumar, Anima},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04611},
  eprint = {2302.04611},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04611},
  urldate = {2023-02-14},
  abstract = {Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins' high-level properties. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP that aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that generates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We empirically verify the effectiveness of ProteinDT from three aspects: (1) consistently superior performance on four out of six protein property prediction benchmarks; (2) over 90\% accuracy for text-guided protein generation; and (3) promising results for zero-shot text-guided protein editing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/95G3UKC8/Liu et al. - 2023 - A Text-guided Protein Design Framework.pdf;/Users/pmg/Zotero/storage/C5AMXWYW/2302.html}
}

@misc{lones_how_2023,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  year = {2023},
  month = feb,
  number = {arXiv:2108.02497},
  eprint = {2108.02497},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.02497},
  urldate = {2023-03-06},
  abstract = {This document is a concise outline of some of the common mistakes that occur when using machine learning, and what can be done to avoid them. Whilst it should be accessible to anyone with a basic understanding of machine learning techniques, it was originally written for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/JJZJ5AWV/Lones - 2023 - How to avoid machine learning pitfalls a guide fo.pdf;/Users/pmg/Zotero/storage/A2JVR9CG/2108.html}
}

@article{lonsdale_practical_2012,
  title = {A Practical Guide to Modelling Enzyme-Catalysed Reactions},
  author = {Lonsdale, Richard and Harvey, Jeremy N. and Mulholland, Adrian J.},
  year = {2012},
  month = apr,
  journal = {Chemical Society Reviews},
  volume = {41},
  number = {8},
  pages = {3025--3038},
  issn = {0306-0012},
  doi = {10.1039/c2cs15297e},
  urldate = {2023-01-15},
  abstract = {Molecular modelling and simulation methods are increasingly at the forefront of elucidating mechanisms of enzyme-catalysed reactions, and shedding light on the determinants of specificity and efficiency of catalysis. These methods have the potential to assist in drug discovery and the design of novel protein catalysts. This Tutorial Review highlights some of the most widely used modelling methods and some successful applications. Modelling protocols commonly applied in studying enzyme-catalysed reactions are outlined here, and some practical implications are considered, with cytochrome P450 enzymes used as a specific example.},
  pmcid = {PMC3371381},
  pmid = {22278388},
  file = {/Users/pmg/Zotero/storage/GDLXKLMY/Lonsdale et al. - 2012 - A practical guide to modelling enzyme-catalysed re.pdf}
}

@article{lopez-del_rio_effect_2020,
  title = {Effect of Sequence Padding on the Performance of Deep Learning Models in Archaeal Protein Functional Prediction},
  author = {{Lopez-del Rio}, Angela and Martin, Maria and {Perera-Lluna}, Alexandre and Saidi, Rabie},
  year = {2020},
  month = sep,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {14634},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-71450-8},
  urldate = {2023-03-15},
  abstract = {The use of raw amino acid sequences as input for deep learning models for protein functional prediction has gained popularity in recent years. This scheme obliges to manage proteins with different lengths, while deep learning models require same-shape input. To accomplish this, zeros are usually added to each sequence up to a established common length in a process called zero-padding. However, the effect of different padding strategies on model performance and data structure is yet unknown. We propose and implement four novel types of padding the amino acid sequences. Then, we analysed the impact of different ways of padding the amino acid sequences in a hierarchical Enzyme Commission number prediction problem. Results show that padding has an effect on model performance even when there are convolutional layers implied. Contrastingly to most of deep learning works which focus mainly on architectures, this study highlights the relevance of the deemed-of-low-importance process of padding and raises awareness of the need to refine it for better performance. The code of this analysis is publicly available at https://github.com/b2slab/padding\_benchmark.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Data processing,Machine learning,Protein analysis,Protein function predictions,Sequence annotation},
  file = {/Users/pmg/Zotero/storage/Z5SZBTYH/Lopez-del Rio et al. - 2020 - Effect of sequence padding on the performance of d.pdf}
}


@misc{loshchilov_decoupled_2019,
  title = {Decoupled {{Weight Decay Regularization}}},
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@misc{lotfi_bayesian_2023,
  title = {Bayesian {{Model Selection}}, the {{Marginal Likelihood}}, and {{Generalization}}},
  author = {Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  year = {2023},
  month = may,
  number = {arXiv:2202.11678},
  eprint = {2202.11678},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.11678},
  urldate = {2023-10-03},
  abstract = {How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re-examine the connection between the marginal likelihood and PAC-Bayes bounds and use this connection to further elucidate the shortcomings of the marginal likelihood for model selection. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/VI8YN89H/Lotfi et al. - 2023 - Bayesian Model Selection, the Marginal Likelihood,.pdf;/Users/pmg/Zotero/storage/IAM8IQB8/2202.html}
}

@misc{lu_prediction_2023,
  title = {Prediction and {{Design}} of {{Protease Enzyme Specificity Using}} a {{Structure-Aware Graph Convolutional Network}}},
  author = {Lu, Changpeng and Lubin, Joseph H. and Sarma, Vidur V. and Stentz, Samuel Z. and Wang, Guanyang and Wang, Sijian and Khare, Sagar D.},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.16.528728},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.16.528728},
  urldate = {2023-02-20},
  abstract = {Site-specific proteolysis by the enzymatic cleavage of small linear sequence motifs is a key post-translational modification involved in physiology and disease. The ability to robustly and rapidly predict protease substrate specificity would also enable targeted proteolytic cleavage {\textendash} editing {\textendash} of a target protein by designed proteases. Current methods for predicting protease specificity are limited to sequence pattern recognition in experimentally-derived cleavage data obtained for libraries of potential substrates and generated separately for each protease variant. We reasoned that a more semantically rich and robust model of protease specificity could be developed by incorporating the three-dimensional structure and energetics of molecular interactions between protease and substrates into machine learning workflows. We present Protein Graph Convolutional Network (PGCN), which develops a physically-grounded, structure-based molecular interaction graph representation that describes molecular topology and interaction energetics to predict enzyme specificity. We show that PGCN accurately predicts the specificity landscapes of several variants of two model proteases: the NS3/4 protease from the Hepatitis C virus (HCV) and the Tobacco Etch Virus (TEV) proteases. Node and edge ablation tests identified key graph elements for specificity prediction, some of which are consistent with known biochemical constraints for protease:substrate recognition. We used a pre-trained PGCN model to guide the design of TEV protease libraries for cleaving two non-canonical substrates, and found good agreement with experimental cleavage results. Importantly, the model can accurately assess designs featuring diversity at positions not present in the training data. The described methodology should enable the structure-based prediction of specificity landscapes of a wide variety of proteases and the construction of tailor-made protease editors for site-selectively and irreversibly modifying chosen target proteins.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/7RTS39VD/Lu et al. - 2023 - Prediction and Design of Protease Enzyme Specifici.pdf}
}

@misc{lu_prediction_2023-1,
  title = {Prediction and {{Design}} of {{Protease Enzyme Specificity Using}} a {{Structure-Aware Graph Convolutional Network}}},
  author = {Lu, Changpeng and Lubin, Joseph H. and Sarma, Vidur V. and Stentz, Samuel Z. and Wang, Guanyang and Wang, Sijian and Khare, Sagar D.},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.16.528728},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.16.528728},
  urldate = {2023-03-09},
  abstract = {Site-specific proteolysis by the enzymatic cleavage of small linear sequence motifs is a key post-translational modification involved in physiology and disease. The ability to robustly and rapidly predict protease substrate specificity would also enable targeted proteolytic cleavage {\textendash} editing {\textendash} of a target protein by designed proteases. Current methods for predicting protease specificity are limited to sequence pattern recognition in experimentally-derived cleavage data obtained for libraries of potential substrates and generated separately for each protease variant. We reasoned that a more semantically rich and robust model of protease specificity could be developed by incorporating the three-dimensional structure and energetics of molecular interactions between protease and substrates into machine learning workflows. We present Protein Graph Convolutional Network (PGCN), which develops a physically-grounded, structure-based molecular interaction graph representation that describes molecular topology and interaction energetics to predict enzyme specificity. We show that PGCN accurately predicts the specificity landscapes of several variants of two model proteases: the NS3/4 protease from the Hepatitis C virus (HCV) and the Tobacco Etch Virus (TEV) proteases. Node and edge ablation tests identified key graph elements for specificity prediction, some of which are consistent with known biochemical constraints for protease:substrate recognition. We used a pre-trained PGCN model to guide the design of TEV protease libraries for cleaving two non-canonical substrates, and found good agreement with experimental cleavage results. Importantly, the model can accurately assess designs featuring diversity at positions not present in the training data. The described methodology should enable the structure-based prediction of specificity landscapes of a wide variety of proteases and the construction of tailor-made protease editors for site-selectively and irreversibly modifying chosen target proteins.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/J5TZKPWR/Lu et al. - 2023 - Prediction and Design of Protease Enzyme Specifici.pdf}
}

@article{lui_network_2013,
  title = {The Network of Stabilizing Contacts in Proteins Studied by Coevolutionary Data},
  author = {Lui, Sara and Tiana, Guido},
  year = {2013},
  month = oct,
  journal = {The Journal of Chemical Physics},
  volume = {139},
  number = {15},
  pages = {155103},
  issn = {1089-7690},
  doi = {10.1063/1.4826096},
  abstract = {The primary structure of proteins, that is their sequence, represents one of the most abundant sets of experimental data concerning biomolecules. The study of correlations in families of co-evolving proteins by means of an inverse Ising-model approach allows to obtain information on their native conformation. Following up on a recent development along this line, we optimize the algorithm to calculate effective energies between the residues, validating the approach both back-calculating interaction energies in a model system, and predicting the free energies associated to mutations in real systems. Making use of these effective energies, we study the network of interactions which stabilizes the native conformation of some well-studied proteins, showing that it displays different properties than the associated contact network.},
  langid = {english},
  pmid = {24160546},
  keywords = {Algorithms,{Evolution, Molecular},Protein Conformation,Protein Interaction Maps,Protein Stability,Proteins,Thermodynamics},
  file = {/Users/pmg/Zotero/storage/SIFZSAI8/Lui and Tiana - 2013 - The network of stabilizing contacts in proteins st.pdf}
}

@misc{luo_contrastive_2022,
  title = {Contrastive Learning of Protein Representations with Graph Neural Networks for Structural and Functional Annotations},
  author = {Luo, Jiaqi and Luo, Yunan},
  year = {2022},
  month = dec,
  primaryclass = {New Results},
  pages = {2022.11.29.518451},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.11.29.518451},
  urldate = {2022-12-06},
  abstract = {Although protein sequence data is growing at an ever-increasing rate, the protein universe is still sparsely annotated with functional and structural annotations. Computational approaches have become efficient solutions to infer annotations for unlabeled proteins by transferring knowledge from proteins with experimental annotations. Despite the increasing availability of protein structure data and the high coverage of high-quality predicted structures, e.g., by AlphaFold, many existing computational tools still only rely on sequence data to predict structural or functional annotations, including alignment algorithms such as BLAST and several sequence-based deep learning models. Here, we develop PenLight, a general deep learning framework for protein structural and functional annotations. PenLight uses a graph neural network (GNN) to integrate 3D protein structure data and protein language model representations. In addition, PenLight applies a contrastive learning strategy to train the GNN for learning protein representations that reflect similarities beyond sequence identity, such as semantic similarities in the function or structure space. We benchmarked PenLight on a structural classification task and a functional annotation task, where PenLight achieved higher prediction accuracy and coverage than state-of-the-art methods.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/L2A43BIB/Luo and Luo - 2022 - Contrastive learning of protein representations wi.pdf;/Users/pmg/Zotero/storage/RDILUGNM/2022.11.29.html}
}

@misc{luo_understanding_2022,
  title = {Understanding {{Diffusion Models}}: {{A Unified Perspective}}},
  shorttitle = {Understanding {{Diffusion Models}}},
  author = {Luo, Calvin},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11970},
  eprint = {2208.11970},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.11970},
  urldate = {2022-12-19},
  abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/QCWN9UVS/Luo - 2022 - Understanding Diffusion Models A Unified Perspect.pdf;/Users/pmg/Zotero/storage/Z3NV9HVI/2208.html}
}

@article{mackenzie_tertiary_2016,
  title = {Tertiary Alphabet for the Observable Protein Structural Universe},
  author = {Mackenzie, Craig O. and Zhou, Jianfu and Grigoryan, Gevorg},
  year = {2016},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {47},
  pages = {E7438-E7447},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1607178113},
  urldate = {2023-08-28},
  abstract = {Here, we systematically decompose the known protein structural universe into its basic elements, which we dub tertiary structural motifs (TERMs). A TERM is a compact backbone fragment that captures the secondary, tertiary, and quaternary environments around a given residue, comprising one or more disjoint segments (three on average). We seek the set of universal TERMs that capture all structure in the Protein Data Bank (PDB), finding remarkable degeneracy. Only {$\sim$}600 TERMs are sufficient to describe 50\% of the PDB at sub-Angstrom resolution. However, more rare geometries also exist, and the overall structural coverage grows logarithmically with the number of TERMs. We go on to show that universal TERMs provide an effective mapping between sequence and structure. We demonstrate that TERM-based statistics alone are sufficient to recapitulate close-to-native sequences given either NMR or X-ray backbones. Furthermore, sequence variability predicted from TERM data agrees closely with evolutionary variation. Finally, locations of TERMs in protein chains can be predicted from sequence alone based on sequence signatures emergent from TERM instances in the PDB. For multisegment motifs, this method identifies spatially adjacent fragments that are not contiguous in sequence{\textemdash}a major bottleneck in structure prediction. Although all TERMs recur in diverse proteins, some appear specialized for certain functions, such as interface formation, metal coordination, or even water binding. Structural biology has benefited greatly from previously observed degeneracies in structure. The decomposition of the known structural universe into a finite set of compact TERMs offers exciting opportunities toward better understanding, design, and prediction of protein structure.},
  file = {/Users/pmg/Zotero/storage/FZG2SVEF/Mackenzie et al. - 2016 - Tertiary alphabet for the observable protein struc.pdf}
}

@misc{madani_deep_2021,
  title = {Deep Neural Language Modeling Enables Functional Protein Generation across Families},
  author = {Madani, Ali and Krause, Ben and Greene, Eric R. and Subramanian, Subu and Mohr, Benjamin P. and Holton, James M. and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z. and Socher, Richard and Fraser, James S. and Naik, Nikhil},
  year = {2021},
  month = jul,
  primaryclass = {New Results},
  pages = {2021.07.18.452833},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.07.18.452833},
  urldate = {2022-10-04},
  abstract = {Bypassing nature's evolutionary trajectory, de novo protein generation{\textemdash}defined as creating artificial protein sequences from scratch{\textemdash}could enable breakthrough solutions for biomedical and environmental challenges. Viewing amino acid sequences as a language, we demonstrate that a deep learning-based language model can generate functional artificial protein sequences across families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. Our protein language model is trained by simply learning to predict the next amino acid for over 280 million protein sequences from thousands of protein families, without biophysical or coevolutionary modeling. We experimentally evaluate model-generated artificial proteins on five distinct antibacterial lysozyme families. Artificial proteins show similar activities and catalytic efficiencies as representative natural lysozymes, including hen egg white lysozyme, while reaching as low as 44\% identity to any known naturally-evolved protein. The X-ray crystal structure of an enzymatically active artificial protein recapitulates the conserved fold and positioning of active site residues found in natural proteins. We demonstrate our language model's ability to be adapted to different protein families by accurately predicting the functionality of artificial chorismate mutase and malate dehydrogenase proteins. These results indicate that neural language models successfully perform de novo protein generation across protein families and may prove to be a tool to shortcut evolution.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/PSRJEJBC/Madani et al. - 2021 - Deep neural language modeling enables functional p.pdf;/Users/pmg/Zotero/storage/J3GCQDDC/2021.07.18.html}
}

@misc{madani_progen_2020,
  title = {{{ProGen}}: {{Language Modeling}} for {{Protein Generation}}},
  shorttitle = {{{ProGen}}},
  author = {Madani, Ali and McCann, Bryan and Naik, Nikhil and Keskar, Nitish Shirish and Anand, Namrata and Eguchi, Raphael R. and Huang, Po-Ssu and Socher, Richard},
  year = {2020},
  month = mar,
  number = {arXiv:2004.03497},
  eprint = {2004.03497},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.03497},
  urldate = {2022-10-04},
  abstract = {Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on {\textasciitilde}280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/IC3D4Q9U/Madani et al. - 2020 - ProGen Language Modeling for Protein Generation.pdf;/Users/pmg/Zotero/storage/DBDPGQDF/2004.html}
}

@misc{madani_progen_2020-1,
  title = {{{ProGen}}: {{Language Modeling}} for {{Protein Generation}}},
  shorttitle = {{{ProGen}}},
  author = {Madani, Ali and McCann, Bryan and Naik, Nikhil and Keskar, Nitish Shirish and Anand, Namrata and Eguchi, Raphael R. and Huang, Po-Ssu and Socher, Richard},
  year = {2020},
  month = mar,
  number = {arXiv:2004.03497},
  eprint = {2004.03497},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.03497},
  urldate = {2022-10-06},
  abstract = {Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on {\textasciitilde}280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/VIXWGZWX/Madani et al. - 2020 - ProGen Language Modeling for Protein Generation.pdf;/Users/pmg/Zotero/storage/QRQGN22N/2004.html}
}

@misc{mahajan_contextual_2023,
  title = {Contextual Protein Encodings from Equivariant Graph Transformers},
  author = {Mahajan, Sai Pooja and Ruffolo, Jeffrey A. and Gray, Jeffrey J.},
  year = {2023},
  month = jul,
  primaryclass = {New Results},
  pages = {2023.07.15.549154},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.07.15.549154},
  urldate = {2023-07-26},
  abstract = {The optimal residue identity at each position in a protein is determined by its structural, evolutionary, and functional context. We seek to learn the representation space of the optimal amino-acid residue in different structural contexts in proteins. Inspired by masked language modeling (MLM), our training aims to transduce learning of amino-acid labels from non-masked residues to masked residues in their structural environments and from general (e.g., a residue in a protein) to specific contexts (e.g., a residue at the interface of a protein or antibody complex). Our results on native sequence recovery and forward folding with AlphaFold2 suggest that the amino acid label for a protein residue may be determined from its structural context alone (i.e., without knowledge of the sequence labels of surrounding residues). We further find that the sequence space sampled from our masked models recapitulate the evolutionary sequence neighborhood of the wildtype sequence. Remarkably, the sequences conditioned on highly plastic structures recapitulate the conformational flexibility encoded in the structures. Furthermore, maximum-likelihood interfaces designed with masked models recapitulate wildtype binding energies for a wide range of protein interfaces and binding strengths. We also propose and compare fine-tuning strategies to train models for designing CDR loops of antibodies in the structural context of the antibody-antigen interface by leveraging structural databases for proteins, antibodies (synthetic and experimental) and protein-protein complexes. We show that pretraining on more general contexts improves native sequence recovery for antibody CDR loops, especially for the hypervariable CDR H3, while fine-tuning helps to preserve patterns observed in special contexts.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/T93FTL9E/Mahajan et al. - 2023 - Contextual protein encodings from equivariant grap.pdf}
}

@misc{mahmud_accurate_2023,
  title = {Accurate Prediction of Protein Tertiary Structural Changes Induced by Single-Site Mutations with Equivariant Graph Neural Networks},
  author = {Mahmud, Sajid and Morehead, Alex and Cheng, Jianlin},
  year = {2023},
  month = oct,
  primaryclass = {New Results},
  pages = {2023.10.03.560758},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.10.03.560758},
  urldate = {2023-10-06},
  abstract = {Predicting the change of protein tertiary structure caused by single-site mutations is important for studying protein structure, function, and interaction. Even though computational protein structure prediction methods such as AlphaFold can predict the overall tertiary structures of most proteins rather accurately, they are not sensitive enough to accurately predict the structural changes induced by single-site amino mutations on proteins. Specialized mutation prediction methods mostly focus on predicting the overall stability or function changes caused by mutations without attempting to predict the exact mutation-induced structural changes, limiting their use in protein mutation study. In this work, we develop the first deep learning method based on equivariant graph neural networks (EGNN) to directly predict the tertiary structural changes caused by single-site mutations and the tertiary structure of any protein mutant from the structure of its wild-type counterpart. The results show that it performs substantially better in predicting the tertiary structures of protein mutants than the widely used protein structure prediction method AlphaFold.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/TUZQMZX5/Mahmud et al. - 2023 - Accurate prediction of protein tertiary structural.pdf}
}

@misc{malik_structome_2023,
  title = {Structome: {{Exploring}} the Structural Neighbourhood of Proteins},
  shorttitle = {Structome},
  author = {Malik, Ashar J. and Verma, Chandra S. and Poole, Anthony M. and Allison, Jane R.},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.18.529083},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.18.529083},
  urldate = {2023-03-09},
  abstract = {Protein structures carry signal of common ancestry and can therefore aid in reconstructing their evolutionary histories. To expedite the structure-informed inference process, a web server, Structome, has been developed, that allows users to rapidly identify protein structures similar to a query protein and to assemble datasets useful for structure-based phylogenetics. Structome was created by clustering {$\sim$} 94\% of the structures in RCSB PDB using 90\% sequence identity and representing each cluster by a centroid structure. Structure similarity between centroid proteins was calculated, and annotations from PDB, SCOP and CATH were integrated. To illustrate utility, an H3 histone was used as a query, and results show that the protein structures returned by Structome span both sequence and structural diversity of the histone fold. Additionally, the pre-computed nexus-formated distance matrix, provided by Structome, enables analysis of evolutionary relationships between proteins not identifiable using searches based on sequence similarity alone. Our results demonstrate that, beginning with a single structure, Structome can be used to rapidly generate a dataset of structural neighbours and allows deep evolutionary history of proteins to be studied. Structome is available at: https://structome.bii.a-star.edu.sg},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/YV8XQW7H/Malik et al. - 2023 - Structome Exploring the structural neighbourhood .pdf}
}

@misc{mallet_atomsurf_2023,
  title = {{{AtomSurf}} : {{Surface Representation}} for {{Learning}} on {{Protein Structures}}},
  shorttitle = {{{AtomSurf}}},
  author = {Mallet, Vincent and Attaiki, Souhaib and Ovsjanikov, Maks},
  year = {2023},
  month = sep,
  number = {arXiv:2309.16519},
  eprint = {2309.16519},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.16519},
  urldate = {2023-09-29},
  abstract = {Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method. In this paper, we investigate representing proteins as \${\textbackslash}textit\{3D mesh surfaces\}\$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with graph-based methods, resulting in a general framework that incorporates both representations in learning. We show that using this combination, we are able to obtain state-of-the-art results across \${\textbackslash}textit\{all tested tasks\}\$. Our code and data can be found online: https://github.com/Vincentx15/atom2D .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/D3SCVEZ9/Mallet et al. - 2023 - AtomSurf  Surface Representation for Learning on .pdf;/Users/pmg/Zotero/storage/N5QTDXPV/2309.html}
}

@misc{mardikoraem_protein_2023,
  title = {Protein {{Fitness Prediction}} Is {{Impacted}} by the {{Interplay}} of {{Language Models}}, {{Ensemble Learning}}, and {{Sampling Methods}}},
  author = {Mardikoraem, Mehrsa and Woldring, Daniel},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.09.527362},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.09.527362},
  urldate = {2023-03-09},
  abstract = {Advances in machine learning (ML) and the availability of protein sequences via high-throughput sequencing techniques have transformed our ability to design novel diagnostic and therapeutic proteins. ML allows protein engineers to capture complex trends hidden within protein sequences that would otherwise be difficult to identify in the context of the immense and rugged protein fitness landscape. Despite this potential, there persists a need for guidance during the training and evaluation of ML methods over sequencing data. Two key challenges for training discriminative models and evaluating their performance include handling severely imbalanced datasets (e.g., few high-fitness proteins among an abundance of non-functional proteins) and selecting appropriate protein sequence representations. Here, we present a framework for applying ML over assay-labeled datasets to elucidate the capacity of sampling methods and protein representations to improve model performance in two different datasets with binding affinity and thermal stability prediction tasks. For protein sequence representations, we incorporate two widely used methods (One-Hot encoding, physiochemical encoding) and two language-based methods (next-token prediction, UniRep; masked-token prediction, ESM). Elaboration on performance is provided over protein fitness, length, data size, and sampling methods. In addition, an ensemble of representation methods is generated to discover the contribution of distinct representations to the final prediction score. Within the context of these datasets, the synthetic minority oversampling technique (SMOTE) outperformed undersampling while encoding sequences with One-Hot, UniRep, and ESM representations. In addition, ensemble learning increased the predictive performance of the affinity-based dataset by 4\% compared to the best single encoding candidate (F1-score = 97\%), while ESM alone was rigorous enough in stability prediction (F1-score = 92\%).},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/EGGZNWVF/Mardikoraem and Woldring - 2023 - Protein Fitness Prediction is Impacted by the Inte.pdf}
}

@misc{marin_bend_2023,
  title = {{{BEND}}: {{Benchmarking DNA Language Models}} on Biologically Meaningful Tasks},
  shorttitle = {{{BEND}}},
  author = {Marin, Frederikke Isa and Teufel, Felix and Horlacher, Marc and Madsen, Dennis and Pultz, Dennis and Winther, Ole and Boomsma, Wouter},
  year = {2023},
  month = nov,
  number = {arXiv:2311.12570},
  eprint = {2311.12570},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.12570},
  urldate = {2023-12-08},
  abstract = {The genome sequence contains the blueprint for governing cellular processes. While the availability of genomes has vastly increased over the last decades, experimental annotation of the various functional, non-coding and regulatory elements encoded in the DNA sequence remains both expensive and challenging. This has sparked interest in unsupervised language modeling of genomic DNA, a paradigm that has seen great success for protein sequence data. Although various DNA language models have been proposed, evaluation tasks often differ between individual works, and might not fully recapitulate the fundamental challenges of genome annotation, including the length, scale and sparsity of the data. In this study, we introduce BEND, a Benchmark for DNA language models, featuring a collection of realistic and biologically meaningful downstream tasks defined on the human genome. We find that embeddings from current DNA LMs can approach performance of expert methods on some tasks, but only capture limited information about long-range features. BEND is available at https://github.com/frederikkemarin/BEND.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Genomics},
  file = {/Users/pmg/Zotero/storage/SD9BQ7ZU/Marin et al. - 2023 - BEND Benchmarking DNA Language Models on biologic.pdf;/Users/pmg/Zotero/storage/5PLPCEEX/2311.html}
}

@article{marmont_pela_2017,
  title = {{{PelA}} and {{PelB}} Proteins Form a Modification and Secretion Complex Essential for {{Pel}} Polysaccharide-Dependent Biofilm Formation in {{Pseudomonas}} Aeruginosa},
  author = {Marmont, Lindsey S. and Whitfield, Gregory B. and Rich, Jacquelyn D. and Yip, Patrick and Giesbrecht, Laura B. and Stremick, Carol A. and Whitney, John C. and Parsek, Matthew R. and Harrison, Joe J. and Howell, P. Lynne},
  year = {2017},
  month = nov,
  journal = {The Journal of Biological Chemistry},
  volume = {292},
  number = {47},
  pages = {19411--19422},
  issn = {1083-351X},
  doi = {10.1074/jbc.M117.812842},
  abstract = {The pellicle (PEL) polysaccharide is synthesized by the opportunistic pathogen Pseudomonas aeruginosa and is an important biofilm constituent critical for bacterial virulence and persistence. PEL is a cationic polymer that promotes cell-cell interactions within the biofilm matrix through electrostatic interactions with extracellular DNA. Translocation of PEL across the outer membrane is proposed to occur via PelB, a membrane-embedded porin with a large periplasmic domain predicted to contain 19 tetratricopeptide repeats (TPRs). TPR-containing domains are typically involved in protein-protein interactions, and we therefore sought to determine whether PelB serves as a periplasmic scaffold that recruits other components of the PEL secretion apparatus. In this study, we show that the TPR domain of PelB interacts with PelA, an enzyme with PEL deacetylase and hydrolase activities. Structure determination of PelB TPRs 8-11 enabled us to design systematic deletions of individual TPRs and revealed that repeats 9-14, which are required for the cellular localization of PelA with PelB are also essential for PEL-dependent biofilm formation. Copurification experiments indicated that the interaction between PelA and PelB is direct and that the deacetylase activity of PelA increases and its hydrolase activity decreases when these proteins interact. Combined, our results indicate that the TPR-containing domain of PelB localizes PelA to the PEL secretion apparatus within the periplasm and that this may allow for efficient deacetylation of PEL before its export from the cell.},
  langid = {english},
  pmcid = {PMC5702679},
  pmid = {28972168},
  keywords = {Bacterial Proteins,biofilm,Biofilms,Extracellular Matrix,{Gene Expression Regulation, Bacterial},Microbial Sensitivity Tests,Microbial Viability,Periplasm,polysaccharide,{Polysaccharides, Bacterial},Protein Conformation,protein-protein interaction,Pseudomonas aeruginosa,Pseudomonas aeruginosa (P. aeruginosa),X-ray crystallography},
  file = {/Users/pmg/Zotero/storage/H62HJZK9/Marmont et al. - 2017 - PelA and PelB proteins form a modification and sec.pdf}
}

@article{matsumoto_novel_2021,
  title = {Novel Metric for Hyperbolic Phylogenetic Tree Embeddings},
  author = {Matsumoto, Hirotaka and Mimori, Takahiro and Fukunaga, Tsukasa},
  year = {2021},
  month = jan,
  journal = {Biology Methods and Protocols},
  volume = {6},
  number = {1},
  pages = {bpab006},
  issn = {2396-8923},
  doi = {10.1093/biomethods/bpab006},
  urldate = {2022-08-26},
  abstract = {Advances in experimental technologies, such as DNA sequencing, have opened up new avenues for the applications of phylogenetic methods to various fields beyond their traditional application in evolutionary investigations, extending to the fields of development, differentiation, cancer genomics, and immunogenomics. Thus, the importance of phylogenetic methods is increasingly being recognized, and the development of a novel phylogenetic approach can contribute to several areas of research. Recently, the use of hyperbolic geometry has attracted attention in artificial intelligence research. Hyperbolic space can better represent a hierarchical structure compared to Euclidean space, and can therefore be useful for describing and analyzing a phylogenetic tree. In this study, we developed a novel metric that considers the characteristics of a phylogenetic tree for representation in hyperbolic space. We compared the performance of the proposed hyperbolic embeddings, general hyperbolic embeddings, and Euclidean embeddings, and confirmed that our method could be used to more precisely reconstruct evolutionary distance. We also demonstrate that our approach is useful for predicting the nearest-neighbor node in a partial phylogenetic tree with missing nodes. Furthermore, we proposed a novel approach based on our metric to integrate multiple trees for analyzing tree nodes or imputing missing distances. This study highlights the utility of adopting a geometric approach for further advancing the applications of phylogenetic methods.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/Z3D5QG26/Matsumoto et al. - 2021 - Novel metric for hyperbolic phylogenetic tree embe.pdf}
}

@misc{maus_inverse_2023,
  title = {Inverse {{Protein Folding Using Deep Bayesian Optimization}}},
  author = {Maus, Natalie and Zeng, Yimeng and Anderson, Daniel Allen and Maffettone, Phillip and Solomon, Aaron and Greenside, Peyton and Bastani, Osbert and Gardner, Jacob R.},
  year = {2023},
  month = may,
  number = {arXiv:2305.18089},
  eprint = {2305.18089},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18089},
  urldate = {2023-05-30},
  abstract = {Inverse protein folding -- the task of predicting a protein sequence from its backbone atom coordinates -- has surfaced as an important problem in the "top down", de novo design of proteins. Contemporary approaches have cast this problem as a conditional generative modelling problem, where a large generative model over protein sequences is conditioned on the backbone. While these generative models very rapidly produce promising sequences, independent draws from generative models may fail to produce sequences that reliably fold to the correct backbone. Furthermore, it is challenging to adapt pure generative approaches to other settings, e.g., when constraints exist. In this paper, we cast the problem of improving generated inverse folds as an optimization problem that we solve using recent advances in "deep" or "latent space" Bayesian optimization. Our approach consistently produces protein sequences with greatly reduced structural error to the target backbone structure as measured by TM score and RMSD while using fewer computational resources. Additionally, we demonstrate other advantages of an optimization-based approach to the problem, such as the ability to handle constraints.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/ZAH7QD8G/Maus et al. - 2023 - Inverse Protein Folding Using Deep Bayesian Optimi.pdf;/Users/pmg/Zotero/storage/HKU37ADH/2305.html}
}

@misc{maus_local_2022,
  title = {Local {{Latent Space Bayesian Optimization}} over {{Structured Inputs}}},
  author = {Maus, Natalie and Jones, Haydn T. and Moore, Juston S. and Kusner, Matt J. and Bradshaw, John and Gardner, Jacob R.},
  year = {2022},
  month = jan,
  number = {arXiv:2201.11872},
  eprint = {2201.11872},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.11872},
  urldate = {2023-06-08},
  abstract = {Bayesian optimization over the latent spaces of deep autoencoder models (DAEs) has recently emerged as a promising new approach for optimizing challenging black-box functions over structured, discrete, hard-to-enumerate search spaces (e.g., molecules). Here the DAE dramatically simplifies the search space by mapping inputs into a continuous latent space where familiar Bayesian optimization tools can be more readily applied. Despite this simplification, the latent space typically remains high-dimensional. Thus, even with a well-suited latent space, these approaches do not necessarily provide a complete solution, but may rather shift the structured optimization problem to a high-dimensional one. In this paper, we propose LOL-BO, which adapts the notion of trust regions explored in recent work on high-dimensional Bayesian optimization to the structured setting. By reformulating the encoder to function as both an encoder for the DAE globally and as a deep kernel for the surrogate model within a trust region, we better align the notion of local optimization in the latent space with local optimization in the input space. LOL-BO achieves as much as 20 times improvement over state-of-the-art latent space Bayesian optimization methods across six real-world benchmarks, demonstrating that improvement in optimization strategies is as important as developing better DAE models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/I8YXMZMG/Maus et al. - 2022 - Local Latent Space Bayesian Optimization over Stru.pdf;/Users/pmg/Zotero/storage/YMFVEVYK/2201.html}
}

@article{mavor_determination_2016,
  title = {{Determination of Ubiquitin Fitness Landscapes under Different Chemical Stresses in a Classroom Setting}},
  author = {Mavor, David and Barlow, Kyle and Thompson, Samuel and Barad, Benjamin A and Bonny, Alain R and Cario, Clinton L and Gaskins, Garrett and Liu, Zairan and Deming, Laura and Axen, Seth D and Caceres, Elena and Chen, Weilin and Cuesta, Adolfo and Gate, Rachel E and Green, Evan M and Hulce, Kaitlin R and Ji, Weiyue and Kenner, Lillian R and Mensa, Bruk and Morinishi, Leanna S and Moss, Steven M and Mravic, Marco and Muir, Ryan K and Niekamp, Stefan and Nnadi, Chimno I and Palovcak, Eugene and Poss, Erin M and Ross, Tyler D and Salcedo, Eugenia
      and See, Stephanie K and Subramaniam, Meena and Wong, Allison W and Li, Jennifer and Thorn, Kurt S and Conch{\'u}ir, Shane {\'O} and Roscoe, Benjamin P and Chow, Eric D and DeRisi, Joseph L and Kortemme, Tanja and Bolon, Daniel N and Fraser, James S},
  year = {2016},
  month = apr,
  journal = {eLife},
  volume = {5},
  pages = {e15802},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.15802},
  urldate = {2023-11-01},
  abstract = {Ubiquitin is essential for eukaryotic life and varies in only 3 amino acid positions between yeast and humans. However, recent deep sequencing studies indicate that ubiquitin is highly tolerant to single mutations. We hypothesized that this tolerance would be reduced by chemically induced physiologic perturbations. To test this hypothesis, a class of first year UCSF graduate students employed deep mutational scanning to determine the fitness landscape of all possible single residue mutations in the presence of five different small molecule perturbations. These perturbations uncover 'shared sensitized positions' localized to areas around the hydrophobic patch and the C-terminus. In addition, we identified perturbation specific effects such as a sensitization of His68 in HU and a tolerance to mutation at Lys63 in DTT. Our data show how chemical stresses can reduce buffering effects in the ubiquitin proteasome system. Finally, this study demonstrates the potential of lab-based interdisciplinary graduate curriculum.},
  keywords = {biophysics,chemical biology,deep mutational scanning,proteasome,proteostasis,ubiquitin},
  file = {/Users/pmg/Zotero/storage/59KTWCF5/Mavor et al. - 2016 - Determination of ubiquitin fitness landscapes unde.pdf}
}

@article{mazurenko_machine_2020,
  title = {Machine {{Learning}} in {{Enzyme Engineering}}},
  author = {Mazurenko, Stanislav and Prokop, Zbynek and Damborsky, Jiri},
  year = {2020},
  month = jan,
  journal = {ACS Catalysis},
  volume = {10},
  number = {2},
  pages = {1210--1223},
  publisher = {{American Chemical Society}},
  doi = {10.1021/acscatal.9b04321},
  urldate = {2022-10-17},
  abstract = {Enzyme engineering plays a central role in developing efficient biocatalysts for biotechnology, biomedicine, and life sciences. Apart from classical rational design and directed evolution approaches, machine learning methods have been increasingly applied to find patterns in data that help predict protein structures, improve enzyme stability, solubility, and function, predict substrate specificity, and guide rational protein design. In this Perspective, we analyze the state of the art in databases and methods used for training and validating predictors in enzyme engineering. We discuss current limitations and challenges which the community is facing and recent advancements in experimental and theoretical methods that have the potential to address those challenges. We also present our view on possible future directions for developing the applications to the design of efficient biocatalysts.},
  file = {/Users/pmg/Zotero/storage/FZBYVVS4/Mazurenko et al. - 2020 - Machine Learning in Enzyme Engineering.pdf;/Users/pmg/Zotero/storage/CFJ6EMMH/acscatal.html}
}

@misc{mcallester_mathematics_2023,
  title = {On the {{Mathematics}} of {{Diffusion Models}}},
  author = {McAllester, David},
  year = {2023},
  month = feb,
  number = {arXiv:2301.11108},
  eprint = {2301.11108},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.11108},
  urldate = {2023-02-20},
  abstract = {This paper presents the stochastic differential equations of diffusion models assuming only familiarity with Gaussian distributions. This treatment of the diffusion model SDE and the associated reverse-time SDEs unifies the VAE and score-matching treatments. It also yields the contribution of this paper -- a novel likelihood formula derived from a non-variational VAE analysis (equations (10) and (12) in the text). The paper presents the mathematics directly with attributions saved for a final section.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Probability},
  file = {/Users/pmg/Zotero/storage/LF2M395G/McAllester - 2023 - On the Mathematics of Diffusion Models.pdf;/Users/pmg/Zotero/storage/4P62XKYD/2301.html}
}

@misc{mcinnes_umap_2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  number = {arXiv:1802.03426},
  eprint = {1802.03426},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.03426},
  urldate = {2023-09-15},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/PDNR2UN3/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;/Users/pmg/Zotero/storage/IYSPEUXA/1802.html}
}

@misc{mcpartlon_deep_2022,
  title = {A {{Deep SE}}(3)-{{Equivariant Model}} for {{Learning Inverse Protein Folding}}},
  author = {McPartlon, Matt and Lai, Ben and Xu, Jinbo},
  year = {2022},
  month = apr,
  pages = {2022.04.15.488492},
  institution = {{bioRxiv}},
  doi = {10.1101/2022.04.15.488492},
  urldate = {2022-04-26},
  abstract = {In this work, we establish a framework to tackle the inverse protein design problem; the task of predicting a protein's primary sequence given its backbone conformation. To this end, we develop a generative SE(3)-equivariant model which significantly improves upon existing autoregressive methods. Conditioned on backbone structure, and trained with our novel partial masking scheme and side-chain conformation loss, we achieve state-of-the-art native sequence recovery on structurally independent CASP13, CASP14, CATH4.2, and TS50 test sets. On top of accurately recovering native sequences, we demonstrate that our model captures functional aspects of the underlying protein by accurately predicting the effects of point mutations through testing on Deep Mutational Scanning datasets. We further verify the efficacy of our approach by comparing with recently proposed inverse protein folding methods and by rigorous ablation studies.},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/EFM3GAV6/McPartlon et al. - 2022 - A Deep SE(3)-Equivariant Model for Learning Invers.pdf;/Users/pmg/Zotero/storage/3LMHMJBB/2022.04.15.488492v1.html}
}

  @article{meier_language_2021,
  title={Language models enable zero-shot prediction of the effects of mutations on protein function},
  author={Meier, Joshua and Rao, Roshan and Verkuil, Robert and Liu, Jason and Sercu, Tom and Rives, Alex},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29287--29303},
  year={2021}
}




@misc{minot_meta_2023,
  title = {Meta {{Learning Improves Robustness}} and {{Performance}} in {{Machine Learning-Guided Protein Engineering}}},
  author = {Minot, Mason and Reddy, Sai T.},
  year = {2023},
  month = jan,
  primaryclass = {New Results},
  pages = {2023.01.30.526201},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.01.30.526201},
  urldate = {2023-02-20},
  abstract = {Machine learning-guided protein engineering continues to rapidly progress, however, collecting large, well-labeled data sets remains time and resource intensive. Directed evolution and protein engineering studies often require extensive experimental processes to eliminate noise and fully label high-throughput protein sequence-function data. Meta learning methods established in other fields (e.g. computer vision and natural language processing) have proven effective in learning from noisy data, given the availability of a small data set with trusted labels and thus could be applied for protein engineering. Here, we generate yeast display antibody mutagenesis libraries and screen them for target antigen binding followed by deep sequencing. Meta learning approaches are able to learn under high synthetic and experimental noise as well as in under labeled data settings, typically outperforming baselines significantly and often requiring a fraction of the training data. Thus, we demonstrate meta learning may expedite and improve machine learning-guided protein engineering. Availability and implementation The code used in this study is publicly available at https://github.com/LSSI-ETH/meta-learning-for-protein-engineering. {$<$}img class="highwire-fragment fragment-image" alt="Figure" src="https://www.biorxiv.org/content/biorxiv/early/2023/01/30/2023.01.30.526201/F1.medium.gif" width="440" height="405"/{$>$}Download figureOpen in new tab},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/TZMKCIAN/Minot and Reddy - 2023 - Meta Learning Improves Robustness and Performance .pdf}
}

@article{mirdita_colabfold_2022,
  title = {{{ColabFold}}: Making Protein Folding Accessible to All},
  shorttitle = {{{ColabFold}}},
  author = {Mirdita, Milot and Sch{\"u}tze, Konstantin and Moriwaki, Yoshitaka and Heo, Lim and Ovchinnikov, Sergey and Steinegger, Martin},
  year = {2022},
  month = jun,
  journal = {Nature Methods},
  volume = {19},
  number = {6},
  pages = {679--682},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-022-01488-1},
  urldate = {2023-01-24},
  abstract = {ColabFold offers accelerated prediction of protein structures and complexes by combining the fast homology search of MMseqs2 with AlphaFold2 or RoseTTAFold. ColabFold's 40-60-fold faster search and optimized model utilization enables prediction of close to 1,000 structures per day on a server with one graphics processing unit. Coupled with Google Colaboratory, ColabFold becomes a free and accessible platform for protein folding. ColabFold is open-source software available at https://github.com/sokrypton/ColabFoldand its novel environmental databases are available at https://colabfold.mmseqs.com.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computational models,Protein databases,Protein structure predictions,Software},
  file = {/Users/pmg/Zotero/storage/VFFNZ8V6/Mirdita et al. - 2022 - ColabFold making protein folding accessible to al.pdf}
}

@article{mistry_pfam_2021,
  title = {Pfam: {{The}} Protein Families Database in 2021},
  shorttitle = {Pfam},
  author = {Mistry, Jaina and Chuguransky, Sara and Williams, Lowri and Qureshi, Matloob and Salazar, Gustavo~A and Sonnhammer, Erik L. L. and Tosatto, Silvio C. E. and Paladin, Lisanna and Raj, Shriya and Richardson, Lorna J. and Finn, Robert D. and Bateman, Alex},
  year = {2021},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {49},
  number = {D1},
  pages = {D412-D419},
  issn = {0305-1048},
  doi = {10.1093/nar/gkaa913},
  urldate = {2022-09-26},
  abstract = {The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and refine Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.},
  file = {/Users/pmg/Zotero/storage/LN54ZP2A/Mistry et al. - 2021 - Pfam The protein families database in 2021.pdf;/Users/pmg/Zotero/storage/8TU5NTJQ/5943818.html}
}

@article{moal_skempi_2012,
  title = {{{SKEMPI}}: A {{Structural Kinetic}} and {{Energetic}} Database of {{Mutant Protein Interactions}} and Its Use in Empirical Models},
  shorttitle = {{{SKEMPI}}},
  author = {Moal, Iain H. and {Fern{\'a}ndez-Recio}, Juan},
  year = {2012},
  month = oct,
  journal = {Bioinformatics},
  volume = {28},
  number = {20},
  pages = {2600--2607},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bts489},
  urldate = {2022-09-28},
  abstract = {Motivation: Empirical models for the prediction of how changes in sequence alter protein{\textendash}protein binding kinetics and thermodynamics can garner insights into many aspects of molecular biology. However, such models require empirical training data and proper validation before they can be widely applied. Previous databases contained few stabilizing mutations and no discussion of their inherent biases or how this impacts model construction or validation.Results: We present SKEMPI, a database of 3047 binding free energy changes upon mutation assembled from the scientific literature, for protein{\textendash}protein heterodimeric complexes with experimentally determined structures. This represents over four times more data than previously collected. Changes in 713 association and dissociation rates and 127 enthalpies and entropies were also recorded. The existence of biases towards specific mutations, residues, interfaces, proteins and protein families is discussed in the context of how the data can be used to construct predictive models. Finally, a cross-validation scheme is presented which is capable of estimating the efficacy of derived models on future data in which these biases are not present.Availability: The database is available online at http://life.bsc.es/pid/mutation\_database/Contact:juanf@bsc.es},
  file = {/Users/pmg/Zotero/storage/4NERRTK3/Moal and Fernández-Recio - 2012 - SKEMPI a Structural Kinetic and Energetic databas.pdf;/Users/pmg/Zotero/storage/9YVW5IKG/202485.html}
}

@misc{mohamed_monte_2020,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2020},
  month = sep,
  number = {arXiv:1906.10652},
  eprint = {1906.10652},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.10652},
  urldate = {2023-01-18},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/TKAT5MVH/Mohamed et al. - 2020 - Monte Carlo Gradient Estimation in Machine Learnin.pdf;/Users/pmg/Zotero/storage/5HSCKAD6/1906.html}
}

@article{morcos_direct-coupling_2011,
  title = {Direct-Coupling Analysis of Residue Coevolution Captures Native Contacts across Many Protein Families},
  author = {Morcos, Faruck and Pagnani, Andrea and Lunt, Bryan and Bertolino, Arianna and Marks, Debora S. and Sander, Chris and Zecchina, Riccardo and Onuchic, Jos{\'e} N. and Hwa, Terence and Weigt, Martin},
  year = {2011},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {49},
  pages = {E1293-E1301},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1111471108},
  urldate = {2022-09-02},
  file = {/Users/pmg/Zotero/storage/HQNX2UQE/Morcos et al. - 2011 - Direct-coupling analysis of residue coevolution ca.pdf}
}

@article{moreta_probabilistic_2019,
  title = {A {{Probabilistic Programming Approach}} to {{Protein Structure Superposition}}},
  author = {Moreta, Lys Sanz and {Al-Sibahi}, Ahmad Salim and Theobald, Douglas and Bullock, William and Rommes, Basile Nicolas and Manoukian, Andreas and Hamelryck, Thomas},
  year = {2019},
  month = jul,
  journal = {Proceedings of the ... IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology: CIBCB. IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology},
  volume = {2019},
  doi = {10.1109/cibcb.2019.8791469},
  abstract = {Optimal superposition of protein structures or other biological molecules is crucial for understanding their structure, function, dynamics and evolution. Here, we investigate the use of probabilistic programming to superimpose protein structures guided by a Bayesian model. Our model THESEUS-PP is based on the THESEUS model, a probabilistic model of protein superposition based on rotation, translation and perturbation of an underlying, latent mean structure. The model was implemented in the probabilistic programming language Pyro. Unlike conventional methods that minimize the sum of the squared distances, THESEUS takes into account correlated atom positions and heteroscedasticity (ie. atom positions can feature different variances). THESEUS performs maximum likelihood estimation using iterative expectation-maximization. In contrast, THESEUS-PP allows automated maximum a-posteriori (MAP) estimation using suitable priors over rotation, translation, variances and latent mean structure. The results indicate that probabilistic programming is a powerful new paradigm for the formulation of Bayesian probabilistic models concerning biomolecular structure. Specifically, we envision the use of the THESEUS-PP model as a suitable error model or likelihood in Bayesian protein structure prediction using deep probabilistic programming.},
  langid = {english},
  pmcid = {PMC8515897},
  pmid = {34661202},
  keywords = {Bayesian modelling,deep probabilistic programming,protein structure prediction,protein superposition},
  file = {/Users/pmg/Zotero/storage/XW9ZJLMA/Moreta et al. - 2019 - A Probabilistic Programming Approach to Protein St.pdf}
}

@misc{moschella_relative_2023,
  title = {Relative Representations Enable Zero-Shot Latent Space Communication},
  author = {Moschella, Luca and Maiorca, Valentino and Fumero, Marco and Norelli, Antonio and Locatello, Francesco and Rodol{\`a}, Emanuele},
  year = {2023},
  month = mar,
  number = {arXiv:2209.15430},
  eprint = {2209.15430},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.15430},
  urldate = {2023-08-29},
  abstract = {Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change. In this work, we propose the latent similarity between each sample and a fixed set of anchors as an alternative data representation, demonstrating that it can enforce the desired invariances without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).},
  archiveprefix = {arxiv},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6},
  file = {/Users/pmg/Zotero/storage/5STRRTL7/Moschella et al. - 2023 - Relative representations enable zero-shot latent s.pdf;/Users/pmg/Zotero/storage/8M7UYCF9/2209.html}
}

@article{munsamy_zymctrl_nodate,
  title = {{{ZymCTRL}}: A Conditional Language Model for the Controllable Generation of Artificial Enzymes},
  author = {Munsamy, Geraldene and Lindner, Sebastian and Lorenz, Philipp and Ferruz, Noelia},
  pages = {16},
  abstract = {The design of custom-tailored proteins has the potential to provide novel and groundbreaking solutions in many fields, including molecular medicine or environmental sciences. Among protein classes, enzymes are particularly attractive because their complex active sites can accelerate chemical transformations by several orders of magnitude. Since enzymes are biodegradable nanoscopic materials, they hold an unmatched promise as sustainable, large-scale industrial catalysts. Motivated by the enormous success of language models in designing novel yet nature-like proteins, we hypothesised that an enzyme-specific language model could provide new opportunities to design purpose-built artificial enzymes. Here, we describe ZymCTRL, a conditional language model trained on the BRENDA database of enzymes, which generates enzymes of a specific enzymatic class upon a user prompt. ZymCTRL generates artificial enzymes distant from natural ones while their intended functionality matches predictions from orthogonal methods. We release the model to the community.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/GBFXBVXC/Munsamy et al. - ZymCTRL a conditional language model for the cont.pdf}
}

@misc{nathansen_evaluating_2023,
  title = {Evaluating {{Prompt Tuning}} for {{Conditional Protein Sequence Generation}}},
  author = {Nathansen, Andrea and Klein, Kevin and Renard, Bernhard Y. and Nowicka, Melania and Bartoszewicz, Jakub M.},
  year = {2023},
  month = mar,
  primaryclass = {New Results},
  pages = {2023.02.28.530492},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.28.530492},
  urldate = {2023-03-09},
  abstract = {Text generation models originally developed for natural language processing have proven to be successful in generating protein sequences. These models are often finetuned for improved performance on more specific tasks, such as generation of proteins from families unseen in training. Considering the high computational cost of finetuning separate models for each downstream task, prompt tuning has been proposed as an alternative. However, no openly available implementation of this approach compatible with protein language models has been previously published. Thus, we adapt an open-source codebase designed for NLP models to build a pipeline for prompt tuning on protein sequence data, supporting the protein language models ProtGPT2 and RITA. We evaluate our implementation by learning prompts for conditional sampling of sequences belonging to a specific protein family. This results in improved performance compared to the base model. However, in the presented use case, we observe discrepancies between text-based evaluation and predicted biological properties of the generated sequences, identifying open problems for principled assessment of protein sequence generation quality.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/DUVCT6ZA/Nathansen et al. - 2023 - Evaluating Prompt Tuning for Conditional Protein S.pdf}
}

@misc{nichol_improved_2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  author = {Nichol, Alex and Dhariwal, Prafulla},
  year = {2021},
  month = feb,
  number = {arXiv:2102.09672},
  eprint = {2102.09672},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.09672},
  urldate = {2022-12-19},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/BKATF7K6/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf;/Users/pmg/Zotero/storage/YES7KYSS/2102.html}
}

@misc{nijkamp_progen2_2022,
  title = {{{ProGen2}}: {{Exploring}} the {{Boundaries}} of {{Protein Language Models}}},
  shorttitle = {{{ProGen2}}},
  author = {Nijkamp, Erik and Ruffolo, Jeffrey and Weinstein, Eli N. and Naik, Nikhil and Madani, Ali},
  year = {2022},
  month = jun,
  number = {arXiv:2206.13517},
  eprint = {2206.13517},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.13517},
  urldate = {2022-08-03},
  abstract = {Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/4WWINGIU/Nijkamp et al. - 2022 - ProGen2 Exploring the Boundaries of Protein Langu.pdf;/Users/pmg/Zotero/storage/63RC5DDX/2206.html}
}

@article{nixon_measuring_2019,
  title = {Measuring {{Calibration}} in {{Deep Learning}}},
  author = {Nixon, Jeremy and Dusenberry, Mike and Jerfel, Ghassen and Zhang, Linchuan and Tran, Dustin},
  year = {2019},
  month = dec,
  urldate = {2023-03-01},
  abstract = {Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction. We propose two new measures for calibration, the Static Calibration Error (SCE) and Adaptive Calibration Error (ACE). These measures take into account every prediction made by a model, in contrast to the popular Expected Calibration Error.},
  langid = {english}
}

@misc{nixon_measuring_2020,
  title = {Measuring {{Calibration}} in {{Deep Learning}}},
  author = {Nixon, Jeremy and Dusenberry, Mike and Jerfel, Ghassen and Nguyen, Timothy and Liu, Jeremiah and Zhang, Linchuan and Tran, Dustin},
  year = {2020},
  month = aug,
  number = {arXiv:1904.01685},
  eprint = {1904.01685},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.01685},
  urldate = {2023-03-01},
  abstract = {Overconfidence and underconfidence in machine learning classifiers is measured by calibration: the degree to which the probabilities predicted for each class match the accuracy of the classifier on that prediction. How one measures calibration remains a challenge: expected calibration error, the most popular metric, has numerous flaws which we outline, and there is no clear empirical understanding of how its choices affect conclusions in practice, and what recommendations there are to counteract its flaws. In this paper, we perform a comprehensive empirical study of choices in calibration measures including measuring all probabilities rather than just the maximum prediction, thresholding probability values, class conditionality, number of bins, bins that are adaptive to the datapoint density, and the norm used to compare accuracies to confidences. To analyze the sensitivity of calibration measures, we study the impact of optimizing directly for each variant with recalibration techniques. Across MNIST, Fashion MNIST, CIFAR-10/100, and ImageNet, we find that conclusions on the rank ordering of recalibration methods is drastically impacted by the choice of calibration measure. We find that conditioning on the class leads to more effective calibration evaluations, and that using the L2 norm rather than the L1 norm improves both optimization for calibration metrics and the rank correlation measuring metric consistency. Adaptive binning schemes lead to more stablity of metric rank ordering when the number of bins vary, and is also recommended. We open source a library for the use of our calibration measures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/WDFERKXQ/Nixon et al. - 2020 - Measuring Calibration in Deep Learning.pdf;/Users/pmg/Zotero/storage/TCJDFKSV/1904.html}
}

@article{nojoomi_string_2017,
  title = {String Kernels for Protein Sequence Comparisons: Improved Fold Recognition},
  shorttitle = {String Kernels for Protein Sequence Comparisons},
  author = {Nojoomi, Saghi and Koehl, Patrice},
  year = {2017},
  month = feb,
  journal = {BMC Bioinformatics},
  volume = {18},
  number = {1},
  pages = {137},
  issn = {1471-2105},
  doi = {10.1186/s12859-017-1560-9},
  urldate = {2024-01-31},
  abstract = {The amino acid sequence of a protein is the blueprint from which its structure and ultimately function can be derived. Therefore, sequence comparison methods remain essential for the determination of similarity between proteins. Traditional approaches for comparing two protein sequences begin with strings of letters (amino acids) that represent the sequences, before generating textual alignments between these strings and providing scores for each alignment. When the similitude between the two protein sequences to be compared is low however, the quality of the corresponding sequence alignment is usually poor, leading to poor performance for the recognition of similarity.},
  keywords = {Alignment free methods,Kernel,Protein sequence},
  file = {/Users/pmg/Zotero/storage/VW9HC4D8/Nojoomi and Koehl - 2017 - String kernels for protein sequence comparisons i.pdf}
}

@inproceedings{notin_proteingym_2023,
  title = {{{ProteinGym}}: {{Large-Scale Benchmarks}} for {{Protein Fitness Prediction}} and {{Design}}},
  shorttitle = {{{ProteinGym}}},
  booktitle = {Thirty-Seventh {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Notin, Pascal and Kollasch, Aaron W. and Ritter, Daniel and Niekerk, Lood Van and Paul, Steffan and Spinner, Han and Rollins, Nathan J. and Shaw, Ada and Orenbuch, Rose and Weitzman, Ruben and Frazer, Jonathan and Dias, Mafalda and Franceschi, Dinko and Gal, Yarin and Marks, Debora Susan},
  year = {2023},
  month = nov,
  urldate = {2023-12-19},
  abstract = {Predicting the effects of mutations in proteins is critical to many applications, from understanding genetic disease to designing novel proteins to address our most pressing challenges in climate, agriculture and healthcare. Despite an increase in machine learning-based protein modeling methods, assessing their effectiveness is problematic due to the use of distinct, often contrived, experimental datasets and variable performance across different protein families. Addressing these challenges requires scale. To that end we introduce ProteinGym v1.0, a large-scale and holistic set of benchmarks specifically designed for protein fitness prediction and design. It encompasses both a broad collection of over 250 standardized deep mutational scanning assays, spanning millions of mutated sequences, as well as curated clinical datasets providing high-quality expert annotations about mutation effects. We devise a robust evaluation framework that combines metrics for both fitness prediction and design, factors in known limitations of the underlying experimental methods, and covers both zero-shot and supervised settings. We report the performance of a diverse set of over 40 high-performing models from various subfields (eg., mutation effects, inverse folding) into a unified benchmark. We open source the corresponding codebase, datasets, MSAs, structures, predictions and develop a user-friendly website that facilitates comparisons across all settings.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/P4BD83QA/Notin et al. - 2023 - ProteinGym Large-Scale Benchmarks for Protein Fit.pdf}
}

@inproceedings{notin_proteinnpt_2023,
 author = {Notin, Pascal and Weitzman, Ruben and Marks, Debora and Gal, Yarin},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {33529--33563},
 publisher = {Curran Associates, Inc.},
 title = {{ProteinNPT: Improving Protein Property Prediction and Design with Non-Parametric Transformers}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6a4d5d85f7a52f062d23d98d544a5578-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@inproceedings{notin_trancepteve_2022,
  title = {{{TranceptEVE}}: {{Combining Family-specific}} and {{Family-agnostic Models}} of {{Protein Sequences}} for {{Improved Fitness Prediction}}},
  shorttitle = {{{TranceptEVE}}},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Learning Meaningful Representations}} of {{Life}}},
  author = {Notin, Pascal and Niekerk, Lood Van and Kollasch, Aaron W. and Ritter, Daniel and Gal, Yarin and Marks, Debora Susan},
  year = {2022},
  month = nov,
  urldate = {2022-12-06},
  abstract = {Successful approaches that model the fitness landscape of protein sequences have typically relied on family-specific sets of homologous sequences called multiple-sequence alignments (Hopf et al. 2017; Riesselman et al. 2018; Frazer et al. 2021). They are however limited by the fact many proteins are difficult to align or have shallow alignments. Newer models such as transformers that do not rely on alignments have been promising (Madani et al. 2020; Rives et al. 2021; Notin et al. 2022; Hesselow et al. 2022) to progressively bridge the gap with their alignment-based counterparts. In this work, we introduce TranceptEVE -- a hybrid between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach to achieve state-of-the-art performance on the fitness prediction task. We demonstrate that it outperforms all other baselines on the recently released ProteinGym benchmarks (Notin et al. 2022) -- a curated set of 94 deep mutational scanning assays to assess the effects of substitution and indel mutations. We also quantify its ability to predict the pathogenicity of genetic mutations in humans based on annotations from ClinVar.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/S5PHWAEJ/Notin et al. - 2022 - TranceptEVE Combining Family-specific and Family-.pdf;/Users/pmg/Zotero/storage/5P4SYD2E/forum.html}
}

@misc{notin_trancepteve_2022-1,
  title = {{{TranceptEVE}}: {{Combining Family-specific}} and {{Family-agnostic Models}} of {{Protein Sequences}} for {{Improved Fitness Prediction}}},
  shorttitle = {{{TranceptEVE}}},
  author = {Notin, Pascal and Niekerk, Lood Van and Kollasch, Aaron W. and Ritter, Daniel and Gal, Yarin and Marks, Debora S.},
  year = {2022},
  month = dec,
  primaryclass = {New Results},
  pages = {2022.12.07.519495},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.12.07.519495},
  urldate = {2023-08-28},
  abstract = {Modeling the fitness landscape of protein sequences has historically relied on training models on family-specific sets of homologous sequences called Multiple Sequence Alignments. Many proteins are however difficult to align or have shallow alignments which limits the potential scope of alignment-based methods. Not subject to these limitations, large protein language models trained on non-aligned sequences across protein families have achieved increasingly high predictive performance {\textendash} but have not yet fully bridged the gap with their alignment-based counterparts. In this work, we introduce TranceptEVE {\textendash} a hybrid method between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach. Our method gracefully adapts to the depth of the alignment, fully relying on its autoregressive transformer when dealing with shallow alignments and leaning more heavily on the family-specific models for proteins with deeper alignments. Besides its broader application scope, it achieves state-of-the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from ClinVar.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/28LZJBL3/Notin et al. - 2022 - TranceptEVE Combining Family-specific and Family-.pdf}
}

@inproceedings{notin_tranception_2022,
  title={Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval},
  author={Notin, Pascal and Dias, Mafalda and Frazer, Jonathan and Marchena-Hurtado, Javier and Gomez, Aidan N. and Marks, Debora and Gal, Yarin},
  booktitle={International Conference on Machine Learning},
  pages={16990--17017},
  year={2022},
  organization={PMLR}
}


@article{olson_comprehensive_2014,
  title = {A Comprehensive Biophysical Description of Pairwise Epistasis throughout an Entire Protein Domain},
  author = {Olson, C. Anders and Wu, Nicholas C. and Sun, Ren},
  year = {2014},
  month = nov,
  journal = {Current biology : CB},
  volume = {24},
  number = {22},
  pages = {2643--2651},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2014.09.072},
  urldate = {2023-11-21},
  abstract = {Background Non-additivity in fitness effects from two or more mutations, termed epistasis, can result in compensation of deleterious mutations or negation of beneficial mutations. Recent evidence shows the importance of epistasis in individual evolutionary pathways. However, an unresolved question in molecular evolution is how often and how significantly fitness effects change in alternative genetic backgrounds. Results To answer this question we quantified the effects of all single mutations and double mutations between all positions in the IgG-binding domain of protein G (GB1). By observing the first two steps of all possible evolutionary pathways, this fitness profile enabled the characterization of the extent and magnitude of pairwise epistasis throughout an entire protein molecule. Furthermore, we developed a novel approach to quantitatively determine the effects of single mutations on structural stability ({$\Delta\Delta$}GU). This enabled determination of the importance of stability effects in functional epistasis. Conclusions Our results illustrate common biophysical mechanisms for occurrences of positive and negative epistasis. Our results show pervasive positive epistasis within a conformationally dynamic network of residues. The stability analysis shows that significant negative epistasis, which is more common than positive epistasis, mostly occurs between combinations of destabilizing mutations. Furthermore, we show that although significant positive epistasis is rare, many deleterious mutations are beneficial in at least one alternative mutational background. The distribution of conditionally beneficial mutations throughout the domain demonstrates that the functional portion of sequence space can be significantly expanded by epistasis.},
  pmcid = {PMC4254498},
  pmid = {25455030},
  file = {/Users/pmg/Zotero/storage/RCFFMGI9/Olson et al. - 2014 - A comprehensive biophysical description of pairwis.pdf}
}

@inproceedings{ortegon_conditional_2022,
  title = {Conditional {{Neural Processes}} for {{Molecules}}},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Learning Meaningful Representations}} of {{Life}}},
  author = {Ortegon, Miguel Garcia and Bender, Andreas and Bacallado, Sergio},
  year = {2022},
  month = nov,
  urldate = {2022-12-06},
  abstract = {Neural processes (NPs) are models for transfer learning with properties reminiscent of Gaussian Processes (GPs). They are adept at modelling data consisting of few observations of many related functions on the same input space and are trained by minimizing a variational objective, which is computationally much less expensive than the Bayesian updating required by GPs. So far, most studies of NPs have focused on low-dimensional datasets which are not representative of realistic transfer learning tasks. Drug discovery is one application area that is characterized by datasets consisting of many chemical properties or functions which are sparsely observed, yet depend on shared features or representations of the molecular inputs. This paper applies the conditional neural process (CNP) to DOCKSTRING, a dataset of docking scores for benchmarking ML models. CNPs show competitive performance in few-shot learning tasks relative to supervised learning baselines common in QSAR modelling, as well as an alternative model for transfer learning based on pre-training and refining neural network regressors. We present a Bayesian optimization experiment which showcases the probabilistic nature of CNPs and discuss shortcomings of the model in uncertainty quantification.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/QKVPY8QD/Ortegon et al. - 2022 - Conditional Neural Processes for Molecules.pdf;/Users/pmg/Zotero/storage/TZ88ZG9K/forum.html}
}

@article{pan_recent_2021,
  title = {Recent Advances in de Novo Protein Design: {{Principles}}, Methods, and Applications},
  shorttitle = {Recent Advances in de Novo Protein Design},
  author = {Pan, Xingjie and Kortemme, Tanja},
  year = {2021},
  month = jan,
  journal = {Journal of Biological Chemistry},
  volume = {296},
  pages = {100558},
  issn = {0021-9258},
  doi = {10.1016/j.jbc.2021.100558},
  urldate = {2023-06-08},
  abstract = {The computational de novo protein design is increasingly applied to address a number of key challenges in biomedicine and biological engineering. Successes in expanding applications are driven by advances in design principles and methods over several decades. Here, we review recent innovations in major aspects of the de novo protein design and include how these advances were informed by principles of protein architecture and interactions derived from the wealth of structures in the Protein Data Bank. We describe developments in de novo generation of designable backbone structures, optimization of sequences, design scoring functions, and the design of the function. The advances not only highlight design goals reachable now but also point to the challenges and opportunities for the future of the field.},
  langid = {english},
  keywords = {biophysics,computational protein design,PDB,protein design,protein structure,Rosetta},
  file = {/Users/pmg/Zotero/storage/FQ97BWJ3/Pan and Kortemme - 2021 - Recent advances in de novo protein design Princip.pdf;/Users/pmg/Zotero/storage/YNASQR5B/S0021925821003367.html}
}

@misc{papillon_architectures_2023,
  title = {Architectures of {{Topological Deep Learning}}: {{A Survey}} on {{Topological Neural Networks}}},
  shorttitle = {Architectures of {{Topological Deep Learning}}},
  author = {Papillon, Mathilde and Sanborn, Sophia and Hajij, Mustafa and Miolane, Nina},
  year = {2023},
  month = apr,
  number = {arXiv:2304.10031},
  eprint = {2304.10031},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.10031},
  urldate = {2023-04-24},
  abstract = {The natural world is full of complex systems characterized by intricate relations between their components: from social interactions between individuals in a social network to electrostatic interactions between atoms in a protein. Topological Deep Learning (TDL) provides a comprehensive framework to process and extract knowledge from data associated with these systems, such as predicting the social community to which an individual belongs or predicting whether a protein can be a reasonable target for drug development. TDL has demonstrated theoretical and practical advantages that hold the promise of breaking ground in the applied sciences and beyond. However, the rapid growth of the TDL literature has also led to a lack of unification in notation and language across Topological Neural Network (TNN) architectures. This presents a real obstacle for building upon existing works and for deploying TNNs to new real-world problems. To address this issue, we provide an accessible introduction to TDL, and compare the recently published TNNs using a unified mathematical and graphical notation. Through an intuitive and critical review of the emerging field of TDL, we extract valuable insights into current challenges and exciting opportunities for future development.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/TZYHVKJZ/Papillon et al. - 2023 - Architectures of Topological Deep Learning A Surv.pdf;/Users/pmg/Zotero/storage/GRWV9N4N/2304.html}
}

@article{papyan_prevalence_2020,
  title = {Prevalence of {{Neural Collapse}} during the Terminal Phase of Deep Learning Training},
  author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
  year = {2020},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {40},
  eprint = {2008.08186},
  primaryclass = {cs, stat},
  pages = {24652--24663},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2015509117},
  urldate = {2023-08-23},
  abstract = {Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier's decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/IZEZ7FLW/Papyan et al. - 2020 - Prevalence of Neural Collapse during the terminal .pdf;/Users/pmg/Zotero/storage/GY9XZS62/2008.html}
}

@misc{park_symmetric_2019,
  title = {Symmetric {{Graph Convolutional Autoencoder}} for {{Unsupervised Graph Representation Learning}}},
  author = {Park, Jiwoong and Lee, Minsik and Chang, Hyung Jin and Lee, Kyuewang and Choi, Jin Young},
  year = {2019},
  month = aug,
  number = {arXiv:1908.02441},
  eprint = {1908.02441},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.02441},
  urldate = {2023-04-24},
  abstract = {We propose a symmetric graph convolutional autoencoder which produces a low-dimensional latent representation from a graph. In contrast to the existing graph autoencoders with asymmetric decoder parts, the proposed autoencoder has a newly designed decoder which builds a completely symmetric autoencoder form. For the reconstruction of node features, the decoder is designed based on Laplacian sharpening as the counterpart of Laplacian smoothing of the encoder, which allows utilizing the graph structure in the whole processes of the proposed autoencoder architecture. In order to prevent the numerical instability of the network caused by the Laplacian sharpening introduction, we further propose a new numerically stable form of the Laplacian sharpening by incorporating the signed graphs. In addition, a new cost function which finds a latent representation and a latent affinity matrix simultaneously is devised to boost the performance of image clustering tasks. The experimental results on clustering, link prediction and visualization tasks strongly support that the proposed model is stable and outperforms various state-of-the-art algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/TSNPGBZM/Park et al. - 2019 - Symmetric Graph Convolutional Autoencoder for Unsu.pdf;/Users/pmg/Zotero/storage/ZDPSDRPL/1908.html}
}

@misc{parkinson_linear-scaling_2023,
  title = {Linear-Scaling Kernels for Protein Sequences and Small Molecules Outperform Deep Learning While Providing Uncertainty Quantitation and Improved Interpretability},
  author = {Parkinson, Jonathan and Wang, Wei},
  year = {2023},
  month = jun,
  number = {arXiv:2302.03294},
  eprint = {2302.03294},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.03294},
  urldate = {2023-09-29},
  abstract = {Gaussian process (GP) is a Bayesian model which provides several advantages for regression tasks in machine learning such as reliable quantitation of uncertainty and improved interpretability. Their adoption has been precluded by their excessive computational cost and by the difficulty in adapting them for analyzing sequences (e.g. amino acid and nucleotide sequences) and graphs (e.g. ones representing small molecules). In this study, we develop efficient and scalable approaches for fitting GP models as well as fast convolution kernels which scale linearly with graph or sequence size. We implement these improvements by building an open-source Python library called xGPR. We compare the performance of xGPR with the reported performance of various deep learning models on 20 benchmarks, including small molecule, protein sequence and tabular data. We show that xGRP achieves highly competitive performance with much shorter training time. Furthermore, we also develop new kernels for sequence and graph data and show that xGPR generally outperforms convolutional neural networks on predicting key properties of proteins and small molecules. Importantly, xGPR provides uncertainty information not available from typical deep learning models. Additionally, xGPR provides a representation of the input data that can be used for clustering and data visualization. These results demonstrate that xGPR provides a powerful and generic tool that can be broadly useful in protein engineering and drug discovery.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/5ZZV6JML/Parkinson and Wang - 2023 - Linear-scaling kernels for protein sequences and s.pdf;/Users/pmg/Zotero/storage/DWTQW8T9/2302.html}
}

@misc{parnami_learning_2022,
  title = {Learning from {{Few Examples}}: {{A Summary}} of {{Approaches}} to {{Few-Shot Learning}}},
  shorttitle = {Learning from {{Few Examples}}},
  author = {Parnami, Archit and Lee, Minwoo},
  year = {2022},
  month = mar,
  number = {arXiv:2203.04291},
  eprint = {2203.04291},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.04291},
  urldate = {2023-06-09},
  abstract = {Few-Shot Learning refers to the problem of learning the underlying pattern in the data just from a few training samples. Requiring a large number of data samples, many deep learning solutions suffer from data hunger and extensively high computation time and resources. Furthermore, data is often not available due to not only the nature of the problem or privacy concerns but also the cost of data preparation. Data collection, preprocessing, and labeling are strenuous human tasks. Therefore, few-shot learning that could drastically reduce the turnaround time of building machine learning applications emerges as a low-cost solution. This survey paper comprises a representative list of recently proposed few-shot learning algorithms. Given the learning dynamics and characteristics, the approaches to few-shot learning problems are discussed in the perspectives of meta-learning, transfer learning, and hybrid approaches (i.e., different variations of the few-shot learning problem).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/C5Y9DWW3/Parnami and Lee - 2022 - Learning from Few Examples A Summary of Approache.pdf;/Users/pmg/Zotero/storage/E95JJTNQ/2203.html}
}

@article{pires_mcsm_2014,
  title = {{{mCSM}}: Predicting the Effects of Mutations in Proteins Using Graph-Based Signatures},
  shorttitle = {{{mCSM}}},
  author = {Pires, Douglas E. V. and Ascher, David B. and Blundell, Tom L.},
  year = {2014},
  month = feb,
  journal = {Bioinformatics},
  volume = {30},
  number = {3},
  pages = {335--342},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btt691},
  urldate = {2022-11-10},
  abstract = {Motivation: Mutations play fundamental roles in evolution by introducing diversity into genomes. Missense mutations in structural genes may become either selectively advantageous or disadvantageous to the organism by affecting protein stability and/or interfering with interactions between partners. Thus, the ability to predict the impact of mutations on protein stability and interactions is of significant value, particularly in understanding the effects of Mendelian and somatic mutations on the progression of disease. Here, we propose a novel approach to the study of missense mutations, called mCSM, which relies on graph-based signatures. These encode distance patterns between atoms and are used to represent the protein residue environment and to train predictive models. To understand the roles of mutations in disease, we have evaluated their impacts not only on protein stability but also on protein{\textendash}protein and protein{\textendash}nucleic acid interactions.Results: We show that mCSM performs as well as or better than other methods that are used widely. The mCSM signatures were successfully used in different tasks demonstrating that the impact of a mutation can be correlated with the atomic-distance patterns surrounding an amino acid residue. We showed that mCSM can predict stability changes of a wide range of mutations occurring in the tumour suppressor protein p53, demonstrating the applicability of the proposed method in a challenging disease scenario.Availability and implementation: A web server is available at http://structure.bioc.cam.ac.uk/mcsm.Contact:dpires@dcc.ufmg.br; tom@cryst.bioc.cam.ac.ukSupplementary information:Supplementary data are available at Bioinformatics online.},
  file = {/Users/pmg/Zotero/storage/QAZJ5BFP/Pires et al. - 2014 - mCSM predicting the effects of mutations in prote.pdf;/Users/pmg/Zotero/storage/EBTQ6K8H/228906.html}
}

@article{plesa_multiplexed_2018,
  title = {Multiplexed Gene Synthesis in Emulsions for Exploring Protein Functional Landscapes},
  author = {Plesa, Calin and Sidore, Angus M. and Lubock, Nathan B. and Zhang, Di and Kosuri, Sriram},
  year = {2018},
  month = jan,
  journal = {Science},
  volume = {359},
  number = {6373},
  pages = {343--347},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aao5167},
  urldate = {2022-08-29},
  file = {/Users/pmg/Zotero/storage/XHJM2C85/Plesa et al. - 2018 - Multiplexed gene synthesis in emulsions for explor.pdf}
}

@article{pokusaeva_experimental_2019,
  title = {An Experimental Assay of the Interactions of Amino Acids from Orthologous Sequences Shaping a Complex Fitness Landscape},
  author = {Pokusaeva, Victoria O. and Usmanova, Dinara R. and Putintseva, Ekaterina V. and Espinar, Lorena and Sarkisyan, Karen S. and Mishin, Alexander S. and Bogatyreva, Natalya S. and Ivankov, Dmitry N. and Akopyan, Arseniy V. and Avvakumov, Sergey Ya and Povolotskaya, Inna S. and Filion, Guillaume J. and Carey, Lucas B. and Kondrashov, Fyodor A.},
  year = {2019},
  month = apr,
  journal = {PLOS Genetics},
  volume = {15},
  number = {4},
  pages = {e1008079},
  publisher = {{Public Library of Science}},
  issn = {1553-7404},
  doi = {10.1371/journal.pgen.1008079},
  urldate = {2022-11-11},
  abstract = {Characterizing the fitness landscape, a representation of fitness for a large set of genotypes, is key to understanding how genetic information is interpreted to create functional organisms. Here we determined the evolutionarily-relevant segment of the fitness landscape of His3, a gene coding for an enzyme in the histidine synthesis pathway, focusing on combinations of amino acid states found at orthologous sites of extant species. Just 15\% of amino acids found in yeast His3 orthologues were always neutral while the impact on fitness of the remaining 85\% depended on the genetic background. Furthermore, at 67\% of sites, amino acid replacements were under sign epistasis, having both strongly positive and negative effect in different genetic backgrounds. 46\% of sites were under reciprocal sign epistasis. The fitness impact of amino acid replacements was influenced by only a few genetic backgrounds but involved interaction of multiple sites, shaping a rugged fitness landscape in which many of the shortest paths between highly fit genotypes are inaccessible.},
  langid = {english},
  keywords = {Amino acid substitution,Evolutionary genetics,Fitness epistasis,Protein sequencing,Saccharomyces cerevisiae,Sign epistasis,Variant genotypes,Yeast},
  file = {/Users/pmg/Zotero/storage/7WIZ7NA9/Pokusaeva et al. - 2019 - An experimental assay of the interactions of amino.pdf;/Users/pmg/Zotero/storage/2I863NRF/article.html}
}


@article{polson_half-cauchy_2011,
  title = {{On the Half-Cauchy Prior for a Global Scale Parameter}},
  author={Polson, Nicholas G. and Scott, James G.},
  journal={Bayesian Analysis},
  volume={7},
  number={4},
  pages={887--902},
  year={2012},
  publisher={International Society for Bayesian Analysis}
}


@inproceedings{praljak_auto-regressive_2022,
  title = {{Auto-Regressive WaveNet Variational Autoencoders for Alignment-free Generative Protein Design and Fitness Prediction}},
  booktitle = {{{ICLR2022 Machine Learning}} for {{Drug Discovery}}},
  author = {Praljak, Niksa and Ferguson, Andrew},
  year = {2022},
  month = mar,
  urldate = {2022-04-27},
  abstract = {Our results demonstrate that combining a semi-supervised InfoVAE model with a WaveNet-based generator provides a robust framework for state-of-the-art functional prediction and generative protein...},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/DEKPDQM5/Praljak and Ferguson - 2022 - Auto-regressive WaveNet Variational Autoencoders f.pdf;/Users/pmg/Zotero/storage/GIPAER7R/forum.html}
}

@article{price_fasttree_2009,
  title = {{{FastTree}}: {{Computing Large Minimum Evolution Trees}} with {{Profiles}} Instead of a {{Distance Matrix}}},
  shorttitle = {{{FastTree}}},
  author = {Price, Morgan N. and Dehal, Paramvir S. and Arkin, Adam P.},
  year = {2009},
  month = jul,
  journal = {Molecular Biology and Evolution},
  volume = {26},
  number = {7},
  pages = {1641--1650},
  issn = {0737-4038},
  doi = {10.1093/molbev/msp077},
  urldate = {2023-01-24},
  abstract = {Gene families are growing rapidly, but standard methods for inferring phylogenies do not scale to alignments with over 10,000 sequences. We present FastTree, a method for constructing large phylogenies and for estimating their reliability. Instead of storing a distance matrix, FastTree stores sequence profiles of internal nodes in the tree. FastTree uses these profiles to implement Neighbor-Joining and uses heuristics to quickly identify candidate joins. FastTree then uses nearest neighbor interchanges to reduce the length of the tree. For an alignment with N sequences, L sites, and a different characters, a distance matrix requires O(N2) space and O(N2L) time, but FastTree requires just O(NLa + N) memory and O(Nlog (N)La) time. To estimate the tree's reliability, FastTree uses local bootstrapping, which gives another 100-fold speedup over a distance matrix. For example, FastTree computed a tree and support values for 158,022 distinct 16S ribosomal RNAs in 17 h and 2.4 GB of memory. Just computing pairwise Jukes{\textendash}Cantor distances and storing them, without inferring a tree or bootstrapping, would require 17 h and 50 GB of memory. In simulations, FastTree was slightly more accurate than Neighbor-Joining, BIONJ, or FastME; on genuine alignments, FastTree's topologies had higher likelihoods. FastTree is available at http://microbesonline.org/fasttree.},
  file = {/Users/pmg/Zotero/storage/NS832DVB/Price et al. - 2009 - FastTree Computing Large Minimum Evolution Trees .pdf;/Users/pmg/Zotero/storage/I9HCC6KR/1128976.html}
}

@article{pucci_quantification_2018,
  title = {Quantification of Biases in Predictions of Protein Stability Changes upon Mutations},
  author = {Pucci, Fabrizio and Bernaerts, Katrien V and Kwasigroch, Jean Marc and Rooman, Marianne},
  year = {2018},
  month = nov,
  journal = {Bioinformatics},
  volume = {34},
  number = {21},
  pages = {3659--3665},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bty348},
  urldate = {2022-11-10},
  abstract = {Bioinformatics tools that predict protein stability changes upon point mutations have made a lot of progress in the last decades and have become accurate and fast enough to make computational mutagenesis experiments feasible, even on a proteome scale. Despite these achievements, they still suffer from important issues that must be solved to allow further improving their performances and utilizing them to deepen our insights into protein folding and stability mechanisms. One of these problems is their bias toward the learning datasets which, being dominated by destabilizing mutations, causes predictions to be better for destabilizing than for stabilizing mutations.We thoroughly analyzed the biases in the prediction of folding free energy changes upon point mutations ({$\Delta\Delta$}G0) and proposed some unbiased solutions. We started by constructing a dataset Ssym of experimentally measured {$\Delta\Delta$}G0s with an equal number of stabilizing and destabilizing mutations, by collecting mutations for which the structure of both the wild-type and mutant protein is available. On this balanced dataset, we assessed the performances of 15 widely used {$\Delta\Delta$}G0 predictors. After the astonishing observation that almost all these methods are strongly biased toward destabilizing mutations, especially those that use black-box machine learning, we proposed an elegant way to solve the bias issue by imposing physical symmetries under inverse mutations on the model structure, which we implemented in PoPMuSiCsym. This new predictor constitutes an efficient trade-off between accuracy and absence of biases. Some final considerations and suggestions for further improvement of the predictors are discussed.Supplementary data are available at Bioinformatics online.The article 10.1093/bioinformatics/bty340/, published alongside this paper, also addresses the problem of biases in protein stability change predictions.},
  file = {/Users/pmg/Zotero/storage/523QYYUT/Pucci et al. - 2018 - Quantification of biases in predictions of protein.pdf;/Users/pmg/Zotero/storage/A5YL5A82/4987874.html}
}

@misc{qin_graph_2023,
  title = {Graph {{Coordinates}} and {{Conventional Neural Networks}} -- {{An Alternative}} for {{Graph Neural Networks}}},
  author = {Qin, Zheyi and Paffenroth, Randy and Jayasumana, Anura P.},
  year = {2023},
  month = dec,
  number = {arXiv:2312.01342},
  eprint = {2312.01342},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.01342},
  urldate = {2023-12-08},
  abstract = {Graph-based data present unique challenges and opportunities for machine learning. Graph Neural Networks (GNNs), and especially those algorithms that capture graph topology through message passing for neighborhood aggregation, have been a leading solution. However, these networks often require substantial computational resources and may not optimally leverage the information contained in the graph's topology, particularly for large-scale or complex graphs. We propose Topology Coordinate Neural Network (TCNN) and Directional Virtual Coordinate Neural Network (DVCNN) as novel and efficient alternatives to message passing GNNs, that directly leverage the graph's topology, sidestepping the computational challenges presented by competing algorithms. Our proposed methods can be viewed as a reprise of classic techniques for graph embedding for neural network feature engineering, but they are novel in that our embedding techniques leverage ideas in Graph Coordinates (GC) that are lacking in current practice. Experimental results, benchmarked against the Open Graph Benchmark Leaderboard, demonstrate that TCNN and DVCNN achieve competitive or superior performance to message passing GNNs. For similar levels of accuracy and ROC-AUC, TCNN and DVCNN need far fewer trainable parameters than contenders of the OGBN Leaderboard. The proposed TCNN architecture requires fewer parameters than any neural network method currently listed in the OGBN Leaderboard for both OGBN-Proteins and OGBN-Products datasets. Conversely, our methods achieve higher performance for a similar number of trainable parameters. By providing an efficient and effective alternative to message passing GNNs, our work expands the toolbox of techniques for graph-based machine learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/Q49F5VLN/Qin et al. - 2023 - Graph Coordinates and Conventional Neural Networks.pdf;/Users/pmg/Zotero/storage/ER3MZUBK/2312.html}
}

@misc{qiu_artificial_2023,
  title = {Artificial Intelligence-Aided Protein Engineering: From Topological Data Analysis to Deep Protein Language Models},
  shorttitle = {Artificial Intelligence-Aided Protein Engineering},
  author = {Qiu, Yuchi and Wei, Guo-Wei},
  year = {2023},
  month = jul,
  number = {arXiv:2307.14587},
  eprint = {2307.14587},
  primaryclass = {q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.14587},
  urldate = {2023-09-05},
  abstract = {Protein engineering is an emerging field in biotechnology that has the potential to revolutionize various areas, such as antibody design, drug discovery, food security, ecology, and more. However, the mutational space involved is too vast to be handled through experimental means alone. Leveraging accumulative protein databases, machine learning (ML) models, particularly those based on natural language processing (NLP), have considerably expedited protein engineering. Moreover, advances in topological data analysis (TDA) and artificial intelligence-based protein structure prediction, such as AlphaFold2, have made more powerful structure-based ML-assisted protein engineering strategies possible. This review aims to offer a comprehensive, systematic, and indispensable set of methodological components, including TDA and NLP, for protein engineering and to facilitate their future development.},
  archiveprefix = {arxiv},
  keywords = {Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/CUDTIPXA/Qiu and Wei - 2023 - Artificial intelligence-aided protein engineering.pdf;/Users/pmg/Zotero/storage/XLMTBI6V/2307.html}
}

@misc{qiu_quantifying_2020,
  title = {Quantifying {{Point-Prediction Uncertainty}} in {{Neural Networks}} via {{Residual Estimation}} with an {{I}}/{{O Kernel}}},
  author = {Qiu, Xin and Meyerson, Elliot and Miikkulainen, Risto},
  year = {2020},
  month = jun,
  number = {arXiv:1906.00588},
  eprint = {1906.00588},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.00588},
  urldate = {2022-10-17},
  abstract = {Neural Networks (NNs) have been extensively used for a wide spectrum of real-world regression tasks, where the goal is to predict a numerical outcome such as revenue, effectiveness, or a quantitative result. In many such tasks, the point prediction is not enough: the uncertainty (i.e. risk or confidence) of that prediction must also be estimated. Standard NNs, which are most often used in such tasks, do not provide uncertainty information. Existing approaches address this issue by combining Bayesian models with NNs, but these models are hard to implement, more expensive to train, and usually do not predict as accurately as standard NNs. In this paper, a new framework (RIO) is developed that makes it possible to estimate uncertainty in any pretrained standard NN. The behavior of the NN is captured by modeling its prediction residuals with a Gaussian Process, whose kernel includes both the NN's input and its output. The framework is evaluated in twelve real-world datasets, where it is found to (1) provide reliable estimates of uncertainty, (2) reduce the error of the point predictions, and (3) scale well to large datasets. Given that RIO can be applied to any standard NN without modifications to model architecture or training pipeline, it provides an important ingredient for building real-world NN applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/8EUSG4I6/Qiu et al. - 2020 - Quantifying Point-Prediction Uncertainty in Neural.pdf;/Users/pmg/Zotero/storage/YT7QI4B7/1906.html}
}

@article{rao_evaluating_2019,
  title={{Evaluating protein transfer learning with TAPE}},
  author={Rao, Roshan M. and Bhattacharya, Nicholas and Thomas, Neil and Duan, Yan and Chen, Peter and Canny, John and Abbeel, Pieter and Song, Yun},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{rao_msa_2021,
  title = {{{MSA Transformer}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Rao, Roshan M. and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},
  year = {2021},
  month = jul,
  pages = {8844--8856},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-07},
  abstract = {Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/7GZY52HI/Rao et al. - 2021 - MSA Transformer.pdf;/Users/pmg/Zotero/storage/U9666I93/Rao et al. - 2021 - MSA Transformer.pdf}
}

@misc{rapp_self-driving_2023,
  title = {Self-Driving Laboratories to Autonomously Navigate the Protein Fitness Landscape},
  author = {Rapp, Jacob T. and Bremer, Bennett J. and Romero, Philip A.},
  year = {2023},
  month = may,
  primaryclass = {New Results},
  pages = {2023.05.20.541582},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.05.20.541582},
  urldate = {2023-05-30},
  abstract = {Protein engineering has nearly limitless applications across chemistry, energy, and medicine, but creating new proteins with improved or novel functions remains slow, labor-intensive, and inefficient. In this work, we present the Self-driving Autonomous Machines for Protein Landscape Exploration (SAMPLE) platform for fully autonomous protein engineering. SAMPLE is driven by an intelligent agent that learns protein sequence-function relationships, designs new proteins, and sends designs to a fully automated robotic system that experimentally tests designed proteins and provides feedback to improve the agent's understanding of the system. We deployed four SAMPLE agents with the goal of engineering glycoside hydrolase enzymes with enhanced thermal tolerance. Despite showing individual differences in their search behavior, all four agents quickly converged on thermostable enzymes that were at least 12 {\textdegree}C more stable than the starting sequences. Self-driving laboratories automate and accelerate the scientific discovery process and hold great potential for the fields of protein engineering and synthetic biology.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/9P4QS9XY/Rapp et al. - 2023 - Self-driving laboratories to autonomously navigate.pdf}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/Users/pmg/Zotero/storage/ISYS5LZ3/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@misc{razin_ability_2022,
  title = {On the {{Ability}} of {{Graph Neural Networks}} to {{Model Interactions Between Vertices}}},
  author = {Razin, Noam and Verbin, Tom and Cohen, Nadav},
  year = {2022},
  month = dec,
  number = {arXiv:2211.16494},
  eprint = {2211.16494},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.16494},
  urldate = {2023-04-17},
  abstract = {Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic that we define by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Index Sparsification (WIS), which preserves the ability of a GNN to model interactions when input edges are removed. WIS is simple, computationally efficient, and markedly outperforms alternative methods in terms of induced prediction accuracy. More broadly, it showcases the potential of improving GNNs by theoretically analyzing the interactions they can model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/GP93MKZ3/Razin et al. - 2022 - On the Ability of Graph Neural Networks to Model I.pdf;/Users/pmg/Zotero/storage/RI6CX2AU/2211.html}
}

@article{reetz_importance_2013,
  title = {The {{Importance}} of {{Additive}} and {{Non-Additive Mutational Effects}} in {{Protein Engineering}}},
  author = {Reetz, Manfred T.},
  year = {2013},
  journal = {Angewandte Chemie International Edition},
  volume = {52},
  number = {10},
  pages = {2658--2666},
  issn = {1521-3773},
  doi = {10.1002/anie.201207842},
  urldate = {2023-09-15},
  abstract = {Additive or non-additive, that is the question: The increasing awareness of non-additive cooperative mutational effects in protein engineering is currently providing new theoretical and practical insights. In particular, the directed evolution of stereoselective enzymes as catalysts in organic chemistry and biotechnology profits from this intriguing development.},
  copyright = {Copyright {\textcopyright} 2013 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
  keywords = {directed evolution,enzymes,mutational additivity,mutational non-additivity,saturation mutagenesis},
  file = {/Users/pmg/Zotero/storage/G3HTJI9W/Reetz - 2013 - The Importance of Additive and Non-Additive Mutati.pdf;/Users/pmg/Zotero/storage/SAMR8HVH/anie.html}
}

@misc{reeves_zero-shot_2023,
  title = {Zero-{{Shot Transfer}} of {{Protein Sequence Likelihood Models}} to {{Thermostability Prediction}}},
  author = {Reeves, Shawn and Kalyaanamoorthy, Subha},
  year = {2023},
  month = jul,
  primaryclass = {New Results},
  pages = {2023.07.17.549396},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.07.17.549396},
  urldate = {2023-07-26},
  abstract = {Protein sequence likelihood models (PSLMs) are an emerging class of self-supervised deep learning algorithms which learn distributions over amino acid identities in structural and evolutionary contexts. Recently, PSLMs have demonstrated impressive performance in predicting the relative fitness of variant sequences without any task-specific training. In this work, we comprehensively analyze the capacity of six PSLMs to predict experimental measurements of thermostability for variants of hundreds of heterogeneous proteins. We assess performance of PSLMs relative to state-of-the-art supervised models, highlight relative strengths and weaknesses, and examine the complementarity between these models. We focus our analyses on stability engineering applications, assessing which methods and combinations of methods can most consistently identify and prioritize mutations for experimental validation. Our results indicate that structure-based PSLMs have competitive performance with the best existing supervised methods and can augment the predictions of supervised methods by integrating insights from their disparate training objectives.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/7VQT9FGM/Reeves and Kalyaanamoorthy - 2023 - Zero-Shot Transfer of Protein Sequence Likelihood .pdf}
}

@article{riesselman_deep_2018,
  title = {Deep Generative Models of Genetic Variation Capture the Effects of Mutations},
  author = {Riesselman, Adam J. and Ingraham, John B. and Marks, Debora S.},
  year = {2018},
  month = oct,
  journal = {Nature Methods},
  volume = {15},
  number = {10},
  pages = {816--822},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-018-0138-4},
  urldate = {2022-04-05},
  abstract = {The functions of proteins and RNAs are defined by the collective interactions of many residues, and yet most statistical models of biological sequences consider sites nearly independently. Recent approaches have demonstrated benefits of including interactions to capture pairwise covariation, but leave higher-order dependencies out of reach. Here we show how it is possible to capture higher-order, context-dependent constraints in biological sequences via latent variable models with nonlinear dependencies. We found that DeepSequence (https://github.com/debbiemarkslab/DeepSequence), a probabilistic model for sequence families, predicted the effects of mutations across a variety of deep mutational scanning experiments substantially better than existing methods based on the same evolutionary data. The model, learned in an unsupervised manner solely on the basis of sequence information, is grounded with biologically motivated priors, reveals the latent organization of sequence families, and can be used to explore new parts of sequence space.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Computational models,Machine learning},
  file = {/Users/pmg/Zotero/storage/PT6C9CFJ/Riesselman et al. - 2018 - Deep generative models of genetic variation captur.pdf;/Users/pmg/Zotero/storage/RMUZNCBU/s41592-018-0138-4.html}
}

@article{rives_biological_2020,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Acad Sciences}
}


@article{rocklin_global_2017,
  title = {Global Analysis of Protein Folding Using Massively Parallel Design, Synthesis, and Testing},
  author = {Rocklin, Gabriel J. and Chidyausiku, Tamuka M. and Goreshnik, Inna and Ford, Alex and Houliston, Scott and Lemak, Alexander and Carter, Lauren and Ravichandran, Rashmi and Mulligan, Vikram K. and Chevalier, Aaron and Arrowsmith, Cheryl H. and Baker, David},
  year = {2017},
  month = jul,
  journal = {Science (New York, N.Y.)},
  volume = {357},
  number = {6347},
  pages = {168--175},
  issn = {1095-9203},
  doi = {10.1126/science.aan0693},
  abstract = {Proteins fold into unique native structures stabilized by thousands of weak interactions that collectively overcome the entropic cost of folding. Although these forces are "encoded" in the thousands of known protein structures, "decoding" them is challenging because of the complexity of natural proteins that have evolved for function, not stability. We combined computational protein design, next-generation gene synthesis, and a high-throughput protease susceptibility assay to measure folding and stability for more than 15,000 de novo designed miniproteins, 1000 natural proteins, 10,000 point mutants, and 30,000 negative control sequences. This analysis identified more than 2500 stable designed proteins in four basic folds-a number sufficient to enable us to systematically examine how sequence determines folding and stability in uncharted protein space. Iteration between design and experiment increased the design success rate from 6\% to 47\%, produced stable proteins unlike those found in nature for topologies where design was initially unsuccessful, and revealed subtle contributions to stability as designs became increasingly optimized. Our approach achieves the long-standing goal of a tight feedback cycle between computation and experiment and has the potential to transform computational protein design into a data-driven science.},
  langid = {english},
  pmcid = {PMC5568797},
  pmid = {28706065},
  keywords = {DNA,DNA Mutational Analysis,Mutation,Protein Conformation,Protein Engineering,Protein Folding,Protein Stability,Proteins,Proteolysis},
  file = {/Users/pmg/Zotero/storage/TUBBAH3B/Rocklin et al. - 2017 - Global analysis of protein folding using massively.pdf}
}

@misc{rogers_primer_2020,
  title = {A {{Primer}} in {{BERTology}}: {{What}} We Know about How {{BERT}} Works},
  shorttitle = {A {{Primer}} in {{BERTology}}},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2020},
  month = nov,
  number = {arXiv:2002.12327},
  eprint = {2002.12327},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.12327},
  urldate = {2023-08-24},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/pmg/Zotero/storage/57KEI89K/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf;/Users/pmg/Zotero/storage/AICMBF5V/2002.html}
}

@misc{rombach_high-resolution_2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2023-02-06},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/pmg/Zotero/storage/LFG8B2FX/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf;/Users/pmg/Zotero/storage/M3BN7Q4K/2112.html}
}

@misc{rombach_high-resolution_2022-1,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {2112.10752},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10752},
  urldate = {2023-02-06},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/pmg/Zotero/storage/9N9HI63K/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf;/Users/pmg/Zotero/storage/W2BTN7BE/2112.html}
}

@inproceedings{rombach_high-resolution_2022-2,
  title = {High-{{Resolution Image Synthesis With Latent Diffusion Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  pages = {10684--10695},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/HS3IZDWT/Rombach et al. - 2022 - High-Resolution Image Synthesis With Latent Diffus.pdf}
}

@article{romero_navigating_2013,
  title = {Navigating the Protein Fitness Landscape with {{Gaussian}} Processes},
  author = {Romero, Philip A. and Krause, Andreas and Arnold, Frances H.},
  year = {2013},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {3},
  pages = {E193-E201},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1215251110},
  urldate = {2024-02-01},
  abstract = {Knowing how protein sequence maps to function (the ``fitness landscape'') is critical for understanding protein evolution as well as for engineering proteins with new and useful properties. We demonstrate that the protein fitness landscape can be inferred from experimental data, using Gaussian processes, a Bayesian learning technique. Gaussian process landscapes can model various protein sequence properties, including functional status, thermostability, enzyme activity, and ligand binding affinity. Trained on experimental data, these models achieve unrivaled quantitative accuracy. Furthermore, the explicit representation of model uncertainty allows for efficient searches through the vast space of possible sequences. We develop and test two protein sequence design algorithms motivated by Bayesian decision theory. The first one identifies small sets of sequences that are informative about the landscape; the second one identifies optimized sequences by iteratively improving the Gaussian process model in regions of the landscape that are predicted to be optimized. We demonstrate the ability of Gaussian processes to guide the search through protein sequence space by designing, constructing, and testing chimeric cytochrome P450s. These algorithms allowed us to engineer active P450 enzymes that are more thermostable than any previously made by chimeragenesis, rational design, or directed evolution.},
  file = {/Users/pmg/Zotero/storage/I9UUTJVN/Romero et al. - 2013 - Navigating the protein fitness landscape with Gaus.pdf}
}

@misc{romero-romero_exploring_2023,
  title = {Exploring the {{Protein Sequence Space}} with {{Global Generative Models}}},
  author = {{Romero-Romero}, Sergio and Lindner, Sebastian and Ferruz, Noelia},
  year = {2023},
  month = may,
  number = {arXiv:2305.01941},
  eprint = {2305.01941},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.01941},
  urldate = {2023-05-08},
  abstract = {Recent advancements in specialized large-scale architectures for training image and language have profoundly impacted the field of computer vision and natural language processing (NLP). Language models, such as the recent ChatGPT and GPT4 have demonstrated exceptional capabilities in processing, translating, and generating human languages. These breakthroughs have also been reflected in protein research, leading to the rapid development of numerous new methods in a short time, with unprecedented performance. Language models, in particular, have seen widespread use in protein research, as they have been utilized to embed proteins, generate novel ones, and predict tertiary structures. In this book chapter, we provide an overview of the use of protein generative models, reviewing 1) language models for the design of novel artificial proteins, 2) works that use non-Transformer architectures, and 3) applications in directed evolution approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/63ECEEDV/Romero-Romero et al. - 2023 - Exploring the Protein Sequence Space with Global G.pdf;/Users/pmg/Zotero/storage/7CIVVQUX/2305.html}
}

@misc{romero-romero_exploring_2023-1,
  title = {Exploring the {{Protein Sequence Space}} with {{Global Generative Models}}},
  author = {{Romero-Romero}, Sergio and Lindner, Sebastian and Ferruz, Noelia},
  year = {2023},
  month = may,
  number = {arXiv:2305.01941},
  eprint = {2305.01941},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.01941},
  urldate = {2023-05-26},
  abstract = {Recent advancements in specialized large-scale architectures for training image and language have profoundly impacted the field of computer vision and natural language processing (NLP). Language models, such as the recent ChatGPT and GPT4 have demonstrated exceptional capabilities in processing, translating, and generating human languages. These breakthroughs have also been reflected in protein research, leading to the rapid development of numerous new methods in a short time, with unprecedented performance. Language models, in particular, have seen widespread use in protein research, as they have been utilized to embed proteins, generate novel ones, and predict tertiary structures. In this book chapter, we provide an overview of the use of protein generative models, reviewing 1) language models for the design of novel artificial proteins, 2) works that use non-Transformer architectures, and 3) applications in directed evolution approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/LH6IZGBS/Romero-Romero et al. - 2023 - Exploring the Protein Sequence Space with Global G.pdf;/Users/pmg/Zotero/storage/ZR5KX5SY/2305.html}
}

@inproceedings{rong_dropedge_2020,
  title = {{{DropEdge}}: {{Towards Deep Graph Convolutional Networks}} on {{Node Classification}}},
  shorttitle = {{{DropEdge}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Rong, Yu and Huang, Wenbing and Xu, Tingyang and Huang, Junzhou},
  year = {2020},
  month = mar,
  urldate = {2022-09-01},
  abstract = {This paper proposes DropEdge, a novel and flexible technique to alleviate over-smoothing and overfitting issue in deep Graph Convolutional Networks.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/R9BEBCJJ/Rong et al. - 2020 - DropEdge Towards Deep Graph Convolutional Network.pdf}
}

@misc{rossi_unreasonable_2022,
  title = {On the {{Unreasonable Effectiveness}} of {{Feature}} Propagation in {{Learning}} on {{Graphs}} with {{Missing Node Features}}},
  author = {Rossi, Emanuele and Kenlay, Henry and Gorinova, Maria I. and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael},
  year = {2022},
  month = may,
  number = {arXiv:2111.12128},
  eprint = {2111.12128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.12128},
  urldate = {2022-11-25},
  abstract = {While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational data, they impose a strong assumption on the availability of the node or edge features of the graph. In many real-world applications, however, features are only partially available; for example, in social networks, age and gender are available only for a small subset of users. We present a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which we call Feature Propagation. We experimentally show that the proposed approach outperforms previous methods on seven common node-classification benchmarks and can withstand surprisingly high rates of missing features: on average we observe only around 4\% relative accuracy drop when 99\% of the features are missing. Moreover, it takes only 10 seconds to run on a graph with \${\textbackslash}sim\$2.5M nodes and \${\textbackslash}sim\$123M edges on a single GPU.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{rossi_unreasonable_2022-1,
  title = {On the {{Unreasonable Effectiveness}} of {{Feature}} Propagation in {{Learning}} on {{Graphs}} with {{Missing Node Features}}},
  author = {Rossi, Emanuele and Kenlay, Henry and Gorinova, Maria I. and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael},
  year = {2022},
  month = may,
  number = {arXiv:2111.12128},
  eprint = {2111.12128},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.12128},
  urldate = {2022-11-25},
  abstract = {While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational data, they impose a strong assumption on the availability of the node or edge features of the graph. In many real-world applications, however, features are only partially available; for example, in social networks, age and gender are available only for a small subset of users. We present a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which we call Feature Propagation. We experimentally show that the proposed approach outperforms previous methods on seven common node-classification benchmarks and can withstand surprisingly high rates of missing features: on average we observe only around 4\% relative accuracy drop when 99\% of the features are missing. Moreover, it takes only 10 seconds to run on a graph with \${\textbackslash}sim\$2.5M nodes and \${\textbackslash}sim\$123M edges on a single GPU.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}

@misc{rozen_moser_2021,
  title = {Moser {{Flow}}: {{Divergence-based Generative Modeling}} on {{Manifolds}}},
  shorttitle = {Moser {{Flow}}},
  author = {Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
  year = {2021},
  month = nov,
  number = {arXiv:2108.08052},
  eprint = {2108.08052},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.08052},
  urldate = {2022-08-26},
  abstract = {We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/YLGFXYVC/Rozen et al. - 2021 - Moser Flow Divergence-based Generative Modeling o.pdf;/Users/pmg/Zotero/storage/MC2IYM66/2108.html}
}

@article{rudnicki_amino_2014,
  title = {Amino {{Acid Properties Conserved}} in {{Molecular Evolution}}},
  author = {Rudnicki, Witold and Mroczek, Teresa and Cudek, Pawe{\l}},
  year = {2014},
  month = jun,
  journal = {PloS one},
  volume = {9},
  pages = {e98983},
  doi = {10.1371/journal.pone.0098983},
  abstract = {That amino acid properties are responsible for the way protein molecules evolve is natural and is also reasonably well supported both by the structure of the genetic code and, to a large extent, by the experimental measures of the amino acid similarity. Nevertheless, there remains a significant gap between observed similarity matrices and their reconstructions from amino acid properties. Therefore, we introduce a simple theoretical model of amino acid similarity matrices, which allows splitting the matrix into two parts - one that depends only on mutabilities of amino acids and another that depends on pairwise similarities between them. Then the new synthetic amino acid properties are derived from the pairwise similarities and used to reconstruct similarity matrices covering a wide range of information entropies. Our model allows us to explain up to 94\% of the variability in the BLOSUM family of the amino acids similarity matrices in terms of amino acid properties. The new properties derived from amino acid similarity matrices correlate highly with properties known to be important for molecular evolution such as hydrophobicity, size, shape and charge of amino acids. This result closes the gap in our understanding of the influence of amino acids on evolution at the molecular level. The methods were applied to the single family of similarity matrices used often in general sequence homology searches, but it is general and can be used also for more specific matrices. The new synthetic properties can be used in analyzes of protein sequences in various biological applications.},
  file = {/Users/pmg/Zotero/storage/GZHFR2U3/Rudnicki et al. - 2014 - Amino Acid Properties Conserved in Molecular Evolu.pdf}
}

@article{russ_evolution-based_2020,
  title = {An Evolution-Based Model for Designing Chorismate Mutase Enzymes},
  author = {Russ, William P. and Figliuzzi, Matteo and Stocker, Christian and {Barrat-Charlaix}, Pierre and Socolich, Michael and Kast, Peter and Hilvert, Donald and Monasson, Remi and Cocco, Simona and Weigt, Martin and Ranganathan, Rama},
  year = {2020},
  month = jul,
  journal = {Science},
  volume = {369},
  number = {6502},
  pages = {440--445},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aba3304},
  urldate = {2022-04-05},
  file = {/Users/pmg/Zotero/storage/Y8DWD2U4/Russ et al. - 2020 - An evolution-based model for designing chorismate .pdf}
}

@inproceedings{sanchez_standards_2022,
  title = {Standards, Tooling and Benchmarks to Probe Representation Learning on Proteins},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Learning Meaningful Representations}} of {{Life}}},
  author = {Sanchez, Joaquin Gomez and Franz, Sebastian and Heinzinger, Michael and Rost, Burkhard and Dallago, Christian},
  year = {2022},
  month = nov,
  urldate = {2022-12-06},
  abstract = {With the advent of novel foundational approaches to represent proteins, a race to evaluate and assess their effectiveness to embed biological data for a variety of downstream tasks, from structure prediction to protein engineering, has gained tremendous traction. While tasks like protein 3D structure prediction from sequence have well characterized datasets and methodological approaches, many others, for instance probing the ability to encode protein function from sequence, lack standardization. This becomes particularly relevant when employing experimental biological datasets for machine learning, as curating biologically meaningful data splits requires biological intuition, whilst engineering appropriate machine learning models requires data science expertise. Gold standard experimental datasets annotated with machine learning relevant metadata are thus scarce and often scattered in different file formats in the literature, using a variety of metrics to measure success, hindering rapid evaluation of new foundational representation techniques or machine learning models built on top of them. To address these challenges, we propose a suite of solutions including a) standards for sequence datasets and embedding interfaces, b) curated and machine learning metadata annotated protein sequence datasets, c) machine learning architectures and training scripts, and d) an extensible, automatic evaluation pipeline connecting all these components. In practice, we described new, broad data standards for machine learning protein sequence datasets, including definitions for predictions of a categorical attribute for a residue in a sequence (e.g., secondary structure), or predicting a single value for the entire sequence (e.g., protein fitness). We expanded a previous collection of datasets for protein engineering (FLIP) by adding five traditional tasks from the literature, like residue secondary structure, residue conservation, and protein subcellular location prediction. We created a novel software solution (biotrainer) that collects machine learning architectures used for protein predictions and exposes a reproducible training pipeline that can consume any dataset adhering to the newly proposed data standards. Lastly, we connected all components in a new software solution (autoeval), which collects definitions for embedding methods, datasets and downstream machine learning models to automatically evaluate them. With these solutions, biological experimentalists can contribute new datasets and even train standard models using popular embedding methods, while machine learning researchers can easily plug in new foundational models or architectures in a common interface and test them on a variety of tasks against other solutions. In turn, the combination of solutions presented here unlocks the ability of interest groups to create challenges around new biological datasets, new machine learning architectures, new foundational models, or a combination thereof.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/PLPD2MGR/Sanchez et al. - 2022 - Standards, tooling and benchmarks to probe represe.pdf;/Users/pmg/Zotero/storage/PY8XDMPF/forum.html}
}

@article{sarkisyan_local_2016,
  title = {Local Fitness Landscape of the Green Fluorescent Protein},
  author = {Sarkisyan, Karen S. and Bolotin, Dmitry A. and Meer, Margarita V. and Usmanova, Dinara R. and Mishin, Alexander S. and Sharonov, George V. and Ivankov, Dmitry N. and Bozhanova, Nina G. and Baranov, Mikhail S. and Soylemez, Onuralp and Bogatyreva, Natalya S. and Vlasov, Peter K. and Egorov, Evgeny S. and Logacheva, Maria D. and Kondrashov, Alexey S. and Chudakov, Dmitry M. and Putintseva, Ekaterina V. and Mamedov, Ilgar Z. and Tawfik, Dan S. and Lukyanov, Konstantin A. and Kondrashov, Fyodor A.},
  year = {2016},
  month = may,
  journal = {Nature},
  volume = {533},
  number = {7603},
  pages = {397--401},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature17995},
  urldate = {2022-11-11},
  abstract = {Comprehensive genotype{\textendash}phenotype mapping of the green fluorescent protein shows that the local fitness peak is narrow, shaped by a high prevalence of epistatic interactions, providing for the loss of fluorescence when the joint effect of mutations exceeds a threshold.},
  copyright = {2015 Macmillan Publishers Limited. All rights reserved},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Evolution},
  file = {/Users/pmg/Zotero/storage/UJY3UBHZ/Sarkisyan et al. - 2016 - Local fitness landscape of the green fluorescent p.pdf;/Users/pmg/Zotero/storage/CFLEAP2P/nature17995.html}
}

@article{satorras_en_2022,
  title = {E(n) {{Equivariant Graph Neural Networks}}},
  author = {Satorras, Victor Garcia and Hoogeboom, Emiel and Welling, Max},
  year = {2022},
  month = feb,
  journal = {arXiv:2102.09844 [cs, stat]},
  eprint = {2102.09844},
  primaryclass = {cs, stat},
  urldate = {2022-04-07},
  abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/HPBAEAHH/Satorras et al. - 2022 - E(n) Equivariant Graph Neural Networks.pdf;/Users/pmg/Zotero/storage/ASYJJ8BB/2102.html}
}

@article{scalia_evaluating_2020,
  title = {Evaluating {{Scalable Uncertainty Estimation Methods}} for {{Deep Learning-Based Molecular Property Prediction}}},
  author = {Scalia, Gabriele and Grambow, Colin A. and Pernici, Barbara and Li, Yi-Pei and Green, William H.},
  year = {2020},
  journal = {ACS},
  publisher = {{American Chemical Society (ACS)}},
  urldate = {2024-01-17},
  abstract = {{\textcopyright} 2020 American Chemical Society. Advances in deep neural network (DNN)-based molecular property prediction have recently led to the development of models of remarkable accuracy and generalization ability, with graph convolutional neural networks (GCNNs) reporting state-of-the-art performance for this task. However, some challenges remain, and one of the most important that needs to be fully addressed concerns uncertainty quantification. DNN performance is affected by the volume and the quality of the training samples. Therefore, establishing when and to what extent a prediction can be considered reliable is just as important as outputting accurate predictions, especially when out-of-domain molecules are targeted. Recently, several methods to account for uncertainty in DNNs have been proposed, most of which are based on approximate Bayesian inference. Among these, only a few scale to the large data sets required in applications. Evaluating and comparing these methods has recently attracted great interest, but results are generally fragmented and absent for molecular property prediction. In this paper, we quantitatively compare scalable techniques for uncertainty estimation in GCNNs. We introduce a set of quantitative criteria to capture different uncertainty aspects and then use these criteria to compare MC-dropout, Deep Ensembles, and bootstrapping, both theoretically in a unified framework that separates aleatoric/epistemic uncertainty and experimentally on public data sets. Our experiments quantify the performance of the different uncertainty estimation methods and their impact on uncertainty-related error reduction. Our findings indicate that Deep Ensembles and bootstrapping consistently outperform MC-dropout, with different context-specific pros and cons. Our analysis leads to a better understanding of the role of aleatoric/epistemic uncertainty, also in relation to the target data set features, and highlights the challenge posed by out-of-domain uncertainty.},
  copyright = {Creative Commons Attribution-Noncommercial-Share Alike},
  langid = {english},
  annotation = {Accepted: 2021-10-27T20:05:54Z},
  file = {/Users/pmg/Zotero/storage/5KQLKCCT/Scalia et al. - 2020 - Evaluating Scalable Uncertainty Estimation Methods.pdf}
}

@article{schneuing_structure-based_nodate,
  title = {Structure-Based {{Drug Design}} with {{Equivariant Diffusion Models}}},
  author = {Schneuing, Arne and Du, Yuanqi and Harris, Charles and Jamasb, Arian and Igashov, Ilia and Du, Weitao and Blundell, Tom and Li{\`o}, Pietro and Gomes, Carla and Welling, Max and Bronstein, Michael and Correia, Bruno},
  pages = {19},
  abstract = {Structure-based drug design (SBDD) aims to design small-molecule ligands that bind with high affinity and specificity to pre-determined protein targets. Traditional SBDD pipelines start with large-scale docking of compound libraries from public databases, thus limiting the exploration of chemical space to existent previously studied regions. Recent machine learning methods approached this problem using an atom-by-atom generation approach, which is computationally expensive. In this paper, we formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an E(3)-equivariant 3D-conditional diffusion model that generates novel ligands conditioned on protein pockets. Furthermore, we curate a new dataset of experimentally determined binding complex data from Binding MOAD to provide a realistic binding scenario that complements the synthetic CrossDocked dataset. Comprehensive in silico experiments demonstrate the efficiency of DiffSBDD in generating novel and diverse drug-like ligands that engage protein pockets with high binding energies as predicted by in silico docking.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/REGLGJHR/Schneuing et al. - Structure-based Drug Design with Equivariant Diffu.pdf}
}

@misc{schreiber_esmbind_2023,
  title = {{{ESMBind}} and {{QBind}}: {{LoRA}}, {{QLoRA}}, and {{ESM-2}} for {{Predicting Binding Sites}} and {{Post Translational Modification}}},
  shorttitle = {{{ESMBind}} and {{QBind}}},
  author = {Schreiber, Amelie},
  year = {2023},
  month = nov,
  primaryclass = {New Results},
  pages = {2023.11.13.566930},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.11.13.566930},
  urldate = {2023-12-14},
  abstract = {In this study we discuss the viability of applying protein language models to the problem of predicting bindings sites of protein sequences from single sequences alone using Low Rank Adaptation (LoRA) and Quantized Low Rank Adaptation (QLoRA). No Multiple Sequence Alignment (MSA) or structural information for the proteins was used. Moreover, using LoRA and QLoRA shows improved performance over vanilla full finetuning, and significantly helps in mitigating overfitting. Also, due to the efficiency of LoRA and QLoRA, we are able to train the larger ESM-2 models on modest hardware, making the method very attractive and accessible. We also note that this technique serves as an important regularization technique and serves to improve generalization of models on unseen data.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/DFHXPUI2/Schreiber - 2023 - ESMBind and QBind LoRA, QLoRA, and ESM-2 for Pred.pdf}
}

@article{schulz_tutorial_2018,
  title = {A Tutorial on {{Gaussian}} Process Regression: {{Modelling}}, Exploring, and Exploiting Functions},
  shorttitle = {A Tutorial on {{Gaussian}} Process Regression},
  author = {Schulz, Eric and Speekenbrink, Maarten and Krause, Andreas},
  year = {2018},
  month = aug,
  journal = {Journal of Mathematical Psychology},
  volume = {85},
  pages = {1--16},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2018.03.001},
  urldate = {2023-12-05},
  abstract = {This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration{\textendash}exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.},
  keywords = {Active learning,Bandit problems,Exploration{\textendash}exploitation,Gaussian process regression},
  file = {/Users/pmg/Zotero/storage/UVU3PJBM/Schulz et al. - 2018 - A tutorial on Gaussian process regression Modelli.pdf;/Users/pmg/Zotero/storage/6DWAG2WJ/S0022249617302158.html}
}

@article{schymkowitz_foldx_2005,
  title = {The {{FoldX}} Web Server: An Online Force Field},
  shorttitle = {The {{FoldX}} Web Server},
  author = {Schymkowitz, Joost and Borg, Jesper and Stricher, Francois and Nys, Robby and Rousseau, Frederic and Serrano, Luis},
  year = {2005},
  month = jul,
  journal = {Nucleic Acids Research},
  volume = {33},
  number = {suppl\_2},
  pages = {W382-W388},
  issn = {0305-1048},
  doi = {10.1093/nar/gki387},
  urldate = {2022-10-20},
  abstract = {FoldX is an empirical force field that was developed for the rapid evaluation of the effect of mutations on the stability, folding and dynamics of proteins and nucleic acids. The core functionality of FoldX, namely the calculation of the free energy of a macromolecule based on its high-resolution 3D structure, is now publicly available through a web server at http://foldx.embl.de/ . The current release allows the calculation of the stability of a protein, calculation of the positions of the protons and the prediction of water bridges, prediction of metal binding sites and the analysis of the free energy of complex formation. Alanine scanning, the systematic truncation of side chains to alanine, is also included. In addition, some reporting functions have been added, and it is now possible to print both the atomic interaction networks that constitute the protein, print the structural and energetic details of the interactions per atom or per residue, as well as generate a general quality report of the pdb structure. This core functionality will be further extended as more FoldX applications are developed.},
  file = {/Users/pmg/Zotero/storage/I3XX9HRS/Schymkowitz et al. - 2005 - The FoldX web server an online force field.pdf;/Users/pmg/Zotero/storage/AHQD39NF/2505499.html}
}

@misc{seita_can_nodate,
  title = {Can {{We Learn}} the {{Language}} of {{Proteins}}?},
  author = {Seita, Daniel},
  journal = {The Berkeley Artificial Intelligence Research Blog},
  urldate = {2023-09-01},
  abstract = {The BAIR Blog},
  howpublished = {http://bair.berkeley.edu/blog/2019/11/04/proteins/},
  file = {/Users/pmg/Zotero/storage/FKCAGJKA/proteins.html}
}

@article{shahriari_taking_2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2015.2494218},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  keywords = {Bayes methods,Big data,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,Linear programming,optimization,Optimization,response surface methodology,Statistical analysis,statistical learning},
  file = {/Users/pmg/Zotero/storage/FG66RW82/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf}
}

@inproceedings{shaw_designing_2022,
  title = {Designing {{Proteins}} Using {{Sparse Data}}},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Learning Meaningful Representations}} of {{Life}}},
  author = {Shaw, Ada and Shin, Jung-Eun and Thadani, Nicole Nisha and Amin, Alan Nawzad and Marks, Debora Susan},
  year = {2022},
  month = nov,
  urldate = {2022-12-06},
  abstract = {A major goal in biotechnology is to generate libraries of functional proteins that display useful phenotypes. Towards this goal, previous approaches have leveraged probabilistic models of evolutionary sequences to design proteins reflecting the constraints that govern natural evolution. Other approaches have incorporated labeled data from experiments reflecting a desired phenotype, either alone or alongside models of evolutionary sequences, to design proteins exhibiting a useful functional property. With the goal of minimizing experimental effort and accelerating design cycles, we seek to quantify the minimal amounts and types of evolutionary and experimental data required for designing novel sequences with useful properties, and to identify the best models for utilizing all available data. Using a published model dataset of AAV gene therapy vector designs developed to achieve a desired tissue tropism, we evaluate models using evolutionary and experimental data independently and in concert for their ability to predict capsid liver targeting. We find that particularly when using data on capsid formation for the related phenotype of liver tropism and when evaluating sequences farther away from the wild-type, natural sequence data becomes more important and a combination of both data-types outperforms other supervised and unsupervised benchmarks. We introduce a semi-supervised Bayesian approach trained on a combination of evolutionary sequences and capsid viability that can best predict AAV2 liver tropism for sequences greater than 3 mutations away from wild-type. This has beneficial implications for the design of diverse and functional AAV2 libraries, as well as the broader objective of protein design.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/YX6QGHA7/Shaw et al. - 2022 - Designing Proteins using Sparse Data.pdf;/Users/pmg/Zotero/storage/RALYTTL7/forum.html}
}

@misc{shaw_removing_2023,
  title = {Removing Bias in Sequence Models of Protein Fitness},
  author = {Shaw, Ada Y. and Spinner, Hansen B. and Gurev, Sarah and Shin, Jung-Eun and Rollins, Nathan and Marks, Debora S.},
  year = {2023},
  month = sep,
  primaryclass = {New Results},
  pages = {2023.09.28.560044},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.09.28.560044},
  urldate = {2023-10-02},
  abstract = {Unsupervised sequence models for protein fitness have emerged as powerful tools for protein design in order to engineer therapeutics and industrial enzymes, yet they are strongly biased towards potential designs that are close to their training data. This hinders their ability to generate functional sequences that are far away from natural sequences, as is often desired to design new functions. To address this problem, we introduce a de-biasing approach that enables the comparison of protein sequences across mutational depths to overcome the extant sequence similarity bias in natural sequence models. We demonstrate our method's effectiveness at improving the relative natural sequence model predictions of experimentally measured variant functions across mutational depths. Using case studies proteins with very low functional percentages further away from the wild type, we demonstrate that our method improves the recovery of top-performing variants in these sparsely functional regimes. Our method is generally applicable to any unsupervised fitness prediction model, and for any function for any protein, and can thus easily be incorporated into any computational protein design pipeline. These studies have the potential to develop more efficient and cost-effective computational methods for designing diverse functional proteins and to inform underlying experimental library design to best take advantage of machine learning capabilities.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/HMVNASYU/Shaw et al. - 2023 - Removing bias in sequence models of protein fitnes.pdf}
}

@misc{shaw_removing_2023-1,
  title = {Removing Bias in Sequence Models of Protein Fitness},
  author = {Shaw, Ada Y. and Spinner, Hansen B. and Gurev, Sarah and Shin, Jung-Eun and Rollins, Nathan and Marks, Debora S.},
  year = {2023},
  month = sep,
  primaryclass = {New Results},
  pages = {2023.09.28.560044},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.09.28.560044},
  urldate = {2023-10-05},
  abstract = {Unsupervised sequence models for protein fitness have emerged as powerful tools for protein design in order to engineer therapeutics and industrial enzymes, yet they are strongly biased towards potential designs that are close to their training data. This hinders their ability to generate functional sequences that are far away from natural sequences, as is often desired to design new functions. To address this problem, we introduce a de-biasing approach that enables the comparison of protein sequences across mutational depths to overcome the extant sequence similarity bias in natural sequence models. We demonstrate our method's effectiveness at improving the relative natural sequence model predictions of experimentally measured variant functions across mutational depths. Using case studies proteins with very low functional percentages further away from the wild type, we demonstrate that our method improves the recovery of top-performing variants in these sparsely functional regimes. Our method is generally applicable to any unsupervised fitness prediction model, and for any function for any protein, and can thus easily be incorporated into any computational protein design pipeline. These studies have the potential to develop more efficient and cost-effective computational methods for designing diverse functional proteins and to inform underlying experimental library design to best take advantage of machine learning capabilities.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/BJI7EJCA/Shaw et al. - 2023 - Removing bias in sequence models of protein fitnes.pdf}
}

@misc{shi_graphaf_2020,
  title = {{{GraphAF}}: A {{Flow-based Autoregressive Model}} for {{Molecular Graph Generation}}},
  shorttitle = {{{GraphAF}}},
  author = {Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  year = {2020},
  month = feb,
  number = {arXiv:2001.09382},
  eprint = {2001.09382},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2001.09382},
  urldate = {2022-08-24},
  abstract = {Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68\% chemically valid molecules even without chemical knowledge rules and 100\% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/HTQQD9V3/Shi et al. - 2020 - GraphAF a Flow-based Autoregressive Model for Mol.pdf;/Users/pmg/Zotero/storage/GS66HABM/2001.html}
}

@misc{shi_masked_2021,
  title = {Masked {{Label Prediction}}: {{Unified Message Passing Model}} for {{Semi-Supervised Classification}}},
  shorttitle = {Masked {{Label Prediction}}},
  author = {Shi, Yunsheng and Huang, Zhengjie and Feng, Shikun and Zhong, Hui and Wang, Wenjin and Sun, Yu},
  year = {2021},
  month = may,
  number = {arXiv:2009.03509},
  eprint = {2009.03509},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.03509},
  urldate = {2023-07-31},
  abstract = {Graph neural network (GNN) and label propagation algorithm (LPA) are both message passing algorithms, which have achieved superior performance in semi-supervised classification. GNN performs feature propagation by a neural network to make predictions, while LPA uses label propagation across graph adjacency matrix to get results. However, there is still no effective way to directly combine these two kinds of algorithms. To address this issue, we propose a novel Unified Message Passaging Model (UniMP) that can incorporate feature and label propagation at both training and inference time. First, UniMP adopts a Graph Transformer network, taking feature embedding and label embedding as input information for propagation. Second, to train the network without overfitting in self-loop input label information, UniMP introduces a masked label prediction strategy, in which some percentage of input label information are masked at random, and then predicted. UniMP conceptually unifies feature propagation and label propagation and is empirically powerful. It obtains new state-of-the-art semi-supervised classification results in Open Graph Benchmark (OGB).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/UE4Q8UJV/Shi et al. - 2021 - Masked Label Prediction Unified Message Passing M.pdf;/Users/pmg/Zotero/storage/GBZULV6A/2009.html}
}

@misc{shiau_global_2022,
  title = {Global Statistical Models of Protein Coevolution Reveal Higher-Order Sectors beyond Those Obtained from Structure Alone},
  author = {Shiau, Carina and Wang, Haobo and Lee, Young and Ovchinnikov, Sergey},
  year = {2022},
  month = may,
  primaryclass = {New Results},
  pages = {2022.05.27.493723},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.05.27.493723},
  urldate = {2023-11-06},
  abstract = {1 Abstract Recent methods have shown promise in using pairwise sequence coevolution predictions to illuminate physical interactions and functional relationships between pairs of protein residues. As a result, there has been an increased interest in identifying higher-order correlations between sequence positions in an effort to further understand how the multiple sequence alignment (MSA) encodes the conserved biological properties of a protein family. To this end, we propose a robust and generalizable spectral clustering model that can extract interconnected networks of coevolving residues {\textendash} termed ``protein sectors'' {\textendash} using pairwise sequence coevolution predicted by a global statistical model. We assess the statistical and evolutionary origins for protein sectors extracted from the MSA for 120 protein families. We show that protein sectors are extracted from a subset of densely connected components in the sequence coevolution matrix, many of which are not present in the pairwise residue contact graph that is constructed from the protein crystal structure, revealing the existence of networks of paired residues that are not necessarily in direct physical contact but are nonetheless evolutionarily coupled. We found that protein sectors form structurally connected entities in three-dimensional space, despite sector identification being independent of protein crystal structure. Interestingly, protein families with high structural similarity do not share similar protein sectors, suggesting that nuances in the sequence coevolution matrix can differentiate between the evolutionary histories of structurally-related protein families.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/QUMT2CEM/Shiau et al. - 2022 - Global statistical models of protein coevolution r.pdf}
}

@article{shin_protein_2021,
  title = {Protein Design and Variant Prediction Using Autoregressive Generative Models},
  author = {Shin, Jung-Eun and Riesselman, Adam J. and Kollasch, Aaron W. and McMahon, Conor and Simon, Elana and Sander, Chris and Manglik, Aashish and Kruse, Andrew C. and Marks, Debora S.},
  year = {2021},
  month = apr,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {2403},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-22732-w},
  urldate = {2022-05-02},
  abstract = {The ability to design functional sequences and predict effects of variation is central to protein engineering and biotherapeutics. State-of-art computational methods rely on models that leverage evolutionary information but are inadequate for important applications where multiple sequence alignments are not robust. Such applications include the prediction of variant effects of indels, disordered proteins, and the design of proteins such as antibodies due to the highly variable complementarity determining regions. We introduce a deep generative model adapted from natural language processing for prediction and design of diverse functional sequences without the need for alignments. The model performs state-of-art prediction of missense and indel effects and we successfully design and test a diverse 105-nanobody library that shows better expression than a 1000-fold larger synthetic library. Our results demonstrate the power of the alignment-free autoregressive model in generalizing to regions of sequence space traditionally considered beyond the reach of prediction and design.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Computational models,Machine learning,Protein design,Protein engineering,Protein function predictions},
  file = {/Users/pmg/Zotero/storage/JGDESMUR/Shin et al. - 2021 - Protein design and variant prediction using autore.pdf;/Users/pmg/Zotero/storage/5XZKPFEL/s41467-021-22732-w.html}
}

@misc{simonovsky_graphvae_2018,
  title = {{{GraphVAE}}: {{Towards Generation}} of {{Small Graphs Using Variational Autoencoders}}},
  shorttitle = {{{GraphVAE}}},
  author = {Simonovsky, Martin and Komodakis, Nikos},
  year = {2018},
  month = feb,
  number = {arXiv:1802.03480},
  eprint = {1802.03480},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.03480},
  urldate = {2022-08-17},
  abstract = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/pmg/Zotero/storage/ZMUD9VG6/Simonovsky and Komodakis - 2018 - GraphVAE Towards Generation of Small Graphs Using.pdf;/Users/pmg/Zotero/storage/NCDPDYKD/1802.html}
}

@article{simons_assembly_1997,
  title = {Assembly of Protein Tertiary Structures from Fragments with Similar Local Sequences Using Simulated Annealing and {{Bayesian}} Scoring Functions},
  author = {Simons, K. T. and Kooperberg, C. and Huang, E. and Baker, D.},
  year = {1997},
  month = apr,
  journal = {Journal of Molecular Biology},
  volume = {268},
  number = {1},
  pages = {209--225},
  issn = {0022-2836},
  doi = {10.1006/jmbi.1997.0959},
  abstract = {We explore the ability of a simple simulated annealing procedure to assemble native-like structures from fragments of unrelated protein structures with similar local sequences using Bayesian scoring functions. Environment and residue pair specific contributions to the scoring functions appear as the first two terms in a series expansion for the residue probability distributions in the protein database; the decoupling of the distance and environment dependencies of the distributions resolves the major problems with current database-derived scoring functions noted by Thomas and Dill. The simulated annealing procedure rapidly and frequently generates native-like structures for small helical proteins and better than random structures for small beta sheet containing proteins. Most of the simulated structures have native-like solvent accessibility and secondary structure patterns, and thus ensembles of these structures provide a particularly challenging set of decoys for evaluating scoring functions. We investigate the effects of multiple sequence information and different types of conformational constraints on the overall performance of the method, and the ability of a variety of recently developed scoring functions to recognize the native-like conformations in the ensembles of simulated structures.},
  langid = {english},
  pmid = {9149153},
  keywords = {Bayes Theorem,Computer Simulation,{Databases, Factual},{Models, Molecular},{Models, Statistical},Peptide Fragments,Protein Folding,{Protein Structure, Tertiary},Proteins,{Sequence Homology, Amino Acid}}
}

@misc{sinai_generative_2021,
  title = {Generative {{AAV}} Capsid Diversification by Latent Interpolation},
  author = {Sinai, Sam and Jain, Nina and Church, George M. and Kelsic, Eric D.},
  year = {2021},
  month = apr,
  primaryclass = {New Results},
  pages = {2021.04.16.440236},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.04.16.440236},
  urldate = {2023-11-23},
  abstract = {Adeno-associated virus (AAV) capsids have shown clinical promise as delivery vectors for gene therapy. However, the high prevalence of pre-existing immunity against natural capsids poses a challenge for widespread treatment. The generation of diverse capsids that are potentially more capable of immune evasion is challenging because introducing multiple mutations often breaks capsid assembly. Here we target a representative, immunologically relevant 28-amino-acid segment of the AAV2 capsid and show that a low-complexity Variational Auto-encoder (VAE) can interpolate in sequence space to produce diverse and novel capsids capable of packaging their own genomes. We first train the VAE on a 564-sample Multiple-Sequence Alignment (MSA) of dependo-parvoviruses, and then further augment this dataset by adding 22,704 samples from a deep mutational exploration (DME) on the target region. In both cases the VAE generated viable variants with many mutations, which we validated experimentally. We propose that this simple approach can be used to optimize and diversify other proteins, as well as other capsid traits of interest for gene delivery.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/KKVGCRD5/Sinai et al. - 2021 - Generative AAV capsid diversification by latent in.pdf}
}

@article{sinai_variational_2018,
  title = {Variational Auto-Encoding of Protein Sequences},
  author = {Sinai, Sam and Kelsic, Eric and Church, George M. and Nowak, Martin A.},
  year = {2018},
  month = jan,
  journal = {arXiv:1712.03346 [cs, q-bio]},
  eprint = {1712.03346},
  primaryclass = {cs, q-bio},
  urldate = {2022-04-25},
  abstract = {Proteins are responsible for the most diverse set of functions in biology. The ability to extract information from protein sequences and to predict the effects of mutations is extremely valuable in many domains of biology and medicine. However the mapping between protein sequence and function is complex and poorly understood. Here we present an embedding of natural protein sequences using a Variational Auto-Encoder and use it to predict how mutations affect protein function. We use this unsupervised approach to cluster natural variants and learn interactions between sets of positions within a protein. This approach generally performs better than baseline methods that consider no interactions within sequences, and in some cases better than the state-of-the-art approaches that use the inverse-Potts model. This generative model can be used to computationally guide exploration of protein sequence space and to better inform rational and automatic protein design.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/JZHIUC4Y/Sinai et al. - 2018 - Variational auto-encoding of protein sequences.pdf;/Users/pmg/Zotero/storage/JKMUCXZA/1712.html}
}

@article{singh_contrastive_2023,
  title = {Contrastive Learning in Protein Language Space Predicts Interactions between Drugs and Protein Targets},
  author = {Singh, Rohit and Sledzieski, Samuel and Bryson, Bryan and Cowen, Lenore and Berger, Bonnie},
  year = {2023},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {24},
  pages = {e2220778120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2220778120},
  urldate = {2023-06-27},
  abstract = {Sequence-based prediction of drug{\textendash}target interactions has the potential to accelerate drug discovery by complementing experimental screens. Such computational prediction needs to be generalizable and scalable while remaining sensitive to subtle variations in the inputs. However, current computational techniques fail to simultaneously meet these goals, often sacrificing performance of one to achieve the others. We develop a deep learning model, ConPLex, successfully leveraging the advances in pretrained protein language models (``PLex'') and employing a protein-anchored contrastive coembedding (``Con'') to outperform state-of-the-art approaches. ConPLex achieves high accuracy, broad adaptivity to unseen data, and specificity against decoy compounds. It makes predictions of binding based on the distance between learned representations, enabling predictions at the scale of massive compound libraries and the human proteome. Experimental testing of 19 kinase-drug interaction predictions validated 12 interactions, including four with subnanomolar affinity, plus a strongly binding EPHB1 inhibitor (KD = 1.3 nM). Furthermore, ConPLex embeddings are interpretable, which enables us to visualize the drug{\textendash}target embedding space and use embeddings to characterize the function of human cell-surface proteins. We anticipate that ConPLex will facilitate efficient drug discovery by making highly sensitive in silico drug screening feasible at the genome scale. ConPLex is available open source at https://ConPLex.csail.mit.edu.},
  file = {/Users/pmg/Zotero/storage/K9TAF8QJ/Singh et al. - 2023 - Contrastive learning in protein language space pre.pdf}
}

@misc{sledzieski_democratizing_2023,
  title = {Democratizing {{Protein Language Models}} with {{Parameter-Efficient Fine-Tuning}}},
  author = {Sledzieski, Samuel and Kshirsagar, Meghana and Baek, Minkyung and Berger, Bonnie and Dodhia, Rahul and Ferres, Juan Lavista},
  year = {2023},
  month = nov,
  primaryclass = {New Results},
  pages = {2023.11.09.566187},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.11.09.566187},
  urldate = {2023-11-16},
  abstract = {Proteomics has been revolutionized by large pre-trained protein language models, which learn unsupervised representations from large corpora of sequences. The parameters of these models are then fine-tuned in a supervised setting to tailor the model to a specific downstream task. However, as model size increases, the computational and memory footprint of fine-tuning becomes a barrier for many research groups. In the field of natural language processing, which has seen a similar explosion in the size of models, these challenges have been addressed by methods for parameter-efficient fine-tuning (PEFT). In this work, we newly bring parameter-efficient fine-tuning methods to proteomics. Using the parameter-efficient method LoRA, we train new models for two important proteomic tasks: predicting protein-protein interactions (PPI) and predicting the symmetry of homooligomers. We show that for homooligomer symmetry prediction, these approaches achieve performance competitive with traditional fine-tuning while requiring reduced memory and using three orders of magnitude fewer parameters. On the PPI prediction task, we surprisingly find that PEFT models actually outperform traditional fine-tuning while using two orders of magnitude fewer parameters. Here, we go even further to show that freezing the parameters of the language model and training only a classification head also outperforms fine-tuning, using five orders of magnitude fewer parameters, and that both of these models outperform state-of-the-art PPI prediction methods with substantially reduced compute. We also demonstrate that PEFT is robust to variations in training hyper-parameters, and elucidate where best practices for PEFT in proteomics differ from in natural language processing. Thus, we provide a blueprint to democratize the power of protein language model tuning to groups which have limited computational resources.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/NMCWPBHK/Sledzieski et al. - 2023 - Democratizing Protein Language Models with Paramet.pdf}
}

@article{smialowski_proso_2012,
  title = {{{PROSO II--a}} New Method for Protein Solubility Prediction},
  author = {Smialowski, Pawel and Doose, Gero and Torkler, Phillipp and Kaufmann, Stefanie and Frishman, Dmitrij},
  year = {2012},
  month = jun,
  journal = {The FEBS journal},
  volume = {279},
  number = {12},
  pages = {2192--2200},
  issn = {1742-4658},
  doi = {10.1111/j.1742-4658.2012.08603.x},
  abstract = {Many fields of science and industry depend on efficient production of active protein using heterologous expression in Escherichia coli. The solubility of proteins upon expression is dependent on their amino acid sequence. Prediction of solubility from sequence is therefore highly valuable. We present a novel machine-learning-based model called PROSO II which makes use of new classification methods and growth in experimental data to improve coverage and accuracy of solubility predictions. The classification algorithm is organized as a two-layered structure in which the output of a primary Parzen window model for sequence similarity and a logistic regression classifier of amino acid k-mer composition serve as input for a second-level logistic regression classifier. Compared with previously published research our model is trained on five times more data than used by any other method before (82 000 proteins). When tested on a separate holdout set not used at any point of method development our server attained the best results in comparison with other currently available methods: accuracy 75.4\%, Matthew's correlation coefficient 0.39, sensitivity 0.731, specificity 0.759, gain (soluble) 2.263. In summary, due to utilization of cutting edge machine learning technologies combined with the largest currently available experimental data set the PROSO II server constitutes a substantial improvement in protein solubility predictions. PROSO II is available at http://mips.helmholtz-muenchen.de/prosoII.},
  langid = {english},
  pmid = {22536855},
  keywords = {Artificial Intelligence,Proteins,Solubility},
  file = {/Users/pmg/Zotero/storage/6LPPLQYZ/Smialowski et al. - 2012 - PROSO II--a new method for protein solubility pred.pdf}
}

@inproceedings{smith_prediction-oriented_2023,
  title = {Prediction-{{Oriented Bayesian Active Learning}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Smith, Freddie Bickford and Kirsch, Andreas and Farquhar, Sebastian and Gal, Yarin and Foster, Adam and Rainforth, Tom},
  year = {2023},
  month = apr,
  pages = {7331--7348},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-06-21},
  abstract = {Information-theoretic approaches to active learning have traditionally focused on maximising the information gathered about the model parameters, most commonly by optimising the BALD score. We highlight that this can be suboptimal from the perspective of predictive performance. For example, BALD lacks a notion of an input distribution and so is prone to prioritise data of limited relevance. To address this we propose the expected predictive information gain (EPIG), an acquisition function that measures information gain in the space of predictions rather than parameters. We find that using EPIG leads to stronger predictive performance compared with BALD across a range of datasets and models, and thus provides an appealing drop-in replacement.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/C2WFBZBV/Smith et al. - 2023 - Prediction-Oriented Bayesian Active Learning.pdf}
}

@misc{snoek_scalable_2015,
  title = {Scalable {{Bayesian Optimization Using Deep Neural Networks}}},
  author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md Mostofa Ali and Prabhat and Adams, Ryan P.},
  year = {2015},
  month = jul,
  number = {arXiv:1502.05700},
  eprint = {1502.05700},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1502.05700},
  urldate = {2023-06-08},
  abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/KXM7MDN9/Snoek et al. - 2015 - Scalable Bayesian Optimization Using Deep Neural N.pdf;/Users/pmg/Zotero/storage/M6IDGMYA/1502.html}
}

@article{socolich_evolutionary_2005,
  title = {Evolutionary Information for Specifying a Protein Fold},
  author = {Socolich, Michael and Lockless, Steve W. and Russ, William P. and Lee, Heather and Gardner, Kevin H. and Ranganathan, Rama},
  year = {2005},
  month = sep,
  journal = {Nature},
  volume = {437},
  number = {7058},
  pages = {512--518},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature03991},
  urldate = {2022-05-30},
  abstract = {Classical studies show that for many proteins, the information required for specifying the tertiary structure is contained in the amino acid sequence. Here, we attempt to define the sequence rules for specifying a protein fold by computationally creating artificial protein sequences using only statistical information encoded in a multiple sequence alignment and no tertiary structure information. Experimental testing of libraries of artificial WW domain sequences shows that a simple statistical energy function capturing coevolution between amino acid residues is necessary and sufficient to specify sequences that fold into native structures. The artificial proteins show thermodynamic stabilities similar to natural WW domains, and structure determination of one artificial protein shows excellent agreement with the WW fold at atomic resolution. The relative simplicity of the information used for creating sequences suggests a marked reduction to the potential complexity of the protein-folding problem.},
  copyright = {2005 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/pmg/Zotero/storage/SIECTQLA/Socolich et al. - 2005 - Evolutionary information for specifying a protein .pdf;/Users/pmg/Zotero/storage/EK6VJPFY/nature03991.html}
}

@misc{sohl-dickstein_deep_2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1503.03585},
  urldate = {2022-12-16},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/PFWM57TW/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf;/Users/pmg/Zotero/storage/IPE7WJWX/1503.html}
}

@inproceedings{somnath_multi-scale_2021,
  title = {Multi-{{Scale Representation Learning}} on {{Proteins}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Somnath, Vignesh Ram and Bunne, Charlotte and Krause, Andreas},
  year = {2021},
  volume = {34},
  pages = {25244--25255},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-04-12},
  abstract = {Proteins are fundamental biological entities mediating key roles in cellular function and disease. This paper introduces a multi-scale graph construction of a protein {\textendash}HoloProt{\textendash} connecting surface to structure and sequence. The surface captures coarser details of the protein, while sequence as primary component and structure {\textendash}comprising secondary and tertiary components{\textendash} capture finer details. Our graph encoder then learns a multi-scale representation by allowing each level to integrate the encoding from level(s) below with the graph at that level. We test the learned representation on different tasks, (i.) ligand binding affinity (regression), and (ii.) protein function prediction (classification).On the regression task, contrary to previous methods, our model performs consistently and reliably across different dataset splits, outperforming all baselines on most splits. On the classification task, it achieves a performance close to the top-performing model while using 10x fewer parameters. To improve the memory efficiency of our construction, we segment the multiplex protein surface manifold into molecular superpixels and substitute the surface with these superpixels at little to no performance loss.},
  file = {/Users/pmg/Zotero/storage/RLE6ZATB/Somnath et al. - 2021 - Multi-Scale Representation Learning on Proteins.pdf}
}

@misc{somnath_multi-scale_2022,
  title = {Multi-{{Scale Representation Learning}} on {{Proteins}}},
  author = {Somnath, Vignesh Ram and Bunne, Charlotte and Krause, Andreas},
  year = {2022},
  month = apr,
  number = {arXiv:2204.02337},
  eprint = {2204.02337},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.02337},
  urldate = {2023-06-26},
  abstract = {Proteins are fundamental biological entities mediating key roles in cellular function and disease. This paper introduces a multi-scale graph construction of a protein -- HoloProt -- connecting surface to structure and sequence. The surface captures coarser details of the protein, while sequence as primary component and structure -- comprising secondary and tertiary components -- capture finer details. Our graph encoder then learns a multi-scale representation by allowing each level to integrate the encoding from level(s) below with the graph at that level. We test the learned representation on different tasks, (i.) ligand binding affinity (regression), and (ii.) protein function prediction (classification). On the regression task, contrary to previous methods, our model performs consistently and reliably across different dataset splits, outperforming all baselines on most splits. On the classification task, it achieves a performance close to the top-performing model while using 10x fewer parameters. To improve the memory efficiency of our construction, we segment the multiplex protein surface manifold into molecular superpixels and substitute the surface with these superpixels at little to no performance loss.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/2RQ4DGDV/Somnath et al. - 2022 - Multi-Scale Representation Learning on Proteins.pdf;/Users/pmg/Zotero/storage/NEE9AWFT/2204.html}
}

@misc{song_generative_2020,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  author = {Song, Yang and Ermon, Stefano},
  year = {2020},
  month = oct,
  number = {arXiv:1907.05600},
  eprint = {1907.05600},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.05600},
  urldate = {2023-03-10},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/PTPYYEV5/Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the.pdf;/Users/pmg/Zotero/storage/ZNLB7CG5/1907.html}
}

@misc{song_score-based_2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  number = {arXiv:2011.13456},
  eprint = {2011.13456},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.13456},
  urldate = {2023-02-03},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/LHVAXA3D/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf;/Users/pmg/Zotero/storage/UAZV4SPZ/2011.html}
}

@article{soskine_mutational_2010,
  title = {Mutational Effects and the Evolution of New Protein Functions},
  author = {Soskine, Misha and Tawfik, Dan S.},
  year = {2010},
  month = aug,
  journal = {Nature Reviews Genetics},
  volume = {11},
  number = {8},
  pages = {572--582},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0064},
  doi = {10.1038/nrg2808},
  urldate = {2023-08-28},
  abstract = {The divergence of new genes and proteins occurs through mutations that modulate protein function. The effects of these mutations are pleiotropic, thus imposing trade-offs between selection pressures for the existing function and the newly evolving one and among the protein's activity, stability and dosage.Various compensatory and buffering mechanisms, such as gene duplication, upregulation of expression, stabilizing mutations and chaperone folding assistance, can alleviate these trade-offs and so facilitate functional divergence.Despite buffering effects, the fitness distribution of mutations at the protein level, and for whole organisms, is such that most of the mutations are either neutral or deleterious. This results in the rapid and irreversible non-functionalization of proteins that accumulate mutations under no selection.The distribution of fitness effects of mutations for whole organisms is comparable, and possibly even more deleterious, than that of protein mutations.Duplication underlies the divergence of new genes and proteins. Duplication is almost as frequent as point mutations and is a common mechanism for resolving the trade-off conflicts that arise owing to parallel selection pressures. These pressures may regard the existing and the new function and maintenance of the protein's structural stability.Duplication, and the emergence of new genes and proteins, may occur at different stages of the divergence process. The selection pressures that act on the gene and its duplicate may differ, giving rise to different mechanisms of divergence. These mechanisms are described under three schematic models {\textemdash} Ohno's model, the 'divergence before duplication' (DPD) model and the sub-functionalization model.In Ohno's model of divergence, duplication is a neutral event. The duplicated copy of the protein drifts under no selection until a new function becomes under selection. The downside of this model is that under no selection, non-functionalization of the drifting protein is inevitable. Its advantage is that divergence is independent of trade-offs between the new and existing functions.The DPD model is based on a 'generalist' intermediate that confers a selectable degree of both the new and existing functions. Duplication occurs after the acquisition of a new function, and occurs under positive selection to increase protein dosage and/or alleviate trade-offs that make the acquisition of new function depend on loss of the existing one.The sub-functionalization model combines elements of the DPD model and Ohno's model. Duplication is initially a neutral event, but once mutations that partially reduce protein activity or dosage appear, both copies must remain functional. Duplication therefore enables a larger genetic variability to accumulate, thereby facilitating the emergence of new functions.The DPD and sub-functionalization models are both based on mutations with adaptive potential initially accumulating as neutral. As such, they are related to the notions of hidden or apparently neutral variation and of neutral networks.},
  copyright = {2010 Springer Nature Limited},
  langid = {english},
  keywords = {Molecular evolution,Mutation},
  file = {/Users/pmg/Zotero/storage/LF3B34KI/Soskine and Tawfik - 2010 - Mutational effects and the evolution of new protei.pdf}
}

@article{srinivasan_conditional_nodate,
  title = {Conditional {{Invariances}} for {{Conformer Invariant Protein Representations}}},
  author = {Srinivasan, Balasubramaniam and Ioannidis, Vassilis N and Adeshina, Soji and Kakodkar, Mayank and Karypis, George and Ribeiro, Bruno},
  pages = {20},
  abstract = {Representation learning for proteins is an emerging area in geometric deep learning. Recent works have factored in both the relational (atomic bonds) and the geometric aspects (atomic positions) of the task, notably bringing together graph neural networks (GNNs) with neural networks for point clouds. The equivariances and invariances to geometric transformations (group actions such as rotations and translations) so far treat large molecules as rigid structures. However, in many important settings, proteins can co-exist as an ensemble of multiple stable conformations. The conformations of a protein, however, cannot be described as input-independent transformations of the protein: Two proteins may require different sets of transformations in order to describe their set of viable conformations. To address this limitation, we introduce the concept of conditional transformations (CT). CT can capture protein structure, while respecting the constraints on dihedral (torsion) angles and steric repulsions between atoms. We then introduce a Markov chain Monte Carlo framework to learn representations that are invariant to these conditional transformations. Our results show that endowing existing baseline models with these conditional transformations helps improve their performance without sacrificing computational efficiency.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/JKAVK7SQ/Srinivasan et al. - Conditional Invariances for Conformer Invariant Pr.pdf}
}

@article{stanton_accelerating_2022,
  title = {Accelerating {{Bayesian Optimization}} for {{Biological Sequence Design}} with {{Denoising Autoencoders}}},
  author = {Stanton, Samuel and Maddox, Wesley and Gruver, Nate and Maffettone, Phillip and Delaney, Emily and Greenside, Peyton and Wilson, Andrew Gordon},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.12742 [cs, q-bio, stat]},
  eprint = {2203.12742},
  primaryclass = {cs, q-bio, stat},
  urldate = {2022-04-07},
  abstract = {Bayesian optimization is a gold standard for query-efficient continuous optimization. However, its adoption for drug and antibody sequence design has been hindered by the discrete, high-dimensional nature of the decision variables. We develop a new approach (LaMBO) which jointly trains a denoising autoencoder with a discriminative multi-task Gaussian process head, enabling gradient-based optimization of multi-objective acquisition functions in the latent space of the autoencoder. These acquisition functions allow LaMBO to balance the explore-exploit trade-off over multiple design rounds, and to balance objective tradeoffs by optimizing sequences at many different points on the Pareto frontier. We evaluate LaMBO on a small-molecule task based on the ZINC dataset and introduce a new large-molecule task targeting fluorescent proteins. In our experiments, LaMBO outperforms genetic optimizers and does not require a large pretraining corpus, demonstrating that Bayesian optimization is practical and effective for biological sequence design.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/ARRJEYNB/Stanton et al. - 2022 - Accelerating Bayesian Optimization for Biological .pdf;/Users/pmg/Zotero/storage/YRFPA2GC/2203.html}
}

@misc{stanton_bayesian_2022,
  title = {Bayesian {{Optimization}} with {{Conformal Coverage Guarantees}}},
  author = {Stanton, Samuel and Maddox, Wesley and Wilson, Andrew Gordon},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12496},
  eprint = {2210.12496},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12496},
  urldate = {2022-10-25},
  abstract = {Bayesian optimization is a coherent, ubiquitous approach to decision-making under uncertainty, with applications including multi-arm bandits, active learning, and black-box optimization. Bayesian optimization selects decisions (i.e. objective function queries) with maximal expected utility with respect to the posterior distribution of a Bayesian model, which quantifies reducible, epistemic uncertainty about query outcomes. In practice, subjectively implausible outcomes can occur regularly for two reasons: 1) model misspecification and 2) covariate shift. Conformal prediction is an uncertainty quantification method with coverage guarantees even for misspecified models and a simple mechanism to correct for covariate shift. We propose conformal Bayesian optimization, which directs queries towards regions of search space where the model predictions have guaranteed validity, and investigate its behavior on a suite of black-box optimization tasks and tabular ranking tasks. In many cases we find that query coverage can be significantly improved without harming sample-efficiency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/LSRWYJAG/Stanton et al. - 2022 - Bayesian Optimization with Conformal Coverage Guar.pdf;/Users/pmg/Zotero/storage/CII4ZHGX/2210.html}
}

@misc{stark_equivariant_2022,
  title = {Equivariant {{3D-Conditional Diffusion Models}} for {{Molecular Linker Design}}},
  author = {St{\"a}rk, Hannes and Vignac, Cl{\'e}ment and Satorras, Victor Garcia and Frossard, Pascal and Welling, Max and Bronstein, Michael and Correia, Bruno and Igashov, Ilia},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05274},
  eprint = {2210.05274},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.05274},
  urldate = {2022-12-14},
  abstract = {Fragment-based drug discovery has been an effective paradigm in early-stage drug development. An open challenge in this area is designing linkers between disconnected molecular fragments of interest to obtain chemically-relevant candidate drug molecules. In this work, we propose DiffLinker, an E(3)-equivariant 3D-conditional diffusion model for molecular linker design. Given a set of disconnected fragments, our model places missing atoms in between and designs a molecule incorporating all the initial fragments. Unlike previous approaches that are only able to connect pairs of molecular fragments, our method can link an arbitrary number of fragments. Additionally, the model automatically determines the number of atoms in the linker and its attachment points to the input fragments. We demonstrate that DiffLinker outperforms other methods on the standard datasets generating more diverse and synthetically-accessible molecules. Besides, we experimentally test our method in real-world applications, showing that it can successfully generate valid linkers conditioned on target protein pockets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/LHJ9B847/Igashov et al. - 2022 - Equivariant 3D-Conditional Diffusion Models for Mo.pdf;/Users/pmg/Zotero/storage/W3V7YN45/2210.html}
}

@article{stark_light_2021,
  title = {Light Attention Predicts Protein Location from the Language of Life},
  author = {St{\"a}rk, Hannes and Dallago, Christian and Heinzinger, Michael and Rost, Burkhard},
  year = {2021},
  month = jan,
  journal = {Bioinformatics Advances},
  volume = {1},
  number = {1},
  pages = {vbab035},
  issn = {2635-0041},
  doi = {10.1093/bioadv/vbab035},
  urldate = {2022-08-09},
  abstract = {Although knowing where a protein functions in a cell is important to characterize biological processes, this information remains unavailable for most known proteins. Machine learning narrows the gap through predictions from expert-designed input features leveraging information from multiple sequence alignments (MSAs) that is resource expensive to generate. Here, we showcased using embeddings from protein language models for competitive localization prediction without MSAs. Our lightweight deep neural network architecture used a softmax weighted aggregation mechanism with linear complexity in sequence length referred to as light attention. The method significantly outperformed the state-of-the-art (SOTA) for 10 localization classes by about 8 percentage points (Q10). So far, this might be the highest improvement of just embeddings over MSAs. Our new test set highlighted the limits of standard static datasets: while inviting new models, they might not suffice to claim improvements over the SOTA.The novel models are available as a web-service at http://embed.protein.properties. Code needed to reproduce results is provided at https://github.com/HannesStark/protein-localization. Predictions for the human proteome are available at https://zenodo.org/record/5047020.Supplementary data are available at Bioinformatics Advances online.},
  file = {/Users/pmg/Zotero/storage/DNHPP9K3/Stärk et al. - 2021 - Light attention predicts protein location from the.pdf;/Users/pmg/Zotero/storage/NF6WYXNF/6432029.html}
}

@article{stein_biophysical_2019,
  title = {Biophysical and {{Mechanistic Models}} for {{Disease-Causing Protein Variants}}},
  author = {Stein, Amelie and Fowler, Douglas M. and {Hartmann-Petersen}, Rasmus and {Lindorff-Larsen}, Kresten},
  year = {2019},
  month = jul,
  journal = {Trends in Biochemical Sciences},
  volume = {44},
  number = {7},
  pages = {575--588},
  issn = {0968-0004},
  doi = {10.1016/j.tibs.2019.01.003},
  abstract = {The rapid decrease in DNA sequencing cost is revolutionizing medicine and science. In medicine, genome sequencing has revealed millions of missense variants that change protein sequences, yet we only understand the molecular and phenotypic consequences of a small fraction. Within protein science, high-throughput deep mutational scanning experiments enable us to probe thousands of variants in a single, multiplexed experiment. We review efforts that bring together these topics via experimental and computational approaches to determine the consequences of missense variants in proteins. We focus on the role of changes in protein stability as a driver for disease, and how experiments, biophysical models, and computation are providing a framework for understanding and predicting how changes in protein sequence affect cellular protein stability.},
  langid = {english},
  pmcid = {PMC6579676},
  pmid = {30712981},
  keywords = {Computational Biology,computational biophysics,deep mutational scanning,Disease,Genetic Variation,genomics,Humans,{Models, Genetic},{Mutation, Missense},protein quality control,protein stability,Protein Stability,Proteins,variant classification},
  file = {/Users/pmg/Zotero/storage/FH2IQZ89/Stein et al. - 2019 - Biophysical and Mechanistic Models for Disease-Cau.pdf}
}

@article{steinegger_mmseqs2_2017,
  title = {{{MMseqs2}} Enables Sensitive Protein Sequence Searching for the Analysis of Massive Data Sets},
  author = {Steinegger, Martin and S{\"o}ding, Johannes},
  year = {2017},
  month = nov,
  journal = {Nature Biotechnology},
  volume = {35},
  number = {11},
  pages = {1026--1028},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/nbt.3988},
  urldate = {2022-05-09},
  copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Environmental microbiology,Functional clustering,Sequence annotation,Software},
  file = {/Users/pmg/Zotero/storage/S7RR3LI8/Steinegger and Söding - 2017 - MMseqs2 enables sensitive protein sequence searchi.pdf;/Users/pmg/Zotero/storage/L4JCAADB/nbt.html}
}

@article{sternke_consensus_2019,
  title = {Consensus Sequence Design as a General Strategy to Create Hyperstable, Biologically Active Proteins},
  author = {Sternke, Matt and Tripp, Katherine W. and Barrick, Doug},
  year = {2019},
  month = jun,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {23},
  pages = {11275--11284},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1816707116},
  urldate = {2023-09-01},
  abstract = {Consensus sequence design offers a promising strategy for designing proteins of high stability while retaining biological activity since it draws upon an evolutionary history in which residues important for both stability and function are likely to be conserved. Although there have been several reports of successful consensus design of individual targets, it is unclear from these anecdotal studies how often this approach succeeds and how often it fails. Here, we attempt to assess generality by designing consensus sequences for a set of six protein families with a range of chain lengths, structures, and activities. We characterize the resulting consensus proteins for stability, structure, and biological activities in an unbiased way. We find that all six consensus proteins adopt cooperatively folded structures in solution. Strikingly, four of six of these consensus proteins show increased thermodynamic stability over naturally occurring homologs. Each consensus protein tested for function maintained at least partial biological activity. Although peptide binding affinity by a consensus-designed SH3 is rather low, Km values for consensus enzymes are similar to values from extant homologs. Although consensus enzymes are slower than extant homologs at low temperature, they are faster than some thermophilic enzymes at high temperature. An analysis of sequence properties shows consensus proteins to be enriched in charged residues, and rarified in uncharged polar residues. Sequence differences between consensus and extant homologs are predominantly located at weakly conserved surface residues, highlighting the importance of these residues in the success of the consensus strategy.},
  file = {/Users/pmg/Zotero/storage/VNRZH3W4/Sternke et al. - 2019 - Consensus sequence design as a general strategy to.pdf}
}

@article{stiffler_evolvability_2015,
  title = {Evolvability as a {{Function}} of {{Purifying Selection}} in {{TEM-1}} {$\beta$}-{{Lactamase}}},
  author = {Stiffler, Michael A. and Hekstra, Doeke R. and Ranganathan, Rama},
  year = {2015},
  month = feb,
  journal = {Cell},
  volume = {160},
  number = {5},
  pages = {882--892},
  publisher = {{Elsevier}},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2015.01.035},
  urldate = {2022-10-24},
  langid = {english},
  pmid = {25723163},
  file = {/Users/pmg/Zotero/storage/ZNIKT4BZ/Stiffler et al. - 2015 - Evolvability as a Function of Purifying Selection .pdf;/Users/pmg/Zotero/storage/ZXUZR9SH/S0092-8674(15)00078-1.html}
}

@article{stone_cross-validatory_1974,
  title = {Cross-{{Validatory Choice}} and {{Assessment}} of {{Statistical Predictions}}},
  author = {Stone, M.},
  year = {1974},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {36},
  number = {2},
  eprint = {2984809},
  eprinttype = {jstor},
  pages = {111--147},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2023-02-02},
  abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
  file = {/Users/pmg/Zotero/storage/FH7A9F4D/Stone - 1974 - Cross-Validatory Choice and Assessment of Statisti.pdf}
}

@misc{sun_accelerating_2023,
  title = {Accelerating Protein Engineering with Fitness Landscape Modeling and Reinforcement Learning},
  author = {Sun, Haoran and He, Liang and Deng, Pan and Liu, Guoqing and Liu, Haiguang and Cao, Chuan and Ju, Fusong and Wu, Lijun and Qin, Tao and Liu, Tie-Yan},
  year = {2023},
  month = nov,
  primaryclass = {New Results},
  pages = {2023.11.16.565910},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.11.16.565910},
  urldate = {2023-11-30},
  abstract = {Protein engineering is essential for a variety of applications, such as designing biologic drugs, optimizing enzymes, and developing novel functional molecules. Accurate protein fitness landscape modeling, such as predicting protein properties in sequence space, is critical for efficient protein engineering. Yet, due to the complexity of the landscape and high-dimensional sequence space, it remains as an unsolved problem. In this work, we present {\textmu}Former, a deep learning framework that combines a pre-trained protein language model with three scoring modules targeting protein features at multiple levels, to tackle this grand challenge. {\textmu}Former achieves state-of-the-art performance across diverse tasks, including predicting high-order mutants, modeling epistatic effects, handling insertion/deletion mutations, and generalizing to out-of-distribution scenarios. On the basis of prediction power, integrating {\textmu}Former with a reinforcement learning framework enables efficient exploration of the vast mutant space. We showcase that this integrated approach can design protein variants with up to 5-point mutations and potentially significant enhancement in activity for engineering tasks. The results highlight {\textmu}Former as a powerful and versatile tool for protein design, accelerating the development of innovative proteins tailored for specific applications.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/X6PXIL6V/Sun et al. - 2023 - Accelerating protein engineering with fitness land.pdf}
}

@article{swersky_multi-task_nodate,
  title = {Multi-{{Task Bayesian Optimization}}},
  author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
  pages = {9},
  abstract = {Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter settings more efficiently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method significantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyperparameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/BQZUJKMN/Swersky et al. - Multi-Task Bayesian Optimization.pdf}
}

@article{takada_structure_1981,
  title = {Structure of Polygalactosamine Produced by {{Aspergillus}} Parasiticus},
  author = {Takada, H. and Araki, Y. and Ito, E.},
  year = {1981},
  month = apr,
  journal = {Journal of Biochemistry},
  volume = {89},
  number = {4},
  pages = {1265--1274},
  issn = {0021-924X},
  abstract = {The structure of polygalactosamine purified from the culture fluid of Aspergillus parasiticus AHU 7165 was studied. Partial acid hydrolysis of this polysaccharide, in which 55 to 65\% of the monosaccharide residues were N-unsubstituted, gave a series of galactosamine oligosaccharides (dimer to hexamer) in a good yield. From the data on analyses of the polysaccharide and its oligosaccharides by gel filtration, periodate oxidation, methylation, and proton NMR measurement, the polysaccharide was characterized as a linear chain of alpha (1-4)-linked galactosamine residues. The N-unsubstituted galactosamine residues are probably distributed in a random fashion over the polysaccharide chain.},
  langid = {english},
  pmid = {7251580},
  keywords = {Acetylgalactosamine,Aspergillus,Carbohydrate Conformation,Galactans,Galactosamine,Hydrolysis,Molecular Weight,Oligosaccharides}
}

@misc{tan_general_2023,
  title = {A General {{Temperature-Guided Language}} Model to Engineer Enhanced {{Stability}} and {{Activity}} in {{Proteins}}},
  author = {Tan, Pan and Li, Mingchen and Yu, Yuanxi and Jiang, Fan and Zheng, Lirong and Wu, Banghao and Sun, Xinyu and Kang, Liqi and Song, Jie and Zhang, Liang and Xiong, Yi and Ouyang, Wanli and Hu, Zhiqiang and Fan, Guisheng and Pei, Yufeng and Hong, Liang},
  year = {2023},
  month = jul,
  number = {arXiv:2307.12682},
  eprint = {2307.12682},
  primaryclass = {q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.12682},
  urldate = {2023-07-27},
  abstract = {Designing protein mutants with high stability and activity is a critical yet challenging task in protein engineering. Here, we introduce PRIME, an innovative deep learning approach for the zero-shot prediction of both protein stability and enzymatic activity. PRIME leverages temperature-guided language modelling, providing robust and precise predictions without relying on prior experimental mutagenesis data. Tested against 33 protein datasets, PRIME demonstrated superior predictive performance and generalizability compared to current state-of-the-art models},
  archiveprefix = {arxiv},
  keywords = {Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/YNIUMR93/Tan et al. - 2023 - A general Temperature-Guided Language model to eng.pdf;/Users/pmg/Zotero/storage/BJPNIA9B/2307.html}
}

@misc{tan_multi-level_2023,
  title = {Multi-Level {{Protein Representation Learning}} for {{Blind Mutational Effect Prediction}}},
  author = {Tan, Yang and Zhou, Bingxin and Jiang, Yuanhong and Wang, Yu Guang and Hong, Liang},
  year = {2023},
  month = jun,
  number = {arXiv:2306.04899},
  eprint = {2306.04899},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.04899},
  urldate = {2023-06-09},
  abstract = {Directed evolution plays an indispensable role in protein engineering that revises existing protein sequences to attain new or enhanced functions. Accurately predicting the effects of protein variants necessitates an in-depth understanding of protein structure and function. Although large self-supervised language models have demonstrated remarkable performance in zero-shot inference using only protein sequences, these models inherently do not interpret the spatial characteristics of protein structures, which are crucial for comprehending protein folding stability and internal molecular interactions. This paper introduces a novel pre-training framework that cascades sequential and geometric analyzers for protein primary and tertiary structures. It guides mutational directions toward desired traits by simulating natural selection on wild-type proteins and evaluates the effects of variants based on their fitness to perform the function. We assess the proposed approach using a public database and two new databases for a variety of variant effect prediction tasks, which encompass a diverse set of proteins and assays from different taxa. The prediction results achieve state-of-the-art performance over other zero-shot learning methods for both single-site mutations and deep mutations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/8KRVHVZZ/Tan et al. - 2023 - Multi-level Protein Representation Learning for Bl.pdf;/Users/pmg/Zotero/storage/7NCS4HVR/2306.html}
}

@misc{teufel_graphpart_2023,
  title = {{{GraphPart}}: {{Homology}} Partitioning for Biological Sequence Analysis},
  shorttitle = {{{GraphPart}}},
  author = {Teufel, Felix and G{\'i}slason, Magn{\'u}s Halld{\'o}r and Armenteros, Jos{\'e} Juan Almagro and Johansen, Alexander Rosenberg and Winther, Ole and Nielsen, Henrik},
  year = {2023},
  month = apr,
  primaryclass = {New Results},
  pages = {2023.04.14.536886},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.04.14.536886},
  urldate = {2023-08-10},
  abstract = {When splitting biological sequence data for the development and testing of predictive models, it is necessary to avoid too closely related pairs of sequences ending up in different partitions. If this is ignored, performance estimates of prediction methods will tend to be exaggerated. Several algorithms have been proposed for homology reduction, where sequences are removed until no too closely related pairs remain. We present GraphPart, an algorithm for homology partitioning, where as many sequences as possible are kept in the dataset, but partitions are defined such that closely related sequences always end up in the same partition. Evaluation of GraphPart on Protein, DNA and RNA datasets shows that it is capable of retaining a larger number of sequences per dataset, while providing homology separation quality on par with reduction approaches.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/CUNJ5DE3/Teufel et al. - 2023 - GraphPart Homology partitioning for biological se.pdf}
}

@article{teufel_signalp_2022,
  title = {{{SignalP}} 6.0 Predicts All Five Types of Signal Peptides Using Protein Language Models},
  author = {Teufel, Felix and Almagro Armenteros, Jos{\'e} Juan and Johansen, Alexander Rosenberg and G{\'i}slason, Magn{\'u}s Halld{\'o}r and Pihl, Silas Irby and Tsirigos, Konstantinos D. and Winther, Ole and Brunak, S{\o}ren and {von Heijne}, Gunnar and Nielsen, Henrik},
  year = {2022},
  month = jul,
  journal = {Nature Biotechnology},
  volume = {40},
  number = {7},
  pages = {1023--1025},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-021-01156-3},
  urldate = {2023-03-22},
  abstract = {Signal peptides (SPs) are short amino acid sequences that control protein secretion and translocation in all living organisms. SPs can be predicted from sequence data, but existing algorithms are unable to detect all known types of SPs. We introduce SignalP 6.0, a machine learning model that detects all five SP types and is applicable to metagenomic data.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Protein sequence analyses,Protein translocation,Proteolysis,Sequence annotation},
  file = {/Users/pmg/Zotero/storage/MRALEJA9/Teufel et al. - 2022 - SignalP 6.0 predicts all five types of signal pept.pdf}
}

@article{the_uniprot_consortium_uniprot_2023,
  title = {{{UniProt}}: The {{Universal Protein Knowledgebase}} in 2023},
  shorttitle = {{{UniProt}}},
  author = {{The UniProt Consortium}},
  year = {2023},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {51},
  number = {D1},
  pages = {D523-D531},
  issn = {0305-1048},
  doi = {10.1093/nar/gkac1052},
  urldate = {2023-05-25},
  abstract = {The aim of the UniProt Knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. In this publication we describe enhancements made to our data processing pipeline and to our website to adapt to an ever-increasing information content. The number of sequences in UniProtKB has risen to over 227 million and we are working towards including a reference proteome for each taxonomic group. We continue to extract detailed annotations from the literature to update or create reviewed entries, while unreviewed entries are supplemented with annotations provided by automated systems using a variety of machine-learning techniques. In addition, the scientific community continues their contributions of publications and annotations to UniProt entries of their interest. Finally, we describe our new website (https://www.uniprot.org/), designed to enhance our users' experience and make our data easily accessible to the research community. This interface includes access to AlphaFold structures for more than 85\% of all entries as well as improved visualisations for subcellular localisation of proteins.},
  file = {/Users/pmg/Zotero/storage/5ZZBF9BM/The UniProt Consortium - 2023 - UniProt the Universal Protein Knowledgebase in 20.pdf;/Users/pmg/Zotero/storage/E6V54X42/6835362.html}
}

@article{theobald_empirical_2006,
  title = {Empirical {{Bayes}} Hierarchical Models for Regularizing Maximum Likelihood Estimation in the Matrix {{Gaussian Procrustes}} Problem},
  author = {Theobald, Douglas L. and Wuttke, Deborah S.},
  year = {2006},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  volume = {103},
  number = {49},
  pages = {18521--18527},
  issn = {0027-8424},
  doi = {10.1073/pnas.0508445103},
  urldate = {2022-11-21},
  abstract = {Procrustes analysis involves finding the optimal superposition of two or more ``forms'' via rotations, translations, and scalings. Procrustes problems arise in a wide range of scientific disciplines, especially when the geometrical shapes of objects are compared, contrasted, and analyzed. Classically, the optimal transformations are found by minimizing the sum of the squared distances between corresponding points in the forms. Despite its widespread use, the ordinary unweighted least-squares (LS) criterion can give erroneous solutions when the errors have heterogeneous variances (heteroscedasticity) or the errors are correlated, both common occurrences with real data. In contrast, maximum likelihood (ML) estimation can provide accurate and consistent statistical estimates in the presence of both heteroscedasticity and correlation. Here we provide a complete solution to the nonisotropic ML Procrustes problem assuming a matrix Gaussian distribution with factored covariances. Our analysis generalizes, simplifies, and extends results from previous discussions of the ML Procrustes problem. An iterative algorithm is presented for the simultaneous, numerical determination of the ML solutions.},
  pmcid = {PMC1664551},
  pmid = {17130458},
  file = {/Users/pmg/Zotero/storage/ZQGS2KDY/Theobald and Wuttke - 2006 - Empirical Bayes hierarchical models for regularizi.pdf}
}

@misc{thomas_tuned_2022,
  title = {Tuned {{Fitness Landscapes}} for {{Benchmarking Model-Guided Protein Design}}},
  author = {Thomas, Neil and Agarwala, Atish and Belanger, David and Song, Yun S. and Colwell, Lucy J.},
  year = {2022},
  month = oct,
  primaryclass = {New Results},
  pages = {2022.10.28.514293},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.10.28.514293},
  urldate = {2022-11-22},
  abstract = {Advancements in DNA synthesis and sequencing technologies have enabled a novel paradigm of protein design where machine learning (ML) models trained on experimental data are used to guide exploration of a protein fitness landscape. ML-guided directed evolution (MLDE) builds on the success of traditional directed evolution and unlocks strategies which make more efficient use of experimental data. Building an MLDE pipeline involves many design choices across the design-build-test-learn loop ranging from data collection strategies to modeling, each of which has a large impact on the success of designed sequences. The cost of collecting experimental data makes benchmarking every component of these pipelines on real data prohibitively difficult, necessitating the development of synthetic landscapes where MLDE strategies can be tested. In this work, we develop a framework called SLIP (``Synthetic Landscape Inference for Proteins'') for constructing biologically-motivated synthetic landscapes with tunable difficulty based on Potts models. This framework can be extended to any protein family for which there is a sequence alignment. We show that without tuning, Potts models are easy to optimize. In contrast, our tuning framework provides landscapes sufficiently challenging to benchmark MLDE pipelines. SLIP is open-source and is available at https://github.com/google-research/slip.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/J3UAQLMY/Thomas et al. - 2022 - Tuned Fitness Landscapes for Benchmarking Model-Gu.pdf;/Users/pmg/Zotero/storage/TJNI7XWN/2022.10.28.html}
}

@inproceedings{thygesen_efficient_2021,
  title = {Efficient {{Generative Modelling}} of {{Protein Structure Fragments}} Using a {{Deep Markov Model}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Thygesen, Christian B. and Steenmans, Christian Skj{\o}dt and {Al-Sibahi}, Ahmad Salim and Moreta, Lys Sanz and S{\o}rensen, Anders Bundg{\aa}rd and Hamelryck, Thomas},
  year = {2021},
  month = jul,
  pages = {10258--10267},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-11-21},
  abstract = {Fragment libraries are often used in protein structure prediction, simulation and design as a means to significantly reduce the vast conformational search space. Current state-of-the-art methods for fragment library generation do not properly account for aleatory and epistemic uncertainty, respectively due to the dynamic nature of proteins and experimental errors in protein structures. Additionally, they typically rely on information that is not generally or readily available, such as homologous sequences, related protein structures and other complementary information. To address these issues, we developed BIFROST, a novel take on the fragment library problem based on a Deep Markov Model architecture combined with directional statistics for angular degrees of freedom, implemented in the deep probabilistic programming language Pyro. BIFROST is a probabilistic, generative model of the protein backbone dihedral angles conditioned solely on the amino acid sequence. BIFROST generates fragment libraries with a quality on par with current state-of-the-art methods at a fraction of the run-time, while requiring considerably less information and allowing efficient evaluation of probabilities.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/MTNK5MCD/Thygesen et al. - 2021 - Efficient Generative Modelling of Protein Structur.pdf}
}

@article{tibshirani_valerie_nodate,
  title = {Valerie and {{Patrick Hastie}}},
  author = {Tibshirani, Sami and Friedman, Harry},
  pages = {764},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/MEL2QHVU/Tibshirani and Friedman - Valerie and Patrick Hastie.pdf}
}

@article{topping_understanding_2022,
  title = {Understanding Over-Squashing and Bottlenecks on Graphs via Curvature},
  author = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
  year = {2022},
  month = mar,
  journal = {arXiv:2111.14522 [cs, stat]},
  eprint = {2111.14522},
  primaryclass = {cs, stat},
  urldate = {2022-04-27},
  abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of \$k\$-hop neighbors grows rapidly with \$k\$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/3T3I32LJ/Topping et al. - 2022 - Understanding over-squashing and bottlenecks on gr.pdf;/Users/pmg/Zotero/storage/H382FSMV/2111.html}
}

@misc{torres_why_2020,
  title = {The Why, How, and When of Representations for Complex Systems},
  author = {Torres, Leo and Blevins, Ann S. and Bassett, Danielle S. and {Eliassi-Rad}, Tina},
  year = {2020},
  month = jun,
  number = {arXiv:2006.02870},
  eprint = {2006.02870},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.02870},
  urldate = {2023-07-04},
  abstract = {Complex systems thinking is applied to a wide variety of domains, from neuroscience to computer science and economics. The wide variety of implementations has resulted in two key challenges: the progenation of many domain-specific strategies that are seldom revisited or questioned, and the siloing of ideas within a domain due to inconsistency of complex systems language. In this work we offer basic, domain-agnostic language in order to advance towards a more cohesive vocabulary. We use this language to evaluate each step of the complex systems analysis pipeline, beginning with the system and data collected, then moving through different mathematical formalisms for encoding the observed data (i.e. graphs, simplicial complexes, and hypergraphs), and relevant computational methods for each formalism. At each step we consider different types of {\textbackslash}emph\{dependencies\}; these are properties of the system that describe how the existence of one relation among the parts of a system may influence the existence of another relation. We discuss how dependencies may arise and how they may alter interpretation of results or the entirety of the analysis pipeline. We close with two real-world examples using coauthorship data and email communications data that illustrate how the system under study, the dependencies therein, the research question, and choice of mathematical representation influence the results. We hope this work can serve as an opportunity of reflection for experienced complexity scientists, as well as an introductory resource for new researchers.},
  archiveprefix = {arxiv},
  keywords = {68R10,Computer Science - Discrete Mathematics,Computer Science - Social and Information Networks,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/UYAXHAR3/Torres et al. - 2020 - The why, how, and when of representations for comp.pdf;/Users/pmg/Zotero/storage/4SN2VSRV/2006.html}
}


@inproceedings{
townshend_atom3d_2022,
title={{{ATOM3D}}: {{Tasks On Molecules}} in {{Three Dimensions}}},
author={Raphael John Lamarre Townshend and Martin V{\"o}gele and Patricia Adriana Suriana and Alexander Derry and Alexander Powers and Yianni Laloudakis and Sidhika Balachandar and Bowen Jing and Brandon M. Anderson and Stephan Eismann and Risi Kondor and Russ Altman and Ron O. Dror},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=FkDZLpK1Ml2}
}

@misc{trabucco_design-bench_2022,
  title = {Design-{{Bench}}: {{Benchmarks}} for {{Data-Driven Offline Model-Based Optimization}}},
  shorttitle = {Design-{{Bench}}},
  author = {Trabucco, Brandon and Geng, Xinyang and Kumar, Aviral and Levine, Sergey},
  year = {2022},
  month = feb,
  number = {arXiv:2202.08450},
  eprint = {2202.08450},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2202.08450},
  urldate = {2022-05-23},
  abstract = {Black-box model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function, are ubiquitous in a wide range of domains, such as the design of proteins, DNA sequences, aircraft, and robots. Solving model-based optimization problems typically requires actively querying the unknown objective function on design proposals, which means physically building the candidate molecule, aircraft, or robot, testing it, and storing the result. This process can be expensive and time consuming, and one might instead prefer to optimize for the best design using only the data one already has. This setting -- called offline MBO -- poses substantial and different algorithmic challenges than more commonly studied online techniques. A number of recent works have demonstrated success with offline MBO for high-dimensional optimization problems using high-capacity deep neural networks. However, the lack of standardized benchmarks in this emerging field is making progress difficult to track. To address this, we present Design-Bench, a benchmark for offline MBO with a unified evaluation protocol and reference implementations of recent methods. Our benchmark includes a suite of diverse and realistic tasks derived from real-world optimization problems in biology, materials science, and robotics that present distinct challenges for offline MBO. Our benchmark and reference implementations are released at github.com/rail-berkeley/design-bench and github.com/rail-berkeley/design-baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/AB2WHE83/Trabucco et al. - 2022 - Design-Bench Benchmarks for Data-Driven Offline M.pdf;/Users/pmg/Zotero/storage/6L7A6TGV/2202.html}
}

@misc{tripp_sample-efficient_2020,
  title = {Sample-{{Efficient Optimization}} in the {{Latent Space}} of {{Deep Generative Models}} via {{Weighted Retraining}}},
  author = {Tripp, Austin and Daxberger, Erik and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2020},
  month = oct,
  number = {arXiv:2006.09191},
  eprint = {2006.09191},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.09191},
  urldate = {2022-06-23},
  abstract = {Many important problems in science and engineering, such as drug design, involve optimizing an expensive black-box objective function over a complex, high-dimensional, and structured input space. Although machine learning techniques have shown promise in solving such problems, existing approaches substantially lack sample efficiency. We introduce an improved method for efficient black-box optimization, which performs the optimization in the low-dimensional, continuous latent manifold learned by a deep generative model. In contrast to previous approaches, we actively steer the generative model to maintain a latent manifold that is highly useful for efficiently optimizing the objective. We achieve this by periodically retraining the generative model on the data points queried along the optimization trajectory, as well as weighting those data points according to their objective function value. This weighted retraining can be easily implemented on top of existing methods, and is empirically shown to significantly improve their efficiency and performance on synthetic and real-world optimization problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/U878RDX8/Tripp et al. - 2020 - Sample-Efficient Optimization in the Latent Space .pdf;/Users/pmg/Zotero/storage/A8VINRNL/2006.html}
}

@article{tsuboyama_mega-scale_2023,
  title = {Mega-Scale Experimental Analysis of Protein Folding Stability in Biology and Design},
  author = {Tsuboyama, Kotaro and Dauparas, Justas and Chen, Jonathan and Laine, Elodie and Mohseni Behbahani, Yasser and Weinstein, Jonathan J. and Mangan, Niall M. and Ovchinnikov, Sergey and Rocklin, Gabriel J.},
  year = {2023},
  month = jul,
  journal = {Nature},
  pages = {1--11},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06328-6},
  urldate = {2023-07-27},
  abstract = {Advances in DNA sequencing and machine learning are providing insights into protein sequences and structures on an enormous scale1. However, the energetics driving folding are invisible in these structures and remain largely unknown2. The hidden thermodynamics of folding can drive disease3,4, shape protein evolution5{\textendash}7 and guide protein engineering8{\textendash}10, and new approaches are needed to reveal these thermodynamics for every sequence and structure. Here we present cDNA display proteolysis, a method for measuring thermodynamic folding stability for up to 900,000 protein domains in a one-week experiment. From 1.8 million measurements in total, we curated a set of around 776,000 high-quality folding stabilities covering all single amino acid variants and selected double mutants of 331 natural and 148 de novo designed protein domains 40{\textendash}72 amino acids in length. Using this extensive dataset, we quantified (1) environmental factors influencing amino acid fitness, (2) thermodynamic couplings (including unexpected interactions) between protein sites, and (3) the global divergence between evolutionary amino acid usage and protein folding stability. We also examined how our approach could identify stability determinants in designed proteins and evaluate design methods. The cDNA display proteolysis method is fast, accurate and uniquely scalable, and promises to reveal the quantitative rules for how amino acid sequences encode folding stability.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {High-throughput screening,Protein databases,Proteins,Thermodynamics},
  file = {/Users/pmg/Zotero/storage/7DECD5SF/Tsuboyama et al. - 2023 - Mega-scale experimental analysis of protein foldin.pdf}
}

@article{tunyasuvunakool_highly_2021,
  title = {Highly Accurate Protein Structure Prediction for the Human Proteome},
  author = {Tunyasuvunakool, Kathryn and Adler, Jonas and Wu, Zachary and Green, Tim and Zielinski, Michal and {\v Z}{\'i}dek, Augustin and Bridgland, Alex and Cowie, Andrew and Meyer, Clemens and Laydon, Agata and Velankar, Sameer and Kleywegt, Gerard J. and Bateman, Alex and Evans, Richard and Pritzel, Alexander and Figurnov, Michael and Ronneberger, Olaf and Bates, Russ and Kohl, Simon A. A. and Potapenko, Anna and Ballard, Andrew J. and {Romera-Paredes}, Bernardino and Nikolov, Stanislav and Jain, Rishub and Clancy, Ellen and Reiman, David and Petersen, Stig and Senior, Andrew W. and Kavukcuoglu, Koray and Birney, Ewan and Kohli, Pushmeet and Jumper, John and Hassabis, Demis},
  year = {2021},
  month = aug,
  journal = {Nature},
  volume = {596},
  number = {7873},
  pages = {590--596},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-03828-1},
  urldate = {2022-09-23},
  abstract = {Protein structures can provide invaluable information, both for reasoning about biological processes and for enabling interventions such as structure-based drug development or targeted mutagenesis. After decades of effort, 17\% of the total residues in human protein sequences are covered by an experimentally determined structure1. Here we markedly expand the structural coverage of the proteome by applying the state-of-the-art machine learning method, AlphaFold2, at a scale that covers almost the entire human proteome (98.5\% of human proteins). The resulting dataset covers 58\% of residues with a confident prediction, of which a subset (36\% of all residues) have very high confidence. We introduce several metrics developed by building on the AlphaFold model and use them to interpret the dataset, identifying strong multi-domain predictions as well as regions that are likely to be disordered. Finally, we provide some case studies to illustrate how high-quality predictions could be used to generate biological hypotheses. We are making our predictions freely available to the community and anticipate that routine large-scale and high-accuracy structure prediction will become an important tool~that will allow new questions to be addressed from a structural perspective.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Protein structure predictions,Proteomic analysis,Structural biology},
  file = {/Users/pmg/Zotero/storage/6GNB32L4/Tunyasuvunakool et al. - 2021 - Highly accurate protein structure prediction for t.pdf;/Users/pmg/Zotero/storage/X2ELUV32/s41586-021-03828-1.html}
}

@article{urbina_dual_2022,
  title = {Dual Use of Artificial-Intelligence-Powered Drug Discovery},
  author = {Urbina, Fabio and Lentzos, Filippa and Invernizzi, C{\'e}dric and Ekins, Sean},
  year = {2022},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {189--191},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00465-9},
  urldate = {2023-10-05},
  abstract = {An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Cheminformatics,Drug safety,Ethics,Software,Toxicology},
  file = {/Users/pmg/Zotero/storage/UUQTQSJ4/Urbina et al. - 2022 - Dual use of artificial-intelligence-powered drug d.pdf}
}

@article{urbina_dual_2022-1,
  title = {Dual Use of Artificial-Intelligence-Powered Drug Discovery},
  author = {Urbina, Fabio and Lentzos, Filippa and Invernizzi, C{\'e}dric and Ekins, Sean},
  year = {2022},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {189--191},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00465-9},
  urldate = {2023-10-11},
  abstract = {An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Cheminformatics,Drug safety,Ethics,Software,Toxicology},
  file = {/Users/pmg/Zotero/storage/6VFVQXPR/Urbina et al. - 2022 - Dual use of artificial-intelligence-powered drug d.pdf}
}

@article{urbina_dual_2022-2,
  title = {Dual Use of Artificial-Intelligence-Powered Drug Discovery},
  author = {Urbina, Fabio and Lentzos, Filippa and Invernizzi, C{\'e}dric and Ekins, Sean},
  year = {2022},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {3},
  pages = {189--191},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00465-9},
  urldate = {2023-10-11},
  abstract = {An international security conference explored how artificial intelligence (AI) technologies for drug discovery could be misused for de novo design of biochemical weapons. A thought experiment evolved into a computational proof.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Cheminformatics,Drug safety,Ethics,Software,Toxicology},
  file = {/Users/pmg/Zotero/storage/2UYE8HMV/Urbina et al. - 2022 - Dual use of artificial-intelligence-powered drug d.pdf}
}

@article{usmanova_self-consistency_2018,
  title = {Self-Consistency Test Reveals Systematic Bias in Programs for Prediction Change of Stability upon Mutation},
  author = {Usmanova, Dinara R and Bogatyreva, Natalya S and Ari{\~n}o Bernad, Joan and Eremina, Aleksandra A and Gorshkova, Anastasiya A and Kanevskiy, German M and Lonishin, Lyubov R and Meister, Alexander V and Yakupova, Alisa G and Kondrashov, Fyodor A and Ivankov, Dmitry N},
  year = {2018},
  month = nov,
  journal = {Bioinformatics},
  volume = {34},
  number = {21},
  pages = {3653--3658},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bty340},
  urldate = {2022-11-10},
  abstract = {Computational prediction of the effect of mutations on protein stability is used by researchers in many fields. The utility of the prediction methods is affected by their accuracy and bias. Bias, a systematic shift of the predicted change of stability, has been noted as an issue for several methods, but has not been investigated systematically. Presence of the bias may lead to misleading results especially when exploring the effects of combination of different mutations.Here we use a protocol to measure the bias as a function of the number of introduced mutations. It is based on a self-consistency test of the reciprocity the effect of a mutation. An advantage of the used approach is that it relies solely on crystal structures without experimentally measured stability values. We applied the protocol to four popular algorithms predicting change of protein stability upon mutation, FoldX, Eris, Rosetta and I-Mutant, and found an inherent bias. For one program, FoldX, we manage to substantially reduce the bias using additional relaxation by Modeller. Authors using algorithms for predicting effects of mutations should be aware of the bias described here.All calculations were implemented by in-house PERL scripts.Supplementary data are available at Bioinformatics online.The article 10.1093/bioinformatics/bty348, published alongside this paper, also addresses the problem of biases in protein stability change predictions.},
  file = {/Users/pmg/Zotero/storage/WL7EIF7Z/Usmanova et al. - 2018 - Self-consistency test reveals systematic bias in p.pdf;/Users/pmg/Zotero/storage/HWZBKI9E/4990823.html}
}

@misc{vahdat_score-based_2021,
  title = {Score-Based {{Generative Modeling}} in {{Latent Space}}},
  author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  year = {2021},
  month = dec,
  number = {arXiv:2106.05931},
  eprint = {2106.05931},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.05931},
  urldate = {2023-02-07},
  abstract = {Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/XXB2NCTW/Vahdat et al. - 2021 - Score-based Generative Modeling in Latent Space.pdf;/Users/pmg/Zotero/storage/ECPC3DF2/2106.html}
}

@techreport{valeriani_geometry_2022,
  type = {Preprint},
  title = {The Geometry of Hidden Representations of Protein Language Models},
  author = {Valeriani, Lucrezia and Cuturello, Francesca and Ansuini, Alessio and Cazzaniga, Alberto},
  year = {2022},
  month = oct,
  institution = {{Bioinformatics}},
  doi = {10.1101/2022.10.24.513504},
  urldate = {2022-12-06},
  abstract = {Protein language models (pLMs) transform their input into a sequence of hidden representations whose geometric behavior changes across layers. Looking at fundamental geometric properties such as the intrinsic dimension and the neighbor composition of these representations, we observe that these changes highlight a pattern characterized by three distinct phases. This phenomenon emerges across many models trained on diverse datasets, thus revealing a general computational strategy learned by pLMs to reconstruct missing parts of the data. These analyses show the existence of low-dimensional maps that encode evolutionary and biological properties such as remote homology and structural information. Our geometric approach sets the foundations for future systematic attempts to understand the space of protein sequences with representation learning techniques.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/BBHPMNBW/Valeriani et al. - 2022 - The geometry of hidden representations of protein .pdf}
}

@misc{valeriani_geometry_2023,
  title = {The Geometry of Hidden Representations of Large Transformer Models},
  author = {Valeriani, Lucrezia and Doimo, Diego and Cuturello, Francesca and Laio, Alessandro and Ansuini, Alessio and Cazzaniga, Alberto},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00294},
  eprint = {2302.00294},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.00294},
  urldate = {2023-08-22},
  abstract = {Large transformers are powerful architectures for self-supervised analysis of data of various nature, ranging from protein sequences to text to images. In these models, the data representation in the hidden layers live in the same space, and the semantic structure of the dataset emerges by a sequence of functionally identical transformations between one representation and the next. We here characterize the geometric and statistical properties of these representations, focusing on the evolution of such proprieties across the layers. By analyzing geometric properties such as the intrinsic dimension (ID) and the neighbor composition we find that the representations evolve in a strikingly similar manner in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then it contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic complexity of the dataset emerges at the end of the first peak. This phenomenon can be observed across many models trained on diverse datasets. Based on these observations, we suggest using the ID profile as an unsupervised proxy to identify the layers which are more suitable for downstream learning tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/CAT5G5FK/Valeriani et al. - 2023 - The geometry of hidden representations of large tr.pdf;/Users/pmg/Zotero/storage/DNBJJTLU/2302.html}
}

@article{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  primaryclass = {cs},
  urldate = {2022-04-29},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/3DGWRYBQ/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/pmg/Zotero/storage/I45DJZ2I/1706.html}
}

@misc{velickovic_graph_2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.10903},
  urldate = {2022-09-01},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/HUCEIZJS/Veličković et al. - 2018 - Graph Attention Networks.pdf;/Users/pmg/Zotero/storage/BW35VRQP/1710.html}
}

@inproceedings{velickovic_reasoning-modulated_2022,
  title = {Reasoning-{{Modulated Representations}}},
  booktitle = {Learning on {{Graphs Conference}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Bo{\v s}njak, Matko and Kipf, Thomas and Lerchner, Alexander and Hadsell, Raia and Pascanu, Razvan and Blundell, Charles},
  year = {2022},
  month = nov,
  urldate = {2022-11-25},
  abstract = {Neural networks leverage robust internal representations in order to generalise. Learning them is difficult, and often requires a large training set that covers the data distribution densely. We study a common setting where our task is not purely opaque. Indeed, very often we may have access to information about the underlying system (e.g. that observations must obey certain laws of physics) that any "tabula rasa" neural network would need to re-learn from scratch, penalising data efficiency. We incorporate this information into a pre-trained reasoning module, and investigate its role in shaping the discovered representations in diverse self-supervised learning settings from pixels. Our approach paves the way for a new class of data-efficient representation learning.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/GI6CUY4C/forum.html}
}

@misc{verkuil_language_2022,
  title = {Language Models Generalize beyond Natural Proteins},
  author = {Verkuil, Robert and Kabeli, Ori and Du, Yilun and Wicky, Basile I. M. and Milles, Lukas F. and Dauparas, Justas and Baker, David and Ovchinnikov, Sergey and Sercu, Tom and Rives, Alexander},
  year = {2022},
  month = dec,
  primaryclass = {New Results},
  pages = {2022.12.21.521521},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.12.21.521521},
  urldate = {2023-05-09},
  abstract = {Learning the design patterns of proteins from sequences across evolution may have promise toward generative protein design. However it is unknown whether language models, trained on sequences of natural proteins, will be capable of more than memorization of existing protein families. Here we show that language models generalize beyond natural proteins to generate de novo proteins. We focus on two protein design tasks: fixed backbone design where the structure is specified, and unconstrained generation where the structure is sampled from the model. Remarkably although the models are trained only on sequences, we find that they are capable of designing structure. A total of 228 generated proteins are evaluated experimentally with high overall success rates (152/228 or 67\%) in producing a soluble and monomeric species by size exclusion chromatography. Out of 152 experimentally successful designs, 35 have no significant sequence match to known natural proteins. Of the remaining 117, sequence identity to the nearest sequence match is at median 27\%, below 20\% for 6 designs, and as low as 18\% for 3 designs. For fixed backbone design, the language model generates successful designs for each of eight experimentally evaluated artificially created fixed backbone targets. For unconstrained generation, sampled proteins cover diverse topologies and secondary structure compositions, and have high experimental success rate (71/129 or 55\%). The designs reflect deep patterns linking sequence and structure, including motifs that occur in related natural structures, and motifs that are not observed in similar structural contexts in known protein families. The results show that language models, though only trained on sequences, learn a deep grammar that enables the design of protein structure, extending beyond natural proteins.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/8SHAAKFC/Verkuil et al. - 2022 - Language models generalize beyond natural proteins.pdf}
}

@misc{vignac_digress_2023,
  title = {{{DiGress}}: {{Discrete Denoising}} Diffusion for Graph Generation},
  shorttitle = {{{DiGress}}},
  author = {Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
  year = {2023},
  month = feb,
  number = {arXiv:2209.14734},
  eprint = {2209.14734},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14734},
  urldate = {2023-04-17},
  abstract = {This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/N83TFWZR/Vignac et al. - 2023 - DiGress Discrete Denoising diffusion for graph ge.pdf;/Users/pmg/Zotero/storage/Q4GHXT65/2209.html}
}

@inproceedings{vinod_joint_2022,
  title = {Joint {{Protein Sequence-Structure Co-Design}} via {{Equivariant Diffusion}}},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Learning Meaningful Representations}} of {{Life}}},
  author = {Vinod, Ria and Yang, Kevin K. and Crawford, Lorin},
  year = {2022},
  month = nov,
  urldate = {2022-12-06},
  abstract = {Protein macromolecules are known to play key roles in cellular processes. Solving inverse design problems can allow us to control targeted cellular processes by designing proteins optimized for downstream tasks. However, current fixed-backbone protein design methods are limited to generating one type of secondary structure for a set of design candidates, that are learned from distributions of a single modality (either sequence or structure). To this end, we propose a diffusion-based generative modelling method that co-designs sequence and structure properties for an arbitrary distribution of proteins structures by optimizing over a function of a downstream protein task. We demonstrate preliminary results of an equivariant joint diffusion process for 2 modalities, with the goal of scaling to more modalities.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/VC5EQKC8/Vinod et al. - 2022 - Joint Protein Sequence-Structure Co-Design via Equ.pdf;/Users/pmg/Zotero/storage/XFPR4S5L/forum.html}
}

@article{wan_characterizing_2019,
  title = {{Characterizing Variants of Unknown Significance in Rhodopsin: {{A}} Functional Genomics Approach}},
  shorttitle = {Characterizing Variants of Unknown Significance in Rhodopsin},
  author = {Wan, Aliete and Place, Emily and Pierce, Eric A. and Comander, Jason},
  year = {2019},
  month = aug,
  journal = {Human Mutation},
  volume = {40},
  number = {8},
  pages = {1127--1144},
  issn = {1098-1004},
  doi = {10.1002/humu.23762},
  abstract = {Characterizing the pathogenicity of DNA sequence variants of unknown significance (VUS) is a major bottleneck in human genetics, and is increasingly important in determining which patients with inherited retinal diseases could benefit from gene therapy. A library of 210 rhodopsin (RHO) variants from literature and in-house genetic diagnostic testing were created to efficiently detect pathogenic RHO variants that fail to express on the cell surface. This study, while focused on RHO, demonstrates a streamlined, generalizable method for detecting pathogenic VUS. A relatively simple next-generation sequencing-based readout was developed so that a flow cytometry-based assay could be performed simultaneously on all variants in a pooled format, without the need for barcodes or viral transduction. The resulting dataset characterized the surface expression of every RHO library variant with a high degree of reproducibility (r2 \,=\,0.92-0.95), recategorizing 37 variants. For example, three retinitis pigmentosa pedigrees were solved by identifying VUS which showed low expression levels (p.G18D, p.G101V, and p.P180T). Results were validated across multiple assays and correlated with clinical disease severity. This study presents a parallelized, higher-throughput cell-based assay for the functional characterization of VUS in RHO, and can be applied more broadly to other inherited retinal disease genes and other disorders.},
  langid = {english},
  pmcid = {PMC7027811},
  pmid = {30977563},
  keywords = {functional genomics,Gene Expression Regulation,Gene Library,Genetic Predisposition to Disease,Genetic Variation,Genomics,HEK293 Cells,High-Throughput Nucleotide Sequencing,Humans,inherited retinal diseases,{Models, Biological},next-generation sequencing,Retinal Diseases,retinitis pigmentosa,rhodopsin,Rhodopsin,{Sequence Analysis, DNA},variant library,{variant pathogenicity, variant of unknown significance},VUS},
  file = {/Users/pmg/Zotero/storage/HBFA6EI4/Wan et al. - 2019 - Characterizing variants of unknown significance in.pdf}
}

@article{wang_bayestab_nodate,
  title = {{{BayeStab}}: {{Predicting Effects}} of {{Mutations}} on {{Protein Stability}} with {{Uncertainty Quantification}}},
  shorttitle = {{{BayeStab}}},
  author = {Wang, Shuyu and Tang, Hongzhou and Zhao, Yuliang and Zuo, Lei},
  journal = {Protein Science},
  volume = {n/a},
  number = {n/a},
  pages = {e4467},
  issn = {1469-896X},
  doi = {10.1002/pro.4467},
  urldate = {2022-10-17},
  abstract = {Predicting protein thermostability change upon mutation is crucial for understanding diseases and designing therapeutics. However, accurately estimating Gibbs free energy change of the protein remained a challenge. Some methods struggle to generalize on examples with no homology and produce uncalibrated predictions. Here we leverage advances in graph neural networks for protein feature extraction to tackle this structure-property prediction task. Our method, BayeStab, is then tested on four test datasets, including S669, S611, S350, and Myoglobin, showing high generalization and symmetry performance. Meanwhile, we apply concrete dropout enabled Bayesian neural networks to infer plausible models and estimate uncertainty. By decomposing the uncertainty into parts induced by data noise and model, we demonstrate that the probabilistic method allows insights into the inherent noise of the training datasets, which is closely relevant to the upper bound of the task. Finally, the BayeStab web server is created and can be found at: http://www.bayestab.com. The code for this work is available at: https://github.com/HongzhouTang/BayeStab. This article is protected by copyright. All rights reserved.},
  langid = {english},
  keywords = {concrete dropout,graph neural network,protein stability change,uncertainty quantification,web server},
  file = {/Users/pmg/Zotero/storage/VNERQUIT/pro.html}
}

@article{wang_learning_nodate,
  title = {Learning from Physics-Based Features Improves Protein Property Prediction},
  author = {Wang, Amy and Lu, Alex X and Amini, Ava P and Yang, Kevin K.},
  pages = {11},
  abstract = {Data-based and physics-based methods have long been considered as distinct approaches for protein property prediction. However, they share complementary strengths, such that integrating physics-based features with machine learning may improve model generalizability and accuracy. Here, we demonstrate that incorporating pre-computed energetic features in machine learning models improves performance in out-of-distribution and low training data regimes with two distinct protein engineering tasks. By training with sequence, structure, and precomputed Rosetta energy features on graph neural nets, we achieve performance comparable to masked inverse folding pretraining with the same architecture.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/5HP5V7YT/Wang et al. - Learning from physics-based features improves prot.pdf}
}

@misc{wang_lm-gvp_2021,
  title = {{{LM-GVP}}: {{A Generalizable Deep Learning Framework}} for {{Protein Property Prediction}} from {{Sequence}} and {{Structure}}},
  shorttitle = {{{LM-GVP}}},
  author = {Wang, Zichen and Combs, Steven A. and Brand, Ryan and Calvo, Miguel Romero and Xu, Panpan and Price, George and Golovach, Nataliya and Salawu, Emmanuel O. and Wise, Colby J. and Ponnapalli, Sri Priya and Clark, Peter M.},
  year = {2021},
  month = sep,
  pages = {2021.09.21.460852},
  institution = {{bioRxiv}},
  doi = {10.1101/2021.09.21.460852},
  urldate = {2022-04-07},
  abstract = {Proteins perform many essential functions in biological systems and can be successfully developed as bio-therapeutics. It is invaluable to be able to predict their properties based on a proposed sequence and structure. In this study, we developed a novel generalizable deep learning framework, LM-GVP, composed of a protein Language Model (LM) and Graph Neural Network (GNN) to leverage information from both 1D amino acid sequences and 3D structures of proteins. Our approach outperformed the state-of-the-art protein LMs on a variety of property prediction tasks including fluorescence, protease stability, and protein functions from Gene Ontology (GO). We also illustrated insights into how a GNN prediction head can guide the protein LM to better leverage structural information. We envision that our deep learning framework will be generalizable to many protein property prediction problems to greatly accelerate protein engineering and drug development.},
  chapter = {New Results},
  copyright = {{\textcopyright} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/IDL4UUL5/Wang et al. - 2021 - LM-GVP A Generalizable Deep Learning Framework fo.pdf;/Users/pmg/Zotero/storage/UU4RYQG3/2021.09.21.460852v1.html}
}

@article{wang_lm-gvp_2022,
  title = {{{LM-GVP}}: An Extensible Sequence and Structure Informed Deep Learning Framework for Protein Property Prediction},
  shorttitle = {{{LM-GVP}}},
  author = {Wang, Zichen and Combs, Steven A. and Brand, Ryan and Calvo, Miguel Romero and Xu, Panpan and Price, George and Golovach, Nataliya and Salawu, Emmanuel O. and Wise, Colby J. and Ponnapalli, Sri Priya and Clark, Peter M.},
  year = {2022},
  month = apr,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {6832},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-10775-y},
  urldate = {2022-05-12},
  abstract = {Proteins perform many essential functions in biological systems and can be successfully developed as bio-therapeutics. It is invaluable to be able to predict their properties based on a proposed sequence and structure. In this study, we developed a novel generalizable deep learning framework, LM-GVP, composed of a protein Language Model (LM) and Graph Neural Network (GNN) to leverage information from both 1D amino acid sequences and 3D structures of proteins. Our approach outperformed the state-of-the-art protein LMs on a variety of property prediction tasks including fluorescence, protease stability, and protein functions from Gene Ontology (GO). We also illustrated insights into how a GNN prediction head can inform the fine-tuning of protein LMs to better leverage structural information. We envision that our deep learning framework will be generalizable to many protein property prediction problems to greatly accelerate protein engineering and drug development.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Machine learning,Protein function predictions},
  file = {/Users/pmg/Zotero/storage/MPXFVUHP/Wang et al. - 2022 - LM-GVP an extensible sequence and structure infor.pdf;/Users/pmg/Zotero/storage/7E4ZDZTF/s41598-022-10775-y.html}
}

@misc{wang_max-value_2018,
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2018},
  month = jan,
  number = {arXiv:1703.01968},
  eprint = {1703.01968},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.01968},
  urldate = {2023-06-08},
  abstract = {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the \${\textbackslash}arg{\textbackslash}max\$ of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/TWFW33ZF/Wang and Jegelka - 2018 - Max-value Entropy Search for Efficient Bayesian Op.pdf;/Users/pmg/Zotero/storage/K6GWHGLJ/1703.html}
}

@misc{wang_max-value_2018-1,
  title = {Max-Value {{Entropy Search}} for {{Efficient Bayesian Optimization}}},
  author = {Wang, Zi and Jegelka, Stefanie},
  year = {2018},
  month = jan,
  number = {arXiv:1703.01968},
  eprint = {1703.01968},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-06-08},
  abstract = {Entropy Search (ES) and Predictive Entropy Search (PES) are popular and empirically successful Bayesian Optimization techniques. Both rely on a compelling information-theoretic motivation, and maximize the information gained about the arg max of the unknown function; yet, both are plagued by the expensive computation for estimating entropies. We propose a new criterion, Max-value Entropy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian optimization methods, and establish a regret bound. We observe that MES maintains or improves the good empirical performance of ES/PES, while tremendously lightening the computational burden. In particular, MES is much more robust to the number of samples used for computing the entropy, and hence more efficient for higher dimensional problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/AGZLN844/Wang and Jegelka - 2018 - Max-value Entropy Search for Efficient Bayesian Op.pdf}
}

@inproceedings{wang_mgae_2017,
  title = {{{MGAE}}: {{Marginalized Graph Autoencoder}} for {{Graph Clustering}}},
  shorttitle = {{{MGAE}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Wang, Chun and Pan, Shirui and Long, Guodong and Zhu, Xingquan and Jiang, Jing},
  year = {2017},
  month = nov,
  series = {{{CIKM}} '17},
  pages = {889--898},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3132847.3132967},
  urldate = {2023-04-24},
  abstract = {Graph clustering aims to discovercommunity structures in networks, the task being fundamentally challenging mainly because the topology structure and the content of the graphs are difficult to represent for clustering analysis. Recently, graph clustering has moved from traditional shallow methods to deep learning approaches, thanks to the unique feature representation learning capability of deep learning. However, existing deep approaches for graph clustering can only exploit the structure information, while ignoring the content information associated with the nodes in a graph. In this paper, we propose a novel marginalized graph autoencoder (MGAE) algorithm for graph clustering. The key innovation of MGAE is that it advances the autoencoder to the graph domain, so graph representation learning can be carried out not only in a purely unsupervised setting by leveraging structure and content information, it can also be stacked in a deep fashion to learn effective representation. From a technical viewpoint, we propose a marginalized graph convolutional network to corrupt network node content, allowing node content to interact with network features, and marginalizes the corrupted features in a graph autoencoder context to learn graph feature representations. The learned features are fed into the spectral clustering algorithm for graph clustering. Experimental results on benchmark datasets demonstrate the superior performance of MGAE, compared to numerous baselines.},
  isbn = {978-1-4503-4918-5},
  keywords = {autoencoder,graph autoencoder,graph clustering,graph convolutional network,network representation},
  file = {/Users/pmg/Zotero/storage/DTJCKRLS/Wang et al. - 2017 - MGAE Marginalized Graph Autoencoder for Graph Clu.pdf}
}

@misc{wang_pdb-struct_2023,
  title = {{{PDB-Struct}}: {{A Comprehensive Benchmark}} for {{Structure-based Protein Design}}},
  shorttitle = {{{PDB-Struct}}},
  author = {Wang, Chuanrui and Zhong, Bozitao and Zhang, Zuobai and Chaudhary, Narendra and Misra, Sanchit and Tang, Jian},
  year = {2023},
  month = nov,
  number = {arXiv:2312.00080},
  eprint = {2312.00080},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.00080},
  urldate = {2023-12-08},
  abstract = {Structure-based protein design has attracted increasing interest, with numerous methods being introduced in recent years. However, a universally accepted method for evaluation has not been established, since the wet-lab validation can be overly time-consuming for the development of new algorithms, and the \${\textbackslash}textit\{in silico\}\$ validation with recovery and perplexity metrics is efficient but may not precisely reflect true foldability. To address this gap, we introduce two novel metrics: refoldability-based metric, which leverages high-accuracy protein structure prediction models as a proxy for wet lab experiments, and stability-based metric, which assesses whether models can assign high likelihoods to experimentally stable proteins. We curate datasets from high-quality CATH protein data, high-throughput \${\textbackslash}textit\{de novo\}\$ designed proteins, and mega-scale experimental mutagenesis experiments, and in doing so, present the \${\textbackslash}textbf\{PDB-Struct\}\$ benchmark that evaluates both recent and previously uncompared protein design methods. Experimental results indicate that ByProt, ProteinMPNN, and ESM-IF perform exceptionally well on our benchmark, while ESM-Design and AF-Design fall short on the refoldability metric. We also show that while some methods exhibit high sequence recovery, they do not perform as well on our new benchmark. Our proposed benchmark paves the way for a fair and comprehensive evaluation of protein design methods in the future. Code is available at https://github.com/WANG-CR/PDB-Struct.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/K3E5JLHY/Wang et al. - 2023 - PDB-Struct A Comprehensive Benchmark for Structur.pdf;/Users/pmg/Zotero/storage/V3A7YM2T/2312.html}
}

@article{wang_scaffolding_2022,
  title = {Scaffolding Protein Functional Sites Using Deep Learning},
  author = {Wang, Jue and Lisanza, Sidney and Juergens, David and Tischer, Doug and Watson, Joseph L. and Castro, Karla M. and Ragotte, Robert and Saragovi, Amijai and Milles, Lukas F. and Baek, Minkyung and Anishchenko, Ivan and Yang, Wei and Hicks, Derrick R. and Exp{\`o}sit, Marc and Schlichthaerle, Thomas and Chun, Jung-Ho and Dauparas, Justas and Bennett, Nathaniel and Wicky, Basile I. M. and Muenks, Andrew and DiMaio, Frank and Correia, Bruno and Ovchinnikov, Sergey and Baker, David},
  year = {2022},
  month = jul,
  journal = {Science},
  volume = {377},
  number = {6604},
  pages = {387--394},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.abn2100},
  urldate = {2023-09-04},
  abstract = {The binding and catalytic functions of proteins are generally mediated by a small number of functional residues held in place by the overall protein structure. Here, we describe deep learning approaches for scaffolding such functional sites without needing to prespecify the fold or secondary structure of the scaffold. The first approach, ``constrained hallucination,'' optimizes sequences such that their predicted structures contain the desired functional site. The second approach, ``inpainting,'' starts from the functional site and fills in additional sequence and structure to create a viable protein scaffold in a single forward pass through a specifically trained RoseTTAFold network. We use these two methods to design candidate immunogens, receptor traps, metalloproteins, enzymes, and protein-binding proteins and validate the designs using a combination of in silico and experimental tests.},
  file = {/Users/pmg/Zotero/storage/9M26YSPU/Wang et al. - 2022 - Scaffolding protein functional sites using deep le.pdf}
}

@article{wang_scientific_2023,
  title = {Scientific Discovery in the Age of Artificial Intelligence},
  author = {Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and Anandkumar, Anima and Bergen, Karianne and Gomes, Carla P. and Ho, Shirley and Kohli, Pushmeet and Lasenby, Joan and Leskovec, Jure and Liu, Tie-Yan and Manrai, Arjun and Marks, Debora and Ramsundar, Bharath and Song, Le and Sun, Jimeng and Tang, Jian and Veli{\v c}kovi{\'c}, Petar and Welling, Max and Zhang, Linfeng and Coley, Connor W. and Bengio, Yoshua and Zitnik, Marinka},
  year = {2023},
  month = aug,
  journal = {Nature},
  volume = {620},
  number = {7972},
  pages = {47--60},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06221-2},
  urldate = {2023-08-25},
  abstract = {Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain insights that might not have been possible using traditional scientific methods alone. Here we examine breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency. Generative AI methods can create designs, such as small-molecule drugs and proteins, by analysing diverse data modalities, including images and sequences. We discuss how these methods can help scientists throughout the scientific process and the central issues that remain despite such advances. Both developers and users of AI toolsneed a better understanding of when such approaches need improvement, and challenges posed by poor data quality and stewardship remain. These issues cut across scientific disciplines and require developing foundational algorithmic approaches that can contribute to scientific understanding or acquire it autonomously, making them critical areas of focus for AI innovation.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Machine learning,Scientific community,Statistics},
  file = {/Users/pmg/Zotero/storage/DDLKIL95/Wang et al. - 2023 - Scientific discovery in the age of artificial inte.pdf}
}

@misc{watson_broadly_2022,
  title = {Broadly Applicable and Accurate Protein Design by Integrating Structure Prediction Networks and Diffusion Generative Models},
  author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and Bortoli, Valentin De and Mathieu, Emile and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
  year = {2022},
  month = dec,
  primaryclass = {New Results},
  pages = {2022.12.09.519842},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.12.09.519842},
  urldate = {2023-02-07},
  abstract = {There has been considerable recent progress in designing new proteins using deep learning methods1{\textendash}9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of new designs. In a manner analogous to networks which produce images from user-specified inputs, RFdiffusion enables the design of diverse, complex, functional proteins from simple molecular specifications.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/6C9HQ6PG/Watson et al. - 2022 - Broadly applicable and accurate protein design by .pdf}
}

@article{watson_novo_2023,
  title = {De Novo Design of Protein Structure and Function with {{RFdiffusion}}},
  author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
  year = {2023},
  month = jul,
  journal = {Nature},
  pages = {1--3},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06415-8},
  urldate = {2023-07-27},
  abstract = {There has been considerable recent progress in designing new proteins using deep learning methods1{\textendash}9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryo-EM structure of a designed binder in complex with Influenza hemagglutinin which is nearly identical to the design model. In a manner analogous to networks which produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Machine learning,Protein design,Proteins}
}

@article{watson_novo_2023-1,
  title = {De Novo Design of Protein Structure and Function with {{RFdiffusion}}},
  author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
  year = {2023},
  month = jul,
  journal = {Nature},
  pages = {1--3},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06415-8},
  urldate = {2023-07-27},
  abstract = {There has been considerable recent progress in designing new proteins using deep learning methods1{\textendash}9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryo-EM structure of a designed binder in complex with Influenza hemagglutinin which is nearly identical to the design model. In a manner analogous to networks which produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Machine learning,Protein design,Proteins}
}

@inproceedings{weigend_overfitting_1993,
  title = {On Overfitting and the Effective Number of Hidden Units},
  author = {Weigend, A.},
  year = {1993},
  urldate = {2023-08-23},
  abstract = {Semantic Scholar extracted view of "On overfitting and the effective number of hidden units" by A. Weigend}
}

@misc{weinstein_non-identifiability_2022,
  title = {Non-Identifiability and the {{Blessings}} of {{Misspecification}} in {{Models}} of {{Molecular Fitness}} and {{Phylogeny}}},
  author = {Weinstein, Eli N. and Amin, Alan N. and Frazer, Jonathan and Marks, Debora S.},
  year = {2022},
  month = jan,
  pages = {2022.01.29.478324},
  institution = {{bioRxiv}},
  doi = {10.1101/2022.01.29.478324},
  urldate = {2022-04-12},
  abstract = {Understanding the consequences of mutation for molecular fitness and function is a fundamental problem in biology. Recently, generative probabilistic models have emerged as a powerful tool for estimating fitness from evolutionary sequence data, with accuracy sufficient to predict both laboratory measurements of function and disease risk in humans, and to design novel functional proteins. Existing techniques rest on an assumed relationship between density estimation and fitness estimation, a relationship that we interrogate in this article. We prove that fitness is not identifiable from observational sequence data alone, placing fundamental limits on our ability to disentangle fitness landscapes from phylogenetic history. We show on real datasets that perfect density estimation in the limit of infinite data would, with high confidence, result in poor fitness estimation; current models perform accurate fitness estimation because of, not despite, misspecification. Our results challenge the conventional wisdom that bigger models trained on bigger datasets will inevitably lead to better fitness estimation, and suggest novel estimation strategies going forward.},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/W7NE2VA8/Weinstein et al. - 2022 - Non-identifiability and the Blessings of Misspecif.pdf;/Users/pmg/Zotero/storage/JSGY73K9/2022.01.29.html}
}

@inproceedings{wheelock_forecasting_2022,
  title = {Forecasting Labels under Distribution-Shift for Machine-Guided Sequence Design},
  booktitle = {{{NeurIPS}} 2022 {{Workshop}} on {{Learning Meaningful Representations}} of {{Life}}},
  author = {Wheelock, Lauren Berk and Malina, Stephen and Gerold, Jeffrey and Sinai, Sam},
  year = {2022},
  month = nov,
  urldate = {2022-12-06},
  abstract = {The ability to design and optimize biological sequences with specific functionalities would unlock enormous value in technology and healthcare. In recent years, machine learning-guided sequence design has progressed this goal significantly, though validating designed sequences in the lab or clinic takes many months and substantial labor. It is therefore valuable to assess the likelihood that a designed set contains sequences of the desired quality (which often lies outside the label distribution in our training data) before committing resources to an experiment. Forecasting, a prominent concept in many domains where feedback can be delayed (e.g. elections), has not been used or studied in the context of sequence design. Here we propose a method to guide decision-making that forecasts the performance of high-throughput libraries (e.g. containing \$10\^5\$ unique variants) based on estimates provided by models, providing a posterior for the distribution of labels in the library. We show that our method outperforms baselines that naively use model scores to estimate library performance, which are the only tool available today for this purpose.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/AZDP9Y78/Wheelock et al. - 2022 - Forecasting labels under distribution-shift for ma.pdf;/Users/pmg/Zotero/storage/T7WGEGGV/forum.html}
}

@misc{winter_unsupervised_2022,
  title = {Unsupervised {{Learning}} of {{Group Invariant}} and {{Equivariant Representations}}},
  author = {Winter, Robin and Bertolini, Marco and Le, Tuan and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  year = {2022},
  month = sep,
  number = {arXiv:2202.07559},
  eprint = {2202.07559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.07559},
  urldate = {2022-11-10},
  abstract = {Equivariant neural networks, whose hidden features transform according to representations of a group G acting on the data, exhibit training efficiency and an improved generalisation performance. In this work, we extend group invariant and equivariant representation learning to the field of unsupervised deep learning. We propose a general learning strategy based on an encoder-decoder framework in which the latent representation is separated in an invariant term and an equivariant group action component. The key idea is that the network learns to encode and decode data to and from a group-invariant representation by additionally learning to predict the appropriate group action to align input and output pose to solve the reconstruction task. We derive the necessary conditions on the equivariant encoder, and we present a construction valid for any G, both discrete and continuous. We describe explicitly our construction for rotations, translations and permutations. We test the validity and the robustness of our approach in a variety of experiments with diverse data types employing different network architectures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/FWNEWTP6/Winter et al. - 2022 - Unsupervised Learning of Group Invariant and Equiv.pdf;/Users/pmg/Zotero/storage/YB6G7C6D/2202.html}
}

@misc{winter_unsupervised_2022-1,
  title = {Unsupervised {{Learning}} of {{Group Invariant}} and {{Equivariant Representations}}},
  author = {Winter, Robin and Bertolini, Marco and Le, Tuan and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  year = {2022},
  month = sep,
  number = {arXiv:2202.07559},
  eprint = {2202.07559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.07559},
  urldate = {2023-04-17},
  abstract = {Equivariant neural networks, whose hidden features transform according to representations of a group G acting on the data, exhibit training efficiency and an improved generalisation performance. In this work, we extend group invariant and equivariant representation learning to the field of unsupervised deep learning. We propose a general learning strategy based on an encoder-decoder framework in which the latent representation is separated in an invariant term and an equivariant group action component. The key idea is that the network learns to encode and decode data to and from a group-invariant representation by additionally learning to predict the appropriate group action to align input and output pose to solve the reconstruction task. We derive the necessary conditions on the equivariant encoder, and we present a construction valid for any G, both discrete and continuous. We describe explicitly our construction for rotations, translations and permutations. We test the validity and the robustness of our approach in a variety of experiments with diverse data types employing different network architectures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/7CJG8YL2/Winter et al. - 2022 - Unsupervised Learning of Group Invariant and Equiv.pdf;/Users/pmg/Zotero/storage/ZSSM3B65/2202.html}
}

@misc{wirnsberger_flattening_2023,
  title = {Flattening the Curve - {{How}} to Get Better Results with Small Deep-Mutational-Scanning Datasets},
  author = {Wirnsberger, Gregor and Pritisanac, Iva and Oberdorfer, Gustav and Gruber, Karl},
  year = {2023},
  month = oct,
  primaryclass = {New Results},
  pages = {2023.03.27.534314},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.03.27.534314},
  urldate = {2023-11-02},
  abstract = {Proteins are utilized in various biotechnological applications, often requiring the optimization of protein properties by introducing specific amino acid exchanges. Deep mutational scanning (DMS) is an effective high-throughput method for evaluating the effects of these exchanges on protein function. DMS data can then inform the training of a neural network to predict the impact of mutations. Most approaches employ some representation of the protein sequence for training and prediction. As proteins are characterized by complex structures and intricate residue interaction networks, directly providing structural information as input reduces the need to learn these features from the data. We introduce a method for encoding protein structures as stacked 2D contact maps, which capture residue interactions, their evolutionary conservation, and mutation-induced interaction changes. Furthermore, we explored techniques to augment neural network training performance on smaller DMS datasets. To validate our approach, we trained three neural network architectures originally used for image analysis on three DMS datasets, and we compared their performances with networks trained solely on protein sequences. The results confirm the effectiveness of the protein structure encoding in machine learning efforts on DMS data. Using structural representations as direct input to the networks, along with data augmentation and pre-training, significantly reduced demands on training data size and improved prediction performance, especially on smaller datasets, while performance on large datasets was on par with state of-the-art sequence convolutional neural networks. The methods presented here have the potential to provide the same workflow as DMS without the experimental and financial burden of testing thousands of mutants. Additionally, we present an open-source, user-friendly software tool to make these data analysis techniques accessible, particularly to biotechnology and protein engineering researchers who wish to apply them to their mutagenesis data.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/J8CZUFVU/Wirnsberger et al. - 2023 - Flattening the curve - How to get better results w.pdf}
}

@article{wu_adaptation_2016,
  title = {Adaptation in Protein Fitness Landscapes Is Facilitated by Indirect Paths},
  author = {Wu, Nicholas C and Dai, Lei and Olson, C Anders and {Lloyd-Smith}, James O and Sun, Ren},
  year = {2016},
  month = jul,
  journal = {eLife},
  volume = {5},
  pages = {e16965},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.16965},
  urldate = {2022-09-28},
  abstract = {The structure of fitness landscapes is critical for understanding adaptive protein evolution. Previous empirical studies on fitness landscapes were confined to either the neighborhood around the wild type sequence, involving mostly single and double mutants, or a combinatorially complete subgraph involving only two amino acids at each site. In reality, the dimensionality of protein sequence space is higher (20L) and there may be higher-order interactions among more than two sites. Here we experimentally characterized the fitness landscape of four sites in protein GB1, containing 204 = 160,000 variants. We found that while reciprocal sign epistasis blocked many direct paths of adaptation, such evolutionary traps could be circumvented by indirect paths through genotype space involving gain and subsequent loss of mutations. These indirect paths alleviate the constraint on adaptive protein evolution, suggesting that the heretofore neglected dimensions of sequence space may change our views on how proteins evolve.},
  keywords = {adaptive evolution,deep sequencing,epistasis,fitness landscape,saturation mutagenesis},
  file = {/Users/pmg/Zotero/storage/9TQQ84IQ/Wu et al. - 2016 - Adaptation in protein fitness landscapes is facili.pdf}
}

@article{wu_functional_2015,
  title = {Functional {{Constraint Profiling}} of a {{Viral Protein Reveals Discordance}} of {{Evolutionary Conservation}} and {{Functionality}}},
  author = {Wu, Nicholas C. and Olson, C. Anders and Du, Yushen and Le, Shuai and Tran, Kevin and Remenyi, Roland and Gong, Danyang and {Al-Mawsawi}, Laith Q. and Qi, Hangfei and Wu, Ting-Ting and Sun, Ren},
  year = {2015},
  month = jul,
  journal = {PLOS Genetics},
  volume = {11},
  number = {7},
  pages = {e1005310},
  publisher = {{Public Library of Science}},
  issn = {1553-7404},
  doi = {10.1371/journal.pgen.1005310},
  urldate = {2024-02-01},
  abstract = {Viruses often encode proteins with multiple functions due to their compact genomes. Existing approaches to identify functional residues largely rely on sequence conservation analysis. Inferring functional residues from sequence conservation can produce false positives, in which the conserved residues are functionally silent, or false negatives, where functional residues are not identified since they are species-specific and therefore non-conserved. Furthermore, the tedious process of constructing and analyzing individual mutations limits the number of residues that can be examined in a single study. Here, we developed a systematic approach to identify the functional residues of a viral protein by coupling experimental fitness profiling with protein stability prediction using the influenza virus polymerase PA subunit as the target protein. We identified a significant number of functional residues that were influenza type-specific and were evolutionarily non-conserved among different influenza types. Our results indicate that type-specific functional residues are prevalent and may not otherwise be identified by sequence conservation analysis alone. More importantly, this technique can be adapted to any viral (and potentially non-viral) protein where structural information is available.},
  langid = {english},
  keywords = {DNA libraries,Influenza A virus,Influenza viruses,Point mutation,Polymerase chain reaction,Polymerases,Viral evolution,Viral replication},
  file = {/Users/pmg/Zotero/storage/MYH4V3ZJ/Wu et al. - 2015 - Functional Constraint Profiling of a Viral Protein.pdf}
}

@article{wu_protein_2021,
  title = {Protein Sequence Design with Deep Generative Models},
  author = {Wu, Zachary and Johnston, Kadina E. and Arnold, Frances H. and Yang, Kevin K.},
  year = {2021},
  month = dec,
  journal = {Current Opinion in Chemical Biology},
  series = {Mechanistic {{Biology}} * {{Machine Learning}} in {{Chemical Biology}}},
  volume = {65},
  pages = {18--27},
  issn = {1367-5931},
  doi = {10.1016/j.cbpa.2021.04.004},
  urldate = {2022-10-17},
  abstract = {Protein engineering seeks to identify protein sequences with optimized properties. When guided by machine learning, protein sequence generation methods can draw on prior knowledge and experimental efforts to improve this process. In this review, we highlight recent applications of machine learning to generate protein sequences, focusing on the emerging field of deep generative methods.},
  langid = {english},
  keywords = {Deep learning,Generative models,Protein engineering},
  file = {/Users/pmg/Zotero/storage/45767V9R/Wu et al. - 2021 - Protein sequence design with deep generative model.pdf;/Users/pmg/Zotero/storage/K2BXRLVK/S136759312100051X.html}
}

@article{wu_protein_2021-1,
  title = {Protein Sequence Design with Deep Generative Models},
  author = {Wu, Zachary and Johnston, Kadina E. and Arnold, Frances H. and Yang, Kevin K.},
  year = {2021},
  month = dec,
  journal = {Current Opinion in Chemical Biology},
  series = {Mechanistic {{Biology}} * {{Machine Learning}} in {{Chemical Biology}}},
  volume = {65},
  pages = {18--27},
  issn = {1367-5931},
  doi = {10.1016/j.cbpa.2021.04.004},
  urldate = {2022-10-17},
  abstract = {Protein engineering seeks to identify protein sequences with optimized properties. When guided by machine learning, protein sequence generation methods can draw on prior knowledge and experimental efforts to improve this process. In this review, we highlight recent applications of machine learning to generate protein sequences, focusing on the emerging field of deep generative methods.},
  langid = {english},
  keywords = {Deep learning,Generative models,Protein engineering},
  file = {/Users/pmg/Zotero/storage/DI55TBYV/Wu et al. - 2021 - Protein sequence design with deep generative model.pdf;/Users/pmg/Zotero/storage/ZL2BAUL7/S136759312100051X.html}
}

@misc{wu_protein_2022,
  title = {Protein Structure Generation via Folding Diffusion},
  author = {Wu, Kevin E. and Yang, Kevin K. and van den Berg, Rianne and Zou, James Y. and Lu, Alex X. and Amini, Ava P.},
  year = {2022},
  month = nov,
  number = {arXiv:2209.15611},
  eprint = {2209.15611},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.15611},
  urldate = {2022-12-06},
  abstract = {The ability to computationally generate novel yet physically foldable protein structures could lead to new biological discoveries and new treatments targeting yet incurable diseases. Despite recent advances in protein structure prediction, directly generating diverse, novel protein structures from neural networks remains difficult. In this work, we present a new diffusion-based generative model that designs protein backbone structures via a procedure that mirrors the native folding process. We describe protein backbone structure as a series of consecutive angles capturing the relative orientation of the constituent amino acid residues, and generate new structures by denoising from a random, unfolded state towards a stable folded structure. Not only does this mirror how proteins biologically twist into energetically favorable conformations, the inherent shift and rotational invariance of this representation crucially alleviates the need for complex equivariant networks. We train a denoising diffusion probabilistic model with a simple transformer backbone and demonstrate that our resulting model unconditionally generates highly realistic protein structures with complexity and structural patterns akin to those of naturally-occurring proteins. As a useful resource, we release the first open-source codebase and trained models for protein structure diffusion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,I.2.0,J.3,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/5NYRUXKP/Wu et al. - 2022 - Protein structure generation via folding diffusion.pdf;/Users/pmg/Zotero/storage/8MT26V39/2209.html}
}

@misc{xiao_uncertainty_2022,
  title = {Uncertainty {{Quantification}} with {{Pre-trained Language Models}}: {{A Large-Scale Empirical Analysis}}},
  shorttitle = {Uncertainty {{Quantification}} with {{Pre-trained Language Models}}},
  author = {Xiao, Yuxin and Liang, Paul Pu and Bhatt, Umang and Neiswanger, Willie and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2022},
  month = oct,
  number = {arXiv:2210.04714},
  eprint = {2210.04714},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-10-17},
  abstract = {Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/7EKTRNDK/Xiao et al. - 2022 - Uncertainty Quantification with Pre-trained Langua.pdf}
}

@misc{xie_self-supervised_2022,
  title = {Self-{{Supervised Learning}} of {{Graph Neural Networks}}: {{A Unified Review}}},
  shorttitle = {Self-{{Supervised Learning}} of {{Graph Neural Networks}}},
  author = {Xie, Yaochen and Xu, Zhao and Zhang, Jingtun and Wang, Zhengyang and Ji, Shuiwang},
  year = {2022},
  month = apr,
  number = {arXiv:2102.10757},
  eprint = {2102.10757},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.10757},
  urldate = {2023-04-20},
  abstract = {Deep models trained in supervised mode have achieved remarkable success on a variety of tasks. When labeled samples are limited, self-supervised learning (SSL) is emerging as a new paradigm for making use of large amounts of unlabeled samples. SSL has achieved promising performance on natural language and image learning tasks. Recently, there is a trend to extend such success to graph data using graph neural networks (GNNs). In this survey, we provide a unified review of different ways of training GNNs using SSL. Specifically, we categorize SSL methods into contrastive and predictive models. In either category, we provide a unified framework for methods as well as how these methods differ in each component under the framework. Our unified treatment of SSL methods for GNNs sheds light on the similarities and differences of various methods, setting the stage for developing new methods and algorithms. We also summarize different SSL settings and the corresponding datasets used in each setting. To facilitate methodological development and empirical comparison, we develop a standardized testbed for SSL in GNNs, including implementations of common baseline methods, datasets, and evaluation metrics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/EII8VASH/Xie et al. - 2022 - Self-Supervised Learning of Graph Neural Networks.pdf;/Users/pmg/Zotero/storage/JBXLAPBQ/2102.html}
}

@misc{xie_self-training_2020,
  title = {Self-Training with {{Noisy Student}} Improves {{ImageNet}} Classification},
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
  year = {2020},
  month = jun,
  number = {arXiv:1911.04252},
  eprint = {1911.04252},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.04252},
  urldate = {2023-01-10},
  abstract = {We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/X4H2EZF5/Xie et al. - 2020 - Self-training with Noisy Student improves ImageNet.pdf;/Users/pmg/Zotero/storage/9K6VNYFC/1911.html}
}

@inproceedings{xing_distance_2002,
  title = {Distance {{Metric Learning}} with {{Application}} to {{Clustering}} with {{Side-Information}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xing, Eric and Jordan, Michael and Russell, Stuart J and Ng, Andrew},
  year = {2002},
  volume = {15},
  publisher = {{MIT Press}},
  urldate = {2022-08-29},
  abstract = {@cs.berkeley.edu},
  file = {/Users/pmg/Zotero/storage/SABK9WPY/Xing et al. - 2002 - Distance Metric Learning with Application to Clust.pdf}
}

@misc{xu_geodiff_2022,
  title = {{{GeoDiff}}: A {{Geometric Diffusion Model}} for {{Molecular Conformation Generation}}},
  shorttitle = {{{GeoDiff}}},
  author = {Xu, Minkai and Yu, Lantao and Song, Yang and Shi, Chence and Ermon, Stefano and Tang, Jian},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02923},
  eprint = {2203.02923},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02923},
  urldate = {2022-08-24},
  abstract = {Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/BQYCHXHC/Xu et al. - 2022 - GeoDiff a Geometric Diffusion Model for Molecular.pdf;/Users/pmg/Zotero/storage/8BA7NXQV/2203.html}
}

@misc{xu_how_2019,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2019},
  month = feb,
  number = {arXiv:1810.00826},
  eprint = {1810.00826},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.00826},
  urldate = {2023-04-24},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/Z5PZA3RS/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/Users/pmg/Zotero/storage/M3SAW2FV/1810.html}
}

@article{xu_peer_2022,
    title = {{{PEER}}: {{A Comprehensive}} and {{Multi-Task Benchmark}} for {{Protein Sequence Understanding}}},
  author={Xu, Minghao and Zhang, Zuobai and Lu, Jiarui and Zhu, Zhaocheng and Zhang, Yangtian and Chang, Ma and Liu, Runcheng and Tang, Jian},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35156--35173},
  year={2022}
}


@misc{yang_convolutions_2022,
  title = {Convolutions Are Competitive with Transformers for Protein Sequence Pretraining},
  author = {Yang, Kevin K. and Lu, Alex X. and Fusi, Nicolo},
  year = {2022},
  month = may,
  primaryclass = {New Results},
  pages = {2022.05.19.492714},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.05.19.492714},
  urldate = {2023-04-04},
  abstract = {Pretrained protein sequence language models largely rely on the transformer architecture. However, transformer run-time and memory requirements scale quadrat-ically with sequence length. We investigate the potential of a convolution-based architecture for protein sequence masked language model pretraining and subsequent finetuning. CNNs are competitive on the pretraining task with transformers across several orders of magnitude in parameter size while scaling linearly with sequence length. More importantly, CNNs are competitive with and occasionally superior to transformers across an extensive set of downstream evaluations, including structure prediction, zero-shot mutation effect prediction, and out-of-domain generalization. We also demonstrate strong performance on sequences longer than the positional embeddings allowed in the current state-of-the-art transformer protein masked language models. Finally, we close with a call to disentangle the effects of pretraining task and model architecture when studying pretrained protein sequence models.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder has placed this preprint in the Public Domain. It is no longer restricted by copyright. Anyone can legally share, reuse, remix, or adapt this material for any purpose without crediting the original authors.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/6YQHUJ4T/Yang et al. - 2022 - Convolutions are competitive with transformers for.pdf}
}

@misc{yang_fast_2023,
  title = {Fast Non-Autoregressive Inverse Folding with Discrete Diffusion},
  author = {Yang, John J. and Yim, Jason and Barzilay, Regina and Jaakkola, Tommi},
  year = {2023},
  month = dec,
  number = {arXiv:2312.02447},
  eprint = {2312.02447},
  primaryclass = {q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2312.02447},
  urldate = {2023-12-08},
  abstract = {Generating protein sequences that fold into a intended 3D structure is a fundamental step in de novo protein design. De facto methods utilize autoregressive generation, but this eschews higher order interactions that could be exploited to improve inference speed. We describe a non-autoregressive alternative that performs inference using a constant number of calls resulting in a 23 times speed up without a loss in performance on the CATH benchmark. Conditioned on the 3D structure, we fine-tune ProteinMPNN to perform discrete diffusion with a purity prior over the index sampling order. Our approach gives the flexibility in trading off inference speed and accuracy by modulating the diffusion speed. Code: https://github.com/johnyang101/pmpnndiff},
  archiveprefix = {arxiv},
  keywords = {Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/MRT3A2R2/Yang et al. - 2023 - Fast non-autoregressive inverse folding with discr.pdf;/Users/pmg/Zotero/storage/CY9WH34E/2312.html}
}

@article{yang_learned_2018,
  title = {Learned Protein Embeddings for Machine Learning},
  author = {Yang, Kevin K. and Wu, Zachary and Bedbrook, Claire N. and Arnold, Frances H.},
  year = {2018},
  month = aug,
  journal = {Bioinformatics (Oxford, England)},
  volume = {34},
  number = {15},
  pages = {2642--2648},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/bty178},
  abstract = {MOTIVATION: Machine-learning models trained on protein sequences and their measured functions can infer biological properties of unseen sequences without requiring an understanding of the underlying physical or biological mechanisms. Such models enable the prediction and discovery of sequences with optimal properties. Machine-learning models generally require that their inputs be vectors, and the conversion from a protein sequence to a vector representation affects the model's ability to learn. We propose to learn embedded representations of protein sequences that take advantage of the vast quantity of unmeasured protein sequence data available. These embeddings are low-dimensional and can greatly simplify downstream modeling. RESULTS: The predictive power of Gaussian process models trained using embeddings is comparable to those trained on existing representations, which suggests that embeddings enable accurate predictions despite having orders of magnitude fewer dimensions. Moreover, embeddings are simpler to obtain because they do not require alignments, structural data, or selection of informative amino-acid properties. Visualizing the embedding vectors shows meaningful relationships between the embedded proteins are captured. AVAILABILITY AND IMPLEMENTATION: The embedding vectors and code to reproduce the results are available at https://github.com/fhalab/embeddings\_reproduction/. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
  langid = {english},
  pmcid = {PMC6061698},
  pmid = {29584811},
  keywords = {Amino Acid Sequence,Bacteria,Computational Biology,Eukaryota,Humans,Machine Learning,{Models, Biological},Proteins,{Sequence Analysis, Protein},Software},
  file = {/Users/pmg/Zotero/storage/4V6I7325/Yang et al. - 2018 - Learned protein embeddings for machine learning.pdf}
}

@article{yang_machine-learning-guided_2019,
  title = {Machine-Learning-Guided Directed Evolution for Protein Engineering},
  author = {Yang, Kevin K. and Wu, Zachary and Arnold, Frances H.},
  year = {2019},
  month = aug,
  journal = {Nature Methods},
  volume = {16},
  number = {8},
  pages = {687--694},
  publisher = {{Nature Publishing Group}},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0496-6},
  urldate = {2023-03-22},
  abstract = {Protein engineering through machine-learning-guided directed evolution enables the optimization of protein functions. Machine-learning approaches predict how sequence maps to function in a data-driven manner without requiring a detailed model of the underlying physics or biological pathways. Such methods accelerate directed evolution by learning from the properties of characterized variants and using that information to select sequences that are likely to exhibit improved properties. Here we introduce the steps required to build machine-learning sequence{\textendash}function models and to use those models to guide engineering, making recommendations at each stage. This review covers basic concepts relevant to the use of machine learning for protein engineering, as well as the current literature and applications of this engineering paradigm. We illustrate the process with two case studies. Finally, we look to future opportunities for machine learning to enable the discovery of unknown protein functions and uncover the relationship between protein sequence and function.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Machine learning,Proteins},
  file = {/Users/pmg/Zotero/storage/E892RB8K/Yang et al. - 2019 - Machine-learning-guided directed evolution for pro.pdf}
}

@misc{yang_masked_2022,
  title = {Masked Inverse Folding with Sequence Transfer for Protein Representation Learning},
  author = {Yang, Kevin K. and Zanichelli, Niccol{\`o} and Yeh, Hugh},
  year = {2022},
  month = may,
  primaryclass = {New Results},
  pages = {2022.05.25.493516},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.05.25.493516},
  urldate = {2022-08-04},
  abstract = {Self-supervised pretraining on protein sequences has led to state-of-the art performance on protein function and fitness prediction. However, sequence-only methods ignore the rich information contained in experimental and predicted protein structures. Meanwhile, inverse folding methods reconstruct a protein's amino-acid sequence given its structure, but do not take advantage of sequences that do not have known structures. In this study, we train a masked inverse folding protein language model parameterized as a structured graph neural network. We then show that using the outputs from a pretrained sequence-only protein masked language model as input to the inverse folding model further improves pretraining perplexity. We evaluate both of these models on downstream protein engineering tasks and analyze the effect of using information from experimental or predicted structures on performance.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder has placed this preprint in the Public Domain. It is no longer restricted by copyright. Anyone can legally share, reuse, remix, or adapt this material for any purpose without crediting the original authors.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/PTWBUJQT/Yang et al. - 2022 - Masked inverse folding with sequence transfer for .pdf;/Users/pmg/Zotero/storage/HX7Y44I5/2022.05.25.html}
}

@misc{yang_masked_2023,
  title = {Masked Inverse Folding with Sequence Transfer for Protein Representation Learning},
  author = {Yang, Kevin K. and Yeh, Hugh and Zanichelli, Niccol{\`o}},
  year = {2023},
  month = mar,
  primaryclass = {New Results},
  pages = {2022.05.25.493516},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.05.25.493516},
  urldate = {2023-03-20},
  abstract = {Self-supervised pretraining on protein sequences has led to state-of-the art performance on protein function and fitness prediction. However, sequence-only methods ignore the rich information contained in experimental and predicted protein structures. Meanwhile, inverse folding methods reconstruct a protein's amino-acid sequence given its structure, but do not take advantage of sequences that do not have known structures. In this study, we train a masked inverse folding protein language model parameterized as a structured graph neural network. We then show that using the outputs from a pretrained sequence-only protein masked language model as input to the inverse folding model further improves pretraining perplexity. We evaluate both of these models on downstream protein engineering tasks and analyze the effect of using information from experimental or predicted structures on performance.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/JTJYG7LA/Yang et al. - 2023 - Masked inverse folding with sequence transfer for .pdf}
}

@misc{yang_now_2022,
  title = {Now {{What Sequence}}? {{Pre-trained Ensembles}} for {{Bayesian Optimization}} of {{Protein Sequences}}},
  shorttitle = {Now {{What Sequence}}?},
  author = {Yang, Ziyue and Milas, Katarina A. and White, Andrew D.},
  year = {2022},
  month = sep,
  primaryclass = {New Results},
  pages = {2022.08.05.502972},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.08.05.502972},
  urldate = {2022-09-05},
  abstract = {Pre-trained models have been transformative in natural language, computer vision, and now protein sequences by enabling accuracy with few training examples. We show how to use pre-trained sequence models in Bayesian optimization to design new protein sequences with minimal labels (i.e., few experiments). Pre-trained models give good predictive accuracy at low data and Bayesian optimization guides the choice of which sequences to test. Pre-trained sequence models also remove the common requirement of having a list of possible experiments. Any sequence can be considered. We show significantly fewer labeled sequences are required for three sequence design tasks, including creating novel peptide inhibitors with AlphaFold. These de novo peptide inhibitors require only sequence information, no known protein-protein structures, and we can predict highly-efficient binders with less than 10 AlphaFold calculations.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/H5W2X4E7/Yang et al. - 2022 - Now What Sequence Pre-trained Ensembles for Bayes.pdf;/Users/pmg/Zotero/storage/KN6JN7DA/2022.08.05.html}
}

@misc{yang_survey_2021,
  title = {A {{Survey}} on {{Deep Semi-supervised Learning}}},
  author = {Yang, Xiangli and Song, Zixing and King, Irwin and Xu, Zenglin},
  year = {2021},
  month = aug,
  number = {arXiv:2103.00550},
  eprint = {2103.00550},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-10-06},
  abstract = {Deep semi-supervised learning is a fast-growing field with a range of practical applications. This paper provides a comprehensive survey on both fundamentals and recent advances in deep semi-supervised learning methods from perspectives of model design and unsupervised loss functions. We first present a taxonomy for deep semi-supervised learning that categorizes existing methods, including deep generative methods, consistency regularization methods, graph-based methods, pseudo-labeling methods, and hybrid methods. Then we provide a comprehensive review of 52 representative methods and offer a detailed comparison of these methods in terms of the type of losses, contributions, and architecture differences. In addition to the progress in the past few years, we further discuss some shortcomings of existing methods and provide some tentative heuristic solutions for solving these open problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/3SBMXQPA/Yang et al. - 2021 - A Survey on Deep Semi-supervised Learning.pdf}
}

@misc{yi_graph_2023,
  title = {Graph {{Denoising Diffusion}} for {{Inverse Protein Folding}}},
  author = {Yi, Kai and Zhou, Bingxin and Shen, Yiqing and Li{\`o}, Pietro and Wang, Yu Guang},
  year = {2023},
  month = jun,
  number = {arXiv:2306.16819},
  eprint = {2306.16819},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.16819},
  urldate = {2023-07-03},
  abstract = {Inverse protein folding is challenging due to its inherent one-to-many mapping characteristic, where numerous possible amino acid sequences can fold into a single, identical protein backbone. This task involves not only identifying viable sequences but also representing the sheer diversity of potential solutions. However, existing discriminative models, such as transformer-based auto-regressive models, struggle to encapsulate the diverse range of plausible solutions. In contrast, diffusion probabilistic models, as an emerging genre of generative approaches, offer the potential to generate a diverse set of sequence candidates for determined protein backbones. We propose a novel graph denoising diffusion model for inverse protein folding, where a given protein backbone guides the diffusion process on the corresponding amino acid residue types. The model infers the joint distribution of amino acids conditioned on the nodes' physiochemical properties and local environment. Moreover, we utilize amino acid replacement matrices for the diffusion forward process, encoding the biologically-meaningful prior knowledge of amino acids from their spatial and sequential neighbors as well as themselves, which reduces the sampling space of the generative process. Our model achieves state-of-the-art performance over a set of popular baseline methods in sequence recovery and exhibits great potential in generating diverse protein sequences for a determined protein backbone structure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/8RPM96XK/Yi et al. - 2023 - Graph Denoising Diffusion for Inverse Protein Fold.pdf;/Users/pmg/Zotero/storage/U6BR56BI/2306.html}
}

@misc{yim_se3_2023,
  title = {{{SE}}(3) Diffusion Model with Application to Protein Backbone Generation},
  author = {Yim, Jason and Trippe, Brian L. and De Bortoli, Valentin and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi},
  year = {2023},
  month = feb,
  number = {arXiv:2302.02277},
  eprint = {2302.02277},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.02277},
  urldate = {2023-02-07},
  abstract = {The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in 3D (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on SE(3), the space of orientation preserving rigid motions in R3, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of SE(3) invariant diffusion models on multiple frames followed by a novel framework, FrameDiff, for learning the SE(3) equivariant score over multiple frames. We apply FrameDiff on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our samples are capable of generalizing beyond any known protein structure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/2GSGTR77/Yim et al. - 2023 - SE(3) diffusion model with application to protein .pdf;/Users/pmg/Zotero/storage/RY49CDYH/2302.html}
}

@misc{yim_se3_2023-1,
  title = {{{SE}}(3) Diffusion Model with Application to Protein Backbone Generation},
  author = {Yim, Jason and Trippe, Brian L. and De Bortoli, Valentin and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi},
  year = {2023},
  month = feb,
  number = {arXiv:2302.02277},
  eprint = {2302.02277},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.02277},
  urldate = {2023-02-23},
  abstract = {The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in 3D (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on SE(3), the space of orientation preserving rigid motions in R3, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of SE(3) invariant diffusion models on multiple frames followed by a novel framework, FrameDiff, for learning the SE(3) equivariant score over multiple frames. We apply FrameDiff on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our samples are capable of generalizing beyond any known protein structure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/G2H4679S/Yim et al. - 2023 - SE(3) diffusion model with application to protein .pdf;/Users/pmg/Zotero/storage/HWYAVZBV/2302.html}
}

@misc{you_when_2020,
  title = {When {{Does Self-Supervision Help Graph Convolutional Networks}}?},
  author = {You, Yuning and Chen, Tianlong and Wang, Zhangyang and Shen, Yang},
  year = {2020},
  month = jul,
  number = {arXiv:2006.09136},
  eprint = {2006.09136},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.09136},
  urldate = {2023-04-24},
  abstract = {Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of images. Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored. In this study, we report the first systematic exploration and assessment of incorporating self-supervision into GCNs. We first elaborate three mechanisms to incorporate self-supervision into GCNs, analyze the limitations of pretraining \& finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we propose to investigate three novel self-supervised learning tasks for GCNs with theoretical rationales and numerical comparisons. Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. Our codes are available at https://github.com/Shen-Lab/SS-GCNs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/MXMS787I/You et al. - 2020 - When Does Self-Supervision Help Graph Convolutiona.pdf;/Users/pmg/Zotero/storage/UM98ATX3/2006.html}
}

@article{yu_enzyme_2023,
  title = {Enzyme Function Prediction Using Contrastive Learning},
  author = {Yu, Tianhao and Cui, Haiyang and Li, Jianan Canal and Luo, Yunan and Jiang, Guangde and Zhao, Huimin},
  year = {2023},
  month = mar,
  journal = {Science},
  volume = {379},
  number = {6639},
  pages = {1358--1363},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.adf2465},
  urldate = {2023-06-26},
  abstract = {Enzyme function annotation is a fundamental challenge, and numerous computational tools have been developed. However, most of these tools cannot accurately predict functional annotations, such as enzyme commission (EC) number, for less-studied proteins or those with previously uncharacterized functions or multiple activities. We present a machine learning algorithm named CLEAN (contrastive learning{\textendash}enabled enzyme annotation) to assign EC numbers to enzymes with better accuracy, reliability, and sensitivity compared with the state-of-the-art tool BLASTp. The contrastive learning framework empowers CLEAN to confidently (i) annotate understudied enzymes, (ii) correct mislabeled enzymes, and (iii) identify promiscuous enzymes with two or more EC numbers{\textemdash}functions that we demonstrate by systematic in silico and in vitro experiments. We anticipate that this tool will be widely used for predicting the functions of uncharacterized enzymes, thereby advancing many fields, such as genomics, synthetic biology, and biocatalysis.}
}

@misc{zaidi_pre-training_2022,
  title = {Pre-Training via {{Denoising}} for {{Molecular Property Prediction}}},
  author = {Zaidi, Sheheryar and Schaarschmidt, Michael and Martens, James and Kim, Hyunjik and Teh, Yee Whye and {Sanchez-Gonzalez}, Alvaro and Battaglia, Peter and Pascanu, Razvan and Godwin, Jonathan},
  year = {2022},
  month = oct,
  number = {arXiv:2206.00133},
  eprint = {2206.00133},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.00133},
  urldate = {2023-04-17},
  abstract = {Many important problems involving molecular property prediction from 3D structures have limited data, posing a generalization challenge for neural networks. In this paper, we describe a pre-training technique based on denoising that achieves a new state-of-the-art in molecular property prediction by utilizing large datasets of 3D molecular structures at equilibrium to learn meaningful representations for downstream tasks. Relying on the well-known link between denoising autoencoders and score-matching, we show that the denoising objective corresponds to learning a molecular force field -- arising from approximating the Boltzmann distribution with a mixture of Gaussians -- directly from equilibrium structures. Our experiments demonstrate that using this pre-training objective significantly improves performance on multiple benchmarks, achieving a new state-of-the-art on the majority of targets in the widely used QM9 dataset. Our analysis then provides practical insights into the effects of different factors -- dataset sizes, model size and architecture, and the choice of upstream and downstream datasets -- on pre-training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {/Users/pmg/Zotero/storage/8JK4T7T4/Zaidi et al. - 2022 - Pre-training via Denoising for Molecular Property .pdf;/Users/pmg/Zotero/storage/YPVMB6IV/2206.html}
}

@misc{zhang_enhancing_2023,
  title = {Enhancing {{Protein Language Models}} with {{Structure-based Encoder}} and {{Pre-training}}},
  author = {Zhang, Zuobai and Xu, Minghao and Chenthamarakshan, Vijil and Lozano, Aur{\'e}lie and Das, Payel and Tang, Jian},
  year = {2023},
  month = mar,
  number = {arXiv:2303.06275},
  eprint = {2303.06275},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.06275},
  urldate = {2023-07-31},
  abstract = {Protein language models (PLMs) pre-trained on large-scale protein sequence corpora have achieved impressive performance on various downstream protein understanding tasks. Despite the ability to implicitly capture inter-residue contact information, transformer-based PLMs cannot encode protein structures explicitly for better structure-aware protein representations. Besides, the power of pre-training on available protein structures has not been explored for improving these PLMs, though structures are important to determine functions. To tackle these limitations, in this work, we enhance the PLMs with structure-based encoder and pre-training. We first explore feasible model architectures to combine the advantages of a state-of-the-art PLM (i.e., ESM-1b1) and a state-of-the-art protein structure encoder (i.e., GearNet). We empirically verify the ESM-GearNet that connects two encoders in a series way as the most effective combination model. To further improve the effectiveness of ESM-GearNet, we pre-train it on massive unlabeled protein structures with contrastive learning, which aligns representations of co-occurring subsequences so as to capture their biological correlation. Extensive experiments on EC and GO protein function prediction benchmarks demonstrate the superiority of ESM-GearNet over previous PLMs and structure encoders, and clear performance gains are further achieved by structure-based pre-training upon ESM-GearNet. Our implementation is available at https://github.com/DeepGraphLearning/GearNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/2GITIGNY/Zhang et al. - 2023 - Enhancing Protein Language Models with Structure-b.pdf;/Users/pmg/Zotero/storage/PT4X5H6C/2303.html}
}

@misc{zhang_graph-less_2022,
  title = {Graph-Less {{Neural Networks}}: {{Teaching Old MLPs New Tricks}} via {{Distillation}}},
  shorttitle = {Graph-Less {{Neural Networks}}},
  author = {Zhang, Shichang and Liu, Yozen and Sun, Yizhou and Shah, Neil},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08727},
  eprint = {2110.08727},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.08727},
  urldate = {2022-08-24},
  abstract = {Graph Neural Networks (GNNs) are popular for graph machine learning and have shown great results on wide node classification tasks. Yet, they are less popular for practical deployments in the industry owing to their scalability challenges incurred by data dependency. Namely, GNN inference depends on neighbor nodes multiple hops away from the target, and fetching them burdens latency-constrained applications. Existing inference acceleration methods like pruning and quantization can speed up GNNs by reducing Multiplication-and-ACcumulation (MAC) operations, but the improvements are limited given the data dependency is not resolved. Conversely, multi-layer perceptrons (MLPs) have no graph dependency and infer much faster than GNNs, even though they are less accurate than GNNs for node classification in general. Motivated by these complementary strengths and weaknesses, we bring GNNs and MLPs together via knowledge distillation (KD). Our work shows that the performance of MLPs can be improved by large margins with GNN KD. We call the distilled MLPs Graph-less Neural Networks (GLNNs) as they have no inference graph dependency. We show that GLNNs with competitive accuracy infer faster than GNNs by 146X-273X and faster than other acceleration methods by 14X-27X. Under a production setting involving both transductive and inductive predictions across 7 datasets, GLNN accuracies improve over stand-alone MLPs by 12.36\% on average and match GNNs on 6/7 datasets. Comprehensive analysis shows when and why GLNNs can achieve competitive accuracies to GNNs and suggests GLNN as a handy choice for latency-constrained applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/NMRL9P38/Zhang et al. - 2022 - Graph-less Neural Networks Teaching Old MLPs New .pdf;/Users/pmg/Zotero/storage/9Q2GDFCG/2110.html}
}

@misc{zhang_learning_2023,
  title = {Learning {{Universal}} and {{Robust 3D Molecular Representations}} with {{Graph Convolutional Networks}}},
  author = {Zhang, Shuo and Liu, Yang and Xie, Li and Xie, Lei},
  year = {2023},
  month = jul,
  number = {arXiv:2307.12491},
  eprint = {2307.12491},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.12491},
  urldate = {2023-07-27},
  abstract = {To learn accurate representations of molecules, it is essential to consider both chemical and geometric features. To encode geometric information, many descriptors have been proposed in constrained circumstances for specific types of molecules and do not have the properties to be ``robust": 1. Invariant to rotations and translations; 2. Injective when embedding molecular structures. In this work, we propose a universal and robust Directional Node Pair (DNP) descriptor based on the graph representations of 3D molecules. Our DNP descriptor is robust compared to previous ones and can be applied to multiple molecular types. To combine the DNP descriptor and chemical features in molecules, we construct the Robust Molecular Graph Convolutional Network (RoM-GCN) which is capable to take both node and edge features into consideration when generating molecule representations. We evaluate our model on protein and small molecule datasets. Our results validate the superiority of the DNP descriptor in incorporating 3D geometric information of molecules. RoM-GCN outperforms all compared baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/pmg/Zotero/storage/A9Z9AIM7/Zhang et al. - 2023 - Learning Universal and Robust 3D Molecular Represe.pdf;/Users/pmg/Zotero/storage/73MI67E9/2307.html}
}

@misc{zhang_protein_2023,
  title = {Protein {{Representation Learning}} by {{Geometric Structure Pretraining}}},
  author = {Zhang, Zuobai and Xu, Minghao and Jamasb, Arian and Chenthamarakshan, Vijil and Lozano, Aurelie and Das, Payel and Tang, Jian},
  year = {2023},
  month = jan,
  number = {arXiv:2203.06125},
  eprint = {2203.06125},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.06125},
  urldate = {2023-07-31},
  abstract = {Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at https://github.com/DeepGraphLearning/GearNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/2YLYMX69/Zhang et al. - 2023 - Protein Representation Learning by Geometric Struc.pdf;/Users/pmg/Zotero/storage/4YDB8RHP/2203.html}
}

@article{zhang_scoring_2004,
  title = {Scoring Function for Automated Assessment of Protein Structure Template Quality},
  author = {Zhang, Yang and Skolnick, Jeffrey},
  year = {2004},
  journal = {Proteins: Structure, Function, and Bioinformatics},
  volume = {57},
  number = {4},
  pages = {702--710},
  issn = {1097-0134},
  doi = {10.1002/prot.20264},
  urldate = {2023-06-08},
  abstract = {We have developed a new scoring function, the template modeling score (TM-score), to assess the quality of protein structure templates and predicted full-length models by extending the approaches used in Global Distance Test (GDT)1 and MaxSub.2 First, a protein size-dependent scale is exploited to eliminate the inherent protein size dependence of the previous scores and appropriately account for random protein structure pairs. Second, rather than setting specific distance cutoffs and calculating only the fractions with errors below the cutoff, all residue pairs in alignment/modeling are evaluated in the proposed score. For comparison of various scoring functions, we have constructed a large-scale benchmark set of structure templates for 1489 small to medium size proteins using the threading program PROSPECTOR\_3 and built the full-length models using MODELLER and TASSER. The TM-score of the initial threading alignments, compared to the GDT and MaxSub scoring functions, shows a much stronger correlation to the quality of the final full-length models. The TM-score is further exploited as an assessment of all `new fold' targets in the recent CASP5 experiment and shows a close coincidence with the results of human-expert visual assessment. These data suggest that the TM-score is a useful complement to the fully automated assessment of protein structure predictions. The executable program of TM-score is freely downloadable at http://bioinformatics.buffalo.edu/TM-score. Proteins 2004. {\textcopyright} 2004 Wiley-Liss, Inc.},
  copyright = {Copyright {\textcopyright} 2004 Wiley-Liss, Inc.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/48LJ3F6D/prot.html}
}

@article{zhang_survey_2023,
  title = {A {{Survey}} on {{Graph Diffusion Models}}: {{Generative AI}} in {{Science}} for {{Molecule}}, {{Protein}} and {{Material}}},
  shorttitle = {A {{Survey}} on {{Graph Diffusion Models}}},
  author = {Zhang, Mengchun and Qamar, Maryam and Kang, Taegoo and Jung, Yuna and Zhang, Chenshuang and Bae, Sung-Ho and Zhang, Chaoning},
  year = {2023},
  eprint = {2304.01565},
  primaryclass = {cs},
  doi = {10.13140/RG.2.2.26493.64480},
  urldate = {2023-04-24},
  abstract = {Diffusion models have become a new SOTA generative modeling method in various fields, for which there are multiple survey works that provide an overall survey. With the number of articles on diffusion models increasing exponentially in the past few years, there is an increasing need for surveys of diffusion models on specific fields. In this work, we are committed to conducting a survey on the graph diffusion models. Even though our focus is to cover the progress of diffusion models in graphs, we first briefly summarize how other generative modeling methods are used for graphs. After that, we introduce the mechanism of diffusion models in various forms, which facilitates the discussion on the graph diffusion models. The applications of graph diffusion models mainly fall into the category of AI-generated content (AIGC) in science, for which we mainly focus on how graph diffusion models are utilized for generating molecules and proteins but also cover other cases, including materials design. Moreover, we discuss the issue of evaluating diffusion models in the graph domain and the existing challenges.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/2LXKJK9P/Zhang et al. - 2023 - A Survey on Graph Diffusion Models Generative AI .pdf;/Users/pmg/Zotero/storage/H9GDKK22/2304.html}
}

@misc{zhang_systematic_2023,
  title = {A {{Systematic Survey}} in {{Geometric Deep Learning}} for {{Structure-based Drug Design}}},
  author = {Zhang, Zaixi and Yan, Jiaxian and Liu, Qi and Chen, Enhong},
  year = {2023},
  month = jun,
  number = {arXiv:2306.11768},
  eprint = {2306.11768},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.11768},
  urldate = {2023-07-03},
  abstract = {Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, {\textbackslash}emph\{de novo\} molecule generation, linker design, and binding affinity prediction), including the problem setup, representative methods, datasets, and evaluation metrics. Finally, we conclude this survey with the current challenges and highlight potential opportunities of geometric deep learning for structure-based drug design.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Computational Engineering, Finance, and Science},Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/T4DFC9HM/Zhang et al. - 2023 - A Systematic Survey in Geometric Deep Learning for.pdf;/Users/pmg/Zotero/storage/EAWSTWUV/2306.html}
}

@misc{zhang_understanding_2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  month = feb,
  number = {arXiv:1611.03530},
  eprint = {1611.03530},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.03530},
  urldate = {2023-02-02},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/PQLKKMUA/Zhang et al. - 2017 - Understanding deep learning requires rethinking ge.pdf;/Users/pmg/Zotero/storage/QTW7MSNN/1611.html}
}

@misc{zheng_structure-informed_2023,
  title = {Structure-Informed {{Language Models Are Protein Designers}}},
  author = {Zheng, Zaixiang and Deng, Yifan and Xue, Dongyu and Zhou, Yi and Ye, Fei and Gu, Quanquan},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.03.526917},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.03.526917},
  urldate = {2023-02-06},
  abstract = {This paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that our approach outperforms the state-of-the-art methods by a large margin, leading to 4\% to 12\% accuracy gains in sequence recovery (e.g., 55.65\% and 56.63\% on CATH 4.2 and 4.3 single-chain benchmarks, and {$>$}60\% when designing protein complexes). We provide extensive and in-depth analyses, which verify that LM-Design can (1) indeed leverage both structural and sequential knowledge to accurately handle structurally non-deterministic regions, (2) benefit from scaling data and model size, and (3) generalize to other proteins (e.g., antibodies and de novo proteins).},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\textcopyright} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/W8HT6Z2K/Zheng et al. - 2023 - Structure-informed Language Models Are Protein Des.pdf}
}

@misc{zhou_accurate_2023,
  title = {Accurate and {{Definite Mutational Effect Prediction}} with {{Lightweight Equivariant Graph Neural Networks}}},
  author = {Zhou, Bingxin and Lv, Outongyi and Yi, Kai and Xiong, Xinye and Tan, Pan and Hong, Liang and Wang, Yu Guang},
  year = {2023},
  month = apr,
  number = {arXiv:2304.08299},
  eprint = {2304.08299},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.08299},
  urldate = {2023-04-20},
  abstract = {Directed evolution as a widely-used engineering strategy faces obstacles in finding desired mutants from the massive size of candidate modifications. While deep learning methods learn protein contexts to establish feasible searching space, many existing models are computationally demanding and fail to predict how specific mutational tests will affect a protein's sequence or function. This research introduces a lightweight graph representation learning scheme that efficiently analyzes the microenvironment of wild-type proteins and recommends practical higher-order mutations exclusive to the user-specified protein and function of interest. Our method enables continuous improvement of the inference model by limited computational resources and a few hundred mutational training samples, resulting in accurate prediction of variant effects that exhibit near-perfect correlation with the ground truth across deep mutational scanning assays of 19 proteins. With its affordability and applicability to both computer scientists and biochemical laboratories, our solution offers a wide range of benefits that make it an ideal choice for the community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/Users/pmg/Zotero/storage/8J2HI387/Zhou et al. - 2023 - Accurate and Definite Mutational Effect Prediction.pdf;/Users/pmg/Zotero/storage/KPYXX44E/2304.html}
}

@misc{zhou_deep_2021,
  title = {A {{Deep Generative Approach}} to {{Conditional Sampling}}},
  author = {Zhou, Xingyu and Jiao, Yuling and Liu, Jin and Huang, Jian},
  year = {2021},
  month = oct,
  number = {arXiv:2110.10277},
  eprint = {2110.10277},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.10277},
  urldate = {2022-12-08},
  abstract = {We propose a deep generative approach to sampling from a conditional distribution based on a unified formulation of conditional distribution and generalized nonparametric regression function using the noise-outsourcing lemma. The proposed approach aims at learning a conditional generator so that a random sample from the target conditional distribution can be obtained by the action of the conditional generator on a sample drawn from a reference distribution. The conditional generator is estimated nonparametrically with neural networks by matching appropriate joint distributions using the Kullback-Liebler divergence. An appealing aspect of our method is that it allows either of or both the predictor and the response to be high-dimensional and can handle both continuous and discrete type predictors and responses. We show that the proposed method is consistent in the sense that the conditional generator converges in distribution to the underlying conditional distribution under mild conditions. Our numerical experiments with simulated and benchmark image data validate the proposed method and demonstrate that it outperforms several existing conditional density estimation methods.},
  archiveprefix = {arxiv},
  keywords = {{62G05, 62G07, 68T07},Mathematics - Statistics Theory},
  file = {/Users/pmg/Zotero/storage/NCEKUZRR/Zhou et al. - 2021 - A Deep Generative Approach to Conditional Sampling.pdf;/Users/pmg/Zotero/storage/NXIIPF2A/2110.html}
}

@article{zhou_lightweight_nodate,
  title = {Lightweight {{Equivariant Graph Representation Learning}} for {{Protein Engineering}}},
  author = {Zhou, Bingxin and Lv, Outongyi and Yi, Kai and Xiong, Xinye and Tan, Pan and Hong, Liang and Wang, Yu Guang},
  pages = {19},
  abstract = {This work tackles the issue of directed evolution in computational protein design that makes an accurate prediction for the function of a protein mutant. We design a lightweight pre-training graph neural network model for multi-task protein representation learning from its 3D structure. Rather than reconstructing and optimizing the protein structure, the trained model recovers the amino acid types and key properties of the central residues from a given noisy three-dimensional local environment. On the prediction task for the higher-order mutants, where many amino acid sites of the protein are mutated, the proposed training strategy achieves remarkably higher performance by 20\% improvement at the cost of requiring less than 1\% of computational resources that are required by popular transformer-based state-of-the-art deep learning models for protein design.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/KEJVDPFV/Zhou et al. - Lightweight Equivariant Graph Representation Learn.pdf}
}

@article{zhou_unsupervised_nodate,
  title = {Unsupervised Language Models for Disease Variant Prediction},
  author = {Zhou, Allan and Landolfi, Nicholas C and O'Neill, Daniel C},
  pages = {6},
  abstract = {There is considerable interest in predicting the pathogenicity of protein variants in human genes. Due to the sparsity of high quality labels, recent approaches turn to unsupervised learning, using Multiple Sequence Alignments (MSAs) to train generative models of natural sequence variation within each gene. These generative models then predict variant likelihood as a proxy to evolutionary fitness. In this work we instead combine this evolutionary principle with pretrained protein language models (LMs), which have already shown promising results in predicting protein structure and function. Instead of training separate models per-gene, we find that a single protein LM trained on broad sequence datasets can score pathogenicity for any gene variant zero-shot, without MSAs or finetuning. We call this unsupervised approach VELM (Variant Effect via Language Models), and show that it achieves scoring performance comparable to the state of the art when evaluated on clinically labeled variants of disease-related genes.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/664JYBV6/Zhou et al. - Unsupervised language models for disease variant p.pdf}
}

@inproceedings{zhu_survey_2022,
  title = {A {{Survey}} on {{Deep Graph Generation}}: {{Methods}} and {{Application}}},
  shorttitle = {A {{Survey}} on {{Deep Graph Generation}}},
  booktitle = {Learning on {{Graphs Conference}}},
  author = {Zhu, Yanqiao and Du, Yuanqi and Wang, Yinkai and Xu, Yichen and Zhang, Jieyu and Liu, Qiang and Wu, Shu},
  year = {2022},
  month = nov,
  urldate = {2022-11-25},
  abstract = {Graphs are ubiquitous in encoding relational information of real-world objects in many domains. Graph generation, whose purpose is to generate new graphs from a distribution similar to the observed graphs, has received increasing attention thanks to the recent advances of deep learning models. In this paper, we conduct a comprehensive review on the existing literature of deep graph generation from a variety of emerging methods to its wide application areas. Specifically, we first formulate the problem of deep graph generation and discuss its difference with several related graph learning tasks. Secondly, we divide the state-of-the-art methods into three categories based on model architectures and summarize their generation strategies. Thirdly, we introduce three key application areas of deep graph generation. Lastly, we highlight challenges and opportunities in the future study of deep graph generation.},
  langid = {english},
  file = {/Users/pmg/Zotero/storage/3RJ4XDG8/forum.html}
}

@misc{zhu_weaker_2023,
  title = {Weaker {{Than You Think}}: {{A Critical Look}} at {{Weakly Supervised Learning}}},
  shorttitle = {Weaker {{Than You Think}}},
  author = {Zhu, Dawei and Shen, Xiaoyu and Mosbach, Marius and Stephan, Andreas and Klakow, Dietrich},
  year = {2023},
  month = jul,
  number = {arXiv:2305.17442},
  eprint = {2305.17442},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.17442},
  urldate = {2023-09-04},
  abstract = {Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/pmg/Zotero/storage/24NLPNSJ/Zhu et al. - 2023 - Weaker Than You Think A Critical Look at Weakly S.pdf;/Users/pmg/Zotero/storage/HFF5XGYZ/2305.html}
}

@misc{zuobai_zhang_protein_2022,
  title = {Protein {{Representation Learning}} by {{Geometric Structure Pretraining}}},
  author = {{Zuobai Zhang} and Xu, Minghao and Jamasb, Arian and Chenthamarakshan, Vijil and Lozano, Aurelie and Das, Payel and Tang, Jian},
  year = {2022},
  month = may,
  number = {arXiv:2203.06125},
  eprint = {2203.06125},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.06125},
  urldate = {2022-08-24},
  abstract = {Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less data. All codes and models will be published upon acceptance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/pmg/Zotero/storage/7IDU7P4J/Zhang et al. - 2022 - Protein Representation Learning by Geometric Struc.pdf;/Users/pmg/Zotero/storage/CTXDPPBF/2203.html}
}
@inproceedings{10.5555/3625834.3625890,
author = {Foldager, Jonathan and Jordahn, Mikkel and Hansen, Lars Kai and Andersen, Michael Riis},
title = {{On the Role of Model Uncertainties in Bayesian Optimization}},
year = {2023},
publisher = {JMLR.org},
abstract = {Bayesian Optimization (BO) is a popular method for black-box optimization, which relies on uncertainty as part of its decision-making process when deciding which experiment to perform next. However, not much work has addressed the effect of uncertainty on the performance of the BO algorithm and to what extent calibrated uncertainties improve the ability to find the global optimum. In this work, we provide an extensive study of the relationship between the BO performance (regret) and uncertainty calibration for popular surrogate models and acquisition functions, and compare them across both synthetic and real-world experiments. Our results show that Gaussian Processes, and more surprisingly, Deep Ensembles are strong surrogate models. Our results further show a positive association between calibration error and regret, but interestingly, this association disappears when we control for the type of surrogate model in the analysis. We also study the effect of recalibration and demonstrate that it generally does not lead to improved regret. Finally, we provide theoretical justification for why uncertainty calibration might be difficult to combine with BO due to the small sample sizes commonly used.},
booktitle = {Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence},
articleno = {56},
numpages = {10},
location = {Pittsburgh, PA, USA},
series = {UAI '23}
}

@inproceedings{10.5555/645531.656014,
author = {G\"{a}rtner, Thomas and Flach, Peter A. and Kowalczyk, Adam and Smola, Alex J.},
title = {Multi-Instance Kernels},
year = {2002},
isbn = {1558608737},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
pages = {179–186},
numpages = {8},
series = {ICML '02}
}

@article{binois_survey_2022,
	title = {A {Survey} on {High}-dimensional {Gaussian} {Process} {Modeling} with {Application} to {Bayesian} {Optimization}},
	volume = {2},
	url = {https://dl.acm.org/doi/10.1145/3545611},
	doi = {10.1145/3545611},
	abstract = {Bayesian Optimization (BO), the application of Bayesian function approximation to finding optima of expensive functions, has exploded in popularity in recent years. In particular, much attention has been paid to improving its efficiency on problems with many parameters to optimize. This attention has trickled down to the workhorse of high-dimensional BO, high-dimensional Gaussian process regression, which is also of independent interest. The great flexibility that the Gaussian process prior implies is a boon when modeling complicated, low-dimensional surfaces but simply says too little when dimension grows too large. A variety of structural model assumptions have been tested to tame high dimensions, from variable selection and additive decomposition to low-dimensional embeddings and beyond. Most of these approaches in turn require modifications of the acquisition function optimization strategy as well. Here, we review the defining structural model assumptions and discuss the benefits and drawbacks of these approaches in practice.},
	number = {2},
	urldate = {2024-04-02},
	journal = {ACM Transactions on Evolutionary Learning and Optimization},
	author = {Binois, Mickaël and Wycoff, Nathan},
	month = aug,
	year = {2022},
	keywords = {Black-box optimization, active subspace, additivity, low-intrinsic dimensionality, variable selection},
	pages = {8:1--8:26},
}

@inproceedings{meanti_kernel_2020,
 author = {Meanti, Giacomo and Carratino, Luigi and Rosasco, Lorenzo and Rudi, Alessandro},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {14410--14422},
 publisher = {Curran Associates, Inc.},
 title = {{Kernel Methods Through the Roof: Handling Billions of Points Efficiently}},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/a59afb1b7d82ec353921a55c579ee26d-Paper.pdf},
 volume = {33},
 year = {2020}
}




@article{liu_when_2020,
	title = {When {Gaussian} {Process} {Meets} {Big} {Data}: {A} {Review} of {Scalable} {GPs}},
	volume = {31},
	issn = {2162-2388},
	shorttitle = {When {Gaussian} {Process} {Meets} {Big} {Data}},
	url = {https://ieeexplore.ieee.org/abstract/document/8951257?casa_token=GvHXHOd0xRMAAAAA:nLCjO5CC2syjKVbFuxTjny5Q3GZoF-son5de388iOrWjbrPdL_V80jbv-WoN7ZSJmwRTs84Vj5YY},
	doi = {10.1109/TNNLS.2019.2957109},
	abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
	number = {11},
	urldate = {2024-04-02},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
	month = nov,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Big data, Complexity theory, Computational modeling, Gaussian process regression (GPR), Ground penetrating radar, Kernel, Predictive models, Scalability, Sparse representation, local approximations, scalability, sparse approximations},
	pages = {4405--4423},
}

@article{wang_exact_2019,
	title = {Exact {Gaussian} {Processes} on a {Million} {Data} {Points}},
  author={Wang, Ke and Pleiss, Geoff and Gardner, Jacob and Tyree, Stephen and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{10.5555/2976248.2976406,
title = {{Sparse Gaussian Processes using Pseudo-inputs}},
  author={Snelson, Edward and Ghahramani, Zoubin},
  journal={Advances in neural information processing systems},
  volume={18},
  year={2005}
}


@article{7962ef51-7bc4-3567-8659-0594eb0b8b1f,
 ISSN = {00390526, 14679884},
 URL = {http://www.jstor.org/stable/2987588},
 abstract = {In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one forecaster can exploit the predictions of another forecaster to obtain a better score.},
 author = {Morris H. DeGroot and Stephen E. Fienberg},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 number = {1/2},
 pages = {12--22},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {{The Comparison and Evaluation of Forecasters}},
 urldate = {2024-04-03},
 volume = {32},
 year = {1983}
}

@article{ding_protein_2023,
  title = {Protein Design Using Structure-Based Residue Preferences},
  author = {Ding, David and Shaw, Ada Y. and Sinai, Sam and Rollins, Nathan and Prywes, Noam and Savage, David F. and Laub, Michael T. and Marks, Debora S.},
  year = {2024},
  month = feb,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {1639},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-45621-4},
  urldate = {2024-04-05},
  abstract = {Recent developments in protein design rely on large neural networks with up to 100s of millions of parameters, yet it is unclear which residue dependencies are critical for determining protein function. Here, we show that amino acid preferences at individual residues---without accounting for mutation interactions---explain much and sometimes virtually all of the combinatorial mutation effects across 8 datasets (R2 {\textasciitilde} 78-98\%). Hence, few observations ({\textasciitilde}100 times the number of mutated residues) enable accurate prediction of held-out variant effects (Pearson r\,{$>$}\,0.80). We hypothesized that the local structural contexts around a residue could be sufficient to predict mutation preferences, and develop an unsupervised approach termed CoVES (Combinatorial Variant Effects from Structure). Our results suggest that CoVES outperforms not just model-free methods but also similarly to complex models for creating functional and diverse protein variants. CoVES offers an effective alternative to complicated models for identifying functional protein mutations.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational models,Genetic engineering,High-throughput screening,Protein design,Synthetic biology},
}


@InProceedings{pmlr-v48-gal16,
  title = 	 {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}




@article{michael_systematic_2024,
  title = {{A Systematic Analysis of Regression Models for Protein Engineering}},
  author = {Michael, Richard and {K{\ae}stel-Hansen}, Jacob and Groth, Peter M{\o}rch and Bartels, Simon and Salomon, Jesper and Tian, Pengfei and Hatzakis, Nikos S. and Boomsma, Wouter},
  year = {2024},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {20},
  number = {5},
  pages = {e1012061},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1012061},
  urldate = {2024-05-07},
  abstract = {To optimize proteins for particular traits holds great promise for industrial and pharmaceutical purposes. Machine Learning is increasingly applied in this field to predict properties of proteins, thereby guiding the experimental optimization process. A natural question is: How much progress are we making with such predictions, and how important is the choice of regressor and representation? In this paper, we demonstrate that different assessment criteria for regressor performance can lead to dramatically different conclusions, depending on the choice of metric, and how one defines generalization. We highlight the fundamental issues of sample bias in typical regression scenarios and how this can lead to misleading conclusions about regressor performance. Finally, we make the case for the importance of calibrated uncertainty in this domain.},
  langid = {english},
  keywords = {Algorithms,Extrapolation,Language,Optimization,Protein engineering,Protein sequencing,Sequence alignment,Signal processing},
  file = {/Users/pmg/Zotero/storage/ND7GWCVP/Michael et al. - 2024 - A systematic analysis of regression models for pro.pdf}
}


@article{additivity,
author = {Reetz, Manfred T.},
title = {The Importance of Additive and Non-Additive Mutational Effects in Protein Engineering},
journal = {Angewandte Chemie International Edition},
volume = {52},
number = {10},
pages = {2658-2666},
keywords = {directed evolution, enzymes, mutational additivity, mutational non-additivity, saturation mutagenesis},
doi = {https://doi.org/10.1002/anie.201207842},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201207842},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.201207842},
abstract = {Additive or non-additive, that is the question: The increasing awareness of non-additive cooperative mutational effects in protein engineering is currently providing new theoretical and practical insights. In particular, the directed evolution of stereoselective enzymes as catalysts in organic chemistry and biotechnology profits from this intriguing development.},
year = {2013}
}

@article{amin2023biological,
  title={Biological sequence kernels with guaranteed flexibility},
  author={Amin, Alan Nawzad and Weinstein, Eli Nathan and Marks, Debora Susan},
  journal={arXiv preprint arXiv:2304.03775},
  year={2023}
}
@article{zeng2019quantification,
  title={{Quantification of Uncertainty in Peptide-MHC Binding Prediction Improves High-Affinity Peptide Selection for Therapeutic Design}},
  author={Zeng, Haoyang and Gifford, David K},
  journal={Cell systems},
  volume={9},
  number={2},
  pages={159--166},
  year={2019},
  publisher={Elsevier}
}
@article{liu2020simple,
  title={{Simple and Principled Uncertainty Estimation with
Deterministic Deep Learning via Distance Awareness}},
  author={Liu, Jeremiah and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax Weiss, Tania and Lakshminarayanan, Balaji},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={7498--7512},
  year={2020}
}

@article{ko2024tuna,
  title={{TUnA}: an uncertainty-aware transformer model for sequence-based protein--protein interaction prediction},
  author={Ko, Young Su and Parkinson, Jonathan and Liu, Cong and Wang, Wei},
  journal={Briefings in Bioinformatics},
  volume={25},
  number={5},
  pages={bbae359},
  year={2024},
  publisher={Oxford University Press}
}

@article{thaler2024active,
  title={{Active learning graph neural networks for partial charge prediction of metal-organic frameworks via dropout Monte Carlo}},
  author={Thaler, Stephan and Mayr, Felix and Thomas, Siby and Gagliardi, Alessio and Zavadlav, Julija},
  journal={npj Computational Materials},
  volume={10},
  number={1},
  pages={86},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{
li2023muben,
title={{MUB}en: Benchmarking the Uncertainty of Molecular Representation Models},
author={Yinghao Li and Lingkai Kong and Yuanqi Du and Yue Yu and Yuchen Zhuang and Wenhao Mu and Chao Zhang},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=qYceFeHgm4},
note={}
}
@inproceedings{wilson2016deep,
  title={Deep kernel learning},
  author={Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P},
  booktitle={Artificial intelligence and statistics},
  pages={370--378},
  year={2016},
  organization={PMLR}
}
@inproceedings{stanton2022accelerating,
  title={Accelerating bayesian optimization for biological sequence design with denoising autoencoders},
  author={Stanton, Samuel and Maddox, Wesley and Gruver, Nate and Maffettone, Phillip and Delaney, Emily and Greenside, Peyton and Wilson, Andrew Gordon},
  booktitle={International Conference on Machine Learning},
  pages={20459--20478},
  year={2022},
  organization={PMLR}
}
@article{toussaint2010exploiting,
  title={Exploiting physico-chemical properties in string kernels},
  author={Toussaint, Nora C. and Widmer, Christian and Kohlbacher, Oliver and R{\"a}tsch, Gunnar},
  journal={BMC bioinformatics},
  volume={11},
  pages={1--9},
  year={2010},
  publisher={Springer}
}
@article{yang2018learned,
  title={Learned protein embeddings for machine learning},
  author={Yang, Kevin K. and Wu, Zachary and Bedbrook, Claire N. and Arnold, Frances H.},
  journal={Bioinformatics},
  volume={34},
  number={15},
  pages={2642--2648},
  year={2018},
  publisher={Oxford University Press}
}
@article{nisonoff2023coherent,
  title={{Coherent Blending of Biophysics-Based Knowledge with Bayesian Neural Networks for Robust Protein Property Prediction}},
  author={Nisonoff, Hunter and Wang, Yixin and Listgarten, Jennifer},
  journal={ACS Synthetic Biology},
  volume={12},
  number={11},
  pages={3242--3251},
  year={2023},
  publisher={ACS Publications}
}
@article{hie2020leveraging,
  title={{Leveraging Uncertainty in Machine Learning Accelerates Biological Discovery and Design}},
  author={Hie, Brian and Bryson, Bryan D. and Berger, Bonnie},
  journal={Cell systems},
  volume={11},
  number={5},
  pages={461--477},
  year={2020},
  publisher={Elsevier}
}
@article{greenman2023benchmarking,
  title={Benchmarking uncertainty quantification for protein engineering},
  author={Greenman, Kevin P. and Amini, Ava P. and Yang, Kevin K.},
  journal={bioRxiv},
  pages={2023--04},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}
@article{parkinson2023linear,
  title={Linear-Scaling kernels for protein sequences and small molecules outperform deep learning while providing uncertainty quantitation and improved interpretability},
  author={Parkinson, Jonathan and Wang, Wei},
  journal={Journal of Chemical Information and Modeling},
  volume={63},
  number={15},
  pages={4589--4601},
  year={2023},
  publisher={ACS Publications}
}
@article{moss2020boss,
  title={{BOSS: Bayesian Optimization over String Spaces}},
  author={Moss, Henry and Leslie, David and Beck, Daniel and Gonzalez, Javier and Rayson, Paul},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={15476--15486},
  year={2020}
}
@article{mazzaferro2017predicting,
  title={Predicting protein binding affinity with word embeddings and recurrent neural networks},
  author={Mazzaferro, Carlo},
  journal={bioRxiv},
  pages={128223},
  year={2017},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{
su2023saprot,
title={{SaProt}: {Protein Language Modeling with Structure-aware Vocabulary}},
author={Jin Su and Chenchen Han and Yuyang Zhou and Junjie Shan and Xibin Zhou and Fajie Yuan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=6MRm3G4NiU}
}
@article{asgari2015continuous,
  title={{Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics}},
  author={Asgari, Ehsaneddin and Mofrad, Mohammad R. K.},
  journal={PloS one},
  volume={10},
  number={11},
  pages={e0141287},
  year={2015},
  publisher={Public Library of Science San Francisco, CA USA}
}
@incollection{leaver2011rosetta3,
  title={{ROSETTA3: an object-oriented software suite for the simulation and design of macromolecules}},
  author={Leaver-Fay, Andrew and Tyka, Michael and Lewis, Steven M. and Lange, Oliver F. and Thompson, James and Jacak, Ron and Kaufman, Kristian W. and Renfrew, P. Douglas and Smith, Colin A. and Sheffler, Will and others},
  booktitle={Methods in enzymology},
  volume={487},
  pages={545--574},
  year={2011},
  publisher={Elsevier}
}
@article{hie2022adaptive,
  title={Adaptive machine learning for protein engineering},
  author={Hie, Brian L and Yang, Kevin K},
  journal={Current opinion in structural biology},
  volume={72},
  pages={145--152},
  year={2022},
  publisher={Elsevier}
}
@article{greenhalgh2021machine,
  title={{Machine learning-guided acyl-ACP reductase engineering for improved in vivo fatty alcohol production}},
  author={Greenhalgh, Jonathan C. and Fahlberg, Sarah A. and Pfleger, Brian F. and Romero, Philip A.},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={5825},
  year={2021},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{gustafsson2020evaluating,
  title={{Evaluating Scalable Bayesian Deep Learning
Methods for Robust Computer Vision}},
  author={Gustafsson, Fredrik K. and Danelljan, Martin and Schon, Thomas B.},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={318--319},
  year={2020}
}
@article{tran2020methods,
  title={Methods for comparing uncertainty quantifications for material property predictions},
  author={Tran, Kevin and Neiswanger, Willie and Yoon, Junwoong and Zhang, Qingyang and Xing, Eric and Ulissi, Zachary W},
  journal={Machine Learning: Science and Technology},
  volume={1},
  number={2},
  pages={025006},
  year={2020},
  publisher={IOP Publishing}
}
@article{hirschfeld2020uncertainty,
  title={{Uncertainty Quantification Using Neural Networks for Molecular Property Prediction}},
  author={Hirschfeld, Lior and Swanson, Kyle and Yang, Kevin and Barzilay, Regina and Coley, Connor W},
  journal={Journal of Chemical Information and Modeling},
  volume={60},
  number={8},
  pages={3770--3780},
  year={2020},
  publisher={ACS Publications}
}
@article{fazekas2024locohd,
  title={{LoCoHD: a metric for comparing local environments of proteins}},
  author={Fazekas, Zsolt and K. Menyh{\'a}rd, D{\'o}ra and Perczel, Andr{\'a}s},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={4029},
  year={2024},
  publisher={Nature Publishing Group UK London}
}
@article{kulikova2021learning,
  title={Learning the local landscape of protein structures with convolutional neural networks},
  author={Kulikova, Anastasiya V. and Diaz, Daniel J. and Loy, James M. and Ellington, Andrew D. and Wilke, Claus O.},
  journal={Journal of Biological Physics},
  volume={47},
  number={4},
  pages={435--454},
  year={2021},
  publisher={Springer}
}
@article{shroff2020discovery,
  title={{Discovery of Novel Gain-of-Function Mutations Guided by Structure-Based Deep Learning}},
  author={Shroff, Raghav and Cole, Austin W. and Diaz, Daniel J. and Morrow, Barrett R. and Donnell, Isaac and Annapareddy, Ankur and Gollihar, Jimmy and Ellington, Andrew D. and Thyer, Ross},
  journal={ACS synthetic biology},
  volume={9},
  number={11},
  pages={2927--2935},
  year={2020},
  publisher={ACS Publications}
}

@article{torng20173d,
  title={{3D deep convolutional neural networks for amino acid environment similarity analysis}},
  author={Torng, Wen and Altman, Russ B.},
  journal={BMC bioinformatics},
  volume={18},
  pages={1--23},
  year={2017},
  publisher={Springer}
}

@phdthesis{duvenaud-thesis-2014,
   title = {Automatic Model Construction with {G}aussian Processes},
   author = {David Duvenaud},
   year = {2014},
   school = {{Computational and Biological Learning Laboratory, University of Cambridge}},
}
@inproceedings{
gao2023knowledge,
title={{KW-Design: Pushing the Limit of Protein Design via Knowledge Refinement}},
author={Zhangyang Gao and Cheng Tan and Xingran Chen and Yijie Zhang and Jun Xia and Siyuan Li and Stan Z. Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
urll ={https://openreview.net/forum?id=mpqMVWgqjn}
}
@article{ren2024accurate,
  title={{Accurate and robust protein sequence design with CarbonDesign}},
  author={Ren, Milong and Yu, Chungong and Bu, Dongbo and Zhang, Haicang},
  journal={Nature Machine Intelligence},
  volume={6},
  number={5},
  pages={536--547},
  year={2024},
  publisher={Nature Publishing Group}
}
@article{zhou2023prorefiner,
  title={{ProRefiner: an entropy-based refining strategy for inverse protein folding with global graph attention}},
  author={Zhou, Xinyi and Chen, Guangyong and Ye, Junjie and Wang, Ercheng and Zhang, Jun and Mao, Cong and Li, Zhanwei and Hao, Jianye and Huang, Xingxu and Tang, Jin and Heng, Pheng Ann},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={7434},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{10.1093/nar/gkad1011,
  title = {{AlphaFold Protein Structure Database in 2024: Providing Structure Coverage for over 214 Million Protein Sequences}},
  author = {Varadi, Mihaly and Bertoni, Damian and Magana, Paulyna and Paramval, Urmila and Pidruchna, Ivanna and Radhakrishnan, Malarvizhi and Tsenkov, Maxim and Nair, Sreenath and Mirdita, Milot and Yeo, Jingi and Kovalevskiy, Oleg and Tunyasuvunakool, Kathryn and Laydon, Agata and {\v Z}{\'i}dek, Augustin and Tomlinson, Hamish and Hariharan, Dhavanthi and Abrahamson, Josh and Green, Tim and Jumper, John and Birney, Ewan and Steinegger, Martin and Hassabis, Demis and Velankar, Sameer},
  year = {2023},
  month = nov,
  journal = {Nucleic Acids Research},
  volume = {52},
  number = {D1},
  eprint = {https://academic.oup.com/nar/article-pdf/52/D1/D368/55039845/gkad1011.pdf},
  pages = {D368-D375},
  issn = {0305-1048},
  doi = {10.1093/nar/gkad1011},
  abstract = {The AlphaFold Database Protein Structure Database (AlphaFold DB, https://alphafold.ebi.ac.uk) has significantly impacted structural biology by amassing over 214 million predicted protein structures, expanding from the initial 300k structures released in 2021. Enabled by the groundbreaking AlphaFold2 artificial intelligence (AI) system, the predictions archived in AlphaFold DB have been integrated into primary data resources such as PDB, UniProt, Ensembl, InterPro and MobiDB. Our manuscript details subsequent enhancements in data archiving, covering successive releases encompassing model organisms, global health proteomes, Swiss-Prot integration, and a host of curated protein datasets. We detail the data access mechanisms of AlphaFold DB, from direct file access via FTP to advanced queries using Google Cloud Public Datasets and the programmatic access endpoints of the database. We also discuss the improvements and services added since its initial release, including enhancements to the Predicted Aligned Error viewer, customisation options for the 3D viewer, and improvements in the search engine of AlphaFold DB.The AlphaFold Protein Structure Database (AlphaFold DB) is a massive digital library of predicted protein structures, with over 214 million entries, marking a 500-times expansion in size since its initial release in 2021. The structures are predicted using Google DeepMind's AlphaFold 2 artificial intelligence (AI) system. Our new report highlights the latest updates we have made to this database. We have added more data on specific organisms and proteins related to global health and expanded to cover almost the complete UniProt database, a primary data resource of protein sequences. We also made it easier for our users to access the data by directly downloading files or using advanced cloud-based tools. Finally, we have also improved how users view and search through these protein structures, making the user experience smoother and more informative. In short, AlphaFold DB has been growing rapidly and has become more user-friendly and robust to support the broader scientific community.}
}
