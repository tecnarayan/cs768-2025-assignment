@book{russell2016artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart J and Norvig, Peter},
  year={2016},
  publisher={Malaysia; Pearson Education Limited,}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@incollection{hadfield2016cooperative,
title = {Cooperative Inverse Reinforcement Learning},
author = {Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
booktitle = {Advances in Neural Information Processing Systems 29},
pages = {3909--3917},
year = {2016}
}

@article{arora2018survey,
  title={A survey of inverse reinforcement learning: Challenges, methods and progress},
  author={Arora, Saurabh and Doshi, Prashant},
  journal={arXiv preprint arXiv:1806.06877},
  year={2018}
}

@article{fard2011non,
  title={Non-deterministic policies in Markovian decision processes},
  author={Fard, M Milani and Pineau, Joelle},
  journal={Journal of Artificial Intelligence Research},
  volume={40},
  pages={1--24},
  year={2011}
}

@incollection{fisac2020pragmatic,
  title={Pragmatic-pedagogic value alignment},
  author={Fisac, Jaime F and Gates, Monica A and Hamrick, Jessica B and Liu, Chang and Hadfield-Menell, Dylan and Palaniappan, Malayandi and Malik, Dhruv and Sastry, S Shankar and Griffiths, Thomas L and Dragan, Anca D},
  booktitle={Robotics Research},
  pages={49--57},
  year={2020},
  publisher={Springer}
}


@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Jul},
  pages={1633--1685},
  year={2009}
}

@inproceedings{hadfield2017inverse,
  title={Inverse reward design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart J and Dragan, Anca},
  booktitle={Advances in neural information processing systems},
  pages={6765--6774},
  year={2017}
}


@inproceedings{huang2018establishing,
  title={Establishing appropriate trust via critical states},
  author={Huang, Sandy H and Bhatia, Kush and Abbeel, Pieter and Dragan, Anca D},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3929--3936},
  year={2018},
  organization={IEEE}
}

@inproceedings{huang2018learning,
  title={Learning safe policies with expert guidance},
  author={Huang, Jessie and Wu, Fa and Precup, Doina and Cai, Yang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9105--9114},
  year={2018}
}

@article{paulraj2010comparative,
  title={A comparative study of redundant constraints identification methods in linear programming problems},
  author={Paulraj, S and Sumathi, P and others},
  journal={Mathematical Problems in Engineering},
  year={2010},
  publisher={Hindawi}
}


@inproceedings{amin2017repeated,
  title={Repeated inverse reinforcement learning},
  author={Amin, Kareem and Jiang, Nan and Singh, Satinder},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1815--1824},
  year={2017}
}

@article{amin2016towards,
  title={Towards resolving unidentifiability in inverse reinforcement learning},
  author={Amin, Kareem and Singh, Satinder},
  journal={arXiv preprint arXiv:1601.06569},
  year={2016}
}


@article{dayan1993improving,
  title={Improving generalization for temporal difference learning: The successor representation},
  author={Dayan, Peter},
  journal={Neural Computation},
  volume={5},
  number={4},
  pages={613--624},
  year={1993},
  publisher={MIT Press}
}


@inproceedings{sadigh2017active,
    author = "Sadigh, Dorsa and Dragan, Anca D. and Sastry, S. Shankar and Seshia, Sanjit A.",
    title = "Active Preference-Based Learning of Reward Functions",
    booktitle = "Proceedings of Robotics: Science and Systems ({RSS})",
    year = "2017",
    month = "July",
    doi = "10.15607/RSS.2017.XIII.053"
}



@article{gunning2017explainable,
  title={Explainable artificial intelligence (XAI)},
  author={Gunning, David},
  year={2017},
  publisher={Defense Advanced Research Projects Agency (DARPA)}
}

@inproceedings{barreto2017successor,
  title={Successor features for transfer in reinforcement learning},
  author={Barreto, Andr{\'e} and Dabney, Will and Munos, R{\'e}mi and Hunt, Jonathan J and Schaul, Tom and van Hasselt, Hado P and Silver, David},
  booktitle={Advances in neural information processing systems},
  pages={4055--4065},
  year={2017}
}


@article{valiant1979complexity,
  title={The complexity of computing the permanent},
  author={Valiant, Leslie G},
  journal={Theoretical computer science},
  volume={8},
  number={2},
  pages={189--201},
  year={1979},
  publisher={Elsevier}
}

@inproceedings{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4299--4307},
  year={2017}
}

@article{zhu2018overview,
  title={An overview of machine teaching},
  author={Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and Rafferty, Anna N},
  journal={arXiv preprint arXiv:1801.05927},
  year={2018}
}


@inproceedings{stone2010ad,
  title={Ad Hoc Autonomous Agent Teams: Collaboration without Pre-Coordination.},
  author={Stone, Peter and Kaminka, Gal A and Kraus, Sarit and Rosenschein, Jeffrey S and others},
  booktitle={AAAI},
  year={2010}
}


@inproceedings{ho2016showing,
  title={Showing versus doing: Teaching by demonstration},
  author={Ho, Mark K and Littman, Michael and MacGlashan, James and Cushman, Fiery and Austerweil, Joseph L},
  booktitle={Advances In Neural Information Processing Systems},
  pages={3027--3035},
  year={2016}
}

@inproceedings{shafto2008teaching,
  title={Teaching games: Statistical sampling assumptions for learning in pedagogical situations},
  author={Shafto, Patrick and Goodman, Noah},
  booktitle={Proceedings of the 30th annual conference of the Cognitive Science Society},
  pages={1632--1637},
  year={2008}
}


@article{fu2017learning,
  title={Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  journal={arXiv preprint arXiv:1710.11248},
  year={2017}
}

@inproceedings{zheng2014robust,
  title={Robust Bayesian Inverse Reinforcement Learning with Sparse Behavior Noise.},
  author={Zheng, Jiangchuan and Liu, Siyuan and Ni, Lionel M},
  year={2014}
}



@inproceedings{huang2017enabling,
  title={Enabling robots to communicate their objectives},
  author={Huang, Sandy H and Held, David and Abbeel, Pieter and Dragan, Anca D},
  booktitle={Robotics: Science and Systems},
  year={2017}
}


@article{wolsey1982analysis,
  title={An analysis of the greedy algorithm for the submodular set covering problem},
  author={Wolsey, Laurence A},
  journal={Combinatorica},
  volume={2},
  number={4},
  pages={385--393},
  year={1982},
  publisher={Springer}
}



@inproceedings{balbach2009recent,
  title={Recent developments in algorithmic teaching},
  author={Balbach, Frank J and Zeugmann, Thomas},
  booktitle={International Conference on Language and Automata Theory and Applications},
  pages={1--18},
  year={2009}
}

@article{goldman1995complexity,
  title={On the complexity of teaching},
  author={Goldman, Sally A and Kearns, Michael J},
  journal={Journal of Computer and System Sciences},
  volume={50},
  number={1},
  pages={20--31},
  year={1995},
  publisher={Elsevier}
}





@article{smith1984efficient,
  title={Efficient Monte Carlo procedures for generating points uniformly distributed over bounded regions},
  author={Smith, Robert L},
  journal={Operations Research},
  volume={32},
  number={6},
  pages={1296--1308},
  year={1984},
  publisher={INFORMS}
}

@inproceedings{melo2010analysis,
  title={Analysis of Inverse Reinforcement Learning with Perturbed Demonstrations.},
  author={Melo, Francisco S and Lopes, Manuel and Ferreira, Ricardo},
  booktitle={ECAI},
  pages={349--354},
  year={2010}
}


@inproceedings{neu2007apprenticeship,
  title={Apprenticeship learning using inverse reinforcement learing and gradient methods},
  author={Neu, Gergely and Szepesv{\'a}ri, Csaba},
  booktitle={Proc. of 23rd Conference Annual Conference on Uncertainty in Artificial Intelligence},
  pages={295--302},
  year={2007}
}



@inproceedings{mei2015using,
  title={Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners.},
  author={Mei, Shike and Zhu, Xiaojin},
  booktitle={AAAI},
  pages={2871--2877},
  year={2015}
}

@article{doliwa2014recursive,
  title={Recursive teaching dimension, VC-dimension and sample compression},
  author={Doliwa, Thorsten and Fan, Gaojian and Simon, Hans Ulrich and Zilles, Sandra},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={3107--3131},
  year={2014},
  publisher={JMLR. org}
}

@article{gao2017preference,
  title={Preference-based teaching},
  author={Gao, Ziyuan and Ries, Christoph and Simon, Hans U and Zilles, Sandra},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={31},
  pages={1--32},
  year={2017}
}

@inproceedings{brown2019machine,
  title={Machine teaching for inverse reinforcement learning: Algorithms and applications},
  author={Brown, Daniel S. and Niekum, Scott},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={7749--7758},
  year={2019}
}


@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@inproceedings{brown2017toward,
  title={Toward Probabilistic Safety Bounds for Robot Learning from Demonstration.},
  author={Brown, Daniel S. and Niekum, Scott},
  booktitle={AAAI Fall Symposium on AI for HRI},
  year={2017}
}

@article{settles2012active,
  title={Active learning},
  author={Settles, Burr},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={6},
  number={1},
  pages={1--114},
  year={2012},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{zhang2009policy,
  title={Policy teaching through reward function learning},
  author={Zhang, Haoqi and Parkes, David C and Chen, Yiling},
  booktitle={Proceedings of the 10th ACM conference on Electronic commerce},
  pages={295--304},
  year={2009},
  organization={ACM}
}

@inproceedings{rathnasabapathy2006exact,
  title={Exact solutions of interactive POMDPs using behavioral equivalence},
  author={Rathnasabapathy, Bharanee and Doshi, Prashant and Gmytrasiewicz, Piotr},
  booktitle={Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems},
  pages={1025--1032},
  year={2006}
}

@article{zeng2012exploiting,
  title={Exploiting model equivalences for solving interactive dynamic influence diagrams},
  author={Zeng, Yifeng and Doshi, Prashant},
  journal={Journal of Artificial Intelligence Research},
  volume={43},
  pages={211--255},
  year={2012}
}

@inproceedings{bobu2018learning,
  title={Learning under Misspecified Objective Spaces},
  author={Bobu, Andreea and Bajcsy, Andrea and Fisac, Jaime F and Dragan, Anca D},
  booktitle={Conference on Robot Learning},
  pages={796--805},
  year={2018}
}


@article{bobu2020quantifying,
  title={Quantifying hypothesis space misspecification in learning from human--robot demonstrations and physical corrections},
  author={Bobu, Andreea and Bajcsy, Andrea and Fisac, Jaime F and Deglurkar, Sampada and Dragan, Anca D},
  journal={IEEE Transactions on Robotics},
  volume={36},
  number={3},
  pages={835--854},
  year={2020},
  publisher={IEEE}
}

@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes and others},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017},
  publisher={Journal of Machine Learning Research/Massachusetts Institute of Technology~…}
}


@inproceedings{laskin2020curl,
  title={Curl: Contrastive unsupervised representations for reinforcement learning},
  author={Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={5639--5650},
  year={2020},
  organization={PMLR}
}


@article{shahbenefits,
  title={Benefits of Assistance over Reward Learning},
  author={Shah, Rohin and Freire, Pedro and Alex, Neel and Freedman, Rachel and Krasheninnikov, Dmitrii and Chan, Lawrence and Dennis, Michael and Abbeel, Pieter and Dragan, Anca and Russell, Stuart},
  journal={Workshop on Cooperative AI (Cooperative AI @ NeurIPS},
  year={2020}
}


@inproceedings{bobu2021feature,
  title={Feature Expansive Reward Learning: Rethinking Human Input},
  author={Bobu, Andreea and Wiggert, Marius and Tomlin, Claire and Dragan, Anca D},
  booktitle={Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={216--224},
  year={2021}
}



@inproceedings{singla2014near,
  title={Near-Optimally Teaching the Crowd to Classify.},
  author={Singla, Adish and Bogunovic, Ilija and Bart{\'o}k, G{\'a}bor and Karbasi, Amin and Krause, Andreas},
  booktitle={ICML},
  pages={154--162},
  year={2014}
}



@inproceedings{brown2018risk,
  title={Risk-Aware Active Inverse Reinforcement Learning},
  author={Brown, Daniel S and Cui, Yuchen and Niekum, Scott},
  booktitle={Proceedings of the 2nd Annual Conference on Robot Learning (CoRL)},
  year={2018}
}

@inproceedings{babes2011apprenticeship,
  title={Apprenticeship learning about multiple intentions},
  author={Babes, Monica and Marivate, Vukosi and Subramanian, Kaushik and Littman, Michael L},
  booktitle={Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
  pages={897--904},
  year={2011}
}


@article{nemhauser1978analysis,
  title={An analysis of approximations for maximizing submodular set functions—I},
  author={Nemhauser, George L and Wolsey, Laurence A and Fisher, Marshall L},
  journal={Mathematical Programming},
  volume={14},
  number={1},
  pages={265--294},
  year={1978},
  publisher={Springer}
}


@article{michini2015bayesian,
  title={Bayesian nonparametric reward learning from demonstration},
  author={Michini, Bernard and Walsh, Thomas J and Agha-Mohammadi, Ali-Akbar and How, Jonathan P},
  journal={IEEE Transactions on Robotics},
  volume={31},
  number={2},
  pages={369--386},
  year={2015},
  publisher={IEEE}
}


@article{simonovits2003compute,
  title={How to compute the volume in high dimension?},
  author={Simonovits, Mikl{\'o}s},
  journal={Mathematical programming},
  volume={97},
  number={1},
  pages={337--374},
  year={2003},
  publisher={Springer}
}


@inproceedings{alfeld2017explicit,
  title={Explicit Defense Actions Against Test-Set Attacks.},
  author={Alfeld, Scott and Zhu, Xiaojin and Barford, Paul},
  booktitle={AAAI},
  pages={1274--1280},
  year={2017}
}




@inproceedings{sadigh2016information,
  title={Information gathering actions over human internal state},
  author={Sadigh, Dorsa and Sastry, S Shankar and Seshia, Sanjit A and Dragan, Anca},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={66--73},
  year={2016}
}

@article{green2009reversible,
  title={Reversible jump MCMC},
  author={Green, Peter J and Hastie, David I},
  journal={Genetics},
  volume={155},
  number={3},
  pages={1391--1403},
  year={2009}
}



@inproceedings{zhu2015machine,
  title={Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education.},
  author={Zhu, Xiaojin},
  booktitle={AAAI},
  pages={4083--4087},
  year={2015}
}

@inproceedings{zhu2013machine,
  title={Machine teaching for bayesian learners in the exponential family},
  author={Zhu, Xiaojin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1905--1913},
  year={2013}
}

@article{liu2016teaching,
  title={The teaching dimension of linear learners},
  author={Liu, Ji and Zhu, Xiaojin},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={162},
  pages={1--25},
  year={2016}
}

@inproceedings{levine2010feature,
  title={Feature construction for inverse reinforcement learning},
  author={Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1342--1350},
  year={2010}
}


@inproceedings{levine2011nonlinear,
  title={Nonlinear inverse reinforcement learning with gaussian processes},
  author={Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={19--27},
  year={2011}
}


@inproceedings{syed2008game,
  title={A game-theoretic approach to apprenticeship learning},
  author={Syed, Umar and Schapire, Robert E},
  booktitle={Advances in neural information processing systems},
  pages={1449--1456},
  year={2008}
}

@inproceedings{cuiactive2017,
  title={Active Learning from Critiques via Bayesian Inverse Reinforcement Learning},
  author={Cui, Yuchen and Niekum, Scott},
  booktitle={Robotics: Science and Systems Workshop on Mathematical Models, Algorithms, and Human-Robot Interaction},
  year={2017}
}


@inproceedings{cohn2011comparing,
  title={Comparing action-query strategies in semi-autonomous agents},
  author={Cohn, Robert and Durfee, Edmund and Singh, Satinder},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 3},
  pages={1287--1288},
  year={2011}
}


@inproceedings{lopes2007affordance,
  title={Affordance-based imitation learning in robots},
  author={Lopes, Manuel and Melo, Francisco S and Montesano, Luis},
  booktitle={Intelligent Robots and Systems, 2007. IROS 2007. IEEE/RSJ International Conference on},
  pages={1015--1021},
  year={2007},
  organization={IEEE}
}


@article{lopes2009computational,
  title={A computational model of social-learning mechanisms},
  author={Lopes, Manuel and Melo, Francisco S and Kenward, Ben and Santos-Victor, Jos{\'e}},
  journal={Adaptive Behavior},
  volume={17},
  number={6},
  pages={467--483},
  year={2009},
  publisher={Sage Publications Sage UK: London, England}
}

@inproceedings{michini2012bayesian,
  title={Bayesian nonparametric inverse reinforcement learning},
  author={Michini, Bernard and How, Jonathan P},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={148--163},
  year={2012},
  organization={Springer}
}

@article{rothkopf2013modular,
  title={Modular inverse reinforcement learning for visuomotor behavior},
  author={Rothkopf, Constantin A and Ballard, Dana H},
  journal={Biological cybernetics},
  volume={107},
  number={4},
  pages={477--490},
  year={2013},
  publisher={Springer}
}

@article{baker2009action,
  title={Action understanding as inverse planning},
  author={Baker, Chris L and Saxe, Rebecca and Tenenbaum, Joshua B},
  journal={Cognition},
  volume={113},
  number={3},
  pages={329--349},
  year={2009},
  publisher={Elsevier}
}


@inproceedings{karasev2016intent,
  title={Intent-aware long-term prediction of pedestrian motion},
  author={Karasev, Vasiliy and Ayvaci, Alper and Heisele, Bernd and Soatto, Stefano},
  booktitle={Robotics and Automation (ICRA), 2016 IEEE International Conference on},
  pages={2543--2549},
  year={2016},
  organization={IEEE}
}

@article{kim2016socially,
  title={Socially adaptive path planning in human environments using inverse reinforcement learning},
  author={Kim, Beomjoon and Pineau, Joelle},
  journal={International Journal of Social Robotics},
  volume={8},
  number={1},
  pages={51--66},
  year={2016},
  publisher={Springer}
}








@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

@article{van1972application,
  title={An application of Fourier methods to the problem of sharpening the Berry-Esseen inequality},
  author={van Beek, Paul},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte Gebiete},
  volume={23},
  number={3},
  pages={187--196},
  year={1972},
  publisher={Springer}
}


@article{santara2017rail,
  title={RAIL: Risk-Averse Imitation Learning},
  author={Santara, Anirban and Naik, Abhishek and Ravindran, Balaraman and Das, Dipankar and Mudigere, Dheevatsa and Avancha, Sasikanth and Kaul, Bharat},
  journal={arXiv preprint arXiv:1707.06658},
  year={2017}
}

@book{jorion1997value,
  title={Value at risk},
  author={Jorion, Philippe},
  year={1997},
  publisher={McGraw-Hill, New York}
}

@article{rockafellar2000optimization,
  title={Optimization of conditional value-at-risk},
  author={Rockafellar, R Tyrrell and Uryasev, Stanislav},
  journal={Journal of risk},
  volume={2},
  pages={21--42},
  year={2000}
}


@article{hansen2006cma,
  title={The CMA evolution strategy: a comparing review},
  author={Hansen, Nikolaus},
  journal={Towards a new evolutionary computation},
  pages={75--102},
  year={2006},
  publisher={Springer}
}


@inproceedings{brown2018efficient,
   author = {{Brown}, Daniel~S. and {Niekum}, Scott},
    title = "{Efficient Probabilistic Performance Bounds for Inverse Reinforcement Learning}",
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{brown2016exact,
  title={Exact and Heuristic Algorithms for Risk-Aware Stochastic Physical Search},
  author={Brown, Daniel S and Hudack, Jeffrey and Gemelli, Nathaniel and Banerjee, Bikramjit},
  journal={Computational Intelligence},
  year={2016},
  publisher={Wiley Online Library}
}

@inproceedings{chow2015risk,
  title={Risk-sensitive and robust decision-making: a cvar optimization approach},
  author={Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1522--1530},
  year={2015}
}

@inproceedings{tamar2015optimizing,
  title={Optimizing the CVaR via sampling},
  author={Tamar, Aviv and Glassner, Yonatan and Mannor, Shie},
  booktitle={Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  pages={2993--2999},
  year={2015},
  organization={AAAI Press}
}

@inproceedings{ho2016model,
  title={Model-free imitation learning with policy optimization},
  author={Ho, Jonathan and Gupta, Jayesh and Ermon, Stefano},
  booktitle={International Conference on Machine Learning},
  pages={2760--2769},
  year={2016}
}


@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={springer}
}


@article{Argall2009,
abstract = {We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Argall, Brenna D. and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
doi = {10.1016/j.robot.2008.10.024},
file = {:home/dsbrown/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Argall et al. - 2009 - A survey of robot learning from demonstration.pdf:pdf},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Autonomous systems,Learning from demonstration,Machine learning,Robotics},
number = {5},
pages = {469--483},
pmid = {21045796},
publisher = {Elsevier B.V.},
title = {{A survey of robot learning from demonstration}},
url = {http://dx.doi.org/10.1016/j.robot.2008.10.024},
volume = {57},
year = {2009}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart J},
  booktitle={ICML},
  pages={663--670},
  year={2000}
}

@inproceedings{ratliff2006maximum,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006},
  organization={ACM}
}



@inproceedings{ramachandran2007bayesian,
  title={Bayesian inverse reinforcement learning},
  author={Ramachandran, Deepak and Amir, Eyal},
  booktitle={Proceedings of the 20th International Joint Conference on Artifical intelligence},
  pages={2586--2591},
  year={2007}
}

@inproceedings{choi2013bayesian,
  title={Bayesian Nonparametric Feature Construction for Inverse Reinforcement Learning.},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  booktitle={IJCAI},
  pages={1287--1293},
  year={2013}
}


@inproceedings{choi2011map,
  title={Map inference for bayesian inverse reinforcement learning},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1989--1997},
  year={2011}
}

@inproceedings{cakmak2012algorithmic,
  title={Algorithmic and Human Teaching of Sequential Decision Tasks.},
  author={Cakmak, Maya and Lopes, Manuel},
  booktitle={AAAI},
  year={2012}
}


@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  organization={ACM}
}

@inproceedings{thomas2015improvement,
  title={High Confidence Policy Improvement},
  author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},
  pages={2380--2388},
  year={2015}
}



@inproceedings{lopes2009active,
  title={Active learning for reward estimation in inverse reinforcement learning},
  author={Lopes, Manuel and Melo, Francisco and Montesano, Luis},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={31--46},
  year={2009},
  organization={Springer}
}


@inproceedings{michini2012improving,
  title={Improving the efficiency of Bayesian inverse reinforcement learning},
  author={Michini, Bernard and How, Jonathan P},
  booktitle={Robotics and Automation (ICRA), 2012 IEEE International Conference on},
  pages={3651--3656},
  year={2012},
  organization={IEEE}
}

@inproceedings{ziebart2008maximum,
  title={Maximum Entropy Inverse Reinforcement Learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={AAAI},
  year={2008}
}

@article{gao2012survey,
  title={A survey of inverse reinforcement learning techniques},
  author={Gao, Yang and Peters, Jan and Tsourdos, Antonios and Zhifei, Shao and Meng Joo, Er},
  journal={International Journal of Intelligent Computing and Cybernetics},
  volume={5},
  number={3},
  pages={293--311},
  year={2012},
  publisher={Emerald Group Publishing Limited}
}

@book{hollander1999nonparametric,
  title={Nonparametric Statistical Methods: By Myles Hollander, Douglas A. Wolfe},
  author={Hollander, Myles and Wolfe, Douglas A},
  year={1999},
  publisher={J. Wiley}
}

@article{barthe2005probabilistic,
  title={A probabilistic approach to the geometry of the $\ell^n_p$-ball},
  author={Barthe, Franck and Gu{\'e}don, Olivier and Mendelson, Shahar and Naor, Assaf},
  journal={The Annals of Probability},
  volume={33},
  number={2},
  pages={480--513},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{zheng2014robust,
  title={Robust Bayesian Inverse Reinforcement Learning with Sparse Behavior Noise.},
  author={Zheng, Jiangchuan and Liu, Siyuan and Ni, Lionel M},
  booktitle={AAAI},
  pages={2198--2205},
  year={2014}
}

@article{weissteinball,
  title={Ball Point Picking},
  author={Weisstein, EW},
  journal={From MathWorld--A Wolfram Web Resource. http://mathworld. wolfram. com/BallPointPicking. html},
  year={2017}
}

@inproceedings{thomas2015high,
  title={High-Confidence Off-Policy Evaluation.},
  author={Thomas, Philip S and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={AAAI},
  pages={3000--3006},
  year={2015}
}

@article{hanna2016high,
  title={High Confidence Off-Policy Evaluation with Models},
  author={Hanna, Josiah P and Stone, Peter and Niekum, Scott},
  journal={arXiv preprint arXiv:1606.06126},
  year={2016}
}

@inproceedings{pirotta2016inverse,
  title={Inverse Reinforcement Learning through Policy Gradient Minimization.},
  author={Pirotta, Matteo and Restelli, Marcello},
  booktitle={AAAI},
  year={2016}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@inproceedings{lacotte2019risk,
  title={Risk-Sensitive Generative Adversarial Imitation Learning},
  author={Lacotte, Jonathan and Ghavamzadeh, Mohammad and Chow, Yinlam and Pavone, Marco},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2154--2163},
  year={2019}
}

@inproceedings{akrour2011preference,
  title={Preference-based policy learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={12--27},
  year={2011},
  organization={Springer}
}

@inproceedings{wilson2012bayesian,
  title={A bayesian approach for policy learning from trajectory preference queries},
  author={Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
  booktitle={Advances in neural information processing systems},
  pages={1133--1141},
  year={2012}
}



@article{leike2017ai,
  title={AI safety gridworlds},
  author={Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
  journal={arXiv preprint arXiv:1711.09883},
  year={2017}
}

@inproceedings{jacq2019learning,
  title={Learning from a Learner},
  author={Jacq, Alexis and Geist, Matthieu and Paiva, Ana and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={2990--2999},
  year={2019}
}

@inproceedings{de2019causal,
  title={Causal confusion in imitation learning},
  author={de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11693--11704},
  year={2019}
}

@article{markowitz1952portfolio,
  title={Portfolio selection},
  author={Markowitz, Harry},
  journal={The journal of finance},
  volume={7},
  number={1},
  pages={77--91},
  year={1952},
  publisher={Wiley Online Library}
}

@article{thananjeyan2019safety,
  title={Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks},
  author={Thananjeyan, Brijen and Balakrishna, Ashwin and Rosolia, Ugo and Li, Felix and McAllister, Rowan and Gonzalez, Joseph E and Levine, Sergey and Borrelli, Francesco and Goldberg, Ken},
  journal={arXiv preprint arXiv:1905.13402},
  year={2019}
}

@article{volkovs2014new,
  title={New learning methods for supervised and unsupervised preference aggregation},
  author={Volkovs, Maksims N and Zemel, Richard S},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1135--1176},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{szummer2011semi,
  title={Semi-supervised learning to rank with preference regularization},
  author={Szummer, Martin and Yilmaz, Emine},
  booktitle={Proceedings of the 20th ACM international conference on Information and knowledge management},
  pages={269--278},
  year={2011}
}



@inproceedings{ghavamzadeh2016safe,
  title={Safe policy improvement by minimizing robust baseline regret},
  author={Ghavamzadeh, Mohammad and Petrik, Marek and Chow, Yinlam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2298--2306},
  year={2016}
}

@article{prasad2019defining,
  title={Defining Admissible Rewards for High Confidence Policy Evaluation},
  author={Prasad, Niranjani and Engelhardt, Barbara E and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1905.13167},
  year={2019}
}


@article{eckersley2018impossibility,
  title={Impossibility and Uncertainty Theorems in AI Value Alignment (or why your AGI should not have a utility function)},
  author={Eckersley, Peter},
  journal={arXiv preprint arXiv:1901.00064},
  year={2018}
}

@inproceedings{milli2017should,
  title={Should robots be obedient?},
  author={Milli, Smitha and Hadfield-Menell, Dylan and Dragan, Anca and Russell, Stuart},
  booktitle={Proceedings of the 26th International Joint Conference on Artificial Intelligence},
  pages={4754--4760},
  year={2017}
}

@article{russell2015research,
  title={Research priorities for robust and beneficial artificial intelligence},
  author={Russell, Stuart and Dewey, Daniel and Tegmark, Max},
  journal={Ai Magazine},
  volume={36},
  number={4},
  pages={105--114},
  year={2015}
}


@inproceedings{hanna2017grounded,
  title={Grounded action transformation for robot learning in simulation},
  author={Hanna, Josiah P and Stone, Peter},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@article{ghasemipour2019divergence,
  title={A Divergence Minimization Perspective on Imitation Learning Methods},
  author={Ghasemipour, Seyed Kamyar Seyed and Zemel, Richard and Gu, Shixiang},
  journal={arXiv preprint arXiv:1911.02256},
  year={2019}
}

@article{sun2019functional,
  title={Functional variational bayesian neural networks},
  author={Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  journal={arXiv preprint arXiv:1903.05779},
  year={2019}
}

@article{mackay1992practical,
  title={A practical Bayesian framework for backpropagation networks},
  author={MacKay, David JC},
  journal={Neural computation},
  volume={4},
  number={3},
  pages={448--472},
  year={1992},
  publisher={MIT Press}
}









@InProceedings{pmlr-v87-biyik18a,
  title = 	 {Batch Active Preference-Based Learning of Reward Functions},
  author = 	 {Biyik, Erdem and Sadigh, Dorsa},
  year = 	 {2018},
  publisher = 	 {PMLR},
}


@article{khan2018fast,
  title={Fast and scalable bayesian deep learning by weight-perturbation in adam},
  author={Khan, Mohammad Emtiyaz and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
  journal={arXiv preprint arXiv:1806.04854},
  year={2018}
}

@article{pradier2018projected,
  title={Projected BNNs: Avoiding weight-space pathologies by learning latent representations of neural network weights},
  author={Pradier, Melanie F and Pan, Weiwei and Yao, Jiayu and Ghosh, Soumya and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1811.07006},
  year={2018}
}



@article{finn2016connection,
  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},
  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.03852},
  year={2016}
}

@inproceedings{makhzani2017pixelgan,
  title={Pixelgan autoencoders},
  author={Makhzani, Alireza and Frey, Brendan J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1975--1985},
  year={2017}
}

@article{doersch2016tutorial,
  title={Tutorial on variational autoencoders},
  author={Doersch, Carl},
  journal={arXiv preprint arXiv:1606.05908},
  year={2016}
}

@article{thananjeyan2019extending,
  title={Extending Deep Model Predictive Control with Safety Augmented Value Estimation from Demonstrations},
  author={Thananjeyan, Brijen and Balakrishna, Ashwin and Rosolia, Ugo and Li, Felix and McAllister, Rowan and Gonzalez, Joseph E and Levine, Sergey and Borrelli, Francesco and Goldberg, Ken},
  journal={arXiv preprint arXiv:1905.13402},
  year={2019}
}


@inproceedings{oh2015action,
  title={Action-conditional video prediction using deep networks in atari games},
  author={Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  booktitle={Advances in neural information processing systems},
  pages={2863--2871},
  year={2015}
}



@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@inproceedings{safedagger,
  title={Query-efficient imitation learning for end-to-end simulated driving},
  author={Zhang, Jiakai and Cho, Kyunghyun},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}


@inproceedings{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4565--4573},
  year={2016}
}

@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@book{von2007theory,
  title={Theory of games and economic behavior (commemorative edition)},
  author={Von Neumann, John and Morgenstern, Oskar},
  year={2007},
  publisher={Princeton university press}
}

@article{sarafian2018safe,
  title={Safe Policy Learning from Observations},
  author={Sarafian, Elad and Tamar, Aviv and Kraus, Sarit},
  journal={arXiv preprint arXiv:1805.07805},
  year={2018}
}

@article{bain1999framework,
  title={A framework for behavioural claning},
  author={Bain, Michael and Sommut, Claude},
  journal={Machine intelligence},
  volume={15},
  number={15},
  pages={103},
  year={1999}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}



@article{laskey2017dart,
  title={DART: Noise Injection for Robust Imitation Learning},
  author={Laskey, M and Lee, J and Fox, R and Dragan, A and Goldberg, K},
  journal={Conference on Robot Learning (CoRL)},
  year={2017}
}

@inproceedings{maddox2019simple,
  title={A simple baseline for bayesian uncertainty in deep learning},
  author={Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13132--13143},
  year={2019}
}

@article{bobu2018learning,
  title={Learning under misspecified objective spaces},
  author={Bobu, Andreea and Bajcsy, Andrea and Fisac, Jaime F and Dragan, Anca D},
  journal={arXiv preprint arXiv:1810.05157},
  year={2018}
}


@inproceedings{xie2019towards,
  title={Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling},
  author={Xie, Tengyang and Ma, Yifei and Wang, Yu-Xiang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9665--9675},
  year={2019}
}

@article{littman1995complexity,
  title={On the complexity of solving Markov decision problems},
  author={Littman, Michael L and Dean, Thomas L and Kaelbling, Leslie Pack},
  journal={Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
  year={1995}
}


@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}


@inproceedings{kendall2017uncertainties,
  title={What uncertainties do we need in bayesian deep learning for computer vision?},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in neural information processing systems},
  pages={5574--5584},
  year={2017}
}


@inproceedings{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in neural information processing systems},
  pages={6402--6413},
  year={2017}
}



@inproceedings{biyik2019asking,
  title={Asking Easy Questions: A User-Friendly Approach to Active Reward Learning},
  author={B{\i}y{\i}k, Erdem and Palan, Malayandi and Landolfi, Nicholas C and Losey, Dylan P and Sadigh, Dorsa},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2019}
}

@inproceedings{liu2016stein,
  title={Stein variational gradient descent: A general purpose bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances in neural information processing systems},
  pages={2378--2386},
  year={2016}
}


@inproceedings{kalakrishnan2013learning,
  title={Learning objective functions for manipulation},
  author={Kalakrishnan, Mrinal and Pastor, Peter and Righetti, Ludovic and Schaal, Stefan},
  booktitle={2013 IEEE International Conference on Robotics and Automation},
  pages={1331--1336},
  year={2013},
  organization={IEEE}
}


@inproceedings{boularias2011relative,
  title={Relative entropy inverse reinforcement learning},
  author={Boularias, Abdeslam and Kober, Jens and Peters, Jan},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={182--189},
  year={2011}
}


@inproceedings{browngoo2019trex,
  title={Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations},
  author={Brown, Daniel S. and Goo, Wonjoon and Nagarajan Prabhat and Niekum, Scott},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019},
  year={2019}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}


@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}


@article{pomerleau1991efficient,
  title={Efficient training of artificial neural networks for autonomous navigation},
  author={Pomerleau, Dean A},
  journal={Neural Computation},
  volume={3},
  number={1},
  pages={88--97},
  year={1991},
  publisher={MIT Press}
}

@book{sutton1998introduction,
  title={Introduction to reinforcement learning},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={135},
  year={1998},
  publisher={MIT press Cambridge}
}


@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}



@article{osa2018algorithmic,
  title={An algorithmic perspective on imitation learning},
  author={Osa, Takayuki and Pajarinen, Joni and Neumann, Gerhard and Bagnell, J Andrew and Abbeel, Pieter and Peters, Jan and others},
  journal={Foundations and Trends{\textregistered} in Robotics},
  volume={7},
  number={1-2},
  pages={1--179},
  year={2018},
  publisher={Now Publishers, Inc.}
}

@article{gao2018reinforcement,
  title={Reinforcement learning from imperfect demonstrations},
  author={Gao, Yang and Lin, Ji and Yu, Fisher and Levine, Sergey and Darrell, Trevor and others},
  journal={arXiv preprint arXiv:1802.05313},
  year={2018}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}


@article{menda2018ensembledagger,
  title={EnsembleDAgger: A Bayesian Approach to Safe Imitation Learning},
  author={Menda, Kunal and Driggs-Campbell, Katherine and Kochenderfer, Mykel J},
  journal={arXiv preprint arXiv:1807.08364},
  year={2018}
}

@article{lee2018safe,
  title={Safe end-to-end imitation learning for model predictive control},
  author={Lee, Keuntaek and Saigol, Kamil and Theodorou, Evangelos A},
  journal={arXiv preprint arXiv:1803.10231},
  year={2018}
}


@article{rajeswaran2017learning,
  title={Learning complex dexterous manipulation with deep reinforcement learning and demonstrations},
  author={Rajeswaran, Aravind and Kumar, Vikash and Gupta, Abhishek and Vezzani, Giulia and Schulman, John and Todorov, Emanuel and Levine, Sergey},
  journal={arXiv preprint arXiv:1709.10087},
  year={2017}
}

@inproceedings{palan2019learning,
 title={Learning Reward Functions by Integrating Human Demonstrations and Preferences},
 author={Palan, Malayandi and Landolfi, Nicholas C. and Shevchuk, Gleb and Sadigh, Dorsa},
 booktitle={Proceedings of Robotics: Science and Systems (RSS)},
 year={2019},
 month={June}
}

@inproceedings{eric2008active,
  title={Active preference learning with discrete choice data},
  author={Eric, Brochu and Freitas, Nando D and Ghosh, Abhijeet},
  booktitle={Advances in neural information processing systems},
  pages={409--416},
  year={2008}
}


@article{choi2019robust,
  title={Robust Learning From Demonstrations With Mixed Qualities Using Leveraged Gaussian Processes},
  author={Choi, Sungjoon and Lee, Kyungjae and Oh, Songhwai},
  journal={IEEE Transactions on Robotics},
  year={2019},
  publisher={IEEE}
}


@article{ziebart2010modeling,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement Learning with Deep Energy-Based Policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1352--1361},
  year={2017}
}


@article{ezzeddine2018combination,
  title={Combination of learning from non-optimal demonstrations and feedbacks using inverse reinforcement learning and Bayesian policy improvement},
  author={Ezzeddine, Ali and Mourad, Nafee and Araabi, Babak Nadjar and Ahmadabadi, Majid Nili},
  journal={Expert Systems with Applications},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{knox2009interactively,
  title={Interactively shaping agents via human reinforcement: The TAMER framework},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={Proceedings of the fifth international conference on Knowledge capture},
  pages={9--16},
  year={2009},
  organization={ACM}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}


@article{warnell2017deep,
  title={Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces},
  author={Warnell, Garrett and Waytowich, Nicholas and Lawhern, Vernon and Stone, Peter},
  journal={arXiv preprint arXiv:1709.10163},
  year={2017}
}


@inproceedings{macglashan2017interactive,
  title={Interactive Learning from Policy-Dependent Human Feedback},
  author={MacGlashan, James and Ho, Mark K and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L and Taylor, Matthew E and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={2285--2294},
  year={2017}
}


@inproceedings{cui2018active,
  title={Active Reward Learning from Critiques},
  author={Cui, Yuchen and Niekum, Scott},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2018}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={ICML},
  volume={99},
  pages={278--287},
  year={1999}
}


@article{leike2018scalable,
  title={Scalable agent alignment via reward modeling: a research direction},
  author={Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  journal={arXiv preprint arXiv:1811.07871},
  year={2018}
}

@book{luce2012individual,
  title={Individual choice behavior: A theoretical analysis},
  author={Luce, R Duncan},
  year={2012},
  publisher={Courier Corporation}
}



@inproceedings{sermanet2018time,
  title={Time-contrastive networks: Self-supervised learning from video},
  author={Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1134--1141},
  year={2018},
  organization={IEEE}
}



@inproceedings{brown2019drex,
  title={Better-than-Demonstrator Imitation Learning via Automaticaly-Ranked Demonstrations},
  author={Brown, Daniel S and Goo, Wonjoon and Niekum, Scott},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2019}
}

@inproceedings{chaplot2016transfer,
  title={Transfer deep reinforcement learning in 3d environments: An empirical study},
  author={Chaplot, Devendra Singh and Lample, Guillaume and Sathyendra, Kanthashree Mysore and Salakhutdinov, Ruslan},
  booktitle={NIPS Deep Reinforcemente Leaning Workshop},
  year={2016}
}

@article{taylor2009transfer,
  title={Transfer learning for reinforcement learning domains: A survey},
  author={Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Jul},
  pages={1633--1685},
  year={2009}
}



@inproceedings{liu2018imitation,
  title={Imitation from observation: Learning to imitate behaviors from raw video via context translation},
  author={Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1118--1125},
  year={2018},
  organization={IEEE}
}




@inproceedings{kober2009policy,
  title={Policy search for motor primitives in robotics},
  author={Kober, Jens and Peters, Jan R},
  booktitle={Advances in neural information processing systems},
  pages={849--856},
  year={2009}
}

@inproceedings{taylor2011integrating,
  title={Integrating reinforcement learning with human demonstrations of varying ability},
  author={Taylor, Matthew E and Suay, Halit Bener and Chernova, Sonia},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
  pages={617--624},
  year={2011},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}


@article{hester2017deep,
  title={Deep Q-learning from Demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Dulac-Arnold, Gabriel and others},
  journal={arXiv preprint arXiv:1704.03732},
  year={2017}
}




@article{thomas2017ensuring,
  title={On Ensuring that Intelligent Machines Are Well-Behaved},
  author={Thomas, Philip S and da Silva, Bruno Castro and Barto, Andrew G and Brunskill, Emma},
  journal={arXiv preprint arXiv:1708.05448},
  year={2017}
}


@inproceedings{hanna2017bootstrapping,
  title={Bootstrapping with models: Confidence intervals for off-policy evaluation},
  author={Hanna, Josiah P and Stone, Peter and Niekum, Scott},
  booktitle={Proceedings of the 16th Conference on Autonomous Agents and Multiagent Systems},
  year={2017}
}

@inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  year={2016}
}

@article{wulfmeier2015maximum,
  title={Maximum entropy deep inverse reinforcement learning},
  author={Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
  journal={arXiv preprint arXiv:1507.04888},
  year={2015}
}


@article{follmer2011entropic,
  title={Entropic risk measures: Coherence vs. convexity, model ambiguity and robust large deviations},
  author={F{\"o}llmer, Hans and Knispel, Thomas},
  journal={Stochastics and Dynamics},
  volume={11},
  number={02n03},
  pages={333--351},
  year={2011},
  publisher={World Scientific}
}

@article{ogryczak1999stochastic,
  title={From stochastic dominance to mean-risk models: Semideviations as risk measures},
  author={Ogryczak, W{\l}odzimierz and Ruszczy{\'n}ski, Andrzej},
  journal={European Journal of Operational Research},
  volume={116},
  number={1},
  pages={33--50},
  year={1999},
  publisher={Elsevier}
}



@inproceedings{levine2011nonlinear,
  title={Nonlinear inverse reinforcement learning with gaussian processes},
  author={Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
  booktitle={Advances in Neural Information Processing Systems},
  year={2011}
}


@inproceedings{syed2008game,
  title={A game-theoretic approach to apprenticeship learning},
  author={Syed, Umar and Schapire, Robert E},
  booktitle={Advances in neural information processing systems},
  pages={1449--1456},
  year={2008}
}

@inproceedings{cohn2011comparing,
  title={Comparing action-query strategies in semi-autonomous agents},
  author={Cohn, Robert and Durfee, Edmund and Singh, Satinder},
  booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems},
  year={2011}
}


@inproceedings{lopes2007affordance,
  title={Affordance-based imitation learning in robots},
  author={Lopes, Manuel and Melo, Francisco S and Montesano, Luis},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={1015--1021},
  year={2007}
}

@inproceedings{babes2011apprenticeship,
  title={Apprenticeship learning about multiple intentions},
  author={Babes, Monica and Marivate, Vukosi and Subramanian, Kaushik and Littman, Michael L},
  booktitle={Proceedings of the 28th International Conference on Machine Learning},
  year={2011}
}

@article{lopes2009computational,
  title={A computational model of social-learning mechanisms},
  author={Lopes, Manuel and Melo, Francisco S and Kenward, Ben and Santos-Victor, Jos{\'e}},
  journal={Adaptive Behavior},
  volume={17},
  number={6},
  pages={467--483},
  year={2009},
  publisher={Sage Publications Sage UK: London, England}
}

@inproceedings{michini2012bayesian,
  title={Bayesian nonparametric inverse reinforcement learning},
  author={Michini, Bernard and How, Jonathan P},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  year={2012}
}

@article{rothkopf2013modular,
  title={Modular inverse reinforcement learning for visuomotor behavior},
  author={Rothkopf, Constantin A and Ballard, Dana H},
  journal={Biological cybernetics},
  volume={107},
  number={4},
  pages={477--490},
  year={2013}
}

@article{baker2009action,
  title={Action understanding as inverse planning},
  author={Baker, Chris L and Saxe, Rebecca and Tenenbaum, Joshua B},
  journal={Cognition},
  volume={113},
  number={3},
  pages={329--349},
  year={2009},
  publisher={Elsevier}
}


@inproceedings{karasev2016intent,
  title={Intent-aware long-term prediction of pedestrian motion},
  author={Karasev, Vasiliy and Ayvaci, Alper and Heisele, Bernd and Soatto, Stefano},
  booktitle={IEEE International Conference on Robotics and Automation},
  pages={2543--2549},
  year={2016}
}

@article{kim2016socially,
  title={Socially adaptive path planning in human environments using inverse reinforcement learning},
  author={Kim, Beomjoon and Pineau, Joelle},
  journal={International Journal of Social Robotics},
  volume={8},
  number={1},
  pages={51--66},
  year={2016},
  publisher={Springer}
}


@article{van1972application,
  title={An application of Fourier methods to the problem of sharpening the Berry-Esseen inequality},
  author={van Beek, Paul},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte Gebiete},
  volume={23},
  number={3},
  pages={187--196},
  year={1972},
  publisher={Springer}
}


@article{santara2017rail,
  title={RAIL: Risk-Averse Imitation Learning},
  author={Santara, Anirban and Naik, Abhishek and Ravindran, Balaraman and Das, Dipankar and Mudigere, Dheevatsa and Avancha, Sasikanth and Kaul, Bharat},
  journal={arXiv preprint arXiv:1707.06658},
  year={2017}
}

@book{jorion1997value,
  title={Value at risk},
  author={Jorion, Philippe},
  year={1997},
  publisher={McGraw-Hill, New York}
}

@article{rockafellar2000optimization,
  title={Optimization of conditional value-at-risk},
  author={Rockafellar, R Tyrrell and Uryasev, Stanislav},
  journal={Journal of risk},
  volume={2},
  pages={21--42},
  year={2000}
}


@article{hansen2006cma,
  title={The CMA evolution strategy: a comparing review},
  author={Hansen, Nikolaus},
  journal={Towards a new evolutionary computation},
  pages={75--102},
  year={2006},
  publisher={Springer}
}


@ARTICLE{brown2017efficient,
   author = {{Brown}, D.~S. and {Niekum}, S.},
    title = "{Efficient Probabilistic Performance Bounds for Inverse Reinforcement Learning}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1707.00724},
 primaryClass = "cs.AI",
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170700724B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}


@article{brown2016exact,
  title={Exact and Heuristic Algorithms for Risk-Aware Stochastic Physical Search},
  author={Brown, Daniel S and Hudack, Jeffrey and Gemelli, Nathaniel and Banerjee, Bikramjit},
  journal={Computational Intelligence},
  year={2016},
  publisher={Wiley Online Library}
}



@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@inproceedings{greydanus2018visualizing,
  title={Visualizing and Understanding Atari Agents},
  author={Greydanus, Samuel and Koul, Anurag and Dodge, Jonathan and Fern, Alan},
  booktitle={International Conference on Machine Learning},
  pages={1787--1796},
  year={2018}
}


@InProceedings{torabi2018behavioral,
  author = {Faraz Torabi and Garrett Warnell and Peter Stone},
  title = {Behavioral Cloning from Observation},
  booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)},
  location = {Stockholm, Sweden},
  month = {July},
  year = {2018},
  abstract = {
Humans often learn how to perform tasks via imitation: they observe others 
perform a task, and then very quickly infer the appropriate actions to take 
based on their observations. While extending this paradigm to autonomous 
agents is a well-studied problem in general, there are two particular aspects 
that have largely been overlooked: (1) that the learning is done from 
observation only (i.e., without explicit action information), and (2) that 
the learning is typically done very quickly. In this work, we propose a 
two-phase, autonomous imitation learning technique called behavioral cloning 
from observation (BCO), that aims to provide improved performance with respect 
to both of these aspects. First, we allow the agent to acquire experience in a 
self-supervised fashion. This experience is used to develop a model which is 
then utilized to learn a particular task by observing an expert perform that 
task without the knowledge of the specific actions taken. We experimentally 
compare BCO to imitation learning methods, including the state-of-the-art, 
generative adversarial imitation learning (GAIL) technique, and we show 
comparable task performance in several different simulation domains while 
exhibiting increased learning speed after expert trajectories become available. 
  },
  wwwnote={Also available from <a href="https://arxiv.org/abs/1805.01954">arXiv</a>},
}

@article{yu2018one,
  title={One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning},
  author={Yu, Tianhe and Finn, Chelsea and Xie, Annie and Dasari, Sudeep and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.01557},
  year={2018}
}

@inproceedings{goo2019one,
  title     = {One-Shot Learning of Multi-Step Tasks from Observation via Activity Localization in Auxiliary Video},
  author    = {Wonjoon Goo and Scott Niekum},
  year      = {2019},
  booktitle = {2019 IEEE International Conference on Robotics and Automation (ICRA)},
  tppubtype = {inproceedings}
}

@article{torabi2018generative,
  title={Generative adversarial imitation from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1807.06158},
  year={2018}
}


@inproceedings{henderson2018optiongan,
  title={Optiongan: Learning joint reward-policy options using generative adversarial inverse reinforcement learning},
  author={Henderson, Peter and Chang, Wei-Di and Bacon, Pierre-Luc and Meger, David and Pineau, Joelle and Precup, Doina},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{kurin2017atari,
  title={The atari grand challenge dataset},
  author={Kurin, Vitaly and Nowozin, Sebastian and Hofmann, Katja and Beyer, Lucas and Leibe, Bastian},
  journal={arXiv preprint arXiv:1705.10998},
  year={2017}
}

@inproceedings{ibarz2018reward,
  title={Reward learning from human preferences and demonstrations in Atari},
  author={Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}


@inproceedings{chow2015risk,
  title={Risk-sensitive and robust decision-making: a cvar optimization approach},
  author={Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
  booktitle={Advances in Neural Information Processing Systems},
  year={2015}
}

@inproceedings{tamar2015optimizing,
  title={Optimizing the CVaR via sampling},
  author={Tamar, Aviv and Glassner, Yonatan and Mannor, Shie},
  booktitle={Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  pages={2993--2999},
  year={2015}
}

@inproceedings{ho2016model,
  title={Model-free imitation learning with policy optimization},
  author={Ho, Jonathan and Gupta, Jayesh and Ermon, Stefano},
  booktitle={International Conference on Machine Learning},
  pages={2760--2769},
  year={2016}
}


@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={springer}
}


@article{Argall2009,
  title={A survey of robot learning from demonstration},
  author={Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  journal={Robotics and autonomous systems},
  volume={57},
  number={5},
  pages={469--483},
  year={2009},
  publisher={Elsevier}
}

@article{petrik2019beyond,
  title={Beyond Confidence Regions: Tight Bayesian Ambiguity Sets for Robust MDPs},
  author={Petrik, Marek and Russell, Reazul Hasan},
  journal={arXiv preprint arXiv:1902.07605},
  year={2019}
}


@inproceedings{hanna2019importance,
  title={Importance Sampling Policy Evaluation with an Estimated Behavior Policy},
  author={Hanna, Josiah and Niekum, Scott and Stone, Peter},
  booktitle={Proceedings of the 36th International Conference on Machine Learning (ICML)},
  year={2019},
  month={June},
  abstract={
We consider the problem of off-policy evaluation in Markov decision processes. Off-policy evaluation is the task of evaluating the expected return of one policy with data generated by a different, behavior policy. Importance sampling is a technique for off-policy evaluation that re-weights off-policy returns to account for differences in the likelihood of the returns between the two policies. In this paper, we study importance sampling with an estimated behavior policy where the behavior policy estimate comes from the same set of data used to compute the importance sampling estimate. We find that this estimator often lowers the mean squared error of off-policy evaluation compared to importance sampling with the true behavior policy or using a behavior policy that is estimated from a separate data set. Intuitively, estimating the behavior policy in this way corrects for error due to sampling in the action-space. Our empirical results also extend to other popular variants of importance sampling and show that estimating a non-Markovian behavior policy can further lower large-sample mean squared error even when the true behavior policy is Markovian.

}
}

@inproceedings{ramachandran2007bayesian,
  title={Bayesian inverse reinforcement learning},
  author={Ramachandran, Deepak and Amir, Eyal},
  booktitle={Proceedings of the 20th International Joint Conference on Artifical intelligence},
  pages={2586--2591},
  year={2007}
}

@inproceedings{choi2011map,
  title={Map inference for bayesian inverse reinforcement learning},
  author={Choi, Jaedeug and Kim, Kee-Eung},
  booktitle={Advances in Neural Information Processing Systems},
  year={2011}
}



@inproceedings{thomas2015improvement,
  title={High Confidence Policy Improvement},
  author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning},
  pages={2380--2388},
  year={2015}
}



@inproceedings{michini2012improving,
  title={Improving the efficiency of Bayesian inverse reinforcement learning},
  author={Michini, Bernard and How, Jonathan P},
  booktitle={IEEE International Conference on Robotics and Automation},
  pages={3651--3656},
  year={2012}
}

@article{gao2012survey,
  title={A survey of inverse reinforcement learning techniques},
  author={Gao, Yang and Peters, Jan and Tsourdos, Antonios and Zhifei, Shao and Meng Joo, Er},
  journal={International Journal of Intelligent Computing and Cybernetics},
  volume={5},
  number={3},
  pages={293--311},
  year={2012},
  publisher={Emerald Group Publishing Limited}
}

@book{hollander1999nonparametric,
  title={Nonparametric Statistical Methods: By Myles Hollander, Douglas A. Wolfe},
  author={Hollander, Myles and Wolfe, Douglas A},
  year={1999},
  publisher={J. Wiley}
}

@article{barthe2005probabilistic,
  title={A probabilistic approach to the geometry of the $\ell^n_p$-ball},
  author={Barthe, Franck and Gu{\'e}don, Olivier and Mendelson, Shahar and Naor, Assaf and others},
  journal={The Annals of Probability},
  volume={33},
  number={2},
  pages={480--513},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{zheng2014robust,
  title={Robust Bayesian Inverse Reinforcement Learning with Sparse Behavior Noise.},
  author={Zheng, Jiangchuan and Liu, Siyuan and Ni, Lionel M},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={2198--2205},
  year={2014}
}

@inproceedings{shiarlis2016inverse,
  title={Inverse reinforcement learning from failure},
  author={Shiarlis, Kyriacos and Messias, Joao and Whiteson, Shimon},
  booktitle={Proceedings of the 2016 International Conference on Autonomous Agents \& Multiagent Systems},
  year={2016}
}


@inproceedings{grollman2011donut,
  title={Donut as I do: Learning from failed demonstrations},
  author={Grollman, Daniel H and Billard, Aude},
  booktitle={Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  pages={3804--3809},
  year={2011},
  organization={IEEE}
}

@article{meltzoff1995understanding,
  title={Understanding the intentions of others: re-enactment of intended acts by 18-month-old children.},
  author={Meltzoff, Andrew N},
  journal={Developmental psychology},
  volume={31},
  number={5},
  pages={838},
  year={1995},
  publisher={US: American Psychological Association}
}



@article{weissteinball,
  title={Ball Point Picking},
  author={Weisstein, EW},
  journal={From MathWorld--A Wolfram Web Resource. http://mathworld. wolfram. com/BallPointPicking. html},
  year={2017}
}


@article{hanna2016high,
  title={High Confidence Off-Policy Evaluation with Models},
  author={Hanna, Josiah P and Stone, Peter and Niekum, Scott},
  journal={arXiv preprint arXiv:1606.06126},
  year={2016}
}




@article{rlfdfail,
  title={Robot learning from failed demonstrations},
  author={Grollman, Daniel H and Billard, Aude G},
  journal={International Journal of Social Robotics},
  volume={4},
  number={4},
  pages={331--342},
  year={2012},
  publisher={Springer}
}

@inproceedings{multidonut,
  title={Learning from failed demonstrations in unreliable systems},
  author={Rai, Akshara and De Chambrier, Guillaume and Billard, Aude},
  booktitle={2013 13th IEEE-RAS International Conference on Humanoid Robots (Humanoids)},
  pages={410--416},
  year={2013},
  organization={IEEE}
}


@inproceedings{donut,
  title={Donut as I do: Learning from failed demonstrations},
  author={Grollman, Daniel H and Billard, Aude},
  booktitle={Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  pages={3804--3809},
  year={2011},
  organization={IEEE}
}

@inproceedings{activeirl,
  title={Active advice seeking for inverse reinforcement learning},
  author={Odom, Phillip and Natarajan, Sriraam},
  booktitle={Proceedings of the 2016 International Conference on Autonomous Agents \& Multiagent Systems},
  pages={512--520},
  year={2016},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@inproceedings{activeirl2,
  title={April: Active preference learning-based reinforcement learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={116--131},
  year={2012},
  organization={Springer}
}

@inproceedings{scorebasedirl,
  title={Score-based Inverse Reinforcement Learning},
  author={El Asri, Layla and Piot, Bilal and Geist, Matthieu and Laroche, Romain and Pietquin, Olivier},
  booktitle={Proceedings of the 2016 International Conference on Autonomous Agents \& Multiagent Systems},
  pages={457--465},
  year={2016},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@inproceedings{irldialog,
  title={Preference-learning based Inverse Reinforcement Learning for Dialog Control.},
  author={Sugiyama, Hiroaki and Meguro, Toyomi and Minami, Yasuhiro},
  booktitle={INTERSPEECH},
  pages={222--225},
  year={2012}
}

@inproceedings{pbirl,
  title={Model-free preference-based reinforcement learning},
  author={Wirth, Christian and F{\"u}rnkranz, Johannes and Neumann, Gerhard},
  booktitle={Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  pages={2222--2228},
  year={2016},
  organization={AAAI Press}
}

@article{preferencethesis,
  title={Preference-driven demonstration ranking for inverse reinforcement learning},
  author={van der Wijden, Renée},
  year={2016}
}

@inproceedings{trajpreferences,
  title={Learning trajectory preferences for manipulators via iterative improvement},
  author={Jain, Ashesh and Wojcik, Brian and Joachims, Thorsten and Saxena, Ashutosh},
  booktitle={Advances in neural information processing systems},
  pages={575--583},
  year={2013}
}

@inproceedings{preferencepolicylearning,
  title={Preference-based policy learning},
  author={Akrour, Riad and Schoenauer, Marc and Sebag, Michele},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={12--27},
  year={2011},
  organization={Springer}
}


@inproceedings{cao2007learning,
  title={Learning to rank: from pairwise approach to listwise approach},
  author={Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  year={2007}
}



@inproceedings{chen2009ranking,
  title={Ranking measures and loss functions in learning to rank},
  author={Chen, Wei and Liu, Tie-Yan and Lan, Yanyan and Ma, Zhi-Ming and Li, Hang},
  booktitle={Advances in Neural Information Processing Systems},
  pages={315--323},
  year={2009}
}

@article{humancheckpoint,
  title={Playing Atari games with deep reinforcement learning and human checkpoint replay},
  author={Hosu, Ionel-Alexandru and Rebedea, Traian},
  journal={arXiv preprint arXiv:1607.05077},
  year={2016}
}

@inproceedings{armstrong2018occam,
  title={Occam's razor is insufficient to infer the preferences of irrational agents},
  author={Armstrong, Stuart and Mindermann, S{\"o}ren},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5598--5609},
  year={2018}
}

@article{stooke2020decoupling,
  title={Decoupling Representation Learning from Reinforcement Learning},
  author={Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  journal={arXiv preprint arXiv:2009.08319},
  year={2020}
}

@article{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
   booktitle={International Conference on Machine Learning},
 year = {2020}
}



@incollection{brown2020safe,
 abstract = {Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by first pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference via sampling. We evaluate our proposed approach on the task of learning to play Atari games from demonstrations, without access to the game score, and achieve state-of-the-art imitation learning performance. Furthermore, we also demonstrate that our approach enables efficient high-confidence performance bounds for any evaluation policy. We show that these high-confidence performance bounds can be used to accurately rank the performance and risk of a variety of different evaluation policies, despite not having samples of the true reward function. },
 author = {Brown, Daniel S. and Niekum, Scott and Coleman, Russell and Srinivasan, Ravi},
 booktitle={International Conference on Machine Learning},
 title = {Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences},
 year = {2020}
}



@inproceedings{irlvideogames,
    title={Inverse reinforcement learning for video games},
    author={Aaron Tucker and Adam Gleave and Stuart Russell},
    year={2018},
    booktitle={Proceedings of the Workshop on Deep Reinforcement Learning at NeurIPS},
}

@inproceedings{dmirl,
  title={Distance Minimization for Reward Learning from Scored Trajectories.},
  author={Burchfiel, Benjamin and Tomasi, Carlo and Parr, Ronald},
  booktitle={AAAI},
  pages={3330--3336},
  year={2016}
}

@article{preferencesurvey,
  author  = {Christian Wirth and Riad Akrour and Gerhard Neumann and Johannes F{{{\"u}}}rnkranz},
  title   = {A Survey of Preference-Based Reinforcement Learning Methods},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {136},
  pages   = {1-46}
}

@article{pohlen2018observe,
  title={Observe and Look Further: Achieving Consistent Performance on Atari},
  author={Pohlen, Tobias and Piot, Bilal and Hester, Todd and Azar, Mohammad Gheshlaghi and Horgan, Dan and Budden, David and Barth-Maron, Gabriel and van Hasselt, Hado and Quan, John and Ve{\v{c}}er{\'\i}k, Mel and others},
  journal={arXiv preprint arXiv:1805.11593},
  year={2018}
}

@inproceedings{cheng2011preference,
  title={Preference-based policy iteration: Leveraging preference learning for reinforcement learning},
  author={Cheng, Weiwei and F{\"u}rnkranz, Johannes and H{\"u}llermeier, Eyke and Park, Sang-Hyeun},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={312--327},
  year={2011},
  organization={Springer}
}

@article{singh1994upper,
  title={An upper bound on the loss from approximate optimal-value functions},
  author={Singh, Satinder P and Yee, Richard C},
  journal={Machine Learning},
  volume={16},
  number={3},
  pages={227--233},
  year={1994},
  publisher={Springer}
}


@article{imitationyoutube,
  title={Playing hard exploration games by watching YouTube},
  author={Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and de Freitas, Nando},
  journal={arXiv preprint arXiv:1805.11592},
  year={2018}
}

@article{airl,
  title={Learning Robust Rewards with Adversarial Inverse Reinforcement Learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  journal={arXiv preprint arXiv:1710.11248},
  year={2017}
}

@article{qureshi2018adversarial,
  title={Adversarial Imitation via Variational Inverse Reinforcement Learning},
  author={Qureshi, Ahmed H and Yip, Michael C},
  journal={arXiv preprint arXiv:1809.06404},
  year={2018}
}

@article{dqn,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}


@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={ICML},
  year={2016}
}

@article{littman2017environment,
  title={Environment-independent task specifications via GLTL},
  author={Littman, Michael L and Topcu, Ufuk and Fu, Jie and Isbell, Charles and Wen, Min and MacGlashan, James},
  journal={arXiv preprint arXiv:1704.04341},
  year={2017}
}

@article{brown2019deep,
  title={Deep bayesian reward learning from preferences},
  author={Brown, Daniel S and Niekum, Scott},
  journal={NeurIPS Workshop on Safety and Robustness in Decision Making },
  year={2020}
}

@article{BACHRACH2020103356,
title = "Negotiating team formation using deep reinforcement learning",
journal = "Artificial Intelligence",
volume = "288",
pages = "103356",
year = "2020",
issn = "0004-3702",
doi = "https://doi.org/10.1016/j.artint.2020.103356",
url = "http://www.sciencedirect.com/science/article/pii/S0004370220301077",
author = "Yoram Bachrach and Richard Everett and Edward Hughes and Angeliki Lazaridou and Joel Z. Leibo and Marc Lanctot and Michael Johanson and Wojciech M. Czarnecki and Thore Graepel",
keywords = "Multi-agent systems, Team formation, Coalition formation, Reinforcement learning, Deep learning, Cooperative games, Shapley value",
abstract = "When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning. Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory. Additionally, we investigate how the physical location of agents influences negotiation outcomes."
}

@inproceedings{ijcai2020-220,
  title     = {Boolean Games: Inferring Agents' Goals Using Taxation Queries},
  author    = {Adiga, Abhijin and Kraus, Sarit and Maksimov, Oleg and Ravi, S. S.},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  year      = {2020}
}

