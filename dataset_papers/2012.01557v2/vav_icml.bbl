\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, pp.\ ~1. ACM, 2004.

\bibitem[Amin \& Singh(2016)Amin and Singh]{amin2016towards}
Amin, K. and Singh, S.
\newblock Towards resolving unidentifiability in inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1601.06569}, 2016.

\bibitem[Amin et~al.(2017)Amin, Jiang, and Singh]{amin2017repeated}
Amin, K., Jiang, N., and Singh, S.
\newblock Repeated inverse reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1815--1824, 2017.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and
  Man{\'e}, D.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Arora \& Doshi(2018)Arora and Doshi]{arora2018survey}
Arora, S. and Doshi, P.
\newblock A survey of inverse reinforcement learning: Challenges, methods and
  progress.
\newblock \emph{arXiv preprint arXiv:1806.06877}, 2018.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt,
  and Silver]{barreto2017successor}
Barreto, A., Dabney, W., Munos, R., Hunt, J.~J., Schaul, T., van Hasselt,
  H.~P., and Silver, D.
\newblock Successor features for transfer in reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4055--4065, 2017.

\bibitem[Biyik \& Sadigh(2018)Biyik and Sadigh]{pmlr-v87-biyik18a}
Biyik, E. and Sadigh, D.
\newblock Batch active preference-based learning of reward functions.
\newblock PMLR, 2018.

\bibitem[B{\i}y{\i}k et~al.(2019)B{\i}y{\i}k, Palan, Landolfi, Losey, and
  Sadigh]{biyik2019asking}
B{\i}y{\i}k, E., Palan, M., Landolfi, N.~C., Losey, D.~P., and Sadigh, D.
\newblock Asking easy questions: A user-friendly approach to active reward
  learning.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2019.

\bibitem[Bobu et~al.(2020)Bobu, Bajcsy, Fisac, Deglurkar, and
  Dragan]{bobu2020quantifying}
Bobu, A., Bajcsy, A., Fisac, J.~F., Deglurkar, S., and Dragan, A.~D.
\newblock Quantifying hypothesis space misspecification in learning from
  human--robot demonstrations and physical corrections.
\newblock \emph{IEEE Transactions on Robotics}, 36\penalty0 (3):\penalty0
  835--854, 2020.

\bibitem[Bobu et~al.(2021)Bobu, Wiggert, Tomlin, and Dragan]{bobu2021feature}
Bobu, A., Wiggert, M., Tomlin, C., and Dragan, A.~D.
\newblock Feature expansive reward learning: Rethinking human input.
\newblock In \emph{Proceedings of the 2021 ACM/IEEE International Conference on
  Human-Robot Interaction}, pp.\  216--224, 2021.

\bibitem[Brown \& Niekum(2019)Brown and Niekum]{brown2019machine}
Brown, D.~S. and Niekum, S.
\newblock Machine teaching for inverse reinforcement learning: Algorithms and
  applications.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  7749--7758, 2019.

\bibitem[Brown et~al.(2019)Brown, Goo, and Niekum]{brown2019drex}
Brown, D.~S., Goo, W., and Niekum, S.
\newblock Better-than-demonstrator imitation learning via automaticaly-ranked
  demonstrations.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2019.

\bibitem[Brown et~al.(2020)Brown, Niekum, Coleman, and
  Srinivasan]{brown2020safe}
Brown, D.~S., Niekum, S., Coleman, R., and Srinivasan, R.
\newblock Safe imitation learning via fast bayesian reward inference from
  preferences.
\newblock In \emph{International Conference on Machine Learning}. 2020.

\bibitem[Cakmak \& Lopes(2012)Cakmak and Lopes]{cakmak2012algorithmic}
Cakmak, M. and Lopes, M.
\newblock Algorithmic and human teaching of sequential decision tasks.
\newblock In \emph{AAAI}, 2012.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4299--4307, 2017.

\bibitem[Fisac et~al.(2020)Fisac, Gates, Hamrick, Liu, Hadfield-Menell,
  Palaniappan, Malik, Sastry, Griffiths, and Dragan]{fisac2020pragmatic}
Fisac, J.~F., Gates, M.~A., Hamrick, J.~B., Liu, C., Hadfield-Menell, D.,
  Palaniappan, M., Malik, D., Sastry, S.~S., Griffiths, T.~L., and Dragan,
  A.~D.
\newblock Pragmatic-pedagogic value alignment.
\newblock In \emph{Robotics Research}, pp.\  49--57. Springer, 2020.

\bibitem[Hadfield-Menell et~al.(2016)Hadfield-Menell, Russell, Abbeel, and
  Dragan]{hadfield2016cooperative}
Hadfield-Menell, D., Russell, S.~J., Abbeel, P., and Dragan, A.
\newblock Cooperative inverse reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pp.\
  3909--3917. 2016.

\bibitem[Hanna et~al.(2017)Hanna, Stone, and Niekum]{hanna2017bootstrapping}
Hanna, J.~P., Stone, P., and Niekum, S.
\newblock Bootstrapping with models: Confidence intervals for off-policy
  evaluation.
\newblock In \emph{Proceedings of the 16th Conference on Autonomous Agents and
  Multiagent Systems}, 2017.

\bibitem[Huang et~al.(2017)Huang, Held, Abbeel, and Dragan]{huang2017enabling}
Huang, S.~H., Held, D., Abbeel, P., and Dragan, A.~D.
\newblock Enabling robots to communicate their objectives.
\newblock In \emph{Robotics: Science and Systems}, 2017.

\bibitem[Huang et~al.(2018)Huang, Bhatia, Abbeel, and
  Dragan]{huang2018establishing}
Huang, S.~H., Bhatia, K., Abbeel, P., and Dragan, A.~D.
\newblock Establishing appropriate trust via critical states.
\newblock In \emph{2018 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  3929--3936. IEEE, 2018.

\bibitem[Laskin et~al.(2020)Laskin, Srinivas, and Abbeel]{laskin2020curl}
Laskin, M., Srinivas, A., and Abbeel, P.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5639--5650. PMLR, 2020.

\bibitem[Leike et~al.(2017)Leike, Martic, Krakovna, Ortega, Everitt, Lefrancq,
  Orseau, and Legg]{leike2017ai}
Leike, J., Martic, M., Krakovna, V., Ortega, P.~A., Everitt, T., Lefrancq, A.,
  Orseau, L., and Legg, S.
\newblock Ai safety gridworlds.
\newblock \emph{arXiv preprint arXiv:1711.09883}, 2017.

\bibitem[Leike et~al.(2018)Leike, Krueger, Everitt, Martic, Maini, and
  Legg]{leike2018scalable}
Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S.
\newblock Scalable agent alignment via reward modeling: a research direction.
\newblock \emph{arXiv preprint arXiv:1811.07871}, 2018.

\bibitem[Ng \& Russell(2000)Ng and Russell]{ng2000algorithms}
Ng, A.~Y. and Russell, S.~J.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{ICML}, pp.\  663--670, 2000.

\bibitem[Paulraj et~al.(2010)Paulraj, Sumathi, et~al.]{paulraj2010comparative}
Paulraj, S., Sumathi, P., et~al.
\newblock A comparative study of redundant constraints identification methods
  in linear programming problems.
\newblock \emph{Mathematical Problems in Engineering}, 2010.

\bibitem[Precup(2000)]{precup2000eligibility}
Precup, D.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series}, pp.\
  ~80, 2000.

\bibitem[Russell et~al.(2015)Russell, Dewey, and Tegmark]{russell2015research}
Russell, S., Dewey, D., and Tegmark, M.
\newblock Research priorities for robust and beneficial artificial
  intelligence.
\newblock \emph{Ai Magazine}, 36\penalty0 (4):\penalty0 105--114, 2015.

\bibitem[Russell \& Norvig(2016)Russell and Norvig]{russell2016artificial}
Russell, S.~J. and Norvig, P.
\newblock \emph{Artificial intelligence: a modern approach}.
\newblock Malaysia; Pearson Education Limited,, 2016.

\bibitem[Sadigh et~al.(2017)Sadigh, Dragan, Sastry, and
  Seshia]{sadigh2017active}
Sadigh, D., Dragan, A.~D., Sastry, S.~S., and Seshia, S.~A.
\newblock Active preference-based learning of reward functions.
\newblock In \emph{Proceedings of Robotics: Science and Systems ({RSS})}, July
  2017.
\newblock \doi{10.15607/RSS.2017.XIII.053}.

\bibitem[Shah et~al.(2020)Shah, Freire, Alex, Freedman, Krasheninnikov, Chan,
  Dennis, Abbeel, Dragan, and Russell]{shahbenefits}
Shah, R., Freire, P., Alex, N., Freedman, R., Krasheninnikov, D., Chan, L.,
  Dennis, M., Abbeel, P., Dragan, A., and Russell, S.
\newblock Benefits of assistance over reward learning.
\newblock \emph{Workshop on Cooperative AI (Cooperative AI @ NeurIPS}, 2020.

\bibitem[Stone et~al.(2010)Stone, Kaminka, Kraus, Rosenschein,
  et~al.]{stone2010ad}
Stone, P., Kaminka, G.~A., Kraus, S., Rosenschein, J.~S., et~al.
\newblock Ad hoc autonomous agent teams: Collaboration without
  pre-coordination.
\newblock In \emph{AAAI}, 2010.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998introduction}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Introduction to reinforcement learning}, volume 135.
\newblock MIT press Cambridge, 1998.

\bibitem[Thomas et~al.(2015)Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015high}
Thomas, P.~S., Theocharous, G., and Ghavamzadeh, M.
\newblock High-confidence off-policy evaluation.
\newblock In \emph{AAAI}, pp.\  3000--3006, 2015.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz,
  et~al.]{wirth2017survey}
Wirth, C., Akrour, R., Neumann, G., F{\"u}rnkranz, J., et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (136):\penalty0 1--46, 2017.

\bibitem[Zhu et~al.(2018)Zhu, Singla, Zilles, and Rafferty]{zhu2018overview}
Zhu, X., Singla, A., Zilles, S., and Rafferty, A.~N.
\newblock An overview of machine teaching.
\newblock \emph{arXiv preprint arXiv:1801.05927}, 2018.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI}, 2008.

\end{thebibliography}
