###################################### TUTORIALS ############################################

@book{villani2009optimal,
author = {Cédric Villani},
address = {Berlin},
booktitle = {Optimal transport : old and new / Cédric Villani},
isbn = {978-3-540-71049-3},
keywords = {Systèmes dynamiques},
language = {eng},
publisher = {Springer},
series = {Grundlehren der mathematischen Wissenschaften},
title = {Optimal transport  : old and new / Cédric Villani},
year = {2009},
}

@inproceedings{cohen2016geometric,
  title={Geometric median in nearly linear time},
  author={Cohen, Michael B and Lee, Yin Tat and Miller, Gary and Pachocki, Jakub and Sidford, Aaron},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={9--21},
  year={2016}
}

@article{lian2015asynchronous,
  title={Asynchronous parallel stochastic gradient for nonconvex optimization},
  author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  pages={2737--2745},
  year={2015}
}

@article{lei2019stochastic,
  title={Stochastic gradient descent for nonconvex learning without bounded gradient assumptions},
  author={Lei, Yunwen and Hu, Ting and Li, Guiying and Tang, Ke},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={10},
  pages={4394--4400},
  year={2019},
  publisher={IEEE}
}

@article{tsitsiklis1986distributed,
  title={Distributed asynchronous deterministic and stochastic gradient optimization algorithms},
  author={Tsitsiklis, John and Bertsekas, Dimitri and Athans, Michael},
  journal={IEEE transactions on automatic control},
  volume={31},
  number={9},
  pages={803--812},
  year={1986},
  publisher={IEEE}
}

@article{arnold1979bounds,
  title={Bounds on expectations of linear systematic statistics based on dependent samples},
  author={Arnold, Barry C and Groeneveld, Richard A},
  journal={The Annals of Statistics},
  pages={220--223},
  year={1979},
  publisher={JSTOR}
}

@article{bertsimas2006tight,
  title={Tight bounds on expected order statistics},
  author={Bertsimas, Dimitris and Natarajan, Karthik and Teo, Chung-Piaw},
  journal={Probability in the Engineering and Informational Sciences},
  volume={20},
  number={4},
  pages={667--686},
  year={2006},
  publisher={Cambridge University Press}
}

@book{huber2004robust,
  title={Robust statistics},
  author={Huber, Peter J},
  volume={523},
  year={2004},
  publisher={John Wiley \& Sons}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}
@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@book{lynch1996distributed,
  title={Distributed algorithms},
  author={Lynch, Nancy A},
  year={1996},
  publisher={Elsevier}
}

@InProceedings{DP,
author="Dwork, Cynthia",
editor="Bugliesi, Michele
and Preneel, Bart
and Sassone, Vladimiro
and Wegener, Ingo",
title="Differential Privacy",
booktitle="Automata, Languages and Programming",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--12",
abstract="In 1977 Dalenius articulated a desideratum for statistical databases: nothing about an individual should be learnable from the database that cannot be learned without access to the database. We give a general impossibility result showing that a formalization of Dalenius' goal along the lines of semantic security cannot be achieved. Contrary to intuition, a variant of the result threatens the privacy even of someone not in the database. This state of affairs suggests a new measure, differential privacy, which, intuitively, captures the increased risk to one's privacy incurred by participating in a database. The techniques developed in a sequence of papers [8, 13, 3], culminating in those described in [12], can achieve any desired level of privacy under this measure. In many cases, extremely accurate information about the database can be provided while simultaneously ensuring very high levels of privacy.",
isbn="978-3-540-35908-1"
}

@inproceedings{zhang2013information,
  title={Information-theoretic lower bounds for distributed statistical estimation with communication constraints.},
  author={Zhang, Yuchen and Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
  booktitle={NIPS},
  pages={2328--2336},
  year={2013},
  organization={Citeseer}
}

@article{wu2017lecture,
  title={Lecture notes on information-theoretic methods for high-dimensional statistics},
  author={Wu, Yihong},
  journal={Lecture Notes for ECE598YW (UIUC)},
  volume={16},
  year={2017}
}

@article{yang1999information,
  title={Information-theoretic determination of minimax rates of convergence},
  author={Yang, Yuhong and Barron, Andrew},
  journal={Annals of Statistics},
  pages={1564--1599},
  year={1999},
  publisher={JSTOR}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{su2016fault,
  title={Fault-tolerant multi-agent optimization: optimal iterative distributed algorithms},
  author={Su, Lili and Vaidya, Nitin H},
  booktitle={Proceedings of the 2016 ACM symposium on principles of distributed computing},
  pages={425--434},
  year={2016}
}

@inproceedings{simard2003best,
  title={Best practices for convolutional neural networks applied to visual document analysis.},
  author={Simard, Patrice Y and Steinkraus, David and Platt, John C and others},
  booktitle={Icdar},
  volume={3},
  number={2003},
  year={2003},
  organization={Citeseer}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy.},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends in Theoretical Computer Science},
  volume={9},
  number={3-4},
  pages={211--407},
  year={2014}
}

@article{averaging,
author = {Polyak, Boris and Juditsky, Anatoli},
year = {1992},
month = {07},
pages = {838-855},
title = {Acceleration of Stochastic Approximation by Averaging},
volume = {30},
journal = {SIAM Journal on Control and Optimization},
doi = {10.1137/0330046}
}

@inproceedings{imagenet,
AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
BOOKTITLE = {CVPR09},
YEAR = {2009},
BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}

@article{openimages,
   title={The Open Images Dataset V4},
   volume={128},
   ISSN={1573-1405},
   DOI={10.1007/s11263-020-01316-z},
   number={7},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and et al.},
   year={2020},
   month={Mar},
   pages={1956–1981}
}

@InProceedings{SGD,
author="Bottou, L{\'e}on",
editor="Lechevallier, Yves
and Saporta, Gilbert",
title="Large-Scale Machine Learning with Stochastic Gradient Descent",
booktitle="Proceedings of COMPSTAT'2010",
year="2010",
publisher="Physica-Verlag HD",
address="Heidelberg",
pages="177--186",
abstract="During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",
isbn="978-3-7908-2604-3"
}

@article{reduce,
title = "Bandwidth optimal all-reduce algorithms for clusters of workstations",
journal = "Journal of Parallel and Distributed Computing",
volume = "69",
number = "2",
pages = "117 - 124",
year = "2009",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2008.09.002",
author = "Pitch Patarasuk and Xin Yuan",
keywords = "All-reduce, Collective communication, Tree topology, Cluster of workstations",
abstract = "We consider an efficient realization of the all-reduce operation with large data sizes in cluster environments, under the assumption that the reduce operator is associative and commutative. We derive a tight lower bound of the amount of data that must be communicated in order to complete this operation and propose a ring-based algorithm that only requires tree connectivity to achieve bandwidth optimality. Unlike the widely used butterfly-like all-reduce algorithm that incurs network contention in SMP/multi-core clusters, the proposed algorithm can achieve contention-free communication in almost all contemporary clusters, including SMP/multi-core clusters and Ethernet switched clusters with multiple switches. We demonstrate that the proposed algorithm is more efficient than other algorithms on clusters with different nodal architectures and networking technologies when the data size is sufficiently large."
}


######################################## PRIVACY AND FAULTS IN LEARNING ###############################################
@incollection{DLG,
title = {Deep Leakage from Gradients},
author = {Zhu, Ligeng and Liu, Zhijian and Han, Song},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {14774--14784},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@inproceedings{yin2018byzantine,
  title={Byzantine-robust distributed learning: Towards optimal statistical rates},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={5650--5659},
  year={2018},
  organization={PMLR}
}

@article{ghosh2021communication,
  title={Communication-Efficient and Byzantine-Robust Distributed Learning With Error Feedback},
  author={Ghosh, Avishek and Maity, Raj Kumar and Kadhe, Swanand and Mazumdar, Arya and Ramchandran, Kannan},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={2},
  number={3},
  pages={942--953},
  year={2021},
  publisher={IEEE}
}

@incollection{krum,
title = {Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent},
author = {Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {119--129},
year = {2017},
publisher = {Curran Associates, Inc.},
}

@misc{brute_bulyan,
      title={The Hidden Vulnerability of Distributed Learning in Byzantium},
      author={El Mhamdi, El Mahdi and Guerraoui, Rachid and Rouault, Sébastien},
      year={2018},
      eprint={1802.07927},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{el2020genuinely,
  title={Genuinely distributed byzantine machine learning},
  author={El Mhamdi, El Mahdi and Guerraoui, Rachid  and Guirguis, Arsany  and Hoang, L{\^e} Nguy{\^e}n and Rouault, S{\'e}bastien },
  booktitle={Proceedings of the 39th Symposium on Principles of Distributed Computing},
  pages={355--364},
  year={2020}
}

@misc{median,
      title={Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates}, 
      author={Dong Yin and Yudong Chen and Kannan Ramchandran and Peter Bartlett},
      year={2018},
      eprint={1803.01498},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{phocas,
      title={Phocas: dimensional Byzantine-resilient stochastic gradient descent}, 
      author={Cong Xie and Oluwasanmi Koyejo and Indranil Gupta},
      year={2018},
      eprint={1805.09682},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{meamed,
      title={Generalized Byzantine-tolerant SGD}, 
      author={Cong Xie and Oluwasanmi Koyejo and Indranil Gupta},
      year={2018},
      eprint={1802.10116},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@article{rousseeuw1985multivariate,
  title={Multivariate estimation with high breakdown point},
  author={Rousseeuw, Peter J},
  journal={Mathematical statistics and applications},
  volume={8},
  number={37},
  pages={283--297},
  year={1985}
}

@inproceedings{paramServer,
author = {Li, Mu and Andersen, David G. and Park, Jun Woo and Smola, Alexander J. and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J. and Su, Bor-Yiing},
title = {Scaling Distributed Machine Learning with the Parameter Server},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {583–598},
numpages = {16},
location = {Broomfield, CO},
series = {OSDI’14}
}

@inbook{bottou, place={Cambridge}, series={Publications of the Newton Institute}, title={On-line Learning and Stochastic Approximations}, DOI={10.1017/CBO9780511569920.003}, booktitle={On-Line Learning in Neural Networks}, publisher={Cambridge University Press}, author={Bottou, Léon}, year={1999}, pages={9–42}, collection={Publications of the Newton Institute}}

@article{shokri,
  author    = {Reza Shokri and
               Marco Stronati and
               Vitaly Shmatikov},
  title     = {Membership Inference Attacks against Machine Learning Models},
  journal   = {CoRR},
  volume    = {abs/1610.05820},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1610.05820},
  timestamp = {Mon, 13 Aug 2018 16:47:19 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Mlleaks,
      title={ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models}, 
      author={Ahmed Salem and Yang Zhang and Mathias Humbert and Pascal Berrang and Mario Fritz and Michael Backes},
      year={2018},
      eprint={1806.01246},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{ByzProblem,
author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
title = {The Byzantine Generals Problem},
year = {1982},
issue_date = {July 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {0164-0925},
doi = {10.1145/357172.357176},
journal = {ACM Trans. Program. Lang. Syst.},
month = jul,
pages = {382–401},
numpages = {20}
}

@article{mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  username = {mhwombat},
  year = 2010
}

@article{fashion-mnist,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  journal      = {arXiv preprint arXiv:1708.07747},
}

@article{cifar,
title= {CIFAR-100 (Canadian Institute for Advanced Research)},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2009},
abstract= {This dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a "fine" label (the class to which it belongs) and a "coarse" label (the superclass to which it belongs).
Here is the list of classes in the CIFAR-100:

Superclass	Classes
aquatic mammals	beaver, dolphin, otter, seal, whale
fish	aquarium fish, flatfish, ray, shark, trout
flowers	orchids, poppies, roses, sunflowers, tulips
food containers	bottles, bowls, cans, cups, plates
fruit and vegetables	apples, mushrooms, oranges, pears, sweet peppers
household electrical devices	clock, computer keyboard, lamp, telephone, television
household furniture	bed, chair, couch, table, wardrobe
insects	bee, beetle, butterfly, caterpillar, cockroach
large carnivores	bear, leopard, lion, tiger, wolf
large man-made outdoor things	bridge, castle, house, road, skyscraper
large natural outdoor scenes	cloud, forest, mountain, plain, sea
large omnivores and herbivores	camel, cattle, chimpanzee, elephant, kangaroo
medium-sized mammals	fox, porcupine, possum, raccoon, skunk
non-insect invertebrates	crab, lobster, snail, spider, worm
people	baby, boy, girl, man, woman
reptiles	crocodile, dinosaur, lizard, snake, turtle
small mammals	hamster, mouse, rabbit, shrew, squirrel
trees	maple, oak, palm, pine, willow
vehicles 1	bicycle, bus, motorcycle, pickup truck, train
vehicles 2	lawn-mower, rocket, streetcar, tank, tractor

Yes, I know mushrooms aren't really fruit or vegetables and bears aren't really carnivores. },
keywords= {Dataset},
terms= {}
}

@article{SVHN,
author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew},
year = {2011},
month = {01},
pages = {},
title = {Reading Digits in Natural Images with Unsupervised Feature Learning},
journal = {NIPS}
}

@TechReport{LFW,
author = {Gary B. Huang and Manu Ramesh and Tamara Berg and Erik Learned-Miller},
title = {Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments},
institution =  {University of Massachusetts, Amherst},
year =         2007,
number =       {07-49},
month =        {October}
}

@InProceedings{AdvancedComposition, title = {The Composition Theorem for Differential Privacy}, author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod}, booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {1376--1385}, year = {2015}, editor = {Bach, Francis and Blei, David}, volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v37/kairouz15.pdf}, 
abstract = {Interactive querying of a database degrades the privacy level. In this paper we answer the fundamental question of characterizing the level of privacy degradation as a function of the number of adaptive interactions and the differential privacy levels maintained by the individual queries. Our solution is complete: the privacy degradation guarantee is true for every privacy mechanism, and further, we demonstrate a sequence of privacy mechanisms that do degrade in the characterized manner. The key innovation is the introduction of an operational interpretation (involving hypothesis testing) to differential privacy and the use of the corresponding data processing inequalities. Our result improves over the state of the art and has immediate applications to several problems studied in the literature.} } 
@INPROCEEDINGS{LearningChain,
  author={X. {Chen} and J. {Ji} and C. {Luo} and W. {Liao} and P. {Li}},
  booktitle={2018 IEEE International Conference on Big Data (Big Data)},
  title={When Machine Learning Meets Blockchain: A Decentralized, Privacy-preserving and Secure Design}, 
  year={2018},
  volume={},
  number={},
  pages={1178-1187},}
  
@article{chen2017distributed,
  title={Distributed statistical machine learning in adversarial settings: Byzantine gradient descent},
  author={Chen, Yudong and Su, Lili and Xu, Jiaming},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={1},
  number={2},
  pages={1--25},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{federated,
title	= {Federated Learning: Strategies for Improving Communication Efficiency},
author	= {Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richtarik and Ananda Theertha Suresh and Dave Bacon},
year	= {2016},
booktitle	= {NIPS Workshop on Private Multi-Party Machine Learning}
}

@misc{twoServer_Jaggi,
      title={Secure Byzantine-Robust Machine Learning}, 
      author={Lie He and Sai Praneeth Karimireddy and Martin Jaggi},
      year={2020},
      eprint={2006.04747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{brea,
      title={Byzantine-Resilient Secure Federated Learning}, 
      author={Jinhyun So and Basak Guler and A. Salman Avestimehr},
      year={2020},
      eprint={2007.11115},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@INPROCEEDINGS{DP_SGD1,
  author={S. {Song} and K. {Chaudhuri} and A. D. {Sarwate}},
  booktitle={2013 IEEE Global Conference on Signal and Information Processing},
  title={Stochastic gradient descent with differentially private updates}, 
  year={2013},
  volume={},
  number={},
  pages={245-248},
  doi={10.1109/GlobalSIP.2013.6736861}}
  
@misc{DP_SGD2,
      title={Differentially Private Stochastic Coordinate Descent}, 
      author={Georgios Damaskinos and Celestine Mendler-Dünner and Rachid Guerraoui and Nikolaos Papandreou and Thomas Parnell},
      year={2020},
      eprint={2006.07272},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{gradObfuscation,
  author={Gade, Shripad and Vaidya, Nitin H.},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)}, 
  title={Privacy-Preserving Distributed Learning via Obfuscated Stochastic Gradients}, 
  year={2018},
  volume={},
  number={},
  pages={184-191},
  doi={10.1109/CDC.2018.8619133}}


@article{GeneralizedGaussian,
author = { Saralees   Nadarajah },
title = {A generalized normal distribution},
journal = {Journal of Applied Statistics},
volume = {32},
number = {7},
pages = {685-694},
year  = {2005},
publisher = {Taylor & Francis}
}

@misc{bassily,
      title={Differentially Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds}, 
      author={Raef Bassily and Adam Smith and Abhradeep Thakurta},
      year={2014},
      eprint={1405.7085},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{papernot,
      title={Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data}, 
      author={Nicolas Papernot and Martín Abadi and Úlfar Erlingsson and Ian Goodfellow and Kunal Talwar},
      year={2017},
      eprint={1610.05755},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{papernot2018scalable,
  title={Scalable Private Learning with PATE},
  author={Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Ulfar},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{shokri2015privacy,
  title={Privacy-preserving deep learning},
  author={Shokri, Reza and Shmatikov, Vitaly},
  booktitle={Proceedings of the 22nd ACM SIGSAC conference on computer and communications security},
  pages={1310--1321},
  year={2015}
}

  
  @article{DP_SGD_Fed2,
  title={Toward Robustness and Privacy in Federated Learning: Experimenting with Local and Central Differential Privacy},
  author={M. Naseri and J. Hayes and Emiliano De Cristofaro},
  journal={ArXiv},
  year={2020},
  volume={abs/2009.03561}
}

@misc{composition,
      title={The Composition Theorem for Differential Privacy}, 
      author={Peter Kairouz and Sewoong Oh and Pramod Viswanath},
      year={2015},
      eprint={1311.0776},
      archivePrefix={arXiv},
      primaryClass={cs.DS}
}

@misc{clipping,
      title={Why gradient clipping accelerates training: A theoretical justification for adaptivity}, 
      author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
      year={2020},
      eprint={1905.11881},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{DBLP:journals/corr/ZagoruykoK16,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Wide Residual Networks},
  journal   = {CoRR},
  volume    = {abs/1605.07146},
  year      = {2016},
  archivePrefix = {arXiv},
  eprint    = {1605.07146},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{encryptGrads,
author = {Tang, Fengyi and Wu, Wei and Liu, Jian and Xian, Ming},
year = {2019},
month = {04},
pages = {411},
title = {Privacy-Preserving Distributed Deep Learning via Homomorphic Re-Encryption},
volume = {8},
journal = {Electronics},
doi = {10.3390/electronics8040411}
}

@misc{HEGrads,
      title={MYSTIKO : : Cloud-Mediated, Private, Federated Gradient Descent}, 
      author={K. R. Jayaram and Archit Verma and Ashish Verma and Gegi Thomas and Colin Sutcher-Shepard},
      year={2020},
      eprint={2012.00740},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}


############################## Non-IID Setting ###################################

@article{karimireddy2020byzantine,
  title={Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  journal={arXiv preprint arXiv:2006.09365},
  year={2020}
}

@inproceedings{steinhardt2018resilience,
  title={Resilience: A Criterion for Learning in the Presence of Arbitrary Outliers},
  author={Steinhardt, Jacob and Charikar, Moses and Valiant, Gregory},
  booktitle={9th Innovations in Theoretical Computer Science Conference (ITCS 2018)},
  year={2018},
  organization={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}

############################## OTHER FILTERS and Fault-Tolerance Works ###################################
@inproceedings{bhatia2015robust,
author = {Bhatia, Kush and Jain, Prateek and Kar, Purushottam},
title = {Robust Regression via Hard Thresholding},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X ∈ ℝp x n and an underlying model w*, the response vector is generated as y = XT w* + b where b ∈ ℝn is the corruption vector supported over at most C · n coordinates. Existing exact recovery results for RLSR focus solely on L1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X.In this work, we study a simple hard-thresholding algorithm called TORRENT which, under mild conditions on X, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w*. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w*, generated independently of X, our results are universal and hold for any w* ∈ ℝp.Next, we propose gradient descent-based extensions of TORRENT that can scale efficiently to large scale problems, such as high dimensional sparse recovery. and prove similar recovery guarantees for these extensions. Empirically we find TORRENT, and more so its extensions, offering significantly faster recovery than the state-of-the-art L1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called TORRENT-HYB is more than 20 x faster than the best L1 solver.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {721–729},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@article{su2019finite,
  title={Finite-time guarantees for Byzantine-resilient distributed state estimation with noisy measurements},
  author={Su, Lili and Shahrampour, Shahin},
  journal={IEEE Transactions on Automatic Control},
  volume={65},
  number={9},
  pages={3758--3771},
  year={2019},
  publisher={IEEE}
}

@article{yang2019byrdie,
  title={ByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning},
  author={Yang, Zhixiong and Bajwa, Waheed U},
  journal={IEEE Transactions on Signal and Information Processing over Networks},
  volume={5},
  number={4},
  pages={611--627},
  year={2019},
  publisher={IEEE}
}

@inproceedings{gupta2020fault,
  title={Fault-tolerance in distributed optimization: The case of redundancy},
  author={Gupta, Nirupam and Vaidya, Nitin H},
  booktitle={Proceedings of the 39th Symposium on Principles of Distributed Computing},
  pages={365--374},
  year={2020}
}

@inproceedings{liu2021approximate,
author = {Liu, Shuo and Gupta, Nirupam and Vaidya, Nitin H.},
title = {Approximate Byzantine Fault-Tolerance in Distributed Optimization},
year = {2021},
isbn = {9781450385480},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3465084.3467902},
abstract = {This paper considers the problem of Byzantine fault-tolerance in distributed multi-agent optimization. In this problem, each agent has a local cost function, and in the fault-free case, the goal is to design a distributed algorithm that allows all the agents to find a minimum point of all the agents' aggregate cost function. We consider a scenario where some agents might be Byzantine faulty that renders the original goal of computing a minimum point of all the agents' aggregate cost vacuous. A more reasonable objective for an algorithm in this scenario is to allow all the non-faulty agents to compute the minimum point of only the non-faulty agents' aggregate cost. Prior work shows that if there are up to f (out of n) Byzantine agents then a minimum point of the non-faulty agents' aggregate cost can be computed exactly if and only if the non-faulty agents' costs satisfy a certain redundancy property called 2f-redundancy. However, 2f-redundancy is an ideal property that can be satisfied only in systems free from noise or uncertainties, which can make the goal of exact fault-tolerance unachievable in some applications. Thus, we introduce the notion of (f,ε)-resilience, a generalization of exact fault-tolerance wherein the objective is to find an approximate minimum point of the non-faulty aggregate cost, with ε accuracy. This approximate fault-tolerance can be achieved under a weaker condition that is easier to satisfy in practice, compared to 2f-redundancy. We obtain necessary and sufficient conditions for achieving (f, ε)-resilience characterizing the correlation between relaxation in redundancy and approximation in resilience. In case when the agents' cost functions are differentiable, we obtain conditions for (f, ε)-resilience of the distributed gradient-descent method when equipped with robust gradient aggregation; such as comparative gradient elimination or coordinate-wise trimmed mean.},
booktitle = {Proceedings of the 2021 ACM Symposium on Principles of Distributed Computing},
pages = {379–389},
numpages = {11},
keywords = {distributed optimization, approximate fault-tolerance, distributed gradient-descent},
location = {Virtual Event, Italy},
series = {PODC'21}
}

@inproceedings{gupta2021byzantine,
  title={Byzantine Fault-Tolerant Distributed Machine Learning with Norm-Based Comparative Gradient Elimination},
  author={Gupta, Nirupam and Liu, Shuo and Vaidya, Nitin},
  booktitle={2021 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)},
  pages={175--181},
  year={2021},
  organization={IEEE}
}

@inproceedings{allen2020byzantine,
  title={Byzantine-Resilient Non-Convex Stochastic Gradient Descent},
  author={Allen-Zhu, Zeyuan and Ebrahimianghazani, Faeze and Li, Jerry and Alistarh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{prasad2020robust,
  title={Robust estimation via robust gradient estimation},
  author={Prasad, Adarsh and Suggala, Arun Sai and Balakrishnan, Sivaraman and Ravikumar, Pradeep},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={82},
  number={3},
  pages={601--627},
  year={2020},
  publisher={Wiley Online Library}
}

@inproceedings{diakonikolas2019sever,
  title={Sever: A robust meta-algorithm for stochastic optimization},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Steinhardt, Jacob and Stewart, Alistair},
  booktitle={International Conference on Machine Learning},
  pages={1596--1606},
  year={2019},
  organization={PMLR}
}

@article{bernstein2018signsgd,
  title={signSGD with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{alistarh2018byzantine,
  title={Byzantine stochastic gradient descent},
  author={Alistarh, Dan and Allen-Zhu, Zeyuan and Li, Jerry},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4618--4628},
  year={2018}
}

@article{gupta2019randomized,
  title={Randomized Reactive Redundancy for Byzantine Fault-Tolerance in Parallelized Learning},
  author={Gupta, Nirupam and Vaidya, Nitin H},
  journal={arXiv preprint arXiv:1912.09528},
  year={2019}
}

@inproceedings{data2021byzantine,
  title={Byzantine-resilient SGD in high dimensions on heterogeneous data},
  author={Data, Deepesh and Diggavi, Suhas},
  booktitle={2021 IEEE International Symposium on Information Theory (ISIT)},
  pages={2310--2315},
  year={2021},
  organization={IEEE}
}

@article{data2020data,
  title={Data encoding for byzantine-resilient distributed optimization},
  author={Data, Deepesh and Song, Linqi and Diggavi, Suhas N},
  journal={IEEE Transactions on Information Theory},
  volume={67},
  number={2},
  pages={1117--1140},
  year={2020},
  publisher={IEEE}
}

@inproceedings{charikar2017learning,
  title={Learning from untrusted data},
  author={Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={47--60},
  year={2017}
}
################################# ATTACKS ######################################
@inproceedings{empire,
  author    = {Cong Xie and
               Oluwasanmi Koyejo and
               Indranil Gupta},
  title     = {Fall of Empires: Breaking Byzantine-tolerant {SGD} by Inner Product
               Manipulation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {83},
  year      = {2019},
  timestamp = {Fri, 19 Jul 2019 13:05:12 +0200},
}

@inproceedings{little,
  author    = {Moran Baruch and
               Gilad Baruch and
               Yoav Goldberg},
  title     = {A Little Is Enough: Circumventing Defenses For Distributed Learning},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, 8-14 December 2019,
               Long Beach, CA, {USA}},
  year      = {2019},
}

@InProceedings{pmlr-v28-sutskever13, 
title = {On the importance of initialization and momentum in deep learning}, 
author = {Ilya Sutskever and James Martens and George Dahl and Geoffrey Hinton}, 
booktitle = {Proceedings of the 30th International Conference on Machine Learning}, 
pages = {1139--1147}, 
year = {2013}, 
editor = {Sanjoy Dasgupta and David McAllester}, 
volume = {28}, 
number = {3}, 
series = {Proceedings of Machine Learning Research}, 
address = {Atlanta, Georgia, USA}, 
month = {17--19 Jun}, 
publisher = {PMLR}, 
pdf = {http://proceedings.mlr.press/v28/sutskever13.pdf}, 
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods. } 
}

@article{momentum,
  author = {Polyak, Boris},
  year = {1964},
  month = {12},
  pages = {1-17},
  title = {Some methods of speeding up the convergence of iteration methods},
  volume = {4},
  journal = {USSR Computational Mathematics and Mathematical Physics},
  doi = {10.1016/0041-5553(64)90137-5}
}

@article{cutkosky2019momentum,
  title={Momentum-Based Variance Reduction in Non-Convex SGD},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15236--15245},
  year={2019}
}

# (As support that two state-of-the-art attacks)
@inproceedings{distributed-momentum,
  author    = {El Mhamdi, El Mahdi and
               Guerraoui, Rachid and
              Rouault, S\'{e}bastien},
  title     = {Distributed Momentum for Byzantine-resilient Stochastic Gradient Descent},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Vienna, Austria, May 4–8, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
}

# (Moment accountants for DP)
@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages={308--318},
  year={2016}
}

@inproceedings{redundancy,
  author    = {Lingjiao Chen and
               Hongyi Wang and
               Zachary B. Charles and
               Dimitris S. Papailiopoulos},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {{DRACO:} Byzantine-resilient Distributed Training via
Redundant Gradients},
  booktitle = {Proceedings of the 35th International Conference on
Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden,
July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {902--911},
  publisher = {{PMLR}},
  year      = {2018},
}

@inproceedings{rajput2019detox,
  title={DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation},
  author={Rajput, Shashank and Wang, Hongyi and Charles, Zachary and Papailiopoulos, Dimitris},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{suspicion,
  author    = {Cong Xie and
               Sanmi Koyejo and
               Indranil Gupta},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Zeno: Distributed Stochastic Gradient Descent with
Suspicion-based
               Fault-tolerance},
  booktitle = {Proceedings of the 36th International Conference on
Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California,
{USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {6893--6901},
  publisher = {{PMLR}},
  year      = {2019},
}

@inproceedings{xie2020zeno++,
  title={Zeno++: Robust fully asynchronous SGD},
  author={Xie, Cong and Koyejo, Sanmi and Gupta, Indranil},
  booktitle={International Conference on Machine Learning},
  pages={10495--10503},
  year={2020},
  organization={PMLR}
}

@article{yao2019federated,
  title={Federated learning with unbiased gradient aggregation and controllable meta updating},
  author={Yao, Xin and Huang, Tianchi and Zhang, Rui-Xiao and Li, Ruiyu and Sun, Lifeng},
  journal={arXiv preprint arXiv:1910.08234},
  year={2019}
}

@inproceedings{gupta2019byzantine,
  title={Byzantine fault-tolerant parallelized stochastic gradient descent for linear regression},
  author={Gupta, Nirupam and Vaidya, Nitin H},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={415--420},
  year={2019},
  organization={IEEE}
}

@article{diakonikolas2019robust,
  title={Robust estimators in high-dimensions without the computational intractability},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
  journal={SIAM Journal on Computing},
  volume={48},
  number={2},
  pages={742--864},
  year={2019},
  publisher={SIAM}
}

@inproceedings{data2021byzantine_icml,
  title={Byzantine-resilient high-dimensional sgd with local iterations on heterogeneous data},
  author={Data, Deepesh and Diggavi, Suhas},
  booktitle={International Conference on Machine Learning},
  pages={2478--2488},
  year={2021},
  organization={PMLR}
}

@inproceedings{yin2019defending,
  title={Defending against saddle point attack in Byzantine-robust distributed learning},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={7074--7084},
  year={2019},
  organization={PMLR}
}

@article{cao2019distributed,
  title={Distributed gradient descent algorithm robust to an arbitrary number of byzantine attackers},
  author={Cao, Xinyang and Lai, Lifeng},
  journal={IEEE Transactions on Signal Processing},
  volume={67},
  number={22},
  pages={5850--5864},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ji2019learning,
  title={Learning to Learn Gradient Aggregation by Gradient Descent.},
  author={Ji, Jinlong and Chen, Xuhui and Wang, Qianlong and Yu, Lixing and Li, Pan},
  booktitle={IJCAI},
  pages={2614--2620},
  year={2019}
}

@inproceedings{li2019rsa,
  title={RSA: Byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets},
  author={Li, Liping and Xu, Wei and Chen, Tianyi and Giannakis, Georgios B and Ling, Qing},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={1544--1551},
  year={2019}
}

@article{regatti2020bygars,
  title={ByGARS: Byzantine SGD with Arbitrary Number of Attackers},
  author={Regatti, Jayanth and Chen, Hao and Gupta, Abhishek},
  journal={arXiv preprint arXiv:2006.13421},
  year={2020}
}

@inproceedings{boussetta2021aksel,
  title={AKSEL: Fast Byzantine SGD},
  author={Boussetta, Amine and El Mhamdi, El Mahdi and Guerraoui, Rachid and Maurer, Alexandre and Rouault, S{\'e}bastien},
  booktitle={24th International Conference on Principles of Distributed Systems (OPODIS 2020)},
  year={2021},
  organization={Schloss Dagstuhl-Leibniz-Zentrum f{\"u}r Informatik}
}

@misc{amplification,
      title={Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity}, 
      author={Úlfar Erlingsson and Vitaly Feldman and Ilya Mironov and Ananth Raghunathan and Kunal Talwar and Abhradeep Thakurta},
      year={2020},
      eprint={1811.12469},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{renyi,
   title={Rényi Differential Privacy},
   ISBN={9781538632178},
   DOI={10.1109/csf.2017.11},
   journal={2017 IEEE 30th Computer Security Foundations Symposium (CSF)},
   publisher={IEEE},
   author={Mironov, Ilya},
   year={2017},
   month={Aug}
}

@techreport{renyiDiv,
  title={On measures of entropy and information},
  author={R{\'e}nyi, Alfr{\'e}d},
  year={1961},
  institution={Hungarian Academy of Sciences Budapest Hungary}
}

@article{6832827,
  author={T. {van Erven} and P. {Harremos}},
  journal={IEEE Transactions on Information Theory}, 
  title={R\'enyi Divergence and Kullback-Leibler Divergence}, 
  year={2014},
  volume={60},
  number={7},
  pages={3797-3820}
 }
 
 @article{KiferM14,
  author    = {Daniel Kifer and
               Ashwin Machanavajjhala},
  title     = {Pufferfish: {A} framework for mathematical privacy definitions},
  journal   = {{ACM} Trans. Database Syst.},
  volume    = {39},
  number    = {1},
  pages     = {3:1--3:36},
  year      = {2014},
  doi       = {10.1145/2514689},
  timestamp = {Tue, 06 Nov 2018 12:51:47 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{POLYAK19641,
title = {Some methods of speeding up the convergence of iteration methods},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {4},
number = {5},
pages = {1-17},
year = {1964},
issn = {0041-5553},
doi = {https://doi.org/10.1016/0041-5553(64)90137-5},
author = {B.T. Polyak},
abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.}
}



#### Distributed systems

@inproceedings{DistributedNetworks2012,
 author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc\textquotesingle aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Large Scale Distributed Deep Networks},
 volume = {25},
 year = {2012}
}

@misc{Tensorflow2015,
title	= {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
author	= {Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mané and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viégas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
year	= {2015},
}

@misc{podc,
      title={Differential Privacy and Byzantine Resilience in SGD: Do They Add Up?}, 
      author={Rachid Guerraoui and Nirupam Gupta and Rafaël Pinot and Sébastien Rouault and John Stephan},
      year={2021},
      eprint={2102.08166},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{PrivacyPreservingDeep2015, author={Shokri, Reza and Shmatikov, Vitaly}, booktitle={2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, title={Privacy-preserving deep learning}, year={2015}, volume={}, number={}, pages={909-910}, doi={10.1109/ALLERTON.2015.7447103}} 

@inproceedings{TrainingLargemodels2015,
 author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J\"{u}rgen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Training Very Deep Networks},
 volume = {28},
 year = {2015}
}

@InProceedings{Wang2019RDPMomentAcc,
  title = 	 {Subsampled Renyi Differential Privacy and Analytical Moments Accountant},
  author =       {Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1226--1235},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/wang19b/wang19b.pdf},
  abstract = 	 {We study the problem of subsampling in differential privacy (DP), a question that is the centerpiece behind many successful differentially private machine learning algorithms.  Specifically, we provide a tight upper bound on the Renyi Differential Privacy (RDP) [Mironov 2017] parameters for algorithms that: (1) subsample the dataset, and then (2) applies a randomized mechanism M to the subsample, in terms of the RDP parameters of M and the subsampling probability parameter. Our results generalize the moments accounting technique, developed by [Abadi et al. 2016] for the Gaussian mechanism, to any subsampled RDP mechanism.}
}



@inproceedings{Balle2018Subsampling,
author = {Balle, Borja and Barthe, Gilles and Gaboardi, Marco},
title = {Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Differential privacy comes equipped with multiple analytical tools for the design of private data analyses. One important tool is the so-called "privacy amplification by subsampling" principle, which ensures that a differentially private mechanism run on a random subsample of a population provides higher privacy guarantees than when run on the entire population. Several instances of this principle have been studied for different random subsampling methods, each with an ad-hoc analysis. In this paper we present a general method that recovers and improves prior analyses, yields lower bounds and derives new instances of privacy amplification by subsampling. Our method leverages a characterization of differential privacy as a divergence which emerged in the program verification community. Furthermore, it introduces new tools, including advanced joint convexity and privacy profiles, which might be of independent interest.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {6280–6290},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

  



@article{Kasiviswanathan2011WhatCanWeLearn,
title = "What can we learn privately?",
abstract = "Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask, What concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. Our goal is a broad understanding of the resources required for private learning in terms of samples, computation time, and interaction. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (nonprivate) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private probabilistically approximately correct learner for the class of parity functions. This result dispels the similarity between learning with noise and private learning (both must be robust to small changes in inputs), since parity is thought to be very hard to learn given random classification noise. Local (or randomized response) algorithms are a practical class of private algorithms that have received extensive investigation. We provide a precise characterization of local private learning algorithms. We show that a concept class is learnable by a local algorithm if and only if it is learnable in the statistical query (SQ) model. Therefore, for local private learning algorithms, the similarity to learning with noise is stronger: local learning is equivalent to SQ learning, and SQ algorithms include most known noise-tolerant learning algorithms. Finally, we present a separation between the power of interactive and noninteractive local learning algorithms. Because of the equivalence to SQ learning, this result also separates adaptive and nonadaptive SQ learning.",
author = "Kasiviswanathan, {Shiva Prasad} and Lee, {Omin K.} and Kobbi Nissim and Sofya Raskhodnikova and Adam Smith",
year = "2011",
doi = "10.1137/090756090",
language = "English (US)",
volume = "40",
pages = "793--826",
journal = "SIAM Journal on Computing",
issn = "0097-5397",
publisher = "Society for Industrial and Applied Mathematics Publications",
number = "3",
}


@inproceedings{Beimel2013SampleComplexity,
author = {Beimel, Amos and Nissim, Kobbi and Stemmer, Uri},
title = {Characterizing the Sample Complexity of Private Learners},
year = {2013},
isbn = {9781450318594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/2422436.2422450},
abstract = {In 2008, Kasiviswanathan el al. defined private learning as a combination of PAC learning and differential privacy [16]. Informally, a private learner is applied to a collection of labeled individual information and outputs a hypothesis while preserving the privacy of each individual. Kasiviswanathan et al. gave a generic construction of private learners for (finite) concept classes, with sample complexity logarithmic in the size of the concept class. This sample complexity is higher than what is needed for non-private learners, hence leaving open the possibility that the sample complexity of private learning may be sometimes significantly higher than that of non-private learning. We give a combinatorial characterization of the sample size sufficient and necessary to privately learn a class of concepts. This characterization is analogous to the well known characterization of the sample complexity of non-private learning in terms of the VC dimension of the concept class. We introduce the notion of probabilistic representation of a concept class, and our new complexity measure RepDim corresponds to the size of the smallest probabilistic representation of the concept class. We show that any private learning algorithm for a concept class C with sample complexity m implies RepDim(C) = O(m), and that there exists a private learning algorithm with sample complexity m = O(RepDim(C)).We further demonstrate that a similar characterization holds for the database size needed for privately computing a large class of optimization problems and also for the well studied problem of private data release.},
booktitle = {Proceedings of the 4th Conference on Innovations in Theoretical Computer Science},
pages = {97–110},
numpages = {14},
keywords = {pac learning, probabilistic representation, differential privacy, sample complexity},
location = {Berkeley, California, USA},
series = {ITCS '13}
}

  

####################### clipping ############################

@article{chen2020understanding,
  title={Understanding gradient clipping in private SGD: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{bu2021convergence,
  title={On the Convergence of Deep Learning with Differential Privacy},
  author={Bu, Zhiqi and Wang, Hua and Long, Qi and Su, Weijie J},
  journal={arXiv preprint arXiv:2106.07830},
  year={2021}
}

@article{mai2021stability,
  title={Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness},
  author={Mai, Vien V and Johansson, Mikael},
  journal={arXiv preprint arXiv:2102.06489},
  year={2021}
}

@inproceedings{qian2021understanding,
  title={Understanding Gradient Clipping In Incremental Gradient Methods},
  author={Qian, Jiang and Wu, Yuren and Zhuang, Bojin and Wang, Shaojun and Xiao, Jing and others},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1504--1512},
  year={2021},
  organization={PMLR}
}

@misc{Opacus, 
  title = "{O}pacus {PyTorch} library", 
  howpublished = "Available from \href{https://opacus.ai}{opacus.ai}",
  key = "Opacus"
}

@misc{pillutla2019robust,
      title={Robust Aggregation for Federated Learning}, 
      author={Krishna Pillutla and Sham M. Kakade and Zaid Harchaoui},
      year={2019},
      eprint={1912.13445},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{OpenProblemsinFed2021,
year = {2021},
volume = {14},
journal = {Foundations and Trends® in Machine Learning},
title = {Advances and Open Problems in Federated Learning},
doi = {10.1561/2200000083},
issn = {1935-8237},
number = {1–2},
pages = {1-210},
author = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D’Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konecný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Hang Qi and Daniel Ramage and Ramesh Raskar and Mariana Raykova and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao}
}

@misc{feng2015distributed,
      title={Distributed Robust Learning}, 
      author={Jiashi Feng and Huan Xu and Shie Mannor},
      year={2015},
      eprint={1409.5937},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{feng2017onlinelearning,
  author    = {Jiashi Feng and
               Huan Xu and
               Shie Mannor},
  title     = {Outlier Robust Online Learning},
  journal   = {CoRR},
  volume    = {abs/1701.00251},
  year      = {2017},
  eprinttype = {arXiv},
  eprint    = {1701.00251},
  timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Karimireddy2021,
      title = {Learning from History for Byzantine Robust Optimization},
      author = {Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
      publisher = {JMLR-JOURNAL MACHINE LEARNING RESEARCH},
      journal = {International Conference On Machine Learning, Vol 139},
      address = {San Diego},
      volume = {139},
      series = {Proceedings of Machine Learning Research},
      year = {2021},
      abstract = {Byzantine robustness has received significant attention  recently given its importance for distributed and federated  learning. In spite of this, we identify severe flaws in  existing algorithms even when the data across the  participants is identically distributed. First, we show  realistic examples where current state of the art robust  aggregation rules fail to converge even in the absence of  any Byzantine attackers. Secondly, we prove that even if  the aggregation rules may succeed in limiting the influence  of the attackers in a single round, the attackers can  couple their attacks across time eventually leading to  divergence. To address these issues, we present two  surprisingly simple strategies: a new robust iterative  clipping procedure, and incorporating worker momentum to  overcome time-coupled attacks. This is the first provably  robust method for the standard stochastic optimization  setting. Our code is open sourced at this link(2).},
}


@book{bertsekas2015parallel,
  title={Parallel and distributed computation: numerical methods},
  author={Bertsekas, Dimitri and Tsitsiklis, John},
  year={2015},
  publisher={Athena Scientific}
}

@inproceedings{
collaborativeElMhamdi21,
title={Collaborative Learning in the Jungle (Decentralized, Byzantine, Heterogeneous, Asynchronous and Nonconvex Learning)},
author={El Mhamdi, El Mahdi and Farhadkhani, Sadegh  and Guerraoui, Rachid  and Guirguis, Arsany  and Hoang, L{\^e} Nguy{\^e}n and Rouault, S{\'e}bastien },
booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
year={2021},
}

@article{Geomed21,
  author    = {El Mahdi El Mhamdi and
               Sadegh Farhadkhani and
               Rachid Guerraoui and
               L{\^{e}} Nguy{\^{e}}n Hoang},
  title     = {On the Strategyproofness of the Geometric Median},
  journal   = {CoRR},
  volume    = {abs/2106.02394},
  year      = {2021},
  eprinttype = {arXiv},
  eprint    = {2106.02394},
  timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


################################## learning with corrupted data ###################################
@inproceedings{lai2016agnostic,
  title={Agnostic estimation of mean and covariance},
  author={Lai, Kevin A and Rao, Anup B and Vempala, Santosh},
  booktitle={2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={665--674},
  year={2016},
  organization={IEEE}
}

@inproceedings{diakonikolas2017statistical,
  title={Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures},
  author={Diakonikolas, Ilias and Kane, Daniel M and Stewart, Alistair},
  booktitle={2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={73--84},
  year={2017},
  organization={IEEE}
}


