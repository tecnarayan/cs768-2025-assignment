\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agakov \& Barber(2004)Agakov and Barber]{agakov2004auxiliary}
Agakov, F.~V. and Barber, D.
\newblock {An auxiliary variational method}.
\newblock In \emph{International Conference on Neural Information Processing},
  pp.\  561--566. Springer, 2004.

\bibitem[Akimoto \& Hansen(2016)Akimoto and Hansen]{akimoto2016projection}
Akimoto, Y. and Hansen, N.
\newblock Projection-based restricted covariance matrix adaptation for high
  dimension.
\newblock In \emph{Proceedings of the Genetic and Evolutionary Computation
  Conference 2016}, pp.\  197--204, 2016.

\bibitem[Amari et~al.(2018)Amari, Ozeki, Karakida, Yoshida, and
  Okada]{amari2018dynamics}
Amari, S.-i., Ozeki, T., Karakida, R., Yoshida, Y., and Okada, M.
\newblock Dynamics of learning in mlp: Natural gradient and singularity
  revisited.
\newblock \emph{Neural computation}, 30\penalty0 (1):\penalty0 1--33, 2018.

\bibitem[Baba(1981)]{baba1981convergence}
Baba, N.
\newblock Convergence of a random optimization method for constrained
  optimization problems.
\newblock \emph{Journal of Optimization Theory and Applications}, 33\penalty0
  (4):\penalty0 451--461, 1981.

\bibitem[Belk(2013)]{Group-notes}
Belk, J.
\newblock {Lecture Notes: Matrix Groups}.
\newblock \url{http://faculty.bard.edu/belk/math332/MatrixGroups.pdf}, 2013.
\newblock Accessed: 2021/02.

\bibitem[Beyer(2001)]{beyer2001theory}
Beyer, H.-G.
\newblock \emph{The theory of evolution strategies}.
\newblock Springer Science \& Business Media, 2001.

\bibitem[Brickell et~al.(2008)Brickell, Dhillon, Sra, and
  Tropp]{brickell2008metric}
Brickell, J., Dhillon, I.~S., Sra, S., and Tropp, J.~A.
\newblock The metric nearness problem.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 30\penalty0
  (1):\penalty0 375--396, 2008.

\bibitem[Chen et~al.(2019)Chen, Chou, and Chang]{chen2019ea}
Chen, S.-W., Chou, C.-N., and Chang, E.~Y.
\newblock Ea-cg: An approximate second-order method for training
  fully-connected neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  3337--3346, 2019.

\bibitem[Dangel et~al.(2020)Dangel, Harmeling, and Hennig]{dangel2020modular}
Dangel, F., Harmeling, S., and Hennig, P.
\newblock Modular block-diagonal curvature approximations for feedforward
  architectures.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  799--808. PMLR, 2020.

\bibitem[Figurnov et~al.(2018)Figurnov, Mohamed, and
  Mnih]{figurnov2018implicit}
Figurnov, M., Mohamed, S., and Mnih, A.
\newblock Implicit reparameterization gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  441--452, 2018.

\bibitem[Glasmachers et~al.(2010)Glasmachers, Schaul, Yi, Wierstra, and
  Schmidhuber]{glasmachers2010exponential}
Glasmachers, T., Schaul, T., Yi, S., Wierstra, D., and Schmidhuber, J.
\newblock Exponential natural evolution strategies.
\newblock In \emph{Proceedings of the 12th annual conference on Genetic and
  evolutionary computation}, pp.\  393--400, 2010.

\bibitem[Graves(2011)]{graves2011practical}
Graves, A.
\newblock {Practical variational inference for neural networks}.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2348--2356, 2011.

\bibitem[Hazan et~al.(2016)Hazan, Levy, and Shalev-Shwartz]{hazan2016graduated}
Hazan, E., Levy, K.~Y., and Shalev-Shwartz, S.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{International conference on machine learning}, pp.\
  1833--1841. PMLR, 2016.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
Hoffman, M.~D., Blei, D.~M., Wang, C., and Paisley, J.
\newblock {Stochastic variational inference}.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Hosseini \& Sra(2015)Hosseini and Sra]{hosseini2015matrix}
Hosseini, R. and Sra, S.
\newblock {Matrix manifold optimization for Gaussian mixtures}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  910--918, 2015.

\bibitem[Johansen(1979)]{johansen1979introduction}
Johansen, S.
\newblock {Introduction to the theory of regular exponential famelies}.
\newblock 1979.

\bibitem[Khan \& Lin(2017)Khan and Lin]{khan2017conjugate}
Khan, M. and Lin, W.
\newblock Conjugate-computation variational inference: Converting variational
  inference in non-conjugate models to inferences in conjugate models.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  878--887,
  2017.

\bibitem[Khan \& Nielsen(2018)Khan and Nielsen]{khan2018fast}
Khan, M.~E. and Nielsen, D.
\newblock {Fast yet Simple Natural-Gradient Descent for Variational Inference
  in Complex Models}.
\newblock \emph{arXiv preprint arXiv:1807.04489}, 2018.

\bibitem[Khan \& Rue(2020)Khan and Rue]{emti2020bayesprinciple}
Khan, M.~E. and Rue, H.
\newblock {Learning-algorithms from Bayesian principles}.
\newblock 2020.
\newblock \url{https://emtiyaz.github.io/papers/learning_from_bayes.pdf}.

\bibitem[Khan et~al.(2017)Khan, Lin, Tangkaratt, Liu, and
  Nielsen]{khan2017variational}
Khan, M.~E., Lin, W., Tangkaratt, V., Liu, Z., and Nielsen, D.
\newblock {Variational adaptive-Newton method for explorative learning}.
\newblock \emph{arXiv preprint arXiv:1711.05560}, 2017.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{khan18a}
Khan, M.~E., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y., and Srivastava, A.
\newblock {Fast and scalable {B}ayesian deep learning by weight-perturbation in
  {A}dam}.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  2611--2620, 2018.

\bibitem[Leordeanu \& Hebert(2008)Leordeanu and Hebert]{leordeanu2008smoothing}
Leordeanu, M. and Hebert, M.
\newblock Smoothing-based optimization.
\newblock In \emph{2008 IEEE Conference on Computer Vision and Pattern
  Recognition}, pp.\  1--8. IEEE, 2008.

\bibitem[Lezcano~Casado(2019)]{lezcano2019trivializations}
Lezcano~Casado, M.
\newblock Trivializations for gradient-based optimization on manifolds.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 9157--9168, 2019.

\bibitem[Li \& Zhang(2017)Li and Zhang]{li2017simple}
Li, Z. and Zhang, Q.
\newblock A simple yet efficient evolution strategy for large-scale black-box
  optimization.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 22\penalty0
  (5):\penalty0 637--646, 2017.

\bibitem[Lin(2021)]{4114067}
Lin, W.
\newblock An upper triangular version of the cholesky decompostion.
\newblock https://math.stackexchange.com/q/4114067, 2021.
\newblock Accessed: 2021/04.

\bibitem[Lin et~al.(2019{\natexlab{a}})Lin, Khan, and Schmidt]{lin2019fast}
Lin, W., Khan, M.~E., and Schmidt, M.
\newblock Fast and simple natural-gradient variational inference with mixture
  of exponential-family approximations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3992--4002, 2019{\natexlab{a}}.

\bibitem[Lin et~al.(2019{\natexlab{b}})Lin, Khan, and Schmidt]{wu-report}
Lin, W., Khan, M.~E., and Schmidt, M.
\newblock {Stein's Lemma for the Reparameterization Trick with
  Exponential-family Mixtures}.
\newblock \emph{arXiv preprint arXiv:1910.13398}, 2019{\natexlab{b}}.

\bibitem[Lin et~al.(2020)Lin, Schmidt, and Khan]{lin2020handling}
Lin, W., Schmidt, M., and Khan, M.~E.
\newblock Handling the positive-definite constraint in the bayesian learning
  rule.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6116--6126. PMLR, 2020.

\bibitem[Lin et~al.(2021)Lin, Nielsen, Khan, and Schmidt]{lin2021snd}
Lin, W., Nielsen, F., Khan, M.~E., and Schmidt, M.
\newblock Structured second-order methods via natural gradient descent.
\newblock \emph{arXiv preprint arXiv:2107.10884}, 2021.

\bibitem[Malag{\`o} et~al.(2011)Malag{\`o}, Matteucci, and
  Pistone]{malago2011towards}
Malag{\`o}, L., Matteucci, M., and Pistone, G.
\newblock Towards the geometry of estimation of distribution algorithms based
  on the exponential family.
\newblock In \emph{Proceedings of the 11th workshop proceedings on Foundations
  of genetic algorithms}, pp.\  230--242, 2011.

\bibitem[Mishkin et~al.(2018)Mishkin, Kunstner, Nielsen, Schmidt, and
  Khan]{mishkin2018slang}
Mishkin, A., Kunstner, F., Nielsen, D., Schmidt, M., and Khan, M.~E.
\newblock {SLANG: Fast Structured Covariance Approximations for Bayesian Deep
  Learning with Natural Gradient}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6246--6256, 2018.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937. PMLR, 2016.

\bibitem[Mobahi \& Fisher~III(2015)Mobahi and
  Fisher~III]{mobahi2015theoretical}
Mobahi, H. and Fisher~III, J.
\newblock A theoretical analysis of optimization by gaussian continuation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~29, 2015.

\bibitem[O'leary \& Stewart(1990)O'leary and Stewart]{o1990computing}
O'leary, D. and Stewart, G.
\newblock Computing the eigenvalues and eigenvectors of symmetric arrowhead
  matrices.
\newblock \emph{Journal of Computational Physics}, 90\penalty0 (2):\penalty0
  497--505, 1990.

\bibitem[Opper \& Archambeau(2009)Opper and Archambeau]{opper2009variational}
Opper, M. and Archambeau, C.
\newblock {The variational Gaussian approximation revisited}.
\newblock \emph{Neural computation}, 21\penalty0 (3):\penalty0 786--792, 2009.

\bibitem[Osawa et~al.(2019{\natexlab{a}})Osawa, Swaroop, Khan, Jain,
  Eschenhagen, Turner, and Yokota]{osawa2019practical}
Osawa, K., Swaroop, S., Khan, M. E.~E., Jain, A., Eschenhagen, R., Turner,
  R.~E., and Yokota, R.
\newblock {Practical deep learning with Bayesian principles}.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4287--4299, 2019{\natexlab{a}}.

\bibitem[Osawa et~al.(2019{\natexlab{b}})Osawa, Tsuji, Ueno, Naruse, Yokota,
  and Matsuoka]{osawa2019large}
Osawa, K., Tsuji, Y., Ueno, Y., Naruse, A., Yokota, R., and Matsuoka, S.
\newblock Large-scale distributed second-order optimization using
  kronecker-factored approximate curvature for deep convolutional neural
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12359--12367, 2019{\natexlab{b}}.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and
  Blei]{ranganath2014black}
Ranganath, R., Gerrish, S., and Blei, D.
\newblock {Black box variational inference}.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  814--822,
  2014.

\bibitem[Ros \& Hansen(2008)Ros and Hansen]{ros2008simple}
Ros, R. and Hansen, N.
\newblock A simple modification in cma-es achieving linear time and space
  complexity.
\newblock In \emph{International Conference on Parallel Problem Solving from
  Nature}, pp.\  296--305. Springer, 2008.

\bibitem[Salimbeni et~al.(2018)Salimbeni, Eleftheriadis, and
  Hensman]{salimbeni2018natural}
Salimbeni, H., Eleftheriadis, S., and Hensman, J.
\newblock {Natural Gradients in Practice: Non-Conjugate Variational Inference
  in Gaussian Process Models}.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2018.

\bibitem[Schulz \& Seesanea(2018)Schulz and Seesanea]{HeisenbergdDim-2018}
Schulz, E. and Seesanea, A.
\newblock {Extensions of the Heisenberg group by two-parameter groups of
  dilations}.
\newblock \emph{arXiv preprint arXiv:1804.10305}, 2018.

\bibitem[Smith et~al.(1972)Smith, Hocking, et~al.]{smith1972wishart}
Smith, W., Hocking, R., et~al.
\newblock Wishart variate generator.
\newblock \emph{Applied Statistics}, 21:\penalty0 341--345, 1972.

\bibitem[Spall(2005)]{spall2005introduction}
Spall, J.~C.
\newblock \emph{Introduction to stochastic search and optimization: estimation,
  simulation, and control}, volume~65.
\newblock John Wiley \& Sons, 2005.

\bibitem[Sun \& Nielsen(2017)Sun and Nielsen]{sun2017relative}
Sun, K. and Nielsen, F.
\newblock Relative fisher information and natural gradient for learning large
  modular models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3289--3298, 2017.

\bibitem[Sun et~al.(2009)Sun, Wierstra, Schaul, and
  Schmidhuber]{sun2009efficient}
Sun, Y., Wierstra, D., Schaul, T., and Schmidhuber, J.
\newblock Efficient natural evolution strategies.
\newblock In \emph{Proceedings of the 11th Annual conference on Genetic and
  evolutionary computation}, pp.\  539--546, 2009.

\bibitem[Sun et~al.(2013)Sun, Schaul, Gomez, and Schmidhuber]{sun2013linear}
Sun, Y., Schaul, T., Gomez, F., and Schmidhuber, J.
\newblock A linear time natural evolution strategy for non-separable functions.
\newblock In \emph{Proceedings of the 15th annual conference companion on
  Genetic and evolutionary computation}, pp.\  61--62, 2013.

\bibitem[Sutton et~al.(1998)Sutton, Barto, et~al.]{sutton1998introduction}
Sutton, R.~S., Barto, A.~G., et~al.
\newblock \emph{Introduction to reinforcement learning}, volume 135.
\newblock MIT press Cambridge, 1998.

\bibitem[Teboulle(1992)]{teboulle1992}
Teboulle, M.
\newblock Entropic proximal mappings with applications to nonlinear
  programming.
\newblock \emph{Math. Oper. Res.}, 17\penalty0 (3):\penalty0 670–690, August
  1992.
\newblock ISSN 0364-765X.

\bibitem[Tran et~al.(2020)Tran, Nguyen, Nott, and Kohn]{tran2020bayesian}
Tran, M.-N., Nguyen, N., Nott, D., and Kohn, R.
\newblock Bayesian deep net glm and glmm.
\newblock \emph{Journal of Computational and Graphical Statistics}, 29\penalty0
  (1):\penalty0 97--113, 2020.

\bibitem[Wierstra et~al.(2008)Wierstra, Schaul, Peters, and
  Schmidhuber]{wierstra2008natural}
Wierstra, D., Schaul, T., Peters, J., and Schmidhuber, J.
\newblock Natural evolution strategies.
\newblock In \emph{2008 IEEE Congress on Evolutionary Computation (IEEE World
  Congress on Computational Intelligence)}, pp.\  3381--3387. IEEE, 2008.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Williams \& Peng(1991)Williams and Peng]{williams1991function}
Williams, R.~J. and Peng, J.
\newblock Function optimization using connectionist reinforcement learning
  algorithms.
\newblock \emph{Connection Science}, 3\penalty0 (3):\penalty0 241--268, 1991.

\bibitem[Zellner(1986)]{zellner1986bayesian}
Zellner, A.
\newblock Bayesian estimation and prediction using asymmetric loss functions.
\newblock \emph{Journal of the American Statistical Association}, 81\penalty0
  (394):\penalty0 446--451, 1986.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and Grosse]{zhang2018noisy}
Zhang, G., Sun, S., Duvenaud, D., and Grosse, R.
\newblock Noisy natural gradient as variational inference.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5847--5856, 2018.

\end{thebibliography}
