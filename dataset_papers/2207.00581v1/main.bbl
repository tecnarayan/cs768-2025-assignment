\begin{thebibliography}{10}

\bibitem{achille2019information}
Alessandro Achille, Giovanni Paolini, and Stefano Soatto.
\newblock Where is the information in a deep neural network?
\newblock {\em arXiv preprint arXiv:1905.12213}, 2019.

\bibitem{achille2018emergence}
Alessandro Achille and Stefano Soatto.
\newblock Emergence of invariance and disentanglement in deep representations.
\newblock {\em The Journal of Machine Learning Research}, 19(1):1947--1980,
  2018.

\bibitem{bu2019}
Yuheng Bu, Shaofeng Zou, and Venugopal~V. Veeravalli.
\newblock Tightening mutual information based bounds on generalization error.
\newblock In {\em 2019 IEEE International Symposium on Information Theory
  (ISIT)}, pages 587--591, 2019.

\bibitem{chaudhari2019entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2019(12):124018, 2019.

\bibitem{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In {\em International Conference on Machine Learning}, pages
  1019--1028. PMLR, 2017.

\bibitem{6289001}
J.-L. Durrieu, J.-Ph. Thiran, and F.~Kelly.
\newblock Lower and upper bounds for approximation of the kullback-leibler
  divergence between gaussian mixture models.
\newblock In {\em 2012 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4833--4836, 2012.

\bibitem{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock {\em arXiv preprint arXiv:1703.11008}, 2017.

\bibitem{haghifam2020}
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel~M Roy, and
  Gintare~Karolina Dziugaite.
\newblock Sharpened generalization bounds based on conditional mutual
  information and an application to noisy, iterative algorithms.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 9925--9935. Curran Associates, Inc., 2020.

\bibitem{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em International conference on machine learning}, pages
  1225--1234. PMLR, 2016.

\bibitem{recht2015}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48}, ICML'16, page
  1225–1234. JMLR.org, 2016.

\bibitem{harutyunyan2021estimating}
Hrayr Harutyunyan, Alessandro Achille, Giovanni Paolini, Orchid Majumder,
  Avinash Ravichandran, Rahul Bhotika, and Stefano Soatto.
\newblock Estimating informativeness of samples with smooth unique information.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{harutyunyan21}
Hrayr Harutyunyan, Maxim Raginsky, Greg Ver~Steeg, and Aram Galstyan.
\newblock Information-theoretic generalization bounds for black-box learning
  algorithms.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 24670--24682. Curran Associates, Inc., 2021.

\bibitem{hinton93keeping}
GE~Hinton and Drew van Camp.
\newblock Keeping neural networks simple by minimising the description length
  of weights. 1993.
\newblock In {\em Proceedings of COLT-93}, pages 5--13.

\bibitem{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock {\em Neural computation}, 9(1):1--42, 1997.

\bibitem{koh2017}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, page 1885–1894. JMLR.org, 2017.

\bibitem{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{negrea2019}
Jeffrey Negrea, Mahdi Haghifam, Gintare~Karolina Dziugaite, Ashish Khisti, and
  Daniel~M Roy.
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{neu21}
Gergely Neu, Gintare~Karolina Dziugaite, Mahdi Haghifam, and Daniel~M. Roy.
\newblock Information-theoretic generalization bounds for stochastic gradient
  descent.
\newblock In Mikhail Belkin and Samory Kpotufe, editors, {\em Proceedings of
  Thirty Fourth Conference on Learning Theory}, volume 134 of {\em Proceedings
  of Machine Learning Research}, pages 3526--3545. PMLR, 15--19 Aug 2021.

\bibitem{parkhi12a}
Omkar~M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C.~V. Jawahar.
\newblock Cats and dogs.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2012.

\bibitem{pensia2018}
Ankit Pensia, Varun Jog, and Po-Ling Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock {\em 2018 IEEE International Symposium on Information Theory (ISIT)},
  pages 546--550, 2018.

\bibitem{quattoni2009recognizing}
Ariadna Quattoni and Antonio Torralba.
\newblock Recognizing indoor scenes.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 413--420. IEEE, 2009.

\bibitem{russo16}
Daniel Russo and James Zou.
\newblock Controlling bias in adaptive data analysis using information theory.
\newblock In Arthur Gretton and Christian~C. Robert, editors, {\em Proceedings
  of the 19th International Conference on Artificial Intelligence and
  Statistics}, volume~51 of {\em Proceedings of Machine Learning Research},
  pages 1232--1240, Cadiz, Spain, 09--11 May 2016. PMLR.

\bibitem{shwartz2017opening}
Ravid Shwartz-Ziv and Naftali Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock {\em arXiv preprint arXiv:1703.00810}, 2017.

\bibitem{steinke2020}
Thomas Steinke and Lydia Zakynthinou.
\newblock {R}easoning {A}bout {G}eneralization via {C}onditional {M}utual
  {I}nformation.
\newblock In Jacob Abernethy and Shivani Agarwal, editors, {\em Proceedings of
  Thirty Third Conference on Learning Theory}, volume 125 of {\em Proceedings
  of Machine Learning Research}, pages 3437--3452. PMLR, 09--12 Jul 2020.

\bibitem{xu2017}
Aolin Xu and Maxim Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\end{thebibliography}
