\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Zhang(2004)]{zhang2004solving}
Tong Zhang.
\newblock Solving large scale linear prediction problems using stochastic
  gradient descent algorithms.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page 116. ACM, 2004.

\bibitem[Bottou(2010)]{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1223--1231, 2012.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Alistarh(2018)]{alistarh2018brief}
Dan Alistarh.
\newblock A brief tutorial on distributed and concurrent machine learning.
\newblock In \emph{Proceedings of the 2018 ACM Symposium on Principles of
  Distributed Computing}, pages 487--488. ACM, 2018.

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pages 265--283, 2016.

\bibitem[Seide and Agarwal(2016)]{seide2016cntk}
Frank Seide and Amit Agarwal.
\newblock Cntk: Microsoft's open-source deep-learning toolkit.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 2135--2135. ACM, 2016.

\bibitem[Chen et~al.(2015)Chen, Li, Li, Lin, Wang, Wang, Xiao, Xu, Zhang, and
  Zhang]{chen2015mxnet}
Tianqi Chen, Mu~Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
  Bing Xu, Chiyuan Zhang, and Zheng Zhang.
\newblock Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock \emph{arXiv preprint arXiv:1512.01274}, 2015.

\bibitem[Li et~al.(2014{\natexlab{a}})Li, Andersen, Park, Smola, Ahmed,
  Josifovski, Long, Shekita, and Su]{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{OSDI}, volume~14, pages 583--598, 2014{\natexlab{a}}.

\bibitem[Li et~al.(2014{\natexlab{b}})Li, Andersen, Smola, and
  Yu]{li2014communication}
Mu~Li, David~G Andersen, Alexander~J Smola, and Kai Yu.
\newblock Communication efficient distributed machine learning with the
  parameter server.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  19--27, 2014{\natexlab{b}}.

\bibitem[Gropp et~al.(1999)Gropp, Thakur, and Lusk]{gropp1999using}
William Gropp, Rajeev Thakur, and Ewing Lusk.
\newblock \emph{Using MPI-2: Advanced features of the message passing
  interface}.
\newblock MIT press, 1999.

\bibitem[Lian et~al.(2017{\natexlab{a}})Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5330--5340, 2017{\natexlab{a}}.

\bibitem[Lian et~al.(2017{\natexlab{b}})Lian, Zhang, Zhang, and
  Liu]{lian2017asynchronous}
Xiangru Lian, Wei Zhang, Ce~Zhang, and Ji~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1710.06952}, 2017{\natexlab{b}}.

\bibitem[Tang et~al.(2018{\natexlab{a}})Tang, Lian, Yan, Zhang, and
  Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D2: Decentralized training over decentralized data.
\newblock \emph{arXiv preprint arXiv:1803.07068}, 2018{\natexlab{a}}.

\bibitem[Hendrikx et~al.(2018)Hendrikx, Massouli{\'e}, and
  Bach]{hendrikx2018accelerated}
Hadrien Hendrikx, Laurent Massouli{\'e}, and Francis Bach.
\newblock Accelerated decentralized optimization with local updates for smooth
  and strongly convex objectives.
\newblock \emph{arXiv preprint arXiv:1810.02660}, 2018.

\bibitem[Zhang et~al.(2017)Zhang, Li, Kara, Alistarh, Liu, and
  Zhang]{zhang2017zipml}
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji~Liu, and Ce~Zhang.
\newblock Zipml: Training linear models with end-to-end low precision, and a
  little bit of deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4035--4043, 2017.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1509--1519, 2017.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem[Doan et~al.(2018)Doan, Maguluri, and Romberg]{doan2018convergence}
Thinh~T Doan, Siva~Theja Maguluri, and Justin Romberg.
\newblock On the convergence of distributed subgradient methods under
  quantization.
\newblock In \emph{2018 56th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 567--574. IEEE, 2018.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{wang2018atomo}
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris
  Papailiopoulos, and Stephen Wright.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9850--9861, 2018.

\bibitem[Tang et~al.(2018{\natexlab{b}})Tang, Gan, Zhang, Zhang, and
  Liu]{tang2018communication}
Hanlin Tang, Shaoduo Gan, Ce~Zhang, Tong Zhang, and Ji~Liu.
\newblock Communication compression for decentralized training.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7663--7673, 2018{\natexlab{b}}.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1306--1316, 2018.

\bibitem[De~Sa et~al.(2018)De~Sa, Leszczynski, Zhang, Marzoev, Aberger,
  Olukotun, and R{\'e}]{de2018high}
Christopher De~Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher~R
  Aberger, Kunle Olukotun, and Christopher R{\'e}.
\newblock High-accuracy low-precision training.
\newblock \emph{arXiv preprint arXiv:1803.03383}, 2018.

\bibitem[Tang et~al.(2018{\natexlab{c}})Tang, Yu, Renggli, Kassing, Singla,
  Alistarh, Liu, and Zhang]{tang2018distributed}
Hanlin Tang, Chen Yu, Cedric Renggli, Simon Kassing, Ankit Singla, Dan
  Alistarh, Ji~Liu, and Ce~Zhang.
\newblock Distributed learning over unreliable networks.
\newblock \emph{arXiv preprint arXiv:1810.07766}, 2018{\natexlab{c}}.

\bibitem[Tang et~al.(2019)Tang, Lian, Qiu, Yuan, Zhang, Zhang, and
  Liu]{tang2019texttt}
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce~Zhang, Tong Zhang, and
  Ji~Liu.
\newblock Deepsqueeze: Parallel stochastic gradient descent with double-pass
  error-compensated compression.
\newblock \emph{arXiv preprint arXiv:1907.07346}, 2019.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock \emph{arXiv preprint arXiv:1902.00340}, 2019.

\bibitem[Mokhtari and Ribeiro(2015)]{mokhtari2015decentralized}
Aryan Mokhtari and Alejandro Ribeiro.
\newblock Decentralized double stochastic averaging gradient.
\newblock In \emph{Signals, Systems and Computers, 2015 49th Asilomar
  Conference on}, pages 406--410. IEEE, 2015.

\bibitem[Sirb and Ye(2016)]{sirb2016consensus}
Benjamin Sirb and Xiaojing Ye.
\newblock Consensus optimization with delayed and stochastic gradients on
  decentralized networks.
\newblock In \emph{Big Data (Big Data), 2016 IEEE International Conference on},
  pages 76--85. IEEE, 2016.

\bibitem[Lan et~al.(2017)Lan, Lee, and Zhou]{lan2017communication}
Guanghui Lan, Soomin Lee, and Yi~Zhou.
\newblock Communication-efficient algorithms for decentralized and stochastic
  optimization.
\newblock \emph{arXiv preprint arXiv:1701.03961}, 2017.

\bibitem[Wu et~al.(2018{\natexlab{a}})Wu, Yuan, Ling, Yin, and
  Sayed]{wu2018decentralized}
Tianyu Wu, Kun Yuan, Qing Ling, Wotao Yin, and Ali~H Sayed.
\newblock Decentralized consensus optimization with asynchrony and delays.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 4\penalty0 (2):\penalty0 293--307, 2018{\natexlab{a}}.

\bibitem[He et~al.(2018)He, Bian, and Jaggi]{he2018cola}
Lie He, An~Bian, and Martin Jaggi.
\newblock Cola: Decentralized linear learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4541--4551, 2018.

\bibitem[Nazari et~al.(2019)Nazari, Tarzanagh, and
  Michailidis]{nazari2019dadam}
Parvin Nazari, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Dadam: A consensus-based distributed adaptive gradient method for
  online optimization.
\newblock \emph{arXiv preprint arXiv:1901.09109}, 2019.

\bibitem[Zhang and You(2019)]{zhang2019asynchronous}
Jiaqi Zhang and Keyou You.
\newblock Asynchronous decentralized optimization in directed networks.
\newblock \emph{arXiv preprint arXiv:1901.08215}, 2019.

\bibitem[Assran et~al.(2018)Assran, Loizou, Ballas, and
  Rabbat]{assran2018stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock \emph{arXiv preprint arXiv:1811.10792}, 2018.

\bibitem[Suresh et~al.(2017)Suresh, Yu, Kumar, and
  McMahan]{suresh2017distributed}
Ananda~Theertha Suresh, Felix~X Yu, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3329--3337. JMLR. org, 2017.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4452--4463, 2018.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov,
  Khirirat, and Renggli]{alistarh2018convergence}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit
  Khirirat, and C{\'e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5973--5983, 2018.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Grubic et~al.(2018)Grubic, Tam, Alistarh, and
  Zhang]{grubic2018synchronous}
D~Grubic, L~Tam, Dan Alistarh, and Ce~Zhang.
\newblock Synchronous multi-gpu deep learning with low-precision communication:
  An experimental study.
\newblock \emph{Proceedings of the EDBT 2018}, 2018.

\bibitem[Jiang and Agrawal(2018)]{jiang2018linear}
Peng Jiang and Gagan Agrawal.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2525--2536, 2018.

\bibitem[Acharya et~al.(2019)Acharya, De~Sa, Foster, and
  Sridharan]{acharya2019distributed}
Jayadev Acharya, Christopher De~Sa, Dylan~J Foster, and Karthik Sridharan.
\newblock Distributed learning with sublinear communication.
\newblock \emph{arXiv preprint arXiv:1902.11259}, 2019.

\bibitem[Reisizadeh et~al.(2018)Reisizadeh, Mokhtari, Hassani, and
  Pedarsani]{DBLP:journals/corr/abs-1806-11536}
Amirhossein Reisizadeh, Aryan Mokhtari, S.~Hamed Hassani, and Ramtin Pedarsani.
\newblock Quantized decentralized consensus optimization.
\newblock \emph{CoRR}, abs/1806.11536, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.11536}.

\bibitem[Wu et~al.(2018{\natexlab{b}})Wu, Huang, Huang, and Zhang]{wu2018error}
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang.
\newblock Error compensated quantized sgd and its applications to large-scale
  distributed optimization.
\newblock \emph{arXiv preprint arXiv:1806.08054}, 2018{\natexlab{b}}.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International Conference on Machine Learning}, pages
  1737--1746, 2015.

\bibitem[De~Sa et~al.(2017)De~Sa, Feldman, R{\'e}, and
  Olukotun]{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In \emph{ACM SIGARCH Computer Architecture News}, volume~45, pages
  561--574. ACM, 2017.

\bibitem[Stich(2018)]{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Li et~al.(2017)Li, De, Xu, Studer, Samet, and
  Goldstein]{li2017training}
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein.
\newblock Training quantized nets: A deeper understanding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5811--5821, 2017.

\bibitem[Levin and Peres(2017)]{levin2017markov}
David~A Levin and Yuval Peres.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Krizhevsky et~al.(2014)Krizhevsky, Nair, and
  Hinton]{krizhevsky2014cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock The cifar-10 dataset.
\newblock \emph{online: http://www. cs. toronto. edu/kriz/cifar. html}, 2014.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Bergstra and Bengio(2012)]{bergstra2012random}
James Bergstra and Yoshua Bengio.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of machine learning research}, 13\penalty0
  (Feb):\penalty0 281--305, 2012.

\bibitem[Al-Riyami and Paterson(2003)]{al2003certificateless}
Sattam~S Al-Riyami and Kenneth~G Paterson.
\newblock Certificateless public key cryptography.
\newblock In \emph{International conference on the theory and application of
  cryptology and information security}, pages 452--473. Springer, 2003.

\end{thebibliography}
