@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{russo2013eluder,
  title={Eluder dimension and the sample complexity of optimistic exploration},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}
@article{wang2020reinforcement,
  title={Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension},
  author={Wang, Ruosong and Salakhutdinov, Russ R and Yang, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6123--6135},
  year={2020}
}

@inproceedings{wu2022nearly,
  title={Nearly optimal policy optimization with stable at any time guarantee},
  author={Wu, Tianhao and Yang, Yunchang and Zhong, Han and Wang, Liwei and Du, Simon and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={24243--24265},
  year={2022},
  organization={PMLR}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
@inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={49--58},
  year={2016},
  organization={PMLR}
}

@article{berner2019dota,
  author       = {Christopher Berner and
                  Greg Brockman and
                  Brooke Chan and
                  Vicki Cheung and
                  Przemyslaw Debiak and
                  Christy Dennison and
                  David Farhi and
                  Quirin Fischer and
                  Shariq Hashme and
                  Christopher Hesse and
                  Rafal J{\'{o}}zefowicz and
                  Scott Gray and
                  Catherine Olsson and
                  Jakub Pachocki and
                  Michael Petrov and
                  Henrique Pond{\'{e}} de Oliveira Pinto and
                  Jonathan Raiman and
                  Tim Salimans and
                  Jeremy Schlatter and
                  Jonas Schneider and
                  Szymon Sidor and
                  Ilya Sutskever and
                  Jie Tang and
                  Filip Wolski and
                  Susan Zhang},
  title        = {Dota 2 with Large Scale Deep Reinforcement Learning},
  year         = {2019},
  journal={arXiv preprint arXiv:1912.06680},
}


@misc{chatgpt,
      author = {OpenAI},
      title = {{ChatGPT}: Optimizing Language Models for Dialogue},
      url = {https://openai.com/blog/chatgpt/},
      year = {2022}}
      
@article{agarwal2021theory,
  title={On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={98},
  pages={1--76},
  year={2021}
}
@article{bhandari2019global,
  title={Global optimality guarantees for policy gradient methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:1906.01786},
  year={2019}
}
@article{liu2019neural,
  title={Neural proximal/trust region policy optimization attains globally optimal policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1906.10306},
  year={2019}
}
@article{neu2017unified,
  title={A unified view of entropy-regularized {M}arkov decision processes},
  author={Neu, Gergely and Jonsson, Anders and G{\'o}mez, Vicen{\c{c}}},
  journal={arXiv preprint arXiv:1705.07798},
  year={2017}
}
@inproceedings{abbasi2019politex,
  title={Politex: Regret bounds for policy iteration using expert prediction},
  author={Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gell{\'e}rt},
  booktitle={International Conference on Machine Learning},
  pages={3692--3702},
  year={2019},
  organization={PMLR}
}
@article{agarwal2020pc,
  title={Pc-pg: Policy cover directed exploration for provable policy gradient learning},
  author={Agarwal, Alekh and Henaff, Mikael and Kakade, Sham and Sun, Wen},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={13399--13412},
  year={2020}
}
@inproceedings{zanette2021cautiously,
  title={Cautiously optimistic policy optimization and exploration with linear function approximation},
  author={Zanette, Andrea and Cheng, Ching-An and Agarwal, Alekh},
  booktitle={Conference on Learning Theory},
  pages={4473--4525},
  year={2021},
  organization={PMLR}
}
@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent {B}ellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}
@article{jin2021bellman,
  title={{B}ellman eluder dimension: New rich classes of {RL} problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13406--13418},
  year={2021}
}
@article{he2022nearly,
  title={Nearly Minimax Optimal Reinforcement Learning for Linear {M}arkov Decision Processes},
  author={He, Jiafan and Zhao, Heyang and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2212.06132},
  year={2022}
}
@article{agarwal2022vo,
  title={{VO}$Q${L}: Towards Optimal Regret in Model-free {RL} with Nonlinear Function Approximation},
  author={Agarwal, Alekh and Jin, Yujia and Zhang, Tong},
  journal={arXiv preprint arXiv:2212.06069},
  year={2022}
}
@inproceedings{wagenmaker2022reward,
  title={Reward-free {RL} is no harder than reward-aware {RL} in linear {M}arkov decision processes},
  author={Wagenmaker, Andrew J and Chen, Yifang and Simchowitz, Max and Du, Simon and Jamieson, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={22430--22456},
  year={2022},
  organization={PMLR}
}

@inproceedings{cai2020provably,
  title={Provably efficient exploration in policy optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={1283--1294},
  year={2020},
  organization={PMLR}
}
@inproceedings{zhou2021nearly,
  title={Nearly minimax optimal reinforcement learning for linear mixture {M}arkov decision processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  pages={4532--4576},
  year={2021},
  organization={PMLR}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@inproceedings{shani2020optimistic,
  title={Optimistic policy optimization with bandit feedback},
  author={Shani, Lior and Efroni, Yonathan and Rosenberg, Aviv and Mannor, Shie},
  booktitle={International Conference on Machine Learning},
  pages={8604--8613},
  year={2020},
  organization={PMLR}
}
@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}
@article{abdolmaleki2018maximum,
  title={Maximum a posteriori policy optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1806.06920},
  year={2018}
}
@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}
@article{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@article{zhang2020almost,
  title={Almost optimal model-free reinforcement learningvia reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}
@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}