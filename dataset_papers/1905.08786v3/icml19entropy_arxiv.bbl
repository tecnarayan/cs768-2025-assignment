\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pp.\  265--283, 2016.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, O.~P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5048–5058, 2017.

\bibitem[Asadi \& Littman(2016)Asadi and Littman]{asadi2016alternative}
Asadi, K. and Littman, M.~L.
\newblock An alternative softmax operator for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1612.05628}, 2016.

\bibitem[Bakker \& Schmidhuber(2004)Bakker and
  Schmidhuber]{bakker2004hierarchical}
Bakker, B. and Schmidhuber, J.
\newblock Hierarchical reinforcement learning based on subgoal discovery and
  subpolicy specialization.
\newblock In \emph{Proc. of the 8-th Conf. on Intelligent Autonomous Systems},
  pp.\  438--445, 2004.

\bibitem[Benesty et~al.(2009)Benesty, Chen, Huang, and
  Cohen]{benesty2009pearson}
Benesty, J., Chen, J., Huang, Y., and Cohen, I.
\newblock Pearson correlation coefficient.
\newblock In \emph{Noise reduction in speech processing}, pp.\  1--4. Springer,
  2009.

\bibitem[Bordes et~al.(2016)Bordes, Boureau, and Weston]{bordes2016learning}
Bordes, A., Boureau, Y.-L., and Weston, J.
\newblock Learning end-to-end goal-oriented dialog.
\newblock \emph{arXiv preprint arXiv:1605.07683}, 2016.

\bibitem[Caruana(1997)]{caruana1997multitask}
Caruana, R.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Chebotar et~al.(2017)Chebotar, Kalakrishnan, Yahya, Li, Schaal, and
  Levine]{chebotar2017path}
Chebotar, Y., Kalakrishnan, M., Yahya, A., Li, A., Schaal, S., and Levine, S.
\newblock Path integral guided policy search.
\newblock In \emph{Robotics and Automation (ICRA), 2017 IEEE International
  Conference on}, pp.\  3381--3388. IEEE, 2017.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{baselines}
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SJx63jRqFm}.

\bibitem[Florensa et~al.(2018)Florensa, Held, Geng, and
  Abbeel]{florensa2018automatic}
Florensa, C., Held, D., Geng, X., and Abbeel, P.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1514--1523, 2018.

\bibitem[Forestier et~al.(2017)Forestier, Mollard, and
  Oudeyer]{forestier2017intrinsically}
Forestier, S., Mollard, Y., and Oudeyer, P.-Y.
\newblock Intrinsically motivated goal exploration processes with automatic
  curriculum learning.
\newblock \emph{arXiv preprint arXiv:1708.02190}, 2017.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem[Guia{\c{s}}u(1971)]{guiacsu1971weighted}
Guia{\c{s}}u, S.
\newblock Weighted entropy.
\newblock \emph{Reports on Mathematical Physics}, 2\penalty0 (3):\penalty0
  165--179, 1971.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock \emph{arXiv preprint arXiv:1702.08165}, 2017.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Hartikainen, Abbeel, and
  Levine]{haarnoja2018latent}
Haarnoja, T., Hartikainen, K., Abbeel, P., and Levine, S.
\newblock Latent space policies for hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1804.02808}, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Pong, Zhou, Dalal,
  Abbeel, and Levine]{haarnoja2018composable}
Haarnoja, T., Pong, V., Zhou, A., Dalal, M., Abbeel, P., and Levine, S.
\newblock Composable deep reinforcement learning for robotic manipulation.
\newblock \emph{arXiv preprint arXiv:1803.06773}, 2018{\natexlab{b}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{c}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  1861--1870. PMLR, 2018{\natexlab{c}}.

\bibitem[Kadelburg et~al.(2005)Kadelburg, Dukic, Lukic, and
  Matic]{kadelburg2005inequalities}
Kadelburg, Z., Dukic, D., Lukic, M., and Matic, I.
\newblock Inequalities of karamata, schur and muirhead, and some applications.
\newblock \emph{The Teaching of Mathematics}, 8\penalty0 (1):\penalty0 31--45,
  2005.

\bibitem[Kaelbling(1993)]{kaelbling1993hierarchical}
Kaelbling, L.~P.
\newblock Hierarchical learning in stochastic domains: Preliminary results.
\newblock In \emph{Proceedings of the tenth international conference on machine
  learning}, volume 951, pp.\  167--173, 1993.

\bibitem[Kelbert et~al.(2017)Kelbert, Stuhl, and Suhov]{Kelbert2017}
Kelbert, M., Stuhl, I., and Suhov, Y.
\newblock {Weighted entropy: basic inequalities}.
\newblock \emph{Modern Stochastics: Theory and Applications}, 4\penalty0
  (3):\penalty0 233--252, 2017.
\newblock \doi{10.15559/17-VMSTA85}.
\newblock URL \url{www.i-journals.org/vmsta}.

\bibitem[Levine(2018)]{levine2018reinforcement}
Levine, S.
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334–1373, 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Lin(1992)]{lin1992self}
Lin, L.-J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293–321, 1992.

\bibitem[Marshall et~al.(1979)Marshall, Olkin, and
  Arnold]{marshall1979inequalities}
Marshall, A.~W., Olkin, I., and Arnold, B.~C.
\newblock \emph{Inequalities: theory of majorization and its applications},
  volume 143.
\newblock Springer, 1979.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1928--1937, 2016.

\bibitem[Mohamed \& Rezende(2015)Mohamed and Rezende]{mohamed2015variational}
Mohamed, S. and Rezende, D.~J.
\newblock Variational information maximisation for intrinsically motivated
  reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2125--2133, 2015.

\bibitem[Murphy(2012)]{murphy2012machine}
Murphy, K.~P.
\newblock Machine learning: A probabilistic perspective. adaptive computation
  and machine learning, 2012.

\bibitem[Nachum et~al.(2016)Nachum, Norouzi, and
  Schuurmans]{nachum2016improving}
Nachum, O., Norouzi, M., and Schuurmans, D.
\newblock Improving policy gradient by exploring under-appreciated rewards.
\newblock \emph{arXiv preprint arXiv:1611.09321}, 2016.

\bibitem[Nair et~al.(2018)Nair, Pong, Dalal, Bahl, Lin, and
  Levine]{nair2018visual}
Nair, A.~V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S.
\newblock Visual reinforcement learning with imagined goals.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9191--9200, 2018.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Ng et~al.(2006)Ng, Coates, Diel, Ganapathi, Schulte, Tse, Berger, and
  Liang]{ng2006autonomous}
Ng, A.~Y., Coates, A., Diel, M., Ganapathi, V., Schulte, J., Tse, B., Berger,
  E., and Liang, E.
\newblock Autonomous inverted helicopter flight via reinforcement learning.
\newblock In \emph{Experimental Robotics IX}, pp.\  363–372. Springer, 2006.

\bibitem[Pan et~al.(2010)Pan, Yang, et~al.]{pan2010survey}
Pan, S.~J., Yang, Q., et~al.
\newblock A survey on transfer learning.
\newblock \emph{IEEE Transactions on knowledge and data engineering},
  22\penalty0 (10):\penalty0 1345--1359, 2010.

\bibitem[P{\'e}r{\'e} et~al.(2018)P{\'e}r{\'e}, Forestier, Sigaud, and
  Oudeyer]{pere2018unsupervised}
P{\'e}r{\'e}, A., Forestier, S., Sigaud, O., and Oudeyer, P.-Y.
\newblock Unsupervised learning of goal spaces for intrinsically motivated goal
  exploration.
\newblock \emph{arXiv preprint arXiv:1803.00781}, 2018.

\bibitem[Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and
  Hinton]{pereyra2017regularizing}
Pereyra, G., Tucker, G., Chorowski, J., Kaiser, {\L}., and Hinton, G.
\newblock Regularizing neural networks by penalizing confident output
  distributions.
\newblock \emph{arXiv preprint arXiv:1701.06548}, 2017.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Petrov()]{entrostack}
Petrov, F.
\newblock Shannon entropy of $p(x)(1-p(x))$ is no less than entropy of $p(x)$.
\newblock MathOverflow.
\newblock URL \url{https://mathoverflow.net/q/320726}.
\newblock URL:https://mathoverflow.net/q/320726 (version: 2019-01-12).

\bibitem[Pinto \& Gupta(2017)Pinto and Gupta]{pinto2017learning}
Pinto, L. and Gupta, A.
\newblock Learning to push by grasping: Using multiple tasks for effective
  learning.
\newblock In \emph{Robotics and Automation (ICRA), 2017 IEEE International
  Conference on}, pp.\  2161--2168. IEEE, 2017.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, McGrew, Baker,
  Powell, Schneider, Tobin, Chociej, Welinder, et~al.]{plappert2018multi}
Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G.,
  Schneider, J., Tobin, J., Chociej, M., Welinder, P., et~al.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock \emph{arXiv preprint arXiv:1802.09464}, 2018.

\bibitem[Rauber et~al.(2017)Rauber, Mutz, and Schmidhuber]{rauber2017hindsight}
Rauber, P., Mutz, F., and Schmidhuber, J.
\newblock Hindsight policy gradients.
\newblock \emph{arXiv preprint arXiv:1711.06006}, 2017.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{schaul2015universal}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1312--1320, 2015.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Chen, and
  Abbeel]{schulman2017equivalence}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484–489, 2016.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{sutton2011horde}
Sutton, R.~S., Modayil, J., Delp, M., Degris, T., Pilarski, P.~M., White, A.,
  and Precup, D.
\newblock Horde: A scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pp.\  761--768. International Foundation for
  Autonomous Agents and Multiagent Systems, 2011.

\bibitem[Szepesvari et~al.(2014)Szepesvari, Sutton, Modayil, Bhatnagar,
  et~al.]{szepesvari2014universal}
Szepesvari, C., Sutton, R.~S., Modayil, J., Bhatnagar, S., et~al.
\newblock Universal option models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  990--998, 2014.

\bibitem[Warde-Farley et~al.(2019)Warde-Farley, Van~de Wiele, Kulkarni,
  Ionescu, Hansen, and Mnih]{warde2018unsupervised}
Warde-Farley, D., Van~de Wiele, T., Kulkarni, T., Ionescu, C., Hansen, S., and
  Mnih, V.
\newblock Unsupervised control through non-parametric discriminative rewards.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=r1eVMnA9K7}.

\bibitem[Williams \& Peng(1991)Williams and Peng]{williams1991function}
Williams, R.~J. and Peng, J.
\newblock Function optimization using connectionist reinforcement learning
  algorithms.
\newblock \emph{Connection Science}, 3\penalty0 (3):\penalty0 241--268, 1991.

\bibitem[Wu \& Tian(2016)Wu and Tian]{wu2016training}
Wu, Y. and Tian, Y.
\newblock Training agent for first-person shooter game with actor-critic
  curriculum learning.
\newblock 2016.

\bibitem[Zhao \& Tresp(2018{\natexlab{a}})Zhao and Tresp]{zhao2018efficient}
Zhao, R. and Tresp, V.
\newblock Efficient dialog policy learning via positive memory retention.
\newblock In \emph{2018 IEEE Spoken Language Technology Workshop (SLT)}, pp.\
  823--830. IEEE, 2018{\natexlab{a}}.

\bibitem[Zhao \& Tresp(2018{\natexlab{b}})Zhao and Tresp]{zhao2018energy}
Zhao, R. and Tresp, V.
\newblock Energy-based hindsight experience prioritization.
\newblock In \emph{Proceedings of the 2nd Conference on Robot Learning}, pp.\
  113--122, 2018{\natexlab{b}}.

\bibitem[Zhao \& Tresp(2018{\natexlab{c}})Zhao and Tresp]{zhao2018improving}
Zhao, R. and Tresp, V.
\newblock Improving goal-oriented visual dialog agents via advanced recurrent
  nets with tempered policy gradient.
\newblock \emph{arXiv preprint arXiv:1807.00737}, 2018{\natexlab{c}}.

\bibitem[Zhao \& Tresp(2018{\natexlab{d}})Zhao and Tresp]{zhao2018learning}
Zhao, R. and Tresp, V.
\newblock Learning goal-oriented visual dialog via tempered policy gradient.
\newblock In \emph{2018 IEEE Spoken Language Technology Workshop (SLT)}, pp.\
  868--875. IEEE, 2018{\natexlab{d}}.

\bibitem[Zhao \& Tresp(2019)Zhao and Tresp]{zhao2019curiosity}
Zhao, R. and Tresp, V.
\newblock Curiosity-driven experience prioritization via density estimation.
\newblock \emph{arXiv preprint arXiv:1902.08039}, 2019.

\bibitem[Zhao et~al.(2017)Zhao, Ali, and Van~der Smagt]{zhao2017two}
Zhao, R., Ali, H., and Van~der Smagt, P.
\newblock Two-stream rnn/cnn for action recognition in 3d videos.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  4260--4267. IEEE, 2017.

\end{thebibliography}
