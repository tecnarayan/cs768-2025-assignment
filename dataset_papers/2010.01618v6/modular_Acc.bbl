\begin{thebibliography}{109}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alacaoglu et~al.(2020)Alacaoglu, Malitsky, Mertikopoulos, and
  Cevher]{AMMC20}
Alacaoglu, A., Malitsky, Y., Mertikopoulos, P., and Cevher, V.
\newblock A new regret analysis for adam-type algorithms.
\newblock \emph{ICML}, 2020.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{ZL19_icml}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via overparameterization.
\newblock \emph{ICML}, 2019.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{ACH18}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock \emph{ICML}, 2018.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{ACGH19}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Cohen, Hu, and Luo]{ACHL19}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{NerurIPS}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Du, Hu, Li, and
  Wang]{ADHLSW19_icml}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{NeurIPS}, 2019{\natexlab{c}}.

\bibitem[Aujol et~al.(2020)Aujol, Dossal, and Rondepierre]{ADR20}
Aujol, J.-F., Dossal, C., and Rondepierre, A.
\newblock Convergence rates of the heavy-ball method with lojasiewicz property.
\newblock \emph{hal-02928958}, 2020.

\bibitem[Bai \& Lee(2020)Bai and Lee]{BL20}
Bai, Y. and Lee, J.~D.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock \emph{ICLR}, 2020.

\bibitem[Bietti \& Mairal(2019)Bietti and Mairal]{BM19}
Bietti, A. and Mairal, J.
\newblock On the inductive bias of neural tangent kernels.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Brutzkus \& Globerson(2017)Brutzkus and Globerson]{BG17}
Brutzkus, A. and Globerson, A.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock \emph{ICML}, 2017.

\bibitem[Cai et~al.(2019)Cai, Gao, Hou, Chen, Wang, He, Zhang, and
  Wang]{Cetal19}
Cai, T., Gao, R., Hou, J., Chen, S., Wang, D., He, D., Zhang, Z., and Wang, L.
\newblock A gram-gauss-newton method learning overparameterized deep neural
  networks for regression problems.
\newblock \emph{arXiv.org:1905.11675}, 2019.

\bibitem[Can et~al.(2019)Can, G{\"u}rb{\"u}zbalaban, and Zhu]{CGZ19}
Can, B., G{\"u}rb{\"u}zbalaban, M., and Zhu, L.
\newblock Accelerated linear convergence of stochastic momentum methods in
  wasserstein distances.
\newblock \emph{ICML}, 2019.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, He, and Su]{CHS20}
Chen, S., He, H., and Su, W.~J.
\newblock Label-aware neural tangent kernel: Toward better generalization and
  local elasticity.
\newblock \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Cao, Gu, and Zhang]{CCGZ20}
Chen, Z., Cao, Y., Gu, Q., and Zhang, T.
\newblock A generalized neural tangent kernel analysis for two-layer neural
  network.
\newblock \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{COB19}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{CO19}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Daniely(2017)]{Dan17}
Daniely, A.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Daniely(2020)]{D20}
Daniely, A.
\newblock Memorizing gaussians with no over-parameterizaion via gradient decent
  on neural networks.
\newblock \emph{arXiv:1909.11837}, 2020.

\bibitem[Danilova et~al.(2018)Danilova, Kulakova, and Polyak]{DKB18}
Danilova, M., Kulakova, A., and Polyak, B.
\newblock Non-monotone behavior of the heavy ball method.
\newblock \emph{arXiv:1811.00658}, 2018.

\bibitem[Diakonikolas \& Jordan(2019)Diakonikolas and Jordan]{DJ19}
Diakonikolas, J. and Jordan, M.~I.
\newblock Generalized momentum-based methods: A hamiltonian perspective.
\newblock \emph{arXiv:1906.00436}, 2019.

\bibitem[Du \& Hu(2019)Du and Hu]{DH19}
Du, S.~S. and Hu, W.
\newblock Width provably matters in optimization for deep linear neural
  networks.
\newblock \emph{ICML}, 2019.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{DLLWZ16}
Du, S.~S., Lee, J.~D., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{ICML}, 2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{DZPS19}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{ICLR}, 2019{\natexlab{b}}.

\bibitem[Dukler et~al.(2020)Dukler, Gu, and Montufar]{DGM20}
Dukler, Y., Gu, Q., and Montufar, G.
\newblock Optimization theory for relu neural networks trained with
  normalization layers.
\newblock \emph{ICML}, 2020.

\bibitem[Fang et~al.(2019)Fang, Dong, and Zhang]{FDZ19}
Fang, C., Dong, H., and Zhang, T.
\newblock Over parameterized two-level neural networks can learn near optimal
  feature representations.
\newblock \emph{arXiv:1910.11508}, 2019.

\bibitem[Flammarion \& Bach(2015)Flammarion and Bach]{NB15}
Flammarion, N. and Bach, F.
\newblock From averaging to acceleration, there is only a step-size.
\newblock \emph{COLT}, 2015.

\bibitem[Foucart(2018)]{F18}
Foucart, S.
\newblock Matrix norms and spectral radii.
\newblock \emph{Online lecture note}, 2018.

\bibitem[Franca et~al.(2020)Franca, Sulam, Robinson, and Vidal]{FSRV20}
Franca, G., Sulam, J., Robinson, D.~P., and Vidal, R.
\newblock Conformal symplectic and relativistic optimization.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2020.

\bibitem[Gadat et~al.(2016)Gadat, Panloup, and Saadane]{GPS16}
Gadat, S., Panloup, F., and Saadane, S.
\newblock Stochastic heavy ball.
\newblock \emph{arXiv:1609.04228}, 2016.

\bibitem[Ge et~al.(2019)Ge, Kuditipudi, Li, and Wang]{GKLW16}
Ge, R., Kuditipudi, R., Li, Z., and Wang, X.
\newblock Learning two-layer neural networks with symmetric inputs.
\newblock \emph{ICLR}, 2019.

\bibitem[Gelfand(1941)]{G41}
Gelfand, I.
\newblock Normierte ringe.
\newblock \emph{Mat. Sbornik}, 1941.

\bibitem[Ghadimi et~al.(2015)Ghadimi, Feyzmahdavian, and Johansson]{GFJ15}
Ghadimi, E., Feyzmahdavian, H.~R., and Johansson, M.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock \emph{ECC}, 2015.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, , and
  Montanari]{BSMM19}
Ghorbani, B., Mei, S., Misiakiewicz, T., , and Montanari, A.
\newblock Linearized two-layers neural networks in high dimension.
\newblock \emph{arXiv:1904.12191}, 2019.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{GBL19}
Gidel, G., Bach, F., and Lacoste-Julien, S.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Gitman et~al.(2019)Gitman, Lang, Zhang, and Xiao]{GLZX19}
Gitman, I., Lang, H., Zhang, P., and Xiao, L.
\newblock Understanding the role of momentum in stochastic gradient methods.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Goh(2017)]{goh2017why}
Goh, G.
\newblock Why momentum really works.
\newblock \emph{Distill}, 2017.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{GWBNS17}
Gunasekar, S., Woodworth, B., Bhojanapalli, S., Neyshabur, B., and Srebro, N.
\newblock Implicit regularization in matrix factorization.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Hanin \& Nica(2020)Hanin and Nica]{HN20}
Hanin, B. and Nica, M.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock \emph{ICLR}, 2020.

\bibitem[Hardt \& Ma(2016)Hardt and Ma]{HM16}
Hardt, M. and Ma, T.
\newblock Identity matters in deep learning.
\newblock \emph{ICLR}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Rnet16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{Conference on Computer Vision and Pattern Recognition (CVPR)},
  2016.

\bibitem[Hu(2020)]{H20}
Hu, B.
\newblock Unifying the analysis in control and optimization via semidefinite
  programs.
\newblock \emph{Lecture Note}, 2020.

\bibitem[Hu et~al.(2020{\natexlab{a}})Hu, Xiao, Adlam, and Pennington]{HXAP20}
Hu, W., Xiao, L., Adlam, B., and Pennington, J.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Hu et~al.(2020{\natexlab{b}})Hu, Xiao, and Pennington]{HXP20}
Hu, W., Xiao, L., and Pennington, J.
\newblock Provable benefit of orthogonal initialization in optimizing deep
  linear networks.
\newblock \emph{ICLR}, 2020{\natexlab{b}}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{JGH18}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{JT19}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{ICLR}, 2019.

\bibitem[Ji \& Telgarsky(2020)Ji and Telgarsky]{JT20}
Ji, Z. and Telgarsky, M.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock \emph{ICLR}, 2020.

\bibitem[Kawaguchi(2016)]{K16}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock \emph{NeurIPS}, 2016.

\bibitem[Kidambi et~al.(2018)Kidambi, Netrapalli, Jain, and Kakade]{NKJK18}
Kidambi, R., Netrapalli, P., Jain, P., and Kakade, S.~M.
\newblock On the insufficiency of existing momentum schemes for stochastic
  optimization.
\newblock \emph{ICLR}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KB15}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Krichene et~al.(2020)Krichene, Caluyay, and Halder]{KCH20}
Krichene, W., Caluyay, K.~F., and Halder, A.
\newblock Global convergence of second-order dynamics in two-layer neural
  networks.
\newblock \emph{arXiv:2006.07867}, 2020.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{KSH12}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{NeurIPS}, 2012.

\bibitem[Laurent \& von Brecht(2018)Laurent and von Brecht]{LvB18}
Laurent, T. and von Brecht, J.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock \emph{ICML}, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein, and
  Pennington]{LXSBSP19}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Sohl-Dickstein, J., and
  Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Lee et~al.(2020)Lee, Shen, Song, Wang, and Yu]{LSSWY20}
Lee, J.~D., Shen, R., Song, Z., Wang, M., and Yu, Z.
\newblock Generalized leverage score sampling for neural networks.
\newblock \emph{arXiv:2009.09829}, 2020.

\bibitem[Lessard et~al.(2016)Lessard, Recht, and Packard]{LRP16}
Lessard, L., Recht, B., and Packard, A.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 2016.

\bibitem[Li \& Liang(2018)Li and Liang]{LL18}
Li, Y. and Liang, Y.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Li \& Yuan(2017)Li and Yuan]{LY17}
Li, Y. and Yuan, Y.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{LMZ18}
Li, Y., Ma, T., and Zhang, H.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock \emph{COLT}, 2018.

\bibitem[Li et~al.(2020)Li, Ma, and Zhang]{LMZ20}
Li, Y., Ma, T., and Zhang, H.
\newblock Learning over-parametrized two-layer relu neural networks beyond ntk.
\newblock \emph{COLT}, 2020.

\bibitem[Liu \& Belkin(2018)Liu and Belkin]{LB18}
Liu, C. and Belkin, M.
\newblock Parametrized accelerated methods free of condition number.
\newblock \emph{arXiv:1802.10235}, 2018.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Zhu, and Belkin]{LZB20a}
Liu, C., Zhu, L., and Belkin, M.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock \emph{arXiv:2010.01092}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhu, and Belkin]{LZB20b}
Liu, C., Zhu, L., and Belkin, M.
\newblock Toward a theory of optimization for over-parameterized systems of
  non-linear equations: the lessons of deep learning.
\newblock \emph{arXiv:2003.00307}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2020{\natexlab{c}})Liu, Gao, and Yin]{LGY20}
Liu, Y., Gao, Y., and Yin, W.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock \emph{NeurIPS}, 2020{\natexlab{c}}.

\bibitem[Loizou \& Richt{\'a}rik(2017)Loizou and Richt{\'a}rik]{LR17}
Loizou, N. and Richt{\'a}rik, P.
\newblock Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods.
\newblock \emph{arXiv:1712.09677}, 2017.

\bibitem[Loizou \& Richt{\'a}rik(2018)Loizou and Richt{\'a}rik]{LR18}
Loizou, N. and Richt{\'a}rik, P.
\newblock Accelerated gossip via stochastic heavy ball method.
\newblock \emph{Allerton}, 2018.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{KH1918}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{ICLR}, 2019.

\bibitem[Lu \& Kawaguchi(2017)Lu and Kawaguchi]{LK17}
Lu, H. and Kawaguchi, K.
\newblock Depth creates no bad local minima.
\newblock \emph{arXiv:1702.08580}, 2017.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{LXLS19}
Luo, L., Xiong, Y., Liu, Y., and Sun, X.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{ICLR}, 2019.

\bibitem[Lyu \& Li(2020)Lyu and Li]{LL20}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{ICLR}, 2020.

\bibitem[Mai \& Johansson(2020)Mai and Johansson]{MJ20}
Mai, V.~V. and Johansson, M.
\newblock Convergence of a stochastic gradient method with momentum for
  non-smooth non-convex optimization.
\newblock \emph{ICML}, 2020.

\bibitem[Mitliagkas(2019)]{M19}
Mitliagkas, I.
\newblock Accelerated methods - polyak’s momentum (heavy ball method).
\newblock \emph{Online Lecture Note}, 2019.

\bibitem[Moroshko et~al.(2020)Moroshko, Gunasekar, Woodworth, Lee, Srebro, and
  Soudry]{MGWLSS20}
Moroshko, E., Gunasekar, S., Woodworth, B., Lee, J.~D., Srebro, N., and Soudry,
  D.
\newblock Implicit bias in deep linear classification: Initialization scale vs
  training accuracy.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Nesterov(2013)]{N13}
Nesterov, Y.
\newblock Introductory lectures on convex optimization: a basic course.
\newblock \emph{Springer}, 2013.

\bibitem[Oymak \& Soltanolkotabi(2019)Oymak and Soltanolkotabi]{OS19}
Oymak, S. and Soltanolkotabi, M.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock \emph{arXiv:1902.04674}, 2019.

\bibitem[Panigrahi et~al.(2020)Panigrahi, Shetty, and Goyal]{PSG2020}
Panigrahi, A., Shetty, A., and Goyal, N.
\newblock Effect of activation functions on the training of overparametrized
  neural nets.
\newblock \emph{ICLR}, 2020.

\bibitem[Pilanci \& Ergen(2020)Pilanci and Ergen]{PE20}
Pilanci, M. and Ergen, T.
\newblock Neural networks are convex regularizers: Exact polynomial-time convex
  optimization formulations for two-layer networks.
\newblock \emph{ICML}, 2020.

\bibitem[Polyak(1963)]{P63}
Polyak, B.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki},
  1963.

\bibitem[Polyak(1964)]{P64}
Polyak, B.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 1964.

\bibitem[Razin \& Cohen(2020)Razin and Cohen]{RC20}
Razin, N. and Cohen, N.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock \emph{NeurIPS2020}, 2020.

\bibitem[Recht(2018)]{R18}
Recht, B.
\newblock Lyapunov analysis and the heavy ball method.
\newblock \emph{Lecture note}, 2018.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{RKK18}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock \emph{ICLR}, 2018.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{SMG14}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{ICLR}, 2014.

\bibitem[Scieur \& Pedregosa(2020)Scieur and Pedregosa]{SP20}
Scieur, D. and Pedregosa, F.
\newblock Universal average-case optimality of polyak momentum.
\newblock \emph{ICML}, 2020.

\bibitem[Shamir(2019)]{BHL18}
Shamir, O.
\newblock Exponential convergence time of gradient descent for one-dimensional
  deep linear neural networks.
\newblock \emph{COLT}, 2019.

\bibitem[Shi et~al.(2018)Shi, Du, Jordan, and Su]{SDJS18}
Shi, B., Du, S.~S., Jordan, M.~I., and Su, W.~J.
\newblock Understanding the acceleration phenomenon via high-resolution
  differential equations.
\newblock \emph{arXiv:1810.08907}, 2018.

\bibitem[Soltanolkotabi(2017)]{S17}
Soltanolkotabi, M.
\newblock Learning relus via gradient descent.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Song \& Yang(2019)Song and Yang]{ZY19}
Song, Z. and Yang, X.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock \emph{arXiv:1906.03593}, 2019.

\bibitem[Su \& Yang(2019)Su and Yang]{SY19}
Su, L. and Yang, P.
\newblock On learning over-parameterized neural networks: A functional
  approximation perspective.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Sun et~al.(2019)Sun, Yin, Li, Huang, Guan, and Jiang]{SYLHGJ19}
Sun, T., Yin, P., Li, D., Huang, C., Guan, L., and Jiang, H.
\newblock Non-ergodic convergence analysis of heavy-ball algorithms.
\newblock \emph{AAAI}, 2019.

\bibitem[Tian(2017)]{T17}
Tian, Y.
\newblock An analytical formula of population gradient for two-layered relu
  network and its applications in convergence and critical point analysis.
\newblock \emph{ICML}, 2017.

\bibitem[van~den Brand et~al.(2020)van~den Brand, Peng, Song, and
  Weinstein]{BPSW20}
van~den Brand, J., Peng, B., Song, Z., and Weinstein, O.
\newblock Training (overparametrized) neural networks in near-linear time.
\newblock \emph{arXiv:2006.11648}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, and et~al.]{attention17}
Vaswani, A., Shazeer, N., Parmar, N., and et~al.
\newblock Attention is all you need.
\newblock \emph{{NeurIPS}}, 2017.

\bibitem[Wang et~al.(2020)Wang, Lin, and Abernethy]{WCA20}
Wang, J.-K., Lin, C.-H., and Abernethy, J.
\newblock Escaping saddle points faster with stochastic momentum.
\newblock \emph{ICLR}, 2020.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{WLLM19}
Wei, C., Lee, J.~D., Liu, Q., and Ma, T.
\newblock Regularization matters: Generalization and optimization of neural
  nets v.s. their induced kernel.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, , and
  Recht.]{WRSSR17}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., , and Recht., B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Wilson et~al.(2021)Wilson, Jordan, and Recht]{WJR21}
Wilson, A.~C., Jordan, M., and Recht, B.
\newblock A lyapunov analysis of momentum methods in optimization.
\newblock \emph{JMLR}, 2021.

\bibitem[Wu et~al.(2019{\natexlab{a}})Wu, Wang, and Ma]{WWM19}
Wu, L., Wang, Q., and Ma, C.
\newblock Global convergence of gradient descent for deep linear residual
  networks.
\newblock \emph{NeurIPS}, 2019{\natexlab{a}}.

\bibitem[Wu et~al.(2019{\natexlab{b}})Wu, Dimakis, and Sanghavi]{WDS19}
Wu, S., Dimakis, A.~G., and Sanghavi, S.
\newblock Learning distributions generated by one-layer relu networks.
\newblock \emph{NeurIPS}, 2019{\natexlab{b}}.

\bibitem[Wu et~al.(2019{\natexlab{c}})Wu, Du, and Ward]{WDW19}
Wu, X., Du, S.~S., and Ward, R.
\newblock Global convergence of adaptive gradient methods for an
  over-parameterized neural network.
\newblock \emph{arXiv:1902.07111}, 2019{\natexlab{c}}.

\bibitem[Yang(2019)]{Y19}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv:1902.04760}, 2019.

\bibitem[Yang et~al.(2018)Yang, Lin, and Li]{YLL18}
Yang, T., Lin, Q., and Li, Z.
\newblock Unified convergence analysis of stochastic momentum methods for
  convex and non-convex optimization.
\newblock \emph{IJCAI}, 2018.

\bibitem[Yehudai \& Shamir(2020)Yehudai and Shamir]{YS20}
Yehudai, G. and Shamir, O.
\newblock Learning a single neuron with gradient methods.
\newblock \emph{COLT}, 2020.

\bibitem[Yun et~al.(2018)Yun, Sra, and Jadbabaie]{YSJ17}
Yun, C., Sra, S., and Jadbabaie, A.
\newblock Global optimality conditions for deep neural networks.
\newblock \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Martens, and Grosse]{ZMG19}
Zhang, G., Martens, J., and Grosse, R.~B.
\newblock Fast convergence of natural gradient descent for over-parameterized
  neural networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and Dhillon]{ZSJBD17}
Zhong, K., Song, Z., Jain, P., Bartlett, P.~L., and Dhillon, I.~S.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock \emph{ICML}, 2017.

\bibitem[Zhou \& Liang(2018)Zhou and Liang]{ZL18}
Zhou, Y. and Liang, Y.
\newblock Critical points of linear neural networks: Analytical forms and
  landscape.
\newblock \emph{ICLR}, 2018.

\bibitem[Zou \& Gu(2019)Zou and Gu]{ZG19}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Zou et~al.(2019)Zou, Cao, Zhou, and Gu]{ZCZG19}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Stochastic gradient descent optimizes overparameterized deep relu
  networks.
\newblock \emph{Machine Learning, Springer}, 2019.

\bibitem[Zou et~al.(2020)Zou, Long, and Gu]{ZLG20}
Zou, D., Long, P.~M., and Gu, Q.
\newblock On the global convergence of training deep linear resnets.
\newblock \emph{ICLR}, 2020.

\end{thebibliography}
