\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreas et~al.(2016)Andreas, Rohrbach, Darrell, and
  Klein]{andreas2016neural}
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D.
\newblock Neural module networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  39--48, 2016.

\bibitem[Arora et~al.(2014{\natexlab{a}})Arora, Bhaskara, Ge, and
  Ma]{arora2014provable}
Arora, S., Bhaskara, A., Ge, R., and Ma, T.
\newblock Provable bounds for learning some deep representations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  584--592, 2014{\natexlab{a}}.

\bibitem[Arora et~al.(2014{\natexlab{b}})Arora, Bhaskara, Ge, and
  Ma]{pmlr-v32-arora14}
Arora, S., Bhaskara, A., Ge, R., and Ma, T.
\newblock Provable bounds for learning some deep representations.
\newblock In Xing, E.~P. and Jebara, T. (eds.), \emph{Proceedings of the 31st
  International Conference on Machine Learning}, volume~32 of \emph{Proceedings
  of Machine Learning Research}, pp.\  584--592, Bejing, China, 22--24 Jun
  2014{\natexlab{b}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v32/arora14.html}.

\bibitem[Arora et~al.(2014{\natexlab{c}})Arora, Ge, and Moitra]{arora2014new}
Arora, S., Ge, R., and Moitra, A.
\newblock New algorithms for learning incoherent and overcomplete dictionaries.
\newblock In \emph{Conference on Learning Theory}, pp.\  779--806,
  2014{\natexlab{c}}.

\bibitem[Arora et~al.(2015)Arora, Ge, Ma, and Moitra]{arora2015simple}
Arora, S., Ge, R., Ma, T., and Moitra, A.
\newblock Simple, efficient, and neural algorithms for sparse coding.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory}, pp.\
   113--149, 2015.

\bibitem[Azam(2000)]{azam2000biologically}
Azam, F.
\newblock \emph{Biologically inspired modular neural networks}.
\newblock PhD thesis, Virginia Tech, 2000.

\bibitem[Brannon \& Roitman(2003)Brannon and Roitman]{brannon2003nonverbal}
Brannon, E.~M. and Roitman, J.~D.
\newblock Nonverbal representations of time and number in animals and human
  infants.
\newblock 2003.

\bibitem[Buciluǎ et~al.(2006)Buciluǎ, Caruana, and
  Niculescu-Mizil]{bucilua2006model}
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  535--541. ACM, 2006.

\bibitem[Clune et~al.(2013)Clune, Mouret, and Lipson]{clune2013evolutionary}
Clune, J., Mouret, J.-B., and Lipson, H.
\newblock The evolutionary origins of modularity.
\newblock \emph{Proc. R. Soc. B}, 280\penalty0 (1755):\penalty0 20122863, 2013.

\bibitem[Danihelka et~al.(2016)Danihelka, Wayne, Uria, Kalchbrenner, and
  Graves]{danihelka2016associative}
Danihelka, I., Wayne, G., Uria, B., Kalchbrenner, N., and Graves, A.
\newblock Associative long short-term memory.
\newblock In \emph{Proceedings of the 33rd International Conference on
  International Conference on Machine Learning}, pp.\  1986--1994. JMLR.org,
  2016.

\bibitem[Du et~al.(2017{\natexlab{a}})Du, Lee, and Tian]{du2017convolutional}
Du, S.~S., Lee, J.~D., and Tian, Y.
\newblock When is a convolutional filter easy to learn?
\newblock \emph{arXiv preprint arXiv:1709.06129}, 2017{\natexlab{a}}.

\bibitem[Du et~al.(2017{\natexlab{b}})Du, Lee, Tian, Poczos, and
  Singh]{du2017gradient}
Du, S.~S., Lee, J.~D., Tian, Y., Poczos, B., and Singh, A.
\newblock Gradient descent learns one-hidden-layer cnn: Don't be afraid of
  spurious local minima.
\newblock \emph{arXiv preprint arXiv:1712.00779}, 2017{\natexlab{b}}.

\bibitem[Ellis et~al.(2018)Ellis, Ritchie, Solar-Lezama, and
  Tenenbaum]{ellis2018learning}
Ellis, K., Ritchie, D., Solar-Lezama, A., and Tenenbaum, J.
\newblock Learning to infer graphics programs from hand-drawn images.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6059--6068, 2018.

\bibitem[Fernando et~al.(2017)Fernando, Banarse, Blundell, Zwols, Ha, Rusu,
  Pritzel, and Wierstra]{fernando2017pathnet}
Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A.~A.,
  Pritzel, A., and Wierstra, D.
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks.
\newblock \emph{arXiv preprint arXiv:1701.08734}, 2017.

\bibitem[Gayler(2004)]{gayler2004vector}
Gayler, R.~W.
\newblock Vector symbolic architectures answer jackendoff's challenges for
  cognitive neuroscience.
\newblock \emph{arXiv preprint cs/0412059}, 2004.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Graves, A., Wayne, G., and Danihelka, I.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska-Barwi{\'n}ska, Colmenarejo, Grefenstette, Ramalho, Agapiou,
  et~al.]{graves2016hybrid}
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I.,
  Grabska-Barwi{\'n}ska, A., Colmenarejo, S.~G., Grefenstette, E., Ramalho, T.,
  Agapiou, J., et~al.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 538\penalty0 (7626):\penalty0 471, 2016.

\bibitem[Grefenstette et~al.(2015)Grefenstette, Hermann, Suleyman, and
  Blunsom]{grefenstette2015learning}
Grefenstette, E., Hermann, K.~M., Suleyman, M., and Blunsom, P.
\newblock Learning to transduce with unbounded memory.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1828--1836, 2015.

\bibitem[Hanson \& Wright(1971)Hanson and Wright]{hanson1971bound}
Hanson, D.~L. and Wright, F.~T.
\newblock A bound on tail probabilities for quadratic forms in independent
  random variables.
\newblock \emph{The Annals of Mathematical Statistics}, 42\penalty0
  (3):\penalty0 1079--1083, 1971.

\bibitem[Hawkins \& Blakeslee(2007)Hawkins and
  Blakeslee]{hawkins2007intelligence}
Hawkins, J. and Blakeslee, S.
\newblock \emph{On intelligence: How a new understanding of the brain will lead
  to the creation of truly intelligent machines}.
\newblock Macmillan, 2007.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hinton et~al.(2000)Hinton, Ghahramani, and Teh]{hinton2000learning}
Hinton, G.~E., Ghahramani, Z., and Teh, Y.~W.
\newblock Learning to parse images.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  463--469, 2000.

\bibitem[Hinton et~al.(2011)Hinton, Krizhevsky, and
  Wang]{hinton2011transforming}
Hinton, G.~E., Krizhevsky, A., and Wang, S.~D.
\newblock Transforming auto-encoders.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  44--51. Springer, 2011.

\bibitem[Hoeffding(1963)]{hoeffding1963probability}
Hoeffding, W.
\newblock Probability inequalities for sums of bounded random variables.
\newblock \emph{J. American Statist. Assoc.}, 58:\penalty0 13 -- 30, 03 1963.
\newblock \doi{10.2307/2282952}.

\bibitem[Hu et~al.(2017)Hu, Andreas, Rohrbach, Darrell, and
  Saenko]{hu2017learning}
Hu, R., Andreas, J., Rohrbach, M., Darrell, T., and Saenko, K.
\newblock Learning to reason: End-to-end module networks for visual question
  answering.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  804--813, 2017.

\bibitem[Joulin \& Mikolov(2015)Joulin and Mikolov]{joulin2015inferring}
Joulin, A. and Mikolov, T.
\newblock Inferring algorithmic patterns with stack-augmented recurrent nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  190--198, 2015.

\bibitem[Kane \& Nelson(2014)Kane and Nelson]{Kane:2014:SJT:2578041.2559902}
Kane, D.~M. and Nelson, J.
\newblock Sparser johnson-lindenstrauss transforms.
\newblock \emph{J. ACM}, 61\penalty0 (1):\penalty0 4:1--4:23, January 2014.
\newblock ISSN 0004-5411.
\newblock \doi{10.1145/2559902}.
\newblock URL \url{http://doi.acm.org/10.1145/2559902}.

\bibitem[Levy \& Gayler(2008)Levy and Gayler]{levy2008vector}
Levy, S.~D. and Gayler, R.
\newblock Vector symbolic architectures: A new building material for artificial
  general intelligence.
\newblock In \emph{Proceedings of the 2008 Conference on Artificial General
  Intelligence 2008: Proceedings of the First AGI Conference}, pp.\  414--418.
  IOS Press, 2008.

\bibitem[Li \& Yuan(2017)Li and Yuan]{li2017convergence}
Li, Y. and Yuan, Y.
\newblock Convergence analysis of two-layer neural networks with {ReLU}
  activation.
\newblock \emph{arXiv preprint arXiv:1705.09886}, 2017.

\bibitem[Oh et~al.(2017)Oh, Singh, Lee, and Kohli]{oh2017zero}
Oh, J., Singh, S., Lee, H., and Kohli, P.
\newblock Zero-shot task generalization with multi-task deep reinforcement
  learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2661--2670. JMLR. org, 2017.

\bibitem[Papyan et~al.(2018)Papyan, Romano, Sulam, and
  Elad]{papyan2018theoretical}
Papyan, V., Romano, Y., Sulam, J., and Elad, M.
\newblock Theoretical foundations of deep learning via sparse representations:
  A multilayer sparse model and its connection to convolutional neural
  networks.
\newblock \emph{IEEE Signal Processing Magazine}, 35\penalty0 (4):\penalty0
  72--89, 2018.

\bibitem[Real et~al.(2018)Real, Aggarwal, Huang, and Le]{real2018regularized}
Real, E., Aggarwal, A., Huang, Y., and Le, Q.~V.
\newblock Regularized evolution for image classifier architecture search.
\newblock \emph{arXiv preprint arXiv:1802.01548}, 2018.

\bibitem[Sabour et~al.(2017)Sabour, Frosst, and Hinton]{sabour2017dynamic}
Sabour, S., Frosst, N., and Hinton, G.~E.
\newblock Dynamic routing between capsules.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3856--3866, 2017.

\bibitem[Smolensky(1990)]{smolensky1990tensor}
Smolensky, P.
\newblock Tensor product variable binding and the representation of symbolic
  structures in connectionist systems.
\newblock \emph{Artificial intelligence}, 46\penalty0 (1-2):\penalty0 159--216,
  1990.

\bibitem[Spielman et~al.(2012)Spielman, Wang, and Wright]{spielman2012exact}
Spielman, D.~A., Wang, H., and Wright, J.
\newblock Exact recovery of sparsely-used dictionaries.
\newblock In \emph{Conference on Learning Theory}, pp.\  37--1, 2012.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, Weston, Fergus,
  et~al.]{sukhbaatar2015end}
Sukhbaatar, S., Weston, J., Fergus, R., et~al.
\newblock End-to-end memory networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2440--2448, 2015.

\bibitem[Wu et~al.(2017)Wu, Tenenbaum, and Kohli]{nsd}
Wu, J., Tenenbaum, J.~B., and Kohli, P.
\newblock Neural scene de-rendering.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem[Yi et~al.(2018)Yi, Wu, Gan, Torralba, Kohli, and Tenenbaum]{nsvqa}
Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J.~B.
\newblock {Neural-Symbolic VQA: Disentangling Reasoning from Vision and
  Language Understanding}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Zaremba \& Sutskever(2015)Zaremba and
  Sutskever]{zaremba2015reinforcement}
Zaremba, W. and Sutskever, I.
\newblock Reinforcement learning neural turing machines-revised.
\newblock \emph{arXiv preprint arXiv:1505.00521}, 2015.

\bibitem[Zhang et~al.(2017)Zhang, Panigrahy, Sachdeva, and
  Rahimi]{zhang2017electron}
Zhang, Q., Panigrahy, R., Sachdeva, S., and Rahimi, A.
\newblock Electron-proton dynamics in deep learning.
\newblock \emph{arXiv preprint arXiv:1702.00458}, 2017.

\bibitem[Zhong et~al.(2017{\natexlab{a}})Zhong, Song, and
  Dhillon]{zhong2017learning}
Zhong, K., Song, Z., and Dhillon, I.~S.
\newblock Learning non-overlapping convolutional neural networks with multiple
  kernels.
\newblock \emph{arXiv preprint arXiv:1711.03440}, 2017{\natexlab{a}}.

\bibitem[Zhong et~al.(2017{\natexlab{b}})Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Zhong, K., Song, Z., Jain, P., Bartlett, P.~L., and Dhillon, I.~S.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1706.03175}, 2017{\natexlab{b}}.

\end{thebibliography}
