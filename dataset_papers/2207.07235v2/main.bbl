\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Wilson and Izmailov(2020)]{wilson2020bayesian}
Andrew~G Wilson and Pavel Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 4697--4708, 2020.

\bibitem[He et~al.(2020)He, Lakshminarayanan, and Teh]{he2020bayesian}
Bobby He, Balaji Lakshminarayanan, and Yee~Whye Teh.
\newblock Bayesian deep ensembles via the neural tangent kernel.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1010--1022, 2020.

\bibitem[Neal(2012)]{neal2012bayesian}
Radford~M Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{International Conference on Machine Learning}, pages
  1613--1622. PMLR, 2015.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pages
  1050--1059. PMLR, 2016.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{Ovadia2019}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
  Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Van~Amersfoort et~al.(2020)Van~Amersfoort, Smith, Teh, and
  Gal]{van2020uncertainty}
Joost Van~Amersfoort, Lewis Smith, Yee~Whye Teh, and Yarin Gal.
\newblock Uncertainty estimation using a single deep deterministic neural
  network.
\newblock In \emph{International Conference on Machine Learning}, pages
  9690--9700. PMLR, 2020.

\bibitem[Jain et~al.(2021)Jain, Lahlou, Nekoei, Butoi, Bertin, Rector-Brooks,
  Korablyov, and Bengio]{jain2021deup}
Moksh Jain, Salem Lahlou, Hadi Nekoei, Victor Butoi, Paul Bertin, Jarrid
  Rector-Brooks, Maksym Korablyov, and Yoshua Bengio.
\newblock Deup: Direct epistemic uncertainty prediction.
\newblock \emph{arXiv preprint arXiv:2102.08501}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin
  Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7537--7547, 2020.

\bibitem[Graves(2011)]{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  2348--2356. Citeseer, 2011.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688. Citeseer, 2011.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:1912.02757}, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[Bietti and Mairal(2019)]{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lee et~al.(2018)Lee, Sohl-dickstein, Pennington, Novak, Schoenholz,
  and Bahri]{lee2018deep}
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam
  Schoenholz, and Yasaman Bahri.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1EA-M-0Z}.

\bibitem[de~G.~Matthews et~al.(2018)de~G.~Matthews, Hron, Rowland, Turner, and
  Ghahramani]{gMatthews2018gaussian}
Alexander~G. de~G.~Matthews, Jiri Hron, Mark Rowland, Richard~E. Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=H1-nGgWC-}.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Abolafia, Pennington,
  and Sohl-dickstein]{novak2019bayesian}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel~A.
  Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g30j0qF7}.

\bibitem[Woodbury(1950)]{woodbury1950inverting}
Max~A Woodbury.
\newblock \emph{Inverting modified matrices}.
\newblock Statistical Research Group, 1950.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1321--1330. PMLR, 2017.

\bibitem[Krishnan and Tickoo(2020)]{krishnan2020improving}
Ranganath Krishnan and Omesh Tickoo.
\newblock Improving model calibration with accuracy versus uncertainty
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 18237--18248, 2020.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2018benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJz6tiCqYm}.

\bibitem[Liu et~al.(2020)Liu, Wang, Owens, and Li]{energyood}
Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.
\newblock Energy-based out-of-distribution detection.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 21464--21475, 2020.

\bibitem[Kailkhura et~al.(2018)Kailkhura, Thiagarajan, Rastogi, Varshney, and
  Bremer]{kailkhura2018spectral}
Bhavya Kailkhura, Jayaraman~J Thiagarajan, Charvi Rastogi, Pramod~K Varshney,
  and Peer-Timo Bremer.
\newblock A spectral approach for the design of experiments: Design, analysis
  and algorithms.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1214--1259, 2018.

\bibitem[Shahriari et~al.(2015)Shahriari, Swersky, Wang, Adams, and
  De~Freitas]{shahriari2015taking}
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan~P Adams, and Nando De~Freitas.
\newblock Taking the human out of the loop: A review of bayesian optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2015.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Sun et~al.(2017)Sun, Chen, and Carin]{sun2017learning}
Shengyang Sun, Changyou Chen, and Lawrence Carin.
\newblock Learning structured weight uncertainty in bayesian neural networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1283--1292.
  PMLR, 2017.

\bibitem[Nado et~al.(2021)Nado, Band, Collier, Djolonga, Dusenberry, Farquhar,
  Filos, Havasi, Jenatton, Jerfel, Liu, Mariet, Nixon, Padhy, Ren, Rudner, Wen,
  Wenzel, Murphy, Sculley, Lakshminarayanan, Snoek, Gal, and
  Tran]{nado2021uncertainty}
Zachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael Dusenberry,
  Sebastian Farquhar, Angelos Filos, Marton Havasi, Rodolphe Jenatton, Ghassen
  Jerfel, Jeremiah Liu, Zelda Mariet, Jeremy Nixon, Shreyas Padhy, Jie Ren, Tim
  Rudner, Yeming Wen, Florian Wenzel, Kevin Murphy, D.~Sculley, Balaji
  Lakshminarayanan, Jasper Snoek, Yarin Gal, and Dustin Tran.
\newblock {Uncertainty Baselines}: Benchmarks for uncertainty \& robustness in
  deep learning.
\newblock \emph{arXiv preprint arXiv:2106.04015}, 2021.

\bibitem[Bottou(2012)]{bottou2012stochastic}
L{\'e}on Bottou.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 421--436.
  Springer, 2012.

\bibitem[Liu and Nocedal(1989)]{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1):\penalty0 503--528,
  1989.

\bibitem[Audet and Hare(2017)]{audet2017derivative}
Charles Audet and Warren Hare.
\newblock \emph{Derivative-free and blackbox optimization}, volume~2.
\newblock Springer, 2017.

\bibitem[Schneider et~al.(2020)Schneider, Walters, Plowright, Sieroka,
  Listgarten, Goodnow, Fisher, Jansen, Duca, Rush,
  et~al.]{schneider2020rethinking}
Petra Schneider, W~Patrick Walters, Alleyn~T Plowright, Norman Sieroka,
  Jennifer Listgarten, Robert~A Goodnow, Jasmin Fisher, Johanna~M Jansen,
  Jos{\'e}~S Duca, Thomas~S Rush, et~al.
\newblock Rethinking drug design in the artificial intelligence era.
\newblock \emph{Nature Reviews Drug Discovery}, 19\penalty0 (5):\penalty0
  353--364, 2020.

\bibitem[Wang et~al.(2020)Wang, Tan, Tor, and Lim]{wang2020machine}
Chengcheng Wang, XP~Tan, SB~Tor, and CS~Lim.
\newblock Machine learning in additive manufacturing: State-of-the-art and
  perspectives.
\newblock \emph{Additive Manufacturing}, 36:\penalty0 101538, 2020.

\bibitem[Gonzalvez et~al.(2019)Gonzalvez, Lezmi, Roncalli, and
  Xu]{gonzalvez2019financial}
Joan Gonzalvez, Edmond Lezmi, Thierry Roncalli, and Jiali Xu.
\newblock Financial applications of gaussian processes and bayesian
  optimization.
\newblock \emph{arXiv preprint arXiv:1903.04841}, 2019.

\bibitem[Ren et~al.(2021)Ren, Xiao, Chang, Huang, Li, Chen, and
  Wang]{ren2021comprehensive}
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen,
  and Xin Wang.
\newblock A comprehensive survey of neural architecture search: Challenges and
  solutions.
\newblock \emph{ACM Computing Surveys (CSUR)}, 54\penalty0 (4):\penalty0 1--34,
  2021.

\end{thebibliography}
