\begin{thebibliography}{10}

\bibitem{agarwal2024approximate}
Shubham Agarwal, Subrata Mitra, Sarthak Chakraborty, Srikrishna Karanam, Koyel Mukherjee, and Shiv~Kumar Saini.
\newblock Approximate caching for efficiently serving $\{$Text-to-Image$\}$ diffusion models.
\newblock In {\em 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)}, pages 1173--1189, 2024.

\bibitem{bao2022analytic}
Fan Bao, Chongxuan Li, Jun Zhu, and Bo~Zhang.
\newblock Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2201.06503}, 2022.

\bibitem{bao2023uvit}
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.
\newblock All are worth words: A vit backbone for diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22669--22679, 2023.

\bibitem{videoworldsimulators2024sora}
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li~Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.
\newblock Video generation models as world simulators.
\newblock 2024.

\bibitem{cao2023large}
Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, and Ziwei Liu.
\newblock Large-vocabulary 3d diffusion model with transformer.
\newblock {\em arXiv preprint arXiv:2309.07920}, 2023.

\bibitem{castells2024ld}
Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi.
\newblock Ld-pruner: Efficient pruning of latent diffusion models using task-agnostic insights.
\newblock {\em arXiv preprint arXiv:2404.11936}, 2024.

\bibitem{chen2023gentron}
Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua.
\newblock Gentron: Delving deep into diffusion transformers for image and video generation.
\newblock {\em arXiv preprint arXiv:2312.04557}, 2023.

\bibitem{chen20230}
Zigeng Chen, Gongfan Fang, Xinyin Ma, and Xinchao Wang.
\newblock 0.1\% data makes segment anything slim.
\newblock {\em arXiv preprint arXiv:2312.05284}, 2023.

\bibitem{chen2024asyncdiff}
Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, and Xinchao Wang.
\newblock Asyncdiff: Parallelizing diffusion models by asynchronous denoising.
\newblock {\em arXiv preprint arXiv:2406.06911}, 2024.

\bibitem{dhariwal2021adm}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in neural information processing systems}, 34:8780--8794, 2021.

\bibitem{dockhorn2021score}
Tim Dockhorn, Arash Vahdat, and Karsten Kreis.
\newblock Score-based generative modeling with critically-damped langevin diffusion.
\newblock {\em arXiv preprint arXiv:2112.07068}, 2021.

\bibitem{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{esser2024scaling}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock In {\em Forty-first International Conference on Machine Learning}, 2024.

\bibitem{Fan2020Reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{fang2024isomorphic}
Gongfan Fang, Xinyin Ma, Michael~Bi Mi, and Xinchao Wang.
\newblock Isomorphic pruning for vision models.
\newblock {\em arXiv preprint arXiv:2407.04616}, 2024.

\bibitem{fang2024structural}
Gongfan Fang, Xinyin Ma, and Xinchao Wang.
\newblock Structural pruning for diffusion models.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{gao2023masked}
Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan.
\newblock Masked diffusion transformer is a strong image synthesizer.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 23164--23173, 2023.

\bibitem{PTQD}
Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.
\newblock Ptqd: Accurate post-training quantization for diffusion models.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 13237--13249. Curran Associates, Inc., 2023.

\bibitem{ho2020ddpm}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems}, 33:6840--6851, 2020.

\bibitem{hunter2023fast}
Rosco Hunter, {\L}ukasz Dudziak, Mohamed~S Abdelfattah, Abhinav Mehrotra, Sourav Bhattacharya, and Hongkai Wen.
\newblock Fast inference through the reuse of attention maps in diffusion models.
\newblock {\em arXiv preprint arXiv:2401.01008}, 2023.

\bibitem{jing2023u}
Xin Jing, Yi~Chang, Zijiang Yang, Jiangjian Xie, Andreas Triantafyllopoulos, and Bjoern~W Schuller.
\newblock U-dit tts: U-diffusion vision transformer for text-to-speech.
\newblock In {\em Speech Communication; 15th ITG Conference}, pages 56--60. VDE, 2023.

\bibitem{jolicoeur2021gotta}
Alexia Jolicoeur-Martineau, Ke~Li, R{\'e}mi Pich{\'e}-Taillefer, Tal Kachman, and Ioannis Mitliagkas.
\newblock Gotta go fast when generating data with score-based models.
\newblock {\em arXiv preprint arXiv:2105.14080}, 2021.

\bibitem{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:26565--26577, 2022.

\bibitem{kim2024shortened}
Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song.
\newblock Shortened llama: A simple depth pruning for large language models.
\newblock {\em arXiv preprint arXiv:2402.02834}, 2024.

\bibitem{kim2023bk}
Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi.
\newblock Bk-sdm: Architecturally compressed stable diffusion for efficient text-to-image generation.
\newblock In {\em Workshop on Efficient Systems for Foundation Models@ ICML2023}, 2023.

\bibitem{kynkaanniemi2019precisionrecall}
Tuomas Kynk{\"a}{\"a}nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
\newblock Improved precision and recall metric for assessing generative models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{li2024distrifusion}
Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han.
\newblock Distrifusion: Distributed parallel inference for high-resolution diffusion models.
\newblock {\em arXiv preprint arXiv:2402.19481}, 2024.

\bibitem{li2023faster}
Senmao Li, Taihang Hu, Fahad~Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang.
\newblock Faster diffusion: Rethinking the role of unet encoder in diffusion models.
\newblock {\em arXiv preprint arXiv:2312.09608}, 2023.

\bibitem{Li_2023_ICCV}
Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.
\newblock Q-diffusion: Quantizing diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 17535--17545, October 2023.

\bibitem{li2024snapfusion}
Yanyu Li, Huan Wang, Qing Jin, Ju~Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren.
\newblock Snapfusion: Text-to-image diffusion model on mobile devices within two seconds.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{lin2024sdxl}
Shanchuan Lin, Anran Wang, and Xiao Yang.
\newblock Sdxl-lightning: Progressive adversarial diffusion distillation.
\newblock {\em arXiv preprint arXiv:2402.13929}, 2024.

\bibitem{liu2023oms}
Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, and Yu~Wang.
\newblock Oms-dpm: Optimizing the model schedule for diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2306.08860}, 2023.

\bibitem{liu2023vit}
Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, and Zhou Zhao.
\newblock Vit-tts: visual text-to-speech with scalable diffusion transformer.
\newblock {\em arXiv preprint arXiv:2305.12708}, 2023.

\bibitem{liu2022pseudo}
Luping Liu, Yi~Ren, Zhijie Lin, and Zhou Zhao.
\newblock Pseudo numerical methods for diffusion models on manifolds.
\newblock {\em arXiv preprint arXiv:2202.09778}, 2022.

\bibitem{liu2024linfusion}
Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang.
\newblock Linfusion: 1 gpu, 1 minute, 16k image.
\newblock {\em arXiv preprint arXiv:2409.02097}, 2024.

\bibitem{lu2022dpmsolver}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock {\em Advances in Neural Information Processing Systems}, 35:5775--5787, 2022.

\bibitem{lu2022dpm++}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models.
\newblock {\em arXiv preprint arXiv:2211.01095}, 2022.

\bibitem{lu2023vdt}
Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding.
\newblock Vdt: General-purpose video diffusion transformers via mask modeling.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{luo2023latent}
Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.
\newblock Latent consistency models: Synthesizing high-resolution images with few-step inference.
\newblock {\em arXiv preprint arXiv:2310.04378}, 2023.

\bibitem{ma2023deepcache}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Deepcache: Accelerating diffusion models for free.
\newblock {\em arXiv preprint arXiv:2312.00858}, 2023.

\bibitem{mo2024dit}
Shentong Mo, Enze Xie, Ruihang Chu, Lanqing Hong, Matthias Niessner, and Zhenguo Li.
\newblock Dit-3d: Exploring plain diffusion transformers for 3d shape generation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{moon2023early}
Taehong Moon, Moonseok Choi, EungGu Yun, Jongmin Yoon, Gayoung Lee, and Juho Lee.
\newblock Early exiting for accelerated inference in diffusion models.
\newblock In {\em ICML 2023 Workshop on Structured Probabilistic Inference $\{$$\backslash$\&$\}$ Generative Modeling}, 2023.

\bibitem{nash2021sfid}
Charlie Nash, Jacob Menick, Sander Dieleman, and Peter~W Battaglia.
\newblock Generating images with sparse representations.
\newblock {\em arXiv preprint arXiv:2103.03841}, 2021.

\bibitem{pan2024t}
Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, and Anima Anandkumar.
\newblock T-stitch: Accelerating sampling in pre-trained diffusion models with trajectory stitching.
\newblock {\em arXiv preprint arXiv:2402.14167}, 2024.

\bibitem{pan2024tstitch}
Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding Yu, Chaowei Xiao, Jianfei Cai, and Anima Anandkumar.
\newblock T-stitch: Accelerating sampling in pre-trained diffusion models with trajectory stitching, 2024.

\bibitem{peebles2023dit}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{qin2023infobatch}
Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, et~al.
\newblock Infobatch: Lossless training speed up by unbiased dynamic data pruning.
\newblock {\em arXiv preprint arXiv:2303.04947}, 2023.

\bibitem{raposo2024mixture}
David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter~Conway Humphreys, and Adam Santoro.
\newblock Mixture-of-depths: Dynamically allocating compute in transformer-based language models.
\newblock {\em arXiv preprint arXiv:2404.02258}, 2024.

\bibitem{rombach2022ldm}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem{ronneberger2015unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18}, pages 234--241. Springer, 2015.

\bibitem{salimans2016IS}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2202.00512}, 2022.

\bibitem{shang2023post}
Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan.
\newblock Post-training quantization on diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1972--1981, 2023.

\bibitem{shih2023parallel}
Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, and Nima Anari.
\newblock Parallel sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2305.16317}, 2023.

\bibitem{smith1982cache}
Alan~Jay Smith.
\newblock Cache memories.
\newblock {\em ACM Computing Surveys (CSUR)}, 14(3):473--530, 1982.

\bibitem{so2023frdiff}
Junhyuk So, Jungwon Lee, and Eunhyeok Park.
\newblock Frdiff: Feature reuse for exquisite zero-shot acceleration of diffusion models.
\newblock {\em arXiv preprint arXiv:2312.03517}, 2023.

\bibitem{song2020ddim}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{song2023consistency}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock {\em arXiv preprint arXiv:2303.01469}, 2023.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{tan2024litefocus}
Zhenxiong Tan, Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Litefocus: Accelerated diffusion inference for long audio synthesis.
\newblock {\em arXiv preprint arXiv:2407.10468}, 2024.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2024sparsedm}
Kafeng Wang, Jianfei Chen, He~Li, Zhenpeng Mi, and Jun Zhu.
\newblock Sparsedm: Toward sparse efficient diffusion models, 2024.

\bibitem{wimbauer2023cache}
Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji~Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et~al.
\newblock Cache me if you can: Accelerating diffusion models through block caching.
\newblock {\em arXiv preprint arXiv:2312.03209}, 2023.

\bibitem{yang2023ddsm}
Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, and Yingcong Chen.
\newblock Denoising diffusion step-aware models.
\newblock {\em arXiv preprint arXiv:2310.03337}, 2023.

\bibitem{yang2024hash3d}
Xingyi Yang and Xinchao Wang.
\newblock Hash3d: Training-free acceleration for 3d generation.
\newblock {\em arXiv preprint arXiv:2404.06091}, 2024.

\bibitem{yang2023diffusion}
Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
\newblock Diffusion probabilistic model made slim.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22552--22562, 2023.

\bibitem{yang2022your}
Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting Zhao, and Shihao Ji.
\newblock Your vit is secretly a hybrid discriminative-generative diffusion model.
\newblock {\em arXiv preprint arXiv:2208.07791}, 2022.

\bibitem{yu2023distribution}
Runpeng Yu, Songhua Liu, Xingyi Yang, and Xinchao Wang.
\newblock Distribution shift inversion for out-of-distribution prediction.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3592--3602, 2023.

\bibitem{yu2024neural}
Runpeng Yu and Xinchao Wang.
\newblock Neural lineage.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4797--4807, 2024.

\bibitem{zhang2024laptop}
Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu.
\newblock Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models.
\newblock {\em arXiv preprint arXiv:2404.11098}, 2024.

\bibitem{zhang2022fast}
Qinsheng Zhang and Yongxin Chen.
\newblock Fast sampling of diffusion models with exponential integrator.
\newblock {\em arXiv preprint arXiv:2204.13902}, 2022.

\bibitem{zhang2022gddim}
Qinsheng Zhang, Molei Tao, and Yongxin Chen.
\newblock gddim: Generalized denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2206.05564}, 2022.

\bibitem{zhang2024crossattention}
Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike~Zheng Shou, and JÃ¼rgen Schmidhuber.
\newblock Cross-attention makes inference cumbersome in text-to-image diffusion models, 2024.

\bibitem{zhao2023mobilediffusion}
Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou.
\newblock Mobilediffusion: Subsecond text-to-image generation on mobile devices.
\newblock {\em arXiv preprint arXiv:2311.16567}, 2023.

\bibitem{zheng2023fast}
Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar.
\newblock Fast training of diffusion models with masked transformers.
\newblock {\em arXiv preprint arXiv:2306.09305}, 2023.

\end{thebibliography}
