\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori \& Szepesvári(2011)Abbasi-Yadkori and
  Szepesvári]{abbasi-yadkori}
Abbasi-Yadkori, Y. and Szepesvári, C.
\newblock Regret bounds for the adaptive control of linear quadratic systems.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, volume~19 of \emph{Proceedings of Machine Learning Research}, pp.\
  1--26, Budapest, Hungary, 09--11 Jun 2011. JMLR Workshop and Conference
  Proceedings.

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Kakade, Lee, and
  Mahajan]{Agarwal2020}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Proceedings of Thirty Third Conference on Learning Theory},
  pp.\  64--66. PMLR, 2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Brukhim, Hazan, and
  Lu]{hazan-etal20boosting-control-dynamical}
Agarwal, N., Brukhim, N., Hazan, E., and Lu, Z.
\newblock Boosting for control of dynamical systems.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  96--103. PMLR, 13--18
  Jul 2020{\natexlab{b}}.

\bibitem[{Auer} et~al.(1995){Auer}, {Cesa-Bianchi}, {Freund}, and
  {Schapire}]{Exp3}
{Auer}, P., {Cesa-Bianchi}, N., {Freund}, Y., and {Schapire}, R.~E.
\newblock Gambling in a rigged casino: The adversarial multi-armed bandit
  problem.
\newblock In \emph{Proceedings of IEEE 36th Annual Foundations of Computer
  Science}, pp.\  322--331, 1995.

\bibitem[Auer et~al.(2009)Auer, Jaksch, and Ortner]{UCRL}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~21, pp.\  89--96. Curran Associates, Inc., 2009.

\bibitem[Banijamali et~al.(2019)Banijamali, Abbasi-Yadkori, Ghavamzadeh, and
  Vlassis]{pmlr-v89-banijamali19a}
Banijamali, E., Abbasi-Yadkori, Y., Ghavamzadeh, M., and Vlassis, N.
\newblock Optimizing over a restricted policy class in mdps.
\newblock In Chaudhuri, K. and Sugiyama, M. (eds.), \emph{Proceedings of the
  Twenty-Second International Conference on Artificial Intelligence and
  Statistics}, volume~89 of \emph{Proceedings of Machine Learning Research},
  pp.\  3042--3050. PMLR, 16--18 Apr 2019.

\bibitem[Barakat et~al.(2021)Barakat, Bianchi, and Lehmann]{Target-AC}
Barakat, A., Bianchi, P., and Lehmann, J.
\newblock Analysis of a target-based actor-critic algorithm with linear
  function approximation.
\newblock \emph{CoRR}, abs/2106.07472, 2021.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt,
  and Silver]{options1}
Barreto, A., Dabney, W., Munos, R., Hunt, J.~J., Schaul, T., van Hasselt,
  H.~P., and Silver, D.
\newblock Successor features for transfer in reinforcement learning.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Bertsekas(2011)]{bertsekas11dynamic}
Bertsekas, D.~P.
\newblock Dynamic programming and optimal control 3rd edition, volume ii.
\newblock \emph{Belmont, MA: Athena Scientific}, 2011.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{Bhandari2019GlobalOG}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{ArXiv}, abs/1906.01786, 2019.

\bibitem[Bhandari et~al.(2018)Bhandari, Russo, and Singal]{Bhandari-TD}
Bhandari, J., Russo, D., and Singal, R.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock \emph{Oper. Res.}, 69:\penalty0 950--973, 2018.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Sutton, Ghavamzadeh, and
  Lee]{BhatnagarNAC}
Bhatnagar, S., Sutton, R.~S., Ghavamzadeh, M., and Lee, M.
\newblock Natural actor–critic algorithms.
\newblock \emph{Automatica}, 45\penalty0 (11):\penalty0 2471--2482, 2009.
\newblock ISSN 0005-1098.

\bibitem[Bolzern et~al.(2008)Bolzern, Colaneri, and
  De~Nicolao]{bolzern-etal08almost-sure-stability-ergodic-linear}
Bolzern, P., Colaneri, P., and De~Nicolao, G.
\newblock Almost sure stability of stochastic linear systems with ergodic
  parameters.
\newblock \emph{European Journal of Control}, 14\penalty0 (2):\penalty0
  114--123, 2008.

\bibitem[Borkar(2008)]{Borkar}
Borkar, V.~S.
\newblock \emph{{Stochastic Approximation}}.
\newblock Cambridge Books. Cambridge University Press, December 2008.

\bibitem[Cassel et~al.(2020)Cassel, Cohen, and Koren]{pmlr-v119-cassel20a}
Cassel, A., Cohen, A., and Koren, T.
\newblock Logarithmic regret for learning linear quadratic regulators
  efficiently.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of \emph{Proceedings of Machine Learning Research},
  pp.\  1328--1337. PMLR, 13--18 Jul 2020.

\bibitem[Chen \& Hazan(2020)Chen and
  Hazan]{chen-hazan20black-box-control-linear-dynamical}
Chen, X. and Hazan, E.
\newblock Black-box control for linear dynamical systems.
\newblock \emph{arXiv preprint arXiv:2007.06650}, 2020.

\bibitem[Daniely et~al.(2013)Daniely, Linial, and Shalev-Shwartz]{Improper2}
Daniely, A., Linial, N., and Shalev-Shwartz, S.
\newblock More data speeds up training time in learning halfspaces over sparse
  vectors.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~26, pp.\  145--153. Curran Associates, Inc., 2013.

\bibitem[Daniely et~al.(2014)Daniely, Linial, and Shalev-Shwartz]{Improper1}
Daniely, A., Linial, N., and Shalev-Shwartz, S.
\newblock From average case complexity to improper learning complexity.
\newblock In \emph{Proceedings of the Forty-Sixth Annual ACM Symposium on
  Theory of Computing}, STOC '14, pp.\  441–448, New York, NY, USA, 2014.
  Association for Computing Machinery.

\bibitem[{Dean} et~al.(2017){Dean}, {Mania}, {Matni}, {Recht}, and
  {Tu}]{SarahDean}
{Dean}, S., {Mania}, H., {Matni}, N., {Recht}, B., and {Tu}, S.
\newblock {On the Sample Complexity of the Linear Quadratic Regulator}.
\newblock \emph{arXiv e-prints}, art. arXiv:1710.01688, October 2017.

\bibitem[Denisov \& Walton(2020)Denisov and Walton]{Denisov2020RegretAO}
Denisov, D. and Walton, N.
\newblock Regret analysis of a markov policy gradient algorithm for multi-arm
  bandits.
\newblock \emph{ArXiv}, abs/2007.10229, 2020.

\bibitem[Durrett(2011)]{Durrett11probability:theory}
Durrett, R.
\newblock Probability: Theory and examples, 2011.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2019global}
Fazel, M., Ge, R., Kakade, S.~M., and Mesbahi, M.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator, 2018.

\bibitem[Flaxman et~al.(2005)Flaxman, Kalai, and McMahan]{Flaxman05}
Flaxman, A.~D., Kalai, A.~T., and McMahan, H.~B.
\newblock Online convex optimization in the bandit setting: Gradient descent
  without a gradient.
\newblock SODA '05, pp.\  385–394, USA, 2005. Society for Industrial and
  Applied Mathematics.

\bibitem[Gao \& Pavel(2017)Gao and Pavel]{softmax}
Gao, B. and Pavel, L.
\newblock On the properties of the softmax function with application in game
  theory and reinforcement learning.
\newblock \emph{ArXiv}, abs/1704.00805, 2017.

\bibitem[Gopalan \& Mannor(2015)Gopalan and Mannor]{pmlr-v40-Gopalan15}
Gopalan, A. and Mannor, S.
\newblock {Thompson Sampling for Learning Parameterized Markov Decision
  Processes}.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory},
  volume~40 of \emph{Proceedings of Machine Learning Research}, pp.\  861--898,
  Paris, France, 03--06 Jul 2015. PMLR.

\bibitem[Ibrahimi et~al.(2012)Ibrahimi, Javanmard, and Roy]{Ibrahimi}
Ibrahimi, M., Javanmard, A., and Roy, B.
\newblock Efficient reinforcement learning for high dimensional linear
  quadratic systems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~25, pp.\  2636--2644. Curran Associates, Inc., 2012.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning},
  2002.

\bibitem[Khalil(2015)]{khalil15nonlinear-control}
Khalil, H.~K.
\newblock \emph{Nonlinear Control}.
\newblock Pearson, 2015.

\bibitem[Koc\'{a}k et~al.(2014)Koc\'{a}k, Neu, Valko, and Munos]{KocakExpIX}
Koc\'{a}k, T., Neu, G., Valko, M., and Munos, R.
\newblock Efficient learning by implicit exploration in bandit problems with
  side observations.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~27, pp.\  613--621. Curran Associates, Inc., 2014.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{Konda_NIPS_AC}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock In Solla, S., Leen, T., and M\"{u}ller, K. (eds.), \emph{Advances in
  Neural Information Processing Systems}, volume~12. MIT Press, 2000.

\bibitem[Lai \& Robbins(1985)Lai and Robbins]{LAI19854}
Lai, T. and Robbins, H.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in Applied Mathematics}, 6\penalty0 (1):\penalty0 4 --
  22, 1985.
\newblock ISSN 0196-8858.

\bibitem[Li et~al.(2021)Li, Wei, Chi, Gu, and
  Chen]{li-etal21softmax-policy-gradient-exponential-converge-time}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y.
\newblock Softmax policy gradient methods can take exponential time to
  converge.
\newblock \emph{arXiv preprint arXiv:2102.11270}, 2021.

\bibitem[Littlestone \& Warmuth(1994)Littlestone and Warmuth]{EXP-WTS}
Littlestone, N. and Warmuth, M.~K.
\newblock The weighted majority algorithm.
\newblock \emph{Inform. Comput.}, 108\penalty0 (2):\penalty0 212--261, 1994.

\bibitem[{\L}ojasiewicz(1963)]{lojasiewicz1963equations}
{\L}ojasiewicz, S.
\newblock Les {\'e}quations aux d{\'e}riv{\'e}es partielles (paris, 1962),
  1963.

\bibitem[Lowe et~al.(2020)Lowe, Wu, Tamar, Harb, Abbeel, and Mordatch]{MADDPG}
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments, 2020.

\bibitem[Maclin \& Opitz(2011)Maclin and Opitz]{ensembleRL1}
Maclin, R. and Opitz, D.~W.
\newblock Popular ensemble methods: An empirical study.
\newblock \emph{CoRR}, abs/1106.0257, 2011.

\bibitem[Mania et~al.(2019)Mania, Tu, and Recht]{mania2019certainty}
Mania, H., Tu, S., and Recht, B.
\newblock Certainty equivalence is efficient for linear quadratic control.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pp.\  10154--10164. Curran Associates, Inc., 2019.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{Mei2020}
Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  6820--6829. PMLR, 2020.

\bibitem[Mohan et~al.(2016)Mohan, Chattopadhyay, and Kumar]{LQF2016}
Mohan, A., Chattopadhyay, A., and Kumar, A.
\newblock Hybrid mac protocols for low-delay scheduling.
\newblock In \emph{2016 IEEE 13th International Conference on Mobile Ad Hoc and
  Sensor Systems (MASS)}, pp.\  47--55, Los Alamitos, CA, USA, oct 2016. IEEE
  Computer Society.

\bibitem[Mohan et~al.(2020)Mohan, Gopalan, and Kumar]{Mohan2020ThroughputOD}
Mohan, A., Gopalan, A., and Kumar, A.
\newblock Throughput optimal decentralized scheduling with single-bit state
  feedback for a class of queueing systems.
\newblock \emph{ArXiv}, abs/2002.08141, 2020.

\bibitem[Neu(2015)]{Neu15a}
Neu, G.
\newblock Explore no more: Improved high-probability regret bounds for
  non-stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~28, pp.\  3168--3176. Curran Associates, Inc., 2015.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{OsbandTS_NIPS2013}
Osband, I., Russo, D., and Van~Roy, B.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~26, pp.\  3003--3011. Curran Associates, Inc., 2013.

\bibitem[Ouyang et~al.(2017)Ouyang, Gagrani, Nayyar, and
  Jain]{Ouyang2017LearningUM}
Ouyang, Y., Gagrani, M., Nayyar, A., and Jain, R.
\newblock Learning unknown markov decision processes: A thompson sampling
  approach.
\newblock In \emph{NIPS}, 2017.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{PetersNAC}
Peters, J. and Schaal, S.
\newblock Natural actor-critic.
\newblock \emph{Neurocomputing}, 71\penalty0 (7):\penalty0 1180--1190, 2008.
\newblock ISSN 0925-2312.
\newblock Progress in Modeling, Theory, and Application of Computational
  Intelligenc.

\bibitem[Radac \& Precup(2018)Radac and Precup]{ABScitation}
Radac, M.-B. and Precup, R.-E.
\newblock Data-driven model-free slip control of anti-lock braking systems
  using reinforcement q-learning.
\newblock \emph{Neurocomput.}, 275\penalty0 (C):\penalty0 317–329, January
  2018.

\bibitem[Rummery \& Niranjan(1994)Rummery and
  Niranjan]{Rummery94on-lineq-learning-SARSA}
Rummery, G.~A. and Niranjan, M.
\newblock On-line q-learning using connectionist systems.
\newblock Technical report, 1994.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{TRPO}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In Bach, F. and Blei, D. (eds.), \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1889--1897, Lille, France, 07--09 Jul
  2015. PMLR.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms, 2017.

\bibitem[Shani et~al.(2020)Shani, Efroni, and Mannor]{Shani2020AdaptiveTR}
Shani, L., Efroni, Y., and Mannor, S.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized mdps.
\newblock \emph{ArXiv}, abs/1909.02769, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{AlphaGo}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529:\penalty0 484--503, 2016.

\bibitem[{Singh} et~al.(2017){Singh}, {Okun}, and {Jackson}]{AlphaGozero}
{Singh}, S., {Okun}, A., and {Jackson}, A.
\newblock {Artificial intelligence: Learning to play Go from scratch}.
\newblock 550\penalty0 (7676):\penalty0 336--337, October 2017.
\newblock \doi{10.1038/550336a}.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{Sutton1998}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock The MIT Press, second edition, 2018.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{options2}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial Intelligence}, 112\penalty0 (1):\penalty0 181--211,
  1999.
\newblock ISSN 0004-3702.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and Mansour]{Sutton2000}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~12, pp.\  1057--1063. MIT Press, 2000.

\bibitem[{Tassiulas} \& {Ephremides}(1992){Tassiulas} and
  {Ephremides}]{tassiulas-ephremides92stability-queueing}
{Tassiulas}, L. and {Ephremides}, A.
\newblock Stability properties of constrained queueing systems and scheduling
  policies for maximum throughput in multihop radio networks.
\newblock \emph{IEEE Transactions on Automatic Control}, 37\penalty0
  (12):\penalty0 1936--1948, 1992.
\newblock \doi{10.1109/9.182479}.

\bibitem[Wiering \& van Hasselt(2008)Wiering and van Hasselt]{ensembleRL3}
Wiering, M.~A. and van Hasselt, H.
\newblock Ensemble algorithms in reinforcement learning.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 38\penalty0 (4):\penalty0 930--936, 2008.

\bibitem[Xiliang et~al.(2018)Xiliang, Cao, Li, Xu, and Lai]{ensembleRL2}
Xiliang, C., Cao, L., Li, C.-x., Xu, Z.-x., and Lai, J.
\newblock Ensemble network architecture for deep reinforcement learning.
\newblock \emph{Mathematical Problems in Engineering}, 2018:\penalty0 1--6, 04
  2018.

\bibitem[Xu et~al.(2020)Xu, Wang, and Liang]{Improving-AC-NAC}
Xu, T., Wang, Z., and Liang, Y.
\newblock Improving sample complexity bounds for (natural) actor-critic
  algorithms.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  4358--4369. Curran Associates, Inc., 2020.

\end{thebibliography}
