\begin{thebibliography}{10}

\bibitem{opt-net}
Brandon Amos and J.~Zico Kolter.
\newblock Opt{N}et: Differentiable optimization as a layer in neural networks.
\newblock In {\em International Conference on Machine Learning, {ICML}},
  Proceedings of Machine Learning Research, 2017.

\bibitem{input-convex}
Brandon Amos, Lei Xu, and J.~Zico Kolter.
\newblock Input convex neural networks.
\newblock In {\em International Conference on Machine Learning, {ICML}}, 2017.

\bibitem{deep-equibrilium}
Shaojie Bai, J.~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  {NeurIPS}}, 2019.

\bibitem{large-scale-optimization}
L{\'{e}}on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em {SIAM} Rev.}, 60(2):223--311, 2018.

\bibitem{geometric-deep-learning}
Michael~M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic.
\newblock Geometric deep learning: Grids, groups, graphs, geodesics, and
  gauges.
\newblock {\em CoRR}, abs/2104.13478, 2021.

\bibitem{optimization-models}
Giuseppe~C Calafiore and Laurent El~Ghaoui.
\newblock {\em Optimization models}.
\newblock Cambridge university press, 2014.

\bibitem{gcnii}
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li.
\newblock Simple and deep graph convolutional networks.
\newblock In {\em International Conference on Machine Learning, {ICML}},
  Proceedings of Machine Learning Research, 2020.

\bibitem{graph-denoising-via-unfolding}
Siheng Chen and Yonina~C Eldar.
\newblock Graph signal denoising via unrolling networks.
\newblock In {\em International Conference on Acoustics, Speech and Signal
  Processing, ICASSP}, 2021.

\bibitem{convex-analysis-new}
Patrick Cheridito.
\newblock Convex analysis.
\newblock {\em Lecture Notes (Princeton University, Princeton, NJ)}, 2013.

\bibitem{spacse-attention}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em CoRR}, abs/1904.10509, 2019.

\bibitem{bilevel-survey}
Beno{\^{\i}}t Colson, Patrice Marcotte, and Gilles Savard.
\newblock An overview of bilevel optimization.
\newblock {\em Ann. Oper. Res.}, 153(1):235--256, 2007.

\bibitem{combettes2011proximal}
Patrick~L Combettes and Jean-Christophe Pesquet.
\newblock Proximal splitting methods in signal processing.
\newblock In {\em Fixed-point algorithms for inverse problems in science and
  engineering}, pages 185--212. Springer, 2011.

\bibitem{inexact-first-order-fixederror}
Olivier Devolder, Fran{\c{c}}ois Glineur, and Yurii~E. Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock {\em Math. Program.}, 146(1-2):37--75, 2014.

\bibitem{bilevel2021}
Jordan Fr{\'{e}}con, Gilles Gasso, Massimiliano Pontil, and Saverio Salzo.
\newblock Bregman neural networks.
\newblock In {\em International Conference on Machine Learning, {ICML}}, 2022.

\bibitem{hamburger}
Zhengyang Geng, Meng{-}Hao Guo, Hongxu Chen, Xia Li, Ke~Wei, and Zhouchen Lin.
\newblock Is attention better than matrix decomposition?
\newblock In {\em International Conference on Learning Representations,
  {ICLR}}, 2021.

\bibitem{gregor2010learning}
Karol Gregor and Yann LeCun.
\newblock Learning fast approximations of sparse coding.
\newblock In {\em International Conference on Machine Learning, ICML}, 2010.

\bibitem{inexact-pgd-gu(AAAI18)}
Bin Gu, De~Wang, Zhouyuan Huo, and Heng Huang.
\newblock Inexact proximal gradient methods for non-convex and non-smooth
  optimization.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{star-transformer}
Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng
  Zhang.
\newblock Star-transformer.
\newblock In {\em Conference of the North American Chapter of the Association
  for Computational Linguistics: Human Language Technologies, {NAACL-HLT}},
  2019.

\bibitem{alternating-hardt}
Moritz Hardt.
\newblock Understanding alternating minimization for matrix completion.
\newblock In {\em Annual Symposium on Foundations of Computer Science, {FOCS}},
  2014.

\bibitem{Hao2017sparse}
Hao He, Bo~Xin, Satoshi Ikehata, and David Wipf.
\newblock From bayesian sparsity to gated recurrent nets.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  {NeurIPS}}, 2017.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition,
  {CVPR}}, 2016.

\bibitem{hershey2014deep}
John Hershey, Jonathan~Le Roux, and Felix Weninger.
\newblock Deep unfolding: {M}odel-based inspiration of novel deep
  architectures.
\newblock {\em CoRR}, abs/1409.2574, 2014.

\bibitem{symmetric-deep-learning}
Shell~Xu Hu, Sergey Zagoruyko, and Nikos Komodakis.
\newblock Exploring weight symmetry in deep neural networks.
\newblock {\em Comput. Vis. Image Underst.}, 187, 2019.

\bibitem{deep-bilevel-2018}
Simon Jenni and Paolo Favaro.
\newblock Deep bilevel learning.
\newblock In {\em European Conference on Computer Vision, ECCV}, 2018.

\bibitem{reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  {NeurIPS}}, 2012.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444, 2015.

\bibitem{energy-base-learning-tutorial}
Yann LeCun, Sumit Chopra, Raia Hadsell, M~Ranzato, and F~Huang.
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting structured data}, 1(0), 2006.

\bibitem{proximaloperator-zhouchen}
Jia Li, Mingqing Xiao, Cong Fang, Yue Dai, Chao Xu, and Zhouchen Lin.
\newblock Training neural networks by lifted proximal operator machines.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2020.

\bibitem{pay-attention-to-mlp}
Hanxiao Liu, Zihang Dai, David~R. So, and Quoc~V. Le.
\newblock Pay attention to mlps.
\newblock {\em CoRR}, abs/2105.08050, 2021.

\bibitem{elastic-gnn}
Xiaorui Liu, Wei Jin, Yao Ma, Yaxin Li, Hua Liu, Yiqi Wang, Ming Yan, and
  Jiliang Tang.
\newblock Elastic graph neural networks.
\newblock In {\em International Conference on Machine Learning, ICML}, 2021.

\bibitem{soft}
Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao,
  Chunjing Xu, Tao Xiang, and Li~Zhang.
\newblock {SOFT:} softmax-free transformer with linear complexity.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  {{NeurIPS}}}, 2021.

\bibitem{alternating-wang}
Songtao Lu, Mingyi Hong, and Zhengdao Wang.
\newblock {PA-GD:} on the convergence of perturbed alternating gradient descent
  to second-order stationary points for structured nonconvex optimization.
\newblock In {\em International Conference on Machine Learning, ICML}, 2019.

\bibitem{a-univied-view-on-gnn-as-denoising}
Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah.
\newblock A unified view on graph neural networks as graph signal denoising.
\newblock {\em CoRR}, abs/2010.01777, 2020.

\bibitem{IMDB}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Annual Meeting of the Association for Computational
  Linguistics, ACL}, 2011.

\bibitem{IGLU}
S.~Deepak Narayanan, Aditya Sinha, Prateek Jain, Purushottam Kar, and
  Sundararajan Sellamanickam.
\newblock {IGLU:} efficient {GCN} training via lazy updates.
\newblock {\em CoRR}, abs/2109.13995, 2021.

\bibitem{a-unified-view-on-gcn}
Xuran Pan, Shiji Song, and Gao Huang.
\newblock A unified framework for convolution-based graph neural networks,
  2021.

\bibitem{proximal-survey}
Neal Parikh and Stephen Boyd.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends in optimization}, 1(3):127--239, 2014.

\bibitem{glove}
Jeffrey Pennington, Richard Socher, and Christopher~D. Manning.
\newblock Glo{V}e: Global vectors for word representation.
\newblock In {\em Conference on Empirical Methods in Natural Language
  Processing, {EMNLP}}, 2014.

\bibitem{cosformer}
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie
  Yan, Lingpeng Kong, and Yiran Zhong.
\newblock {cosFormer}: Rethinking softmax in attention.
\newblock In {\em The Tenth International Conference on Learning
  Representations, {ICLR}}, 2022.

\bibitem{hopfield-is-all-you-need}
Hubert Ramsauer, Bernhard Sch{\"{a}}fl, Johannes Lehner, Philipp Seidl, Michael
  Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David~P. Kreil,
  Michael~K. Kopp, G{\"{u}}nter Klambauer, Johannes Brandstetter, and Sepp
  Hochreiter.
\newblock Hopfield networks is all you need.
\newblock In {\em International Conference on Learning Representations,
  {ICLR}}, 2021.

\bibitem{convex-anaysis}
R.~Tyrrell Rockafellar.
\newblock {\em Convex Analysis}.
\newblock Princeton University Press, 1970.

\bibitem{high-school-geometry}
Pierre Samuel.
\newblock {\em Projective Geometry}.
\newblock Springer, 1988.

\bibitem{inexact-pgd-mark(NIPS11)}
Mark Schmidt, Nicolas~Le Roux, and Francis~R. Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  {NeurIPS}}, 2011.

\bibitem{transformer-oversmooth}
Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen
  Lee, and James~T Kwok.
\newblock Revisiting over-smoothing in {BERT} from the perspective of graph.
\newblock {\em International Conference on Learning Representations, {ICLR}},
  2022.

\bibitem{SST2}
Richard Socher, John Bauer, Christopher~D. Manning, and Andrew~Y. Ng.
\newblock Parsing with compositional vector grammars.
\newblock In {\em Annual Meeting of the Association for Computational
  Linguistics, {ACL}}, 2013.

\bibitem{Sprechmann15}
Pablo Sprechmann, Alex Bronstein, and Guillermo Sapiro.
\newblock Learning efficient sparse and low rank models.
\newblock {\em IEEE Trans. Pattern Analysis and Machine Intelligence}, 37(9),
  2015.

\bibitem{mm-intro}
Ying Sun, Prabhu Babu, and Daniel~P Palomar.
\newblock Majorization-minimization algorithms in signal processing,
  communications, and machine learning.
\newblock {\em IEEE Transactions on Signal Processing}, 65(3):794--816, 2016.

\bibitem{attention-is-all-you-need}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  {NeurIPS}}, 2017.

\bibitem{gat}
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
  Li{\`{o}}, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In {\em International Conference on Learning Representations,
  {ICLR}}, 2018.

\bibitem{wang2016learning}
Zhangyang Wang, Qing Ling, and Thomas Huang.
\newblock Learning deep $\ell_0$ encoders.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2016.

\bibitem{inexact-pgd-stochastic(14)}
Lin Xiao and Tong Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em {SIAM} J. Optim.}, 24(4):2057--2075, 2014.

\bibitem{opt-induced-equil}
Xingyu Xie, Qiuhao Wang, Zenan Ling, Xia Li, Yisen Wang, Guangcan Liu, and
  Zhouchen Lin.
\newblock Optimization induced equilibrium networks.
\newblock {\em CoRR}, abs/2105.13228, 2021.

\bibitem{revisiting-oversmoothing}
Chaoqi Yang, Ruijie Wang, Shuochao Yao, Shengzhong Liu, and Tarek~F.
  Abdelzaher.
\newblock Revisiting "over-smoothing" in deep {GCN}s.
\newblock {\em CoRR}, abs/2003.13663, 2020.

\bibitem{twirls}
Yongyi Yang, Tang Liu, Yangkun Wang, Jinjing Zhou, Quan Gan, Zhewei Wei, Zheng
  Zhang, Zengfeng Huang, and David Wipf.
\newblock Graph neural networks inspired by classical iterative algorithms.
\newblock In {\em International Conference on Machine Learning, {ICML}}, 2021.

\bibitem{UvsI}
Yongyi Yang, Yangkun Wang, Zengfeng Huang, and David Wipf.
\newblock Implicit {vs} unfolded graph neural networks.
\newblock {\em CoRR}, abs/2111.06592, 2021.

\bibitem{CCCP}
Alan~L. Yuille and Anand Rangarajan.
\newblock The concave-convex procedure {(CCCP)}.
\newblock In {\em Annual Conference on Neural Information Processing Systems,
  {{NeurIPS}}}, 2001.

\bibitem{hongwei}
Hongwei Zhang, Tijin Yan, Zenjun Xie, Yuanqing Xia, and Yuan Zhang.
\newblock Revisiting graph convolutional network on semi-supervised node
  classification from an optimization perspective.
\newblock {\em CoRR}, abs/2009.11469, 2020.

\bibitem{dirichlet-energy}
Kaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li~Li, Soo{-}Hyun Choi, and
  Xia Hu.
\newblock Dirichlet energy constrained learning for deep graph neural networks.
\newblock {\em CoRR}, abs/2107.02392, 2021.

\bibitem{unified-gnn-as-optimization}
Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui.
\newblock Interpreting and unifying graph neural networks with an optimization
  framework.
\newblock {\em CoRR}, abs/2101.11859, 2021.

\end{thebibliography}
