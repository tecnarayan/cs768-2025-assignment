\begin{thebibliography}{10}

\bibitem{AllenZhu2019ACT}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em ICML}, 2019.

\bibitem{AndrieuMP2005}
Christophe Andrieu, Eric Moulines, and Pierre Priouret.
\newblock Stability of stochastic approximation under verifiable conditions.
\newblock {\em SIAM Journal on Control and Optimization}, 44(1):283--312, 2005.

\bibitem{Banijamali2018DeepVS}
Ershad Banijamali, Amir-Hossein Karimi, and Ali Ghodsi.
\newblock Deep variational sufficient dimensionality reduction.
\newblock In {\em Third Workshop on Bayesian Deep Learning (NeurIPS 2018)},
  2018.

\bibitem{Albert90}
Albert Benveniste, Michael M\'etivier, and Pierre Priouret.
\newblock {\em Adaptive Algorithms and Stochastic Approximations}.
\newblock Berlin: Springer, 1990.

\bibitem{bolley2005weighted}
Fran{\c{c}}ois Bolley and C{\'e}dric Villani.
\newblock Weighted csisz{\'a}r-kullback-pinsker inequalities and applications
  to transportation inequalities.
\newblock {\em Annales de la Facult{\'e} des sciences de Toulouse:
  Math{\'e}matiques}, 14(3):331--352, 2005.

\bibitem{Bura2001EstimatingTS}
Efstathia Bura and R.~Dennis Cook.
\newblock Estimating the structural dimension of regressions via parametric
  inverse regression.
\newblock {\em Journal of The Royal Statistical Society Series B-statistical
  Methodology}, 63:393--410, 2001.

\bibitem{SGHMC2014}
Tianqi Chen, Emily Fox, and Carlos Guestrin.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In {\em International conference on machine learning}, pages
  1683--1691, 2014.

\bibitem{Cook2000save}
R.~Dennis Cook.
\newblock Save: a method for dimension reduction and graphics in regression.
\newblock {\em Communications in statistics-Theory and methods},
  29(9-10):2109--2121, 2000.

\bibitem{cook2007fisher}
R.~Dennis Cook.
\newblock Fisher lecture: Dimension reduction in regression.
\newblock {\em Statistical Science}, 22(1):1--26, 2007.

\bibitem{Cook1991save}
R.~Dennis Cook and S.~Weisberg.
\newblock Discussion of `sliced inverse regression for dimension reduction,' by
  k.c. li.
\newblock {\em \JASA}, 86:328--332, 1991.

\bibitem{DengLiang2020Contour}
Wei Deng, Guang Lin, and Faming Liang.
\newblock A contour stochastic gradient langevin dynamics algorithm for
  simulations of multi-modal distributions.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 15725--15736. Curran Associates, Inc., 2020.

\bibitem{deng2019adaptive}
Wei Deng, Xiao Zhang, Faming Liang, and Guang Lin.
\newblock An adaptive empirical bayesian method for sparse deep learning.
\newblock {\em Advances in neural information processing systems}, 2019:5563,
  2019.

\bibitem{Du2019GradientDF}
Simon~S. Du, Jason~D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em ICML}, 2019.

\bibitem{Fertl2021ConditionalVE}
Lukas Fertl and Efstathia Bura.
\newblock Conditional variance estimator for sufficient dimension reduction.
\newblock {\em arXiv: Methodology}, 2021.

\bibitem{fukumizu2009kernel}
Kenji Fukumizu, Francis~R Bach, and Michael~I Jordan.
\newblock Kernel dimension reduction in regression.
\newblock {\em The Annals of Statistics}, 37(4):1871--1905, 2009.

\bibitem{Fukumizu2014GradientBasedKD}
Kenji Fukumizu and Chenlei Leng.
\newblock Gradient-based kernel dimension reduction for regression.
\newblock {\em Journal of the American Statistical Association}, 109:359 --
  370, 2014.

\bibitem{gao2021global}
Xuefeng Gao, Mert G{\"u}rb{\"u}zbalaban, and Lingjiong Zhu.
\newblock Global convergence of stochastic gradient hamiltonian monte carlo for
  nonconvex stochastic optimization: Nonasymptotic performance bounds and
  momentum-based acceleration.
\newblock {\em Operations Research}, 2021.

\bibitem{Gori1992OnTP}
Marco Gori and Alberto Tesi.
\newblock On the problem of local minima in backpropagation.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}, 14:76--86, 1992.

\bibitem{Glehre2016NoisyAF}
{\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Marcin Moczulski, Misha Denil, and Yoshua
  Bengio.
\newblock Noisy activation functions.
\newblock In {\em {ICML}}, pages 3059--3068, 2016.

\bibitem{Han2015nips}
Song Han, Jeff Pool, John Tran, and William~J. Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in Neural Information Processing Systems 28}, pages
  1135--1143, 2015.

\bibitem{Hinton2007}
Geoffrey Hinton.
\newblock Learning multiple layers of representation.
\newblock {\em Trends in Cognitive Sciences}, 11(10):428--434, 2007.

\bibitem{Hinton2006ReducingTD}
Geoffrey~E. Hinton and R.~Salakhutdinov.
\newblock Reducing the dimensionality of data with neural networks.
\newblock {\em Science}, 313:504 -- 507, 2006.

\bibitem{hristache2001structure}
Marian Hristache, Anatoli Juditsky, J{\"o}rg Polzehl, and Vladimir Spokoiny.
\newblock Structure adaptive approach for dimension reduction.
\newblock {\em The Annals of Statistics}, 29(6):1537--1566, 2001.

\bibitem{Kapla2021FusingSD}
Daniel Kapla, Lukas Fertl, and Efstathia Bura.
\newblock Fusing sufficient dimension reduction with neural networks.
\newblock {\em Computational Statistics \& Data Analysis}, 2021.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lee2013general}
Kuang-Yao Lee, Bing Li, and Francesca Chiaromonte.
\newblock A general theory for nonlinear sufficient dimension reduction:
  Formulation and estimation.
\newblock {\em The Annals of Statistics}, 41(1):221--249, 2013.

\bibitem{li2018sufficient}
Bing Li.
\newblock {\em Sufficient dimension reduction: Methods and applications with
  R}.
\newblock CRC Press, 2018.

\bibitem{Li2007OnDR}
Bing Li and Shaoli Wang.
\newblock On directional regression for dimension reduction.
\newblock {\em Journal of the American Statistical Association}, 102:1008 --
  997, 2007.

\bibitem{Li2005ContourRA}
Bing~Dong Li, Hongyuan Zha, and Francesca Chiaromonte.
\newblock Contour regression: A general approach to dimension reduction.
\newblock {\em Annals of Statistics}, 33:1580--1616, 2005.

\bibitem{li1991sliced}
Ker-Chau Li.
\newblock Sliced inverse regression for dimension reduction.
\newblock {\em Journal of the American Statistical Association},
  86(414):316--327, 1991.

\bibitem{li1992principal}
Ker-Chau Li.
\newblock On principal hessian directions for data visualization and dimension
  reduction: Another application of stein's lemma.
\newblock {\em Journal of the American Statistical Association},
  87(420):1025--1039, 1992.

\bibitem{liang2018bayesian}
Faming Liang, Qizhai Li, and Lei Zhou.
\newblock Bayesian neural networks for selection of drug sensitive genes.
\newblock {\em Journal of the American Statistical Association},
  113(523):955--972, 2018.

\bibitem{Liang2021SufficientDR}
Siqi Liang, Wei-Heng Huang, and Faming Liang.
\newblock Sufficient dimension reduction with deep neural networks for
  phenotype prediction.
\newblock {\em Proceedings of the 3rd International Conference on Statistics:
  Theory and Applications}, 2021.

\bibitem{Lin2018OnCA}
Qian Lin, Zhigen Zhao, and Jun~S. Liu.
\newblock On consistency and sparsity for sliced inverse regression in high
  dimensions.
\newblock {\em \ANNALS}, 46(2):580--610, 2018.

\bibitem{Lin2019SparseSI}
Qian Lin, Zhigen Zhao, and Jun~S. Liu.
\newblock Sparse sliced inverse regression via lasso.
\newblock {\em Journal of the American Statistical Association}, 114:1726 --
  1739, 2019.

\bibitem{Neelakantan2017AddingGN}
Arvind Neelakantan, Luke Vilnis, Quoc~V. Le, Ilya Sutskever, Lukasz Kaiser,
  Karol Kurach, and James Martens.
\newblock Adding gradient noise improves learning for very deep networks.
\newblock {\em ArXiv}, abs/1511.06807, 2017.

\bibitem{Nemeth2019StochasticGM}
Christopher Nemeth and Paul Fearnhead.
\newblock Stochastic gradient markov chain monte carlo.
\newblock {\em Journal of the American Statistical Association}, 116:433 --
  450, 2019.

\bibitem{nguyen2017loss}
Quynh Nguyen and Matthias Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In {\em International conference on machine learning}, pages
  2603--2612. PMLR, 2017.

\bibitem{nilsson2007regression}
Jens Nilsson, Fei Sha, and Michael~I Jordan.
\newblock Regression on manifolds using kernel dimension reduction.
\newblock In {\em Proceedings of the 24th international conference on Machine
  learning}, pages 697--704. ACM, 2007.

\bibitem{Noh2017RegularizingDN}
Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, and Bohyung Han.
\newblock Regularizing deep neural networks by noise: Its interpretation and
  optimization.
\newblock {\em ArXiv}, abs/1710.05179, 2017.

\bibitem{raginsky2017non}
Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In {\em Conference on Learning Theory}, pages 1674--1703. PMLR, 2017.

\bibitem{AdaBoost2001}
Gunnar R{\"a}tsch, Takashi Onoda, and Klaus-Robert M{\"u}ller.
\newblock Soft margins for adaboost.
\newblock {\em Machine Learning}, 42:287--320, 2001.

\bibitem{SHinton2009}
Ruslan Salakhutdinov and Geoffrey Hinton.
\newblock Deep boltzmann machines.
\newblock In {\em Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, pages 448--455, 2009.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{SunLiangKstonet2022}
Yan Sun and Faming Liang.
\newblock A kernel-expanded stochastic neural network.
\newblock {\em \JRSSB}, 84:547--578, 2022.

\bibitem{SunSLiang2021}
Yan Sun, Qifan Song, and Faming Liang.
\newblock Consistent sparse deep learning: Theory and computation.
\newblock {\em Journal of the American Statistical Association}, page in press,
  2021.

\bibitem{suzuki2010sufficient}
Taiji Suzuki and Masashi Sugiyama.
\newblock Sufficient dimension reduction via squared-loss mutual information
  estimation.
\newblock In {\em Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 804--811. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem{Whye2016ConsistencyAF}
TehYee Whye, H~ThieryAlexandre, and J~VollmerSebastian.
\newblock Consistency and fluctuations for stochastic gradient langevin
  dynamics.
\newblock {\em Journal of Machine Learning Research}, 2016.

\bibitem{wu2008kernel}
Han-Ming Wu.
\newblock Kernel sliced inverse regression with applications to classification.
\newblock {\em Journal of Computational and Graphical Statistics},
  17(3):590--610, 2008.

\bibitem{xia2007constructive}
Yingcun Xia.
\newblock A constructive approach to the estimation of dimension reduction
  directions.
\newblock {\em The Annals of Statistics}, 35(6):2654--2690, 2007.

\bibitem{xia2002adaptive}
Yingcun Xia, Howell Tong, WK~Li, and Li-Xing Zhu.
\newblock An adaptive estimation of dimension reduction space.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 64(3):363--410, 2002.

\bibitem{yamada2011SDR}
Makoto Yamada, Gang Niu, Jun Takagi, and Masashi Sugiyama.
\newblock Computationally efficient sufficient dimension reduction via
  squared-loss mutual information.
\newblock In Chun-Nan Hsu and Wee~Sun Lee, editors, {\em Proceedings of the
  Asian Conference on Machine Learning}, volume~20 of {\em Proceedings of
  Machine Learning Research}, pages 247--262, South Garden Hotels and Resorts,
  Taoyuan, Taiwain, 14--15 Nov 2011. PMLR.

\bibitem{You2018AdversarialNL}
Zhonghui You, Jinmian Ye, Kunming Li, and Ping Wang.
\newblock Adversarial noise layer: Regularize neural network by adding noise.
\newblock In {\em 2019 IEEE International Conference on Image Processing
  (ICIP)}, pages 909--913, 2018.

\bibitem{Yu2021SimpleAE}
Tianyuan Yu, Yongxin Yang, Da~Li, Timothy~M. Hospedales, and T.~Xiang.
\newblock Simple and effective stochastic neural networks.
\newblock In {\em AAAI}, 2021.

\bibitem{Zou2020GradientDO}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock {\em Machine Learning}, 109:467 -- 492, 2020.

\bibitem{ZouGu2019}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In {\em NuerIPS}, 2019.

\end{thebibliography}
