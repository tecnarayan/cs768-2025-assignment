\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore, Murray, Steiner,
  Tucker, Vasudevan, Warden, Wicke, Yu, and Zheng]{tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R.,
  Moore, S., Murray, D.~G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P.,
  Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: A system for {Large-Scale} machine learning.
\newblock In \emph{12th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 16)}, pp.\  265--283, Savannah, GA, November 2016.
  USENIX Association.
\newblock ISBN 978-1-931971-33-1.
\newblock URL
  \url{https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi}.

\bibitem[Abnar et~al.(2021)Abnar, Berg, Ghiasi, Dehghani, Kalchbrenner, and
  Sedghi]{abnar2021gradual}
Abnar, S., Berg, R. v.~d., Ghiasi, G., Dehghani, M., Kalchbrenner, N., and
  Sedghi, H.
\newblock Gradual domain adaptation in the wild: When intermediate
  distributions are absent.
\newblock \emph{arXiv preprint arXiv:2106.06080}, 2021.

\bibitem[Ajakan et~al.(2014)Ajakan, Germain, Larochelle, Laviolette, and
  Marchand]{ajakan2014domain}
Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., and Marchand, M.
\newblock Domain-adversarial neural networks.
\newblock \emph{arXiv preprint arXiv:1412.4446}, 2014.

\bibitem[Anderson(1972)]{anderson1972more}
Anderson, P.~W.
\newblock More is different.
\newblock \emph{Science}, 177\penalty0 (4047):\penalty0 393--396, 1972.

\bibitem[Arazo et~al.(2020)Arazo, Ortego, Albert, O’Connor, and
  McGuinness]{arazo2020pseudo}
Arazo, E., Ortego, D., Albert, P., O’Connor, N.~E., and McGuinness, K.
\newblock Pseudo-labeling and confirmation bias in deep semi-supervised
  learning.
\newblock In \emph{IJCNN}, pp.\  1--8. IEEE, 2020.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{WGAN}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock {W}asserstein generative adversarial networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{ICML}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pp.\  214--223. PMLR, 06--11
  Aug 2017.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{fine-grained}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--332, 2019.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Ben-David et~al.(2007)Ben-David, Blitzer, Crammer, Pereira,
  et~al.]{ben2007analysis}
Ben-David, S., Blitzer, J., Crammer, K., Pereira, F., et~al.
\newblock Analysis of representations for domain adaptation.
\newblock \emph{Advances in neural information processing systems},
  19:\penalty0 137, 2007.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben-david2010theory}
Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan,
  J.~W.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79\penalty0 (1):\penalty0 151--175, 2010.

\bibitem[Blackard \& Dean(1999)Blackard and Dean]{CoverType}
Blackard, J.~A. and Dean, D.~J.
\newblock Comparative accuracies of artificial neural networks and discriminant
  analysis in predicting forest cover types from cartographic variables.
\newblock \emph{Computers and electronics in agriculture}, 24\penalty0
  (3):\penalty0 131--151, 1999.

\bibitem[Bobu et~al.(2018)Bobu, Tzeng, Hoffman, and Darrell]{bobu2018adapting}
Bobu, A., Tzeng, E., Hoffman, J., and Darrell, T.
\newblock Adapting to continuously shifting domains, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJsBjPJvf}.

\bibitem[Cao \& Gu(2019)Cao and Gu]{cao2019generalization}
Cao, Y. and Gu, Q.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10835--10845, 2019.

\bibitem[Chen \& Chao(2021)Chen and Chao]{chen2021gradual}
Chen, H.-Y. and Chao, W.-L.
\newblock Gradual domain adaptation without indexed intermediate domains.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Chollet et~al.(2015)]{keras}
Chollet, F. et~al.
\newblock Keras.
\newblock \url{https://keras.io}, 2015.

\bibitem[Courty et~al.(2014)Courty, Flamary, and Tuia]{courty2014domain}
Courty, N., Flamary, R., and Tuia, D.
\newblock Domain adaptation with regularized optimal transport.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  274--289. Springer, 2014.

\bibitem[Courty et~al.(2016)Courty, Flamary, Tuia, and
  Rakotomamonjy]{courty2016optimal}
Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A.
\newblock Optimal transport for domain adaptation.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 39\penalty0 (9):\penalty0 1853--1865, 2016.

\bibitem[Courty et~al.(2017)Courty, Flamary, Habrard, and
  Rakotomamonjy]{courty2017joint}
Courty, N., Flamary, R., Habrard, A., and Rakotomamonjy, A.
\newblock Joint distribution optimal transportation for domain adaptation.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Dong et~al.(2022)Dong, Zhou, Wang, and Zhao]{dong2022algorithms}
Dong, J., Zhou, S., Wang, B., and Zhao, H.
\newblock Algorithms and theory for supervised gradual domain adaptation.
\newblock \emph{arXiv preprint arXiv:2204.11644}, 2022.

\bibitem[Dua \& Graff(2017)Dua and Graff]{UCI_repository}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Farshchian et~al.(2019)Farshchian, Gallego, Cohen, Bengio, Miller, and
  Solla]{farshchian2018adversarial}
Farshchian, A., Gallego, J.~A., Cohen, J.~P., Bengio, Y., Miller, L.~E., and
  Solla, S.~A.
\newblock Adversarial domain adaptation for stable brain-machine interfaces.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Hyx6Bi0qYm}.

\bibitem[Gadermayr et~al.(2018)Gadermayr, Eschweiler, Klinkhammer, Boor, and
  Merhof]{gadermayr2018gradual}
Gadermayr, M., Eschweiler, D., Klinkhammer, B.~M., Boor, P., and Merhof, D.
\newblock Gradual domain adaptation for segmenting whole slide images showing
  pathological variability.
\newblock In \emph{International Conference on Image and Signal Processing},
  pp.\  461--469. Springer, 2018.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,
  F., Marchand, M., and Lempitsky, V.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The journal of machine learning research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Ginosar et~al.(2015)Ginosar, Rakelly, Sachs, Yin, and
  Efros]{ginosar2015century}
Ginosar, S., Rakelly, K., Sachs, S., Yin, B., and Efros, A.~A.
\newblock A century of portraits: A visual historical record of american high
  school yearbooks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision Workshops}, pp.\  1--7, 2015.

\bibitem[Gulrajani \& Lopez-Paz(2021)Gulrajani and Lopez-Paz]{gulrajani2021in}
Gulrajani, I. and Lopez-Paz, D.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Zhao, Basart, Steinhardt, and
  Song]{hendrycks2021natural}
Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D.
\newblock Natural adversarial examples.
\newblock In \emph{CVPR}, pp.\  15262--15271, 2021.

\bibitem[Hoffman et~al.(2014)Hoffman, Darrell, and
  Saenko]{hoffman2014continuous}
Hoffman, J., Darrell, T., and Saenko, K.
\newblock Continuous manifold based adaptation for evolving visual domains.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  867--874, 2014.

\bibitem[Huang et~al.(2014)Huang, Shi, and Suykens]{huang2014ramp}
Huang, X., Shi, L., and Suykens, J.~A.
\newblock Ramp loss linear programming support vector machine.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 2185--2211, 2014.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  448--456, 2015.

\bibitem[Kantorovich(1939)]{kantorovich1939}
Kantorovich, L.~V.
\newblock Mathematical methods of organizing and planning production.
\newblock \emph{Management science}, 6\penalty0 (4):\penalty0 366--422, 1939.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Koh, P.~W., Sagawa, S., Marklund, H., Xie, S.~M., Zhang, M., Balsubramani, A.,
  Hu, W., Yasunaga, M., Phillips, R.~L., Gao, I., et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5637--5664. PMLR, 2021.

\bibitem[Kumar et~al.(2020)Kumar, Ma, and Liang]{kumar2020understanding}
Kumar, A., Ma, T., and Liang, P.
\newblock Understanding self-training for gradual domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5468--5479. PMLR, 2020.

\bibitem[Kuznetsov \& Mohri(2014)Kuznetsov and
  Mohri]{kuznetsov2014generalization}
Kuznetsov, V. and Mohri, M.
\newblock Generalization bounds for time series prediction with non-stationary
  processes.
\newblock In \emph{International conference on algorithmic learning theory},
  pp.\  260--274. Springer, 2014.

\bibitem[Kuznetsov \& Mohri(2015)Kuznetsov and Mohri]{kuznetsov2015learning}
Kuznetsov, V. and Mohri, M.
\newblock Learning theory and algorithms for forecasting non-stationary time
  series.
\newblock In \emph{NIPS}, pp.\  541--549. Citeseer, 2015.

\bibitem[Kuznetsov \& Mohri(2016)Kuznetsov and Mohri]{kuznetsov2016time}
Kuznetsov, V. and Mohri, M.
\newblock Time series prediction and online learning.
\newblock In \emph{Conference on Learning Theory}, pp.\  1190--1213. PMLR,
  2016.

\bibitem[Kuznetsov \& Mohri(2017)Kuznetsov and
  Mohri]{kuznetsov2017generalization}
Kuznetsov, V. and Mohri, M.
\newblock Generalization bounds for non-stationary mixing processes.
\newblock \emph{Machine Learning}, 106\penalty0 (1):\penalty0 93--117, 2017.

\bibitem[Kuznetsov \& Mohri(2020)Kuznetsov and Mohri]{kuznetsov2020discrepancy}
Kuznetsov, V. and Mohri, M.
\newblock Discrepancy-based theory and algorithms for forecasting
  non-stationary time series.
\newblock \emph{Annals of Mathematics and Artificial Intelligence}, 88\penalty0
  (4):\penalty0 367--399, 2020.

\bibitem[LeCun \& Cortes(1998)LeCun and Cortes]{mnist}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock 1998.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
Lee, D.-H. et~al.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML},
  2013.

\bibitem[Liang et~al.(2019)Liang, He, Sun, and Tan]{liang2019distant}
Liang, J., He, R., Sun, Z., and Tan, T.
\newblock Distant supervised centroid shift: A simple and efficient approach to
  visual domain adaptation.
\newblock In \emph{CVPR}, pp.\  2975--2984, 2019.

\bibitem[Liang et~al.(2020)Liang, Hu, and Feng]{liang2020we}
Liang, J., Hu, D., and Feng, J.
\newblock Do we really need to access the source data? source hypothesis
  transfer for unsupervised domain adaptation.
\newblock In \emph{ICML}, pp.\  6028--6039, 2020.

\bibitem[Liang(2016)]{liang2016cs229t}
Liang, P.
\newblock Statistical learning theory, 2016.
\newblock URL \url{https://web.stanford.edu/class/cs229t/notes.pdf}.

\bibitem[Littlestone(1988)]{littlestone1988learning}
Littlestone, N.
\newblock Learning quickly when irrelevant attributes abound: A new
  linear-threshold algorithm.
\newblock \emph{Machine learning}, 2\penalty0 (4):\penalty0 285--318, 1988.

\bibitem[Long et~al.(2015)Long, Cao, Wang, and Jordan]{long2015learning}
Long, M., Cao, Y., Wang, J., and Jordan, M.
\newblock Learning transferable features with deep adaptation networks.
\newblock In \emph{International conference on machine learning}, pp.\
  97--105. PMLR, 2015.

\bibitem[Malinin et~al.(2021)Malinin, Band, Gal, Gales, Ganshin, Chesnokov,
  Noskov, Ploskonosov, Prokhorenkova, Provilkov, Raina, Raina, Roginskiy,
  Shmatova, Tigas, and Yangel]{malinin2021shifts}
Malinin, A., Band, N., Gal, Y., Gales, M., Ganshin, A., Chesnokov, G., Noskov,
  A., Ploskonosov, A., Prokhorenkova, L., Provilkov, I., Raina, V., Raina, V.,
  Roginskiy, D., Shmatova, M., Tigas, P., and Yangel, B.
\newblock Shifts: A dataset of real distributional shift across multiple
  large-scale tasks.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qM45LHaWM6E}.

\bibitem[Peyr{\'e} et~al.(2019)Peyr{\'e}, Cuturi,
  et~al.]{peyre2019computational}
Peyr{\'e}, G., Cuturi, M., et~al.
\newblock Computational optimal transport: With applications to data science.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  11\penalty0 (5-6):\penalty0 355--607, 2019.

\bibitem[Pham et~al.(2021)Pham, Dai, Xie, and Le]{pham2021meta}
Pham, H., Dai, Z., Xie, Q., and Le, Q.~V.
\newblock Meta pseudo labels.
\newblock In \emph{CVPR}, pp.\  11557--11568, 2021.

\bibitem[Rakhlin \& Sridharan(2014)Rakhlin and Sridharan]{rakhlin2014notes}
Rakhlin, A. and Sridharan, K.
\newblock Statistical learning and sequential prediction, 2014.

\bibitem[Rakhlin et~al.(2010)Rakhlin, Sridharan, and Tewari]{rakhlin2010online}
Rakhlin, A., Sridharan, K., and Tewari, A.
\newblock Online learning: Random averages, combinatorial parameters, and
  learnability.
\newblock In Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and
  Culotta, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~23. Curran Associates, Inc., 2010.

\bibitem[Rakhlin et~al.(2015)Rakhlin, Sridharan, and Tewari]{rakhlin2015online}
Rakhlin, A., Sridharan, K., and Tewari, A.
\newblock Online learning via sequential complexities.
\newblock \emph{J. Mach. Learn. Res.}, 16\penalty0 (1):\penalty0 155--186,
  2015.

\bibitem[Redko et~al.(2019)Redko, Courty, Flamary, and Tuia]{redko2019optimal}
Redko, I., Courty, N., Flamary, R., and Tuia, D.
\newblock Optimal transport for multi-source domain adaptation under target
  shift.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  849--858. PMLR, 2019.

\bibitem[Sagawa et~al.(2021)Sagawa, Koh, Lee, Gao, Xie, Shen, Kumar, Hu,
  Yasunaga, Marklund, Beery, David, Stavness, Guo, Leskovec, Saenko, Hashimoto,
  Levine, Finn, and Liang]{sagawa2021extending}
Sagawa, S., Koh, P.~W., Lee, T., Gao, I., Xie, S.~M., Shen, K., Kumar, A., Hu,
  W., Yasunaga, M., Marklund, H., Beery, S., David, E., Stavness, I., Guo, W.,
  Leskovec, J., Saenko, K., Hashimoto, T., Levine, S., Finn, C., and Liang, P.
\newblock Extending the {WILDS} benchmark for unsupervised adaptation.
\newblock In \emph{NeurIPS 2021 Workshop on Distribution Shifts: Connecting
  Methods and Applications}, 2021.
\newblock URL \url{https://openreview.net/forum?id=2EhHKKXMbG0}.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel,
  Cubuk, Kurakin, and Li]{FixMatch}
Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.~A.,
  Cubuk, E.~D., Kurakin, A., and Li, C.-L.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  596--608. Curran Associates, Inc., 2020.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sun \& Saenko(2016)Sun and Saenko]{deepCORAL}
Sun, B. and Saenko, K.
\newblock Deep coral: Correlation alignment for deep domain adaptation, 2016.

\bibitem[Talagrand(1995)]{talagrand1995concentration}
Talagrand, M.
\newblock Concentration of measure and isoperimetric inequalities in product
  spaces.
\newblock \emph{Publications Math{\'e}matiques de l'Institut des Hautes Etudes
  Scientifiques}, 81\penalty0 (1):\penalty0 73--205, 1995.

\bibitem[Tolstikhin et~al.(2018)Tolstikhin, Bousquet, Gelly, and
  Schoelkopf]{WAE}
Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B.
\newblock Wasserstein auto-encoders.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HkL7n1-0b}.

\bibitem[Tzeng et~al.(2017)Tzeng, Hoffman, Saenko, and
  Darrell]{tzeng2017adversarial}
Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T.
\newblock Adversarial discriminative domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  7167--7176, 2017.

\bibitem[Van~Engelen \& Hoos(2020)Van~Engelen and Hoos]{van2020survey}
Van~Engelen, J.~E. and Hoos, H.~H.
\newblock A survey on semi-supervised learning.
\newblock \emph{Machine Learning}, 109\penalty0 (2):\penalty0 373--440, 2020.

\bibitem[Vapnik(1999)]{vapnik1999nature}
Vapnik, V.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 1999.

\bibitem[Villani(2009)]{villani2009optimal}
Villani, C.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer, 2009.

\bibitem[Wang et~al.(2020)Wang, He, and Katabi]{wang2020continuously}
Wang, H., He, H., and Katabi, D.
\newblock Continuously indexed domain adaptation.
\newblock In \emph{ICML}, volume 119 of \emph{Proceedings of Machine Learning
  Research}, pp.\  9898--9907. PMLR, 13--18 Jul 2020.

\bibitem[Wulfmeier et~al.(2018)Wulfmeier, Bewley, and
  Posner]{wulfmeier2018incremental}
Wulfmeier, M., Bewley, A., and Posner, I.
\newblock Incremental adversarial domain adaptation for continually changing
  environments.
\newblock In \emph{2018 IEEE International conference on robotics and
  automation (ICRA)}, pp.\  4489--4495. IEEE, 2018.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Xie, Q., Luong, M.-T., Hovy, E., and Le, Q.~V.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10687--10698, 2020.

\bibitem[Yarowsky(1995)]{yarowsky1995unsupervised}
Yarowsky, D.
\newblock Unsupervised word sense disambiguation rivaling supervised methods.
\newblock In \emph{33rd annual meeting of the association for computational
  linguistics}, pp.\  189--196, 1995.

\bibitem[Ye et~al.(2022)Ye, Jiang, Wang, Choudhary, Du, Bhushanam, Mokhtari,
  Kejariwal, and qiang liu]{ye2022future}
Ye, M., Jiang, R., Wang, H., Choudhary, D., Du, X., Bhushanam, B., Mokhtari,
  A., Kejariwal, A., and qiang liu.
\newblock Future gradient descent for adapting the temporal shifting data
  distribution in online recommendation system.
\newblock In \emph{The 38th Conference on Uncertainty in Artificial
  Intelligence}, 2022.

\bibitem[Zhao et~al.(2018)Zhao, Zhang, Wu, Moura, Costeira, and
  Gordon]{zhao2018adversarial}
Zhao, H., Zhang, S., Wu, G., Moura, J.~M., Costeira, J.~P., and Gordon, G.~J.
\newblock Adversarial multiple source domain adaptation.
\newblock \emph{Advances in neural information processing systems},
  31:\penalty0 8559--8570, 2018.

\bibitem[Zhao et~al.(2019)Zhao, Des~Combes, Zhang, and Gordon]{zhao2019domain}
Zhao, H., Des~Combes, R.~T., Zhang, K., and Gordon, G.
\newblock On learning invariant representations for domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7523--7532. PMLR, 2019.

\bibitem[Zhao et~al.(2021)Zhao, Qiao, Xiao, Glass, and Sun]{zhao2021pyhealth}
Zhao, Y., Qiao, Z., Xiao, C., Glass, L., and Sun, J.
\newblock Pyhealth: A python library for health predictive models.
\newblock \emph{arXiv preprint arXiv:2101.04209}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Zhao, Zhang, Wang, Chang, Wang, and
  Zhu]{zhou2022online}
Zhou, S., Zhao, H., Zhang, S., Wang, L., Chang, H., Wang, Z., and Zhu, W.
\newblock Online continual adaptation with active self-training.
\newblock In \emph{AISTATS}. PMLR, 2022.

\bibitem[Zou et~al.(2018)Zou, Yu, Kumar, and Wang]{zou2018unsupervised}
Zou, Y., Yu, Z., Kumar, B.~V., and Wang, J.
\newblock Unsupervised domain adaptation for semantic segmentation via
  class-balanced self-training.
\newblock In \emph{ECCV}, pp.\  289--305, 2018.

\bibitem[Zou et~al.(2019)Zou, Yu, Liu, Kumar, and Wang]{zou2019confidence}
Zou, Y., Yu, Z., Liu, X., Kumar, B.~V., and Wang, J.
\newblock Confidence regularized self-training.
\newblock In \emph{ICCV}, October 2019.

\end{thebibliography}
