\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[An(1996)]{an1996effects}
An, Guozhong.
\newblock The effects of adding noise during backpropagation training on a
  generalization performance.
\newblock \emph{Neural computation}, 8\penalty0 (3):\penalty0 643--674, 1996.

\bibitem[Bartlett et~al.(2005)Bartlett, Bousquet, Mendelson,
  et~al.]{bartlett2005local}
Bartlett, Peter~L, Bousquet, Olivier, Mendelson, Shahar, et~al.
\newblock Local rademacher complexities.
\newblock \emph{The Annals of Statistics}, 33\penalty0 (4):\penalty0
  1497--1537, 2005.

\bibitem[Bengio et~al.(2009)]{bengio2009}
Bengio, Yoshua et~al.
\newblock Learning deep architectures for ai.
\newblock \emph{Foundations and trends{\textregistered} in Machine Learning},
  2\penalty0 (1):\penalty0 1--127, 2009.

\bibitem[Bishop(1995)]{bishop1995training}
Bishop, Chris~M.
\newblock Training with noise is equivalent to tikhonov regularization.
\newblock \emph{Neural computation}, 7\penalty0 (1):\penalty0 108--116, 1995.

\bibitem[{Bojanowski} \& {Joulin}(2017){Bojanowski} and
  {Joulin}]{bojanowski2017}
{Bojanowski}, P. and {Joulin}, A.
\newblock {Unsupervised Learning by Predicting Noise}.
\newblock \emph{ArXiv e-prints}, April 2017.

\bibitem[Bottou(1998)]{bottou1998online}
Bottou, L{\'e}on.
\newblock Online learning and stochastic approximations.
\newblock \emph{On-line learning in neural networks}, 17\penalty0 (9):\penalty0
  142, 1998.

\bibitem[Chaudhari et~al.(2016)Chaudhari, Choromanska, Soatto, and
  LeCun]{chaudhari2016entropy}
Chaudhari, Pratik, Choromanska, Anna, Soatto, Stefano, and LeCun, Yann.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{arXiv preprint arXiv:1611.01838}, 2016.

\bibitem[Chollet et~al.(2015)]{keras}
Chollet, Fran\c{c}ois et~al.
\newblock Keras.
\newblock \url{https://github.com/fchollet/keras}, 2015.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
Cybenko, George.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals, and Systems (MCSS)},
  2\penalty0 (4):\penalty0 303--314, 1989.

\bibitem[Fix \& Hodges~Jr(1951)Fix and Hodges~Jr]{fix1951discriminatory}
Fix, Evelyn and Hodges~Jr, Joseph~L.
\newblock Discriminatory analysis-nonparametric discrimination: consistency
  properties.
\newblock Technical report, DTIC Document, 1951.

\bibitem[Gini(1913)]{gini}
Gini, Corrado.
\newblock Variabilita e mutabilita.
\newblock \emph{Journal of the Royal Statistical Society}, 76\penalty0 (3),
  1913.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{dl_book}
Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{goodfellow2013empirical}
Goodfellow, Ian~J, Mirza, Mehdi, Xiao, Da, Courville, Aaron, and Bengio,
  Yoshua.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6211}, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, Ian~J, Shlens, Jonathon, and Szegedy, Christian.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{hardt2015train}
Hardt, Moritz, Recht, Benjamin, and Singer, Yoram.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock \emph{arXiv preprint arXiv:1509.01240}, 2015.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Im et~al.(2016)Im, Tao, and Branson]{im2016empirical}
Im, Daniel~Jiwoong, Tao, Michael, and Branson, Kristin.
\newblock An empirical analysis of deep network loss surfaces.
\newblock \emph{arXiv preprint arXiv:1612.04010}, 2016.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, Nitish~Shirish, Mudigere, Dheevatsa, Nocedal, Jorge, Smelyanskiy,
  Mikhail, and Tang, Ping Tak~Peter.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, Pang~Wei and Liang, Percy.
\newblock Understanding black-box predictions via influence functions.
\newblock \emph{arXiv preprint arXiv:1703.04730}, 2017.

\bibitem[Krizhevsky et~al.()Krizhevsky, Nair, and Hinton]{CIFAR10}
Krizhevsky, Alex, Nair, Vinod, and Hinton, Geoffrey.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Kurakin, Alexey, Goodfellow, Ian, and Bengio, Samy.
\newblock Adversarial examples in the physical world.
\newblock \emph{arXiv preprint arXiv:1607.02533}, 2016.

\bibitem[LeCun et~al.(1998)LeCun, Cortes, and Burges]{lecun1998mnist}
LeCun, Yann, Cortes, Corinna, and Burges, Christopher~JC.
\newblock The mnist database of handwritten digits, 1998.

\bibitem[Lin \& Tegmark(2016)Lin and Tegmark]{Tegmark}
Lin, Henry~W and Tegmark, Max.
\newblock Why does deep and cheap learning work so well?
\newblock \emph{arXiv preprint arXiv:1608.08225}, 2016.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, Dougal, Duvenaud, David~K, and Adams, Ryan~P.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{ICML}, pp.\  2113--2122, 2015.

\bibitem[Miyato et~al.(2015)Miyato, Maeda, Koyama, Nakae, and
  Ishii]{miyato2015distributional}
Miyato, Takeru, Maeda, Shin-ichi, Koyama, Masanori, Nakae, Ken, and Ishii,
  Shin.
\newblock Distributional smoothing with virtual adversarial training.
\newblock \emph{stat}, 1050:\penalty0 25, 2015.

\bibitem[Montavon et~al.(2011)Montavon, Braun, and M\"{u}ller]{Montavon2011}
Montavon, Gr{\'e}goire, Braun, Mikio~L., and M\"{u}ller, Klaus-Robert.
\newblock Kernel analysis of deep networks.
\newblock \emph{Journal of Machine Learning Research}, 12, 2011.

\bibitem[Montufar et~al.(2014)Montufar, Pascanu, Cho, and Bengio]{montufar2014}
Montufar, Guido~F, Pascanu, Razvan, Cho, Kyunghyun, and Bengio, Yoshua.
\newblock On the number of linear regions of deep neural networks.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.~D., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 27}, pp.\  2924--2932. Curran Associates, Inc., 2014.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{ganguli2016}
Poole, Ben, Lahiri, Subhaneil, Raghu, Maithreyi, Sohl-Dickstein, Jascha, and
  Ganguli, Surya.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In Lee, D.~D., Sugiyama, M., Luxburg, U.~V., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  3360--3368. Curran Associates, Inc., 2016.

\bibitem[Raghu et~al.(2016)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{raghu2016expressive}
Raghu, Maithra, Poole, Ben, Kleinberg, Jon, Ganguli, Surya, and Sohl-Dickstein,
  Jascha.
\newblock On the expressive power of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1606.05336}, 2016.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, Andrew~M, McClelland, James~L, and Ganguli, Surya.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Sjoberg et~al.(1995)Sjoberg, Sjoeberg, Sjöberg, and
  Ljung]{CIS-125961}
Sjoberg, J., Sjoeberg, J., Sjöberg, J., and Ljung, L.
\newblock Overtraining, regularization and searching for a minimum, with
  application to neural networks.
\newblock \emph{International Journal of Control}, 62:\penalty0 1391--1407,
  1995.

\bibitem[Sokolic et~al.(2016)Sokolic, Giryes, Sapiro, and
  Rodrigues]{sokolic2016robust}
Sokolic, Jure, Giryes, Raja, Sapiro, Guillermo, and Rodrigues, Miguel~RD.
\newblock Robust large margin deep neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08254}, 2016.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{adversarial_examples}
Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan,
  Dumitru, Goodfellow, Ian~J., and Fergus, Rob.
\newblock Intriguing properties of neural networks.
\newblock \emph{CoRR}, abs/1312.6199, 2013.
\newblock URL \url{http://arxiv.org/abs/1312.6199}.

\bibitem[Theano Development~Team(2016)]{theano}
Theano Development~Team, {and others}.
\newblock {Theano: A {Python} framework for fast computation of mathematical
  expressions}.
\newblock \emph{arXiv e-prints}, abs/1605.02688, May 2016.

\bibitem[Vapnik \& Vapnik(1998)Vapnik and Vapnik]{vapnik1998statistical}
Vapnik, Vladimir~Naumovich and Vapnik, Vlamimir.
\newblock \emph{Statistical learning theory}, volume~1.
\newblock Wiley New York, 1998.

\bibitem[Wang()]{wang2016analysis}
Wang, Shengjie.
\newblock Analysis of deep neural networks with the extended data jacobian
  matrix.

\bibitem[Wilson \& Martinez(2003)Wilson and Martinez]{wilson2003general}
Wilson, D~Randall and Martinez, Tony~R.
\newblock The general inefficiency of batch training for gradient descent
  learning.
\newblock \emph{Neural Networks}, 16\penalty0 (10):\penalty0 1429--1451, 2003.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yao, Yuan, Rosasco, Lorenzo, and Caponnetto, Andrea.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{understanding_DL}
Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin, and Vinyals,
  Oriol.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\end{thebibliography}
