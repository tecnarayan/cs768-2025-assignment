\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Arjovsky, M., Shah, A., and Bengio, Y.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{ICML}, pp.\  1120--1128, 2016.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{ICLR}, 2015.

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai:2018}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock \emph{CoRR}, abs/1803.01271, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.01271}.

\bibitem[Bayer \& Osendorfer(2014)Bayer and Osendorfer]{bayer2014learning}
Bayer, J. and Osendorfer, C.
\newblock Learning stochastic recurrent networks.
\newblock \emph{arXiv preprint arXiv:1411.7610}, 2014.

\bibitem[Berkes \& Wiskott(2006)Berkes and Wiskott]{berkes2006analysis}
Berkes, P. and Wiskott, L.
\newblock On the analysis and interpretation of inhomogeneous quadratic forms
  as receptive fields.
\newblock \emph{Neural computation}, 18\penalty0 (8):\penalty0 1868--1895,
  2006.

\bibitem[Campos et~al.(2018)Campos, Jou, Gir{\'o}-i Nieto, Torres, and
  Chang]{campos2017skip}
Campos, V., Jou, B., Gir{\'o}-i Nieto, X., Torres, J., and Chang, S.-F.
\newblock Skip rnn: Learning to skip state updates in recurrent neural
  networks.
\newblock \emph{ICLR}, 2018.

\bibitem[Chang et~al.(2017)Chang, Zhang, Han, Yu, Guo, Tan, Cui, Witbrock,
  Hasegawa-Johnson, and Huang]{chang2017dilated}
Chang, S., Zhang, Y., Han, W., Yu, M., Guo, X., Tan, W., Cui, X., Witbrock, M.,
  Hasegawa-Johnson, M.~A., and Huang, T.~S.
\newblock Dilated recurrent neural networks.
\newblock In \emph{NIPS}, pp.\  77--87, 2017.

\bibitem[Chung et~al.(2014)Chung, G{\"{u}}l{\c c}ehre, Cho, and
  Bengio]{Chung:2014}
Chung, J., G{\"{u}}l{\c c}ehre, {\c C}., Cho, K., and Bengio, Y.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv e-prints}, abs/1412.3555, 2014.
\newblock Presented at the Deep Learning workshop at NIPS2014.

\bibitem[Cooijmans et~al.(2017)Cooijmans, Ballas, Laurent, G{\"u}l{\c{c}}ehre,
  and Courville]{cooijmans2016recurrent}
Cooijmans, T., Ballas, N., Laurent, C., G{\"u}l{\c{c}}ehre, {\c{C}}., and
  Courville, A.
\newblock Recurrent batch normalization.
\newblock \emph{ICLR}, 2017.

\bibitem[Dahl et~al.(2012)Dahl, Adams, and Larochelle]{dahl2012training}
Dahl, G.~E., Adams, R.~P., and Larochelle, H.
\newblock Training restricted boltzmann machines on word observations.
\newblock \emph{ICML}, 2012.

\bibitem[Dai \& Le(2015)Dai and Le]{dai2015semi}
Dai, A.~M. and Le, Q.~V.
\newblock Semi-supervised sequence learning.
\newblock In \emph{NIPS}, pp.\  3079--3087, 2015.

\bibitem[Danihelka et~al.(2016)Danihelka, Wayne, Uria, Kalchbrenner, and
  Graves]{danihelka2016associative}
Danihelka, I., Wayne, G., Uria, B., Kalchbrenner, N., and Graves, A.
\newblock Associative long short-term memory.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1986--1994, 2016.

\bibitem[Daniluk et~al.(2017{\natexlab{a}})Daniluk, Rockt{\"a}schel, Welbl, and
  Riedel]{Daniluk:2017}
Daniluk, M., Rockt{\"a}schel, T., Welbl, J., and Riedel, S.
\newblock Frustratingly short attention spans in neural language modeling.
\newblock 2017{\natexlab{a}}.

\bibitem[Daniluk et~al.(2017{\natexlab{b}})Daniluk, Rockt{\"a}schel, Welbl, and
  Riedel]{daniluk2017frustratingly}
Daniluk, M., Rockt{\"a}schel, T., Welbl, J., and Riedel, S.
\newblock Frustratingly short attention spans in neural language modeling.
\newblock \emph{ICLR}, 2017{\natexlab{b}}.

\bibitem[Dieng et~al.(2018)Dieng, Ranganath, Altosaar, and
  Blei]{dieng2018noisin}
Dieng, A.~B., Ranganath, R., Altosaar, J., and Blei, D.~M.
\newblock Noisin: Unbiased regularization for recurrent neural networks.
\newblock \emph{ICML}, 2018.

\bibitem[Elman(1990)]{elman1990finding}
Elman, J.~L.
\newblock Finding structure in time.
\newblock \emph{Cognitive science}, 14\penalty0 (2):\penalty0 179--211, 1990.

\bibitem[Foerster et~al.(2017)Foerster, Gilmer, Sohl-Dickstein, Chorowski, and
  Sussillo]{foerster2017input}
Foerster, J.~N., Gilmer, J., Sohl-Dickstein, J., Chorowski, J., and Sussillo,
  D.
\newblock Input switched affine networks: An rnn architecture designed for
  interpretability.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1136--1145. JMLR. org, 2017.

\bibitem[Fraccaro et~al.(2016)Fraccaro, S\o~nderby, Paquet, and
  Winther]{Fraccaro:2016}
Fraccaro, M., S\o~nderby, S. r.~K., Paquet, U., and Winther, O.
\newblock Sequential neural models with stochastic layers.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2199--2207. 2016.

\bibitem[Frasconi \& Bengio(1994)Frasconi and Bengio]{frasconi1994approach}
Frasconi, P. and Bengio, Y.
\newblock An em approach to grammatical inference: input/output hmms.
\newblock In \emph{Proceedings of 12th International Conference on Pattern
  Recognition}, pp.\  289--294. IEEE, 1994.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016theoretically}
Gal, Y. and Ghahramani, Z.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1019--1027, 2016.

\bibitem[Gers \& Schmidhuber(2001)Gers and Schmidhuber]{gers2001lstm}
Gers, F.~A. and Schmidhuber, E.
\newblock Lstm recurrent networks learn simple context-free and
  context-sensitive languages.
\newblock \emph{IEEE Transactions on Neural Networks}, 12\penalty0
  (6):\penalty0 1333--1340, 2001.

\bibitem[Gers \& Schmidhuber(2000)Gers and Schmidhuber]{Gers:2000}
Gers, F.~A. and Schmidhuber, J.
\newblock Recurrent nets that time and count.
\newblock In \emph{{IJCNN} {(3)}}, pp.\  189--194, 2000.

\bibitem[Giles et~al.(1991)Giles, Chen, Miller, Chen, Sun, and
  Lee]{giles1991second}
Giles, C.~L., Chen, D., Miller, C., Chen, H., Sun, G., and Lee, Y.
\newblock Second-order recurrent neural networks for grammatical inference.
\newblock In \emph{Neural Networks, 1991., IJCNN-91-Seattle International Joint
  Conference on}, volume~2, pp.\  273--281. IEEE, 1991.

\bibitem[Goyal et~al.(2017)Goyal, Sordoni, C\^{o}t\'{e}, Ke, and
  Bengio]{Goyal:2017}
Goyal, A., Sordoni, A., C\^{o}t\'{e}, M.-A., Ke, N., and Bengio, Y.
\newblock Z-forcing: Training stochastic recurrent networks.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  6713--6723. 2017.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Graves, A., Wayne, G., and Danihelka, I.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska{-}Barwinska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia,
  Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and
  Hassabis]{Graves:2016b}
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I.,
  Grabska{-}Barwinska, A., Colmenarejo, S.~G., Grefenstette, E., Ramalho, T.,
  Agapiou, J., Badia, A.~P., Hermann, K.~M., Zwols, Y., Ostrovski, G., Cain,
  A., King, H., Summerfield, C., Blunsom, P., Kavukcuoglu, K., and Hassabis, D.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 538\penalty0 (7626):\penalty0 471--476, 2016.

\bibitem[Grefenstette et~al.(2015)Grefenstette, Hermann, Suleyman, and
  Blunsom]{grefenstette2015learning}
Grefenstette, E., Hermann, K.~M., Suleyman, M., and Blunsom, P.
\newblock Learning to transduce with unbounded memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1828--1836, 2015.

\bibitem[Hao et~al.(2018)Hao, Merrill, Angluin, Frank, Amsel, Benz, and
  Mendelsohn]{hao:2018}
Hao, Y., Merrill, W., Angluin, D., Frank, R., Amsel, N., Benz, A., and
  Mendelsohn, S.
\newblock Context-free transductions with neural stacks.
\newblock In \emph{Proceedings of the Analyzing and Interpreting Neural
  Networks for NLP workshop at EMNLP}, 2018.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{Hochreiter:1997}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Comput.}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Jing et~al.(2017)Jing, Shen, Dubcek, Peurifoy, Skirlo, LeCun, Tegmark,
  and Solja{\v{c}}i{\'c}]{pmlr-v70-jing17a}
Jing, L., Shen, Y., Dubcek, T., Peurifoy, J., Skirlo, S., LeCun, Y., Tegmark,
  M., and Solja{\v{c}}i{\'c}, M.
\newblock Tunable efficient unitary neural networks ({EUNN}) and their
  application to {RNN}s.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1733--1741, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Johnson \& Zhang(2015)Johnson and Zhang]{johnson2014effective}
Johnson, R. and Zhang, T.
\newblock Effective use of word order for text categorization with
  convolutional neural networks.
\newblock \emph{NAACL HLT}, 2015.

\bibitem[Le et~al.(2015)Le, Jaitly, and Hinton]{le2015simple}
Le, Q.~V., Jaitly, N., and Hinton, G.~E.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv preprint arXiv:1504.00941}, 2015.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas-EtAl:2011:ACL-HLT2011}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{ACL-HLT}, pp.\  142--150, Portland, Oregon, USA, June 2011.
  Association for Computational Linguistics.

\bibitem[Mensch \& Blondel(2018)Mensch and Blondel]{pmlr-v80-mensch18a}
Mensch, A. and Blondel, M.
\newblock Differentiable dynamic programming for structured prediction and
  attention.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pp.\  3462--3471, 2018.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2017regularizing}
Merity, S., Keskar, N.~S., and Socher, R.
\newblock Regularizing and optimizing lstm language models.
\newblock \emph{ICLR}, 2018.

\bibitem[Miller \& Hardt(2018)Miller and Hardt]{miller:2018}
Miller, J. and Hardt, M.
\newblock When recurrent models don't need to be recurrent.
\newblock \emph{CoRR}, abs/1805.10369, 2018.

\bibitem[Murdoch \& Szlam(2017)Murdoch and Szlam]{murdoch2017automatic}
Murdoch, W.~J. and Szlam, A.
\newblock Automatic rule extraction from long short term memory networks.
\newblock In \emph{ICLR}, 2017.

\bibitem[Nguyen et~al.(2016)Nguyen, Dosovitskiy, Yosinski, Brox, and
  Clune]{nguyen2016synthesizing}
Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and Clune, J.
\newblock Synthesizing the preferred inputs for neurons in neural networks via
  deep generator networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3387--3395, 2016.

\bibitem[Ostmeyer \& Cowell(2017)Ostmeyer and Cowell]{ostmeyer2017machine}
Ostmeyer, J. and Cowell, L.
\newblock Machine learning on sequential data using a recurrent weighted
  average.
\newblock \emph{arXiv preprint arXiv:1703.01253}, 2017.

\bibitem[Schellhammer et~al.(1998)Schellhammer, Diederich, Towsey, and
  Brugman]{Schellhammer:1998}
Schellhammer, I., Diederich, J., Towsey, M., and Brugman, C.
\newblock Knowledge extraction and recurrent neural networks: An analysis of an
  elman network trained on a natural language learning task.
\newblock In \emph{Proceedings of the Joint Conferences on New Methods in
  Language Processing and Computational Natural Language Learning}, pp.\
  73--78, 1998.

\bibitem[Schmidhuber et~al.(2002)Schmidhuber, Gers, and
  Eck]{schmidhuber2002learning}
Schmidhuber, J., Gers, F., and Eck, D.
\newblock Learning nonregular languages: A comparison of simple recurrent
  networks and lstm.
\newblock \emph{Neural Computation}, 14\penalty0 (9):\penalty0 2039--2041,
  2002.

\bibitem[Siegelmann(2012)]{siegelmann2012neural}
Siegelmann, H.~T.
\newblock \emph{Neural networks and analog computation: beyond the Turing
  limit}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Siegelmann \& Sontag(1992)Siegelmann and
  Sontag]{siegelmann1992computational}
Siegelmann, H.~T. and Sontag, E.~D.
\newblock On the computational power of neural nets.
\newblock In \emph{Proceedings of the fifth annual workshop on Computational
  learning theory}, pp.\  440--449. ACM, 1992.

\bibitem[Siegelmann \& Sontag(1994)Siegelmann and Sontag]{siegelmann1994analog}
Siegelmann, H.~T. and Sontag, E.~D.
\newblock Analog computation via neural networks.
\newblock \emph{Theoretical Computer Science}, 131\penalty0 (2):\penalty0
  331--360, 1994.

\bibitem[Soltani \& Jiang(2016)Soltani and Jiang]{soltani2016higher}
Soltani, R. and Jiang, H.
\newblock Higher order recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1605.00064}, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Strobelt et~al.(2016)Strobelt, Gehrmann, Huber, Pfister, Rush,
  et~al.]{strobelt2016visual}
Strobelt, H., Gehrmann, S., Huber, B., Pfister, H., Rush, A.~M., et~al.
\newblock Visual analysis of hidden state dynamics in recurrent neural
  networks.
\newblock \emph{CoRR, abs/1606.07461}, 2016.

\bibitem[{Theano Development Team}(2016)]{theano}
{Theano Development Team}.
\newblock {Theano: A {Python} framework for fast computation of mathematical
  expressions}.
\newblock \emph{arXiv e-prints}, abs/1605.02688, 2016.

\bibitem[Tomita(1982)]{tomita:cogsci82}
Tomita, M.
\newblock Dynamic construction of finite automata from examples using
  hill-climbing.
\newblock In \emph{{P}roceedings of the Fourth Annual Conference of the
  Cognitive Science Society}, pp.\  105--108, 1982.

\bibitem[Tran et~al.(2016)Tran, Bisazza, and Monz]{tran2016recurrent}
Tran, K., Bisazza, A., and Monz, C.
\newblock Recurrent memory networks for language modeling.
\newblock \emph{NAACL-HLT}, 2016.

\bibitem[Trinh et~al.(2018)Trinh, Dai, Luong, and Le]{trinh2018learning}
Trinh, T.~H., Dai, A.~M., Luong, T., and Le, Q.~V.
\newblock Learning longer-term dependencies in rnns with auxiliary losses.
\newblock \emph{ICML}, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{NIPS2017_7181}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, pp.\  5998--6008. 2017.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Zhang, Ororbia, Alexander, Xing,
  Liu, and Giles]{wang2018comparison}
Wang, Q., Zhang, K., Ororbia, I., Alexander, G., Xing, X., Liu, X., and Giles,
  C.~L.
\newblock A comparison of rule extraction for different recurrent neural
  network models and grammatical complexity.
\newblock \emph{arXiv preprint arXiv:1801.05420}, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Zhang, Ororbia~II, Xing, Liu, and
  Giles]{Wang:2007:nc}
Wang, Q., Zhang, K., Ororbia~II, A.~G., Xing, X., Liu, X., and Giles, C.~L.
\newblock An empirical evaluation of rule extraction from recurrent neural
  networks.
\newblock \emph{Neural Computation}, 30\penalty0 (9):\penalty0 2568--2591,
  2018{\natexlab{b}}.

\bibitem[Weiss et~al.(2018{\natexlab{a}})Weiss, Goldberg, and
  Yahav]{Weiss:2018-power}
Weiss, G., Goldberg, Y., and Yahav, E.
\newblock On the practical computational power of finite precision rnns for
  language recognition.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pp.\  740--745,
  2018{\natexlab{a}}.

\bibitem[Weiss et~al.(2018{\natexlab{b}})Weiss, Goldberg, and
  Yahav]{pmlr-v80-weiss18a}
Weiss, G., Goldberg, Y., and Yahav, E.
\newblock Extracting automata from recurrent neural networks using queries and
  counterexamples.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80, pp.\  5247--5256, 2018{\natexlab{b}}.

\bibitem[Weston et~al.(2015)Weston, Chopra, and Bordes]{WestonCB14}
Weston, J., Chopra, S., and Bordes, A.
\newblock Memory networks.
\newblock 2015.

\bibitem[Wisdom et~al.(2016)Wisdom, Powers, Hershey, Le~Roux, and
  Atlas]{NIPS2016_6327}
Wisdom, S., Powers, T., Hershey, J., Le~Roux, J., and Atlas, L.
\newblock Full-capacity unitary recurrent neural networks.
\newblock In Lee, D.~D., Sugiyama, M., Luxburg, U.~V., Guyon, I., and Garnett,
  R. (eds.), \emph{NIPS}, pp.\  4880--4888. 2016.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yu et~al.(2017)Yu, Lee, and Le]{yu2017learning}
Yu, A.~W., Lee, H., and Le, Q.~V.
\newblock Learning to skim text.
\newblock \emph{ACL}, 2017.

\bibitem[Zaremba \& Sutskever(2014)Zaremba and Sutskever]{Zaremba:2014}
Zaremba, W. and Sutskever, I.
\newblock Learning to execute.
\newblock \emph{CoRR}, abs/1410.4615, 2014.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and
  Vinyals]{zaremba2014recurrent}
Zaremba, W., Sutskever, I., and Vinyals, O.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\bibitem[Zeng et~al.(1993)Zeng, Goodman, and Smyth]{zeng1993learning}
Zeng, Z., Goodman, R.~M., and Smyth, P.
\newblock Learning finite state machines with self-clustering recurrent
  networks.
\newblock \emph{Neural Computation}, 5\penalty0 (6):\penalty0 976--990, 1993.

\bibitem[Zhang et~al.(2016)Zhang, Wu, Che, Lin, Memisevic, Salakhutdinov, and
  Bengio]{zhang2016architectural}
Zhang, S., Wu, Y., Che, T., Lin, Z., Memisevic, R., Salakhutdinov, R.~R., and
  Bengio, Y.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1822--1830, 2016.

\bibitem[Zilly et~al.(2017)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber]{zilly2017recurrent}
Zilly, J.~G., Srivastava, R.~K., Koutn{\'\i}k, J., and Schmidhuber, J.
\newblock Recurrent highway networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  4189--4198. JMLR. org, 2017.

\end{thebibliography}
