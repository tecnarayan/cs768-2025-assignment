\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{arora_deeplinearconv}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock \emph{CoRR}, abs/1810.02281, 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1810.02281}.

\bibitem[Arora et~al.(2018{\natexlab{b}})Arora, Cohen, and
  Hazan]{arora_linear_alignment}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock \emph{arXiv preprint arXiv:1802.06509}, 2018{\natexlab{b}}.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora_implicit_deepmf}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{arXiv preprint arXiv:1905.13655}, 2019.

\bibitem[Bach(2017)]{bach2017breaking}
Bach, F.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Bhojanapalli et~al.(2016)Bhojanapalli, Neyshabur, and
  Srebro]{srebro_lowrankrecovery}
Bhojanapalli, S., Neyshabur, B., and Srebro, N.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3873--3881, 2016.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd_convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brutzkus et~al.(2017)Brutzkus, Globerson, Malach, and
  Shalev{-}Shwartz]{brutzkus_overparameterized_linear}
Brutzkus, A., Globerson, A., Malach, E., and Shalev{-}Shwartz, S.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{CoRR}, abs/1710.10174, 2017.
\newblock URL \url{http://arxiv.org/abs/1710.10174}.

\bibitem[Chen et~al.(2018)Chen, Liu, Kira, Wang, and Huang]{fewshot}
Chen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C.~F., and Huang, J.-B.
\newblock A closer look at few-shot classification.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Du \& Hu(2019)Du and Hu]{du_deeplinear}
Du, S. and Hu, W.
\newblock Width provably matters in optimization for deep linear neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1655--1664, 2019.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Hu, and Lee]{du_relu_alignment}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  384--395, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Zhai, P{\'{o}}czos, and
  Singh]{du_overparameterized}
Du, S.~S., Zhai, X., P{\'{o}}czos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{CoRR}, abs/1810.02054, 2018{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1810.02054}.

\bibitem[Ergen \& Pilanci(2019)Ergen and Pilanci]{ergen2019cutting}
Ergen, T. and Pilanci, M.
\newblock Convex duality and cutting plane methods for over-parameterized
  neural networks.
\newblock In \emph{OPT-ML workshop}, 2019.

\bibitem[{Ergen} \& {Pilanci}(2019){Ergen} and {Pilanci}]{ergen2019shallow}
{Ergen}, T. and {Pilanci}, M.
\newblock Convex optimization for shallow neural networks.
\newblock In \emph{2019 57th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pp.\  79--83, 2019.

\bibitem[Ergen \& Pilanci(2020{\natexlab{a}})Ergen and
  Pilanci]{ergen2020aistats}
Ergen, T. and Pilanci, M.
\newblock Convex geometry of two-layer relu networks: Implicit autoencoding and
  interpretable models.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  4024--4033, Online, 26--28 Aug 2020{\natexlab{a}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v108/ergen20a.html}.

\bibitem[Ergen \& Pilanci(2020{\natexlab{b}})Ergen and
  Pilanci]{ergen2020journal}
Ergen, T. and Pilanci, M.
\newblock Convex geometry and duality of over-parameterized neural networks.
\newblock \emph{arXiv preprint arXiv:2002.11219}, 2020{\natexlab{b}}.

\bibitem[Ergen \& Pilanci(2020{\natexlab{c}})Ergen and
  Pilanci]{ergen2020workshop}
Ergen, T. and Pilanci, M.
\newblock Convex programs for global optimization of convolutional neural
  networks in polynomial-time.
\newblock In \emph{OPT-ML workshop}, 2020{\natexlab{c}}.

\bibitem[Ergen \& Pilanci(2021)Ergen and Pilanci]{ergen2020cnn}
Ergen, T. and Pilanci, M.
\newblock Implicit convex regularizers of cnn architectures: Convex
  optimization of two- and three-layer networks in polynomial time.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=0N8jUH4JMv6}.

\bibitem[Ergen et~al.(2021)Ergen, Sahiner, Ozturkler, Pauly, Mardani, and
  Pilanci]{ergen2021bn}
Ergen, T., Sahiner, A., Ozturkler, B., Pauly, J.~M., Mardani, M., and Pilanci,
  M.
\newblock Demystifying batch normalization in relu networks: Equivalent convex
  optimization models and implicit regularization.
\newblock \emph{CoRR}, abs/2103.01499, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.01499}.

\bibitem[Goberna \& L\'{o}pez-Cerd\'{a}(1998)Goberna and
  L\'{o}pez-Cerd\'{a}]{semiinfinite_goberna}
Goberna, M.~A. and L\'{o}pez-Cerd\'{a}, M.
\newblock \emph{Linear semi-infinite optimization}.
\newblock 01 1998.
\newblock \doi{10.1007/978-1-4899-8044-1_3}.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar_implicit_mf}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6151--6159, 2017.

\bibitem[Gupta et~al.(2021)Gupta, Bartan, Ergen, and
  Pilanci]{vikul2021generative}
Gupta, V., Bartan, B., Ergen, T., and Pilanci, M.
\newblock Convex neural autoregressive models: Towards tractable, expressive,
  and theoretically-backed models for sequential forecasting and generation.
\newblock In \emph{ICASSP 2021 - 2021 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  3890--3894, 2021.
\newblock \doi{10.1109/ICASSP39728.2021.9413662}.

\bibitem[Huang et~al.(2018)Huang, Yang, Lang, and Deng]{whitening}
Huang, L., Yang, D., Lang, B., and Deng, J.
\newblock Decorrelated batch normalization.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  791--800, 2018.

\bibitem[Hyun et~al.(2020)Hyun, Kim, Cho, Choi, and Seo]{highdim_medical}
Hyun, C.~M., Kim, K.~C., Cho, H.~C., Choi, J.~K., and Seo, J.~K.
\newblock Framelet pooling aided deep learning network: the method to process
  high dimensional medical data.
\newblock \emph{Machine Learning: Science and Technology}, 1\penalty0
  (1):\penalty0 015009, 2020.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{batch_norm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In Bach, F. and Blei, D. (eds.), \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pp.\  448--456, Lille, France, 07--09 Jul
  2015. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v37/ioffe15.html}.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{telgrasky_deeplinear}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJflg30qKX}.

\bibitem[Krizhevsky et~al.(2014)Krizhevsky, Nair, and Hinton]{cifar10}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock The {CIFAR}-10 dataset.
\newblock \url{http://www. cs. toronto. edu/kriz/cifar. html}, 2014.

\bibitem[Laurent \& Brecht(2018)Laurent and Brecht]{recht_deeplinear}
Laurent, T. and Brecht, J.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2902--2907, 2018.

\bibitem[LeCun()]{mnist}
LeCun, Y.
\newblock The {MNIST} database of handwritten digits.
\newblock \url{http://yann. lecun. com/exdb/mnist/}.

\bibitem[Li \& Liang(2018)Li and Liang]{li_overparameterized}
Li, Y. and Liang, Y.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock \emph{CoRR}, abs/1808.01204, 2018.
\newblock URL \url{http://arxiv.org/abs/1808.01204}.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and Srebro]{neyshabur_reg}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020neuralcollapse}
Papyan, V., Han, X., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Parhi \& Nowak(2019)Parhi and Nowak]{parhi_minimum}
Parhi, R. and Nowak, R.~D.
\newblock Minimum ``norm'' neural networks are splines.
\newblock \emph{arXiv preprint arXiv:1910.02333}, 2019.

\bibitem[Pilanci \& Ergen(2020)Pilanci and Ergen]{pilanci2020convex}
Pilanci, M. and Ergen, T.
\newblock Neural networks are convex regularizers: Exact polynomial-time convex
  optimization formulations for two-layer networks.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  7695--7705. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/pilanci20a.html}.

\bibitem[Rosset et~al.(2007)Rosset, Swirszcz, Srebro, and Zhu]{rosset2007}
Rosset, S., Swirszcz, G., Srebro, N., and Zhu, J.
\newblock L1 regularization in infinite dimensional feature spaces.
\newblock In \emph{International Conference on Computational Learning Theory},
  pp.\  544--558. Springer, 2007.

\bibitem[Sahiner et~al.(2021)Sahiner, Ergen, Pauly, and
  Pilanci]{sahiner2021vectoroutput}
Sahiner, A., Ergen, T., Pauly, J.~M., and Pilanci, M.
\newblock Vector-output relu neural network problems are copositive programs:
  Convex analysis of two layer networks and polynomial-time algorithms.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=fGF8qAqpXXG}.

\bibitem[Savarese et~al.(2019)Savarese, Evron, Soudry, and
  Srebro]{infinite_width}
Savarese, P., Evron, I., Soudry, D., and Srebro, N.
\newblock How do infinite width bounded norm networks look in function space?
\newblock \emph{CoRR}, abs/1902.05040, 2019.
\newblock URL \url{http://arxiv.org/abs/1902.05040}.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe_deeplinear}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Shamir(2018)]{shamir_convergence_deeplinear}
Shamir, O.
\newblock Exponential convergence time of gradient descent for one-dimensional
  deep linear neural networks.
\newblock \emph{arXiv preprint arXiv:1809.08587}, 2018.

\bibitem[Singh \& Yamada(2020)Singh and Yamada]{highdim_bio}
Singh, D. and Yamada, M.
\newblock Fsnet: Feature selection network on high-dimensional biological data.
\newblock \emph{arXiv preprint arXiv:2001.08322}, 2020.

\bibitem[Wei et~al.(2018)Wei, Lee, Liu, and Ma]{margin_theory_tengyu}
Wei, C., Lee, J.~D., Liu, Q., and Ma, T.
\newblock On the margin theory of feedforward neural networks.
\newblock \emph{arXiv preprint arXiv:1810.05369}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Shao, and
  Salakhutdinov]{zhang2019multibranch}
Zhang, H., Shao, J., and Salakhutdinov, R.
\newblock Deep neural networks with multi-branch architectures are
  intrinsically less non-convex.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1099--1109, 2019.

\end{thebibliography}
