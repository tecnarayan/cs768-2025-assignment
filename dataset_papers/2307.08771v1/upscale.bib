% Encoding: UTF-8
@string {accv = {Proceedings of the Asian Conference on Computer Vision (ACCV)}}
@string {bmvc = {Proceedings of the British Machine Vision Conference (BMVC)}}
@string {compimg = {IEEE Transactions on Computational Imaging (TCI)}}
@string {cviu = {Journal of Computer Vision and Image Understanding (CVIU)}}
@string {cvpr = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}}
@string {eccv = {Proceedings of the European Conference on Computer Vision (ECCV)}}
@string {icassp = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}}
@string {iccp = {IEEE International Conference in Computational Photography (ICCP)}}
@string {iccv = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}}
@string {icip = {IEEE International Conference on Image Processing (ICIP)}}
@string {iclr = {International Conference on Learning Representations (ICLR)}}
@string {icml = {International Conference on Machine Learning (ICML)}}
@string {ijcv = {International Journal of Computer Vision (IJCV)}}
@string {jmiv = {Journal of Mathematical Imaging and Vision}}
@string {threedv = {International Conference on 3D Vision (3DV)}}
@string {wacv = {IEEE Winter Conference on Applications of Computer Vision (WACV)}}
% Less common:
@string {jvcir = {Journal of Visual Communication and Image Representation}}
@string {icra  = {IEEE International Conference on Robotics and Automation (ICRA)}}
%!TEX Root = main_submission.tex
@string {ijcai = {International Joint Conference on Artificial Intelligence (IJCAI)}}

@string {neurips = {Advances in Neural Information Processing Systems (NeurIPS)}}
@string {pami = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}}
@string {siggraph = {ACM Transactions on Graphics (SIGGRAPH)}}
@string {siggraphasia = {ACM Transactions on Graphics (SIGGRAPH ASIA)}}
@string {tip = {IEEE Transactions on Image Processing (TIP)}}
@string {tog = {ACM Transactions on Graphics}}
@string {tra = {IEEE Transactions on Robotics and Automation (TRA)}}
@string {corl = {Conference on Robot Learning (CoRL)}}
@string {iros = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}}




@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle=iccv,
  pages={1389--1397},
  year={2017}
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle=iccv,
  pages={2736--2744},
  year={2017}
}

@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle=eccv,
  pages={784--800},
  year={2018}
}

@article{you2019gate,
  title={Gate decorator: Global filter pruning method for accelerating deep convolutional neural networks},
  author={You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping},
  journal=neurips,
  volume={32},
  year={2019}
}

@article{gao2018dynamic,
  title={Dynamic channel pruning: Feature boosting and suppression},
  author={Gao, Xitong and Zhao, Yiren and Dudziak, {\L}ukasz and Mullins, Robert and Xu, Cheng-zhong},
  journal=iclr,
  year={2019}
}

@article{liu2021channel,
  title={Channel pruning guided by spatial and channel attention for DNNs in intelligent edge computing},
  author={Liu, Mengran and Fang, Weiwei and Ma, Xiaodong and Xu, Wenyuan and Xiong, Naixue and Ding, Yi},
  journal={Applied Soft Computing},
  volume={110},
  pages={107636},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{han2016deep,
  author    = {Song Han and
               Huizi Mao and
               William J. Dally},
  title     = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
               Quantization and Huffman Coding},
  booktitle = iclr,
  year      = {2016},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle=icml,
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@article{anwar2017structured,
  title={Structured pruning of deep convolutional neural networks},
  author={Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
  journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  volume={13},
  number={3},
  pages={1--18},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{gupta2015deep,
  title={Deep learning with limited numerical precision},
  author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  booktitle=icml,
  pages={1737--1746},
  year={2015},
  organization={PMLR}
}

@article{courbariaux2016binarized,
  title={Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1},
  author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal=neurips,
  year={2016}
}

@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal=bmvc,
  year={2014}
}

@inproceedings{yu2017compressing,
  title={On compressing deep models by low rank and sparse decomposition},
  author={Yu, Xiyu and Liu, Tongliang and Wang, Xinchao and Tao, Dacheng},
  booktitle=cvpr,
  pages={7370--7379},
  year={2017}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal=ijcv,
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{huang2018learning,
  title={Learning to prune filters in convolutional neural networks},
  author={Huang, Qiangui and Zhou, Kevin and You, Suya and Neumann, Ulrich},
  booktitle=wacv,
  pages={709--718},
  year={2018},
  organization={IEEE}
}

@article{yamamoto2018pcas,
  title={Pcas: Pruning channels with attention statistics for deep network compression},
  author={Yamamoto, Kohei and Maeno, Kurato},
  journal=bmvc,
  year={2019}
}

@article{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  journal=neurips,
  volume={29},
  year={2016}
}

@article{savarese2020winning,
  title={Winning the lottery with continuous sparsification},
  author={Savarese, Pedro and Silva, Hugo and Maire, Michael},
  journal=neurips,
  volume={33},
  pages={11380--11390},
  year={2020}
}

@article{courbariaux2015binaryconnect,
  title={Binaryconnect: Training deep neural networks with binary weights during propagations},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  journal=neurips,
  volume={28},
  year={2015}
}

@article{louizos2017learning,
  title={Learning sparse neural networks through $ L\_0 $ regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal=iclr,
  year={2018}
}

@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={138--145},
  year={2017}
}

@article{xiao2019autoprune,
  title={Autoprune: Automatic network pruning by regularizing auxiliary parameters},
  author={Xiao, Xia and Wang, Zigeng and Rajasekaran, Sanguthevar},
  journal=neurips,
  volume={32},
  year={2019}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@article{l1,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal=iclr,
  year={2017}
}

@inproceedings{l2,
  title={Learning structured sparsity in deep learning},
  author={Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle=neurips,
  year={2016}
}

@inproceedings{
lamp,
title={Layer-adaptive Sparsity for the Magnitude-based Pruning},
author={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},
booktitle=iclr,
year={2021},
url={https://openreview.net/forum?id=H6ATjJ0TKdf}
}

@inproceedings{fpgm,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle=cvpr,
  pages={4340--4349},
  year={2019}
}

@inproceedings{hrank,
  title={Hrank: Filter pruning using high-rank feature map},
  author={Lin, Mingbao and Ji, Rongrong and Wang, Yan and Zhang, Yichen and Zhang, Baochang and Tian, Yonghong and Shao, Ling},
  booktitle=cvpr,
  pages={1529--1538},
  year={2020}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle=cvpr,
  pages={770--778},
  year={2016}
}

@inproceedings{densenet,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle=cvpr,
  pages={4700--4708},
  year={2017}
}

@inproceedings{efficientnetv2,
  title={Efficientnetv2: Smaller models and faster training},
  author={Tan, Mingxing and Le, Quoc},
  booktitle=icml,
  pages={10096--10106},
  year={2021},
}

@inproceedings{mobilenetv3,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle=iccv,
  pages={1314--1324},
  year={2019}
}

@inproceedings{mnasnet,
  title={Mnasnet: Platform-aware neural architecture search for mobile},
  author={Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V},
  booktitle=cvpr,
  pages={2820--2828},
  year={2019}
}

@article{squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@InProceedings{He_2017_ICCV,
author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
title = {Channel Pruning for Accelerating Very Deep Neural Networks},
booktitle = iccv,
month = {Oct},
year = {2017}
}

@article{sze2017efficient,
  title={Efficient processing of deep neural networks: A tutorial and survey},
  author={Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S},
  journal={Proceedings of the IEEE},
  volume={105},
  number={12},
  pages={2295--2329},
  year={2017},
  publisher={Ieee}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal=iclr,
  year={2017}
}

@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}

@article{narang2017block,
  title={Block-sparse recurrent neural networks},
  author={Narang, Sharan and Undersander, Eric and Diamos, Gregory},
  journal={arXiv preprint arXiv:1711.02782},
  year={2017}
}

@article{janowsky1989pruning,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, Steven A},
  journal={Physical Review A},
  volume={39},
  number={12},
  pages={6600},
  year={1989},
  publisher={APS}
}

@article{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  journal=neurips,
  volume={28},
  year={2015}
}

@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal=iclr,
  year={2019}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal=iclr,
  year={2019}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@article{reed1993pruning,
  title={Pruning algorithms-a survey},
  author={Reed, Russell},
  journal={IEEE transactions on Neural Networks},
  volume={4},
  number={5},
  pages={740--747},
  year={1993},
  publisher={IEEE}
}

@article{blalock2020state,
  title={What is the state of neural network pruning?},
  author={Blalock, Davis and Gonzalez Ortiz, Jose Javier and Frankle, Jonathan and Guttag, John},
  journal={Proceedings of machine learning and systems},
  volume={2},
  pages={129--146},
  year={2020}
}

@inproceedings{yu2022topology,
  title={Topology-aware network pruning using multi-stage graph embedding and reinforcement learning},
  author={Yu, Sixing and Mazaheri, Arya and Jannesari, Ali},
  booktitle=icml,
  pages={25656--25667},
  year={2022},
  organization={PMLR}
}

@InProceedings{Li_2022_CVPR,
    author    = {Li, Yawei and Adamczewski, Kamil and Li, Wen and Gu, Shuhang and Timofte, Radu and Van Gool, Luc},
    title     = {Revisiting Random Channel Pruning for Neural Network Compression},
    booktitle = cvpr,
    month     = {June},
    year      = {2022},
    pages     = {191-201}
}

@inproceedings{miao2021learning,
  title={Learning pruning-friendly networks via frank-wolfe: One-shot, any-sparsity, and no retraining},
  author={Miao, Lu and Luo, Xiaolong and Chen, Tianlong and Chen, Wuyang and Liu, Dong and Wang, Zhangyang},
  booktitle=iclr,
  year={2021}
}

@article{bulucc2012parallel,
  title={Parallel sparse matrix-matrix multiplication and indexing: Implementation and experiments},
  author={Bulu{\c{c}}, Aydin and Gilbert, John R},
  journal={SIAM Journal on Scientific Computing},
  volume={34},
  number={4},
  pages={C170--C191},
  year={2012},
  publisher={SIAM}
}

@inproceedings{azad2017work,
  title={A work-efficient parallel sparse matrix-sparse vector multiplication algorithm},
  author={Azad, Ariful and Bulu{\c{c}}, Aydin},
  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={688--697},
  year={2017},
  organization={IEEE}
}

@article{baskaran2009optimizing,
  title={Optimizing sparse matrix-vector multiplication on GPUs},
  author={Baskaran, Muthu Manikandan and Bordawekar, Rajesh},
  journal={IBM research report RC24704},
  volume={1},
  number={W0812--047},
  year={2009},
  publisher={IBM}
}

@inproceedings{vuduc2005oski,
  title={OSKI: A library of automatically tuned sparse matrix kernels},
  author={Vuduc, Richard and Demmel, James W and Yelick, Katherine A},
  booktitle={Journal of Physics: Conference Series},
  volume={16},
  pages={521},
  year={2005},
  organization={IOP Publishing}
}

@inproceedings{frantar2022spdy,
  title={SPDY: Accurate pruning with speedup guarantees},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle=icml,
  pages={6726--6743},
  year={2022},
  organization={PMLR}
}

@inproceedings{he2022sparse,
  title={Sparse Double Descent: Where Network Pruning Aggravates Overfitting},
  author={He, Zheng and Xie, Zeke and Zhu, Quanzhi and Qin, Zengchang},
  booktitle=icml,
  pages={8635--8659},
  year={2022},
  organization={PMLR}
}

@inproceedings{yu2022combinatorial,
  title={The combinatorial brain surgeon: Pruning weights that cancel one another in neural networks},
  author={Yu, Xin and Serra, Thiago and Ramalingam, Srikumar and Zhe, Shandian},
  booktitle=icml,
  pages={25668--25683},
  year={2022},
  organization={PMLR}
}

@article{huang2021training,
  title={Training Structured Neural Networks Through Manifold Identification and Variance Reduction},
  author={Huang, Zih-Syuan and Lee, Ching-pei},
  journal={arXiv preprint arXiv:2112.02612},
  year={2021}
}

@inproceedings{wang2022recent,
  title={Recent advances on neural network pruning at initialization},
  author={Wang, Huan and Qin, Can and Bai, Yue and Zhang, Yulun and Fu, Yun},
  booktitle=ijcai,
  pages={23--29},
  year={2022}
}

@article{li2022pruning,
  title={Pruning-as-search: Efficient neural architecture search via channel pruning and structural reparameterization},
  author={Li, Yanyu and Zhao, Pu and Yuan, Geng and Lin, Xue and Wang, Yanzhi and Chen, Xin},
  journal=ijcai,
  year={2022}
}

@article{shang2022neural,
  title={Neural network pruning by cooperative coevolution},
  author={Shang, Haopu and Wu, Jia-Liang and Hong, Wenjing and Qian, Chao},
  journal={arXiv preprint arXiv:2204.05639},
  year={2022}
}

@inproceedings{
liu2018rethinking,
title={Rethinking the Value of Network Pruning},
author={Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJlnB3C5Ym},
}