\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adversa(2023)]{adversa2023universal}
Adversa.
\newblock Universal {LLM} jailbreak: {ChatGPT, GPT-4, Bard, Bing, Anthropic},
  and beyond.
\newblock Adversa Blog, 2023.
\newblock URL
  \url{https://adversa.ai/blog/universal-llm-jailbreak-chatgpt-gpt-4-bard-bing-anthropic-and-beyond/}.

\bibitem[Albert(2023{\natexlab{a}})]{albert2023jailbreak}
Alex Albert.
\newblock {Jailbreak Chat}.
\newblock \url{https://www.jailbreakchat.com/}, 2023{\natexlab{a}}.

\bibitem[Albert(2023{\natexlab{b}})]{albert2023jailbreak2}
Alex Albert.
\newblock {Jailbreak Chat}.
\newblock
  \url{https://web.archive.org/web/20230413032954/https://www.jailbreakchat.com/},
  2023{\natexlab{b}}.

\bibitem[Anthropic(2023{\natexlab{a}})]{anthropic2023api}
Anthropic.
\newblock Anthropic {API} reference.
\newblock \url{https://console.anthropic.com/docs/api/reference},
  2023{\natexlab{a}}.

\bibitem[Anthropic(2023{\natexlab{b}})]{anthropic2023claude}
Anthropic.
\newblock ``{We are offering a new version of our model, Claude-v1.3, that is
  safer and less susceptible to adversarial attacks.}''.
\newblock \url{https://twitter.com/AnthropicAI/status/1648353600350060545},
  2023{\natexlab{b}}.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,
  DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,
  Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional {AI}: Harmlessness from {AI} feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022{\natexlab{b}}.

\bibitem[Barak(2023)]{barak2023another}
Boaz Barak.
\newblock ``{Another jailbreak for GPT4: Talk to it in Morse code}''.
\newblock \url{https://twitter.com/boazbaraktcs/status/1637657623100096513},
  2023.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brockman(2023)]{brockman2023deploying}
Greg Brockman.
\newblock ``{Deploying GPT-4 subject to adversarial pressures of real world has
  been a great practice run for practical AI alignment. Just getting started,
  but encouraged by degree of alignment we've achieved so far (and the
  engineering process we've been maturing to improve issues).}''.
\newblock \url{https://twitter.com/gdb/status/1641560965442576385}, 2023.

\bibitem[Brockman et~al.(2023)Brockman, Eleti, Georges, Jang, Kilpatrick, Lim,
  Miller, and Pokrass]{openai2023introducing}
Greg Brockman, Atty Eleti, Elie Georges, Joanne Jang, Logan Kilpatrick, Rachel
  Lim, Luke Miller, and Michelle Pokrass.
\newblock Introducing {ChatGPT} and {Whisper} {APIs}.
\newblock OpenAI Blog, 2023.
\newblock URL
  \url{https://openai.com/blog/introducing-chatgpt-and-whisper-apis}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Burgess(2023)]{burgess2023hacking}
Matt Burgess.
\newblock The hacking of {ChatGPT} is just getting started.
\newblock Wired, 2023.

\bibitem[Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss,
  Lee, Roberts, Brown, Song, Erlingsson, et~al.]{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom~B Brown, Dawn Song, Ulfar
  Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In \emph{USENIX Security Symposium}, volume~6, 2021.

\bibitem[Chakraborty et~al.(2018)Chakraborty, Alam, Dey, Chattopadhyay, and
  Mukhopadhyay]{chakraborty2018adversarial}
Anirban Chakraborty, Manaar Alam, Vishal Dey, Anupam Chattopadhyay, and Debdeep
  Mukhopadhyay.
\newblock Adversarial attacks and defences: A survey.
\newblock \emph{arXiv preprint arXiv:1810.00069}, 2018.

\bibitem[Christensen and Manuele(1999)]{asme1999safety}
W.C. Christensen and F.A. Manuele.
\newblock \emph{Safety Through Design}.
\newblock American Society of Mechanical Engineers, 1999.

\bibitem[Christian(2023)]{christian2023amazing}
Jon Christian.
\newblock Amazing ``jailbreak'' bypasses {ChatGPT}'s ethics safeguards.
\newblock Futurism, 2023.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[{Cleo Nardo}(2023)]{cleo2023waluigi}
{Cleo Nardo}.
\newblock The {Waluigi} effect (mega-post).
\newblock
  \url{https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post},
  2023.

\bibitem[El-Mhamdi et~al.(2022)El-Mhamdi, Farhadkhani, Guerraoui, Gupta, Hoang,
  Pinot, Rouault, and Stephan]{elmhamdi2022impossible}
El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta,
  Lê-Nguyên Hoang, Rafael Pinot, Sébastien Rouault, and John Stephan.
\newblock On the impossible safety of large {AI} models.
\newblock \emph{arXiv preprint arXiv:2209.15259}, 2022.

\bibitem[Elton(2023)]{elton2023thread}
Dan Elton.
\newblock ``{(humble brag) I've had alpha access to Anthropic's competitor to
  chatGPT the past ~2 weeks. The media embargo was just lifted an hour ago.
  I'll share some comparisons w chatGPT in thread. This summary I'm QT'ing
  aligns w/ my experience. See also screenshot of doc from Anthropic...}''.
\newblock \url{https://twitter.com/moreisdifferent/status/1611514796104351744},
  2023.

\bibitem[Fraser(2023)]{fraser2022thread}
Colin Fraser.
\newblock ``{Master thread of ways I have discovered to get ChatGPT to output
  text that it's not supposed to, including bigotry, URLs and personal
  information, and more.}''.
\newblock \url{https://twitter.com/colin_fraser/status/1630763219450212355},
  2023.

\bibitem[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath,
  Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red}
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav
  Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.
\newblock Red teaming language models to reduce harms: Methods, scaling
  behaviors, and lessons learned.
\newblock \emph{arXiv preprint arXiv:2209.07858}, 2022.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
\newblock {R}eal{T}oxicity{P}rompts: Evaluating neural toxic degeneration in
  language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 3356--3369, 2020.

\bibitem[Goldstein et~al.(2023)Goldstein, Sastry, Musser, DiResta, Gentzel, and
  Sedova]{goldstein2023generative}
Josh~A Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel,
  and Katerina Sedova.
\newblock Generative language models and automated influence operations:
  Emerging threats and potential mitigations.
\newblock \emph{arXiv preprint arXiv:2301.04246}, 2023.

\bibitem[Greshake et~al.(2023)Greshake, Abdelnabi, Mishra, Endres, Holz, and
  Fritz]{greshake2023more}
Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten
  Holz, and Mario Fritz.
\newblock More than you've asked for: A comprehensive analysis of novel prompt
  injection threats to application-integrated large language models.
\newblock \emph{arXiv preprint arXiv:2302.12173}, 2023.

\bibitem[Guzey(2023)]{guzey2023two}
Alexey Guzey.
\newblock A two sentence jailbreak for {GPT-4 and Claude} \& why nobody knows
  how to fix it.
\newblock \url{https://guzey.com/ai/two-sentence-universal-jailbreak/}, 2023.

\bibitem[Hazell(2023)]{hazell2023large}
Julian Hazell.
\newblock Large language models can be used to effectively scale spear phishing
  campaigns.
\newblock \emph{arXiv preprint arXiv:2305.06972}, 2023.

\bibitem[Jones et~al.(2023)Jones, Dragan, Raghunathan, and
  Steinhardt]{jones2023automatically}
Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt.
\newblock Automatically auditing large language models via discrete
  optimization.
\newblock \emph{arXiv preprint arXiv:2303.04381}, 2023.

\bibitem[Kang et~al.(2023)Kang, Li, Stoica, Guestrin, Zaharia, and
  Hashimoto]{kang2023exploiting}
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and
  Tatsunori Hashimoto.
\newblock Exploiting programmatic behavior of {LLMs}: Dual-use through standard
  security attacks.
\newblock \emph{arXiv preprint arXiv:2302.05733}, 2023.

\bibitem[Korbak et~al.(2023)Korbak, Shi, Chen, Bhalerao, Buckley, Phang,
  Bowman, and Perez]{korbak2023pretraining}
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher~L
  Buckley, Jason Phang, Samuel~R Bowman, and Ethan Perez.
\newblock Pretraining language models with human preferences.
\newblock \emph{arXiv preprint arXiv:2302.08582}, 2023.

\bibitem[Kreps et~al.(2022)Kreps, McCain, and Brundage]{kreps2022all}
Sarah Kreps, R.~Miles McCain, and Miles Brundage.
\newblock All the news that’s fit to fabricate: Ai-generated text as a tool
  of media misinformation.
\newblock \emph{Journal of Experimental Political Science}, 9\penalty0
  (1):\penalty0 104--117, 2022.

\bibitem[Li et~al.(2023)Li, Guo, Fan, Xu, and Song]{li2023multi}
Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song.
\newblock Multi-step jailbreaking privacy attacks on {ChatGPT}.
\newblock \emph{arXiv preprint arXiv:2304.05197}, 2023.

\bibitem[Lukas et~al.(2023)Lukas, Salem, Sim, Tople, Wutschitz, and
  Zanella-B{\'e}guelin]{lukas2023analyzing}
Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and
  Santiago Zanella-B{\'e}guelin.
\newblock Analyzing leakage of personally identifiable information in language
  models.
\newblock \emph{arXiv preprint arXiv:2302.00539}, 2023.

\bibitem[Mowshowitz(2023)]{mowshowitz2023jailbreaking}
Zvi Mowshowitz.
\newblock Jailbreaking {ChatGPT} on release day.
\newblock
  \url{https://www.lesswrong.com/posts/RYcoJdvmoBbi5Nax7/jailbreaking-chatgpt-on-release-day},
  2023.

\bibitem[Nanda(2023)]{nanda2023mechanistic}
Neel Nanda.
\newblock Mechanistic interpretability quickstart guide.
\newblock
  \url{https://www.neelnanda.io/mechanistic-interpretability/quickstart}, 2023.

\bibitem[Nin\_kat(2023)]{nin_kat2023new}
Nin\_kat.
\newblock ``{New jailbreak based on virtual functions smuggle}''.
\newblock
  \url{https://old.reddit.com/r/ChatGPT/comments/10urbdj/new_jailbreak_based_on_virtual_functions_smuggle/},
  2023.

\bibitem[OpenAI(2023{\natexlab{a}})]{openai2023gpt4}
OpenAI.
\newblock {GPT}-4 technical report.
\newblock \emph{arXiv preprint 2303.08774}, 2023{\natexlab{a}}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openai2023models}
OpenAI.
\newblock Models.
\newblock OpenAI API Documentation, 2023{\natexlab{b}}.
\newblock URL \url{https://platform.openai.com/docs/models/}.

\bibitem[OpenAI(2023{\natexlab{c}})]{openai2023our}
OpenAI.
\newblock Our approach to {AI} safety.
\newblock \url{https://openai.com/blog/our-approach-to-ai-safety},
  2023{\natexlab{c}}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese,
  McAleese, and Irving]{perez2022red}
Ethan Perez, Saffron Huang, H.~Francis Song, Trevor Cai, Roman Ring, John
  Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 3419--3448, 2022.

\bibitem[Semenov(2023)]{semenov2023rant}
Roman Semenov.
\newblock ``{The new jailbreak is so fun}''.
\newblock \url{https://twitter.com/semenov_roman_/status/1621465137025613825},
  2023.

\bibitem[Shaikh et~al.(2022)Shaikh, Zhang, Held, Bernstein, and
  Yang]{shaikh2022second}
Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang.
\newblock On second thought, let's not think step by step! {B}ias and toxicity
  in zero-shot reasoning.
\newblock \emph{arXiv preprint arXiv:2212.08061}, 2022.

\bibitem[Solaiman and Dennison(2021)]{solaiman2021process}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society ({PALMS}) with
  values-targeted datasets.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5861--5873, 2021.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3008--3021, 2020.

\bibitem[Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and
  Gan]{sun2023principle}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David
  Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with
  minimal human supervision.
\newblock \emph{arXiv preprint arXiv:2305.03047}, 2023.

\bibitem[walkerspider(2022)]{walkerspider2022dan}
walkerspider.
\newblock {DAN} is my new friend.
\newblock
  \url{https://old.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_friend/},
  2022.

\bibitem[Wallace et~al.(2019)Wallace, Feng, Kandpal, Gardner, and
  Singh]{wallace2019universal}
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
\newblock Universal adversarial triggers for attacking and analyzing {NLP}.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing}, pages 2153--2162, 2019.

\bibitem[Wang et~al.(2022)Wang, Ping, Xiao, Xu, Patwary, Shoeybi, Li,
  Anandkumar, and Catanzaro]{wang2022exploring}
Boxin Wang, Wei Ping, Chaowei Xiao, Peng Xu, Mostofa Patwary, Mohammad Shoeybi,
  Bo~Li, Anima Anandkumar, and Bryan Catanzaro.
\newblock Exploring the limits of domain-adaptive training for detoxifying
  large-scale language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama,
  Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
  Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem[Welbl et~al.(2021)Welbl, Glaese, Uesato, Dathathri, Mellor, Hendricks,
  Anderson, Kohli, Coppin, and Huang]{welbl2021challenges}
Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor,
  Lisa~Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen
  Huang.
\newblock Challenges in detoxifying language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 2447--2469, 2021.

\bibitem[WitchBOT(2023)]{witchbot2023you}
WitchBOT.
\newblock You can use {GPT-4} to create prompt injections against {GPT-4}.
\newblock
  \url{https://www.lesswrong.com/posts/bNCDexejSZpkuu3yz/you-can-use-gpt-4-to-create-prompt-injections-against-gpt-4},
  2023.

\bibitem[Witten(2022)]{witten2022thread}
Zack Witten.
\newblock ``{Thread of known ChatGPT jailbreaks}''.
\newblock \url{https://twitter.com/zswitten/status/1598380220943593472}, 2022.

\bibitem[Wolf et~al.(2023)Wolf, Wies, Levine, and Shashua]{wolf2023fundamental}
Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock Fundamental limitations of alignment in large language models.
\newblock \emph{arXiv preprint arXiv:2304.11082}, 2023.

\bibitem[Xu et~al.(2020)Xu, Ju, Li, Boureau, Weston, and Dinan]{xu2020recipes}
Jing Xu, Da~Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan.
\newblock Recipes for safety in open-domain chatbots.
\newblock \emph{arXiv preprint arXiv:2010.07079}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Sheng, Alhazmi, and Li]{zhang2020adversarial}
Wei~Emma Zhang, Quan~Z Sheng, Ahoud Alhazmi, and Chenliang Li.
\newblock Adversarial attacks on deep-learning models in natural language
  processing: A survey.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology (TIST)},
  11\penalty0 (3):\penalty0 1--41, 2020.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving]{ziegler2019fine}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario
  Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
