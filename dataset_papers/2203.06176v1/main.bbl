\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlam \& Pennington(2020)Adlam and Pennington]{adlam20ntk}
Adlam, B. and Pennington, J.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  74--84, 2020.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora19exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8139--8148, 2019.

\bibitem[Bach(2023)]{bach21learning}
Bach, F.
\newblock \emph{Learning Theory from First Principles}.
\newblock MIT, 2023.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and
  Sharma]{bahri21explaining}
Bahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv}, abs/2102.06701, 2021.

\bibitem[Bai \& Silverstein(2010)Bai and Silverstein]{bai2010spectral}
Bai, Z. and Silverstein, J.~W.
\newblock \emph{Spectral Analysis of Large Dimensional Random Matrices}.
\newblock Springer, 2010.

\bibitem[Bartlett(1996)]{bartlett96size}
Bartlett, P.~L.
\newblock For valid generalization the size of the weights is more important
  than the size of the network.
\newblock In \emph{Advances in Neural Information Processing Systems 9}, pp.\
  134--140, 1996.

\bibitem[Bartlett \& Mendelson(2001)Bartlett and
  Mendelson]{bartlett01rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock In \emph{Computational Learning Theory, 14th Annual Conference on
  Computational Learning Theory}, pp.\  224--240, 2001.

\bibitem[Bartlett et~al.(2002)Bartlett, Bousquet, and
  Mendelson]{bartlett02localized}
Bartlett, P.~L., Bousquet, O., and Mendelson, S.
\newblock Localized rademacher complexities.
\newblock In \emph{Computational Learning Theory, 15th Annual Conference on
  Computational Learning Theory}, pp.\  44--58, 2002.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett20benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bates et~al.(2021)Bates, Hastie, and
  Tibshirani]{bates21crossvalidation}
Bates, S., Hastie, T., and Tibshirani, R.
\newblock Cross-validation: what does it estimate and how well does it do it?
\newblock \emph{arXiv}, abs/2104.00673, 2021.

\bibitem[Belkin et~al.(2018)Belkin, Ma, and Mandal]{belkin18understand}
Belkin, M., Ma, S., and Mandal, S.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  540--548, 2018.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{belkin20two}
Belkin, M., Hsu, D., and Xu, J.
\newblock Two models of double descent for weak features.
\newblock \emph{{SIAM} J. Math. Data Sci.}, 2\penalty0 (4):\penalty0
  1167--1180, 2020.

\bibitem[Bloemendal et~al.(2016)Bloemendal, Knowles, Yau, and
  Yin]{bloemendal2016principal}
Bloemendal, A., Knowles, A., Yau, H.-T., and Yin, J.
\newblock On the principal components of sample covariance matrices.
\newblock \emph{Probability theory and related fields}, 164\penalty0
  (1):\penalty0 459--552, 2016.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and Gool]{bossard14food}
Bossard, L., Guillaumin, M., and Gool, L.~V.
\newblock Food-101 - mining discriminative components with random forests.
\newblock In \emph{Proceedings of the Thirteenth European Conference on
  Computer Vision}, pp.\  446--461, 2014.

\bibitem[Canatar et~al.(2021)Canatar, Bordelon, and
  Pehlevan]{canatar21spectral}
Canatar, A., Bordelon, B., and Pehlevan, C.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock \emph{Nature Communications}, 12\penalty0 (1):\penalty0 1--12, 2021.

\bibitem[Cao \& Golubev(2006)Cao and Golubev]{cao06oracle}
Cao, Y. and Golubev, Y.
\newblock On oracle inequalities related to smoothing splines.
\newblock \emph{Mathematical Methods of Statistics}, 15\penalty0 (4):\penalty0
  398--414, 2006.

\bibitem[Caponnetto \& Vito(2007)Caponnetto and Vito]{caponnetto07optimal}
Caponnetto, A. and Vito, E.~D.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Found. Comput. Math.}, 7\penalty0 (3):\penalty0 331--368, 2007.

\bibitem[Craven \& Wahba(1978)Craven and Wahba]{craven78smoothing}
Craven, P. and Wahba, G.
\newblock Smoothing noisy data with spline functions.
\newblock \emph{Numerische Mathematik}, 31:\penalty0 377--403, 1978.

\bibitem[Cui et~al.(2021)Cui, Loureiro, Krzakala, and
  Zdeborova]{cui21generalization}
Cui, H., Loureiro, B., Krzakala, F., and Zdeborova, L.
\newblock Generalization error rates in kernel regression: The crossover from
  the noiseless to noisy regime.
\newblock In \emph{Advances in Neural Information Processing Systems 34}, 2021.

\bibitem[Dobriban \& Wager(2018)Dobriban and Wager]{dobriban18high}
Dobriban, E. and Wager, S.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (1):\penalty0 247--279,
  2018.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy21image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{9th International Conference on Learning Representations},
  2021.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite17computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proceedings of the Thirty-Third Conference on Uncertainty in
  Artificial Intelligence}, 2017.

\bibitem[Dziugaite et~al.(2020)Dziugaite, Drouin, Neal, Rajkumar, Caballero,
  Wang, Mitliagkas, and Roy]{dziugaite20search}
Dziugaite, G.~K., Drouin, A., Neal, B., Rajkumar, N., Caballero, E., Wang, L.,
  Mitliagkas, I., and Roy, D.~M.
\newblock In search of robust measures of generalization.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[Efron(1986)]{efron86biased}
Efron, B.
\newblock How biased is the apparent error rate of a prediction rule?
\newblock \emph{Journal of the American Statistical Association}, 81\penalty0
  (394):\penalty0 461--470, 1986.

\bibitem[Erdős \& Yau(2017)Erdős and Yau]{erdos17dynamical}
Erdős, L. and Yau, H.-T.
\newblock \emph{A Dynamical Approach to Random Matrix Theory}.
\newblock American Mathematical Society, 2017.

\bibitem[Golub et~al.(1979)Golub, Heath, and Wahba]{golub79generalized}
Golub, G.~H., Heath, M., and Wahba, G.
\newblock Generalized cross-validation as a method for choosing a good ridge
  parameter.
\newblock \emph{Technometrics}, 21\penalty0 (2):\penalty0 215--223, 1979.

\bibitem[Hastie et~al.(2020)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie21surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv}, abs/1903.08560, 2020.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he15delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{2015 {IEEE} International Conference on Computer Vision},
  pp.\  1026--1034, 2015.

\bibitem[Hsu et~al.(2014)Hsu, Kakade, and Zhang]{hsu14ridge}
Hsu, D.~J., Kakade, S.~M., and Zhang, T.
\newblock Random design analysis of ridge regression.
\newblock \emph{Found. Comput. Math.}, 14\penalty0 (3):\penalty0 569--600,
  2014.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{jacot18ntk}
Jacot, A., Hongler, C., and Gabriel, F.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pp.\
  8580--8589, 2018.

\bibitem[Jacot et~al.(2020{\natexlab{a}})Jacot, Simsek, Spadaro, Hongler, and
  Gabriel]{jacot2020implicit}
Jacot, A., Simsek, B., Spadaro, F., Hongler, C., and Gabriel, F.
\newblock Implicit regularization of random feature models.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  4631--4640, 2020{\natexlab{a}}.

\bibitem[Jacot et~al.(2020{\natexlab{b}})Jacot, Simsek, Spadaro, Hongler, and
  Gabriel]{jacot20kernel}
Jacot, A., Simsek, B., Spadaro, F., Hongler, C., and Gabriel, F.
\newblock Kernel alignment risk estimator: Risk prediction from training data.
\newblock In \emph{Advances in Neural Information Processing Systems 33},
  2020{\natexlab{b}}.

\bibitem[Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang20fantastic}
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{8th International Conference on Learning Representations},
  2020.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan20scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv}, abs/2001.08361, 2020.

\bibitem[Knowles \& Yin(2017)Knowles and Yin]{knowles17anisotropic}
Knowles, A. and Yin, J.
\newblock Anisotropic local laws for random matrices.
\newblock \emph{Probability Theory and Related Fields}, 169:\penalty0 257--352,
  2017.

\bibitem[Krizhevsky(2009)]{krizhevsky09learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl{-}Dickstein,
  and Pennington]{lee19wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl{-}Dickstein,
  J., and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8570--8581, 2019.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl{-}Dickstein]{lee20finite}
Lee, J., Schoenholz, S.~S., Pennington, J., Adlam, B., Xiao, L., Novak, R., and
  Sohl{-}Dickstein, J.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[Li(1986)]{li86asymptotic}
Li, K.-C.
\newblock {Asymptotic Optimality of $C_L$ and Generalized Cross-Validation in
  Ridge Regression with Application to Spline Smoothing}.
\newblock \emph{The Annals of Statistics}, 14\penalty0 (3):\penalty0
  1101--1112, 1986.

\bibitem[Li et~al.(2019)Li, Wang, Yu, Du, Hu, Salakhutdinov, and
  Arora]{li19enhanced}
Li, Z., Wang, R., Yu, D., Du, S.~S., Hu, W., Salakhutdinov, R., and Arora, S.
\newblock Enhanced convolutional neural tangent kernels.
\newblock \emph{arXiv}, abs/1911.00809, 2019.

\bibitem[Loureiro et~al.(2021)Loureiro, Gerbelot, Cui, Goldt, Krzakala, Mezard,
  and Zdeborova]{loureiro21learning}
Loureiro, B., Gerbelot, C., Cui, H., Goldt, S., Krzakala, F., Mezard, M., and
  Zdeborova, L.
\newblock Learning curves of generic features maps for realistic datasets with
  a teacher-student model.
\newblock In \emph{Advances in Neural Information Processing Systems 34}, 2021.

\bibitem[Marchenko \& Pastur(1967)Marchenko and Pastur]{marchenkopastur}
Marchenko, V.~A. and Pastur, L.~A.
\newblock Distribution of eigenvalues for some sets of random matrices.
\newblock \emph{Matematicheskii Sbornik}, 114\penalty0 (4):\penalty0 507--536,
  1967.

\bibitem[Marquardt \& Snee(1975)Marquardt and Snee]{marquardt1975ridge}
Marquardt, D.~W. and Snee, R.~D.
\newblock Ridge regression in practice.
\newblock \emph{The American Statistician}, 29\penalty0 (1):\penalty0 3--20,
  1975.

\bibitem[McAllester(1999)]{mcallester99pac}
McAllester, D.~A.
\newblock Pac-bayesian model averaging.
\newblock In \emph{Proceedings of the Twelfth Annual Conference on
  Computational Learning Theory}, pp.\  164--170, 1999.

\bibitem[Mei \& Montanari(2020)Mei and Montanari]{mei20generalization}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock \emph{arXiv}, abs/1908.05355, 2020.

\bibitem[Mel \& Ganguli(2021)Mel and Ganguli]{mel21arbitrary}
Mel, G. and Ganguli, S.
\newblock A theory of high dimensional regression with arbitrary correlations
  between input features and target functions: sample complexity, multiple
  descent curves and a hierarchy of phase transitions.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139, pp.\  7578--7587, 2021.

\bibitem[Nagarajan \& Kolter(2019)Nagarajan and Kolter]{nagarajan19uniform}
Nagarajan, V. and Kolter, J.~Z.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  11611--11622, 2019.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Neyshabur, B.
\newblock Implicit regularization in deep learning.
\newblock \emph{arXiv}, abs/1709.01953, 2017.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and Srebro]{neyshabur15norm}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory},
  volume~40, pp.\  1376--1401, 2015.

\bibitem[Nilsback \& Zisserman(2008)Nilsback and Zisserman]{nilsback08flowers}
Nilsback, M. and Zisserman, A.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{Sixth Indian Conference on Computer Vision, Graphics {\&}
  Image Processing}, pp.\  722--729, 2008.

\bibitem[Northcutt et~al.(2021)Northcutt, Athalye, and
  Mueller]{northcutt21pervasive}
Northcutt, C.~G., Athalye, A., and Mueller, J.
\newblock Pervasive label errors in test sets destabilize machine learning
  benchmarks.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2021.

\bibitem[Novikoff(1962)]{novikoff62convergence}
Novikoff, A.~B.
\newblock On convergence proofs on perceptrons.
\newblock In \emph{Proceedings of the Symposium on the Mathematical Theory of
  Automata}, volume~12, pp.\  615--622. Polytechnic Institute of Brooklyn,
  1962.

\bibitem[Patil et~al.(2021)Patil, Wei, Rinaldo, and Tibshirani]{patil21uniform}
Patil, P., Wei, Y., Rinaldo, A., and Tibshirani, R.~J.
\newblock Uniform consistency of cross-validation estimators for
  high-dimensional ridge regression.
\newblock In Banerjee, A. and Fukumizu, K. (eds.), \emph{The 24th International
  Conference on Artificial Intelligence and Statistics}, pp.\  3178--3186,
  2021.

\bibitem[Richards et~al.(2021)Richards, Mourtada, and
  Rosasco]{richards21asymptotics}
Richards, D., Mourtada, J., and Rosasco, L.
\newblock Asymptotics of ridge(less) regression under general source condition.
\newblock In \emph{The 24th International Conference on Artificial Intelligence
  and Statistics}, volume 130, pp.\  3889--3897, 2021.

\bibitem[Rosset \& Tibshirani(2020)Rosset and Tibshirani]{rosset20fixed}
Rosset, S. and Tibshirani, R.~J.
\newblock From fixed-{X} to random-{X} regression: Bias-variance
  decompositions, covariance penalties, and prediction error estimation.
\newblock \emph{Journal of the American Statistical Association}, 115\penalty0
  (529):\penalty0 138--151, 2020.

\bibitem[Simon(2021)]{simon21blog}
Simon, J.~B.
\newblock A first-principles theory of neural network generalization, October
  2021.
\newblock URL \url{https://bair.berkeley.edu/blog/2021/10/25/eigenlearning/}.

\bibitem[Simon et~al.(2021)Simon, Dickens, and DeWeese]{simon21neural}
Simon, J.~B., Dickens, M., and DeWeese, M.~R.
\newblock Neural tangent kernel eigenvalues accurately predict generalization.
\newblock \emph{arXiv}, abs/2110.03922, 2021.

\bibitem[Steinhardt(2021)]{steinhardt21notes}
Steinhardt, J.
\newblock Robust and nonparametric statistics, April 2021.
\newblock URL
  \url{https://jsteinhardt.stat.berkeley.edu/teaching/stat240-spring-2021/notes.pdf}.

\bibitem[Wu \& Xu(2020)Wu and Xu]{wu20optimal}
Wu, D. and Xu, J.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, 2020.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv}, abs/1708.07747, 2017.

\bibitem[Yang(2019)]{yang19scaling}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv}, abs/1902.04760, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang17understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{5th International Conference on Learning Representations},
  2017.

\bibitem[Zhang(2005)]{zhang05learning}
Zhang, T.
\newblock Learning bounds for kernel regression using effective data
  dimensionality.
\newblock \emph{Neural Comput.}, 17\penalty0 (9):\penalty0 2077--2098, 2005.

\end{thebibliography}
