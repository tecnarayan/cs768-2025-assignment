\begin{thebibliography}{10}

\bibitem{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{agarwal2019reinforcement}
Alekh Agarwal, Nan Jiang, Sham~M Kakade, and Wen Sun.
\newblock Reinforcement learning: Theory and algorithms.
\newblock {\em CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, pages 10--4,
  2019.

\bibitem{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR, 2020.

\bibitem{jia2020model}
Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In {\em Learning for Dynamics and Control}, pages 666--686. PMLR,
  2020.

\bibitem{zhou2021provably}
Dongruo Zhou, Jiafan He, and Quanquan Gu.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In {\em International Conference on Machine Learning}, pages
  12793--12802. PMLR, 2021.

\bibitem{he2022near}
Jiafan He, Dongruo Zhou, and Quanquan Gu.
\newblock Near-optimal policy optimization algorithms for learning adversarial
  linear mixture mdps.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4259--4280. PMLR, 2022.

\bibitem{yang2019projection}
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter~J Ramadge.
\newblock Projection-based constrained policy optimization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{brantley2020constrained}
Kiant{\'e} Brantley, Miro Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max
  Simchowitz, Aleksandrs Slivkins, and Wen Sun.
\newblock Constrained episodic reinforcement learning in concave-convex and
  knapsack settings.
\newblock {\em Advances in Neural Information Processing Systems},
  33:16315--16326, 2020.

\bibitem{ding2021provably}
Dongsheng Ding, Xiaohan Wei, Zhuoran Yang, Zhaoran Wang, and Mihailo Jovanovic.
\newblock Provably efficient safe exploration via primal-dual policy
  optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3304--3312. PMLR, 2021.

\bibitem{paternain2022safe}
Santiago Paternain, Miguel Calvo-Fullana, Luiz~FO Chamon, and Alejandro
  Ribeiro.
\newblock Safe policies for reinforcement learning via primal-dual methods.
\newblock {\em IEEE Transactions on Automatic Control}, 2022.

\bibitem{amani2019linear}
Sanae Amani, Mahnoosh Alizadeh, and Christos Thrampoulidis.
\newblock Linear stochastic bandits under safety constraints.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{shi2022stability}
Yuanyuan Shi, Guannan Qu, Steven Low, Anima Anandkumar, and Adam Wierman.
\newblock Stability constrained reinforcement learning for real-time voltage
  control.
\newblock In {\em 2022 American Control Conference (ACC)}, pages 2715--2721.
  IEEE, 2022.

\bibitem{amani2021safe}
Sanae Amani, Christos Thrampoulidis, and Lin Yang.
\newblock Safe reinforcement learning with linear function approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  243--253. PMLR, 2021.

\bibitem{vamvoudakis2021handbook}
Kyriakos~G Vamvoudakis, Yan Wan, Frank~L Lewis, and Derya Cansever.
\newblock {\em Handbook of Reinforcement Learning and Control}.
\newblock Springer, 2021.

\bibitem{turchetta2016safe}
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause.
\newblock Safe exploration in finite markov decision processes with gaussian
  processes.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{wachi2018safe}
Akifumi Wachi, Yanan Sui, Yisong Yue, and Masahiro Ono.
\newblock Safe exploration and optimization of constrained mdps using gaussian
  processes.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{pacchiano2021stochastic}
Aldo Pacchiano, Mohammad Ghavamzadeh, Peter Bartlett, and Heinrich Jiang.
\newblock Stochastic bandits with linear constraints.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2827--2835. PMLR, 2021.

\bibitem{wu2016conservative}
Yifan Wu, Roshan Shariff, Tor Lattimore, and Csaba Szepesv{\'a}ri.
\newblock Conservative bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  1254--1262. PMLR, 2016.

\bibitem{achiam2017constrained}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In {\em International conference on machine learning}, pages 22--31.
  PMLR, 2017.

\bibitem{tessler2018reward}
Chen Tessler, Daniel~J Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{efroni2020exploration}
Yonathan Efroni, Shie Mannor, and Matteo Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock {\em arXiv preprint arXiv:2003.02189}, 2020.

\bibitem{singh2020learning}
Rahul Singh, Abhishek Gupta, and Ness~B Shroff.
\newblock Learning in markov decision processes under constraints.
\newblock {\em arXiv preprint arXiv:2002.12435}, 2020.

\bibitem{ding2020natural}
Dongsheng Ding, Kaiqing Zhang, Tamer Basar, and Mihailo Jovanovic.
\newblock Natural policy gradient primal-dual method for constrained markov
  decision processes.
\newblock {\em Advances in Neural Information Processing Systems},
  33:8378--8390, 2020.

\bibitem{kalagarla2021sample}
Krishna~C Kalagarla, Rahul Jain, and Pierluigi Nuzzo.
\newblock A sample-efficient algorithm for episodic finite-horizon mdp with
  constraints.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 8030--8037, 2021.

\bibitem{liu2021learning}
Tao Liu, Ruida Zhou, Dileep Kalathil, Panganamala Kumar, and Chao Tian.
\newblock Learning policies with zero or bounded constraint violation for
  constrained mdps.
\newblock {\em Advances in Neural Information Processing Systems},
  34:17183--17193, 2021.

\bibitem{wei2021provably}
Honghao Wei, Xin Liu, and Lei Ying.
\newblock A provably-efficient model-free algorithm for constrained markov
  decision processes.
\newblock {\em arXiv preprint arXiv:2106.01577}, 2021.

\bibitem{xu2021crpo}
Tengyu Xu, Yingbin Liang, and Guanghui Lan.
\newblock Crpo: A new approach for safe reinforcement learning with convergence
  guarantee.
\newblock In {\em International Conference on Machine Learning}, pages
  11480--11491. PMLR, 2021.

\bibitem{bai2022achieving}
Qinbo Bai, Amrit~Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal.
\newblock Achieving zero constraint violation for constrained reinforcement
  learning via primal-dual approach.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 3682--3689, 2022.

\bibitem{ghosh2022provably}
Arnob Ghosh, Xingyu Zhou, and Ness Shroff.
\newblock Provably efficient model-free constrained rl with linear function
  approximation.
\newblock {\em arXiv preprint arXiv:2206.11889}, 2022.

\bibitem{caramanis2014efficient}
Constantine Caramanis, Nedialko~B Dimitrov, and David~P Morton.
\newblock Efficient algorithms for budget-constrained markov decision
  processes.
\newblock {\em IEEE Transactions on Automatic Control}, 59(10):2813--2817,
  2014.

\bibitem{wu2018budget}
Di~Wu, Xiujun Chen, Xun Yang, Hao Wang, Qing Tan, Xiaoxun Zhang, Jian Xu, and
  Kun Gai.
\newblock Budget constrained bidding by model-free reinforcement learning in
  display advertising.
\newblock In {\em Proceedings of the 27th ACM International Conference on
  Information and Knowledge Management}, pages 1443--1451, 2018.

\bibitem{zhou2021nearly}
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In {\em Conference on Learning Theory}, pages 4532--4576. PMLR, 2021.

\bibitem{zhou2022computationally}
Dongruo Zhou and Quanquan Gu.
\newblock Computationally efficient horizon-free reinforcement learning for
  linear mixture mdps.
\newblock {\em Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{kaufmann2016complexity}
Emilie Kaufmann, Olivier Capp{\'e}, and Aur{\'e}lien Garivier.
\newblock On the complexity of best-arm identification in multi-armed bandit
  models.
\newblock {\em The Journal of Machine Learning Research}, 17(1):1--42, 2016.

\bibitem{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock {\em Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\end{thebibliography}
