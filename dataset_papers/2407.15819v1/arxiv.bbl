\begin{thebibliography}{100}

\bibitem{agrawal2019nocaps}
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.
\newblock Nocaps: Novel object captioning at scale.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 8948--8957, 2019.

\bibitem{flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:23716--23736, 2022.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{qwenvl}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{bi2024deepseekllm}
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et~al.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock {\em arXiv preprint arXiv:2401.02954}, 2024.

\bibitem{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{kakaobrain2022coyo}
Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
\newblock Coyo-700m: Image-text pair dataset.
\newblock \url{https://github.com/kakaobrain/coyo-dataset}, 2022.

\bibitem{carreira2017i3d}
Joao Carreira and Andrew Zisserman.
\newblock Quo vadis, action recognition? a new model and the kinetics dataset.
\newblock In {\em proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 6299--6308, 2017.

\bibitem{cha2023honeybee}
Junbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh.
\newblock Honeybee: Locality-enhanced projector for multimodal llm.
\newblock {\em arXiv preprint arXiv:2312.06742}, 2023.

\bibitem{chen2023minigptv2}
Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
\newblock Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.
\newblock {\em arXiv preprint arXiv:2310.09478}, 2023.

\bibitem{chen2023shikra}
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.
\newblock Shikra: Unleashing multimodal llm's referential dialogue magic.
\newblock {\em arXiv preprint arXiv:2306.15195}, 2023.

\bibitem{chen2017deeplab}
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan~L Yuille.
\newblock Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 40(4):834--848, 2017.

\bibitem{chen2023palix}
Xi~Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos~Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi~Tay, et~al.
\newblock Pali-x: On scaling up a multilingual vision and language model.
\newblock {\em arXiv preprint arXiv:2305.18565}, 2023.

\bibitem{chen2023pali3}
Xi~Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et~al.
\newblock Pali-3 vision language models: Smaller, faster, stronger.
\newblock {\em arXiv preprint arXiv:2310.09199}, 2023.

\bibitem{chen2022pali}
Xi~Chen, Xiao Wang, Soravit Changpinyo, AJ~Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et~al.
\newblock Pali: A jointly-scaled multilingual language-image model.
\newblock {\em arXiv preprint arXiv:2209.06794}, 2022.

\bibitem{chen2020uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El~Kholy, Faisal Ahmed, Zhe Gan, Yu~Cheng, and Jingjing Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In {\em European conference on computer vision}, pages 104--120. Springer, 2020.

\bibitem{chen2024internvl1_5}
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et~al.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
\newblock {\em arXiv preprint arXiv:2404.16821}, 2024.

\bibitem{chen2023internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock {\em arXiv preprint arXiv:2312.14238}, 2023.

\bibitem{vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality.
\newblock \url{https://lmsys.org/blog/2023-03-30-vicuna/}, 2023.

\bibitem{dai2017deformable}
Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi~Li, Guodong Zhang, Han Hu, and Yichen Wei.
\newblock Deformable convolutional networks.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 764--773, 2017.

\bibitem{dai2024instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale~N Fung, and Steven Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{dao2023flashattentionv2}
Tri Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock {\em arXiv preprint arXiv:2307.08691}, 2023.

\bibitem{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock {\em Advances in Neural Information Processing Systems}, 35:16344--16359, 2022.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dong2024xcomposer4k}
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et~al.
\newblock Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd.
\newblock {\em arXiv preprint arXiv:2404.06512}, 2024.

\bibitem{fan2021mvit}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 6824--6835, 2021.

\bibitem{fedus2022switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock {\em Journal of Machine Learning Research}, 23(120):1--39, 2022.

\bibitem{fu2023mme}
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu~Lin, Jinrui Yang, Xiawu Zheng, Ke~Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large language models.
\newblock {\em arXiv preprint arXiv:2306.13394}, 2023.

\bibitem{fukushima1980neocognitron}
Kunihiko Fukushima.
\newblock Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position.
\newblock {\em Biological cybernetics}, 36(4):193--202, 1980.

\bibitem{gao2024sphinxx}
Peng Gao, Renrui Zhang, Chris Liu, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, et~al.
\newblock Sphinx-x: Scaling data and parameters for a family of multi-modal large language models.
\newblock {\em arXiv preprint arXiv:2402.05935}, 2024.

\bibitem{vqav2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6904--6913, 2017.

\bibitem{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{gqa}
Drew~A Hudson and Christopher~D Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional question answering.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 6700--6709, 2019.

\bibitem{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira.
\newblock Perceiver: General perception with iterative attention.
\newblock In {\em International conference on machine learning}, pages 4651--4664. PMLR, 2021.

\bibitem{kamath2021mdetr}
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.
\newblock Mdetr-modulated detection for end-to-end multi-modal understanding.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1780--1790, 2021.

\bibitem{refcoco}
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.
\newblock Referitgame: Referring to objects in photographs of natural scenes.
\newblock In {\em Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)}, pages 787--798, 2014.

\bibitem{vgcaption}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced dense image annotations.
\newblock {\em International journal of computer vision}, 123:32--73, 2017.

\bibitem{krizhevsky2012alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems}, 25, 2012.

\bibitem{laurenccon2024obelics}
Hugo Lauren{\c{c}}on, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et~al.
\newblock Obelics: An open web-scale filtered dataset of interleaved image-text documents.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{lecun1989backpropagation}
Yann LeCun, Bernhard Boser, John~S Denker, Donnie Henderson, Richard~E Howard, Wayne Hubbard, and Lawrence~D Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1(4):541--551, 1989.

\bibitem{li2023seed}
Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan.
\newblock Seed-bench: Benchmarking multimodal llms with generative comprehension.
\newblock {\em arXiv preprint arXiv:2307.16125}, 2023.

\bibitem{blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{li2019trinet}
Yanghao Li, Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang.
\newblock Scale-aware trident networks for object detection.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 6054--6063, 2019.

\bibitem{li2023flip}
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
\newblock Scaling language-image pre-training via masking.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23390--23400, 2023.

\bibitem{li2022vitdet}
Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
\newblock Exploring plain vision transformer backbones for object detection.
\newblock In {\em European Conference on Computer Vision}, pages 280--296. Springer, 2022.

\bibitem{li2024minigemini}
Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia.
\newblock Mini-gemini: Mining the potential of multi-modality vision language models.
\newblock {\em arXiv preprint arXiv:2403.18814}, 2024.

\bibitem{li2023pope}
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Evaluating object hallucination in large vision-language models.
\newblock {\em arXiv preprint arXiv:2305.10355}, 2023.

\bibitem{li2023monkey}
Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.
\newblock Monkey: Image resolution and text label are important things for large multi-modal models.
\newblock {\em arXiv preprint arXiv:2311.06607}, 2023.

\bibitem{lin2024moellava}
Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li~Yuan.
\newblock Moe-llava: Mixture of experts for large vision-language models.
\newblock {\em arXiv preprint arXiv:2401.15947}, 2024.

\bibitem{lin2023vila}
Ji~Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han.
\newblock Vila: On pre-training for visual language models.
\newblock {\em arXiv preprint arXiv:2312.07533}, 2023.

\bibitem{lin2017fpn}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2117--2125, 2017.

\bibitem{coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem{lin2023sphinx}
Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et~al.
\newblock Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.
\newblock {\em arXiv preprint arXiv:2311.07575}, 2023.

\bibitem{liu2023llava15}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2310.03744}, 2023.

\bibitem{llavanext}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.

\bibitem{llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2304.08485}, 2023.

\bibitem{liu2023mmbench}
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo~Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock {\em arXiv preprint arXiv:2307.06281}, 2023.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem{long2015fcn}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3431--3440, 2015.

\bibitem{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{lu2024deepseek}
Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et~al.
\newblock Deepseek-vl: towards real-world vision-language understanding.
\newblock {\em arXiv preprint arXiv:2403.05525}, 2024.

\bibitem{lu2023uio2}
Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi.
\newblock Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action.
\newblock {\em arXiv preprint arXiv:2312.17172}, 2023.

\bibitem{lu2022sqa}
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock {\em Advances in Neural Information Processing Systems}, 35:2507--2521, 2022.

\bibitem{macawllm}
Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu.
\newblock Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration.
\newblock {\em arXiv preprint arXiv:2306.09093}, 2023.

\bibitem{refcocog}
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan~L Yuille, and Kevin Murphy.
\newblock Generation and comprehension of unambiguous object descriptions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 11--20, 2016.

\bibitem{okvqa}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external knowledge.
\newblock In {\em Proceedings of the IEEE/cvf conference on computer vision and pattern recognition}, pages 3195--3204, 2019.

\bibitem{mckinzie2024mm1}
Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et~al.
\newblock Mm1: Methods, analysis \& insights from multimodal llm pre-training.
\newblock {\em arXiv preprint arXiv:2403.09611}, 2024.

\bibitem{llama3}
Meta.
\newblock Introducing meta llama 3: The most capable openly available llm to date.
\newblock 2024.

\bibitem{ocrvqa}
Anand Mishra, Shashank Shekhar, Ajeet~Kumar Singh, and Anirban Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In {\em 2019 international conference on document analysis and recognition (ICDAR)}, pages 947--952. IEEE, 2019.

\bibitem{mu2024robocodex}
Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian Ge, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, et~al.
\newblock Robocodex: Multimodal code generation for robotic behavior synthesis.
\newblock {\em arXiv preprint arXiv:2402.16117}, 2024.

\bibitem{mu2024embodiedgpt}
Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu~Qiao, and Ping Luo.
\newblock Embodiedgpt: Vision-language pre-training via embodied chain of thought.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{gpt4v}
OpenAI.
\newblock Gpt-4v(ision) system card.
\newblock 2023.

\bibitem{sbu}
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{plummer2015flickr30k}
Bryan~A Plummer, Liwei Wang, Chris~M Cervantes, Juan~C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 2641--2649, 2015.

\bibitem{qing2023mar}
Zhiwu Qing, Shiwei Zhang, Ziyuan Huang, Xiang Wang, Yuehuan Wang, Yiliang Lv, Changxin Gao, and Nong Sang.
\newblock Mar: Masked autoencoders for efficient action recognition.
\newblock {\em IEEE Transactions on Multimedia}, 2023.

\bibitem{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{radford2018gpt}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock {\em Technical Report}, 2018.

\bibitem{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em The Journal of Machine Learning Research}, 21(1):5485--5551, 2020.

\bibitem{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In {\em SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, pages 1--16. IEEE, 2020.

\bibitem{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem{redmon2016yolo}
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 779--788, 2016.

\bibitem{schwenk2022aokvqa}
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
\newblock A-okvqa: A benchmark for visual question answering using world knowledge.
\newblock In {\em European Conference on Computer Vision}, pages 146--162. Springer, 2022.

\bibitem{cc3m}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565, 2018.

\bibitem{textcaps}
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.
\newblock Textcaps: a dataset for image captioning with reading comprehension.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16}, pages 742--758. Springer, 2020.

\bibitem{textvqa}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem{sun2023emu2}
Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et~al.
\newblock Generative multimodal models are in-context learners.
\newblock {\em arXiv preprint arXiv:2312.13286}, 2023.

\bibitem{tang2023salmonn}
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu~Lu, Zejun Ma, and Chao Zhang.
\newblock Salmonn: Towards generic hearing abilities for large language models.
\newblock {\em arXiv preprint arXiv:2310.13289}, 2023.

\bibitem{gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{tsimpoukelli2021multimodal}
Maria Tsimpoukelli, Jacob~L Menick, Serkan Cabi, SM~Eslami, Oriol Vinyals, and Felix Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock {\em Advances in Neural Information Processing Systems}, 34:200--212, 2021.

\bibitem{vaswani2017transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2023tokenizer}
Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, and Ying Shan.
\newblock What makes for good visual tokenizers for large language models?
\newblock {\em arXiv preprint arXiv:2305.12223}, 2023.

\bibitem{ofa}
Peng Wang, An~Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.
\newblock Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.
\newblock In {\em International Conference on Machine Learning}, pages 23318--23340. PMLR, 2022.

\bibitem{wang2023cogvlm}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et~al.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock {\em arXiv preprint arXiv:2311.03079}, 2023.

\bibitem{wang2021pvt}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 568--578, 2021.

\bibitem{ye2023mplugowl2}
Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi~Qian, Ji~Zhang, Fei Huang, and Jingren Zhou.
\newblock mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.
\newblock {\em arXiv preprint arXiv:2311.04257}, 2023.

\bibitem{you2023ferret}
Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang.
\newblock Ferret: Refer and ground anything anywhere at any granularity.
\newblock {\em arXiv preprint arXiv:2310.07704}, 2023.

\bibitem{yu2015dilated}
Fisher Yu and Vladlen Koltun.
\newblock Multi-scale context aggregation by dilated convolutions.
\newblock {\em arXiv preprint arXiv:1511.07122}, 2015.

\bibitem{yu2018mattnet}
Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara~L Berg.
\newblock Mattnet: Modular attention network for referring expression comprehension.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 1307--1315, 2018.

\bibitem{refcocoplus}
Licheng Yu, Patrick Poirson, Shan Yang, Alexander~C Berg, and Tamara~L Berg.
\newblock Modeling context in referring expressions.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14}, pages 69--85. Springer, 2016.

\bibitem{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock {\em arXiv preprint arXiv:2311.16502}, 2023.

\bibitem{zhan2024griffonv2}
Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang.
\newblock Griffon v2: Advancing multimodal perception with high-resolution scaling and visual-language co-referring.
\newblock {\em arXiv preprint arXiv:2403.09333}, 2024.

\bibitem{zhang2021rest}
Qinglong Zhang and Yu-Bin Yang.
\newblock Rest: An efficient transformer for visual recognition.
\newblock {\em Advances in Neural Information Processing Systems}, 34:15475--15485, 2021.

\bibitem{zhang2022rest}
Qinglong Zhang and Yu-Bin Yang.
\newblock Rest v2: simpler, faster and stronger.
\newblock {\em Advances in Neural Information Processing Systems}, 35:36440--36452, 2022.

\bibitem{opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhu2023minigpt4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
