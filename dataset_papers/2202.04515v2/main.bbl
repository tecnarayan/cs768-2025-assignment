\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahle et~al.(2020)Ahle, Kapralov, Knudsen, Pagh, Velingker, Woodruff,
  and Zandieh]{ahle2019oblivious}
Ahle, T.~D., Kapralov, M., Knudsen, J.~B., Pagh, R., Velingker, A., Woodruff,
  D.~P., and Zandieh, A.
\newblock Oblivious sketching of high-degree polynomial kernels.
\newblock In \emph{Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pp.\  141--160. SIAM, 2020.

\bibitem[Ailon \& Chazelle(2009)Ailon and Chazelle]{ailon2009fast}
Ailon, N. and Chazelle, B.
\newblock The fast johnson--lindenstrauss transform and approximate nearest
  neighbors.
\newblock \emph{SIAM Journal on computing}, 39\penalty0 (1):\penalty0 302--322,
  2009.

\bibitem[Avron et~al.(2014)Avron, Nguyen, and Woodruff]{avron2014subspace}
Avron, H., Nguyen, H., and Woodruff, D.
\newblock Subspace embeddings for the polynomial kernel.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Avron et~al.(2017)Avron, Kapralov, Musco, Musco, Velingker, and
  Zandieh]{avron2017random}
Avron, H., Kapralov, M., Musco, C., Musco, C., Velingker, A., and Zandieh, A.
\newblock Random fourier features for kernel ridge regression: Approximation
  bounds and statistical guarantees.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  253--262. JMLR. org, 2017.

\bibitem[Avron et~al.(2019)Avron, Kapralov, Musco, Musco, Velingker, and
  Zandieh]{avron2019universal}
Avron, H., Kapralov, M., Musco, C., Musco, C., Velingker, A., and Zandieh, A.
\newblock A universal sampling method for reconstructing signals with simple
  fourier transforms.
\newblock In \emph{Proceedings of the 51st Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  1051--1063, 2019.

\bibitem[Charikar et~al.(2002)Charikar, Chen, and
  Farach-Colton]{charikar2002finding}
Charikar, M., Chen, K., and Farach-Colton, M.
\newblock Finding frequent items in data streams.
\newblock In \emph{International Colloquium on Automata, Languages, and
  Programming}, pp.\  693--703. Springer, 2002.

\bibitem[Cohen et~al.(2015)Cohen, Lee, Musco, Musco, Peng, and
  Sidford]{cohen2015uniform}
Cohen, M.~B., Lee, Y.~T., Musco, C., Musco, C., Peng, R., and Sidford, A.
\newblock Uniform sampling for matrix approximation.
\newblock In \emph{Proceedings of the 2015 Conference on Innovations in
  Theoretical Computer Science}, pp.\  181--190, 2015.

\bibitem[Dasgupta \& Gupta(2003)Dasgupta and Gupta]{dasgupta2003elementary}
Dasgupta, S. and Gupta, A.
\newblock An elementary proof of a theorem of johnson and lindenstrauss.
\newblock \emph{Random Structures \& Algorithms}, 22\penalty0 (1):\penalty0
  60--65, 2003.

\bibitem[El~Alaoui \& Mahoney(2014)El~Alaoui and Mahoney]{alaoui2014fast}
El~Alaoui, A. and Mahoney, M.~W.
\newblock Fast randomized kernel methods with statistical guarantees.
\newblock \emph{stat}, 1050:\penalty0 2, 2014.

\bibitem[Haagerup \& Musat(2007)Haagerup and Musat]{haagerup2007best}
Haagerup, U. and Musat, M.
\newblock On the best constants in noncommutative khintchine-type inequalities.
\newblock \emph{Journal of Functional Analysis}, 250\penalty0 (2):\penalty0
  588--624, 2007.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li et~al.(2013)Li, Miller, and Peng]{li2013iterative}
Li, M., Miller, G.~L., and Peng, R.
\newblock Iterative row sampling.
\newblock In \emph{2013 IEEE 54th Annual Symposium on Foundations of Computer
  Science}, pp.\  127--136. IEEE, 2013.

\bibitem[Meister et~al.(2019)Meister, Sarlos, and Woodruff]{meister2019tight}
Meister, M., Sarlos, T., and Woodruff, D.
\newblock Tight dimensionality reduction for sketching low degree polynomial
  kernels.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 9475--9486, 2019.

\bibitem[Musco \& Musco(2017)Musco and Musco]{musco2017recursive}
Musco, C. and Musco, C.
\newblock Recursive sampling for the nystrom method.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3833--3845, 2017.

\bibitem[Pham \& Pagh(2013)Pham and Pagh]{pham2013fast}
Pham, N. and Pagh, R.
\newblock Fast and scalable polynomial kernels via explicit feature maps.
\newblock In \emph{Proceedings of the 19th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  239--247, 2013.

\bibitem[Rahimi \& Recht(2008)Rahimi and Recht]{rahimi2008random}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1177--1184, 2008.

\bibitem[Rahimi \& Recht(2009)Rahimi and Recht]{rahimi2007random}
Rahimi, A. and Recht, B.
\newblock
  {\href{https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf}{Random
  Features for Large-Scale Kernel Machines}}.
\newblock 2009.

\bibitem[Song et~al.(2021)Song, Woodruff, Yu, and Zhang]{song2021fast}
Song, Z., Woodruff, D., Yu, Z., and Zhang, L.
\newblock Fast sketching of polynomial kernels of polynomial degree.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9812--9823. PMLR, 2021.

\bibitem[Williams \& Seeger(2001)Williams and Seeger]{williams2001using}
Williams, C. and Seeger, M.
\newblock Using the nystroem method to speed up kernel machines.
\newblock \emph{Advances in Neural Information Processing Systems 13}, 2001.

\bibitem[Woodruff \& Zandieh(2020)Woodruff and Zandieh]{woodruff2020near}
Woodruff, D. and Zandieh, A.
\newblock Near input sparsity time kernel embeddings via adaptive sampling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10324--10333. PMLR, 2020.

\bibitem[Zandieh et~al.(2021)Zandieh, Han, Avron, Shoham, Kim, and
  Shin]{zandieh2021scaling}
Zandieh, A., Han, I., Avron, H., Shoham, N., Kim, C., and Shin, J.
\newblock Scaling neural tangent kernels via sketching and random features.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=vIRFiA658rh}.

\end{thebibliography}
