\begin{thebibliography}{10}

\bibitem{azizi2021t}
Ahmadreza Azizi, Ibrahim~Asadullah Tahmid, Asim Waheed, Neal Mangaokar, Jiameng
  Pu, Mobin Javed, Chandan~K Reddy, and Bimal Viswanath.
\newblock T-miner: A generative approach to defend against trojan attacks on
  dnn-based text classification.
\newblock In {\em 30th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 21)}, 2021.

\bibitem{chen2021mitigating}
Chuanshuai Chen and Jiazhu Dai.
\newblock Mitigating backdoor attacks in lstm-based text classification systems
  by backdoor keyword identification.
\newblock {\em Neurocomputing}, 452:253--262, 2021.

\bibitem{chen2021badnl}
Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni
  Shen, Zhonghai Wu, and Yang Zhang.
\newblock Badnl: Backdoor attacks against nlp models with semantic-preserving
  improvements.
\newblock In {\em ACSAC}, pages 554--569, 2021.

\bibitem{homoglyph}
Unicode Consortium.
\newblock Confusables.
\newblock \url{https://www.unicode.org/Public/security/13.0.0/}, 2020.

\bibitem{cui2022unified}
Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, and Maosong Sun.
\newblock A unified evaluation of textual backdoor learning: Frameworks and
  benchmarks.
\newblock {\em Advances in Neural Information Processing Systems},
  35:5009--5023, 2022.

\bibitem{dai2019backdoor}
Jiazhu Dai, Chuanshuai Chen, and Yufeng Li.
\newblock A backdoor attack against lstm-based text classification systems.
\newblock {\em IEEE Access}, 7, 2019.

\bibitem{fioraldi2020afl++}
Andrea Fioraldi, Dominik Maier, Heiko Ei{\ss}feldt, and Marc Heuse.
\newblock $\{$AFL++$\}$: Combining incremental steps of fuzzing research.
\newblock In {\em 14th USENIX Workshop on Offensive Technologies (WOOT 20)},
  2020.

\bibitem{fioraldi2022libafl}
Andrea Fioraldi, Dominik~Christian Maier, Dongjia Zhang, and Davide Balzarotti.
\newblock Libafl: A framework to build modular and reusable fuzzers.
\newblock In {\em Proceedings of the 2022 ACM SIGSAC Conference on Computer and
  Communications Security}, pages 1051--1065, 2022.

\bibitem{gao2021design}
Yansong Gao, Yeonjae Kim, Bao~Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal,
  Damith~C Ranasinghe, and Hyoungshick Kim.
\newblock Design and evaluation of a multi-domain trojan detection method on
  deep neural networks.
\newblock {\em IEEE Transactions on Dependable and Secure Computing},
  19(4):2349--2364, 2021.

\bibitem{garg2020can}
Siddhant Garg, Adarsh Kumar, Vibhor Goel, and Yingyu Liang.
\newblock Can adversarial weight perturbations inject neural backdoors.
\newblock In {\em Proceedings of the 29th ACM International Conference on
  Information \& Knowledge Management}, pages 2029--2032, 2020.

\bibitem{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock {\em arXiv preprint arXiv:1708.06733}, 2017.

\bibitem{krishna2023paraphrasing}
Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer.
\newblock Paraphrasing evades detectors of ai-generated text, but retrieval is
  an effective defense.
\newblock {\em arXiv preprint arXiv:2303.13408}, 2023.

\bibitem{kurita2020weight}
Keita Kurita, Paul Michel, and Graham Neubig.
\newblock Weight poisoning attacks on pre-trained models.
\newblock In {\em ACL}, 2020.

\bibitem{li2021backdoor}
Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu.
\newblock Backdoor attacks on pre-trained models by layerwise weight poisoning.
\newblock In {\em EMNLP}, pages 3023--3032, 2021.

\bibitem{li2022backdoors}
Shaofeng Li, Tian Dong, Benjamin Zi~Hao Zhao, Minhui Xue, Suguo Du, and Haojin
  Zhu.
\newblock Backdoors against natural language processing: A review.
\newblock {\em IEEE Security \& Privacy}, 20(05):50--59, 2022.

\bibitem{li2021hidden}
Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi~Hao Zhao, Minhui Xue, Haojin Zhu,
  and Jialiang Lu.
\newblock Hidden backdoors in human-centric language models.
\newblock In {\em CCS}, pages 3123--3140, 2021.

\bibitem{liu2022piccolo}
Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, and Xiangyu
  Zhang.
\newblock Piccolo: Exposing complex backdoors in nlp transformer models.
\newblock In {\em 2022 IEEE Symposium on Security and Privacy (SP)}, pages
  2025--2042. IEEE, 2022.

\bibitem{maas2011learning}
Andrew Maas, Raymond~E Daly, Peter~T Pham, Dan Huang, Andrew~Y Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th annual meeting of the association for
  computational linguistics: Human language technologies}, pages 142--150,
  2011.

\bibitem{ni2019justifying}
Jianmo Ni, Jiacheng Li, and Julian McAuley.
\newblock Justifying recommendations using distantly-labeled reviews and
  fine-grained aspects.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 188--197, 2019.

\bibitem{pan2022hidden}
Xudong Pan, Mi~Zhang, Beina Sheng, Jiaming Zhu, and Min Yang.
\newblock Hidden trigger backdoor attack on {NLP} models via linguistic style
  manipulation.
\newblock In {\em USENIX Security}, pages 3611--3628, 2022.

\bibitem{qi2020onion}
Fanchao Qi, Yangyi Chen, Mukai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Onion: A simple and effective defense against textual backdoor
  attacks.
\newblock {\em arXiv preprint arXiv:2011.10369}, 2020.

\bibitem{qi2021mind}
Fanchao Qi, Yangyi Chen, Xurui Zhang, Mukai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Mind the style of text! adversarial and backdoor attacks based on
  text style transfer.
\newblock In {\em EMNLP}, pages 4569--4580, 2021.

\bibitem{qi2021hidden}
Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng Wang,
  and Maosong Sun.
\newblock Hidden killer: Invisible textual backdoor attacks with syntactic
  trigger.
\newblock In {\em ACL/IJCNLP}, 2021.

\bibitem{qi2021turn}
Fanchao Qi, Yuan Yao, Sophia Xu, Zhiyuan Liu, and Maosong Sun.
\newblock Turn the combination lock: Learnable textual backdoor attacks via
  word substitution.
\newblock In {\em ACL/IJCNLP}, pages 4873--4883, 2021.

\bibitem{sadasivan2023can}
Vinu~Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and
  Soheil Feizi.
\newblock Can ai-generated text be reliably detected?
\newblock {\em arXiv preprint arXiv:2303.11156}, 2023.

\bibitem{shen2022constrained}
Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An,
  Shiqing Ma, and Xiangyu Zhang.
\newblock Constrained optimization with dynamic bound-scaling for effective nlp
  backdoor defense.
\newblock In {\em International Conference on Machine Learning}, pages
  19879--19892. PMLR, 2022.

\bibitem{shen2021backdoor}
Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi,
  Chengfang Fang, Jianwei Yin, and Ting Wang.
\newblock Backdoor pre-trained models can transfer to all.
\newblock {\em arXiv preprint arXiv:2111.00197}, 2021.

\bibitem{sheng2022survey}
Xuan Sheng, Zhaoyang Han, Piji Li, and Xiangmao Chang.
\newblock A survey on backdoor attack and defense in natural language
  processing.
\newblock In {\em 2022 IEEE 22nd International Conference on Software Quality,
  Reliability and Security (QRS)}, pages 809--820. IEEE, 2022.

\bibitem{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem{sutton2007fuzzing}
Michael Sutton, Adam Greene, and Pedram Amini.
\newblock {\em Fuzzing: brute force vulnerability discovery}.
\newblock Pearson Education, 2007.

\bibitem{tang2023science}
Ruixiang Tang, Yu-Neng Chuang, and Xia Hu.
\newblock The science of detecting llm-generated texts.
\newblock {\em arXiv preprint arXiv:2303.07205}, 2023.

\bibitem{varshney2020limits}
Lav~R Varshney, Nitish~Shirish Keskar, and Richard Socher.
\newblock Limits of detecting text generated by large-scale language models.
\newblock In {\em 2020 Information Theory and Applications Workshop (ITA)},
  pages 1--5. IEEE, 2020.

\bibitem{xu2022exploring}
Lei Xu, Yangyi Chen, Ganqu Cui, Hongcheng Gao, and Zhiyuan Liu.
\newblock Exploring the universal vulnerability of prompt-based learning
  paradigm.
\newblock {\em arXiv preprint arXiv:2204.05239}, 2022.

\bibitem{xu2019detecting}
Xiaojun Xu, Qi~Wang, Huichen Li, Nikita Borisov, Carl~A Gunter, and Bo~Li.
\newblock Detecting ai trojans using meta neural analysis.
\newblock {\em arXiv preprint arXiv:1910.03137}, 2019.

\bibitem{yang2021careful}
Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu~Sun, and Bin He.
\newblock Be careful about poisoned word embeddings: Exploring the
  vulnerability of the embedding layers in nlp models.
\newblock {\em arXiv preprint arXiv:2103.15543}, 2021.

\bibitem{yang2021rap}
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu~Sun.
\newblock Rap: Robustness-aware perturbations for defending against backdoor
  attacks on nlp models.
\newblock {\em arXiv preprint arXiv:2110.07831}, 2021.

\bibitem{yang2021rethinking}
Wenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and Xu~Sun.
\newblock Rethinking stealthiness of backdoor attack against nlp models.
\newblock In {\em ACL}, pages 5543--5557, 2021.

\bibitem{zhang2015character}
Xiang Zhang, Junbo Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{zhang2021trojaning}
Xinyang Zhang, Zheng Zhang, Shouling Ji, and Ting Wang.
\newblock Trojaning language models for fun and profit.
\newblock In {\em EuroS\&P}, pages 179--197, 2021.

\bibitem{zhang2023red}
Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan Liu,
  Yasheng Wang, Xin Jiang, and Maosong Sun.
\newblock Red alarm for pre-trained models: Universal vulnerability to
  neuron-level backdoor attacks.
\newblock {\em Machine Intelligence Research}, pages 1--14, 2023.

\bibitem{zhang2021stochfuzz}
Zhuo Zhang, Wei You, Guanhong Tao, Yousra Aafer, Xuwei Liu, and Xiangyu Zhang.
\newblock Stochfuzz: Sound and cost-effective fuzzing of stripped binaries by
  incremental and stochastic rewriting.
\newblock In {\em 2021 IEEE Symposium on Security and Privacy (SP)}, pages
  659--676. IEEE, 2021.

\bibitem{zhu2022moderate}
Biru Zhu, Yujia Qin, Ganqu Cui, Yangyi Chen, Weilin Zhao, Chong Fu, Yangdong
  Deng, Zhiyuan Liu, Jingang Wang, Wei Wu, et~al.
\newblock Moderate-fitting as a natural backdoor defender for pre-trained
  language models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:1086--1099, 2022.

\end{thebibliography}
