\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2018)Achille, Rovere, and Soatto]{achille2018critical}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Aghajanyan et~al.(2020)Aghajanyan, Shrivastava, Gupta, Goyal,
  Zettlemoyer, and Gupta]{RXF}
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke
  Zettlemoyer, and Sonal Gupta.
\newblock Better fine-tuning by reducing representational collapse.
\newblock \emph{arXiv preprint arXiv:2008.03156}, 2020.

\bibitem[Aghajanyan et~al.(2022)Aghajanyan, Huang, Ross, Karpukhin, Xu, Goyal,
  Okhonko, Joshi, Ghosh, Lewis, et~al.]{aghajanyan2022cm3}
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu~Xu, Naman
  Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et~al.
\newblock Cm3: A causal masked multimodal model of the internet.
\newblock \emph{arXiv preprint arXiv:2201.07520}, 2022.

\bibitem[AlKhamissi et~al.(2022)AlKhamissi, Li, Celikyilmaz, Diab, and
  Ghazvininejad]{alkhamissi2022review}
Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan
  Ghazvininejad.
\newblock A review on language models as knowledge bases.
\newblock \emph{arXiv preprint arXiv:2204.06031}, 2022.

\bibitem[Amiri et~al.(2017)Amiri, Miller, and Savova]{amiri2017repeat}
Hadi Amiri, Timothy Miller, and Guergana Savova.
\newblock Repeat before forgetting: Spaced repetition for efficient and
  effective training of neural networks.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 2401--2410, 2017.

\bibitem[Arakelyan et~al.(2020)Arakelyan, Soghomonyan, and {The Aim
  team}]{Arakelyan_Aim_2020}
Gor Arakelyan, Gevorg Soghomonyan, and {The Aim team}.
\newblock {Aim}, 6 2020.
\newblock URL \url{https://github.com/aimhubio/aim}.

\bibitem[Artetxe et~al.(2021)Artetxe, Bhosale, Goyal, Mihaylov, Ott, Shleifer,
  Lin, Du, Iyer, Pasunuru, et~al.]{efficient_lm_xlm_g}
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam
  Shleifer, Xi~Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,
  et~al.
\newblock Efficient large scale language modeling with mixtures of experts.
\newblock \emph{arXiv preprint arXiv:2112.10684}, 2021.

\bibitem[Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and
  Sharma]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}, 2021.

\bibitem[Baines et~al.(2021)Baines, Bhosale, Caggiano, Goyal, Goyal, Ott,
  Lefaudeux, Liptchinsky, Rabbat, Sheiffer, Sridhar, and Xu]{fairscale}
Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth
  Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam
  Sheiffer, Anjali Sridhar, and Min Xu.
\newblock Fairscale: A general purpose modular pytorch library for high
  performance and large scale training.
\newblock \url{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem[Blevins et~al.(2022)Blevins, Gonen, and
  Zettlemoyer]{blevins2022analyzing}
Terra Blevins, Hila Gonen, and Luke Zettlemoyer.
\newblock Analyzing the mono-and cross-lingual pretraining dynamics of
  multilingual language models.
\newblock \emph{arXiv preprint arXiv:2205.11758}, 2022.

\bibitem[Borgeaud et~al.(2021)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, Driessche, Lespiau, Damoc, Clark, et~al.]{borgeaud2021improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
  Rutherford, Katie Millican, George van~den Driessche, Jean-Baptiste Lespiau,
  Bogdan Damoc, Aidan Clark, et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock \emph{arXiv preprint arXiv:2112.04426}, 2021.

\bibitem[Bourtoule et~al.(2021)Bourtoule, Chandrasekaran, Choquette-Choo, Jia,
  Travers, Zhang, Lie, and Papernot]{bourtoule2021machine}
Lucas Bourtoule, Varun Chandrasekaran, Christopher~A Choquette-Choo, Hengrui
  Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot.
\newblock Machine unlearning.
\newblock In \emph{2021 IEEE Symposium on Security and Privacy (SP)}, pages
  141--159. IEEE, 2021.

\bibitem[Brown et~al.(2021)Brown, Bun, Feldman, Smith, and
  Talwar]{brown2021memorization}
Gavin Brown, Mark Bun, Vitaly Feldman, Adam Smith, and Kunal Talwar.
\newblock When is memorization of irrelevant training data necessary for
  high-accuracy learning?
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 123--132, 2021.

\bibitem[Carlini et~al.(2019)Carlini, Liu, Erlingsson, Kos, and
  Song]{carlini2019secret}
Nicholas Carlini, Chang Liu, {\'U}lfar Erlingsson, Jernej Kos, and Dawn Song.
\newblock The secret sharer: Evaluating and testing unintended memorization in
  neural networks.
\newblock In \emph{28th USENIX Security Symposium (USENIX Security 19)}, pages
  267--284, 2019.

\bibitem[Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss,
  Lee, Roberts, Brown, Song, Erlingsson, et~al.]{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
  Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pages
  2633--2650, 2021.

\bibitem[Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tramer, and
  Zhang]{carlini2022quantifying}
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
  Tramer, and Chiyuan Zhang.
\newblock Quantifying memorization across neural language models.
\newblock \emph{arXiv preprint arXiv:2202.07646}, 2022.

\bibitem[Chang and Bergen(2022)]{chang-bergen-2022-word}
Tyler~A. Chang and Benjamin~K. Bergen.
\newblock Word acquisition in neural language models.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 1--16, 2022.
\newblock \doi{10.1162/tacl_a_00444}.
\newblock URL \url{https://aclanthology.org/2022.tacl-1.1}.

\bibitem[Chen et~al.(2020)Chen, Hou, Cui, Che, Liu, and Yu]{chen2020recall}
Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu.
\newblock Recall and learn: Fine-tuning deep pretrained language models with
  less forgetting.
\newblock \emph{arXiv preprint arXiv:2004.12651}, 2020.

\bibitem[Chen and Liu(2018)]{chen2018lifelong}
Zhiyuan Chen and Bing Liu.
\newblock Lifelong machine learning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 12\penalty0 (3):\penalty0 1--207, 2018.

\bibitem[Chiang et~al.(2020)Chiang, Huang, and Lee]{chiang2020pretrained}
Cheng-Han Chiang, Sung-Feng Huang, and Hung-yi Lee.
\newblock Pretrained language model embryology: The birth of albert.
\newblock \emph{arXiv preprint arXiv:2010.02480}, 2020.

\bibitem[Choshen et~al.(2021)Choshen, Hacohen, Weinshall, and
  Abend]{choshen2021grammar}
Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend.
\newblock The grammar-learning trajectories of neural language models.
\newblock \emph{arXiv preprint arXiv:2109.06096}, 2021.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Clark et~al.(2022)Clark, Casas, Guy, Mensch, Paganini, Hoffmann,
  Damoc, Hechtman, Cai, Borgeaud, et~al.]{clark2022unified}
Aidan Clark, Diego de~las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini,
  Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian
  Borgeaud, et~al.
\newblock Unified scaling laws for routed language models.
\newblock \emph{arXiv preprint arXiv:2202.01169}, 2022.

\bibitem[Delange et~al.(2021)Delange, Aljundi, Masana, Parisot, Jia, Leonardis,
  Slabaugh, and Tuytelaars]{delange2021continual}
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu~Jia, Ales
  Leonardis, Greg Slabaugh, and Tinne Tuytelaars.
\newblock A continual learning survey: Defying forgetting in classification
  tasks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and
  Smith]{dwork2006calibrating}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In \emph{Theory of cryptography conference}, pages 265--284.
  Springer, 2006.

\bibitem[Feldman(2019)]{feldman2019does}
Vitaly Feldman.
\newblock Does learning require memorization.
\newblock \emph{A short tale about a long tail. CoRR, abs/1906.05271}, 2019.

\bibitem[Feldman(2021)]{feldman2021DoesLearningRequire}
Vitaly Feldman.
\newblock Does {Learning} {Require} {Memorization}? {A} {Short} {Tale} about a
  {Long} {Tail}.
\newblock \emph{arXiv:1906.05271 [cs, stat]}, January 2021.
\newblock URL \url{http://arxiv.org/abs/1906.05271}.
\newblock arXiv: 1906.05271.

\bibitem[Feldman and Zhang(2020)]{feldman2020neural}
Vitaly Feldman and Chiyuan Zhang.
\newblock What neural networks memorize and why: Discovering the long tail via
  influence estimation.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2881--2891, 2020.

\bibitem[Fleischman and Roy(2005)]{fleischman2005verbs}
Michael Fleischman and Deb Roy.
\newblock Why verbs are harder to learn than nouns: Initial insights from a
  computational model of intention recognition in situated word learning.
\newblock In \emph{27th Annual Meeting of the Cognitive Science Society,
  Stresa, Italy}, 2005.

\bibitem[Frankle et~al.(2020)Frankle, Schwab, and Morcos]{frankle2020early}
Jonathan Frankle, David~J Schwab, and Ari~S Morcos.
\newblock The early phase of neural network training.
\newblock \emph{arXiv preprint arXiv:2002.10365}, 2020.

\bibitem[Franklin(2005)]{franklin2005elements}
James Franklin.
\newblock The elements of statistical learning: data mining, inference and
  prediction.
\newblock \emph{The Mathematical Intelligencer}, 27\penalty0 (2):\penalty0
  83--85, 2005.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language
  models.
\newblock \emph{arXiv preprint arXiv:2009.11462}, 2020.

\bibitem[Goyal et~al.(2021)Goyal, Xu, Li, and Durrett]{goyal2021training}
Tanya Goyal, Jiacheng Xu, Junyi~Jessy Li, and Greg Durrett.
\newblock Training dynamics for text summarization models.
\newblock \emph{arXiv preprint arXiv:2110.08370}, 2021.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur2018gradient}
Guy Gur-Ari, Daniel~A Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020realm}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training.
\newblock \emph{arXiv preprint arXiv:2002.08909}, 2020.

\bibitem[Hao et~al.(2020)Hao, Dong, Wei, and Xu]{hao-etal-2020-investigating}
Yaru Hao, Li~Dong, Furu Wei, and Ke~Xu.
\newblock Investigating learning dynamics of {BERT} fine-tuning.
\newblock In \emph{Proceedings of the 1st Conference of the Asia-Pacific
  Chapter of the Association for Computational Linguistics and the 10th
  International Joint Conference on Natural Language Processing}, pages 87--92,
  Suzhou, China, December 2020. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2020.aacl-main.11}.

\bibitem[Harding et~al.(2019)Harding, Vanto, Clark, Hannah~Ji, and
  Ainsworth]{harding2019understanding}
Elizabeth~Liz Harding, Jarno~J Vanto, Reece Clark, L~Hannah~Ji, and Sara~C
  Ainsworth.
\newblock Understanding the scope and impact of the california consumer privacy
  act of 2018.
\newblock \emph{Journal of Data Protection \& Privacy}, 2\penalty0
  (3):\penalty0 234--253, 2019.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,
  Jun, Brown, Dhariwal, Gray, et~al.]{henighan2020scaling}
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
  Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{arXiv preprint arXiv:2010.14701}, 2020.

\bibitem[Hernandez et~al.(2021)Hernandez, Kaplan, Henighan, and
  McCandlish]{hernandez2021scaling}
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish.
\newblock Scaling laws for transfer.
\newblock \emph{arXiv preprint arXiv:2102.01293}, 2021.

\bibitem[Hisamoto et~al.(2019)Hisamoto, Post, and Duh]{hisamoto2019membership}
Sorami Hisamoto, Matt Post, and Kevin Duh.
\newblock Membership inference attacks on sequence-to-sequence models.
\newblock \emph{arXiv preprint arXiv:1904.05506}, 2019.

\bibitem[Honnibal and Montani(2022)]{spacy3}
Matthew Honnibal and Ines Montani.
\newblock {spaCy 3}: Natural language understanding with {B}loom embeddings,
  convolutional neural networks and incremental parsing.
\newblock To appear, 2022.

\bibitem[Ji et~al.(2022)Ji, Lee, Frieske, Yu, Su, Xu, Ishii, Bang, Madotto, and
  Fung]{ji2022survey}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
  Yejin Bang, Andrea Madotto, and Pascale Fung.
\newblock Survey of hallucination in natural language generation.
\newblock \emph{arXiv preprint arXiv:2202.03629}, 2022.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020ScalingLawsNeural}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling {Laws} for {Neural} {Language} {Models}.
\newblock \emph{arXiv:2001.08361 [cs, stat]}, January 2020.
\newblock URL \url{http://arxiv.org/abs/2001.08361}.
\newblock arXiv: 2001.08361.

\bibitem[Karpicke and Roediger~III(2007)]{karpicke2007expanding}
Jeffrey~D Karpicke and Henry~L Roediger~III.
\newblock Expanding retrieval practice promotes short-term retention, but
  equally spaced retrieval enhances long-term retention.
\newblock \emph{Journal of experimental psychology: learning, memory, and
  cognition}, 33\penalty0 (4):\penalty0 704, 2007.

\bibitem[Khandelwal et~al.(2019)Khandelwal, Levy, Jurafsky, Zettlemoyer, and
  Lewis]{khandelwal2019generalization}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
\newblock Generalization through memorization: Nearest neighbor language
  models.
\newblock \emph{arXiv preprint arXiv:1911.00172}, 2019.

\bibitem[Kharitonov et~al.(2021)Kharitonov, Baroni, and
  Hupkes]{kharitonov2021bpe}
Eugene Kharitonov, Marco Baroni, and Dieuwke Hupkes.
\newblock How bpe affects memorization in transformers.
\newblock \emph{arXiv preprint arXiv:2110.02782}, 2021.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Lee et~al.(2021)Lee, Ippolito, Nystrom, Zhang, Eck, Callison-Burch,
  and Carlini]{lee2021deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
  Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock \emph{arXiv preprint arXiv:2107.06499}, 2021.

\bibitem[Li et~al.(2021)Li, Tramer, Liang, and Hashimoto]{li2021large}
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto.
\newblock Large language models can be strong differentially private learners.
\newblock \emph{arXiv preprint arXiv:2110.05679}, 2021.

\bibitem[Li et~al.(2020)Li, Wallace, Shen, Lin, Keutzer, Klein, and
  Gonzalez]{li2020train}
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
  Joey Gonzalez.
\newblock Train big, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock In \emph{International Conference on Machine Learning}, pages
  5958--5968. PMLR, 2020.

\bibitem[Liu et~al.(2021)Liu, Wang, Kasai, Hajishirzi, and
  Smith]{liu2021probing}
Leo~Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah~A Smith.
\newblock Probing across time: What does roberta know and when?
\newblock \emph{arXiv preprint arXiv:2104.07885}, 2021.

\bibitem[Liu et~al.(2020)Liu, Ma, Liu, Liu, Jiang, Ma, Yu, and
  Ren]{liu2020learn}
Yang Liu, Zhuo Ma, Ximeng Liu, Jian Liu, Zhongyuan Jiang, Jianfeng Ma, Philip
  Yu, and Kui Ren.
\newblock Learn to forget: Machine unlearning via neuron masking.
\newblock \emph{arXiv preprint arXiv:2003.10933}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019RoBERTaRobustlyOptimized}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining}
  {Approach}.
\newblock \emph{arXiv e-prints}, July 2019.
\newblock URL \url{https://arxiv.org/abs/1907.11692v1}.

\bibitem[Loftus(1985)]{loftus1985evaluating}
Geoffrey~R Loftus.
\newblock Evaluating forgetting curves.
\newblock \emph{Journal of Experimental Psychology: Learning, Memory, and
  Cognition}, 11\penalty0 (2):\penalty0 397, 1985.

\bibitem[Mantelero(2013)]{mantelero2013eu}
Alessandro Mantelero.
\newblock The eu proposal for a general data protection regulation and the
  roots of the ‘right to be forgotten’.
\newblock \emph{Computer Law \& Security Review}, 29\penalty0 (3):\penalty0
  229--235, 2013.

\bibitem[Masarczyk et~al.(2021)Masarczyk, Deja, and
  Trzcinski]{masarczyk2021robustness}
Wojciech Masarczyk, Kamil Deja, and Tomasz Trzcinski.
\newblock On robustness of generative representations against catastrophic
  forgetting.
\newblock In \emph{International Conference on Neural Information Processing},
  pages 325--333. Springer, 2021.

\bibitem[McCloskey and Cohen(1989)]{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem[McCoy et~al.(2021)McCoy, Smolensky, Linzen, Gao, and
  Celikyilmaz]{mccoy2021much}
R~Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz.
\newblock How much do language models copy from their training data? evaluating
  linguistic novelty in text generation using raven.
\newblock \emph{arXiv preprint arXiv:2111.09509}, 2021.

\bibitem[Merchant et~al.(2020)Merchant, Rahimtoroghi, Pavlick, and
  Tenney]{merchant-etal-2020-happens}
Amil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and Ian Tenney.
\newblock What happens to {BERT} embeddings during fine-tuning?
\newblock In \emph{Proceedings of the Third BlackboxNLP Workshop on Analyzing
  and Interpreting Neural Networks for NLP}, pages 33--44, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.blackboxnlp-1.4}.
\newblock URL \url{https://aclanthology.org/2020.blackboxnlp-1.4}.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{Merity2017PointerSM}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{ArXiv}, abs/1609.07843, 2017.

\bibitem[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh,
  et~al.]{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, et~al.
\newblock Mixed precision training.
\newblock \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem[Mireshghallah et~al.(2022)Mireshghallah, Goyal, Uniyal,
  Berg-Kirkpatrick, and Shokri]{mireshghallah2022quantifying}
Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal, Taylor
  Berg-Kirkpatrick, and Reza Shokri.
\newblock Quantifying privacy risks of masked language models using membership
  inference attacks.
\newblock \emph{arXiv preprint arXiv:2203.03929}, 2022.

\bibitem[Mirzadeh et~al.(2021)Mirzadeh, Chaudhry, Hu, Pascanu, Gorur, and
  Farajtabar]{mirzadeh2021wide}
Seyed~Iman Mirzadeh, Arslan Chaudhry, Huiyi Hu, Razvan Pascanu, Dilan Gorur,
  and Mehrdad Farajtabar.
\newblock Wide neural networks forget less catastrophically.
\newblock \emph{arXiv preprint arXiv:2110.11526}, 2021.

\bibitem[Morcos et~al.(2018)Morcos, Raghu, and Bengio]{morcos2018insights}
Ari Morcos, Maithra Raghu, and Samy Bengio.
\newblock Insights on representational similarity in neural networks with
  canonical correlation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2021deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (12):\penalty0 124003, 2021.

\bibitem[Oren et~al.(2014)Oren, Willerton, and Small]{oren2014effects}
Shiri Oren, Charlene Willerton, and Jeff Small.
\newblock Effects of spaced retrieval training on semantic memory in
  alzheimer's disease: A systematic review.
\newblock \emph{Journal of Speech, Language and Hearing Research (Online)},
  57\penalty0 (1):\penalty0 247, 2014.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1904.01038}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 8026--8037, 2019.

\bibitem[Petroni et~al.(2019)Petroni, Rockt{\"a}schel, Lewis, Bakhtin, Wu,
  Miller, and Riedel]{petroni2019language}
Fabio Petroni, Tim Rockt{\"a}schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
  Alexander~H Miller, and Sebastian Riedel.
\newblock Language models as knowledge bases?
\newblock \emph{arXiv preprint arXiv:1909.01066}, 2019.

\bibitem[Pondenkandath et~al.(2018)Pondenkandath, Alberti, Puran, Ingold, and
  Liwicki]{pondenkandath2018leveraging}
Vinaychandran Pondenkandath, Michele Alberti, Sammer Puran, Rolf Ingold, and
  Marcus Liwicki.
\newblock Leveraging random label memorization for unsupervised pre-training.
\newblock \emph{arXiv preprint arXiv:1811.01640}, 2018.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Raghu et~al.(2017)Raghu, Gilmer, Yosinski, and
  Sohl-Dickstein]{raghu2017svcca}
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein.
\newblock Svcca: Singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ramasesh et~al.(2021)Ramasesh, Lewkowycz, and
  Dyer]{ramasesh2021effect}
Vinay~Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer.
\newblock Effect of scale on catastrophic forgetting in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem[Ratcliff(1990)]{Ratcliff90connectionistmodels}
Roger Ratcliff.
\newblock Connectionist models of recognition memory: Constraints imposed by
  learning and forgetting functions.
\newblock \emph{Psychological Review}, pages 285--308, 1990.

\bibitem[Regulation(2018)]{regulation2018general}
General Data~Protection Regulation.
\newblock General data protection regulation (gdpr).
\newblock \emph{Intersoft Consulting, Accessed in October}, 24\penalty0 (1),
  2018.

\bibitem[Rosenfeld et~al.(2019)Rosenfeld, Rosenfeld, Belinkov, and
  Shavit]{rosenfeld2019constructive}
Jonathan~S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales.
\newblock \emph{arXiv preprint arXiv:1909.12673}, 2019.

\bibitem[Saphra and Lopez(2018)]{saphra2018understanding}
Naomi Saphra and Adam Lopez.
\newblock Understanding learning dynamics of language models with svcca.
\newblock \emph{arXiv preprint arXiv:1811.00225}, 2018.

\bibitem[Savoldi et~al.(2022)Savoldi, Gaido, Bentivogli, Negri, and
  Turchi]{savoldi-etal-2022-dynamics}
Beatrice Savoldi, Marco Gaido, Luisa Bentivogli, Matteo Negri, and Marco
  Turchi.
\newblock On the dynamics of gender learning in speech translation.
\newblock In \emph{Proceedings of the 4th Workshop on Gender Bias in Natural
  Language Processing (GeBNLP)}, pages 94--111, Seattle, Washington, July 2022.
  Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.gebnlp-1.12}.

\bibitem[Shao and Feng(2022)]{shao2022overcoming}
Chenze Shao and Yang Feng.
\newblock Overcoming catastrophic forgetting beyond continual learning:
  Balanced training for neural machine translation.
\newblock \emph{arXiv preprint arXiv:2203.03910}, 2022.

\bibitem[Smith et~al.(2022)Smith, Patwary, Norick, LeGresley, Rajbhandari,
  Casper, Liu, Prabhumoye, Zerveas, Korthikanti, et~al.]{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, et~al.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2201.11990}, 2022.

\bibitem[Smolen et~al.(2016)Smolen, Zhang, and Byrne]{smolen2016right}
Paul Smolen, Yili Zhang, and John~H Byrne.
\newblock The right time to learn: mechanisms and optimization of spaced
  learning.
\newblock \emph{Nature Reviews Neuroscience}, 17\penalty0 (2):\penalty0 77--88,
  2016.

\bibitem[Song and Shmatikov(2019)]{song2019auditing}
Congzheng Song and Vitaly Shmatikov.
\newblock Auditing data provenance in text-generation models.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 196--206, 2019.

\bibitem[Stadler et~al.(2021)Stadler, Macketanz, and
  Avramidis]{stadler2021observing}
Patrick Stadler, Vivien Macketanz, and Eleftherios Avramidis.
\newblock Observing the learning curve of nmt systems with regard to linguistic
  phenomena.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing: Student Research Workshop}, pages 186--196,
  2021.

\bibitem[Tay et~al.(2022)Tay, Tran, Dehghani, Ni, Bahri, Mehta, Qin, Hui, Zhao,
  Gupta, et~al.]{tay2022transformer}
Yi~Tay, Vinh~Q Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen
  Qin, Kai Hui, Zhe Zhao, Jai Gupta, et~al.
\newblock Transformer memory as a differentiable search index.
\newblock \emph{arXiv preprint arXiv:2202.06991}, 2022.

\bibitem[Thakkar et~al.(2020)Thakkar, Ramaswamy, Mathews, and
  Beaufays]{thakkar2020understanding}
Om~Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Fran{\c{c}}oise Beaufays.
\newblock Understanding unintended memorization in federated learning.
\newblock \emph{arXiv preprint arXiv:2006.07490}, 2020.

\bibitem[Thomas et~al.(2020)Thomas, Adelani, Davody, Mogadala, and
  Klakow]{thomas2020investigating}
Aleena Thomas, David~Ifeoluwa Adelani, Ali Davody, Aditya Mogadala, and
  Dietrich Klakow.
\newblock Investigating the impact of pre-trained word embeddings on
  memorization in neural networks.
\newblock In \emph{International Conference on Text, Speech, and Dialogue},
  pages 273--281. Springer, 2020.

\bibitem[Toneva et~al.(2018)Toneva, Sordoni, Combes, Trischler, Bengio, and
  Gordon]{toneva2018empirical}
Mariya Toneva, Alessandro Sordoni, Remi Tachet~des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock \emph{arXiv preprint arXiv:1812.05159}, 2018.

\bibitem[Voigt and Von~dem Bussche(2017)]{voigt2017eu}
Paul Voigt and Axel Von~dem Bussche.
\newblock The eu general data protection regulation (gdpr).
\newblock \emph{A Practical Guide, 1st Ed., Cham: Springer International
  Publishing}, 10\penalty0 (3152676):\penalty0 10--5555, 2017.

\bibitem[Voita et~al.(2021)Voita, Sennrich, and
  Titov]{voita-etal-2021-analyzing}
Elena Voita, Rico Sennrich, and Ivan Titov.
\newblock Analyzing the source and target contributions to predictions in
  neural machine translation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 1126--1140,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.91}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.91}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017UnderstandingDeepLearning}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv:1611.03530 [cs]}, February 2017.
\newblock URL \url{http://arxiv.org/abs/1611.03530}.
\newblock arXiv: 1611.03530.

\bibitem[Zhang et~al.(2021)Zhang, Ippolito, Lee, Jagielski, Tramèr, and
  Carlini]{zhang2021CounterfactualMemorizationNeural}
Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian
  Tramèr, and Nicholas Carlini.
\newblock Counterfactual {Memorization} in {Neural} {Language} {Models}.
\newblock \emph{arXiv:2112.12938 [cs]}, December 2021.
\newblock URL \url{http://arxiv.org/abs/2112.12938}.
\newblock arXiv: 2112.12938 version: 1.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\end{thebibliography}
