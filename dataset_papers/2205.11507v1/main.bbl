\begin{thebibliography}{47}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and
  Szepesv{\'a}ri}]{abbasi2011improved}
\textsc{Abbasi-Yadkori, Y.}, \textsc{P{\'a}l, D.} and \textsc{Szepesv{\'a}ri,
  C.} (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Auer(2002)}]{auer2002using}
\textsc{Auer, P.} (2002).
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock \textit{Journal of Machine Learning Research} \textbf{3} 397--422.

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and
  Yang}]{ayoub2020model}
\textsc{Ayoub, A.}, \textsc{Jia, Z.}, \textsc{Szepesvari, C.}, \textsc{Wang,
  M.} and \textsc{Yang, L.~F.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \textit{arXiv preprint arXiv:2006.01107} .

\bibitem[{Azuma(1967)}]{azuma1967weighted}
\textsc{Azuma, K.} (1967).
\newblock Weighted sums of certain dependent random variables.
\newblock \textit{Tohoku Mathematical Journal, Second Series} \textbf{19}
  357--367.

\bibitem[{Cai et~al.(2019)Cai, Yang, Jin and Wang}]{cai2019provably}
\textsc{Cai, Q.}, \textsc{Yang, Z.}, \textsc{Jin, C.} and \textsc{Wang, Z.}
  (2019).
\newblock Provably efficient exploration in policy optimization.
\newblock \textit{arXiv preprint arXiv:1912.05830} .

\bibitem[{Chu et~al.(2011)Chu, Li, Reyzin and Schapire}]{chu2011contextual}
\textsc{Chu, W.}, \textsc{Li, L.}, \textsc{Reyzin, L.} and \textsc{Schapire,
  R.} (2011).
\newblock Contextual bandits with linear payoff functions.
\newblock In \textit{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}.

\bibitem[{Dani et~al.(2008)Dani, Hayes and Kakade}]{dani2008stochastic}
\textsc{Dani, V.}, \textsc{Hayes, T.~P.} and \textsc{Kakade, S.~M.} (2008).
\newblock Stochastic linear optimization under bandit feedback.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{dann2018oracle}
\textsc{Dann, C.}, \textsc{Jiang, N.}, \textsc{Krishnamurthy, A.},
  \textsc{Agarwal, A.}, \textsc{Langford, J.} and \textsc{Schapire, R.~E.}
  (2018).
\newblock On oracle-efficient pac rl with rich observations.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Du et~al.(2019)Du, Kakade, Wang and Yang}]{du2019good}
\textsc{Du, S.~S.}, \textsc{Kakade, S.~M.}, \textsc{Wang, R.} and \textsc{Yang,
  L.~F.} (2019).
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Dzhaparidze and Van~Zanten(2001)}]{dzhaparidze2001bernstein}
\textsc{Dzhaparidze, K.} and \textsc{Van~Zanten, J.} (2001).
\newblock On bernstein-type inequalities for martingales.
\newblock \textit{Stochastic processes and their applications} \textbf{93}
  109--117.

\bibitem[{Fan et~al.(2017)Fan, Grama and Liu}]{fan2017martingale}
\textsc{Fan, X.}, \textsc{Grama, I.} and \textsc{Liu, Q.} (2017).
\newblock Martingale inequalities of type dzhaparidze and van zanten.
\newblock \textit{Statistics} \textbf{51} 1200--1213.

\bibitem[{He et~al.(2021)He, Zhou and Gu}]{he2020logarithmic}
\textsc{He, J.}, \textsc{Zhou, D.} and \textsc{Gu, Q.} (2021).
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{He et~al.(2022)He, Zhou, Zhang and Gu}]{he2022corrupt}
\textsc{He, J.}, \textsc{Zhou, D.}, \textsc{Zhang, T.} and \textsc{Gu, Q.}
  (2022).
\newblock Nearly optimal algorithms for linear contextual bandits with
  adversarial corruptions.
\newblock \textit{arXiv preprint arXiv:2205.06811} .

\bibitem[{Jia et~al.(2020)Jia, Yang, Szepesvari and Wang}]{jia2020model}
\textsc{Jia, Z.}, \textsc{Yang, L.}, \textsc{Szepesvari, C.} and \textsc{Wang,
  M.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \textit{L4DC}.

\bibitem[{Jiang and Agarwal(2018)}]{jiang2018open}
\textsc{Jiang, N.} and \textsc{Agarwal, A.} (2018).
\newblock Open problem: The dependence of sample complexity lower bounds on
  planning horizon.
\newblock In \textit{Conference On Learning Theory}. PMLR.

\bibitem[{Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford and
  Schapire}]{jiang2017contextual}
\textsc{Jiang, N.}, \textsc{Krishnamurthy, A.}, \textsc{Agarwal, A.},
  \textsc{Langford, J.} and \textsc{Schapire, R.~E.} (2017).
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In \textit{Proceedings of the 34th International Conference on
  Machine Learning-Volume 70}. JMLR. org.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2019provably}
\textsc{Jin, C.}, \textsc{Yang, Z.}, \textsc{Wang, Z.} and \textsc{Jordan,
  M.~I.} (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Kim et~al.(2021)Kim, Yang and Jun}]{kim2021improved}
\textsc{Kim, Y.}, \textsc{Yang, I.} and \textsc{Jun, K.-S.} (2021).
\newblock Improved regret analysis for variance-adaptive linear bandits and
  horizon-free linear mixture mdps.
\newblock \textit{arXiv preprint arXiv:2111.03289} .

\bibitem[{Kirschner and Krause(2018)}]{kirschner2018information}
\textsc{Kirschner, J.} and \textsc{Krause, A.} (2018).
\newblock Information directed sampling and bandits with heteroscedastic noise.
\newblock In \textit{Conference On Learning Theory}.

\bibitem[{Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal and
  Langford}]{krishnamurthy2016pac}
\textsc{Krishnamurthy, A.}, \textsc{Agarwal, A.} and \textsc{Langford, J.}
  (2016).
\newblock Pac reinforcement learning with rich observations.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{29}.

\bibitem[{Lattimore et~al.(2015)Lattimore, Crammer and
  Szepesv{\'a}ri}]{LaCrSze15}
\textsc{Lattimore, T.}, \textsc{Crammer, K.} and \textsc{Szepesv{\'a}ri, C.}
  (2015).
\newblock Linear multi-resource allocation with semi-bandit feedback.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Lattimore and Hutter(2012)}]{lattimore2012pac}
\textsc{Lattimore, T.} and \textsc{Hutter, M.} (2012).
\newblock {PAC} bounds for discounted {MDP}s.
\newblock In \textit{International Conference on Algorithmic Learning Theory}.
  Springer.

\bibitem[{Li et~al.(2020)Li, Wei, Chi, Gu and Chen}]{li2020breaking}
\textsc{Li, G.}, \textsc{Wei, Y.}, \textsc{Chi, Y.}, \textsc{Gu, Y.} and
  \textsc{Chen, Y.} (2020).
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock \textit{Advances in neural information processing systems}
  \textbf{33} 12861--12872.

\bibitem[{Li et~al.(2010)Li, Chu, Langford and Schapire}]{li2010contextual}
\textsc{Li, L.}, \textsc{Chu, W.}, \textsc{Langford, J.} and \textsc{Schapire,
  R.~E.} (2010).
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock In \textit{Proceedings of the 19th international conference on World
  wide web}.

\bibitem[{Li et~al.(2022)Li, Wang and Yang}]{li2022settling}
\textsc{Li, Y.}, \textsc{Wang, R.} and \textsc{Yang, L.~F.} (2022).
\newblock Settling the horizon-dependence of sample complexity in reinforcement
  learning.
\newblock In \textit{2021 IEEE 62nd Annual Symposium on Foundations of Computer
  Science (FOCS)}. IEEE.

\bibitem[{Li et~al.(2019{\natexlab{a}})Li, Wang and Zhou}]{li2019nearly}
\textsc{Li, Y.}, \textsc{Wang, Y.} and \textsc{Zhou, Y.} (2019{\natexlab{a}}).
\newblock Nearly minimax-optimal regret for linearly parameterized bandits.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Li et~al.(2019{\natexlab{b}})Li, Wang and Zhou}]{li2019tight}
\textsc{Li, Y.}, \textsc{Wang, Y.} and \textsc{Zhou, Y.} (2019{\natexlab{b}}).
\newblock Tight regret bounds for infinite-armed linear contextual bandits.
\newblock \textit{arXiv preprint arXiv:1905.01435} .

\bibitem[{Modi et~al.(2020)Modi, Jiang, Tewari and Singh}]{modi2019sample}
\textsc{Modi, A.}, \textsc{Jiang, N.}, \textsc{Tewari, A.} and \textsc{Singh,
  S.} (2020).
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}. PMLR.

\bibitem[{Puterman(2014)}]{puterman2014Markov}
\textsc{Puterman, M.~L.} (2014).
\newblock \textit{{M}arkov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons.

\bibitem[{Ren et~al.(2021)Ren, Li, Dai, Du and Sanghavi}]{ren2021nearly}
\textsc{Ren, T.}, \textsc{Li, J.}, \textsc{Dai, B.}, \textsc{Du, S.~S.} and
  \textsc{Sanghavi, S.} (2021).
\newblock Nearly horizon-free offline reinforcement learning.
\newblock \textit{Advances in neural information processing systems}
  \textbf{34}.

\bibitem[{Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal and
  Langford}]{sun2019model}
\textsc{Sun, W.}, \textsc{Jiang, N.}, \textsc{Krishnamurthy, A.},
  \textsc{Agarwal, A.} and \textsc{Langford, J.} (2019).
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and
  exponential improvements over model-free approaches.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Tarbouriech et~al.(2021)Tarbouriech, Zhou, Du, Pirotta, Valko and
  Lazaric}]{tarbouriech2021stochastic}
\textsc{Tarbouriech, J.}, \textsc{Zhou, R.}, \textsc{Du, S.~S.},
  \textsc{Pirotta, M.}, \textsc{Valko, M.} and \textsc{Lazaric, A.} (2021).
\newblock Stochastic shortest path: Minimax, parameter-free and towards
  horizon-free regret.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{34}.

\bibitem[{Wang et~al.(2020)Wang, Du, Yang and Kakade}]{wang2020long}
\textsc{Wang, R.}, \textsc{Du, S.~S.}, \textsc{Yang, L.~F.} and \textsc{Kakade,
  S.~M.} (2020).
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock \textit{arXiv preprint arXiv:2005.00527} .

\bibitem[{Wang et~al.(2019)Wang, Wang, Du and Krishnamurthy}]{wang2019optimism}
\textsc{Wang, Y.}, \textsc{Wang, R.}, \textsc{Du, S.~S.} and
  \textsc{Krishnamurthy, A.} (2019).
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock \textit{arXiv preprint arXiv:1912.04136} .

\bibitem[{Weisz et~al.(2020)Weisz, Amortila and
  Szepesv{\'a}ri}]{weisz2020exponential}
\textsc{Weisz, G.}, \textsc{Amortila, P.} and \textsc{Szepesv{\'a}ri, C.}
  (2020).
\newblock Exponential lower bounds for planning in {MDP}s with
  linearly-realizable optimal action-value functions.
\newblock \textit{arXiv preprint arXiv:2010.01374} .

\bibitem[{Yang and Wang(2019{\natexlab{a}})}]{yang2019sample}
\textsc{Yang, L.} and \textsc{Wang, M.} (2019{\natexlab{a}}).
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Yang and Wang(2019{\natexlab{b}})}]{yang2019reinforcement}
\textsc{Yang, L.~F.} and \textsc{Wang, M.} (2019{\natexlab{b}}).
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \textit{arXiv preprint arXiv:1905.10389} .

\bibitem[{Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta and Lazaric}]{zanette2020frequentist}
\textsc{Zanette, A.}, \textsc{Brandfonbrener, D.}, \textsc{Brunskill, E.},
  \textsc{Pirotta, M.} and \textsc{Lazaric, A.} (2020{\natexlab{a}}).
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \textit{International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[{Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer and
  Brunskill}]{zanette2020learning}
\textsc{Zanette, A.}, \textsc{Lazaric, A.}, \textsc{Kochenderfer, M.} and
  \textsc{Brunskill, E.} (2020{\natexlab{b}}).
\newblock Learning near optimal policies with low inherent {B}ellman error.
\newblock \textit{arXiv preprint arXiv:2003.00153} .

\bibitem[{Zhang et~al.(2021{\natexlab{a}})Zhang, Zhou and Gu}]{zhang2021reward}
\textsc{Zhang, W.}, \textsc{Zhou, D.} and \textsc{Gu, Q.} (2021{\natexlab{a}}).
\newblock Reward-free model-based reinforcement learning with linear function
  approximation.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{34}.

\bibitem[{Zhang et~al.(2020)Zhang, Du and Ji}]{zhang2020nearly}
\textsc{Zhang, Z.}, \textsc{Du, S.~S.} and \textsc{Ji, X.} (2020).
\newblock Nearly minimax optimal reward-free reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2010.05901} .

\bibitem[{Zhang et~al.(2021{\natexlab{b}})Zhang, Ji and
  Du}]{zhang2021reinforcement}
\textsc{Zhang, Z.}, \textsc{Ji, X.} and \textsc{Du, S.} (2021{\natexlab{b}}).
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhang et~al.(2022)Zhang, Ji and Du}]{zhang2022horizon}
\textsc{Zhang, Z.}, \textsc{Ji, X.} and \textsc{Du, S.~S.} (2022).
\newblock Horizon-free reinforcement learning in polynomial time: the power of
  stationary policies.
\newblock \textit{arXiv preprint arXiv:2203.12922} .

\bibitem[{Zhang et~al.(2021{\natexlab{c}})Zhang, Yang, Ji and
  Du}]{zhang2021improved}
\textsc{Zhang, Z.}, \textsc{Yang, J.}, \textsc{Ji, X.} and \textsc{Du, S.~S.}
  (2021{\natexlab{c}}).
\newblock Improved variance-aware confidence sets for linear bandits and linear
  mixture mdp.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{34}.

\bibitem[{Zhang et~al.(2021{\natexlab{d}})Zhang, Zhou and Ji}]{zhang2021model}
\textsc{Zhang, Z.}, \textsc{Zhou, Y.} and \textsc{Ji, X.} (2021{\natexlab{d}}).
\newblock Model-free reinforcement learning: from clipped pseudo-regret to
  sample complexity.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Zhou et~al.(2021{\natexlab{a}})Zhou, Gu and
  Szepesvari}]{zhou2021nearly}
\textsc{Zhou, D.}, \textsc{Gu, Q.} and \textsc{Szepesvari, C.}
  (2021{\natexlab{a}}).
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhou et~al.(2021{\natexlab{b}})Zhou, He and Gu}]{zhou2020provably}
\textsc{Zhou, D.}, \textsc{He, J.} and \textsc{Gu, Q.} (2021{\natexlab{b}}).
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\end{thebibliography}
