\begin{thebibliography}{10}

\bibitem{akopyan2015truenorth}
Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John Arthur,
  Paul Merolla, Nabil Imam, Yutaka Nakamura, Pallab Datta, Gi-Joon Nam, et~al.
\newblock {TrueNorth: Design and tool flow of a 65 mw 1 million neuron
  programmable neurosynaptic chip}.
\newblock {\em IEEE Transactions on Computer-Aided Design of Integrated
  Circuits and Systems}, 34(10):1537--1557, 2015.

\bibitem{almeida1987learning}
LB~Almeida.
\newblock A learning rule for asynchronous perceptrons with feedback in a
  combinatorial environment.
\newblock In {\em Proceedings, 1st First International Conference on Neural
  Networks}, volume~2, pages 609--618, 1987.

\bibitem{bai2019deep}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, pages 690--701, 2019.

\bibitem{bai2020multiscale}
Shaojie Bai, Vladlen Koltun, and J~Zico Kolter.
\newblock Multiscale deep equilibrium models.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{bellec2018long}
Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and
  Wolfgang Maass.
\newblock Long short-term memory and learning-to-learn in networks of spiking
  neurons.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  795--805, 2018.

\bibitem{bohte2002error}
Sander~M Bohte, Joost~N Kok, and Han La~Poutre.
\newblock Error-backpropagation in temporally encoded networks of spiking
  neurons.
\newblock {\em Neurocomputing}, 48(1-4):17--37, 2002.

\bibitem{davies2018loihi}
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao,
  Sri~Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain,
  et~al.
\newblock Loihi: A neuromorphic manycore processor with on-chip learning.
\newblock {\em IEEE Micro}, 38(1):82--99, 2018.

\bibitem{deng2021optimal}
Shikuang Deng and Shi Gu.
\newblock Optimal conversion of conventional artificial neural networks to
  spiking neural networks.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{diehl2015unsupervised}
Peter~U Diehl and Matthew Cook.
\newblock Unsupervised learning of digit recognition using
  spike-timing-dependent plasticity.
\newblock {\em Frontiers in Computational Neuroscience}, 9:99, 2015.

\bibitem{hebb2005organization}
Donald~Olding Hebb.
\newblock {\em The organization of behavior: A neuropsychological theory}.
\newblock Psychology Press, 2005.

\bibitem{hopfield1982neural}
John~J Hopfield.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock {\em Proceedings of the National Academy of Sciences},
  79(8):2554--2558, 1982.

\bibitem{hopfield1984neurons}
John~J Hopfield.
\newblock Neurons with graded response have collective computational properties
  like those of two-state neurons.
\newblock {\em Proceedings of the National Academy of Sciences},
  81(10):3088--3092, 1984.

\bibitem{hunsberger2015spiking}
Eric Hunsberger and Chris Eliasmith.
\newblock {Spiking deep networks with LIF neurons}.
\newblock {\em arXiv preprint arXiv:1510.08829}, 2015.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International Conference on Machine Learning}, pages
  448--456, 2015.

\bibitem{jin2018hybrid}
Yingyezhe Jin, Wenrui Zhang, and Peng Li.
\newblock Hybrid macro/micro level backpropagation for training deep spiking
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, pages 7005--7015, 2018.

\bibitem{kar2019evidence}
Kohitij Kar, Jonas Kubilius, Kailyn Schmidt, Elias~B Issa, and James~J DiCarlo.
\newblock Evidence that recurrent circuits are critical to the ventral
  stream’s execution of core object recognition behavior.
\newblock {\em Nature Neuroscience}, 22(6):974--983, 2019.

\bibitem{kim2020unifying}
Jinseok Kim, Kyungsu Kim, and Jae-Joon Kim.
\newblock Unifying activation-and timing-based learning rules for spiking
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem{kubilius2019brain}
Jonas Kubilius, Martin Schrimpf, Kohitij Kar, Rishi Rajalingham, Ha~Hong, Najib
  Majaj, Elias Issa, Pouya Bashivan, Jonathan Prescott-Roy, Kailyn Schmidt,
  et~al.
\newblock Brain-like object recognition with high-performing shallow recurrent
  anns.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, pages 12805--12816, 2019.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lee2020enabling}
Chankyu Lee, Syed~Shakib Sarwar, Priyadarshini Panda, Gopalakrishnan
  Srinivasan, and Kaushik Roy.
\newblock Enabling spike-based backpropagation for training deep neural network
  architectures.
\newblock {\em Frontiers in Neuroscience}, 14, 2020.

\bibitem{lee2016training}
Jun~Haeng Lee, Tobi Delbruck, and Michael Pfeiffer.
\newblock Training deep spiking neural networks using backpropagation.
\newblock {\em Frontiers in Neuroscience}, 10:508, 2016.

\bibitem{legenstein2008learning}
Robert Legenstein, Dejan Pecevski, and Wolfgang Maass.
\newblock A learning theory for reward-modulated spike-timing-dependent
  plasticity with application to biofeedback.
\newblock {\em PLoS Comput Biol}, 4(10):e1000180, 2008.

\bibitem{li2020minimax}
Qianyi Li and Cengiz Pehlevan.
\newblock Minimax dynamics of optimally balanced spiking networks of excitatory
  and inhibitory neurons.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{maass1997networks}
Wolfgang Maass.
\newblock Networks of spiking neurons: the third generation of neural network
  models.
\newblock {\em Neural Networks}, 10(9):1659--1671, 1997.

\bibitem{maass2002real}
Wolfgang Maass, Thomas Natschl{\"a}ger, and Henry Markram.
\newblock Real-time computing without stable states: A new framework for neural
  computation based on perturbations.
\newblock {\em Neural Computation}, 14(11):2531--2560, 2002.

\bibitem{mancoo2020understanding}
Allan Mancoo, Sander Keemink, and Christian~K Machens.
\newblock Understanding spiking networks through convex optimization.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{neftci2019surrogate}
Emre~O Neftci, Hesham Mostafa, and Friedemann Zenke.
\newblock Surrogate gradient learning in spiking neural networks: Bringing the
  power of gradient-based optimization to spiking neural networks.
\newblock {\em IEEE Signal Processing Magazine}, 36(6):51--63, 2019.

\bibitem{orchard2015converting}
Garrick Orchard, Ajinkya Jayawant, Gregory~K Cohen, and Nitish Thakor.
\newblock Converting static image datasets to spiking neuromorphic datasets
  using saccades.
\newblock {\em Frontiers in Neuroscience}, 9:437, 2015.

\bibitem{o2019training}
Peter O’Connor, Efstratios Gavves, and Max Welling.
\newblock Training a spiking neural network with equilibrium propagation.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1516--1523, 2019.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, pages 8026--8037, 2019.

\bibitem{pei2019towards}
Jing Pei, Lei Deng, Sen Song, Mingguo Zhao, Youhui Zhang, Shuang Wu, Guanrui
  Wang, Zhe Zou, Zhenzhi Wu, Wei He, et~al.
\newblock {Towards artificial general intelligence with hybrid Tianjic chip
  architecture}.
\newblock {\em Nature}, 572(7767):106--111, 2019.

\bibitem{pineda1987generalization}
Fernando~J Pineda.
\newblock Generalization of back-propagation to recurrent neural networks.
\newblock {\em Physical Review Letters}, 59(19):2229, 1987.

\bibitem{rathi2019enabling}
Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, and Kaushik Roy.
\newblock Enabling deep spiking neural networks with hybrid conversion and
  spike timing dependent backpropagation.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{roy2019towards}
Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.
\newblock Towards spike-based machine intelligence with neuromorphic computing.
\newblock {\em Nature}, 575(7784):607--617, 2019.

\bibitem{rueckauer2017conversion}
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and
  Shih-Chii Liu.
\newblock Conversion of continuous-valued deep networks to efficient
  event-driven networks for image classification.
\newblock {\em Frontiers in Neuroscience}, 11:682, 2017.

\bibitem{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533--536, 1986.

\bibitem{scellier2017equilibrium}
Benjamin Scellier and Yoshua Bengio.
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation.
\newblock {\em Frontiers in Computational Neuroscience}, 11:24, 2017.

\bibitem{sengupta2019going}
Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy.
\newblock Going deeper in spiking neural networks: Vgg and residual
  architectures.
\newblock {\em Frontiers in Neuroscience}, 13:95, 2019.

\bibitem{shrestha2018slayer}
Sumit~Bam Shrestha and Garrick Orchard.
\newblock Slayer: spike layer error reassignment in time.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1419--1428, 2018.

\bibitem{werbos1990backpropagation}
Paul~J Werbos.
\newblock Backpropagation through time: what it does and how to do it.
\newblock {\em Proceedings of the IEEE}, 78(10):1550--1560, 1990.

\bibitem{wu2021training}
Hao Wu, Yueyi Zhang, Wenming Weng, Yongting Zhang, Zhiwei Xiong, Zheng-Jun Zha,
  Xiaoyan Sun, and Feng Wu.
\newblock Training spiking neural networks with accumulated spiking flow.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem{wu2018spatio}
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi.
\newblock Spatio-temporal backpropagation for training high-performance spiking
  neural networks.
\newblock {\em Frontiers in Neuroscience}, 12:331, 2018.

\bibitem{wu2019direct}
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi.
\newblock Direct training for spiking neural networks: Faster, larger, better.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 1311--1318, 2019.

\bibitem{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock {Fashion-MNIST: A novel image dataset for benchmarking machine
  learning algorithms}.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{yan2021near}
Zhanglu Yan, Jun Zhou, and Weng-Fai Wong.
\newblock Near lossless transfer learning for spiking neural networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem{zhang2019spike}
Wenrui Zhang and Peng Li.
\newblock Spike-train level backpropagation for training deep recurrent spiking
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{zhang2020temporal}
Wenrui Zhang and Peng Li.
\newblock Temporal spike sequence learning via backpropagation for deep spiking
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{zheng2020going}
Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li.
\newblock Going deeper with directly-trained larger spiking neural networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\end{thebibliography}
