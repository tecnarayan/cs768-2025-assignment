\begin{thebibliography}{57}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Adlam and Pennington(2020)}]{adlam2020neural}
\textsc{Adlam, B.} and \textsc{Pennington, J.} (2020).
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Allen-Zhu and Li(2020{\natexlab{a}})}]{allen2020feature}
\textsc{Allen-Zhu, Z.} and \textsc{Li, Y.} (2020{\natexlab{a}}).
\newblock Feature purification: How adversarial training performs robust deep
  learning.
\newblock \textit{arXiv preprint arXiv:2005.10190} .

\bibitem[{Allen-Zhu and Li(2020{\natexlab{b}})}]{allen2020towards}
\textsc{Allen-Zhu, Z.} and \textsc{Li, Y.} (2020{\natexlab{b}}).
\newblock Towards understanding ensemble, knowledge distillation and
  self-distillation in deep learning.
\newblock \textit{arXiv preprint arXiv:2012.09816} .

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li and
  Liang}]{allen2018learning}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Liang, Y.}
  (2019{\natexlab{a}}).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li and
  Song}]{allen2018convergence}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Song, Z.}
  (2019{\natexlab{b}}).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li and
  Wang}]{arora2019fine}
\textsc{Arora, S.}, \textsc{Du, S.}, \textsc{Hu, W.}, \textsc{Li, Z.} and
  \textsc{Wang, R.} (2019{\natexlab{a}}).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov and
  Wang}]{arora2019exact}
\textsc{Arora, S.}, \textsc{Du, S.~S.}, \textsc{Hu, W.}, \textsc{Li, Z.},
  \textsc{Salakhutdinov, R.} and \textsc{Wang, R.} (2019{\natexlab{b}}).
\newblock On exact computation with an infinitely wide neural net.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Arora et~al.(2018)Arora, Ge, Neyshabur and Zhang}]{arora2018stronger}
\textsc{Arora, S.}, \textsc{Ge, R.}, \textsc{Neyshabur, B.} and \textsc{Zhang,
  Y.} (2018).
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Bartlett et~al.(2017)Bartlett, Foster and
  Telgarsky}]{bartlett2017spectrally}
\textsc{Bartlett, P.~L.}, \textsc{Foster, D.~J.} and \textsc{Telgarsky, M.~J.}
  (2017).
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Bartlett et~al.(2020)Bartlett, Long, Lugosi and
  Tsigler}]{bartlett2020benign}
\textsc{Bartlett, P.~L.}, \textsc{Long, P.~M.}, \textsc{Lugosi, G.} and
  \textsc{Tsigler, A.} (2020).
\newblock Benign overfitting in linear regression.
\newblock \textit{Proceedings of the National Academy of Sciences} .

\bibitem[{Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, Ma and
  Mandal}]{belkin2019reconciling}
\textsc{Belkin, M.}, \textsc{Hsu, D.}, \textsc{Ma, S.} and \textsc{Mandal, S.}
  (2019{\natexlab{a}}).
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \textit{Proceedings of the National Academy of Sciences} \textbf{116}
  15849--15854.

\bibitem[{Belkin et~al.(2019{\natexlab{b}})Belkin, Hsu and Xu}]{belkin2019two}
\textsc{Belkin, M.}, \textsc{Hsu, D.} and \textsc{Xu, J.} (2019{\natexlab{b}}).
\newblock Two models of double descent for weak features.
\newblock \textit{arXiv preprint arXiv:1903.07571} .

\bibitem[{Belkin et~al.(2018)Belkin, Ma and Mandal}]{belkin2018understand}
\textsc{Belkin, M.}, \textsc{Ma, S.} and \textsc{Mandal, S.} (2018).
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Bousquet and Elisseeff(2002)}]{bousquet2002stability}
\textsc{Bousquet, O.} and \textsc{Elisseeff, A.} (2002).
\newblock Stability and generalization.
\newblock \textit{Journal of machine learning research} \textbf{2} 499--526.

\bibitem[{Brutzkus et~al.(2018)Brutzkus, Globerson, Malach and
  Shalev-Shwartz}]{brutzkus2017sgd}
\textsc{Brutzkus, A.}, \textsc{Globerson, A.}, \textsc{Malach, E.} and
  \textsc{Shalev-Shwartz, S.} (2018).
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Cao and Gu(2019{\natexlab{a}})}]{cao2019generalizationsgd}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019{\natexlab{a}}).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Cao and Gu(2019{\natexlab{b}})}]{cao2019tight}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019{\natexlab{b}}).
\newblock Tight sample complexity of learning one-hidden-layer convolutional
  neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Cao et~al.(2021)Cao, Gu and Belkin}]{cao2021risk}
\textsc{Cao, Y.}, \textsc{Gu, Q.} and \textsc{Belkin, M.} (2021).
\newblock Risk bounds for over-parameterized maximum margin classification on
  sub-gaussian mixtures.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{34}.

\bibitem[{Chatterji and Long(2020)}]{chatterji2020finite}
\textsc{Chatterji, N.~S.} and \textsc{Long, P.~M.} (2020).
\newblock Finite-sample analysis of interpolating linear classifiers in the
  overparameterized regime.
\newblock \textit{arXiv preprint arXiv:2004.12019} .

\bibitem[{Chen et~al.(2018)Chen, Jin and Yu}]{chen2018stability}
\textsc{Chen, Y.}, \textsc{Jin, C.} and \textsc{Yu, B.} (2018).
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \textit{arXiv preprint arXiv:1804.01619} .

\bibitem[{Chen et~al.(2019)Chen, Cao, Zou and Gu}]{chen2019much}
\textsc{Chen, Z.}, \textsc{Cao, Y.}, \textsc{Zou, D.} and \textsc{Gu, Q.}
  (2019).
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?
\newblock \textit{arXiv preprint arXiv:1911.12360} .

\bibitem[{Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang and
  Zhai}]{du2018gradientdeep}
\textsc{Du, S.}, \textsc{Lee, J.}, \textsc{Li, H.}, \textsc{Wang, L.} and
  \textsc{Zhai, X.} (2019{\natexlab{a}}).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Du et~al.(2018{\natexlab{a}})Du, Lee and Tian}]{du2017convolutional}
\textsc{Du, S.~S.}, \textsc{Lee, J.~D.} and \textsc{Tian, Y.}
  (2018{\natexlab{a}}).
\newblock When is a convolutional filter easy to learn?
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Du et~al.(2018{\natexlab{b}})Du, Lee, Tian, Singh and
  Poczos}]{du2017gradient}
\textsc{Du, S.~S.}, \textsc{Lee, J.~D.}, \textsc{Tian, Y.}, \textsc{Singh, A.}
  and \textsc{Poczos, B.} (2018{\natexlab{b}}).
\newblock Gradient descent learns one-hidden-layer {CNN}: Don’t be afraid of
  spurious local minima.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos and
  Singh}]{du2018gradient}
\textsc{Du, S.~S.}, \textsc{Zhai, X.}, \textsc{Poczos, B.} and \textsc{Singh,
  A.} (2019{\natexlab{b}}).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Frei et~al.(2021)Frei, Cao and Gu}]{frei2021provable}
\textsc{Frei, S.}, \textsc{Cao, Y.} and \textsc{Gu, Q.} (2021).
\newblock Provable generalization of sgd-trained neural networks of any width
  in the presence of adversarial label noise.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Frei et~al.(2022)Frei, Chatterji and Bartlett}]{frei2022benign}
\textsc{Frei, S.}, \textsc{Chatterji, N.~S.} and \textsc{Bartlett, P.~L.}
  (2022).
\newblock Benign overfitting without linearity: Neural network classifiers
  trained by gradient descent for noisy linear data.
\newblock \textit{arXiv preprint arXiv:2202.05928} .

\bibitem[{Golowich et~al.(2018)Golowich, Rakhlin and Shamir}]{golowich2017size}
\textsc{Golowich, N.}, \textsc{Rakhlin, A.} and \textsc{Shamir, O.} (2018).
\newblock Size-independent sample complexity of neural networks.
\newblock In \textit{Conference On Learning Theory}.

\bibitem[{Hardt et~al.(2016)Hardt, Recht and Singer}]{hardt2016train}
\textsc{Hardt, M.}, \textsc{Recht, B.} and \textsc{Singer, Y.} (2016).
\newblock Train faster, generalize better: stability of stochastic gradient
  descent.
\newblock In \textit{Proceedings of the 33rd International Conference on
  International Conference on Machine Learning-Volume 48}. JMLR. org.

\bibitem[{Hastie et~al.(2019)Hastie, Montanari, Rosset and
  Tibshirani}]{hastie2019surprises}
\textsc{Hastie, T.}, \textsc{Montanari, A.}, \textsc{Rosset, S.} and
  \textsc{Tibshirani, R.~J.} (2019).
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \textit{arXiv preprint arXiv:1903.08560} .

\bibitem[{Jacot et~al.(2018)Jacot, Gabriel and Hongler}]{jacot2018neural}
\textsc{Jacot, A.}, \textsc{Gabriel, F.} and \textsc{Hongler, C.} (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Ji and Telgarsky(2020)}]{ji2019polylogarithmic}
\textsc{Ji, Z.} and \textsc{Telgarsky, M.} (2020).
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein and
  Pennington}]{lee2019wide}
\textsc{Lee, J.}, \textsc{Xiao, L.}, \textsc{Schoenholz, S.~S.}, \textsc{Bahri,
  Y.}, \textsc{Sohl-Dickstein, J.} and \textsc{Pennington, J.} (2019).
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li and Liang(2018)}]{li2018learning}
\textsc{Li, Y.} and \textsc{Liang, Y.} (2018).
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li et~al.(2019)Li, Wei and Ma}]{li2019towards}
\textsc{Li, Y.}, \textsc{Wei, C.} and \textsc{Ma, T.} (2019).
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li and Yuan(2017)}]{li2017convergence}
\textsc{Li, Y.} and \textsc{Yuan, Y.} (2017).
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li et~al.(2021)Li, Zhou and Gretton}]{li2021towards}
\textsc{Li, Z.}, \textsc{Zhou, Z.-H.} and \textsc{Gretton, A.} (2021).
\newblock Towards an understanding of benign overfitting in neural networks.
\newblock \textit{arXiv preprint arXiv:2106.03212} .

\bibitem[{Liang and Rakhlin(2020)}]{liang2020just}
\textsc{Liang, T.} and \textsc{Rakhlin, A.} (2020).
\newblock Just interpolate: Kernel “ridgeless” regression can generalize.
\newblock \textit{The Annals of Statistics} \textbf{48} 1329--1347.

\bibitem[{Liao et~al.(2020)Liao, Couillet and Mahoney}]{liao2020random}
\textsc{Liao, Z.}, \textsc{Couillet, R.} and \textsc{Mahoney, M.} (2020).
\newblock A random matrix analysis of random fourier features: beyond the
  gaussian kernel, a precise phase transition, and the corresponding double
  descent.
\newblock In \textit{34th Conference on Neural Information Processing Systems
  (NeurIPS 2020)}.

\bibitem[{Lin et~al.(2013)Lin, Chen and Yan}]{lin2013network}
\textsc{Lin, M.}, \textsc{Chen, Q.} and \textsc{Yan, S.} (2013).
\newblock Network in network.
\newblock \textit{arXiv preprint arXiv:1312.4400} .

\bibitem[{Lyu and Li(2019)}]{lyu2019gradient}
\textsc{Lyu, K.} and \textsc{Li, J.} (2019).
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \textit{arXiv preprint arXiv:1906.05890} .

\bibitem[{Mei and Montanari(2019)}]{mei2019generalization}
\textsc{Mei, S.} and \textsc{Montanari, A.} (2019).
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock \textit{arXiv preprint arXiv:1908.05355} .

\bibitem[{Montanari and Zhong(2020)}]{montanari2020interpolation}
\textsc{Montanari, A.} and \textsc{Zhong, Y.} (2020).
\newblock The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training.
\newblock \textit{arXiv preprint arXiv:2007.12826} .

\bibitem[{Mou et~al.(2017)Mou, Wang, Zhai and Zheng}]{mou2017generalization}
\textsc{Mou, W.}, \textsc{Wang, L.}, \textsc{Zhai, X.} and \textsc{Zheng, K.}
  (2017).
\newblock Generalization bounds of sgld for non-convex learning: Two
  theoretical viewpoints.
\newblock \textit{arXiv preprint arXiv:1707.05947} .

\bibitem[{Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, McAllester and
  Srebro}]{neyshabur2017pac}
\textsc{Neyshabur, B.}, \textsc{Bhojanapalli, S.}, \textsc{McAllester, D.} and
  \textsc{Srebro, N.} (2018).
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \textit{International Conference on Learning Representation}.

\bibitem[{Neyshabur et~al.(2019)Neyshabur, Li, Bhojanapalli, LeCun and
  Srebro}]{neyshabur2018role}
\textsc{Neyshabur, B.}, \textsc{Li, Z.}, \textsc{Bhojanapalli, S.},
  \textsc{LeCun, Y.} and \textsc{Srebro, N.} (2019).
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Neyshabur et~al.(2015)Neyshabur, Tomioka and
  Srebro}]{neyshabur2015norm}
\textsc{Neyshabur, B.}, \textsc{Tomioka, R.} and \textsc{Srebro, N.} (2015).
\newblock Norm-based capacity control in neural networks.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Shamir(2022)}]{shamir2022implicit}
\textsc{Shamir, O.} (2022).
\newblock The implicit bias of benign overfitting.
\newblock \textit{arXiv preprint arXiv:2201.11489} .

\bibitem[{Soltanolkotabi(2017)}]{soltanolkotabi2017learning}
\textsc{Soltanolkotabi, M.} (2017).
\newblock Learning {ReLUs} via gradient descent.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar and
  Srebro}]{soudry2017implicit}
\textsc{Soudry, D.}, \textsc{Hoffer, E.}, \textsc{Nacson, M.~S.},
  \textsc{Gunasekar, S.} and \textsc{Srebro, N.} (2018).
\newblock The implicit bias of gradient descent on separable data.
\newblock \textit{The Journal of Machine Learning Research} \textbf{19}
  2822--2878.

\bibitem[{Wu and Xu(2020)}]{wu2020optimal}
\textsc{Wu, D.} and \textsc{Xu, J.} (2020).
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{33}.

\bibitem[{Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht and
  Vinyals}]{zhang2016understanding}
\textsc{Zhang, C.}, \textsc{Bengio, S.}, \textsc{Hardt, M.}, \textsc{Recht, B.}
  and \textsc{Vinyals, O.} (2017).
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Zhang et~al.(2019)Zhang, Yu, Wang and Gu}]{zhang2018learning}
\textsc{Zhang, X.}, \textsc{Yu, Y.}, \textsc{Wang, L.} and \textsc{Gu, Q.}
  (2019).
\newblock Learning one-hidden-layer {ReLU} networks via gradient descent.
\newblock In \textit{The 22nd International Conference on Artificial
  Intelligence and Statistics}.

\bibitem[{Zhong et~al.(2017)Zhong, Song, Jain, Bartlett and
  Dhillon}]{zhong2017recovery}
\textsc{Zhong, K.}, \textsc{Song, Z.}, \textsc{Jain, P.}, \textsc{Bartlett,
  P.~L.} and \textsc{Dhillon, I.~S.} (2017).
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Zou et~al.(2021{\natexlab{a}})Zou, Cao, Li and
  Gu}]{zou2021understanding}
\textsc{Zou, D.}, \textsc{Cao, Y.}, \textsc{Li, Y.} and \textsc{Gu, Q.}
  (2021{\natexlab{a}}).
\newblock Understanding the generalization of adam in learning neural networks
  with proper regularization.
\newblock \textit{arXiv preprint arXiv:2108.11371} .

\bibitem[{Zou et~al.(2019)Zou, Cao, Zhou and Gu}]{zou2019gradient}
\textsc{Zou, D.}, \textsc{Cao, Y.}, \textsc{Zhou, D.} and \textsc{Gu, Q.}
  (2019).
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock \textit{Machine Learning} .

\bibitem[{Zou et~al.(2021{\natexlab{b}})Zou, Wu, Braverman, Gu and
  Kakade}]{zou2021benign}
\textsc{Zou, D.}, \textsc{Wu, J.}, \textsc{Braverman, V.}, \textsc{Gu, Q.} and
  \textsc{Kakade, S.} (2021{\natexlab{b}}).
\newblock Benign overfitting of constant-stepsize sgd for linear regression.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\end{thebibliography}
