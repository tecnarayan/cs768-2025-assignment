\begin{thebibliography}{10}

\bibitem{abnar2020quantifying}
Samira Abnar and Willem Zuidema.
\newblock Quantifying attention flow in transformers.
\newblock {\em arXiv preprint arXiv:2005.00928}, 2020.

\bibitem{baevski2022data2vec}
Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael
  Auli.
\newblock Data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock {\em arXiv preprint arXiv:2202.03555}, 2022.

\bibitem{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12449--12460, 2020.

\bibitem{baudart2021compiling}
Guillaume Baudart, Javier Burroni, Martin Hirzel, Louis Mandel, and Avraham
  Shinnar.
\newblock Compiling stan to generative probabilistic languages and extension to
  deep probabilistic programming.
\newblock In {\em Proceedings of the 42nd ACM SIGPLAN International Conference
  on Programming Language Design and Implementation}, pages 497--510, 2021.

\bibitem{baudart2021automatic}
Guillaume Baudart and Louis Mandel.
\newblock Automatic guide generation for stan via numpyro.
\newblock {\em arXiv preprint arXiv:2110.11790}, 2021.

\bibitem{binder2016layer}
Alexander Binder, Gr{\'e}goire Montavon, Sebastian Lapuschkin, Klaus-Robert
  M{\"u}ller, and Wojciech Samek.
\newblock Layer-wise relevance propagation for neural networks with local
  renormalization layers.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 63--71. Springer, 2016.

\bibitem{bingham2019pyro}
Eli Bingham, Jonathan~P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj
  Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and
  Noah~D Goodman.
\newblock Pyro: Deep universal probabilistic programming.
\newblock {\em The Journal of Machine Learning Research}, 20(1):973--978, 2019.

\bibitem{blei2017variational}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American statistical Association},
  112(518):859--877, 2017.

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{cao2014generalized}
Yanshuai Cao and David~J Fleet.
\newblock Generalized product of experts for automatic and principled fusion of
  gaussian process predictions.
\newblock {\em arXiv preprint arXiv:1410.7827}, 2014.

\bibitem{carpenter2017stan}
Bob Carpenter, Andrew Gelman, Matthew~D Hoffman, Daniel Lee, Ben Goodrich,
  Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen
  Riddell.
\newblock Stan: A probabilistic programming language.
\newblock {\em Journal of statistical software}, 76(1), 2017.

\bibitem{che2021meta}
Gwonsoo Che and Hongseok Yang.
\newblock Meta-learning an inference algorithm for probabilistic programs.
\newblock {\em arXiv preprint arXiv:2103.00737}, 2021.

\bibitem{chefer2021transformer}
Hila Chefer, Shir Gur, and Lior Wolf.
\newblock Transformer interpretability beyond attention visualization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 782--791, 2021.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem{choi2019meta}
Kristy Choi, Mike Wu, Noah Goodman, and Stefano Ermon.
\newblock Meta-amortized variational inference and learning.
\newblock In {\em International Conference on Learning Representation}, 2019.

\bibitem{cusumano2019gen}
Marco~F Cusumano-Towner, Feras~A Saad, Alexander~K Lew, and Vikash~K
  Mansinghka.
\newblock Gen: a general-purpose probabilistic programming system with
  programmable inference.
\newblock In {\em Proceedings of the 40th acm sigplan conference on programming
  language design and implementation}, pages 221--236, 2019.

\bibitem{depaoli2016just}
Sarah Depaoli, James~P Clifton, and Patrice~R Cobb.
\newblock Just another gibbs sampler (jags) flexible software for mcmc
  implementation.
\newblock {\em Journal of Educational and Behavioral Statistics},
  41(6):628--649, 2016.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{durr2022bernstein}
Oliver D{\"u}rr, Stephan H{\"o}rling, Daniel Dold, Ivonne Kovylov, and Beate
  Sick.
\newblock Bernstein flows for flexible posteriors in variational bayes.
\newblock {\em arXiv preprint arXiv:2202.05650}, 2022.

\bibitem{edgeworth1888statistics}
Francis~Ysidro Edgeworth.
\newblock The statistics of examinations.
\newblock {\em Journal of the Royal Statistical Society}, 51(3):599--635, 1888.

\bibitem{feng2020codebert}
Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun
  Shou, Bing Qin, Ting Liu, Daxin Jiang, et~al.
\newblock Codebert: A pre-trained model for programming and natural languages.
\newblock {\em arXiv preprint arXiv:2002.08155}, 2020.

\bibitem{ge2018turing}
Hong Ge, Kai Xu, and Zoubin Ghahramani.
\newblock Turing: Composable inference for probabilistic programming.
\newblock In {\em AISTATS}, volume~84, pages 1682--1690. PMLR, 2018.

\bibitem{gelfand1990sampling}
Alan~E Gelfand and Adrian~FM Smith.
\newblock Sampling-based approaches to calculating marginal densities.
\newblock {\em Journal of the American statistical association},
  85(410):398--409, 1990.

\bibitem{gelman1992inference}
Andrew Gelman and Donald~B Rubin.
\newblock Inference from iterative simulation using multiple sequences.
\newblock {\em Statistical science}, 7(4):457--472, 1992.

\bibitem{gershman2014amortized}
Samuel Gershman and Noah Goodman.
\newblock Amortized inference in probabilistic reasoning.
\newblock In {\em Proceedings of the annual meeting of the cognitive science
  society}, volume~36, 2014.

\bibitem{goodman2012church}
Noah Goodman, Vikash Mansinghka, Daniel~M Roy, Keith Bonawitz, and Joshua~B
  Tenenbaum.
\newblock Church: a language for generative models.
\newblock {\em arXiv preprint arXiv:1206.3255}, 2012.

\bibitem{dippl}
Noah~D Goodman and Andreas Stuhlm\"{u}ller.
\newblock {The Design and Implementation of Probabilistic Programming
  Languages}.
\newblock \url{http://dippl.org}, 2014.
\newblock Accessed: 2022-3-19.

\bibitem{gordon2014probabilistic}
Andrew~D Gordon, Thomas~A Henzinger, Aditya~V Nori, and Sriram~K Rajamani.
\newblock Probabilistic programming.
\newblock In {\em Future of Software Engineering Proceedings}, pages 167--181.
  2014.

\bibitem{gordon2018meta}
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and
  Richard~E Turner.
\newblock Meta-learning probabilistic inference for prediction.
\newblock {\em arXiv preprint arXiv:1805.09921}, 2018.

\bibitem{hambleton1991fundamentals}
Ronald~K Hambleton, Hariharan Swaminathan, and H~Jane Rogers.
\newblock {\em Fundamentals of item response theory}, volume~2.
\newblock Sage, 1991.

\bibitem{hastings1970monte}
W~Keith Hastings.
\newblock Monte carlo sampling methods using markov chains and their
  applications.
\newblock 1970.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hinton2002training}
Geoffrey~E Hinton.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock {\em Neural computation}, 14(8):1771--1800, 2002.

\bibitem{hoffman2013stochastic}
Matthew~D Hoffman, David~M Blei, Chong Wang, and John Paisley.
\newblock Stochastic variational inference.
\newblock {\em Journal of Machine Learning Research}, 2013.

\bibitem{iakovleva2020meta}
Ekaterina Iakovleva, Jakob Verbeek, and Karteek Alahari.
\newblock Meta-learning with shared amortized variational inference.
\newblock In {\em International Conference on Machine Learning}, pages
  4572--4582. PMLR, 2020.

\bibitem{jordan1999introduction}
Michael~I Jordan, Zoubin Ghahramani, Tommi~S Jaakkola, and Lawrence~K Saul.
\newblock An introduction to variational methods for graphical models.
\newblock {\em Machine learning}, 37(2):183--233, 1999.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kucukelbir2017automatic}
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David~M Blei.
\newblock Automatic differentiation variational inference.
\newblock {\em Journal of machine learning research}, 2017.

\bibitem{le2017inference}
Tuan~Anh Le, Atilim~Gunes Baydin, and Frank Wood.
\newblock Inference compilation and universal probabilistic programming.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1338--1348.
  PMLR, 2017.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{maddison2016concrete}
Chris~J Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock {\em arXiv preprint arXiv:1611.00712}, 2016.

\bibitem{magnusson2021}
Mans Magnusson, Paul Burkner, and Aki Vehtari.
\newblock posteriordb: a set of posteriors for bayesian inference and
  probabilistic programming.
\newblock 2021.

\bibitem{mansinghka2014venture}
Vikash Mansinghka, Daniel Selsam, and Yura Perov.
\newblock Venture: a higher-order probabilistic programming platform with
  programmable inference.
\newblock {\em arXiv preprint arXiv:1404.0099}, 2014.

\bibitem{minka2012infer}
Tom Minka.
\newblock Infer. net 2.5.
\newblock {\em http://research. microsoft. com/infernet}, 2012.

\bibitem{mnih2014neural}
Andriy Mnih and Karol Gregor.
\newblock Neural variational inference and learning in belief networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1791--1799. PMLR, 2014.

\bibitem{narayanan2016probabilistic}
Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh Shan, and Robert
  Zinkov.
\newblock Probabilistic inference by program transformation in hakaru (system
  description).
\newblock In {\em International Symposium on Functional and Logic Programming},
  pages 62--79. Springer, 2016.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{ranganath2014black}
Rajesh Ranganath, Sean Gerrish, and David Blei.
\newblock Black box variational inference.
\newblock In {\em Artificial intelligence and statistics}, pages 814--822.
  PMLR, 2014.

\bibitem{rasch1993probabilistic}
Georg Rasch.
\newblock {\em Probabilistic models for some intelligence and attainment
  tests.}
\newblock ERIC, 1993.

\bibitem{ritchie2015controlling}
Daniel Ritchie, Ben Mildenhall, Noah~D Goodman, and Pat Hanrahan.
\newblock Controlling procedural modeling programs with stochastically-ordered
  sequential monte carlo.
\newblock {\em ACM Transactions on Graphics (TOG)}, 34(4):1--11, 2015.

\bibitem{salvatier2016probabilistic}
John Salvatier, Thomas~V Wiecki, and Christopher Fonnesbeck.
\newblock Probabilistic programming in python using pymc3.
\newblock {\em PeerJ Computer Science}, 2:e55, 2016.

\bibitem{schneider2019wav2vec}
Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli.
\newblock wav2vec: Unsupervised pre-training for speech recognition.
\newblock {\em arXiv preprint arXiv:1904.05862}, 2019.

\bibitem{stuhlmuller2013learning}
Andreas Stuhlm{\"u}ller, Jacob Taylor, and Noah Goodman.
\newblock Learning stochastic inverses.
\newblock {\em Advances in neural information processing systems}, 26, 2013.

\bibitem{taylor1953cloze}
Wilson~L Taylor.
\newblock “cloze procedure”: A new tool for measuring readability.
\newblock {\em Journalism quarterly}, 30(4):415--433, 1953.

\bibitem{tehrani2020bean}
Nazanin Tehrani, Nimar~S Arora, Yucen~Lily Li, Kinjal~Divesh Shah, David
  Noursi, Michael Tingley, Narjes Torabi, Eric Lippert, Erik Meijer, et~al.
\newblock Bean machine: A declarative probabilistic programming language for
  efficient programmable inference.
\newblock In {\em International Conference on Probabilistic Graphical Models}.
  PMLR, 2020.

\bibitem{tolpin2016design}
David Tolpin, Jan-Willem van~de Meent, Hongseok Yang, and Frank Wood.
\newblock Design and implementation of probabilistic programming language
  anglican.
\newblock In {\em Proceedings of the 28th Symposium on the Implementation and
  Application of Functional programming Languages}, pages 1--12, 2016.

\bibitem{tran2016edward}
Dustin Tran, Alp Kucukelbir, Adji~B Dieng, Maja Rudolph, Dawen Liang, and
  David~M Blei.
\newblock Edward: A library for probabilistic modeling, inference, and
  criticism.
\newblock {\em arXiv preprint arXiv:1610.09787}, 2016.

\bibitem{tucker2017rebar}
George Tucker, Andriy Mnih, Chris~J Maddison, John Lawson, and Jascha
  Sohl-Dickstein.
\newblock Rebar: Low-variance, unbiased gradient estimates for discrete latent
  variable models.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{van2018introduction}
Jan-Willem van~de Meent, Brooks Paige, Hongseok Yang, and Frank Wood.
\newblock An introduction to probabilistic programming.
\newblock {\em arXiv preprint arXiv:1809.10756}, 2018.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{voita2019analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock {\em arXiv preprint arXiv:1905.09418}, 2019.

\bibitem{wainwright2008graphical}
Martin~J Wainwright, Michael~I Jordan, et~al.
\newblock Graphical models, exponential families, and variational inference.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  1(1--2):1--305, 2008.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3):229--256, 1992.

\bibitem{wu2020variational}
Mike Wu, Richard~L Davis, Benjamin~W Domingue, Chris Piech, and Noah Goodman.
\newblock Variational item response theory: Fast, accurate, and expressive.
\newblock {\em arXiv preprint arXiv:2002.00276}, 2020.

\bibitem{wu2021modeling}
Mike Wu, Richard~L Davis, Benjamin~W Domingue, Chris Piech, and Noah Goodman.
\newblock Modeling item response theory with stochastic variational inference.
\newblock {\em arXiv preprint arXiv:2108.11579}, 2021.

\bibitem{wu2018multimodal}
Mike Wu and Noah Goodman.
\newblock Multimodal generative models for scalable weakly-supervised learning.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{yao2020stacking}
Yuling Yao, Aki Vehtari, and Andrew Gelman.
\newblock Stacking for non-mixing bayesian computations: The curse and blessing
  of multimodal posteriors.
\newblock {\em arXiv preprint arXiv:2006.12335}, 2020.

\bibitem{zhang2021pathfinder}
Lu~Zhang, Bob Carpenter, Andrew Gelman, and Aki Vehtari.
\newblock Pathfinder: Parallel quasi-newton variational inference.
\newblock {\em arXiv preprint arXiv:2108.03782}, 2021.

\end{thebibliography}
