\begin{thebibliography}{97}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th {USENIX} Symposium on Operating Systems Design and
  Implementation ({OSDI} 16)}, pp.\  265--283, 2016.

\bibitem[Ashukha et~al.(2020)Ashukha, Lyzhov, Molchanov, and
  Vetrov]{ashukha2019uncertaintyestimation}
Ashukha, A., Lyzhov, A., Molchanov, D., and Vetrov, D.
\newblock Pitfalls of in-domain uncertainty estimation and ensembling in deep
  learning.
\newblock In \emph{Eigth International Conference on Learning Representations
  ({ICLR} 2020)}, 2020.

\bibitem[Atanov et~al.(2018)Atanov, Ashukha, Molchanov, Neklyudov, and
  Vetrov]{atanov2018stochasticbatchnormalization}
Atanov, A., Ashukha, A., Molchanov, D., Neklyudov, K., and Vetrov, D.
\newblock Uncertainty estimation via stochastic batch normalization.
\newblock \emph{arXiv preprint arXiv:1802.04893}, 2018.

\bibitem[Bae et~al.(2018)Bae, Zhang, and Grosse]{bae2018kfacvb}
Bae, J., Zhang, G., and Grosse, R.
\newblock Eigenvalue corrected noisy natural gradient.
\newblock \emph{arXiv preprint arXiv:1811.12565}, 2018.

\bibitem[Baldock \& Marzari(2019)Baldock and Marzari]{baldock2019bayesiannn}
Baldock, R.~J. and Marzari, N.
\newblock Bayesian neural networks at finite temperature.
\newblock \emph{arXiv preprint, arXiv:1904.04154}, 2019.

\bibitem[Barber \& Bishop(1998)Barber and Bishop]{barber1998ensemblelearning}
Barber, D. and Bishop, C.~M.
\newblock Ensemble learning for multi-layer networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  395--401, 1998.

\bibitem[Berger(1985)]{berger1985statisticaldecisiontheory}
Berger, J.~O.
\newblock \emph{Statistical decision theory and {Bayesian} analysis}.
\newblock Springer, 1985.

\bibitem[Betancourt \& Girolami(2015)Betancourt and
  Girolami]{betancourt2015hamiltonian}
Betancourt, M. and Girolami, M.
\newblock {Hamiltonian Monte Carlo} for hierarchical models.
\newblock \emph{Current trends in Bayesian methodology with applications},
  79:\penalty0 30, 2015.

\bibitem[Bhattacharya et~al.(2019)Bhattacharya, Pati, Yang,
  et~al.]{bhattacharya2019fractionalposteriors}
Bhattacharya, A., Pati, D., Yang, Y., et~al.
\newblock Bayesian fractional posteriors.
\newblock \emph{The Annals of Statistics}, 47\penalty0 (1):\penalty0 39--66,
  2019.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weightuncertainty}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural network.
\newblock 37:\penalty0 1613--1622, 2015.

\bibitem[Brier(1950)]{brier1950verification}
Brier, G.~W.
\newblock Verification of forecasts expressed in terms of probability.
\newblock \emph{Monthly weather review}, 78\penalty0 (1):\penalty0 1--3, 1950.

\bibitem[Brooks et~al.(2011)Brooks, Gelman, Jones, and
  Meng]{brooks2011handbook}
Brooks, S., Gelman, A., Jones, G., and Meng, X.
\newblock \emph{Handbook of Markov Chain Monte Carlo}.
\newblock Chapman \& Hall/CRC Handbooks of Modern Statistical Methods. CRC
  Press, 2011.
\newblock ISBN 9781420079425.
\newblock URL \url{https://books.google.de/books?id=qfRsAIKZ4rIC}.

\bibitem[Chen et~al.(2015)Chen, Ding, and Carin]{chen2015convergencesgmcmc}
Chen, C., Ding, N., and Carin, L.
\newblock On the convergence of stochastic gradient {MCMC} algorithms with
  high-order integrators.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2278--2286, 2015.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014stochastichmc}
Chen, T., Fox, E., and Guestrin, C.
\newblock Stochastic gradient {Hamiltonian Monte Carlo}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1683--1691, 2014.

\bibitem[de~G.~Matthews et~al.(2018)de~G.~Matthews, Hron, Rowland, Turner, and
  Ghahramani]{matthews2018gaussiannetworks}
de~G.~Matthews, A.~G., Hron, J., Rowland, M., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Dieng et~al.(2018)Dieng, Ranganath, Altosaar, and
  Blei]{dieng2018noisin}
Dieng, A.~B., Ranganath, R., Altosaar, J., and Blei, D.~M.
\newblock Noisin: Unbiased regularization for recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1805.01500}, 2018.

\bibitem[Dillon et~al.(2017)Dillon, Langmore, Tran, Brevdo, Vasudevan, Moore,
  Patton, Alemi, Hoffman, and Saurous]{dillon2017tensorflow}
Dillon, J.~V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D.,
  Patton, B., Alemi, A., Hoffman, M., and Saurous, R.~A.
\newblock Tensorflow distributions.
\newblock \emph{arXiv preprint arXiv:1711.10604}, 2017.

\bibitem[Ding et~al.(2014)Ding, Fang, Babbush, Chen, Skeel, and
  Neven]{ding2014bayesian}
Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R.~D., and Neven, H.
\newblock Bayesian sampling using stochastic gradient thermostats.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3203--3211, 2014.

\bibitem[Duane et~al.(1987)Duane, Kennedy, Pendleton, and
  Roweth]{duane1987hybrid}
Duane, S., Kennedy, A.~D., Pendleton, B.~J., and Roweth, D.
\newblock Hybrid {Monte Carlo}.
\newblock \emph{Physics letters B}, 195\penalty0 (2):\penalty0 216--222, 1987.

\bibitem[Earl \& Deem(2005)Earl and Deem]{earl2005paralleltempering}
Earl, D.~J. and Deem, M.~W.
\newblock Parallel tempering: Theory, applications, and new perspectives.
\newblock \emph{Physical Chemistry Chemical Physics}, 7\penalty0 (23):\penalty0
  3910--3916, 2005.

\bibitem[Flam-Shepherd et~al.(2017)Flam-Shepherd, Requeima, and
  Duvenaud]{flam2017mapping}
Flam-Shepherd, D., Requeima, J., and Duvenaud, D.
\newblock Mapping gaussian process priors to bayesian neural networks.
\newblock In \emph{NIPS Bayesian deep learning workshop}, 2017.

\bibitem[Fushiki et~al.(2005)]{fushiki2005bootstrapprediction}
Fushiki, T. et~al.
\newblock Bootstrap prediction and {Bayesian} prediction under misspecified
  models.
\newblock \emph{Bernoulli}, 11\penalty0 (4):\penalty0 747--758, 2005.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a {Bayesian} approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pp.\
  1050--1059, 2016.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Rasmussen, and
  Aitchison]{garrigaalonso2019convnetgp}
Garriga-Alonso, A., Rasmussen, C.~E., and Aitchison, L.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Geisser(1993)]{geisser1993predictiveinference}
Geisser, S.
\newblock \emph{An Introduction to Predictive Inference}.
\newblock Chapman and Hall, New York, 1993.

\bibitem[Gelman \& Rubin(1992)Gelman and Rubin]{gelman1992psrf}
Gelman, A. and Rubin, D.~B.
\newblock Inference from iterative simulation using multiple sequences.
\newblock \emph{Statistical science}, 7\penalty0 (4):\penalty0 457--472, 1992.

\bibitem[Gelman et~al.(2013)Gelman, Carlin, Stern, Dunson, Vehtari, and
  Rubin]{gelman2013bayesiandataanalysis}
Gelman, A., Carlin, J.~B., Stern, H.~S., Dunson, D.~B., Vehtari, A., and Rubin,
  D.~B.
\newblock \emph{Bayesian data analysis}.
\newblock Chapman and Hall/CRC, 2013.

\bibitem[Germain et~al.(2016)Germain, Bach, Lacoste, and
  Lacoste-Julien]{germain2016pacbayesian}
Germain, P., Bach, F., Lacoste, A., and Lacoste-Julien, S.
\newblock Pac-bayesian theory meets bayesian inference.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1884--1892, 2016.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010init}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256, 2010.

\bibitem[Gr{\"u}nwald et~al.(2017)Gr{\"u}nwald, Van~Ommen,
  et~al.]{grunwald2017inconsistency}
Gr{\"u}nwald, P., Van~Ommen, T., et~al.
\newblock Inconsistency of {Bayesian} inference for misspecified linear models,
  and a proposal for repairing it.
\newblock \emph{Bayesian Analysis}, 12\penalty0 (4):\penalty0 1069--1103, 2017.

\bibitem[Hafner et~al.(2018)Hafner, Tran, Lillicrap, Irpan, and
  Davidson]{hafner2018noise}
Hafner, D., Tran, D., Lillicrap, T., Irpan, A., and Davidson, J.
\newblock Noise contrastive priors for functional uncertainty.
\newblock \emph{arXiv preprint arXiv:1807.09289}, 2018.

\bibitem[H{\"a}ggstr{\"o}m \& Rosenthal(2007)H{\"a}ggstr{\"o}m and
  Rosenthal]{haggstrom2007varianceclt}
H{\"a}ggstr{\"o}m, O. and Rosenthal, J.
\newblock On variance conditions for markov chain clts.
\newblock \emph{Electronic Communications in Probability}, 12:\penalty0
  454--464, 2007.

\bibitem[Hayou et~al.(2018)Hayou, Doucet, and
  Rousseau]{hayou2018initializationactivation}
Hayou, S., Doucet, A., and Rousseau, J.
\newblock On the selection of initialization and activation function for deep
  neural networks.
\newblock \emph{arXiv preprint arXiv:1805.08266}, 2018.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015prelu}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Heber et~al.(2019)Heber, Trstanova, and Leimkuhler]{heber2019tati}
Heber, F., Trstanova, Z., and Leimkuhler, B.
\newblock Tati-thermodynamic analytics toolkit: Tensorflow-based software for
  posterior sampling in machine learning applications.
\newblock \emph{arXiv preprint arXiv:1903.08640}, 2019.

\bibitem[Heek \& Kalchbrenner(2020)Heek and Kalchbrenner]{heek2019sgmcmc}
Heek, J. and Kalchbrenner, N.
\newblock Bayesian inference for large scale image classification.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2020)}, 2020.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993mdl}
Hinton, G. and Van~Camp, D.
\newblock Keeping neural networks simple by minimizing the description length
  of the weights.
\newblock In \emph{in Proc. of the 6th Ann. ACM Conf. on Computational Learning
  Theory}, 1993.

\bibitem[Hoffman \& Gelman(2014)Hoffman and Gelman]{hoffman2014no}
Hoffman, M.~D. and Gelman, A.
\newblock The no-u-turn sampler: adaptively setting path lengths in
  {Hamiltonian Monte Carlo}.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1593--1623, 2014.

\bibitem[Inoue(2019)]{inoue2019multisampledropout}
Inoue, H.
\newblock Multi-sample dropout for accelerated training and better
  generalization.
\newblock \emph{arXiv preprint arXiv:1905.09788}, 2019.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batchnormalization}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Jansen(2013)]{jansen2013misspecification}
Jansen, L.
\newblock Robust {Bayesian} inference under model misspecification, 2013.
\newblock Master thesis.

\bibitem[Jones et~al.(2004)]{jones2004markovchainclt}
Jones, G.~L. et~al.
\newblock On the {Markov} chain central limit theorem.
\newblock \emph{Probability surveys}, 1\penalty0 (299-320):\penalty0 5--1,
  2004.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variationaldropout}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2575--2583, 2015.

\bibitem[Komaki(1996)]{komaki1996predictive}
Komaki, F.
\newblock On asymptotic properties of predictive distributions.
\newblock \emph{Biometrika}, 83\penalty0 (2):\penalty0 299--313, 1996.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009cifar}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017deepensembles}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems 30}. 2017.

\bibitem[Langevin(1908)]{langevin1908}
Langevin, P.
\newblock Sur la th{\'e}orie du mouvement brownien.
\newblock \emph{Compt. Rendus}, 146:\penalty0 530--533, 1908.

\bibitem[Lao et~al.(2020)Lao, Suter, Langmore, Chimisov, Saxena, Sountsov,
  Moore, Saurous, Hoffman, and Dillon]{lao2020tfpmcmc}
Lao, J., Suter, C., Langmore, I., Chimisov, C., Saxena, A., Sountsov, P.,
  Moore, D., Saurous, R.~A., Hoffman, M.~D., and Dillon, J.~V.
\newblock tfp.mcmc: Modern {Markov} chain {Monte Carlo} tools built for modern
  hardware, 2020.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2018dnngp}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{ICLR}, 2018.

\bibitem[Leimkuhler \& Matthews(2016)Leimkuhler and
  Matthews]{leimkuhler2016molecular}
Leimkuhler, B. and Matthews, C.
\newblock \emph{Molecular Dynamics}.
\newblock Springer, 2016.

\bibitem[Leimkuhler et~al.(2019)Leimkuhler, Matthews, and
  Vlaar]{leimkuhler2019partitionedlangevin}
Leimkuhler, B., Matthews, C., and Vlaar, T.
\newblock Partitioned integrators for thermodynamic parameterization of neural
  networks.
\newblock \emph{arXiv preprint arXiv:1908.11843}, 2019.

\bibitem[Li et~al.(2016)Li, Chen, Carlson, and Carin]{li2016preconditionedsgld}
Li, C., Chen, C., Carlson, D., and Carin, L.
\newblock Preconditioned stochastic gradient {Langevin} dynamics for deep
  neural networks.
\newblock In \emph{Thirtieth AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Liao \& Berg(2019)Liao and Berg]{liao2019sharpeningjensen}
Liao, J. and Berg, A.
\newblock Sharpening jensen's inequality.
\newblock \emph{The American Statistician}, 73\penalty0 (3):\penalty0 278--281,
  2019.

\bibitem[Ma et~al.(2015)Ma, Chen, and Fox]{ma2015complete}
Ma, Y.-A., Chen, T., and Fox, E.
\newblock A complete recipe for stochastic gradient {MCMC}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2917--2925, 2015.

\bibitem[MacKay et~al.(1995)]{mackay1995ensemblelearning}
MacKay, D.~J. et~al.
\newblock Ensemble learning and evidence maximization.
\newblock In \emph{Proc. Nips}, volume~10, pp.\  4083. Citeseer, 1995.

\bibitem[Mandt et~al.(2016)Mandt, McInerney, Abrol, Ranganath, and
  Blei]{DBLP:conf/aistats/MandtMARB16}
Mandt, S., McInerney, J., Abrol, F., Ranganath, R., and Blei, D.~M.
\newblock Variational tempering.
\newblock In \emph{Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics, {AISTATS}}, {JMLR} Workshop and
  Conference Proceedings, 2016.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
Mandt, S., Hoffman, M.~D., and Blei, D.~M.
\newblock Stochastic gradient descent as approximate {Bayesian} inference.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4873--4907, 2017.

\bibitem[Masegosa(2019)]{masegosa2019misspecification}
Masegosa, A.~R.
\newblock Learning under model misspecification: Applications to variational
  and ensemble methods.
\newblock \emph{arXiv preprint, arXiv:19012.08335}, 2019.

\bibitem[Masters \& Luschi(2018)Masters and Luschi]{masters2018batchsize}
Masters, D. and Luschi, C.
\newblock Revisiting small batch training for deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.07612}, 2018.

\bibitem[Murphy(2012)]{murphy2012machine}
Murphy, K.~P.
\newblock \emph{Machine learning: a probabilistic perspective}.
\newblock MIT press, 2012.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Naeini, M.~P., Cooper, G., and Hauskrecht, M.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Nalisnick et~al.(2019)Nalisnick, Hernandez-Lobato, and
  Smyth]{nalisnick2019dropout}
Nalisnick, E., Hernandez-Lobato, J.~M., and Smyth, P.
\newblock Dropout as a structured shrinkage prior.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4712--4722, 2019.

\bibitem[Neal(1995)]{neal1995bayesianneuralnetworks}
Neal, R.~M.
\newblock \emph{Bayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Neal, R.~M. et~al.
\newblock {MCMC} using {Hamiltonian} dynamics.
\newblock \emph{Handbook of {Markov} chain {Monte Carlo}}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Nemenman et~al.(2002)Nemenman, Shafee, and
  Bialek]{nemenman2002entropy}
Nemenman, I., Shafee, F., and Bialek, W.
\newblock Entropy and inference, revisited.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  471--478, 2002.

\bibitem[Noh et~al.(2017)Noh, You, Mun, and Han]{noh2017regularizing}
Noh, H., You, T., Mun, J., and Han, B.
\newblock Regularizing deep neural networks by noise: Its interpretation and
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5109--5118, 2017.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{novak2019bayesiandeepcnngp}
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Hron, J., Abolafia, D.~A.,
  Pennington, J., and Sohl-Dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Nowozin(2018)]{nowozin2018jvi}
Nowozin, S.
\newblock Debiasing evidence approximations: On importance-weighted
  autoencoders and jackknife variational inference.
\newblock In \emph{Sixth International Conference on Learning Representations
  ({ICLR} 2018)}, 2018.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Jain, Eschenhagen, Turner, Yokota,
  and Khan]{osawa2019practicalbdl}
Osawa, K., Swaroop, S., Jain, A., Eschenhagen, R., Turner, R.~E., Yokota, R.,
  and Khan, M.~E.
\newblock Practical deep learning with {Bayesian} principles.
\newblock \emph{arXiv preprint arXiv:1906.02506}, 2019.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019modeluncertainty}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J.~V., Lakshminarayanan, B., and Snoek, J.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2019)}, 2019.

\bibitem[Pearce et~al.(2019)Pearce, Zaki, Brintrup, and
  Neely]{pearce2019expressive}
Pearce, T., Zaki, M., Brintrup, A., and Neely, A.
\newblock Expressive priors in bayesian neural networks: Kernel combinations
  and periodic functions.
\newblock \emph{arXiv preprint arXiv:1905.06076}, 2019.

\bibitem[Perez \& Wang(2017)Perez and Wang]{perez2017dataaugmentation}
Perez, L. and Wang, J.
\newblock The effectiveness of data augmentation in image classification using
  deep learning.
\newblock \emph{arXiv preprint arXiv:1712.04621}, 2017.

\bibitem[Ramamoorthi et~al.(2015)Ramamoorthi, Sriram, and
  Martin]{ramamoorthi2015misspecification}
Ramamoorthi, R.~V., Sriram, K., and Martin, R.
\newblock On posterior concentration in misspecified models.
\newblock \emph{Bayesian Anal.}, 10\penalty0 (4):\penalty0 759--789, 12 2015.
\newblock \doi{10.1214/15-BA941}.

\bibitem[S{\"a}rkk{\"a} \& Solin(2019)S{\"a}rkk{\"a} and
  Solin]{sarkka2019appliedsde}
S{\"a}rkk{\"a}, S. and Solin, A.
\newblock \emph{Applied stochastic differential equations}, volume~10.
\newblock Cambridge University Press, 2019.

\bibitem[Schoenholz et~al.(2017)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2017deepinformationpropagation}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock Deep information propagation.
\newblock In \emph{ICLR}, 2017.

\bibitem[Schucany et~al.(1971)Schucany, Gray, and
  Owen]{schucany1971biasreduction}
Schucany, W., Gray, H., and Owen, D.
\newblock On bias reduction in estimation.
\newblock \emph{Journal of the American Statistical Association}, 66\penalty0
  (335):\penalty0 524--533, 1971.

\bibitem[Shang et~al.(2015)Shang, Zhu, Leimkuhler, and
  Storkey]{shang2015covariancethermostat}
Shang, X., Zhu, Z., Leimkuhler, B., and Storkey, A.~J.
\newblock Covariance-controlled adaptive {Langevin} thermostat for large-scale
  {Bayesian} sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  37--45, 2015.

\bibitem[Shekhovtsov \& Flach(2018)Shekhovtsov and
  Flach]{shekhovtsov2018stochasticnormalizations}
Shekhovtsov, A. and Flach, B.
\newblock Stochastic normalizations as {Bayesian} learning.
\newblock \emph{arXiv preprint arXiv:1811.00639}, 2018.

\bibitem[Simsekli et~al.(2019)Simsekli, Sagun, and
  Gurbuzbalaban]{simsekli2019minibatchnoise}
Simsekli, U., Sagun, L., and Gurbuzbalaban, M.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.06053}, 2019.

\bibitem[Singh \& Krishnan(2019)Singh and
  Krishnan]{singh2019filterresponsenormalization}
Singh, S. and Krishnan, S.
\newblock Filter response normalization layer: Eliminating batch dependence in
  the training of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1911.09737}, 2019.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sugita \& Okamoto(1999)Sugita and Okamoto]{sugita1999replicaexchange}
Sugita, Y. and Okamoto, Y.
\newblock Replica-exchange molecular dynamics method for protein folding.
\newblock \emph{Chemical physics letters}, 314\penalty0 (1-2):\penalty0
  141--151, 1999.

\bibitem[Sun et~al.(2019)Sun, Zhang, Shi, and Grosse]{sun2019functionalvb}
Sun, S., Zhang, G., Shi, J., and Grosse, R.
\newblock Functional variational {Bayesian} neural networks.
\newblock \emph{arXiv preprint arXiv:1903.05779}, 2019.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013momentum}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1139--1147, 2013.

\bibitem[Swendsen \& Wang(1986)Swendsen and
  Wang]{swendsen1986replicamontecarlo}
Swendsen, R.~H. and Wang, J.-S.
\newblock Replica {Monte Carlo} simulation of spin-glasses.
\newblock \emph{Physical review letters}, 57\penalty0 (21):\penalty0 2607,
  1986.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012rmsprop}
Tieleman, T. and Hinton, G.
\newblock {Lecture 6.5---{RmsProp}: Divide the gradient by a running average of
  its recent magnitude}.
\newblock Coursera: Neural Networks for Machine Learning, 2012.

\bibitem[Welling \& Teh(2011)Welling and Teh]{welling2011bayesian}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient {Langevin} dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pp.\  681--688, 2011.

\bibitem[Wen et~al.(2019)Wen, Tran, and Ba]{wen2019batchensemble}
Wen, Y., Tran, D., and Ba, J.
\newblock {BatchEnsemble}: Efficient ensemble of deep neural networks via
  rank-1 perturbation.
\newblock 2019.
\newblock {Bayesian} deep learning workshop 2019.

\bibitem[Wilson(2019)]{wilson2019bayesian}
Wilson, A.~G.
\newblock The case for {B}ayesian deep learning.
\newblock \emph{NYU Courant Technical Report}, 2019.
\newblock Accessible at \url{https://cims.nyu.edu/~andrewgw/caseforbdl.pdf}.

\bibitem[Wilson \& Izmailov(2020)Wilson and
  Izmailov]{wilson2020bayesianperspective}
Wilson, A.~G. and Izmailov, P.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{arXiv preprint arXiv:2002.08791}, 2020.

\bibitem[Wolpert \& Wolf(1995)Wolpert and Wolf]{wolpert1995estimatingentropy}
Wolpert, D.~H. and Wolf, D.~R.
\newblock Estimating functions of probability distributions from a finite set
  of samples.
\newblock \emph{Physical Review E}, 52\penalty0 (6):\penalty0 6841, 1995.

\bibitem[Yaida(2018)]{yaida2018fluctuationdissipationsgd}
Yaida, S.
\newblock Fluctuation-dissipation relations for stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1810.00004}, 2018.

\bibitem[Yang(2019)]{yang2019wideneuralnetworksgp}
Yang, G.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Sun, Duvenaud, and
  Grosse]{zhang2018noisynaturalgradient}
Zhang, G., Sun, S., Duvenaud, D., and Grosse, R.
\newblock Noisy natural gradient as variational inference.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Li, Zhang, Chen, and
  Wilson]{zhang2019cyclicalsgmcmc}
Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A.~G.
\newblock Cyclical stochastic gradient {MCMC} for {Bayesian} deep learning.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2020)}, 2020.

\bibitem[Zhu et~al.(2019)Zhu, Wu, Yu, Wu, and Ma]{zhu2019anisotropicsgdnoise}
Zhu, Z., Wu, J., Yu, B., Wu, L., and Ma, J.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from sharp minima and regularization effects.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML 2019)}, 2019.

\end{thebibliography}
