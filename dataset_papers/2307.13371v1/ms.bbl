\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barlow et~al.(2018)Barlow, O~Conchuir, Thompson, Suresh, Lucas,
  Heinonen, and Kortemme]{barlow2018flex}
Kyle~A Barlow, Shane O~Conchuir, Samuel Thompson, Pooja Suresh, James~E Lucas,
  Markus Heinonen, and Tanja Kortemme.
\newblock Flex ddg: Rosetta ensemble-based estimation of changes in
  protein--protein binding affinity upon mutation.
\newblock \emph{The Journal of Physical Chemistry B}, 122\penalty0
  (21):\penalty0 5389--5399, 2018.

\bibitem[Bengio et~al.(2005)Bengio, Delalleau, and Le~Roux]{bengio2005curse}
Yoshua Bengio, Olivier Delalleau, and Nicolas Le~Roux.
\newblock The curse of dimensionality for local kernel machines.
\newblock \emph{Techn. Rep}, 1258:\penalty0 12, 2005.

\bibitem[Berkenkamp et~al.(2016)Berkenkamp, Schoellig, and
  Krause]{Berkenkamp2016SafeOpt}
Felix Berkenkamp, Angela~P. Schoellig, and Andreas Krause.
\newblock Safe controller optimization for quadrotors with {G}aussian
  processes.
\newblock In \emph{Proc. of the IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 493--496, 2016.

\bibitem[Binois and Wycoff(2022)]{10.1145/3545611}
Micka\"{e}l Binois and Nathan Wycoff.
\newblock A survey on high-dimensional gaussian process modeling with
  application to bayesian optimization.
\newblock \emph{ACM Trans. Evol. Learn. Optim.}, 2\penalty0 (2), aug 2022.
\newblock ISSN 2688-299X.
\newblock \doi{10.1145/3545611}.
\newblock URL \url{https://doi.org/10.1145/3545611}.

\bibitem[Bogunovic et~al.(2016)Bogunovic, Scarlett, Krause, and
  Cevher]{bogunovic2016truncated}
Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher.
\newblock Truncated variance reduction: A unified approach to bayesian
  optimization and level-set estimation.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Chapelle and Li(2011)]{chapelle2011empirical}
Olivier Chapelle and Lihong Li.
\newblock An empirical evaluation of thompson sampling.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Das and Baker(2008)]{das2008macromolecular}
Rhiju Das and David Baker.
\newblock Macromolecular modeling with rosetta.
\newblock \emph{Annu. Rev. Biochem.}, 77:\penalty0 363--382, 2008.

\bibitem[Desautels et~al.(2020)Desautels, Zemla, Lau, Franco, and
  Faissol]{desautels2020rapid}
Thomas Desautels, Adam Zemla, Edmond Lau, Magdalena Franco, and Daniel Faissol.
\newblock Rapid in silico design of antibodies targeting sars-cov-2 using
  machine learning and supercomputing.
\newblock \emph{BioRxiv}, 2020.

\bibitem[Desautels et~al.(2022)Desautels, Arrildt, Zemla, Lau, Zhu, Ricci,
  Cronin, Zost, Binshtein, Scheaffer, et~al.]{desautels2022computationally}
Thomas~A Desautels, Kathryn~T Arrildt, Adam~T Zemla, Edmond~Y Lau, Fangqiang
  Zhu, Dante Ricci, Stephanie Cronin, Seth Zost, Elad Binshtein, Suzanne~M
  Scheaffer, et~al.
\newblock Computationally restoring the potency of a clinical antibody against
  sars-cov-2 omicron subvariants.
\newblock \emph{bioRxiv}, pages 2022--10, 2022.

\bibitem[Djolonga et~al.(2013)Djolonga, Krause, and Cevher]{djolonga2013high}
Josip Djolonga, Andreas Krause, and Volkan Cevher.
\newblock High-dimensional gaussian process bandits.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Eriksson et~al.(2019)Eriksson, Pearce, Gardner, Turner, and
  Poloczek]{eriksson2019scalable}
David Eriksson, Michael Pearce, Jacob Gardner, Ryan~D Turner, and Matthias
  Poloczek.
\newblock Scalable global optimization via local {Bayesian} optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5496--5507, 2019.

\bibitem[Ferreira et~al.(2020)Ferreira, Camacho, and
  Teixeira]{ferreira2020using}
Mafalda~Falc{\~a}o Ferreira, Rui Camacho, and Lu{\'\i}s~F Teixeira.
\newblock Using autoencoders as a weight initialization method on deep neural
  networks for disease detection.
\newblock \emph{BMC Medical Informatics and Decision Making}, 20\penalty0
  (5):\penalty0 1--18, 2020.

\bibitem[Gotovos et~al.(2013)Gotovos, Casati, Hitz, and
  Krause]{10.5555/2540128.2540322}
Alkis Gotovos, Nathalie Casati, Gregory Hitz, and Andreas Krause.
\newblock Active learning for level set estimation.
\newblock In \emph{Proceedings of the Twenty-Third International Joint
  Conference on Artificial Intelligence}, IJCAI '13, page 1344–1350. AAAI
  Press, 2013.
\newblock ISBN 9781577356332.

\bibitem[Kawaguchi et~al.(2016)Kawaguchi, Maruyama, and
  Zheng]{kawaguchi2016global}
Kenji Kawaguchi, Yu~Maruyama, and Xiaoyu Zheng.
\newblock Global continuous optimization with error bound and fast convergence.
\newblock \emph{Journal of Artificial Intelligence Research}, 56:\penalty0
  153--195, 2016.

\bibitem[Kirschner et~al.(2019)Kirschner, Mutny, Hiller, Ischebeck, and
  Krause]{kirschner2019adaptive}
Johannes Kirschner, Mojmir Mutny, Nicole Hiller, Rasmus Ischebeck, and Andreas
  Krause.
\newblock Adaptive and safe bayesian optimization in high dimensions via
  one-dimensional subspaces.
\newblock In \emph{International Conference on Machine Learning}, pages
  3429--3438. PMLR, 2019.

\bibitem[Letham et~al.(2020)Letham, Calandra, Rai, and Bakshy]{Letham2020Re}
Benjamin Letham, Roberto Calandra, Akshara Rai, and Eytan Bakshy.
\newblock Re-examining linear embeddings for high-dimensional {B}ayesian
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 33},
  NeurIPS, 2020.

\bibitem[Makarova et~al.(2021)Makarova, Usmanova, Bogunovic, and
  Krause]{makarova2021risk}
Anastasia Makarova, Ilnura Usmanova, Ilija Bogunovic, and Andreas Krause.
\newblock Risk-averse heteroscedastic bayesian optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[McIntire et~al.(2016)McIntire, Ratner, and Ermon]{mcintire2016sparse}
Mitchell McIntire, Daniel Ratner, and Stefano Ermon.
\newblock Sparse gaussian processes for bayesian optimization.
\newblock In \emph{UAI}, 2016.

\bibitem[Merrill et~al.(2021)Merrill, Fern, Fern, and
  Dolatnia]{JMLR:v22:18-220}
Erich Merrill, Alan Fern, Xiaoli Fern, and Nima Dolatnia.
\newblock An empirical study of bayesian optimization: Acquisition versus
  partition.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (4):\penalty0 1--25, 2021.

\bibitem[Moss et~al.(2023)Moss, Ober, and Picheny]{moss2023inducing}
Henry~B Moss, Sebastian~W Ober, and Victor Picheny.
\newblock Inducing point allocation for sparse gaussian processes in
  high-throughput bayesian optimisation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 5213--5230. PMLR, 2023.

\bibitem[Munos(2011)]{munos2011optimistic}
R{\'e}mi Munos.
\newblock Optimistic optimization of a deterministic function without the
  knowledge of its smoothness.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Munos(2014)]{8187198}
Rémi Munos.
\newblock \emph{From Bandits to Monte-Carlo Tree Search: The Optimistic
  Principle Applied to Optimization and Planning}.
\newblock now, 2014.

\bibitem[Munteanu et~al.(2019)Munteanu, Nayebi, and Poloczek]{HeSBO19}
Alex Munteanu, Amin Nayebi, and Matthias Poloczek.
\newblock A framework for bayesian optimization in embedded subspaces.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {(ICML)}}, 2019.
\newblock Accepted for publication. The code is available at
  https://github.com/aminnayebi/HesBO.

\bibitem[Papenmeier et~al.(2022)Papenmeier, Nardi, and
  Poloczek]{papenmeier2022increasing}
Leonard Papenmeier, Luigi Nardi, and Matthias Poloczek.
\newblock Increasing the scope as you learn: Adaptive bayesian optimization in
  nested subspaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Rasmussen and Williams(2006)]{rasmussen:williams:2006}
C.~E. Rasmussen and C.~K.~I. Williams.
\newblock \emph{Gaussian Processes for Machine Learning}.
\newblock MIT Press, 2006.

\bibitem[Rives et~al.(2021)Rives, Meier, Sercu, Goyal, Lin, Liu, Guo, Ott,
  Zitnick, Ma, et~al.]{rives2021biological}
Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason
  Liu, Demi Guo, Myle Ott, C~Lawrence Zitnick, Jerry Ma, et~al.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (15):\penalty0 e2016239118, 2021.

\bibitem[Salgia et~al.(2021)Salgia, Vakili, and Zhao]{salgia2021domain}
Sudeep Salgia, Sattar Vakili, and Qing Zhao.
\newblock A domain-shrinking based bayesian optimization algorithm with
  order-optimal regret performance.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28836--28847, 2021.

\bibitem[Sazanovich et~al.(2021)Sazanovich, Nikolskaya, Belousov, and
  Shpilman]{pmlr-v133-sazanovich21a}
Mikita Sazanovich, Anastasiya Nikolskaya, Yury Belousov, and Aleksei Shpilman.
\newblock Solving black-box optimization challenge via learning search space
  partition for local bayesian optimization.
\newblock In Hugo~Jair Escalante and Katja Hofmann, editors, \emph{Proceedings
  of the NeurIPS 2020 Competition and Demonstration Track}, volume 133 of
  \emph{Proceedings of Machine Learning Research}, pages 77--85. PMLR, 06--12
  Dec 2021.

\bibitem[Shahriari et~al.(2016)Shahriari, Swersky, Wang, Adams, and
  de~Freitas]{7352306}
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan~P. Adams, and Nando de~Freitas.
\newblock Taking the human out of the loop: A review of bayesian optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2016.
\newblock \doi{10.1109/JPROC.2015.2494218}.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In \emph{26th Annual Conference on Neural Information Processing
  Systems 2012}, pages 2951--2959, 2012.

\bibitem[Song et~al.(2018)Song, Tokpanov, Chen, Fleischman, Fountaine, Atwater,
  and Yue]{song:18.2}
Jialin Song, Yury Tokpanov, Yuxin Chen, Dagny Fleischman, Kate Fountaine, Harry
  Atwater, and Yisong Yue.
\newblock Optimizing photonic nanostructures via multi-fidelity gaussian
  processes.
\newblock \emph{NeurIPS Workshop on Machine Learning for Molecules and
  Materials}, 2018.

\bibitem[Song et~al.(2022)Song, Xue, Huang, and Qian]{song2022monte}
Lei Song, Ke~Xue, Xiaobin Huang, and Chao Qian.
\newblock Monte carlo tree search based variable selection for high dimensional
  bayesian optimization.
\newblock \emph{arXiv preprint arXiv:2210.01628}, 2022.

\bibitem[Srinivas et~al.(2009)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2009gaussian}
Niranjan Srinivas, Andreas Krause, Sham~M Kakade, and Matthias Seeger.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock \emph{arXiv preprint arXiv:0912.3995}, 2009.

\bibitem[Sui et~al.(2018)Sui, Zhuang, Burdick, and Yue]{sui2018stagewise}
Yanan Sui, Vincent Zhuang, Joel Burdick, and Yisong Yue.
\newblock Stagewise safe bayesian optimization with gaussian processes.
\newblock In \emph{International conference on machine learning}, pages
  4781--4789. PMLR, 2018.

\bibitem[Tripp et~al.(2020)Tripp, Daxberger, and
  Hern{\'a}ndez-Lobato]{tripp2020sample}
Austin Tripp, Erik Daxberger, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Sample-efficient optimization in the latent space of deep generative
  models via weighted retraining.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11259--11272, 2020.

\bibitem[Wabersich and Toussaint(2016)]{wabersich2016advancing}
Kim~Peter Wabersich and Marc Toussaint.
\newblock Advancing bayesian optimization: The mixed-global-local (mgl) kernel
  and length-scale cool down.
\newblock \emph{arXiv preprint arXiv:1612.03117}, 2016.

\bibitem[Wang et~al.(2020)Wang, Fonseca, and Tian]{wang2020learning}
Linnan Wang, Rodrigo Fonseca, and Yuandong Tian.
\newblock Learning search space partition for black-box optimization using
  monte carlo tree search.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 19511--19522, 2020.

\bibitem[Wang and Jegelka(2017)]{wang2017max}
Zi~Wang and Stefanie Jegelka.
\newblock Max-value entropy search for efficient bayesian optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  3627--3635. PMLR, 2017.

\bibitem[Wang et~al.(2016{\natexlab{a}})Wang, Zhou, and
  Jegelka]{wang2016optimization}
Zi~Wang, Bolei Zhou, and Stefanie Jegelka.
\newblock Optimization as estimation with gaussian processes in bandit
  settings.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1022--1031.
  PMLR, 2016{\natexlab{a}}.

\bibitem[Wang et~al.(2016{\natexlab{b}})Wang, Hutter, Zoghi, Matheson, and
  de~Feitas]{wang2016bayesian}
Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, and Nando de~Feitas.
\newblock Bayesian optimization in a billion dimensions via random embeddings.
\newblock \emph{Journal of Artificial Intelligence Research}, 55:\penalty0
  361--387, 2016{\natexlab{b}}.

\bibitem[Wilson and Nickisch(2015)]{wilson2015kernel}
Andrew Wilson and Hannes Nickisch.
\newblock Kernel interpolation for scalable structured gaussian processes
  (kiss-gp).
\newblock In \emph{International conference on machine learning}, pages
  1775--1784. PMLR, 2015.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and
  Xing]{pmlr-v51-wilson16}
Andrew~Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric~P. Xing.
\newblock Deep kernel learning.
\newblock volume~51 of \emph{Proceedings of Machine Learning Research}, pages
  370--378, Cadiz, Spain, 09--11 May 2016. PMLR.

\bibitem[Wistuba and Grabocka(2021)]{wistuba2021few}
Martin Wistuba and Josif Grabocka.
\newblock Few-shot bayesian optimization with deep kernel surrogates.
\newblock \emph{arXiv preprint arXiv:2101.07667}, 2021.

\bibitem[Wu et~al.(2019)Wu, Kan, Lewis, Wittmann, and Arnold]{wu2019machine}
Zachary Wu, SB~Jennifer Kan, Russell~D Lewis, Bruce~J Wittmann, and Frances~H
  Arnold.
\newblock Machine learning-assisted directed protein evolution with
  combinatorial libraries.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (18):\penalty0 8852--8858, 2019.

\bibitem[Yang et~al.(2019)Yang, Wu, and Arnold]{yang2019machine}
Kevin~K Yang, Zachary Wu, and Frances~H Arnold.
\newblock Machine-learning-guided directed evolution for protein engineering.
\newblock \emph{Nature methods}, 16\penalty0 (8):\penalty0 687--694, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Nord, and Chen]{zhang2022learning}
Fengxue Zhang, Brian Nord, and Yuxin Chen.
\newblock Learning representation for bayesian optimization with collision-free
  regularization.
\newblock \emph{arXiv preprint arXiv:2203.08656}, 2022.

\end{thebibliography}
