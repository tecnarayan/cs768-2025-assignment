\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Reynolds and McDonell(2021)]{reynolds2021prompt}
Laria Reynolds and Kyle McDonell.
\newblock Prompt programming for large language models: Beyond the few-shot paradigm.
\newblock In \emph{Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems}, pages 1--7, 2021.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 22199--22213, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber, Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Zelikman et~al.(2022)Zelikman, Wu, Mu, and Goodman]{zelikman2022star}
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
\newblock Star: Bootstrapping reasoning with reasoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 15476--15488, 2022.

\bibitem[Allen-Zhu and Li(2023)]{allen2023physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 3.2, knowledge manipulation.
\newblock \emph{arXiv preprint arXiv:2309.14402}, 2023.

\bibitem[Berglund et~al.(2023)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans]{berglund2023reversal}
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa~Cooper Stickland, Tomasz Korbak, and Owain Evans.
\newblock The reversal curse: Llms trained on" a is b" fail to learn" b is a".
\newblock \emph{arXiv preprint arXiv:2309.12288}, 2023.

\bibitem[Guo et~al.(2024)Guo, Wang, Guo, Tan, Bian, and Yang]{guo2024mitigating}
Qingyan Guo, Rui Wang, Junliang Guo, Xu~Tan, Jiang Bian, and Yujiu Yang.
\newblock Mitigating reversal curse via semantic-aware permutation training.
\newblock \emph{arXiv preprint arXiv:2403.00758}, 2024.

\bibitem[Golovneva et~al.(2024)Golovneva, Allen-Zhu, Weston, and Sukhbaatar]{golovneva2024reverse}
Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, and Sainbayar Sukhbaatar.
\newblock Reverse training to nurse the reversal curse.
\newblock \emph{arXiv preprint arXiv:2403.13799}, 2024.

\bibitem[Lv et~al.(2023)Lv, Zhang, Xie, Tu, Chen, Wen, and Yan]{lv2023we}
Ang Lv, Kaiyi Zhang, Shufang Xie, Quan Tu, Yuhan Chen, Ji-Rong Wen, and Rui Yan.
\newblock Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse.
\newblock \emph{arXiv preprint arXiv:2311.07468}, 2023.

\bibitem[Tian et~al.(2023{\natexlab{a}})Tian, Wang, Chen, and Du]{tian2023scan}
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du.
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer, 2023{\natexlab{a}}.

\bibitem[Feng et~al.(2024)Feng, Zhang, Gu, Ye, He, and Wang]{feng2024towards}
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di~He, and Liwei Wang.
\newblock Towards revealing the mystery behind chain of thought: a theoretical perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pages 9118--9147. PMLR, 2022.

\bibitem[Jung et~al.(2022)Jung, Qin, Welleck, Brahman, Bhagavatula, Bras, and Choi]{jung2022maieutic}
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan~Le Bras, and Yejin Choi.
\newblock Maieutic prompting: Logically consistent reasoning with recursive explanations.
\newblock \emph{arXiv preprint arXiv:2205.11822}, 2022.

\bibitem[Han et~al.(2024)Han, Ransom, Perfors, and Kemp]{han2024inductive}
Simon~Jerome Han, Keith~J Ransom, Andrew Perfors, and Charles Kemp.
\newblock Inductive reasoning in humans and large language models.
\newblock \emph{Cognitive Systems Research}, 83:\penalty0 101155, 2024.

\bibitem[Wang et~al.(2024)Wang, Amayuelas, Zhang, Pan, Chen, and Wang]{wang2024understanding}
Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen, and William~Yang Wang.
\newblock Understanding the reasoning ability of language models from the perspective of reasoning paths aggregation.
\newblock \emph{arXiv preprint arXiv:2402.03268}, 2024.

\bibitem[Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma]{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock \emph{arXiv preprint arXiv:2111.02080}, 2021.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Nichani et~al.(2024)Nichani, Damian, and Lee]{nichani2024transformers}
Eshaan Nichani, Alex Damian, and Jason~D Lee.
\newblock How transformers learn causal structure with gradient descent.
\newblock \emph{arXiv preprint arXiv:2402.14735}, 2024.

\bibitem[Brinkmann et~al.(2024)Brinkmann, Sheshadri, Levoso, Swoboda, and Bartelt]{brinkmann2024mechanistic}
Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt.
\newblock A mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task.
\newblock \emph{arXiv preprint arXiv:2402.11917}, 2024.

\bibitem[Valmeekam et~al.(2022)Valmeekam, Olmo, Sreedharan, and Kambhampati]{valmeekam2022large}
Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.
\newblock Large language models still can't plan (a benchmark for llms on planning and reasoning about change).
\newblock \emph{arXiv preprint arXiv:2206.10498}, 2022.

\bibitem[Chang et~al.(2023)Chang, Wang, Wang, Wu, Yang, Zhu, Chen, Yi, Wang, Wang, et~al.]{chang2023survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Tigges, Zhang, Biderman, Raginsky, and Ringer]{zhang2024transformer}
Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, and Talia Ringer.
\newblock Transformer-based models are not yet perfect at learning to emulate structural recursion.
\newblock \emph{arXiv preprint arXiv:2401.12947}, 2024.

\bibitem[Qi et~al.(2023)Qi, Li, Hui, Wang, Li, Wu, and Laili]{qi2023investigation}
Chengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, and Yuanjun Laili.
\newblock An investigation of llms' inefficacy in understanding converse relations.
\newblock \emph{arXiv preprint arXiv:2310.05163}, 2023.

\bibitem[Luo et~al.(2024)Luo, Gu, Li, Li, Lin, Li, and Yang]{luo2024chain}
Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li, and Yujiu Yang.
\newblock Chain of history: Learning and forecasting with llms for temporal knowledge graph completion.
\newblock \emph{arXiv preprint arXiv:2401.06072}, 2024.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J Reddi, and Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\bibitem[Bhattamishra et~al.(2020{\natexlab{a}})Bhattamishra, Ahuja, and Goyal]{bhattamishra2020ability}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the ability and limitations of transformers to recognize formal languages.
\newblock \emph{arXiv preprint arXiv:2009.11264}, 2020{\natexlab{a}}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{b}})Bhattamishra, Patel, and Goyal]{bhattamishra2020computational}
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
\newblock On the computational power of transformers and its implications in sequence modeling.
\newblock \emph{arXiv preprint arXiv:2006.09286}, 2020{\natexlab{b}}.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz Kaiser.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[P{\'e}rez et~al.(2021)P{\'e}rez, Barcel{\'o}, and Marinkovic]{perez2021attention}
Jorge P{\'e}rez, Pablo Barcel{\'o}, and Javier Marinkovic.
\newblock Attention is turing-complete.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (75):\penalty0 1--35, 2021.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and Zhang]{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In \emph{International Conference on Machine Learning}, pages 5793--5831. PMLR, 2022.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann, Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1:\penalty0 1, 2021.

\bibitem[Likhosherstov et~al.(2021)Likhosherstov, Choromanski, and Weller]{likhosherstov2021expressive}
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller.
\newblock On the expressive power of self-attention matrices.
\newblock \emph{arXiv preprint arXiv:2106.03764}, 2021.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Panigrahi, Ge, and Arora]{zhao2023transformers}
Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora.
\newblock Do transformers parse while predicting the masked word?
\newblock \emph{arXiv preprint arXiv:2303.08117}, 2023.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock \emph{arXiv preprint arXiv:2105.11115}, 2021.

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh, Slone, Gur-Ari, Dyer, and Neyshabur]{anil2022exploring}
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.
\newblock Exploring length generalization in large language models.
\newblock \emph{arXiv preprint arXiv:2207.04901}, 2022.

\bibitem[Barak et~al.(2022)Barak, Edelman, Goel, Kakade, Malach, and Zhang]{barak2022hidden}
Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang.
\newblock Hidden progress in deep learning: Sgd learns parities near the computational limit.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21750--21764, 2022.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30583--30598, 2022.

\bibitem[Von~Oswald et~al.(2022)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock \emph{arXiv preprint arXiv:2212.07677}, 2022.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{bai2023transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with in-context algorithm selection.
\newblock \emph{arXiv preprint arXiv:2306.04637}, 2023.

\bibitem[Li et~al.(2023)Li, Song, Xia, Yu, and Zhou]{li2023closeness}
Shuai Li, Zhao Song, Yu~Xia, Tong Yu, and Tianyi Zhou.
\newblock The closeness of in-context learning and weight shifting for softmax regression.
\newblock \emph{arXiv preprint arXiv:2304.13276}, 2023.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 35151--35174. PMLR, 2023.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2022transformers}
Bingbin Liu, Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Chen, and Ma]{wei2022statistically}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating turing machines with transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 12071--12083, 2022{\natexlab{b}}.

\bibitem[Mei and Wu(2023)]{mei2023deep}
Song Mei and Yuchen Wu.
\newblock Deep networks as denoising algorithms: Sample-efficient learning of diffusion models in high-dimensional graphical models.
\newblock \emph{arXiv preprint arXiv:2309.11420}, 2023.

\bibitem[Lin et~al.(2023)Lin, Bai, and Mei]{lin2023transformers}
Licong Lin, Yu~Bai, and Song Mei.
\newblock Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining.
\newblock \emph{arXiv preprint arXiv:2310.08566}, 2023.

\bibitem[Zhang et~al.(2020)Zhang, He, Sra, and Jadbabaie]{zhang2020why}
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie.
\newblock Why gradient clipping accelerates training: A theoretical justification for adaptivity.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl-Dickstein, and Novak]{hron2020infinite}
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak.
\newblock Infinite attention: Nngp and ntk for deep attention networks.
\newblock In \emph{International Conference on Machine Learning}, pages 4376--4386. PMLR, 2020.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2022tensor}
Greg Yang, Edward~J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Boix-Adsera et~al.(2023)Boix-Adsera, Littwin, Abbe, Bengio, and Susskind]{boix2023transformers}
Enric Boix-Adsera, Etai Littwin, Emmanuel Abbe, Samy Bengio, and Joshua Susskind.
\newblock Transformers learn through gradual rank increase.
\newblock \emph{arXiv preprint arXiv:2306.07042}, 2023.

\bibitem[Bietti et~al.(2023)Bietti, Cabannes, Bouchacourt, Jegou, and Bottou]{bietti2023birth}
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou.
\newblock Birth of a transformer: A memory viewpoint.
\newblock \emph{arXiv preprint arXiv:2306.00802}, 2023.

\bibitem[Jelassi et~al.(2022)Jelassi, Sander, and Li]{jelassi2022vision}
Samy Jelassi, Michael Sander, and Yuanzhi Li.
\newblock Vision transformers provably learn spatial structure.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 37822--37836, 2022.

\bibitem[Snell et~al.(2021)Snell, Zhong, Klein, and Steinhardt]{snell2021approximating}
Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt.
\newblock Approximating how single head attention learns.
\newblock \emph{arXiv preprint arXiv:2103.07601}, 2021.

\bibitem[Mahankali et~al.(2023)Mahankali, Hashimoto, and Ma]{mahankali2023one}
Arvind Mahankali, Tatsunori~B Hashimoto, and Tengyu Ma.
\newblock One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention.
\newblock \emph{arXiv preprint arXiv:2307.03576}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\bibitem[Fu et~al.(2024)Fu, Guo, Bai, and Mei]{fu2024can}
Hengyu Fu, Tianyu Guo, Yu~Bai, and Song Mei.
\newblock What can a single attention layer learn? a study through the random features lens.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023context}
Yu~Huang, Yuan Cheng, and Yingbin Liang.
\newblock In-context convergence of transformers.
\newblock \emph{arXiv preprint arXiv:2310.05249}, 2023.

\bibitem[Tian et~al.(2023{\natexlab{b}})Tian, Wang, Zhang, Chen, and Du]{tian2023joma}
Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du.
\newblock Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention.
\newblock \emph{arXiv preprint arXiv:2310.00535}, 2023{\natexlab{b}}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 38--45, Online, October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Laurent and Massart(2000)]{laurent2000adaptive}
Beatrice Laurent and Pascal Massart.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock \emph{Annals of Statistics}, pages 1302--1338, 2000.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\end{thebibliography}
