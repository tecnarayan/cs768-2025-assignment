\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, and
  et~al.]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., and et~al.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ahuja et~al.(2011)Ahuja, Surana, and Cliff]{AhujaSuranaCliff2011}
Ahuja, S., Surana, A., and Cliff, E.
\newblock Reduced-order models for control of stratified flows in buildings.
\newblock In \emph{American Control Conference (ACC)}, pp.\  2083--2088. IEEE,
  2011.

\bibitem[Antos et~al.(2008)Antos, Szepesv\'ari, and Munos]{AntosSzepesvariML08}
Antos, A., Szepesv\'ari, {\relax Cs}., and Munos, R.
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71:\penalty0 89--129, 2008.

\bibitem[Asadi et~al.(2018)Asadi, Misra, and Littman]{AsadiMisraLittman2018}
Asadi, K., Misra, D., and Littman, M.~L.
\newblock Lipschitz continuity in model-based reinforcement learning.
\newblock \emph{arXiv:1804.07193}, 2018.

\bibitem[Baird \& Klopf(1993)Baird and Klopf]{baird1993reinforcement}
Baird, L. and Klopf, A.~H.
\newblock Reinforcement learning with high-dimensional, continuous actions.
\newblock \emph{Wright Laboratory, Wright-Patterson Air Force Base, Tech. Rep},
  1993.

\bibitem[Balan et~al.(2017)Balan, Singh, and Zou]{BalanSinghZou2017}
Balan, R., Singh, M., and Zou, D.
\newblock Lipschitz properties for deep convolutional networks.
\newblock \emph{arXiv:1701.05217}, 2017.

\bibitem[Barron(1994)]{Barron1994}
Barron, A.~R.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock \emph{Machine Learning}, 14\penalty0 (1):\penalty0 115--133, Jan
  1994.

\bibitem[Belletti et~al.(2018)Belletti, Haziza, Gomes, and
  Bayen]{BellettiHaziza2018}
Belletti, F., Haziza, D., Gomes, G., and Bayen, A.~M.
\newblock Expert level control of ramp metering based on multi-task deep
  reinforcement learning.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems},
  19\penalty0 (4):\penalty0 1198--1207, 2018.

\bibitem[Bertsekas(2013)]{Bertsekas2013}
Bertsekas, D.~P.
\newblock \emph{Abstract dynamic programming}.
\newblock Athena Scientific Belmont, 2013.

\bibitem[Borggaard et~al.(2009)Borggaard, Burns, Surana, and
  Zietsman]{BorggaardBurnsSuranaZietsman2009}
Borggaard, J., Burns, J.~A., Surana, A., and Zietsman, L.
\newblock Control, estimation and optimization of energy efficient buildings.
\newblock In \emph{American Control Conference (ACC)}, pp.\  837--841, 2009.

\bibitem[Brunton \& Noack(2015)Brunton and Noack]{BruntonNoack2015}
Brunton, S.~L. and Noack, B.~R.
\newblock Closed-loop turbulence control: Progress and challenges.
\newblock \emph{Applied Mechanics Reviews}, 67\penalty0 (5), 2015.

\bibitem[Burns \& Hu(2013)Burns and Hu]{BurnsHu2013}
Burns, J.~A. and Hu, W.
\newblock Approximation methods for boundary control of the {B}oussinesq
  equations.
\newblock In \emph{IEEE Conference on Decision and Control (CDC)}, pp.\
  454--459, 2013.

\bibitem[Burns et~al.(2016)Burns, He, and Hu]{BurnsHeHu2016}
Burns, J.~A., He, X., and Hu, W.
\newblock Feedback stabilization of a thermal fluid system with mixed boundary
  control.
\newblock \emph{Computers \& Mathematics with Applications}, 2016.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, and
  Peters]{DeisenrothNeumannPeters2013}
Deisenroth, M.~P., Neumann, G., and Peters, J.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and Trends in Robotics}, 2\penalty0 (1-2):\penalty0
  1--142, 2013.

\bibitem[del R~Mill{\'a}n et~al.(2002)del R~Mill{\'a}n, Posenato, and
  Dedieu]{delrmillan2002continuous}
del R~Mill{\'a}n, J., Posenato, D., and Dedieu, E.
\newblock {Continuous-Action Q-Learning}.
\newblock \emph{Machine Learning}, 2002.

\bibitem[Dulac-Arnold et~al.(2012)Dulac-Arnold, Denoyer, Preux, and
  Gallinari]{dulac2012fast}
Dulac-Arnold, G., Denoyer, L., Preux, P., and Gallinari, P.
\newblock Fast reinforcement learning with large action sets using
  error-correcting output codes for mdp factorization.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  180--194. Springer, 2012.

\bibitem[Dulac-Arnold et~al.(2015)Dulac-Arnold, Evans, van Hasselt, Sunehag,
  Lillicrap, Hunt, Mann, Weber, Degris, and Coppin]{dulac2015deep}
Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt,
  J., Mann, T., Weber, T., Degris, T., and Coppin, B.
\newblock Deep reinforcement learning in large discrete action spaces.
\newblock \emph{arXiv:1512.07679}, 2015.

\bibitem[Duriez et~al.(2016)Duriez, Brunton, and Noack]{DuriezBruntonNoack2016}
Duriez, T., Brunton, S.~L., and Noack, B.~R.
\newblock \emph{Machine Learning Control--Taming Nonlinear Dynamics and
  Turbulence}, volume 116 of \emph{Fluid mechanics and its applications}.
\newblock Springer, 2016.

\bibitem[Farahmand et~al.(2009)Farahmand, Ghavamzadeh, Szepesv\'ari, and
  Mannor]{FarahmandACC09}
Farahmand, A.-m., Ghavamzadeh, M., Szepesv\'ari, {\relax Cs}., and Mannor, S.
\newblock Regularized fitted {Q}-iteration for planning in continuous-space
  {M}arkovian {D}ecision {P}roblems.
\newblock In \emph{Proceedings of American Control Conference (ACC)}, pp.\
  725--730, June 2009.

\bibitem[Farahmand et~al.(2016{\natexlab{a}})Farahmand, Ghavamzadeh,
  Szepesv\'ari, and Mannor]{FarahmandGhavamzadehSzepesvariMannor2016}
Farahmand, A.-m., Ghavamzadeh, M., Szepesv\'ari, {\relax Cs}., and Mannor, S.
\newblock Regularized policy iteration with nonparametric function spaces.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 17\penalty0
  (139):\penalty0 1--66, 2016{\natexlab{a}}.

\bibitem[Farahmand et~al.(2016{\natexlab{b}})Farahmand, Nabi, Grover, and
  Nikovski]{FarahmandNabiGroverNikovski2016}
Farahmand, A.-m., Nabi, S., Grover, P., and Nikovski, D.~N.
\newblock Learning to control partial differential equations: Regularized
  fitted {Q}-iteration approach.
\newblock In \emph{IEEE Conference on Decision and Control (CDC)}, pp.\
  4578--4585, December 2016{\natexlab{b}}.

\bibitem[Farahmand et~al.(2017)Farahmand, Nabi, and
  Nikovski]{FarahmandNabiNikovski2017}
Farahmand, A.-m., Nabi, S., and Nikovski, D.~N.
\newblock Deep reinforcement learning for partial differential equation
  control.
\newblock In \emph{American Control Conference (ACC)}, 2017.

\bibitem[Foures et~al.(2014)Foures, Caulfield, and
  Schmid]{FouresCaulfieldSchmid2014}
Foures, D., Caulfield, C.-c., and Schmid, P.~J.
\newblock Optimal mixing in two-dimensional plane poiseuille flow at finite
  {P}\'eclet number.
\newblock \emph{Journal of Fluid Mechanics}, 748:\penalty0 241--277, 2014.

\bibitem[Gaskett et~al.(1999)Gaskett, Wettergreen, and
  Zelinsky]{gaskett1999qlearning}
Gaskett, C., Wettergreen, D., and Zelinsky, A.
\newblock {Q-Learning in Continuous State and Action Spaces}.
\newblock In \emph{Advanced Topics in Artificial Intelligence}. 1999.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{xavier2010deepnn}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, 2010.

\bibitem[Guyer et~al.(2009)Guyer, Wheeler, and Warren]{GuyerWheelerWarren2009}
Guyer, J.~E., Wheeler, D., and Warren, J.~A.
\newblock {FiPy}: Partial differential equations with {P}ython.
\newblock \emph{Computing in Science and Engineering}, 11\penalty0
  (3):\penalty0 6--15, 2009.
\newblock URL \url{http://www.ctcms.nist.gov/fipy}.

\bibitem[Gy{\"o}rfi et~al.(2002)Gy{\"o}rfi, Kohler, Krzy\.zak, and
  Walk]{Gyorfi02}
Gy{\"o}rfi, L., Kohler, M., Krzy\.zak, A., and Walk, H.
\newblock \emph{A Distribution-Free Theory of Nonparametric Regression}.
\newblock Springer Verlag, New York, 2002.

\bibitem[He et~al.(2015)He, Chen, He, Gao, Li, Deng, and Ostendorf]{he2015deep}
He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., and Ostendorf, M.
\newblock Deep reinforcement learning with an unbounded action space.
\newblock \emph{arXiv:1511.04636}, 2015.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{sergey2015batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv:1502.03167}, 2015.

\bibitem[Kim et~al.(2013)Kim, Choi, and Tan]{kim2013bio}
Kim, K.~J., Choi, H.~R., and Tan, X.
\newblock Biomimetic robotic artificial muscles.
\newblock \emph{World Scientific}, 2013.

\bibitem[Kober et~al.(2013)Kober, Andrew~Bagnell, and Peters]{kober2013}
Kober, J., Andrew~Bagnell, J., and Peters, J.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32:\penalty0
  1238--1274, 09 2013.

\bibitem[Krstic \& Smyshlyaev(2008)Krstic and Smyshlyaev]{KrsticSmyshlyaev2008}
Krstic, M. and Smyshlyaev, A.
\newblock \emph{Boundary control of PDEs: A course on backstepping designs},
  volume~16.
\newblock {SIAM}, 2008.

\bibitem[Lazaric et~al.(2016)Lazaric, Ghavamzadeh, and
  Munos]{LazaricGhavamzadehMunos2016}
Lazaric, A., Ghavamzadeh, M., and Munos, R.
\newblock Analysis of classification-based policy iteration algorithms.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 17\penalty0
  (19):\penalty0 1--30, 2016.

\bibitem[Lighthill \& Whitham(1955)Lighthill and Whitham]{lit1955}
Lighthill, M. and Whitham, J.
\newblock On kinematic waves. i: Flow movement in long rivers. ii: A theory of
  traffic flow on long crowded roads.
\newblock pp.\  229:281--345, 1955.

\bibitem[Lillicrap et~al.(2016)Lillicrap, J.~Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2016}
Lillicrap, T.~P., J.~Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock 2016.

\bibitem[Lions(1971)]{jacques1971}
Lions, J.~L.
\newblock Optimal control of systems governed by partial differential
  equations.
\newblock 1971.

\bibitem[Mohri et~al.(2012)Mohri, Rostamizadeh, and Talwalkar]{Mohri2012mlfund}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of Machine Learning}.
\newblock The MIT Press, 2012.
\newblock ISBN 026201825X, 9780262018258.

\bibitem[Montgomery \& Levine(2016)Montgomery and Levine]{WilliamLevine2016}
Montgomery, W.~H. and Levine, S.
\newblock Guided policy search as approximate mirror descent.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS -
  29)}, pp.\  4008--4016, 2016.

\bibitem[Pazis \& Parr(2011)Pazis and Parr]{pazis2011generalized}
Pazis, J. and Parr, R.
\newblock Generalized value functions for large action sets.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning}, pp.\  1185--1192, 2011.

\bibitem[Popescu et~al.(2008)Popescu, Petrisor, and
  Drighiciu]{Popescu2008aircondition}
Popescu, M.~C., Petrisor, A., and Drighiciu, M.~A.
\newblock Modelling and simulation of a variable speed air-conditioning system.
\newblock In \emph{IEEE International Conference on Automation, Quality and
  Testing, Robotics}, volume~2, pp.\  115--120, May 2008.

\bibitem[Richards(1956)]{Richards1956}
Richards, P.~I.
\newblock Shock waves on the highway.
\newblock \emph{Operations Research}, 4\penalty0 (1):\penalty0 42--51, 1956.

\bibitem[Sallans \& Hinton(2004)Sallans and Hinton]{sallans2004reinforcement}
Sallans, B. and Hinton, G.~E.
\newblock Reinforcement learning with factored states and actions.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 5:\penalty0
  1063--1088, 2004.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{schulman2015}
Schulman, J., Levine, S., Moritz, P., Jordan, M.~I., and Abbeel, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2016high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silverlever2014}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{Proceedings of the 31st International Conference on on
  Machine Learning (ICML)}, pp.\  387--395. JMLR.org, 2014.

\bibitem[Steinwart \& Christmann(2008)Steinwart and
  Christmann]{SteinwartChritmann2008}
Steinwart, I. and Christmann, A.
\newblock \emph{Support Vector Machines}.
\newblock Springer, 2008.

\bibitem[Sunehag et~al.(2015)Sunehag, Evans, Dulac-Arnold, Zwols, Visentin, and
  Coppin]{sunehag2015deep}
Sunehag, P., Evans, R., Dulac-Arnold, G., Zwols, Y., Visentin, D., and Coppin,
  B.
\newblock Deep reinforcement learning with attention for slate markov decision
  processes with high-dimensional states and actions.
\newblock \emph{arXiv:1512.01124}, 2015.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{Sutton98}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock {The MIT Press}, 1998.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{SuttonMcAleesterSinghMansour2000}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS -
  12)}, 2000.

\bibitem[Szepesv\'ari(2010)]{SzepesvariBook10}
Szepesv\'ari, {\relax Cs}.
\newblock \emph{Algorithms for Reinforcement Learning}.
\newblock Morgan Claypool Publishers, 2010.

\bibitem[van~de Geer(2000)]{vandeGeer00}
van~de Geer, S.~A.
\newblock \emph{Empirical Processes in M-Estimation}.
\newblock Cambridge University Press, 2000.

\bibitem[van Hasselt \& Wiering(2007)van Hasselt and
  Wiering]{vanhasselt2007reinforcement}
van Hasselt, H. and Wiering, M.~A.
\newblock Reinforcement learning in continuous action spaces.
\newblock In \emph{IEEE International Symposium on Approximate Dynamic
  Programming and Reinforcement Learning (ADPRL)}, pp.\  272--279, 2007.

\bibitem[Wang et~al.(2013)Wang, Wang, and Xu]{haiyan2013socialpde}
Wang, H., Wang, F., and Xu, K.
\newblock Modeling information diffusion in online social networks with partial
  differential equations.
\newblock \emph{arXiv:1310.0505}, 2013.

\bibitem[Yang \& Barron(1999)Yang and Barron]{YangBarron1999}
Yang, Y. and Barron, A.~R.
\newblock Information-theoretic determination of minimax rates of convergence.
\newblock \emph{The Annals of Statistics}, 27\penalty0 (5):\penalty0
  1564--1599, 1999.

\end{thebibliography}
